{
    "author": "zucchini-nlp",
    "message": "ðŸš¨ [v5] Refactor RoPE for layer types (#39847)\n\n* update\n\n* batch update model code\n\n* typos\n\n* too many diffs, dump\n\n* dump again\n\n* another dump\n\n* fix copies\n\n* make `rope_scaling_dict` self attr\n\n* fix a few more tests\n\n* another update\n\n* fix a few more tests, hopefully last ones\n\n* fox copies\n\n* fix copies again\n\n* fix newly added models, I hate rebasing on main\n\n* update config files\n\n* modular files\n\n* fix rope utils test\n\n* docstring has to be indented more, why?\n\n* oops forgot to update some modualr files\n\n* copy from doesn't copy decorators?\n\n* fix overriden test as well\n\n* add a new test\n\n* fix failing tests again\n\n* update docstrings\n\n* fix phi3\n\n* fix two models\n\n* fix copies\n\n* forgot to add\n\n* stupid bug from modular conversion\n\n* fix slow tests\n\n* update to call rotary emb once per model forward\n\n* 3K tests failing?!\n\n* update\n\n* update more models\n\n* fix copies\n\n* fix the rest of tests hopefully\n\n* fix after rebase\n\n* fix the rope tests\n\n* fix docs omni\n\n* change a bit\n\n* models with layer types\n\n* why it was deleted?\n\n* fix a few tests\n\n* fix last test!\n\n* delete extra empty lines\n\n* add a test case\n\n* more changes\n\n* fix models\n\n* typing hint for nested rope params\n\n* missed when resolving conflicts\n\n* delete layer types and fix typo\n\n* fix copies\n\n* fix copies\n\n* update docs text\n\n* docs\n\n* huuge update all models\n\n* fix copies\n\n* rename attr to align with new format\n\n* delete redundant rope tests\n\n* trigger ci\n\n* update the case\n\n* this is why i hate rebasing\n\n* maybe fixed?\n\n* oops\n\n* now fix?\n\n* fix last tests and copies\n\n* fix copies?\n\n* fix minimax and gemma3n\n\n* update typo\n\n* deprecation end version\n\n* final fix copies :fingers-crossed:\n\n* oh my, add the docs in toctree\n\n* oke, this is really the last fix\n\n* fix copies and hope that tests won't start failing again\n\n* use rope scaling if saved\n\n* fix slow tests\n\n* fix cwm and unrelated deepseek\n\n* fix last\n\n* update\n\n* hope it works now, it took so long\n\n* lets keep None for now, I will try to remove after checking tests\n\n* some more fixes, i find and replace does not always find all cases\n\n* last fix of tests\n\n* arthur's comment for extra foreward kwargs\n\n* delete unused code\n\n* fix slow qwen tests\n\n* delete layer types from models\n\n* faulty modular conversion\n\n* fix qwen omni\n\n* fix copies and style\n\n* address my comment\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "10de06dacef06605be3296f776d4122697637d6c",
    "files": [
        {
            "sha": "f2fe366a69fa937dfdfc83d65b81f16ebdf0ff0b",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -1253,6 +1253,8 @@\n       title: Importing Utilities\n     - local: internal/time_series_utils\n       title: Utilities for Time Series\n+    - local: internal/rope_utils\n+      title: Rotary Embeddings Utilities\n     title: Internal helpers\n   - sections:\n     - local: reference/environment_variables"
        },
        {
            "sha": "36f600bfe91114075fd4850a6b72eefe3510d582",
            "filename": "docs/source/en/internal/rope_utils.md",
            "status": "added",
            "additions": 89,
            "deletions": 0,
            "changes": 89,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/docs%2Fsource%2Fen%2Finternal%2Frope_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/docs%2Fsource%2Fen%2Finternal%2Frope_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Finternal%2Frope_utils.md?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -0,0 +1,89 @@\n+<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# Utilities for Rotary Embedding\n+\n+This page explains how the Rotary Embedding is computed and applied in Transformers and what types of RoPE are supported.\n+\n+\n+## Overview\n+\n+Rotary Position Embeddings are a technique used to inject positional information into attention mechanisms without relying on explicit position encodings.  \n+Instead of adding position vectors to token embeddings, RoPE rotates query and key vectors in the complex plane according to their positions enabling relative positional awareness and better extrapolation to unseen sequence lengths.\n+\n+The Transformers library provides a flexible and extensible implementation of various RoPE types defined in `[`~modeling_rope_utils.ROPE_VALIDATION_FUNCTIONS`]`, including both the default and scaled variants:\n+\n+| Rope Type | Description |\n+|------------|-------------|\n+| `\"default\"` | Standard rotary embedding as in LLaMA. |\n+| `\"linear\"` | Linear-scaled RoPE which allows longer context windows. |\n+| `\"dynamic\"` | NTK-aware scaling computed by rescaling frequency base (`Î¸`) for longer context. |\n+| `\"yarn\"` | YaRN scaling variant providing smoother extrapolation and stability. |\n+| `\"longrope\"` | [LongRoPE](https://github.com/microsoft/LongRoPE) scaling as in Phi-2 model series. |\n+| `\"llama3\"` | RoPE scaling as in Llama3.1. |\n+\n+\n+# Configuration in Model Configs\n+\n+To enable and customize rotary embeddings, add a `rope_parameters` field to your modelâ€™s configuration file (`config.json`). This field controls the RoPE behavior across model layers. Note that each RoPE variant defines its own set of expected keys and missing keys will raise an error. See the example below which creates a llama config with default RoPE parameters: \n+\n+\n+```python\n+from transformers import LlamaConfig\n+\n+config = LlamaConfig()\n+config.rope_parameters = {\n+    \"rope_type\": \"default\", # type of RoPE to use\n+    \"rope_theta\": 10000.0 # base frequency parameter\n+}\n+\n+# If we want to apply a scaled RoPE type, we need to pass extra parameters\n+config.rope_parameters = {\n+    \"rope_type\": \"linear\",\n+    \"rope_theta\": 10000.0,\n+    \"factor\": 8.0  # scale factor for context extension\n+}\n+```\n+\n+## Per-Layer-Type RoPE Configuration\n+\n+Some models such as Gemma-3 use different layer types with different attention mechanisms, i.e. \"full attention\" in some blocks and \"sliding-window attention\" in others. Transformers supports specifying distinct RoPE parameters per layer type for these models. In this case, `rope_parameters` should be a nested dictionary, where top-level keys correspond to `config.layer_types` and values are per-type RoPE parameters. During model initialization, each decoder layer will automatically look up the matching RoPE configuration based on its declared layer type.\n+\n+\n+```python\n+from transformers import Gemma3Config\n+\n+config = Gemma3Config()\n+config.rope_parameters = {\n+    \"full_attention\": {\n+        \"rope_type\": \"dynamic\",\n+        \"rope_theta\": 1000000.0,\n+        \"factor\": 8.0,\n+        \"original_max_position_embeddings\": 8096,\n+    },\n+    \"sliding_attention\": {\n+        \"rope_type\": \"default\",\n+        \"rope_theta\": 10000.0,\n+    }\n+}\n+```\n+\n+# Utilities\n+\n+[[autodoc]] RopeParameters\n+    - __call__\n+\n+"
        },
        {
            "sha": "aa0dd5d052834eca38ea1aa8f6657e1644d0b15a",
            "filename": "docs/source/en/modular_transformers.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/docs%2Fsource%2Fen%2Fmodular_transformers.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/docs%2Fsource%2Fen%2Fmodular_transformers.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodular_transformers.md?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -288,7 +288,7 @@ class Olmo2DecoderLayer(OlmoDecoderLayer):\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         **kwargs,\n     ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         residual = hidden_states"
        },
        {
            "sha": "e0cf091716b2b191fb06ece82f2b0557a9109138",
            "filename": "examples/modular-transformers/configuration_duplicated_method.py",
            "status": "modified",
            "additions": 34,
            "deletions": 67,
            "changes": 101,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/examples%2Fmodular-transformers%2Fconfiguration_duplicated_method.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/examples%2Fmodular-transformers%2Fconfiguration_duplicated_method.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fconfiguration_duplicated_method.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -5,8 +5,10 @@\n #                          modular_duplicated_method.py file directly. One of our CI enforces this.\n #                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n \n+from typing import Optional\n+\n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import rope_config_validation\n+from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n \n \n class DuplicatedMethodConfig(PreTrainedConfig):\n@@ -65,45 +67,10 @@ class DuplicatedMethodConfig(PreTrainedConfig):\n             results. Please refer to [this issue](https://github.com/pytorch/pytorch/issues/76232).\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie weight embeddings\n-        rope_theta (`float`, *optional*, defaults to 10000.0):\n-            The base period of the RoPE embeddings.\n-        rope_scaling (`Dict`, *optional*):\n-            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n-            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n-            accordingly.\n-            Expected contents:\n-                `rope_type` (`str`):\n-                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n-                    'duplicated_method3'], with 'default' being the original RoPE implementation.\n-                `factor` (`float`, *optional*):\n-                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n-                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n-                    original maximum pre-trained length.\n-                `original_max_position_embeddings` (`int`, *optional*):\n-                    Used with 'dynamic', 'longrope' and 'duplicated_method3'. The original max position embeddings used during\n-                    pretraining.\n-                `attention_factor` (`float`, *optional*):\n-                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n-                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n-                    `factor` field to infer the suggested value.\n-                `beta_fast` (`float`, *optional*):\n-                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n-                    ramp function. If unspecified, it defaults to 32.\n-                `beta_slow` (`float`, *optional*):\n-                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n-                    ramp function. If unspecified, it defaults to 1.\n-                `short_factor` (`list[float]`, *optional*):\n-                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n-                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n-                    size divided by the number of attention heads divided by 2\n-                `long_factor` (`list[float]`, *optional*):\n-                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n-                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n-                    size divided by the number of attention heads divided by 2\n-                `low_freq_factor` (`float`, *optional*):\n-                    Only used with 'duplicated_method3'. Scaling factor applied to low frequency components of the RoPE\n-                `high_freq_factor` (`float`, *optional*):\n-                    Only used with 'duplicated_method3'. Scaling factor applied to high frequency components of the RoPE\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         attention_bias (`bool`, *optional*, defaults to `False`):\n             Whether to use a bias in the query, key, value and output projection layers during self-attention.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n@@ -146,28 +113,27 @@ class DuplicatedMethodConfig(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size=32000,\n-        hidden_size=4096,\n-        intermediate_size=11008,\n-        num_hidden_layers=32,\n-        num_attention_heads=32,\n-        num_key_value_heads=None,\n-        hidden_act=\"silu\",\n-        max_position_embeddings=2048,\n-        initializer_range=0.02,\n-        rms_norm_eps=1e-6,\n-        use_cache=True,\n-        pad_token_id=None,\n-        bos_token_id=1,\n-        eos_token_id=2,\n-        pretraining_tp=1,\n-        tie_word_embeddings=False,\n-        rope_theta=10000.0,\n-        rope_scaling=None,\n-        attention_bias=False,\n-        attention_dropout=0.0,\n-        mlp_bias=False,\n-        head_dim=None,\n+        vocab_size: Optional[int] = 32000,\n+        hidden_size: Optional[int] = 4096,\n+        intermediate_size: Optional[int] = 11008,\n+        num_hidden_layers: Optional[int] = 32,\n+        num_attention_heads: Optional[int] = 32,\n+        num_key_value_heads: Optional[int] = None,\n+        hidden_act: Optional[str] = \"silu\",\n+        max_position_embeddings: Optional[int] = 2048,\n+        initializer_range: Optional[float] = 0.02,\n+        rms_norm_eps: Optional[int] = 1e-6,\n+        use_cache: Optional[bool] = True,\n+        pad_token_id: Optional[int] = None,\n+        bos_token_id: Optional[int] = 1,\n+        eos_token_id: Optional[int] = 2,\n+        pretraining_tp: Optional[int] = 1,\n+        tie_word_embeddings: Optional[bool] = False,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        attention_bias: Optional[bool] = False,\n+        attention_dropout: Optional[float] = 0.0,\n+        mlp_bias: Optional[bool] = False,\n+        head_dim: Optional[int] = None,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size\n@@ -187,16 +153,17 @@ def __init__(\n         self.rms_norm_eps = rms_norm_eps\n         self.pretraining_tp = pretraining_tp\n         self.use_cache = use_cache\n-        self.rope_theta = rope_theta\n-        self.rope_scaling = rope_scaling\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n         self.mlp_bias = mlp_bias\n         self.head_dim = head_dim if head_dim is not None else self.hidden_size // self.num_attention_heads\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n+\n         # Validate the correctness of rotary position embeddings parameters\n-        # BC: if there is a 'type' field, copy it it to 'rope_type'.\n-        if self.rope_scaling is not None and \"type\" in self.rope_scaling:\n-            self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n+        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n         rope_config_validation(self)\n \n         super().__init__("
        },
        {
            "sha": "d34088c42799630fce253d6a34aee96f6ac3d7c1",
            "filename": "examples/modular-transformers/configuration_my_new_model.py",
            "status": "modified",
            "additions": 37,
            "deletions": 34,
            "changes": 71,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/examples%2Fmodular-transformers%2Fconfiguration_my_new_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/examples%2Fmodular-transformers%2Fconfiguration_my_new_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fconfiguration_my_new_model.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -5,8 +5,10 @@\n #                          modular_my_new_model.py file directly. One of our CI enforces this.\n #                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n \n+from typing import Optional\n+\n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import rope_config_validation\n+from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n \n \n class MyNewModelConfig(PreTrainedConfig):\n@@ -147,38 +149,30 @@ class MyNewModelConfig(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size=32000,\n-        hidden_size=4096,\n-        intermediate_size=11008,\n-        num_hidden_layers=32,\n-        num_attention_heads=32,\n-        num_key_value_heads=None,\n-        hidden_act=\"silu\",\n-        max_position_embeddings=2048,\n-        initializer_range=0.02,\n-        rms_norm_eps=1e-6,\n-        use_cache=True,\n-        pad_token_id=None,\n-        bos_token_id=1,\n-        eos_token_id=2,\n-        pretraining_tp=1,\n-        tie_word_embeddings=False,\n-        rope_theta=10000.0,\n-        rope_scaling=None,\n-        attention_bias=False,\n-        attention_dropout=0.0,\n+        vocab_size: Optional[int] = 32000,\n+        hidden_size: Optional[int] = 4096,\n+        intermediate_size: Optional[int] = 11008,\n+        num_hidden_layers: Optional[int] = 32,\n+        num_attention_heads: Optional[int] = 32,\n+        num_key_value_heads: Optional[int] = None,\n+        hidden_act: Optional[str] = \"silu\",\n+        max_position_embeddings: Optional[int] = 2048,\n+        initializer_range: Optional[float] = 0.02,\n+        rms_norm_eps: Optional[int] = 1e-6,\n+        use_cache: Optional[bool] = True,\n+        pad_token_id: Optional[int] = None,\n+        bos_token_id: Optional[int] = 1,\n+        eos_token_id: Optional[int] = 2,\n+        pretraining_tp: Optional[int] = 1,\n+        tie_word_embeddings: Optional[bool] = False,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        attention_bias: Optional[bool] = False,\n+        attention_dropout: Optional[float] = 0.0,\n         mlp_bias=True,\n-        head_dim=None,\n+        head_dim: Optional[int] = None,\n         new_param=0,\n         **kwargs,\n     ):\n-        super().__init__(\n-            pad_token_id=pad_token_id,\n-            bos_token_id=bos_token_id,\n-            eos_token_id=eos_token_id,\n-            tie_word_embeddings=tie_word_embeddings,\n-            **kwargs,\n-        )\n         self.vocab_size = vocab_size\n         self.max_position_embeddings = max_position_embeddings\n         self.hidden_size = hidden_size\n@@ -196,15 +190,24 @@ def __init__(\n         self.rms_norm_eps = rms_norm_eps\n         self.pretraining_tp = pretraining_tp\n         self.use_cache = use_cache\n-        self.rope_theta = rope_theta\n-        self.rope_scaling = rope_scaling\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n         self.mlp_bias = mlp_bias\n         self.head_dim = head_dim if head_dim is not None else self.hidden_size // self.num_attention_heads\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n+\n         # Validate the correctness of rotary position embeddings parameters\n-        # BC: if there is a 'type' field, copy it it to 'rope_type'.\n-        if self.rope_scaling is not None and \"type\" in self.rope_scaling:\n-            self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n+        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n         rope_config_validation(self)\n+\n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n         self.new_param = new_param"
        },
        {
            "sha": "8a1415696508ad62f3c24d8cd762aaf6b1c3f2c9",
            "filename": "examples/modular-transformers/configuration_my_new_model2.py",
            "status": "modified",
            "additions": 29,
            "deletions": 28,
            "changes": 57,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/examples%2Fmodular-transformers%2Fconfiguration_my_new_model2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/examples%2Fmodular-transformers%2Fconfiguration_my_new_model2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fconfiguration_my_new_model2.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -4,9 +4,10 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_my_new_model2.py file directly. One of our CI enforces this.\n #                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import rope_config_validation\n+from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n \n \n class MyNewModel2Config(PreTrainedConfig):\n@@ -51,28 +52,27 @@ class MyNewModel2Config(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size=32000,\n-        hidden_size=4096,\n-        intermediate_size=11008,\n-        num_hidden_layers=32,\n-        num_attention_heads=32,\n-        num_key_value_heads=None,\n-        hidden_act=\"silu\",\n-        max_position_embeddings=2048,\n-        initializer_range=0.02,\n-        rms_norm_eps=1e-6,\n-        use_cache=True,\n-        pad_token_id=None,\n-        bos_token_id=1,\n-        eos_token_id=2,\n-        pretraining_tp=1,\n-        tie_word_embeddings=False,\n-        rope_theta=10000.0,\n-        rope_scaling=None,\n-        attention_bias=False,\n-        attention_dropout=0.0,\n-        mlp_bias=False,\n-        head_dim=None,\n+        vocab_size: Optional[int] = 32000,\n+        hidden_size: Optional[int] = 4096,\n+        intermediate_size: Optional[int] = 11008,\n+        num_hidden_layers: Optional[int] = 32,\n+        num_attention_heads: Optional[int] = 32,\n+        num_key_value_heads: Optional[int] = None,\n+        hidden_act: Optional[str] = \"silu\",\n+        max_position_embeddings: Optional[int] = 2048,\n+        initializer_range: Optional[float] = 0.02,\n+        rms_norm_eps: Optional[int] = 1e-6,\n+        use_cache: Optional[bool] = True,\n+        pad_token_id: Optional[int] = None,\n+        bos_token_id: Optional[int] = 1,\n+        eos_token_id: Optional[int] = 2,\n+        pretraining_tp: Optional[int] = 1,\n+        tie_word_embeddings: Optional[bool] = False,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        attention_bias: Optional[bool] = False,\n+        attention_dropout: Optional[float] = 0.0,\n+        mlp_bias: Optional[bool] = False,\n+        head_dim: Optional[int] = None,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size\n@@ -92,16 +92,17 @@ def __init__(\n         self.rms_norm_eps = rms_norm_eps\n         self.pretraining_tp = pretraining_tp\n         self.use_cache = use_cache\n-        self.rope_theta = rope_theta\n-        self.rope_scaling = rope_scaling\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n         self.mlp_bias = mlp_bias\n         self.head_dim = head_dim if head_dim is not None else self.hidden_size // self.num_attention_heads\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n+\n         # Validate the correctness of rotary position embeddings parameters\n-        # BC: if there is a 'type' field, copy it it to 'rope_type'.\n-        if self.rope_scaling is not None and \"type\" in self.rope_scaling:\n-            self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n+        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n         rope_config_validation(self)\n \n         super().__init__("
        },
        {
            "sha": "0dd5efe4e89b89603f7fcd922487921968dfce24",
            "filename": "examples/modular-transformers/modeling_my_new_model2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -156,8 +156,8 @@ def __init__(self, config: MyNewModel2Config, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n-        attention_mask: Optional[torch.Tensor],\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -207,7 +207,6 @@ def __init__(self, config: MyNewModel2Config, layer_idx: int):\n         self.mlp = MyNewModel2MLP(config)\n         self.input_layernorm = MyNewModel2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = MyNewModel2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n-        self.attention_type = config.layer_types[layer_idx]\n \n     def forward(\n         self,\n@@ -217,7 +216,7 @@ def forward(\n         past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> torch.Tensor:\n         residual = hidden_states"
        },
        {
            "sha": "f349978133dbc0f6fe345209b7f842a1a2cadb00",
            "filename": "examples/modular-transformers/modeling_super.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/examples%2Fmodular-transformers%2Fmodeling_super.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/examples%2Fmodular-transformers%2Fmodeling_super.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_super.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -51,8 +51,8 @@ class SuperRotaryEmbedding(nn.Module):\n     def __init__(self, config: SuperConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+        if hasattr(config, \"rope_parameters\") and isinstance(config.rope_parameters, dict):\n+            self.rope_type = config.rope_parameters.get(\"rope_type\", config.rope_parameters.get(\"type\"))\n         else:\n             self.rope_type = \"default\"\n         self.max_seq_len_cached = config.max_position_embeddings\n@@ -258,7 +258,7 @@ def forward(\n         past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> torch.Tensor:\n         residual = hidden_states"
        },
        {
            "sha": "404b46b65b3f478c675f17ab8ac24cfb08f39dfe",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -440,7 +440,7 @@\n     _import_structure[\"modeling_flash_attention_utils\"] = []\n     _import_structure[\"modeling_layers\"] = [\"GradientCheckpointingLayer\"]\n     _import_structure[\"modeling_outputs\"] = []\n-    _import_structure[\"modeling_rope_utils\"] = [\"ROPE_INIT_FUNCTIONS\", \"dynamic_rope_update\"]\n+    _import_structure[\"modeling_rope_utils\"] = [\"ROPE_INIT_FUNCTIONS\", \"dynamic_rope_update\", \"RopeParameters\"]\n     _import_structure[\"modeling_utils\"] = [\"PreTrainedModel\", \"AttentionInterface\"]\n     _import_structure[\"masking_utils\"] = [\"AttentionMaskInterface\"]\n     _import_structure[\"optimization\"] = [\n@@ -619,6 +619,7 @@\n     from .modelcard import ModelCard as ModelCard\n     from .modeling_layers import GradientCheckpointingLayer as GradientCheckpointingLayer\n     from .modeling_rope_utils import ROPE_INIT_FUNCTIONS as ROPE_INIT_FUNCTIONS\n+    from .modeling_rope_utils import RopeParameters as RopeParameters\n     from .modeling_rope_utils import dynamic_rope_update as dynamic_rope_update\n     from .modeling_utils import AttentionInterface as AttentionInterface\n     from .modeling_utils import PreTrainedModel as PreTrainedModel"
        },
        {
            "sha": "4c93c02ea1da56542eaac81c82b61b299d2a3023",
            "filename": "src/transformers/configuration_utils.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconfiguration_utils.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -417,6 +417,14 @@ def torch_dtype(self, value):\n         logger.warning_once(\"`torch_dtype` is deprecated! Use `dtype` instead!\")\n         self.dtype = value\n \n+    @property\n+    def rope_scaling(self):\n+        return self.rope_parameters\n+\n+    @rope_scaling.setter\n+    def rope_scaling(self, value):\n+        self.rope_parameters = value\n+\n     def save_pretrained(self, save_directory: Union[str, os.PathLike], push_to_hub: bool = False, **kwargs):\n         \"\"\"\n         Save a configuration object to the directory `save_directory`, so that it can be re-loaded using the"
        },
        {
            "sha": "927b437c1bc3ff8cbfeacbe0644919bc3abbad94",
            "filename": "src/transformers/modeling_rope_utils.py",
            "status": "modified",
            "additions": 357,
            "deletions": 193,
            "changes": 550,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodeling_rope_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodeling_rope_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_rope_utils.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -14,7 +14,7 @@\n \n import math\n from functools import wraps\n-from typing import Optional\n+from typing import Optional, TypedDict, Union\n \n from .configuration_utils import PreTrainedConfig\n from .utils import is_torch_available, logging\n@@ -27,6 +27,57 @@\n     import torch\n \n \n+def standardize_rope_params(config, rope_theta: Optional[Union[float, dict[str, float]]] = None):\n+    \"\"\"\n+    Helper to standardize the config's rope params field by ensuring the params are defined for each\n+    later type. For old model the fn will duplicate a single rope param in each layer type (backward compatibility)\n+    \"\"\"\n+    rope_parameters = getattr(config, \"rope_parameters\", None)\n+    layer_types = getattr(config, \"layer_types\", None)\n+    if rope_theta is None:\n+        rope_theta = getattr(config, \"rope_theta\", None)\n+\n+    # Case 1: one RoPE theat = one RoPE param per model without nesting\n+    if not isinstance(rope_theta, dict):\n+        if rope_parameters is None:\n+            rope_parameters = {\"rope_type\": \"default\", \"rope_theta\": rope_theta}\n+        else:\n+            # BC: if there is a 'type' field, copy it it to 'rope_type'.\n+            rope_type = rope_parameters.get(\"rope_type\", rope_parameters.get(\"type\", \"default\"))\n+            rope_theta = rope_parameters.get(\"rope_theta\") or rope_theta\n+            rope_parameters.update({\"rope_theta\": rope_theta, \"rope_type\": rope_type})\n+        config.rope_parameters = rope_parameters\n+\n+    # Case 2: different RoPE for each layer as nested dict\n+    else:\n+        rope_parameters_per_layer_type = {}\n+        for layer_type in layer_types:\n+            if rope_parameters is None:\n+                rope_parameters_per_layer_type[layer_type] = {\n+                    \"rope_type\": \"default\",\n+                    \"rope_theta\": rope_theta[layer_type],\n+                }\n+            else:\n+                is_field_in_new_format = any(layer_type in rope_parameters for layer_type in layer_types)\n+                if not is_field_in_new_format:\n+                    curr_rope_type = rope_parameters.get(\"rope_type\", rope_parameters.get(\"type\"))\n+                    rope_parameters_per_layer_type[layer_type] = {\n+                        **rope_parameters,\n+                        \"rope_type\": curr_rope_type,\n+                        \"rope_theta\": rope_theta[layer_type],\n+                    }\n+                else:\n+                    curr_rope_type = rope_parameters[layer_type].get(\n+                        \"rope_type\", rope_parameters[layer_type].get(\"type\")\n+                    )\n+                    rope_parameters_per_layer_type[layer_type] = {\n+                        **rope_parameters[layer_type],\n+                        \"rope_type\": curr_rope_type,\n+                        \"rope_theta\": rope_theta[layer_type],\n+                    }\n+            config.rope_parameters = rope_parameters_per_layer_type\n+\n+\n def dynamic_rope_update(rope_forward):\n     \"\"\"\n     Decorator function to update the RoPE parameters in the forward pass, if the model is using a dynamic RoPE\n@@ -40,62 +91,98 @@ def dynamic_rope_update(rope_forward):\n         The decorated forward pass.\n     \"\"\"\n \n-    def longrope_frequency_update(self, position_ids, device):\n+    def longrope_frequency_update(self, position_ids, device, layer_type=None):\n         \"\"\"Longrope uses long factor if sequence is larger than original pretraining length, short otherwise.\"\"\"\n         seq_len = torch.max(position_ids) + 1\n-        if hasattr(self.config, \"original_max_position_embeddings\"):\n-            original_max_position_embeddings = self.config.original_max_position_embeddings\n+        original_max_position_embeddings = getattr(\n+            self.config, \"original_max_position_embeddings\", self.config.max_position_embeddings\n+        )\n+        if layer_type is None:\n+            rope_type = self.rope_type\n+            original_inv_freq = self.original_inv_freq\n+            prefix = \"\"\n         else:\n-            original_max_position_embeddings = self.config.max_position_embeddings\n+            rope_type = self.rope_type[layer_type]\n+            original_inv_freq = getattr(self, f\"{layer_type}_original_inv_freq\")\n+            prefix = f\"{layer_type}_\"\n+\n         if seq_len > original_max_position_embeddings:\n-            if not hasattr(self, \"long_inv_freq\"):\n-                self.long_inv_freq, _ = self.rope_init_fn(\n-                    self.config, device, seq_len=original_max_position_embeddings + 1\n+            if not hasattr(self, f\"{layer_type}_long_inv_freq\"):\n+                rope_init_fn = ROPE_INIT_FUNCTIONS[rope_type]\n+                long_inv_freq, _ = rope_init_fn(\n+                    self.config,\n+                    device,\n+                    seq_len=original_max_position_embeddings + 1,\n+                    layer_type=layer_type,\n                 )\n-            self.register_buffer(\"inv_freq\", self.long_inv_freq, persistent=False)\n+            self.register_buffer(f\"{prefix}inv_freq\", long_inv_freq, persistent=False)\n+            setattr(self, f\"{prefix}long_inv_freq\", long_inv_freq)\n         else:\n             # This .to() is needed if the model has been moved to a device after being initialized (because\n             # the buffer is automatically moved, but not the original copy)\n-            self.original_inv_freq = self.original_inv_freq.to(device)\n-            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n+            original_inv_freq = original_inv_freq.to(device)\n+            self.register_buffer(f\"{prefix}inv_freq\", original_inv_freq, persistent=False)\n+            setattr(self, f\"{prefix}original_inv_freq\", original_inv_freq)\n \n-    def dynamic_frequency_update(self, position_ids, device):\n+    def dynamic_frequency_update(self, position_ids, device, layer_type=None):\n         \"\"\"\n         dynamic RoPE layers should recompute `inv_freq` in the following situations:\n         1 - growing beyond the cached sequence length (allow scaling)\n         2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n         \"\"\"\n         seq_len = torch.max(position_ids) + 1\n-        if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n-            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n-            self.max_seq_len_cached = seq_len\n+        if layer_type is None:\n+            rope_type = self.rope_type\n+            max_seq_len_cached = self.max_seq_len_cached\n+            original_inv_freq = self.original_inv_freq\n+            prefix = \"\"\n+        else:\n+            rope_type = self.rope_type[layer_type]\n+            max_seq_len_cached = getattr(self, f\"{layer_type}_max_seq_len_cached\", self.max_seq_len_cached)\n+            original_inv_freq = getattr(self, f\"{layer_type}_original_inv_freq\")\n+            prefix = f\"{layer_type}_\"\n+\n+        if seq_len > max_seq_len_cached:  # growth\n+            rope_init_fn = ROPE_INIT_FUNCTIONS[rope_type]\n+            inv_freq, self.attention_scaling = rope_init_fn(\n+                self.config,\n+                device,\n+                seq_len=seq_len,\n+                layer_type=layer_type,\n+            )\n+            # TODO joao: may break with compilation\n+            self.register_buffer(f\"{prefix}inv_freq\", inv_freq, persistent=False)\n+            setattr(self, f\"{layer_type}_max_seq_len_cached\", seq_len)\n \n-        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n+        if seq_len < self.original_max_seq_len and max_seq_len_cached > self.original_max_seq_len:  # reset\n             # This .to() is needed if the model has been moved to a device after being initialized (because\n             # the buffer is automatically moved, but not the original copy)\n-            self.original_inv_freq = self.original_inv_freq.to(device)\n-            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n-            self.max_seq_len_cached = self.original_max_seq_len\n+            original_inv_freq = original_inv_freq.to(device)\n+            self.register_buffer(f\"{prefix}inv_freq\", original_inv_freq, persistent=False)\n+            setattr(self, f\"{prefix}original_inv_freq\", original_inv_freq)\n+            setattr(self, f\"{layer_type}_max_seq_len_cached\", self.original_max_seq_len)\n \n     @wraps(rope_forward)\n-    def wrapper(self, x, position_ids):\n-        if \"dynamic\" in self.rope_type:\n-            dynamic_frequency_update(self, position_ids, device=x.device)\n-        elif self.rope_type == \"longrope\":\n-            longrope_frequency_update(self, position_ids, device=x.device)\n-        return rope_forward(self, x, position_ids)\n+    def wrapper(self, x, position_ids, layer_type=None):\n+        rope_type = self.rope_type if layer_type is None else self.rope_type[layer_type]\n+        kwargs = {\"layer_type\": layer_type} if layer_type is not None else {}\n+        if \"dynamic\" in rope_type:\n+            dynamic_frequency_update(self, position_ids, device=x.device, **kwargs)\n+        elif rope_type == \"longrope\":\n+            longrope_frequency_update(self, position_ids, device=x.device, **kwargs)\n+        return rope_forward(self, x, position_ids, **kwargs)\n \n     return wrapper\n \n \n-def _compute_default_rope_parameters(\n+def _compute_linear_scaling_rope_parameters(\n     config: Optional[PreTrainedConfig] = None,\n     device: Optional[\"torch.device\"] = None,\n     seq_len: Optional[int] = None,\n+    layer_type: Optional[str] = None,\n ) -> tuple[\"torch.Tensor\", float]:\n     \"\"\"\n-    Computes the inverse frequencies according to the original RoPE implementation\n+    Computes the inverse frequencies with linear scaling. Credits to the Reddit user /u/kaiokendev\n     Args:\n         config ([`~transformers.PreTrainedConfig`]):\n             The model configuration. This function assumes that the config will provide at least the following\n@@ -120,53 +207,20 @@ def _compute_default_rope_parameters(\n         Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n         post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n     \"\"\"\n-    base = config.rope_theta\n+    # For backward compatibility standardize the `rope_parameters_dict` if it uses old format\n+    standardize_rope_params(config)\n+    rope_parameters_dict = config.rope_parameters[layer_type] if layer_type is not None else config.rope_parameters\n+    factor = rope_parameters_dict[\"factor\"]\n+\n+    # Gets the default RoPE parameters\n+    base = rope_parameters_dict[\"rope_theta\"]\n     partial_rotary_factor = getattr(config, \"partial_rotary_factor\", 1.0)\n     head_dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n     dim = int(head_dim * partial_rotary_factor)\n-\n     attention_factor = 1.0  # Unused in this type of RoPE\n \n     # Compute the inverse frequencies\n     inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim))\n-    return inv_freq, attention_factor\n-\n-\n-def _compute_linear_scaling_rope_parameters(\n-    config: Optional[PreTrainedConfig] = None,\n-    device: Optional[\"torch.device\"] = None,\n-    seq_len: Optional[int] = None,\n-) -> tuple[\"torch.Tensor\", float]:\n-    \"\"\"\n-    Computes the inverse frequencies with linear scaling. Credits to the Reddit user /u/kaiokendev\n-    Args:\n-        config ([`~transformers.PreTrainedConfig`]):\n-            The model configuration. This function assumes that the config will provide at least the following\n-            properties:\n-\n-            *   rope_theta (`float`): The base wavelength from which the inverse frequencies will be derived.\n-            *   hidden_size (`int`): The numerator when deriving a head_dim, if not provided directly.\n-            *   num_attention_heads (`int`): The denominator when deriving a head_dim, if not provided directly.\n-\n-            Additionally, this function will make use of the following properties if they are found in the config:\n-\n-            *   head_dim (`int`, *optional*): The size of the key-value heads in the model. If None, this value will be\n-                derived as hidden_size // num_attention_heads.\n-            *   partial_rotary_factor (`float`, *optional*): If less than 1.0, inverse frequencies will be returned for\n-                the first fraction of the head_dim. Defaults to 1.0.\n-        device (`torch.device`):\n-            The device to use for initialization of the inverse frequencies.\n-        seq_len (`int`, *optional*):\n-            The current sequence length. Unused for this type of RoPE.\n-\n-    Returns:\n-        Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n-        post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n-    \"\"\"\n-    factor = config.rope_scaling[\"factor\"]\n-\n-    # Gets the default RoPE parameters\n-    inv_freq, attention_factor = _compute_default_rope_parameters(config, device, seq_len)\n \n     # Then applies linear scaling to the frequencies.\n     # NOTE: originally, scaling was applied to the position_ids. However, we get `embs = inv_freq @ position_ids`, so\n@@ -179,6 +233,7 @@ def _compute_dynamic_ntk_parameters(\n     config: Optional[PreTrainedConfig] = None,\n     device: Optional[\"torch.device\"] = None,\n     seq_len: Optional[int] = None,\n+    layer_type: Optional[str] = None,\n ) -> tuple[\"torch.Tensor\", float]:\n     \"\"\"\n     Computes the inverse frequencies with NTK scaling. Credits to the Reddit users /u/bloc97 and /u/emozilla\n@@ -193,7 +248,7 @@ def _compute_dynamic_ntk_parameters(\n             *   num_attention_heads (`int`): The denominator when deriving a head_dim, if not provided directly.\n             *   max_position_embeddings (`int`): The default sequence length used to update the dynamic RoPE at\n                 inference time\n-            *   rope_scaling (`dict[str, float]`): The standard RoPE scaling parameters, from which `factor`\n+            *   rope_parameters (`dict[str, float]`): The standard RoPE scaling parameters, from which `factor`\n                 will be accessed. The value of `factor` is used to determine the new base frequency, along with the\n                 current sequence length (seq_len), the maximum positional embeddings (max_position_embeddings), and the\n                 computed dimensionality (dim) of the rotary embeddings. If seq_len <= max_position_embeddings, this\n@@ -216,14 +271,17 @@ def _compute_dynamic_ntk_parameters(\n         Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n         post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n     \"\"\"\n-    # TODO (joao): use the new `original_max_position_embeddings` from rope_scaling\n-    base = config.rope_theta\n-    partial_rotary_factor = getattr(config, \"partial_rotary_factor\", 1.0)\n+    # TODO (joao): use the new `original_max_position_embeddings` from rope_parameters\n+    # For backward compatibility standardize the `rope_parameters_dict` if it uses old format\n+    standardize_rope_params(config)\n+    rope_parameters_dict = config.rope_parameters[layer_type] if layer_type is not None else config.rope_parameters\n+\n+    base = rope_parameters_dict[\"rope_theta\"]\n+    partial_rotary_factor = config.partial_rotary_factor if hasattr(config, \"partial_rotary_factor\") else 1.0\n     head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n     dim = int(head_dim * partial_rotary_factor)\n     max_position_embeddings = config.max_position_embeddings\n-    factor = config.rope_scaling[\"factor\"]\n-\n+    factor = rope_parameters_dict[\"factor\"]\n     attention_factor = 1.0  # Unused in this type of RoPE\n \n     # seq_len: default to max_position_embeddings, e.g. at init time\n@@ -244,7 +302,10 @@ def _compute_dynamic_ntk_parameters(\n \n \n def _compute_yarn_parameters(\n-    config: PreTrainedConfig, device: \"torch.device\", seq_len: Optional[int] = None\n+    config: PreTrainedConfig,\n+    device: \"torch.device\",\n+    seq_len: Optional[int] = None,\n+    layer_type: Optional[str] = None,\n ) -> tuple[\"torch.Tensor\", float]:\n     \"\"\"\n     Computes the inverse frequencies with NTK scaling. Please refer to the\n@@ -259,7 +320,7 @@ def _compute_yarn_parameters(\n             *   hidden_size (`int`): The numerator when deriving a head_dim, if not provided directly.\n             *   num_attention_heads (`int`): The denominator when deriving a head_dim, if not provided directly.\n             *   max_position_embeddings (`int`): The maximum length of the positional embeddings.\n-            *   rope_scaling (`dict[str, float | int]`): The standard RoPE scaling parameters, from which the following\n+            *   rope_parameters (`dict[str, float | int]`): The standard RoPE scaling parameters, from which the following\n                 keys will be accessed:\n                 *   `attention_factor` (`float`, *optional*): The scaling factor to be applied to the computed cos/sin.\n                     If None, the value is inferred from `factor`, `mscale`, and `mscale_all_dim` as avaialble.\n@@ -298,18 +359,28 @@ def _compute_yarn_parameters(\n         Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n         post-processing scaling factor applied to the computed cos/sin.\n     \"\"\"\n+    # For backward compatibility standardize the `rope_parameters_dict` if it uses old format\n+    standardize_rope_params(config)\n+    rope_parameters_dict = config.rope_parameters[layer_type] if layer_type is not None else config.rope_parameters\n \n-    base = config.rope_theta\n-    partial_rotary_factor = getattr(config, \"partial_rotary_factor\", 1.0)\n+    base = rope_parameters_dict[\"rope_theta\"]\n+    partial_rotary_factor = config.partial_rotary_factor if hasattr(config, \"partial_rotary_factor\") else 1.0\n     head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n     dim = int(head_dim * partial_rotary_factor)\n-    factor = config.rope_scaling[\"factor\"]\n-    attention_factor = config.rope_scaling.get(\"attention_factor\")\n-    mscale = config.rope_scaling.get(\"mscale\")\n-    mscale_all_dim = config.rope_scaling.get(\"mscale_all_dim\")\n-    original_max_position_embeddings = (\n-        config.rope_scaling.get(\"original_max_position_embeddings\") or config.max_position_embeddings\n-    )\n+\n+    factor = rope_parameters_dict[\"factor\"]\n+    attention_factor = rope_parameters_dict.get(\"attention_factor\")\n+    mscale = rope_parameters_dict.get(\"mscale\")\n+    mscale_all_dim = rope_parameters_dict.get(\"mscale_all_dim\")\n+\n+    # NOTE: DeekSeek-V3 (and potentially other models) modify `max_position_embeddings` and have a\n+    # `original_max_position_embeddings` field containing the pretrained value. They use the ratio between these two\n+    # values to compute the default attention scaling factor, instead of using `factor`.\n+    if \"original_max_position_embeddings\" in rope_parameters_dict:\n+        original_max_position_embeddings = rope_parameters_dict[\"original_max_position_embeddings\"]\n+        factor = config.max_position_embeddings / original_max_position_embeddings\n+    else:\n+        original_max_position_embeddings = config.max_position_embeddings\n \n     def get_mscale(scale, mscale=1):\n         if scale <= 1:\n@@ -324,9 +395,9 @@ def get_mscale(scale, mscale=1):\n             attention_factor = get_mscale(factor)\n \n     # Optional config options\n-    # beta_fast/beta_slow: as suggested in the paper, default to 32 and 1 respectively\n-    beta_fast = config.rope_scaling.get(\"beta_fast\") or 32\n-    beta_slow = config.rope_scaling.get(\"beta_slow\") or 1\n+    # beta_fast/beta_slow: as suggested in the paper, default to 32/1 (correspondingly)\n+    beta_fast = rope_parameters_dict.get(\"beta_fast\") or 32\n+    beta_slow = rope_parameters_dict.get(\"beta_slow\") or 1\n \n     # Compute the inverse frequencies\n     def find_correction_dim(num_rotations, dim, base, max_position_embeddings):\n@@ -356,7 +427,7 @@ def linear_ramp_factor(min, max, dim):\n     inv_freq_extrapolation = 1.0 / pos_freqs\n     inv_freq_interpolation = 1.0 / (factor * pos_freqs)\n \n-    truncate = config.rope_scaling.get(\"truncate\", True)\n+    truncate = config.rope_parameters.get(\"truncate\", True)\n     low, high = find_correction_range(beta_fast, beta_slow, dim, base, original_max_position_embeddings, truncate)\n \n     # Get n-dimensional rotational scaling corrected for extrapolation\n@@ -369,7 +440,10 @@ def linear_ramp_factor(min, max, dim):\n \n \n def _compute_longrope_parameters(\n-    config: PreTrainedConfig, device: \"torch.device\", seq_len: Optional[int] = None\n+    config: PreTrainedConfig,\n+    device: \"torch.device\",\n+    seq_len: Optional[int] = None,\n+    layer_type: Optional[str] = None,\n ) -> tuple[\"torch.Tensor\", float]:\n     \"\"\"\n     Computes the inverse frequencies with LongRoPE scaling. Please refer to the\n@@ -386,7 +460,7 @@ def _compute_longrope_parameters(\n             *   max_position_embeddings (`int`): The maximum length of the positional embeddings.\n             *   original_max_position_embeddings (`int`, *optional*): The original max position embeddings used during\n                 pretraining. If not provided, defaults to `max_position_embeddings`.\n-            *   rope_scaling (`dict[str, float]`): The standard RoPE scaling parameters, from which the following keys\n+            *   rope_parameters (`dict[str, float]`): The standard RoPE scaling parameters, from which the following keys\n                 will be accessed:\n                 *   `attention_factor` (`float`, *optional*): The scaling factor to be applied on the attention\n                     computation. If unspecified, it defaults to value recommended by the implementation, inferred from\n@@ -414,15 +488,20 @@ def _compute_longrope_parameters(\n         Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n         post-processing scaling factor applied to the computed cos/sin.\n     \"\"\"\n-    # TODO (joao): use the new `original_max_position_embeddings` from rope_scaling\n-    base = config.rope_theta\n-    partial_rotary_factor = getattr(config, \"partial_rotary_factor\", 1.0)\n+    # TODO (joao): use the new `original_max_position_embeddings` from rope_parameters\n+    # For backward compatibility standardize the `rope_parameters_dict` if it uses old format\n+    standardize_rope_params(config)\n+    rope_parameters_dict = config.rope_parameters[layer_type] if layer_type is not None else config.rope_parameters\n+\n+    base = rope_parameters_dict[\"rope_theta\"]\n+    partial_rotary_factor = config.partial_rotary_factor if hasattr(config, \"partial_rotary_factor\") else 1.0\n     head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n     dim = int(head_dim * partial_rotary_factor)\n-    long_factor = config.rope_scaling[\"long_factor\"]\n-    short_factor = config.rope_scaling[\"short_factor\"]\n-    factor = config.rope_scaling.get(\"factor\")\n-    attention_factor = config.rope_scaling.get(\"attention_factor\")\n+\n+    long_factor = rope_parameters_dict[\"long_factor\"]\n+    short_factor = rope_parameters_dict[\"short_factor\"]\n+    factor = rope_parameters_dict.get(\"factor\")\n+    attention_factor = rope_parameters_dict.get(\"attention_factor\")\n \n     # NOTE: Phi3 (and potentially other models) modify `max_position_embeddings` and have a\n     # `original_max_position_embeddings` field containing the pretrained value. They use the ratio between these two\n@@ -451,7 +530,10 @@ def _compute_longrope_parameters(\n \n \n def _compute_llama3_parameters(\n-    config: PreTrainedConfig, device: \"torch.device\", seq_len: Optional[int] = None\n+    config: PreTrainedConfig,\n+    device: \"torch.device\",\n+    seq_len: Optional[int] = None,\n+    layer_type: Optional[str] = None,\n ) -> tuple[\"torch.Tensor\", float]:\n     \"\"\"\n     Computes the inverse frequencies for llama 3.1.\n@@ -464,7 +546,7 @@ def _compute_llama3_parameters(\n             *   rope_theta (`float`): The base wavelength from which the inverse frequencies will be derived.\n             *   hidden_size (`int`): The numerator when deriving a head_dim, if not provided directly.\n             *   num_attention_heads (`int`): The denominator when deriving a head_dim, if not provided directly.\n-            *   rope_scaling (`dict[str, float | int]`): The standard RoPE scaling parameters, from which the following\n+            *   rope_parameters (`dict[str, float | int]`): The standard RoPE scaling parameters, from which the following\n                 keys will be accessed:\n                 *   `factor` (`float`, *optional*): The scaling factor applied to the inverse frequencies when 1) the\n                     wavelength is greater than `low_freq_wavelen` prior to smoothing, and 2) to all inverse frequencies\n@@ -491,13 +573,24 @@ def _compute_llama3_parameters(\n         Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n         post-processing scaling factor applied to the computed cos/sin.\n     \"\"\"\n+    # For backward compatibility standardize the `rope_parameters_dict` if it uses old format\n+    standardize_rope_params(config)\n+    rope_parameters_dict = config.rope_parameters[layer_type] if layer_type is not None else config.rope_parameters\n+\n     # Gets the default RoPE parameters\n-    inv_freq, attention_factor = _compute_default_rope_parameters(config, device, seq_len)\n+    base = rope_parameters_dict[\"rope_theta\"]\n+    partial_rotary_factor = getattr(config, \"partial_rotary_factor\", 1.0)\n+    head_dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+    dim = int(head_dim * partial_rotary_factor)\n+    attention_factor = 1.0  # Unused in this type of RoPE\n+\n+    # Compute the inverse frequencies\n+    inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim))\n \n-    factor = config.rope_scaling[\"factor\"]  # `8` in the original implementation\n-    low_freq_factor = config.rope_scaling[\"low_freq_factor\"]  # `1` in the original implementation\n-    high_freq_factor = config.rope_scaling[\"high_freq_factor\"]  # `4` in the original implementation\n-    old_context_len = config.rope_scaling[\"original_max_position_embeddings\"]  # `8192` in the original implementation\n+    factor = rope_parameters_dict[\"factor\"]  # `8` in the original implementation\n+    low_freq_factor = rope_parameters_dict[\"low_freq_factor\"]  # `1` in the original implementation\n+    high_freq_factor = rope_parameters_dict[\"high_freq_factor\"]  # `4` in the original implementation\n+    old_context_len = rope_parameters_dict[\"original_max_position_embeddings\"]  # `8192` in the original implementation\n \n     low_freq_wavelen = old_context_len / low_freq_factor\n     high_freq_wavelen = old_context_len / high_freq_factor\n@@ -516,10 +609,9 @@ def _compute_llama3_parameters(\n \n \n # This maps the \"rope_type\" string field in rope config to the corresponding function to compute the RoPE parameters\n-# from the model config. You can append new {'rope_type': callable} pairs to this dictionary to enable custom RoPE\n+# from the model config. You can append new {'rope_type': callable} pairs to this rope_parameters to enable custom RoPE\n # parameterizations, as long as the callable has the same signature.\n ROPE_INIT_FUNCTIONS = {\n-    \"default\": _compute_default_rope_parameters,\n     \"linear\": _compute_linear_scaling_rope_parameters,\n     \"dynamic\": _compute_dynamic_ntk_parameters,\n     \"yarn\": _compute_yarn_parameters,\n@@ -535,7 +627,7 @@ def _check_received_keys(\n     optional_keys: Optional[set] = None,\n     ignore_keys: Optional[set] = None,\n ):\n-    \"\"\"Compare the received keys in `config.rope_scaling` against the expected and optional keys\"\"\"\n+    \"\"\"Compare the received keys in `config.rope_parameters` against the expected and optional keys\"\"\"\n     # BC: \"rope_type\" was originally \"type\" -- let's check for \"rope_type\" when \"type\" is present\n     if \"type\" in received_keys:\n         received_keys -= {\"type\"}\n@@ -547,198 +639,206 @@ def _check_received_keys(\n \n     missing_keys = required_keys - received_keys\n     if missing_keys:\n-        raise KeyError(f\"Missing required keys in `rope_scaling` for 'rope_type'='{rope_type}': {missing_keys}\")\n+        raise KeyError(f\"Missing required keys in `rope_parameters` for 'rope_type'='{rope_type}': {missing_keys}\")\n \n     if optional_keys is not None:\n         unused_keys = received_keys - required_keys - optional_keys\n     else:\n         unused_keys = received_keys - required_keys\n     if unused_keys:\n-        logger.warning(f\"Unrecognized keys in `rope_scaling` for 'rope_type'='{rope_type}': {unused_keys}\")\n+        logger.warning(f\"Unrecognized keys in `rope_parameters` for 'rope_type'='{rope_type}': {unused_keys}\")\n \n \n-def _validate_default_rope_parameters(config: PreTrainedConfig, ignore_keys: Optional[set] = None):\n-    rope_scaling = config.rope_scaling\n-    rope_type = rope_scaling.get(\"rope_type\", rope_scaling.get(\"type\", None))  # BC: \"rope_type\" was originally \"type\"\n-    required_keys = {\"rope_type\"}\n-    received_keys = set(rope_scaling.keys())\n+def _validate_default_rope_parameters(\n+    rope_parameters: dict, config: Optional[PreTrainedConfig] = None, ignore_keys: Optional[set] = None\n+):\n+    required_keys = {\"rope_type\", \"rope_theta\"}\n+    received_keys = set(rope_parameters.keys())\n+    rope_type = rope_parameters[\"rope_type\"]\n     _check_received_keys(rope_type, received_keys, required_keys, ignore_keys=ignore_keys)\n \n \n-def _validate_linear_scaling_rope_parameters(config: PreTrainedConfig, ignore_keys: Optional[set] = None):\n-    rope_scaling = config.rope_scaling\n-    rope_type = rope_scaling.get(\"rope_type\", rope_scaling.get(\"type\", None))  # BC: \"rope_type\" was originally \"type\"\n-    required_keys = {\"rope_type\", \"factor\"}\n-    received_keys = set(rope_scaling.keys())\n+def _validate_linear_scaling_rope_parameters(\n+    rope_parameters: dict, config: Optional[PreTrainedConfig] = None, ignore_keys: Optional[set] = None\n+):\n+    required_keys = {\"rope_type\", \"factor\", \"rope_theta\"}\n+    received_keys = set(rope_parameters.keys())\n+    rope_type = rope_parameters[\"rope_type\"]\n     _check_received_keys(rope_type, received_keys, required_keys, ignore_keys=ignore_keys)\n \n-    factor = rope_scaling[\"factor\"]\n+    factor = rope_parameters[\"factor\"]\n     if factor is None or not isinstance(factor, float) or factor < 1.0:\n-        logger.warning(f\"`rope_scaling`'s factor field must be a float >= 1, got {factor}\")\n+        logger.warning(f\"`rope_parameters`'s factor field must be a float >= 1, got {factor}\")\n \n \n-def _validate_dynamic_scaling_rope_parameters(config: PreTrainedConfig, ignore_keys: Optional[set] = None):\n-    rope_scaling = config.rope_scaling\n-    rope_type = rope_scaling.get(\"rope_type\", rope_scaling.get(\"type\", None))  # BC: \"rope_type\" was originally \"type\"\n-    required_keys = {\"rope_type\", \"factor\"}\n+def _validate_dynamic_scaling_rope_parameters(\n+    rope_parameters: dict, config: Optional[PreTrainedConfig] = None, ignore_keys: Optional[set] = None\n+):\n     # TODO (joao): update logic for the inclusion of `original_max_position_embeddings`\n     optional_keys = {\"original_max_position_embeddings\"}\n-    received_keys = set(rope_scaling.keys())\n+    required_keys = {\"rope_type\", \"factor\"}\n+    received_keys = set(rope_parameters.keys())\n+    rope_type = rope_parameters[\"rope_type\"]\n     _check_received_keys(rope_type, received_keys, required_keys, optional_keys, ignore_keys=ignore_keys)\n \n-    factor = rope_scaling[\"factor\"]\n+    factor = rope_parameters[\"factor\"]\n     if factor is None or not isinstance(factor, float) or factor < 1.0:\n-        logger.warning(f\"`rope_scaling`'s factor field must be a float >= 1, got {factor}\")\n+        logger.warning(f\"`rope_parameters`'s factor field must be a float >= 1, got {factor}\")\n \n \n-def _validate_yarn_parameters(config: PreTrainedConfig, ignore_keys: Optional[set] = None):\n-    rope_scaling = config.rope_scaling\n-    rope_type = rope_scaling.get(\"rope_type\", rope_scaling.get(\"type\", None))  # BC: \"rope_type\" was originally \"type\"\n-    required_keys = {\"rope_type\", \"factor\"}\n+def _validate_yarn_parameters(\n+    rope_parameters: dict, config: Optional[PreTrainedConfig] = None, ignore_keys: Optional[set] = None\n+):\n+    required_keys = {\"rope_type\", \"factor\", \"rope_theta\"}\n     optional_keys = {\n         \"attention_factor\",\n         \"beta_fast\",\n         \"beta_slow\",\n         \"original_max_position_embeddings\",\n         \"mscale\",\n         \"mscale_all_dim\",\n-        \"truncate\",\n     }\n-    received_keys = set(rope_scaling.keys())\n+    received_keys = set(rope_parameters.keys())\n+    rope_type = rope_parameters[\"rope_type\"]\n     _check_received_keys(rope_type, received_keys, required_keys, optional_keys, ignore_keys=ignore_keys)\n \n-    factor = rope_scaling[\"factor\"]\n+    factor = rope_parameters[\"factor\"]\n     if factor is None or not isinstance(factor, float) or factor < 1.0:\n-        logger.warning(f\"`rope_scaling`'s factor field must be a float >= 1, got {factor}\")\n+        logger.warning(f\"`rope_parameters`'s factor field must be a float >= 1, got {factor}\")\n \n-    attention_factor = rope_scaling.get(\"attention_factor\")\n+    attention_factor = rope_parameters.get(\"attention_factor\")\n     if attention_factor is not None and (not isinstance(attention_factor, float) or attention_factor < 0):\n         logger.warning(\n-            f\"`rope_scaling`'s attention_factor field must be a float greater than 0, got {attention_factor}\"\n+            f\"`rope_parameters`'s attention_factor field must be a float greater than 0, got {attention_factor}\"\n         )\n-    beta_fast = rope_scaling.get(\"beta_fast\")\n+    beta_fast = rope_parameters.get(\"beta_fast\")\n     if beta_fast is not None and not isinstance(beta_fast, float):\n-        logger.warning(f\"`rope_scaling`'s beta_fast field must be a float, got {beta_fast}\")\n-    beta_slow = rope_scaling.get(\"beta_slow\")\n+        logger.warning(f\"`rope_parameters`'s beta_fast field must be a float, got {beta_fast}\")\n+    beta_slow = rope_parameters.get(\"beta_slow\")\n     if beta_slow is not None and not isinstance(beta_slow, float):\n-        logger.warning(f\"`rope_scaling`'s beta_slow field must be a float, got {beta_slow}\")\n+        logger.warning(f\"`rope_parameters`'s beta_slow field must be a float, got {beta_slow}\")\n \n     if (beta_fast or 32) < (beta_slow or 1):\n         logger.warning(\n-            f\"`rope_scaling`'s beta_fast field must be greater than beta_slow, got beta_fast={beta_fast} \"\n+            f\"`rope_parameters`'s beta_fast field must be greater than beta_slow, got beta_fast={beta_fast} \"\n             f\"(defaults to 32 if None) and beta_slow={beta_slow} (defaults to 1 if None)\"\n         )\n \n-    # Models should set `config.rope_scaling[\"original_max_position_embeddings\"]` to their original (pre-yarn) context\n+    # Models should set `config.rope_parameters[\"original_max_position_embeddings\"]` to their original (pre-yarn) context\n     # length, with `config.max_position_embeddings` corresponding to their post-yarn context length.\n     # However, for BC purposes, we allow the former to be unset.\n-    original_max_position_embeddings = config.rope_scaling.get(\"original_max_position_embeddings\")\n+    original_max_position_embeddings = config.rope_parameters.get(\"original_max_position_embeddings\")\n     if original_max_position_embeddings is not None:\n         # Double-check: `factor` should be the ratio between the pre-yarn and post-yarn context lengths.\n         implicit_factor = config.max_position_embeddings / original_max_position_embeddings\n         if implicit_factor != factor:\n             logger.warning_once(\n-                f\"The explicitly set RoPE scaling factor (config.rope_scaling['factor'] = {factor}) does not match \"\n+                f\"The explicitly set RoPE scaling factor (config.rope_parameters['factor'] = {factor}) does not match \"\n                 \"the ratio implicitly set by other parameters (implicit factor = \"\n                 \"post-yarn context length / pre-yarn context length = \"\n-                \"config.max_position_embeddings / config.rope_scaling['original_max_position_embeddings'] = \"\n+                \"config.max_position_embeddings / config.rope_parameters['original_max_position_embeddings'] = \"\n                 f\"{implicit_factor}). Using the explicit factor ({factor}) in YaRN. This may cause unexpected \"\n                 \"behaviour in model usage, please correct the 'max_position_embeddings' fields in the model config.\"\n             )\n-    # No `config.rope_scaling[\"original_max_position_embeddings\"]`. Is `config.max_position_embeddings` the\n+    # No `config.rope_parameters[\"original_max_position_embeddings\"]`. Is `config.max_position_embeddings` the\n     # pre-yarn or the post-yarn context length?\n     # BC: we assume it is the pre-yarn context length.\n     else:\n         logger.warning_once(\n-            \"config.rope_scaling['original_max_position_embeddings'], the pre-yarn context length, is unset. We will \"\n+            \"config.rope_parameters['original_max_position_embeddings'], the pre-yarn context length, is unset. We will \"\n             \"**assume** config.max_position_embeddings holds the pre-yarn context length. Some use cases may expect \"\n             \"config.max_position_embeddings to hold the post-yarn context length (pre-yarn context length * \"\n             \"factor) -- we recommend updating both fields for optimal downstream model usage.\"\n         )\n \n \n-def _validate_longrope_parameters(config: PreTrainedConfig, ignore_keys: Optional[set] = None):\n-    rope_scaling = config.rope_scaling\n-    rope_type = rope_scaling.get(\"rope_type\", rope_scaling.get(\"type\", None))  # BC: \"rope_type\" was originally \"type\"\n-    required_keys = {\"rope_type\", \"short_factor\", \"long_factor\"}\n+def _validate_longrope_parameters(rope_parameters: dict, config: PreTrainedConfig, ignore_keys: Optional[set] = None):\n+    required_keys = {\"rope_type\", \"short_factor\", \"long_factor\", \"rope_theta\"}\n     # TODO (joao): update logic for the inclusion of `original_max_position_embeddings`\n     optional_keys = {\"attention_factor\", \"factor\", \"original_max_position_embeddings\"}\n-    received_keys = set(rope_scaling.keys())\n+    received_keys = set(rope_parameters.keys())\n+    rope_type = rope_parameters[\"rope_type\"]\n     _check_received_keys(rope_type, received_keys, required_keys, optional_keys, ignore_keys=ignore_keys)\n \n     partial_rotary_factor = getattr(config, \"partial_rotary_factor\", 1.0)\n     head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n     dim = int(head_dim * partial_rotary_factor)\n \n-    short_factor = rope_scaling.get(\"short_factor\")\n+    short_factor = rope_parameters.get(\"short_factor\")\n     if not isinstance(short_factor, list) and all(isinstance(x, (int, float)) for x in short_factor):\n-        logger.warning(f\"`rope_scaling`'s short_factor field must be a list of numbers, got {short_factor}\")\n+        logger.warning(f\"`rope_parameters`'s short_factor field must be a list of numbers, got {short_factor}\")\n     if len(short_factor) != dim // 2:\n-        logger.warning(f\"`rope_scaling`'s short_factor field must have length {dim // 2}, got {len(short_factor)}\")\n+        logger.warning(f\"`rope_parameters`'s short_factor field must have length {dim // 2}, got {len(short_factor)}\")\n \n-    long_factor = rope_scaling.get(\"long_factor\")\n+    long_factor = rope_parameters.get(\"long_factor\")\n     if not isinstance(long_factor, list) and all(isinstance(x, (int, float)) for x in long_factor):\n-        logger.warning(f\"`rope_scaling`'s long_factor field must be a list of numbers, got {long_factor}\")\n+        logger.warning(f\"`rope_parameters`'s long_factor field must be a list of numbers, got {long_factor}\")\n     if len(long_factor) != dim // 2:\n-        logger.warning(f\"`rope_scaling`'s long_factor field must have length {dim // 2}, got {len(long_factor)}\")\n+        logger.warning(f\"`rope_parameters`'s long_factor field must have length {dim // 2}, got {len(long_factor)}\")\n \n     # Handle Phi3 divergence: prefer the use of `attention_factor` and/or `factor` over\n-    # `original_max_position_embeddings` to compute internal variables. The latter lives outside `rope_scaling` and is\n+    # `original_max_position_embeddings` to compute internal variables. The latter lives outside `rope_parameters` and is\n     # unique to longrope (= undesirable)\n     if hasattr(config, \"original_max_position_embeddings\"):\n         logger.warning_once(\n             \"This model has set a `original_max_position_embeddings` field, to be used together with \"\n-            \"`max_position_embeddings` to determine a scaling factor. Please set the `factor` field of `rope_scaling`\"\n+            \"`max_position_embeddings` to determine a scaling factor. Please set the `factor` field of `rope_parameters`\"\n             \"with this ratio instead -- we recommend the use of this field over `original_max_position_embeddings`, \"\n             \"as it is compatible with most model architectures.\"\n         )\n     else:\n-        factor = rope_scaling.get(\"factor\")\n+        factor = rope_parameters.get(\"factor\")\n         if factor is None:\n-            logger.warning(\"Missing required keys in `rope_scaling`: 'factor'\")\n+            logger.warning(\"Missing required keys in `rope_parameters`: 'factor'\")\n         elif not isinstance(factor, float) or factor < 1.0:\n-            logger.warning(f\"`rope_scaling`'s factor field must be a float >= 1, got {factor}\")\n+            logger.warning(f\"`rope_parameters`'s factor field must be a float >= 1, got {factor}\")\n \n-        attention_factor = rope_scaling.get(\"attention_factor\")\n+        attention_factor = rope_parameters.get(\"attention_factor\")\n         if attention_factor is not None:\n             if not isinstance(attention_factor, float) or attention_factor < 0.0:\n                 logger.warning(\n-                    f\"`rope_scaling`'s attention_factor field must be a float greater than 0, got {attention_factor}\"\n+                    f\"`rope_parameters`'s attention_factor field must be a float greater than 0, got {attention_factor}\"\n                 )\n \n \n-def _validate_llama3_parameters(config: PreTrainedConfig, ignore_keys: Optional[set] = None):\n-    rope_scaling = config.rope_scaling\n-    rope_type = rope_scaling.get(\"rope_type\", rope_scaling.get(\"type\", None))  # BC: \"rope_type\" was originally \"type\"\n-    required_keys = {\"rope_type\", \"factor\", \"original_max_position_embeddings\", \"low_freq_factor\", \"high_freq_factor\"}\n-    received_keys = set(rope_scaling.keys())\n+def _validate_llama3_parameters(rope_parameters: dict, config: PreTrainedConfig, ignore_keys: Optional[set] = None):\n+    required_keys = {\n+        \"rope_type\",\n+        \"factor\",\n+        \"original_max_position_embeddings\",\n+        \"low_freq_factor\",\n+        \"high_freq_factor\",\n+        \"rope_theta\",\n+    }\n+    rope_type = rope_parameters[\"rope_type\"]\n+    received_keys = set(rope_parameters.keys())\n     _check_received_keys(rope_type, received_keys, required_keys, ignore_keys=ignore_keys)\n \n-    factor = rope_scaling[\"factor\"]\n+    factor = rope_parameters[\"factor\"]\n     if factor is None or not isinstance(factor, float) or factor < 1.0:\n-        logger.warning(f\"`rope_scaling`'s factor field must be a float >= 1, got {factor}\")\n+        logger.warning(f\"`rope_parameters`'s factor field must be a float >= 1, got {factor}\")\n \n-    low_freq_factor = rope_scaling[\"low_freq_factor\"]\n-    high_freq_factor = rope_scaling[\"high_freq_factor\"]\n+    low_freq_factor = rope_parameters[\"low_freq_factor\"]\n+    high_freq_factor = rope_parameters[\"high_freq_factor\"]\n     if low_freq_factor is None or not isinstance(low_freq_factor, float):\n-        logger.warning(f\"`rope_scaling`'s low_freq_factor field must be a float, got {low_freq_factor}\")\n+        logger.warning(f\"`rope_parameters`'s low_freq_factor field must be a float, got {low_freq_factor}\")\n     if high_freq_factor is None or not isinstance(high_freq_factor, float):\n-        logger.warning(f\"`rope_scaling`'s high_freq_factor field must be a float, got {high_freq_factor}\")\n+        logger.warning(f\"`rope_parameters`'s high_freq_factor field must be a float, got {high_freq_factor}\")\n     if high_freq_factor <= low_freq_factor:\n         logger.warning(\n-            \"`rope_scaling`'s high_freq_factor field must be greater than low_freq_factor, got high_freq_factor=\"\n+            \"`rope_parameters`'s high_freq_factor field must be greater than low_freq_factor, got high_freq_factor=\"\n             f\"{high_freq_factor} and low_freq_factor={low_freq_factor}\"\n         )\n \n-    original_max_position_embeddings = rope_scaling[\"original_max_position_embeddings\"]\n+    original_max_position_embeddings = rope_parameters[\"original_max_position_embeddings\"]\n     if original_max_position_embeddings is None or not isinstance(original_max_position_embeddings, int):\n         logger.warning(\n-            \"`rope_scaling`'s original_max_position_embeddings field must be an integer, got \"\n+            \"`rope_parameters`'s original_max_position_embeddings field must be an integer, got \"\n             f\"{original_max_position_embeddings}\"\n         )\n     if original_max_position_embeddings >= config.max_position_embeddings:\n         logger.warning(\n-            \"`rope_scaling`'s original_max_position_embeddings field must be less than max_position_embeddings, got \"\n+            \"`rope_parameters`'s original_max_position_embeddings field must be less than max_position_embeddings, got \"\n             f\"{original_max_position_embeddings} and max_position_embeddings={config.max_position_embeddings}\"\n         )\n \n@@ -758,16 +858,80 @@ def rope_config_validation(config: PreTrainedConfig, ignore_keys: Optional[set]\n     \"\"\"\n     Validate the RoPE config arguments, given a `PreTrainedConfig` object\n     \"\"\"\n-    rope_scaling = getattr(config, \"rope_scaling\", None)  # not a default parameter in `PreTrainedConfig`\n-    if rope_scaling is None:\n+    rope_parameters_dict = getattr(config, \"rope_parameters\", None)  # not a default parameter in `PreTrainedConfig`\n+    if rope_parameters_dict is None:\n         return\n \n-    # BC: \"rope_type\" was originally \"type\"\n-    rope_type = rope_scaling.get(\"rope_type\", rope_scaling.get(\"type\", \"default\"))\n-    validation_fn = ROPE_VALIDATION_FUNCTIONS.get(rope_type)\n-    if validation_fn is not None:\n-        validation_fn(config, ignore_keys=ignore_keys)\n+    if getattr(config, \"layer_types\", None) is not None and all(\n+        key in config.layer_types for key in rope_parameters_dict.keys()\n+    ):\n+        pass\n     else:\n-        logger.warning(\n-            f\"Missing validation function mapping in `ROPE_VALIDATION_FUNCTIONS` for 'rope_type'='{rope_type}'\"\n-        )\n+        rope_parameters_dict = {\"full_attention\": rope_parameters_dict}\n+\n+    for rope_parameters in rope_parameters_dict.values():\n+        rope_type = rope_parameters.get(\"rope_type\", rope_parameters.get(\"type\", \"default\"))\n+        validation_fn = ROPE_VALIDATION_FUNCTIONS.get(rope_type)\n+\n+        rope_parameters[\"rope_type\"] = rope_type\n+        # BC: \"rope_theta\" was originally saved in config\n+        rope_parameters[\"rope_theta\"] = rope_parameters.get(\"rope_theta\", getattr(config, \"rope_theta\", None))\n+\n+        if validation_fn is not None:\n+            validation_fn(rope_parameters, config=config, ignore_keys=ignore_keys)\n+        else:\n+            logger.warning(\n+                f\"Missing validation function mapping in `ROPE_VALIDATION_FUNCTIONS` for 'rope_type'='{rope_type}'\"\n+            )\n+\n+\n+class RopeParameters(TypedDict):\n+    \"\"\"\n+    Args:\n+        rope_theta (`float`):\n+            The base period of the RoPE embeddings.\n+        rope_type (`str`, *optional*, defaults to \"default\"):\n+            The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n+            'llama3'], with 'default' being the original RoPE implementation.\n+        factor (`float`, *optional*):\n+            Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n+            most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n+            original maximum pre-trained length.\n+        original_max_position_embeddings (`int`, *optional*):\n+            Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n+            pretraining.\n+        attention_factor (`float`, *optional*):\n+            Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n+            computation. If unspecified, it defaults to value recommended by the implementation, using the\n+            `factor` field to infer the suggested value.\n+        beta_fast (`float`, *optional*):\n+            Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n+            ramp function. If unspecified, it defaults to 32.\n+        beta_slow (`float`, *optional*):\n+            Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n+            ramp function. If unspecified, it defaults to 1.\n+        short_factor (`list[float]`, *optional*):\n+            Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n+            `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+            size divided by the number of attention heads divided by 2\n+        long_factor (`list[float]`, *optional*):\n+            Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n+            `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+            size divided by the number of attention heads divided by 2\n+        low_freq_factor (`float`, *optional*):\n+            Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n+        high_freq_factor (`float`, *optional*):\n+            Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n+    \"\"\"\n+\n+    rope_theta: float\n+    rope_type: Optional[str]\n+    factor: Optional[float]\n+    original_max_position_embeddings: Optional[int]\n+    attention_factor: Optional[float]\n+    beta_fast: Optional[float]\n+    beta_slow: Optional[float]\n+    short_factor: Optional[list[float]]\n+    long_factor: Optional[list[float]]\n+    low_freq_factor: Optional[float]\n+    high_freq_factor: Optional[float]"
        },
        {
            "sha": "92eef2778134a50493160ae07bbb27a8c4f42b61",
            "filename": "src/transformers/models/apertus/configuration_apertus.py",
            "status": "modified",
            "additions": 31,
            "deletions": 64,
            "changes": 95,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fapertus%2Fconfiguration_apertus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fapertus%2Fconfiguration_apertus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fapertus%2Fconfiguration_apertus.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -19,9 +19,10 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n+from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import rope_config_validation\n+from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n \n \n class ApertusConfig(PreTrainedConfig):\n@@ -74,45 +75,10 @@ class ApertusConfig(PreTrainedConfig):\n             End of stream token id.\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie weight embeddings\n-        rope_theta (`float`, *optional*, defaults to 12000000.0):\n-            The base period of the RoPE embeddings.\n-        rope_scaling (`Dict`, *optional*):\n-            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n-            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n-            accordingly.\n-            Expected contents:\n-                `rope_type` (`str`):\n-                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n-                    'llama3'], with 'default' being the original RoPE implementation.\n-                `factor` (`float`, *optional*):\n-                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n-                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n-                    original maximum pre-trained length.\n-                `original_max_position_embeddings` (`int`, *optional*):\n-                    Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n-                    pretraining.\n-                `attention_factor` (`float`, *optional*):\n-                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n-                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n-                    `factor` field to infer the suggested value.\n-                `beta_fast` (`float`, *optional*):\n-                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n-                    ramp function. If unspecified, it defaults to 32.\n-                `beta_slow` (`float`, *optional*):\n-                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n-                    ramp function. If unspecified, it defaults to 1.\n-                `short_factor` (`list[float]`, *optional*):\n-                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n-                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n-                    size divided by the number of attention heads divided by 2\n-                `long_factor` (`list[float]`, *optional*):\n-                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n-                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n-                    size divided by the number of attention heads divided by 2\n-                `low_freq_factor` (`float`, *optional*):\n-                    Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n-                `high_freq_factor` (`float`, *optional*):\n-                    Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         attention_bias (`bool`, *optional*, defaults to `False`):\n             Whether to use a bias in the query, key, value and output projection layers during self-attention.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n@@ -150,31 +116,31 @@ class ApertusConfig(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size=131072,\n-        hidden_size=4096,\n-        intermediate_size=14336,\n-        num_hidden_layers=32,\n-        num_attention_heads=32,\n-        num_key_value_heads=None,\n-        hidden_act=\"xielu\",\n-        max_position_embeddings=65536,\n-        initializer_range=0.02,\n-        rms_norm_eps=1e-5,\n-        use_cache=True,\n-        pad_token_id=3,\n-        bos_token_id=1,\n-        eos_token_id=2,\n-        tie_word_embeddings=False,\n-        rope_theta=12000000.0,\n-        rope_scaling={\n+        vocab_size: Optional[int] = 131072,\n+        hidden_size: Optional[int] = 4096,\n+        intermediate_size: Optional[int] = 14336,\n+        num_hidden_layers: Optional[int] = 32,\n+        num_attention_heads: Optional[int] = 32,\n+        num_key_value_heads: Optional[int] = None,\n+        hidden_act: Optional[str] = \"xielu\",\n+        max_position_embeddings: Optional[int] = 65536,\n+        initializer_range: Optional[float] = 0.02,\n+        rms_norm_eps: Optional[float] = 1e-5,\n+        use_cache: Optional[bool] = True,\n+        pad_token_id: Optional[int] = 3,\n+        bos_token_id: Optional[int] = 1,\n+        eos_token_id: Optional[int] = 2,\n+        tie_word_embeddings: Optional[bool] = False,\n+        rope_parameters: Optional[RopeParameters] = {\n             \"rope_type\": \"llama3\",\n+            \"rope_theta\": 12000000.0,\n             \"factor\": 8.0,\n             \"original_max_position_embeddings\": 8192,\n             \"low_freq_factor\": 1.0,\n             \"high_freq_factor\": 4.0,\n         },\n-        attention_bias=False,\n-        attention_dropout=0.0,\n+        attention_bias: Optional[bool] = False,\n+        attention_dropout: Optional[float] = 0.0,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size\n@@ -193,14 +159,15 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.rms_norm_eps = rms_norm_eps\n         self.use_cache = use_cache\n-        self.rope_theta = rope_theta\n-        self.rope_scaling = rope_scaling\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n+\n         # Validate the correctness of rotary position embeddings parameters\n-        # BC: if there is a 'type' field, copy it it to 'rope_type'.\n-        if self.rope_scaling is not None and \"type\" in self.rope_scaling:\n-            self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n+        rope_theta = kwargs.get(\"rope_theta\", 12000000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n         rope_config_validation(self)\n \n         super().__init__("
        },
        {
            "sha": "e92e87a3c280c5eb0bf4a65cb23ea1bc49512a4e",
            "filename": "src/transformers/models/apertus/modeling_apertus.py",
            "status": "modified",
            "additions": 39,
            "deletions": 10,
            "changes": 49,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fapertus%2Fmodeling_apertus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fapertus%2Fmodeling_apertus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fapertus%2Fmodeling_apertus.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -80,20 +80,49 @@ class ApertusRotaryEmbedding(nn.Module):\n \n     def __init__(self, config: ApertusConfig, device=None):\n         super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n         self.max_seq_len_cached = config.max_position_embeddings\n         self.original_max_seq_len = config.max_position_embeddings\n \n         self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.rope_type = self.config.rope_parameters[\"rope_type\"]\n+        rope_init_fn: Callable = self.compute_default_rope_parameters\n+        if self.rope_type != \"default\":\n+            rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+        inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n+\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n+        self.original_inv_freq = inv_freq\n+\n+    @staticmethod\n+    def compute_default_rope_parameters(\n+        config: Optional[ApertusConfig] = None,\n+        device: Optional[\"torch.device\"] = None,\n+        seq_len: Optional[int] = None,\n+    ) -> tuple[\"torch.Tensor\", float]:\n+        \"\"\"\n+        Computes the inverse frequencies according to the original RoPE implementation\n+        Args:\n+            config ([`~transformers.PreTrainedConfig`]):\n+                The model configuration.\n+            device (`torch.device`):\n+                The device to use for initialization of the inverse frequencies.\n+            seq_len (`int`, *optional*):\n+                The current sequence length. Unused for this type of RoPE.\n+        Returns:\n+            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n+            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n+        \"\"\"\n+        base = config.rope_parameters[\"rope_theta\"]\n+        dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+\n+        attention_factor = 1.0  # Unused in this type of RoPE\n+\n+        # Compute the inverse frequencies\n+        inv_freq = 1.0 / (\n+            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n+        )\n+        return inv_freq, attention_factor\n \n     @torch.no_grad()\n     @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n@@ -378,16 +407,16 @@ def forward(\n         )\n \n         hidden_states = inputs_embeds\n-        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids=position_ids)\n \n         for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n             hidden_states = decoder_layer(\n                 hidden_states,\n                 attention_mask=causal_mask,\n+                position_embeddings=position_embeddings,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n                 cache_position=cache_position,\n-                position_embeddings=position_embeddings,\n                 **kwargs,\n             )\n "
        },
        {
            "sha": "5f5ba8ca4a4ee0425ce978a550e9fc7393082d39",
            "filename": "src/transformers/models/apertus/modular_apertus.py",
            "status": "modified",
            "additions": 30,
            "deletions": 60,
            "changes": 90,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fapertus%2Fmodular_apertus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fapertus%2Fmodular_apertus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fapertus%2Fmodular_apertus.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -20,6 +20,7 @@\n from torch import nn\n \n from ...cache_utils import Cache\n+from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, logging\n@@ -92,45 +93,10 @@ class ApertusConfig(LlamaConfig):\n             End of stream token id.\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie weight embeddings\n-        rope_theta (`float`, *optional*, defaults to 12000000.0):\n-            The base period of the RoPE embeddings.\n-        rope_scaling (`Dict`, *optional*):\n-            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n-            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n-            accordingly.\n-            Expected contents:\n-                `rope_type` (`str`):\n-                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n-                    'llama3'], with 'default' being the original RoPE implementation.\n-                `factor` (`float`, *optional*):\n-                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n-                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n-                    original maximum pre-trained length.\n-                `original_max_position_embeddings` (`int`, *optional*):\n-                    Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n-                    pretraining.\n-                `attention_factor` (`float`, *optional*):\n-                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n-                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n-                    `factor` field to infer the suggested value.\n-                `beta_fast` (`float`, *optional*):\n-                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n-                    ramp function. If unspecified, it defaults to 32.\n-                `beta_slow` (`float`, *optional*):\n-                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n-                    ramp function. If unspecified, it defaults to 1.\n-                `short_factor` (`list[float]`, *optional*):\n-                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n-                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n-                    size divided by the number of attention heads divided by 2\n-                `long_factor` (`list[float]`, *optional*):\n-                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n-                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n-                    size divided by the number of attention heads divided by 2\n-                `low_freq_factor` (`float`, *optional*):\n-                    Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n-                `high_freq_factor` (`float`, *optional*):\n-                    Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         attention_bias (`bool`, *optional*, defaults to `False`):\n             Whether to use a bias in the query, key, value and output projection layers during self-attention.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n@@ -162,31 +128,31 @@ class ApertusConfig(LlamaConfig):\n \n     def __init__(\n         self,\n-        vocab_size=131072,\n-        hidden_size=4096,\n-        intermediate_size=14336,\n-        num_hidden_layers=32,\n-        num_attention_heads=32,\n-        num_key_value_heads=None,\n-        hidden_act=\"xielu\",\n-        max_position_embeddings=65536,\n-        initializer_range=0.02,\n-        rms_norm_eps=1e-5,\n-        use_cache=True,\n-        pad_token_id=3,\n-        bos_token_id=1,\n-        eos_token_id=2,\n-        tie_word_embeddings=False,\n-        rope_theta=12000000.0,\n-        rope_scaling={\n+        vocab_size: Optional[int] = 131072,\n+        hidden_size: Optional[int] = 4096,\n+        intermediate_size: Optional[int] = 14336,\n+        num_hidden_layers: Optional[int] = 32,\n+        num_attention_heads: Optional[int] = 32,\n+        num_key_value_heads: Optional[int] = None,\n+        hidden_act: Optional[str] = \"xielu\",\n+        max_position_embeddings: Optional[int] = 65536,\n+        initializer_range: Optional[float] = 0.02,\n+        rms_norm_eps: Optional[float] = 1e-5,\n+        use_cache: Optional[bool] = True,\n+        pad_token_id: Optional[int] = 3,\n+        bos_token_id: Optional[int] = 1,\n+        eos_token_id: Optional[int] = 2,\n+        tie_word_embeddings: Optional[bool] = False,\n+        rope_parameters: Optional[RopeParameters] = {\n             \"rope_type\": \"llama3\",\n+            \"rope_theta\": 12000000.0,\n             \"factor\": 8.0,\n             \"original_max_position_embeddings\": 8192,\n             \"low_freq_factor\": 1.0,\n             \"high_freq_factor\": 4.0,\n         },\n-        attention_bias=False,\n-        attention_dropout=0.0,\n+        attention_bias: Optional[bool] = False,\n+        attention_dropout: Optional[float] = 0.0,\n         **kwargs,\n     ):\n         super().__init__(\n@@ -205,8 +171,7 @@ def __init__(\n             bos_token_id=bos_token_id,\n             eos_token_id=eos_token_id,\n             tie_word_embeddings=tie_word_embeddings,\n-            rope_theta=rope_theta,\n-            rope_scaling=rope_scaling,\n+            rope_parameters=rope_parameters,\n             attention_bias=attention_bias,\n             attention_dropout=attention_dropout,\n             **kwargs,\n@@ -215,6 +180,11 @@ def __init__(\n         del self.mlp_bias\n         del self.head_dim\n \n+        # Validate the correctness of rotary position embeddings parameters\n+        rope_theta = kwargs.get(\"rope_theta\", 12000000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n+        rope_config_validation(self)\n+\n \n class ApertusMLP(NemotronMLP):\n     def __init__(self, config):"
        },
        {
            "sha": "b4e23ffb3b8f79027c78f825da2cfa0c454246a2",
            "filename": "src/transformers/models/arcee/configuration_arcee.py",
            "status": "modified",
            "additions": 33,
            "deletions": 51,
            "changes": 84,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Farcee%2Fconfiguration_arcee.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Farcee%2Fconfiguration_arcee.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Farcee%2Fconfiguration_arcee.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -19,8 +19,10 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n+from typing import Optional\n+\n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import rope_config_validation\n+from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n \n \n class ArceeConfig(PreTrainedConfig):\n@@ -75,30 +77,10 @@ class ArceeConfig(PreTrainedConfig):\n             End of stream token id.\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie weight embeddings\n-        rope_theta (`float`, *optional*, defaults to 10000.0):\n-            The base period of the RoPE embeddings.\n-        rope_scaling (`Dict`, *optional*):\n-            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n-            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n-            accordingly.\n-            Expected contents:\n-                `rope_type` (`str`):\n-                    The sub-variant of RoPE to use. Can be one of ['default', 'yarn'], with 'default' being the original RoPE implementation.\n-                `factor` (`float`, *optional*):\n-                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n-                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n-                    original maximum pre-trained length.\n-                `original_max_position_embeddings` (`int`, *optional*):\n-                    Used with 'yarn'. The original max position embeddings used during pretraining.\n-                `attention_factor` (`float`, *optional*):\n-                    Used with 'yarn'. The scaling factor to be applied on the attention computation. If unspecified,\n-                    it defaults to value recommended by the implementation, using the `factor` field to infer the suggested value.\n-                `beta_fast` (`float`, *optional*):\n-                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n-                    ramp function. If unspecified, it defaults to 32.\n-                `beta_slow` (`float`, *optional*):\n-                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n-                    ramp function. If unspecified, it defaults to 1.\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         attention_bias (`bool`, *optional*, defaults to `False`):\n             Whether to use a bias in the query, key, value and output projection layers during self-attention.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n@@ -139,27 +121,26 @@ class ArceeConfig(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size=32000,\n-        hidden_size=2560,\n-        intermediate_size=18432,\n-        num_hidden_layers=32,\n-        num_attention_heads=32,\n-        num_key_value_heads=None,\n-        hidden_act=\"relu2\",\n-        max_position_embeddings=4096,\n-        initializer_range=0.02,\n-        rms_norm_eps=1e-5,\n-        use_cache=True,\n-        pad_token_id=None,\n-        bos_token_id=128000,\n-        eos_token_id=128001,\n-        tie_word_embeddings=False,\n-        rope_theta=10000.0,\n-        rope_scaling=None,\n-        attention_bias=False,\n-        attention_dropout=0.0,\n-        mlp_bias=False,\n-        head_dim=None,\n+        vocab_size: Optional[int] = 32000,\n+        hidden_size: Optional[int] = 2560,\n+        intermediate_size: Optional[int] = 18432,\n+        num_hidden_layers: Optional[int] = 32,\n+        num_attention_heads: Optional[int] = 32,\n+        num_key_value_heads: Optional[int] = None,\n+        hidden_act: Optional[str] = \"relu2\",\n+        max_position_embeddings: Optional[int] = 4096,\n+        initializer_range: Optional[float] = 0.02,\n+        rms_norm_eps: Optional[int] = 1e-5,\n+        use_cache: Optional[bool] = True,\n+        pad_token_id: Optional[int] = None,\n+        bos_token_id: Optional[int] = 128000,\n+        eos_token_id: Optional[int] = 128001,\n+        tie_word_embeddings: Optional[bool] = False,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        attention_bias: Optional[bool] = False,\n+        attention_dropout: Optional[float] = 0.0,\n+        mlp_bias: Optional[bool] = False,\n+        head_dim: Optional[int] = None,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size\n@@ -178,16 +159,17 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.rms_norm_eps = rms_norm_eps\n         self.use_cache = use_cache\n-        self.rope_theta = rope_theta\n-        self.rope_scaling = rope_scaling\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n         self.mlp_bias = mlp_bias\n         self.head_dim = head_dim if head_dim is not None else self.hidden_size // self.num_attention_heads\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n+\n         # Validate the correctness of rotary position embeddings parameters\n-        # BC: if there is a 'type' field, copy it it to 'rope_type'.\n-        if self.rope_scaling is not None and \"type\" in self.rope_scaling:\n-            self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n+        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n         rope_config_validation(self)\n \n         super().__init__("
        },
        {
            "sha": "619e72b7a11b957b7552e3f07b2dbdaa405e91ed",
            "filename": "src/transformers/models/arcee/modeling_arcee.py",
            "status": "modified",
            "additions": 42,
            "deletions": 13,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -87,20 +87,49 @@ class ArceeRotaryEmbedding(nn.Module):\n \n     def __init__(self, config: ArceeConfig, device=None):\n         super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n         self.max_seq_len_cached = config.max_position_embeddings\n         self.original_max_seq_len = config.max_position_embeddings\n \n         self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.rope_type = self.config.rope_parameters[\"rope_type\"]\n+        rope_init_fn: Callable = self.compute_default_rope_parameters\n+        if self.rope_type != \"default\":\n+            rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+        inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n+\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n+        self.original_inv_freq = inv_freq\n+\n+    @staticmethod\n+    def compute_default_rope_parameters(\n+        config: Optional[ArceeConfig] = None,\n+        device: Optional[\"torch.device\"] = None,\n+        seq_len: Optional[int] = None,\n+    ) -> tuple[\"torch.Tensor\", float]:\n+        \"\"\"\n+        Computes the inverse frequencies according to the original RoPE implementation\n+        Args:\n+            config ([`~transformers.PreTrainedConfig`]):\n+                The model configuration.\n+            device (`torch.device`):\n+                The device to use for initialization of the inverse frequencies.\n+            seq_len (`int`, *optional*):\n+                The current sequence length. Unused for this type of RoPE.\n+        Returns:\n+            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n+            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n+        \"\"\"\n+        base = config.rope_parameters[\"rope_theta\"]\n+        dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+\n+        attention_factor = 1.0  # Unused in this type of RoPE\n+\n+        # Compute the inverse frequencies\n+        inv_freq = 1.0 / (\n+            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n+        )\n+        return inv_freq, attention_factor\n \n     @torch.no_grad()\n     @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n@@ -219,8 +248,8 @@ def __init__(self, config: ArceeConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n-        attention_mask: Optional[torch.Tensor],\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -279,7 +308,7 @@ def forward(\n         past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> torch.Tensor:\n         residual = hidden_states\n@@ -383,16 +412,16 @@ def forward(\n         )\n \n         hidden_states = inputs_embeds\n-        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids=position_ids)\n \n         for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n             hidden_states = decoder_layer(\n                 hidden_states,\n                 attention_mask=causal_mask,\n+                position_embeddings=position_embeddings,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n                 cache_position=cache_position,\n-                position_embeddings=position_embeddings,\n                 **kwargs,\n             )\n "
        },
        {
            "sha": "cb75888957d8413ebf57bab84d0894ef201b2d0a",
            "filename": "src/transformers/models/arcee/modular_arcee.py",
            "status": "modified",
            "additions": 28,
            "deletions": 47,
            "changes": 75,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Farcee%2Fmodular_arcee.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Farcee%2Fmodular_arcee.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Farcee%2Fmodular_arcee.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -14,8 +14,11 @@\n # limitations under the License.\n \"\"\"PyTorch Arcee model.\"\"\"\n \n+from typing import Optional\n+\n from transformers.utils import auto_docstring, logging\n \n+from ...modeling_rope_utils import RopeParameters\n from ..llama.configuration_llama import LlamaConfig\n from ..llama.modeling_llama import (\n     LlamaForCausalLM,\n@@ -81,30 +84,10 @@ class ArceeConfig(LlamaConfig):\n             End of stream token id.\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie weight embeddings\n-        rope_theta (`float`, *optional*, defaults to 10000.0):\n-            The base period of the RoPE embeddings.\n-        rope_scaling (`Dict`, *optional*):\n-            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n-            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n-            accordingly.\n-            Expected contents:\n-                `rope_type` (`str`):\n-                    The sub-variant of RoPE to use. Can be one of ['default', 'yarn'], with 'default' being the original RoPE implementation.\n-                `factor` (`float`, *optional*):\n-                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n-                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n-                    original maximum pre-trained length.\n-                `original_max_position_embeddings` (`int`, *optional*):\n-                    Used with 'yarn'. The original max position embeddings used during pretraining.\n-                `attention_factor` (`float`, *optional*):\n-                    Used with 'yarn'. The scaling factor to be applied on the attention computation. If unspecified,\n-                    it defaults to value recommended by the implementation, using the `factor` field to infer the suggested value.\n-                `beta_fast` (`float`, *optional*):\n-                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n-                    ramp function. If unspecified, it defaults to 32.\n-                `beta_slow` (`float`, *optional*):\n-                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n-                    ramp function. If unspecified, it defaults to 1.\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         attention_bias (`bool`, *optional*, defaults to `False`):\n             Whether to use a bias in the query, key, value and output projection layers during self-attention.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n@@ -139,27 +122,26 @@ class ArceeConfig(LlamaConfig):\n \n     def __init__(\n         self,\n-        vocab_size=32000,\n-        hidden_size=2560,\n-        intermediate_size=18432,\n-        num_hidden_layers=32,\n-        num_attention_heads=32,\n-        num_key_value_heads=None,\n-        hidden_act=\"relu2\",\n-        max_position_embeddings=4096,\n-        initializer_range=0.02,\n-        rms_norm_eps=1e-5,\n-        use_cache=True,\n-        pad_token_id=None,\n-        bos_token_id=128000,\n-        eos_token_id=128001,\n-        tie_word_embeddings=False,\n-        rope_theta=10000.0,\n-        rope_scaling=None,\n-        attention_bias=False,\n-        attention_dropout=0.0,\n-        mlp_bias=False,\n-        head_dim=None,\n+        vocab_size: Optional[int] = 32000,\n+        hidden_size: Optional[int] = 2560,\n+        intermediate_size: Optional[int] = 18432,\n+        num_hidden_layers: Optional[int] = 32,\n+        num_attention_heads: Optional[int] = 32,\n+        num_key_value_heads: Optional[int] = None,\n+        hidden_act: Optional[str] = \"relu2\",\n+        max_position_embeddings: Optional[int] = 4096,\n+        initializer_range: Optional[float] = 0.02,\n+        rms_norm_eps: Optional[int] = 1e-5,\n+        use_cache: Optional[bool] = True,\n+        pad_token_id: Optional[int] = None,\n+        bos_token_id: Optional[int] = 128000,\n+        eos_token_id: Optional[int] = 128001,\n+        tie_word_embeddings: Optional[bool] = False,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        attention_bias: Optional[bool] = False,\n+        attention_dropout: Optional[float] = 0.0,\n+        mlp_bias: Optional[bool] = False,\n+        head_dim: Optional[int] = None,\n         **kwargs,\n     ):\n         super().__init__(\n@@ -178,8 +160,7 @@ def __init__(\n             bos_token_id=bos_token_id,\n             eos_token_id=eos_token_id,\n             tie_word_embeddings=tie_word_embeddings,\n-            rope_theta=rope_theta,\n-            rope_scaling=rope_scaling,\n+            rope_parameters=rope_parameters,\n             attention_bias=attention_bias,\n             attention_dropout=attention_dropout,\n             mlp_bias=mlp_bias,"
        },
        {
            "sha": "78669c78bcbb73a515be95ab38a7dee8a8594237",
            "filename": "src/transformers/models/aria/configuration_aria.py",
            "status": "modified",
            "additions": 30,
            "deletions": 65,
            "changes": 95,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Faria%2Fconfiguration_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Faria%2Fconfiguration_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fconfiguration_aria.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -21,7 +21,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import rope_config_validation\n+from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n from ..auto import CONFIG_MAPPING, AutoConfig\n \n \n@@ -77,45 +77,10 @@ class AriaTextConfig(PreTrainedConfig):\n             results. Please refer to [this issue](https://github.com/pytorch/pytorch/issues/76232).\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie weight embeddings\n-        rope_theta (`float`, *optional*, defaults to 10000.0):\n-            The base period of the RoPE embeddings.\n-        rope_scaling (`Dict`, *optional*):\n-            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n-            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n-            accordingly.\n-            Expected contents:\n-                `rope_type` (`str`):\n-                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n-                    'llama3'], with 'default' being the original RoPE implementation.\n-                `factor` (`float`, *optional*):\n-                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n-                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n-                    original maximum pre-trained length.\n-                `original_max_position_embeddings` (`int`, *optional*):\n-                    Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n-                    pretraining.\n-                `attention_factor` (`float`, *optional*):\n-                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n-                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n-                    `factor` field to infer the suggested value.\n-                `beta_fast` (`float`, *optional*):\n-                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n-                    ramp function. If unspecified, it defaults to 32.\n-                `beta_slow` (`float`, *optional*):\n-                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n-                    ramp function. If unspecified, it defaults to 1.\n-                `short_factor` (`list[float]`, *optional*):\n-                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n-                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n-                    size divided by the number of attention heads divided by 2\n-                `long_factor` (`list[float]`, *optional*):\n-                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n-                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n-                    size divided by the number of attention heads divided by 2\n-                `low_freq_factor` (`float`, *optional*):\n-                    Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n-                `high_freq_factor` (`float`, *optional*):\n-                    Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         attention_bias (`bool`, *optional*, defaults to `False`):\n             Whether to use a bias in the query, key, value and output projection layers during self-attention.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n@@ -153,28 +118,27 @@ class AriaTextConfig(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size=32000,\n-        hidden_size=4096,\n+        vocab_size: Optional[int] = 32000,\n+        hidden_size: Optional[int] = 4096,\n         intermediate_size: int = 4096,\n-        num_hidden_layers=32,\n-        num_attention_heads=32,\n-        num_key_value_heads=None,\n-        hidden_act=\"silu\",\n-        max_position_embeddings=2048,\n-        initializer_range=0.02,\n-        rms_norm_eps=1e-6,\n-        use_cache=True,\n+        num_hidden_layers: Optional[int] = 32,\n+        num_attention_heads: Optional[int] = 32,\n+        num_key_value_heads: Optional[int] = None,\n+        hidden_act: Optional[str] = \"silu\",\n+        max_position_embeddings: Optional[int] = 2048,\n+        initializer_range: Optional[float] = 0.02,\n+        rms_norm_eps: Optional[int] = 1e-6,\n+        use_cache: Optional[bool] = True,\n         pad_token_id=2,\n-        bos_token_id=1,\n-        eos_token_id=2,\n-        pretraining_tp=1,\n-        tie_word_embeddings=False,\n-        rope_theta=10000.0,\n-        rope_scaling=None,\n-        attention_bias=False,\n-        attention_dropout=0.0,\n-        mlp_bias=False,\n-        head_dim=None,\n+        bos_token_id: Optional[int] = 1,\n+        eos_token_id: Optional[int] = 2,\n+        pretraining_tp: Optional[int] = 1,\n+        tie_word_embeddings: Optional[bool] = False,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        attention_bias: Optional[bool] = False,\n+        attention_dropout: Optional[float] = 0.0,\n+        mlp_bias: Optional[bool] = False,\n+        head_dim: Optional[int] = None,\n         moe_num_experts: int = 8,\n         moe_topk: int = 2,\n         moe_num_shared_experts: int = 2,\n@@ -201,16 +165,17 @@ def __init__(\n         self.rms_norm_eps = rms_norm_eps\n         self.pretraining_tp = pretraining_tp\n         self.use_cache = use_cache\n-        self.rope_theta = rope_theta\n-        self.rope_scaling = rope_scaling\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n         self.mlp_bias = mlp_bias\n         self.head_dim = head_dim if head_dim is not None else self.hidden_size // self.num_attention_heads\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n+\n         # Validate the correctness of rotary position embeddings parameters\n-        # BC: if there is a 'type' field, copy it it to 'rope_type'.\n-        if self.rope_scaling is not None and \"type\" in self.rope_scaling:\n-            self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n+        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n         rope_config_validation(self)\n \n         super().__init__("
        },
        {
            "sha": "e702077bf93098dcaf04a6b71e9df097727526cc",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 42,
            "deletions": 13,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -471,8 +471,8 @@ def __init__(self, config: AriaTextConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n-        attention_mask: Optional[torch.Tensor],\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -542,7 +542,7 @@ def forward(\n         past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> torch.Tensor:\n         residual = hidden_states\n@@ -619,20 +619,49 @@ class AriaTextRotaryEmbedding(nn.Module):\n \n     def __init__(self, config: AriaTextConfig, device=None):\n         super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n         self.max_seq_len_cached = config.max_position_embeddings\n         self.original_max_seq_len = config.max_position_embeddings\n \n         self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.rope_type = self.config.rope_parameters[\"rope_type\"]\n+        rope_init_fn: Callable = self.compute_default_rope_parameters\n+        if self.rope_type != \"default\":\n+            rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+        inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n+\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n+        self.original_inv_freq = inv_freq\n+\n+    @staticmethod\n+    def compute_default_rope_parameters(\n+        config: Optional[AriaTextConfig] = None,\n+        device: Optional[\"torch.device\"] = None,\n+        seq_len: Optional[int] = None,\n+    ) -> tuple[\"torch.Tensor\", float]:\n+        \"\"\"\n+        Computes the inverse frequencies according to the original RoPE implementation\n+        Args:\n+            config ([`~transformers.PreTrainedConfig`]):\n+                The model configuration.\n+            device (`torch.device`):\n+                The device to use for initialization of the inverse frequencies.\n+            seq_len (`int`, *optional*):\n+                The current sequence length. Unused for this type of RoPE.\n+        Returns:\n+            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n+            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n+        \"\"\"\n+        base = config.rope_parameters[\"rope_theta\"]\n+        dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+\n+        attention_factor = 1.0  # Unused in this type of RoPE\n+\n+        # Compute the inverse frequencies\n+        inv_freq = 1.0 / (\n+            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n+        )\n+        return inv_freq, attention_factor\n \n     @torch.no_grad()\n     @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n@@ -709,16 +738,16 @@ def forward(\n         )\n \n         hidden_states = inputs_embeds\n-        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids=position_ids)\n \n         for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n             hidden_states = decoder_layer(\n                 hidden_states,\n                 attention_mask=causal_mask,\n+                position_embeddings=position_embeddings,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n                 cache_position=cache_position,\n-                position_embeddings=position_embeddings,\n                 **kwargs,\n             )\n "
        },
        {
            "sha": "99a28bd96537ab1c92d3037128b6005070719e60",
            "filename": "src/transformers/models/aria/modular_aria.py",
            "status": "modified",
            "additions": 4,
            "deletions": 39,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -147,45 +147,10 @@ class AriaTextConfig(LlamaConfig):\n             results. Please refer to [this issue](https://github.com/pytorch/pytorch/issues/76232).\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie weight embeddings\n-        rope_theta (`float`, *optional*, defaults to 10000.0):\n-            The base period of the RoPE embeddings.\n-        rope_scaling (`Dict`, *optional*):\n-            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n-            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n-            accordingly.\n-            Expected contents:\n-                `rope_type` (`str`):\n-                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n-                    'llama3'], with 'default' being the original RoPE implementation.\n-                `factor` (`float`, *optional*):\n-                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n-                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n-                    original maximum pre-trained length.\n-                `original_max_position_embeddings` (`int`, *optional*):\n-                    Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n-                    pretraining.\n-                `attention_factor` (`float`, *optional*):\n-                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n-                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n-                    `factor` field to infer the suggested value.\n-                `beta_fast` (`float`, *optional*):\n-                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n-                    ramp function. If unspecified, it defaults to 32.\n-                `beta_slow` (`float`, *optional*):\n-                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n-                    ramp function. If unspecified, it defaults to 1.\n-                `short_factor` (`list[float]`, *optional*):\n-                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n-                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n-                    size divided by the number of attention heads divided by 2\n-                `long_factor` (`list[float]`, *optional*):\n-                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n-                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n-                    size divided by the number of attention heads divided by 2\n-                `low_freq_factor` (`float`, *optional*):\n-                    Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n-                `high_freq_factor` (`float`, *optional*):\n-                    Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         attention_bias (`bool`, *optional*, defaults to `False`):\n             Whether to use a bias in the query, key, value and output projection layers during self-attention.\n         attention_dropout (`float`, *optional*, defaults to 0.0):"
        },
        {
            "sha": "07fd3eaa1aab828a2a5f044dbbf31b93be1e98ef",
            "filename": "src/transformers/models/bamba/configuration_bamba.py",
            "status": "modified",
            "additions": 44,
            "deletions": 31,
            "changes": 75,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fbamba%2Fconfiguration_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fbamba%2Fconfiguration_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fconfiguration_bamba.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -14,7 +14,10 @@\n # limitations under the License.\n \"\"\"Bamba model configuration\"\"\"\n \n+from typing import Optional\n+\n from ...configuration_utils import PreTrainedConfig\n+from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n from ...utils import logging\n \n \n@@ -102,42 +105,46 @@ class BambaConfig(PreTrainedConfig):\n             Flag indicating whether or not to use bias in the input and output projections ([\"in_proj\", \"out_proj\"]) of the mamba mixer block\n         z_loss_coefficient (`float`, *optional*, defaults to 0.0):\n             Coefficient for auxiliary z-loss used to control logit growth during training\n-\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n     \"\"\"\n \n     model_type = \"bamba\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n \n     def __init__(\n         self,\n-        vocab_size=128000,\n-        tie_word_embeddings=False,\n-        hidden_size=4096,\n-        intermediate_size=14336,\n-        num_hidden_layers=32,\n-        num_attention_heads=32,\n-        num_key_value_heads=8,\n-        hidden_act=\"silu\",\n-        initializer_range=0.02,\n-        rms_norm_eps=1e-5,\n-        use_cache=True,\n-        num_logits_to_keep=1,\n-        pad_token_id=0,\n-        bos_token_id=1,\n-        eos_token_id=2,\n-        max_position_embeddings=262144,\n-        attention_dropout=0.0,\n-        attn_layer_indices=None,\n-        mamba_n_heads=128,\n-        mamba_d_head=\"auto\",\n-        mamba_n_groups=1,\n-        mamba_d_state=256,\n-        mamba_d_conv=4,\n-        mamba_expand=2,\n-        mamba_chunk_size=256,\n-        mamba_conv_bias=True,\n-        mamba_proj_bias=False,\n-        z_loss_coefficient=0.0,\n+        vocab_size: Optional[int] = 128000,\n+        tie_word_embeddings: Optional[bool] = False,\n+        hidden_size: Optional[int] = 4096,\n+        intermediate_size: Optional[int] = 14336,\n+        num_hidden_layers: Optional[int] = 32,\n+        num_attention_heads: Optional[int] = 32,\n+        num_key_value_heads: Optional[int] = 8,\n+        hidden_act: Optional[str] = \"silu\",\n+        initializer_range: Optional[float] = 0.02,\n+        rms_norm_eps: Optional[float] = 1e-5,\n+        use_cache: Optional[bool] = True,\n+        num_logits_to_keep: Optional[int] = 1,\n+        pad_token_id: Optional[int] = 0,\n+        bos_token_id: Optional[int] = 1,\n+        eos_token_id: Optional[int] = 2,\n+        max_position_embeddings: Optional[int] = 262144,\n+        attention_dropout: Optional[float] = 0.0,\n+        attn_layer_indices: Optional[list[int]] = None,\n+        mamba_n_heads: Optional[int] = 128,\n+        mamba_d_head: Optional[str] = \"auto\",\n+        mamba_n_groups: Optional[int] = 1,\n+        mamba_d_state: Optional[int] = 256,\n+        mamba_d_conv: Optional[int] = 4,\n+        mamba_expand: Optional[int] = 2,\n+        mamba_chunk_size: Optional[int] = 256,\n+        mamba_conv_bias: Optional[bool] = True,\n+        mamba_proj_bias: Optional[bool] = False,\n+        z_loss_coefficient: Optional[float] = 0.0,\n+        rope_parameters: Optional[RopeParameters] = None,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size\n@@ -164,9 +171,15 @@ def __init__(\n         self.num_logits_to_keep = num_logits_to_keep\n \n         self.attn_layer_indices = attn_layer_indices\n-        self.rope_theta = 10000.0\n-        self.rope_scaling = None\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n         self.partial_rotary_factor = 0.5\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n+\n+        # Validate the correctness of rotary position embeddings parameters\n+        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n+        rope_config_validation(self)\n \n         mamba_intermediate = mamba_expand * hidden_size\n "
        },
        {
            "sha": "eddbe7763e6665b9c96bb1f4788051798757b7f6",
            "filename": "src/transformers/models/bamba/modeling_bamba.py",
            "status": "modified",
            "additions": 41,
            "deletions": 14,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -198,20 +198,49 @@ class BambaRotaryEmbedding(nn.Module):\n \n     def __init__(self, config: BambaConfig, device=None):\n         super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n         self.max_seq_len_cached = config.max_position_embeddings\n         self.original_max_seq_len = config.max_position_embeddings\n \n         self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.rope_type = self.config.rope_parameters[\"rope_type\"]\n+        rope_init_fn: Callable = self.compute_default_rope_parameters\n+        if self.rope_type != \"default\":\n+            rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+        inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n+\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n+        self.original_inv_freq = inv_freq\n+\n+    @staticmethod\n+    def compute_default_rope_parameters(\n+        config: Optional[BambaConfig] = None,\n+        device: Optional[\"torch.device\"] = None,\n+        seq_len: Optional[int] = None,\n+    ) -> tuple[\"torch.Tensor\", float]:\n+        \"\"\"\n+        Computes the inverse frequencies according to the original RoPE implementation\n+        Args:\n+            config ([`~transformers.PreTrainedConfig`]):\n+                The model configuration.\n+            device (`torch.device`):\n+                The device to use for initialization of the inverse frequencies.\n+            seq_len (`int`, *optional*):\n+                The current sequence length. Unused for this type of RoPE.\n+        Returns:\n+            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n+            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n+        \"\"\"\n+        base = config.rope_parameters[\"rope_theta\"]\n+        dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+\n+        attention_factor = 1.0  # Unused in this type of RoPE\n+\n+        # Compute the inverse frequencies\n+        inv_freq = 1.0 / (\n+            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n+        )\n+        return inv_freq, attention_factor\n \n     @torch.no_grad()\n     @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n@@ -344,8 +373,8 @@ def __init__(self, config: BambaConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n-        attention_mask: Optional[torch.Tensor],\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -1015,7 +1044,7 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         **kwargs: Unpack[BambaFlashAttentionKwargs],\n     ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n@@ -1174,9 +1203,7 @@ def forward(\n             attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n         )\n         mamba_mask = self._update_mamba_mask(attention_mask, cache_position)\n-\n-        # create position embeddings to be shared across the decoder layers\n-        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids=position_ids)\n \n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None"
        },
        {
            "sha": "b578eb1ce2207056488ff23917bd741c886f6d68",
            "filename": "src/transformers/models/bamba/modular_bamba.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -729,7 +729,7 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         **kwargs: Unpack[BambaFlashAttentionKwargs],\n     ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n@@ -888,9 +888,7 @@ def forward(\n             attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n         )\n         mamba_mask = self._update_mamba_mask(attention_mask, cache_position)\n-\n-        # create position embeddings to be shared across the decoder layers\n-        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids=position_ids)\n \n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None"
        },
        {
            "sha": "1bcf84d0c6c4f2ee0f3569a91d8efefae47fdd2b",
            "filename": "src/transformers/models/bitnet/configuration_bitnet.py",
            "status": "modified",
            "additions": 33,
            "deletions": 21,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fbitnet%2Fconfiguration_bitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fbitnet%2Fconfiguration_bitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbitnet%2Fconfiguration_bitnet.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -13,7 +13,10 @@\n # See the License for the specific language governing permissions and\n \"\"\"BitNet model configuration\"\"\"\n \n+from typing import Optional\n+\n from ...configuration_utils import PreTrainedConfig\n+from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n from ...utils import logging\n \n \n@@ -70,12 +73,14 @@ class BitNetConfig(PreTrainedConfig):\n             End of stream token id.\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie weight embeddings\n-        rope_theta (`float`, *optional*, defaults to 500000.0):\n-            The base period of the RoPE embeddings.\n         attention_bias (`bool`, *optional*, defaults to `False`):\n             Whether to use a bias in the query, key, value and output projection layers during self-attention.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n             The dropout ratio for the attention probabilities.\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n \n     ```python\n     >>> from transformers import BitNetModel, BitNetConfig\n@@ -95,24 +100,24 @@ class BitNetConfig(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size=128256,\n-        hidden_size=2560,\n-        intermediate_size=6912,\n-        num_hidden_layers=30,\n-        num_attention_heads=20,\n-        num_key_value_heads=5,\n-        hidden_act=\"relu2\",\n-        max_position_embeddings=2048,\n-        initializer_range=0.02,\n-        rms_norm_eps=1e-5,\n-        use_cache=True,\n-        pad_token_id=None,\n-        bos_token_id=128000,\n-        eos_token_id=128001,\n-        tie_word_embeddings=False,\n-        rope_theta=500000.0,\n-        attention_bias=False,\n-        attention_dropout=0.0,\n+        vocab_size: Optional[int] = 128256,\n+        hidden_size: Optional[int] = 2560,\n+        intermediate_size: Optional[int] = 6912,\n+        num_hidden_layers: Optional[int] = 30,\n+        num_attention_heads: Optional[int] = 20,\n+        num_key_value_heads: Optional[int] = 5,\n+        hidden_act: Optional[str] = \"relu2\",\n+        max_position_embeddings: Optional[int] = 2048,\n+        initializer_range: Optional[float] = 0.02,\n+        rms_norm_eps: Optional[int] = 1e-5,\n+        use_cache: Optional[bool] = True,\n+        pad_token_id: Optional[int] = None,\n+        bos_token_id: Optional[int] = 128000,\n+        eos_token_id: Optional[int] = 128001,\n+        tie_word_embeddings: Optional[bool] = False,\n+        attention_bias: Optional[bool] = False,\n+        attention_dropout: Optional[str] = 0.0,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size\n@@ -131,9 +136,16 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.rms_norm_eps = rms_norm_eps\n         self.use_cache = use_cache\n-        self.rope_theta = rope_theta\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n+\n+        # Validate the correctness of rotary position embeddings parameters\n+        rope_theta = kwargs.get(\"rope_theta\", 500000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n+        rope_config_validation(self)\n \n         super().__init__(\n             pad_token_id=pad_token_id,"
        },
        {
            "sha": "d3972946a2033874de53bc1dc174738c92ea4f93",
            "filename": "src/transformers/models/bitnet/modeling_bitnet.py",
            "status": "modified",
            "additions": 40,
            "deletions": 11,
            "changes": 51,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -242,7 +242,7 @@ def forward(\n         past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> torch.Tensor:\n         residual = hidden_states\n@@ -273,20 +273,49 @@ class BitNetRotaryEmbedding(nn.Module):\n \n     def __init__(self, config: BitNetConfig, device=None):\n         super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n         self.max_seq_len_cached = config.max_position_embeddings\n         self.original_max_seq_len = config.max_position_embeddings\n \n         self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.rope_type = self.config.rope_parameters[\"rope_type\"]\n+        rope_init_fn: Callable = self.compute_default_rope_parameters\n+        if self.rope_type != \"default\":\n+            rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+        inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n+\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n+        self.original_inv_freq = inv_freq\n+\n+    @staticmethod\n+    def compute_default_rope_parameters(\n+        config: Optional[BitNetConfig] = None,\n+        device: Optional[\"torch.device\"] = None,\n+        seq_len: Optional[int] = None,\n+    ) -> tuple[\"torch.Tensor\", float]:\n+        \"\"\"\n+        Computes the inverse frequencies according to the original RoPE implementation\n+        Args:\n+            config ([`~transformers.PreTrainedConfig`]):\n+                The model configuration.\n+            device (`torch.device`):\n+                The device to use for initialization of the inverse frequencies.\n+            seq_len (`int`, *optional*):\n+                The current sequence length. Unused for this type of RoPE.\n+        Returns:\n+            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n+            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n+        \"\"\"\n+        base = config.rope_parameters[\"rope_theta\"]\n+        dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+\n+        attention_factor = 1.0  # Unused in this type of RoPE\n+\n+        # Compute the inverse frequencies\n+        inv_freq = 1.0 / (\n+            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n+        )\n+        return inv_freq, attention_factor\n \n     @torch.no_grad()\n     @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n@@ -382,16 +411,16 @@ def forward(\n         )\n \n         hidden_states = inputs_embeds\n-        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids=position_ids)\n \n         for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n             hidden_states = decoder_layer(\n                 hidden_states,\n                 attention_mask=causal_mask,\n+                position_embeddings=position_embeddings,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n                 cache_position=cache_position,\n-                position_embeddings=position_embeddings,\n                 **kwargs,\n             )\n "
        },
        {
            "sha": "b20ae8c6dad3bca7ba8916c51312408014374be6",
            "filename": "src/transformers/models/blt/configuration_blt.py",
            "status": "modified",
            "additions": 183,
            "deletions": 158,
            "changes": 341,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fblt%2Fconfiguration_blt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fblt%2Fconfiguration_blt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblt%2Fconfiguration_blt.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -14,7 +14,10 @@\n # limitations under the License.\n \"\"\"Blt model configuration\"\"\"\n \n+from typing import Optional\n+\n from ...configuration_utils import PreTrainedConfig\n+from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n from ...utils import logging\n \n \n@@ -30,22 +33,21 @@ class BltLocalEncoderConfig(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size=260,\n-        cross_attn_all_layers=False,\n-        cross_attn_k=2,\n-        hidden_size_global=2048,\n-        hidden_size=1024,\n-        num_attention_heads=16,\n-        num_key_value_heads=None,\n-        num_hidden_layers=1,\n-        rms_norm_eps=1e-5,\n-        dropout=0.0,\n-        max_position_embeddings=24576,\n-        rope_theta=500000.0,\n-        rope_scaling=None,\n-        hidden_act=\"silu\",\n-        intermediate_size=2816,\n-        initializer_range=0.02,\n+        vocab_size: Optional[int] = 260,\n+        cross_attn_all_layers: Optional[bool] = False,\n+        cross_attn_k: Optional[int] = 2,\n+        hidden_size_global: Optional[int] = 2048,\n+        hidden_size: Optional[int] = 1024,\n+        num_attention_heads: Optional[int] = 16,\n+        num_key_value_heads: Optional[int] = None,\n+        num_hidden_layers: Optional[int] = 1,\n+        rms_norm_eps: Optional[float] = 1e-5,\n+        dropout: Optional[float] = 0.0,\n+        max_position_embeddings: Optional[int] = 24576,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        hidden_act: Optional[str] = \"silu\",\n+        intermediate_size: Optional[int] = 2816,\n+        initializer_range: Optional[float] = 0.02,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size\n@@ -61,10 +63,16 @@ def __init__(\n         self.rms_norm_eps = rms_norm_eps\n         self.dropout = dropout\n         self.max_position_embeddings = max_position_embeddings\n-        self.rope_theta = rope_theta\n-        self.rope_scaling = rope_scaling\n         self.hidden_act = hidden_act\n         self.initializer_range = initializer_range\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n+\n+        # Validate the correctness of rotary position embeddings parameters\n+        rope_theta = kwargs.get(\"rope_theta\", 500000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n+        rope_config_validation(self)\n \n         # Remove tie_word_embeddings from kwargs to avoid duplicate parameter error\n         kwargs.pop(\"tie_word_embeddings\", None)\n@@ -80,22 +88,21 @@ class BltLocalDecoderConfig(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size=260,\n-        cross_attn_all_layers=True,\n-        cross_attn_k=2,\n-        hidden_size_global=2048,\n-        hidden_size=1024,\n-        num_attention_heads=16,\n-        num_key_value_heads=None,\n-        num_hidden_layers=9,\n-        rms_norm_eps=1e-5,\n-        dropout=0.0,\n-        max_position_embeddings=24576,\n-        rope_theta=500000.0,\n-        rope_scaling=None,\n-        hidden_act=\"silu\",\n-        intermediate_size=2816,\n-        initializer_range=0.02,\n+        vocab_size: Optional[int] = 260,\n+        cross_attn_all_layers: Optional[bool] = True,\n+        cross_attn_k: Optional[int] = 2,\n+        hidden_size_global: Optional[int] = 2048,\n+        hidden_size: Optional[int] = 1024,\n+        num_attention_heads: Optional[int] = 16,\n+        num_key_value_heads: Optional[int] = None,\n+        num_hidden_layers: Optional[int] = 9,\n+        rms_norm_eps: Optional[float] = 1e-5,\n+        dropout: Optional[float] = 0.0,\n+        max_position_embeddings: Optional[int] = 24576,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        hidden_act: Optional[str] = \"silu\",\n+        intermediate_size: Optional[int] = 2816,\n+        initializer_range: Optional[float] = 0.02,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size\n@@ -111,10 +118,16 @@ def __init__(\n         self.rms_norm_eps = rms_norm_eps\n         self.dropout = dropout\n         self.max_position_embeddings = max_position_embeddings\n-        self.rope_theta = rope_theta\n-        self.rope_scaling = rope_scaling\n         self.hidden_act = hidden_act\n         self.initializer_range = initializer_range\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n+\n+        # Validate the correctness of rotary position embeddings parameters\n+        rope_theta = kwargs.get(\"rope_theta\", 500000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n+        rope_config_validation(self)\n \n         # Remove tie_word_embeddings from kwargs to avoid duplicate parameter error\n         kwargs.pop(\"tie_word_embeddings\", None)\n@@ -130,18 +143,17 @@ class BltGlobalTransformerConfig(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        hidden_size=2048,\n-        num_attention_heads=16,\n-        num_key_value_heads=None,\n-        num_hidden_layers=25,\n-        rms_norm_eps=1e-5,\n-        dropout=0.0,\n-        max_position_embeddings=4096,\n-        rope_theta=500000.0,\n-        rope_scaling=None,\n-        hidden_act=\"silu\",\n-        intermediate_size=5632,\n-        initializer_range=0.02,\n+        hidden_size: Optional[int] = 2048,\n+        num_attention_heads: Optional[int] = 16,\n+        num_key_value_heads: Optional[int] = None,\n+        num_hidden_layers: Optional[int] = 25,\n+        rms_norm_eps: Optional[float] = 1e-5,\n+        dropout: Optional[float] = 0.0,\n+        max_position_embeddings: Optional[int] = 4096,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        hidden_act: Optional[str] = \"silu\",\n+        intermediate_size: Optional[int] = 5632,\n+        initializer_range: Optional[float] = 0.02,\n         **kwargs,\n     ):\n         self.hidden_size = hidden_size\n@@ -153,10 +165,15 @@ def __init__(\n         self.rms_norm_eps = rms_norm_eps\n         self.dropout = dropout\n         self.max_position_embeddings = max_position_embeddings\n-        self.rope_theta = rope_theta\n-        self.rope_scaling = rope_scaling\n         self.hidden_act = hidden_act\n         self.initializer_range = initializer_range\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n+\n+        # Validate the correctness of rotary position embeddings parameters\n+        rope_theta = kwargs.get(\"rope_theta\", 500000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n \n         # Remove tie_word_embeddings from kwargs to avoid duplicate parameter error\n         kwargs.pop(\"tie_word_embeddings\", None)\n@@ -168,55 +185,54 @@ class BltPatcherConfig(PreTrainedConfig):\n     Configuration class for the Blt Patcher/Entropy model component.\n \n     Args:\n-            vocab_size (`int`, *optional*, defaults to 260):\n-                Vocabulary size of the Blt patcher model. Defines the number of different tokens that can be represented by the\n-                `inputs_ids` passed when calling the patcher model.\n-            hidden_size (`int`, *optional*, defaults to 768):\n-                Dimension of the hidden representations.\n-            num_hidden_layers (`int`, *optional*, defaults to 14):\n-                Number of hidden layers in the Transformer decoder.\n-            num_attention_heads (`int`, *optional*, defaults to 12):\n-                Number of attention heads for each attention layer in the Transformer decoder.\n-            num_key_value_heads (`int`, *optional*):\n-                This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n-                `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n-                `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n-                converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n-                by meanpooling all the original heads within that group. For more details, check out [this\n-                paper](https://huggingface.co/papers/2305.13245). If it is not specified, will default to\n-                `num_attention_heads`.\n-            max_position_embeddings (`int`, *optional*, defaults to 8192):\n-                The maximum sequence length that this model might ever be used with.\n-            rms_norm_eps (`float`, *optional*, defaults to 1e-05):\n-                The epsilon used by the rms normalization layers.\n-            dropout (`float`, *optional*, defaults to 0.0):\n-                The dropout ratio for the attention probabilities.\n-            rope_theta (`float`, *optional*, defaults to 10000.0):\n-                The base period of the RoPE embeddings.\n-            intermediate_size (`int`, *optional*, defaults to 2048):\n-                Dimension of the MLP representations.\n-            rope_scaling (`dict`, *optional*):\n-                Dictionary containing the RoPE scaling configuration.\n-            initializer_range (`float`, *optional*, defaults to 0.02):\n-                The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        vocab_size (`int`, *optional*, defaults to 260):\n+            Vocabulary size of the Blt patcher model. Defines the number of different tokens that can be represented by the\n+            `inputs_ids` passed when calling the patcher model.\n+        hidden_size (`int`, *optional*, defaults to 768):\n+            Dimension of the hidden representations.\n+        num_hidden_layers (`int`, *optional*, defaults to 14):\n+            Number of hidden layers in the Transformer decoder.\n+        num_attention_heads (`int`, *optional*, defaults to 12):\n+            Number of attention heads for each attention layer in the Transformer decoder.\n+        num_key_value_heads (`int`, *optional*):\n+            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n+            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n+            `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n+            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n+            by meanpooling all the original heads within that group. For more details, check out [this\n+            paper](https://huggingface.co/papers/2305.13245). If it is not specified, will default to\n+            `num_attention_heads`.\n+        max_position_embeddings (`int`, *optional*, defaults to 8192):\n+            The maximum sequence length that this model might ever be used with.\n+        rms_norm_eps (`float`, *optional*, defaults to 1e-05):\n+            The epsilon used by the rms normalization layers.\n+        dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        intermediate_size (`int`, *optional*, defaults to 2048):\n+            Dimension of the MLP representations.\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n     \"\"\"\n \n     model_type = \"blt_patcher\"\n \n     def __init__(\n         self,\n-        vocab_size=260,\n-        hidden_size=768,\n-        num_hidden_layers=14,\n-        num_attention_heads=12,\n-        num_key_value_heads=None,\n-        max_position_embeddings=8192,\n-        rms_norm_eps=1e-5,\n-        dropout=0.0,\n-        rope_theta=10000.0,\n-        intermediate_size=2048,\n-        rope_scaling=None,\n-        initializer_range=0.02,\n+        vocab_size: Optional[int] = 260,\n+        hidden_size: Optional[int] = 768,\n+        num_hidden_layers: Optional[int] = 14,\n+        num_attention_heads: Optional[int] = 12,\n+        num_key_value_heads: Optional[int] = None,\n+        max_position_embeddings: Optional[int] = 8192,\n+        rms_norm_eps: Optional[float] = 1e-5,\n+        dropout: Optional[float] = 0.0,\n+        intermediate_size: Optional[int] = 2048,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        initializer_range: Optional[float] = 0.02,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size\n@@ -228,11 +244,16 @@ def __init__(\n         self.max_position_embeddings = max_position_embeddings\n         self.rms_norm_eps = rms_norm_eps\n         self.dropout = dropout\n-        self.rope_theta = rope_theta\n         self.hidden_act = \"silu\"  # Blt uses silu activation\n         self.intermediate_size = intermediate_size or int(8 * self.hidden_size / 3)\n-        self.rope_scaling = rope_scaling\n         self.initializer_range = initializer_range\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n+\n+        # Validate the correctness of rotary position embeddings parameters\n+        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n \n         # Remove tie_word_embeddings from kwargs to avoid duplicate parameter error\n         kwargs.pop(\"tie_word_embeddings\", None)\n@@ -248,47 +269,47 @@ class BltConfig(PreTrainedConfig):\n     documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n-            vocab_size (`int`, *optional*, defaults to 260):\n-                Vocabulary size of the Blt model. Defines the number of different tokens that can be represented by the\n-                `inputs_ids` passed when calling [`BltModel`].\n-            max_position_embeddings (`int`, *optional*, defaults to 4096):\n-                The maximum sequence length that this model might ever be used with.\n-            patch_in_forward (`bool`, *optional*, defaults to `True`):\n-                Whether to perform patching during the forward pass.\n-            patch_size (`int`, *optional*, defaults to 4):\n-                Size of the patches used in the patching mechanism.\n-            patching_mode (`str`, *optional*, defaults to `\"entropy\"`):\n-                The mode used for patching, such as entropy-based patching.\n-            patching_threshold (`float`, *optional*, defaults to 1.34):\n-                Threshold value used for determining when to apply patches.\n-            patching_batch_size (`int`, *optional*, defaults to 1):\n-                Batch size used during the patching process.\n-            max_patch_length (`int`, *optional*):\n-                Maximum length of patches that can be generated.\n-            cross_attn_k (`int`, *optional*, defaults to 2):\n-                Number of cross-attention heads used in the model.\n-            encoder_hash_byte_group_size (`list`, *optional*):\n-                List of byte group sizes used in the encoder hash function.\n-            encoder_hash_byte_group_vocab (`int`, *optional*, defaults to 500002):\n-                Vocabulary size for the encoder hash byte groups.\n-            encoder_hash_byte_group_nb_functions (`int`, *optional*, defaults to 1):\n-                Number of hash functions used in the encoder byte grouping.\n-            patcher_config (`BltPatcherConfig`, *optional*):\n-                Configuration for the patcher component of the model.\n-            encoder_config (`BltLocalEncoderConfig`, *optional*):\n-                Configuration for the local encoder component of the model.\n-            decoder_config (`BltLocalDecoderConfig`, *optional*):\n-                Configuration for the local decoder component of the model.\n-            global_config (`BltGlobalTransformerConfig`, *optional*):\n-                Configuration for the global transformer component of the model.\n-            tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n-                Whether to tie weight embeddings.\n-            initializer_range (`float`, *optional*, defaults to 0.02):\n-                The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n-            rope_theta (`float`, *optional*, defaults to 500000.0):\n-                The base period of the RoPE embeddings.\n-            rope_scaling (`dict`, *optional*):\n-                Dictionary containing the RoPE scaling configuration.\n+        vocab_size (`int`, *optional*, defaults to 260):\n+            Vocabulary size of the Blt model. Defines the number of different tokens that can be represented by the\n+            `inputs_ids` passed when calling [`BltModel`].\n+        max_position_embeddings (`int`, *optional*, defaults to 4096):\n+            The maximum sequence length that this model might ever be used with.\n+        patch_in_forward (`bool`, *optional*, defaults to `True`):\n+            Whether to perform patching during the forward pass.\n+        patch_size (`int`, *optional*, defaults to 4):\n+            Size of the patches used in the patching mechanism.\n+        patching_mode (`str`, *optional*, defaults to `\"entropy\"`):\n+            The mode used for patching, such as entropy-based patching.\n+        patching_threshold (`float`, *optional*, defaults to 1.34):\n+            Threshold value used for determining when to apply patches.\n+        patching_batch_size (`int`, *optional*, defaults to 1):\n+            Batch size used during the patching process.\n+        max_patch_length (`int`, *optional*):\n+            Maximum length of patches that can be generated.\n+        cross_attn_k (`int`, *optional*, defaults to 2):\n+            Number of cross-attention heads used in the model.\n+        encoder_hash_byte_group_size (`list`, *optional*):\n+            List of byte group sizes used in the encoder hash function.\n+        encoder_hash_byte_group_vocab (`int`, *optional*, defaults to 500002):\n+            Vocabulary size for the encoder hash byte groups.\n+        encoder_hash_byte_group_nb_functions (`int`, *optional*, defaults to 1):\n+            Number of hash functions used in the encoder byte grouping.\n+        patcher_config (`BltPatcherConfig`, *optional*):\n+            Configuration for the patcher component of the model.\n+        encoder_config (`BltLocalEncoderConfig`, *optional*):\n+            Configuration for the local encoder component of the model.\n+        decoder_config (`BltLocalDecoderConfig`, *optional*):\n+            Configuration for the local decoder component of the model.\n+        global_config (`BltGlobalTransformerConfig`, *optional*):\n+            Configuration for the global transformer component of the model.\n+        tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n+            Whether to tie weight embeddings.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n \n     ```python\n     >>> from transformers import BltModel, BltConfig\n@@ -317,34 +338,31 @@ class BltConfig(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size=260,\n-        max_position_embeddings=4096,\n-        patch_in_forward=True,\n-        patch_size=4,\n-        patching_mode=\"entropy\",\n-        patching_threshold=1.335442066192627,\n-        patching_batch_size=1,\n-        max_patch_length=None,\n-        cross_attn_k=2,\n-        encoder_hash_byte_group_size=None,\n-        encoder_hash_byte_group_vocab=500002,\n-        encoder_hash_byte_group_nb_functions=1,\n-        patcher_config=None,\n-        encoder_config=None,\n-        decoder_config=None,\n-        global_config=None,\n-        tie_word_embeddings=False,\n-        initializer_range=0.02,\n-        rope_theta=500000.0,\n-        rope_scaling=None,\n+        vocab_size: Optional[int] = 260,\n+        max_position_embeddings: Optional[int] = 4096,\n+        patch_in_forward: Optional[bool] = True,\n+        patch_size: Optional[int] = 4,\n+        patching_mode: Optional[str] = \"entropy\",\n+        patching_threshold: Optional[float] = 1.335442066192627,\n+        patching_batch_size: Optional[int] = 1,\n+        max_patch_length: Optional[int] = None,\n+        cross_attn_k: Optional[int] = 2,\n+        encoder_hash_byte_group_size: Optional[int] = None,\n+        encoder_hash_byte_group_vocab: Optional[int] = 500002,\n+        encoder_hash_byte_group_nb_functions: Optional[int] = 1,\n+        patcher_config: Optional[dict] = None,\n+        encoder_config: Optional[dict] = None,\n+        decoder_config: Optional[dict] = None,\n+        global_config: Optional[dict] = None,\n+        tie_word_embeddings: Optional[bool] = False,\n+        initializer_range: Optional[float] = 0.02,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n         **kwargs,\n     ):\n         # Basic model configuration\n         self.vocab_size = vocab_size\n         self.max_position_embeddings = max_position_embeddings\n         self.initializer_range = initializer_range\n-        self.rope_theta = rope_theta\n-        self.rope_scaling = rope_scaling\n \n         # Patching configuration\n         self.patch_in_forward = patch_in_forward\n@@ -357,6 +375,13 @@ def __init__(\n         self.realtime_patching = kwargs.get(\"realtime_patching\", True)\n         self.patching_threshold_add = kwargs.get(\"patching_threshold_add\")\n         self.monotonicity = kwargs.get(\"monotonicity\", False)\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n+\n+        # Validate the correctness of rotary position embeddings parameters\n+        rope_theta = kwargs.get(\"rope_theta\", 500000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n \n         # Cross attention configurations\n         self.cross_attn_k = cross_attn_k"
        },
        {
            "sha": "8325b6e1db1ff2a5b00b76b381bff2c9c7b240ca",
            "filename": "src/transformers/models/blt/convert_blt_weights_to_hf.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fblt%2Fconvert_blt_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fblt%2Fconvert_blt_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblt%2Fconvert_blt_weights_to_hf.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -87,7 +87,7 @@ def merge_configurations(config_path: str, entropy_params_path: str) -> dict[str\n         \"max_position_embeddings\": unified_config.get(\"max_encoder_seq_length\")\n         or unified_config.get(\"max_seqlen\", 1024),\n         \"rope_theta\": unified_config.get(\"rope_theta\", 10000.0),\n-        \"rope_scaling\": {\"rope_type\": \"default\"},\n+        \"rope_parameters\": {\"rope_type\": \"default\"},\n         \"hidden_act\": unified_config.get(\"hidden_act\", \"silu\"),\n         \"_attn_implementation\": unified_config.get(\"_attn_implementation\", \"sdpa\"),\n         \"intermediate_size\": encoder_intermediate_size,\n@@ -114,7 +114,7 @@ def merge_configurations(config_path: str, entropy_params_path: str) -> dict[str\n         \"max_position_embeddings\": unified_config.get(\"max_encoder_seq_length\")\n         or unified_config.get(\"max_seqlen\", 1024),\n         \"rope_theta\": unified_config.get(\"rope_theta\", 10000.0),\n-        \"rope_scaling\": {\"rope_type\": \"default\"},\n+        \"rope_parameters\": {\"rope_type\": \"default\"},\n         \"hidden_act\": unified_config.get(\"hidden_act\", \"silu\"),\n         \"_attn_implementation\": unified_config.get(\"_attn_implementation\", \"sdpa\"),\n         \"intermediate_size\": decoder_intermediate_size,\n@@ -136,7 +136,7 @@ def merge_configurations(config_path: str, entropy_params_path: str) -> dict[str\n         \"dropout\": unified_config.get(\"dropout\", 0.0),\n         \"max_position_embeddings\": unified_config.get(\"max_seqlen\", 1024),\n         \"rope_theta\": unified_config.get(\"rope_theta\", 10000.0),\n-        \"rope_scaling\": {\"rope_type\": \"default\"},\n+        \"rope_parameters\": {\"rope_type\": \"default\"},\n         \"hidden_act\": unified_config.get(\"hidden_act\", \"silu\"),\n         \"_attn_implementation\": unified_config.get(\"_attn_implementation\", \"sdpa\"),\n         \"intermediate_size\": global_intermediate_size,"
        },
        {
            "sha": "678b67b377b3c760a46247753969933ff903440a",
            "filename": "src/transformers/models/blt/modeling_blt.py",
            "status": "modified",
            "additions": 39,
            "deletions": 10,
            "changes": 49,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fblt%2Fmodeling_blt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fblt%2Fmodeling_blt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblt%2Fmodeling_blt.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -90,20 +90,49 @@ class BltRotaryEmbedding(nn.Module):\n \n     def __init__(self, config: BltConfig, device=None):\n         super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n         self.max_seq_len_cached = config.max_position_embeddings\n         self.original_max_seq_len = config.max_position_embeddings\n \n         self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.rope_type = self.config.rope_parameters[\"rope_type\"]\n+        rope_init_fn: Callable = self.compute_default_rope_parameters\n+        if self.rope_type != \"default\":\n+            rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+        inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n+\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n+        self.original_inv_freq = inv_freq\n+\n+    @staticmethod\n+    def compute_default_rope_parameters(\n+        config: Optional[BltConfig] = None,\n+        device: Optional[\"torch.device\"] = None,\n+        seq_len: Optional[int] = None,\n+    ) -> tuple[\"torch.Tensor\", float]:\n+        \"\"\"\n+        Computes the inverse frequencies according to the original RoPE implementation\n+        Args:\n+            config ([`~transformers.PreTrainedConfig`]):\n+                The model configuration.\n+            device (`torch.device`):\n+                The device to use for initialization of the inverse frequencies.\n+            seq_len (`int`, *optional*):\n+                The current sequence length. Unused for this type of RoPE.\n+        Returns:\n+            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n+            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n+        \"\"\"\n+        base = config.rope_parameters[\"rope_theta\"]\n+        dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+\n+        attention_factor = 1.0  # Unused in this type of RoPE\n+\n+        # Compute the inverse frequencies\n+        inv_freq = 1.0 / (\n+            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n+        )\n+        return inv_freq, attention_factor\n \n     @torch.no_grad()\n     @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n@@ -145,7 +174,7 @@ def forward(\n         past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n@@ -278,7 +307,7 @@ def __init__(self, config: BltConfig, layer_idx: int):\n         self.head_dim = config.hidden_size // self.num_heads\n         self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n         self.scaling = self.head_dim**-0.5\n-        self.rope_theta = config.rope_theta\n+\n         self.layer_idx = layer_idx\n         self.is_causal = True\n "
        },
        {
            "sha": "f25380d7417c58a6ac75bee171c8b37e566d0d04",
            "filename": "src/transformers/models/blt/modular_blt.py",
            "status": "modified",
            "additions": 18,
            "deletions": 6,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fblt%2Fmodular_blt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fblt%2Fmodular_blt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblt%2Fmodular_blt.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -25,14 +25,13 @@\n from ...cache_utils import Cache, DynamicCache\n from ...masking_utils import create_causal_mask\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n+from ...modeling_rope_utils import dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, logging\n from ...utils.generic import OutputRecorder, check_model_inputs\n-from ..cohere2.modeling_cohere2 import (\n-    Cohere2RotaryEmbedding,\n-    rotate_half,  # noqa: F401\n-)\n+from ..cohere2.modeling_cohere2 import rotate_half  # noqa: F401\n+from ..llama.modeling_llama import LlamaRotaryEmbedding\n from ..mllama.modeling_mllama import (\n     MllamaForCausalLM,\n     MllamaPreTrainedModel,\n@@ -270,8 +269,21 @@ class BltRMSNorm(MllamaTextRMSNorm):\n     pass\n \n \n-class BltRotaryEmbedding(Cohere2RotaryEmbedding):\n-    pass\n+class BltRotaryEmbedding(LlamaRotaryEmbedding):\n+    @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n+    def forward(self, x, position_ids):\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.repeat_interleave(freqs, 2, dim=-1)  # diff from Llama: we interleave() instead of cat()\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n \n \n class BltTransformerLayer(MllamaSelfAttentionDecoderLayer):"
        },
        {
            "sha": "72e6eccca2a3a70fba93aa00dd20d1e5d6df779f",
            "filename": "src/transformers/models/chameleon/configuration_chameleon.py",
            "status": "modified",
            "additions": 35,
            "deletions": 58,
            "changes": 93,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fchameleon%2Fconfiguration_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fchameleon%2Fconfiguration_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fconfiguration_chameleon.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -17,6 +17,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n+from ...modeling_rope_utils import RopeParameters, standardize_rope_params\n from ...utils import logging\n \n \n@@ -147,16 +148,10 @@ class ChameleonConfig(PreTrainedConfig):\n             End of stream token id.\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie weight embeddings\n-        rope_theta (`float`, *optional*, defaults to 10000.0):\n-            The base period of the RoPE embeddings.\n-        rope_scaling (`Dict`, *optional*):\n-            Dictionary containing the scaling configuration for the RoPE embeddings. Currently supports two scaling\n-            strategies: linear and dynamic. Their scaling factor must be a float greater than 1. The expected format is\n-            `{\"type\": strategy name, \"factor\": scaling factor}`. When using this flag, don't update\n-            `max_position_embeddings` to the expected new maximum. See the following thread for more information on how\n-            these scaling strategies behave:\n-            https://www.reddit.com/r/Localchameleon/comments/14mrgpr/dynamically_scaled_rope_further_increases/. This is an\n-            experimental feature, subject to breaking API changes in future versions.\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):\n             Whether to use a bias in the query, key, value and output projection layers during self-attention.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n@@ -193,30 +188,29 @@ class ChameleonConfig(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size=65536,\n-        hidden_size=4096,\n-        intermediate_size=11008,\n-        num_hidden_layers=32,\n-        num_attention_heads=32,\n-        num_key_value_heads=32,\n-        hidden_act=\"silu\",\n-        max_position_embeddings=4096,\n-        initializer_range=0.02,\n-        rms_norm_eps=1e-05,\n-        use_cache=True,\n-        pad_token_id=None,\n-        bos_token_id=1,\n-        eos_token_id=2,\n-        tie_word_embeddings=False,\n-        rope_theta=10000.0,\n-        rope_scaling=None,\n-        attention_bias=False,\n-        attention_dropout=0.0,\n-        model_parallel_size=1,\n-        swin_norm=False,\n-        vq_config=None,\n-        vocabulary_map=None,\n-        mlp_bias=False,\n+        vocab_size: Optional[int] = 65536,\n+        hidden_size: Optional[int] = 4096,\n+        intermediate_size: Optional[int] = 11008,\n+        num_hidden_layers: Optional[int] = 32,\n+        num_attention_heads: Optional[int] = 32,\n+        num_key_value_heads: Optional[int] = 32,\n+        hidden_act: Optional[int] = \"silu\",\n+        max_position_embeddings: Optional[int] = 4096,\n+        initializer_range: Optional[float] = 0.02,\n+        rms_norm_eps: Optional[int] = 1e-05,\n+        use_cache: Optional[bool] = True,\n+        pad_token_id: Optional[int] = None,\n+        bos_token_id: Optional[int] = 1,\n+        eos_token_id: Optional[int] = 2,\n+        tie_word_embeddings: Optional[bool] = False,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        attention_bias: Optional[int] = False,\n+        attention_dropout: Optional[float] = 0.0,\n+        model_parallel_size: Optional[int] = 1,\n+        swin_norm: Optional[bool] = False,\n+        vq_config: Optional[dict] = None,\n+        vocabulary_map: Optional[dict] = None,\n+        mlp_bias: Optional[bool] = False,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size\n@@ -232,13 +226,17 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.rms_norm_eps = rms_norm_eps\n         self.use_cache = use_cache\n-        self.rope_theta = rope_theta\n-        self.rope_scaling = rope_scaling\n-        self._rope_scaling_validation()\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n         self.model_parallel_size = model_parallel_size\n         self.swin_norm = swin_norm\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n+\n+        # Validate the correctness of rotary position embeddings parameters\n+        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n \n         if vq_config is None:\n             vq_config = {}\n@@ -257,26 +255,5 @@ def __init__(\n             **kwargs,\n         )\n \n-    def _rope_scaling_validation(self):\n-        \"\"\"\n-        Validate the `rope_scaling` configuration.\n-        \"\"\"\n-        if self.rope_scaling is None:\n-            return\n-\n-        if not isinstance(self.rope_scaling, dict) or len(self.rope_scaling) != 2:\n-            raise ValueError(\n-                \"`rope_scaling` must be a dictionary with with two fields, `type` and `factor`, \"\n-                f\"got {self.rope_scaling}\"\n-            )\n-        rope_scaling_type = self.rope_scaling.get(\"type\", None)\n-        rope_scaling_factor = self.rope_scaling.get(\"factor\", None)\n-        if rope_scaling_type is None or rope_scaling_type not in [\"linear\", \"dynamic\"]:\n-            raise ValueError(\n-                f\"`rope_scaling`'s type field must be one of ['linear', 'dynamic'], got {rope_scaling_type}\"\n-            )\n-        if rope_scaling_factor is None or not isinstance(rope_scaling_factor, float) or rope_scaling_factor <= 1.0:\n-            raise ValueError(f\"`rope_scaling`'s factor field must be a float > 1, got {rope_scaling_factor}\")\n-\n \n __all__ = [\"ChameleonConfig\", \"ChameleonVQVAEConfig\"]"
        },
        {
            "sha": "136b47b016c26610ae3d3e67cbb998923e42da79",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 62,
            "deletions": 83,
            "changes": 145,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -29,6 +29,7 @@\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import (\n@@ -64,70 +65,70 @@ def extra_repr(self):\n         return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n \n \n-# copied from transformers.models.llama.modeling_llama.LlamaRotaryEmbedding with Llama->Chameleon\n-# TODO(joao): add me back asap :)\n+# Copied from transformers.models.llama.modeling_llama.LlamaRotaryEmbedding with Llama->Chameleon\n class ChameleonRotaryEmbedding(nn.Module):\n     inv_freq: torch.Tensor  # fix linting for `register_buffer`\n \n-    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None, scaling_factor=1.0):\n+    def __init__(self, config: ChameleonConfig, device=None):\n         super().__init__()\n-        self.scaling_factor = scaling_factor\n-        self.dim = dim\n-        self.max_position_embeddings = max_position_embeddings\n-        self.base = base\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+\n+        self.rope_type = self.config.rope_parameters[\"rope_type\"]\n+        rope_init_fn: Callable = self.compute_default_rope_parameters\n+        if self.rope_type != \"default\":\n+            rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+        inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n+\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = inv_freq\n+\n+    @staticmethod\n+    def compute_default_rope_parameters(\n+        config: Optional[ChameleonConfig] = None,\n+        device: Optional[\"torch.device\"] = None,\n+        seq_len: Optional[int] = None,\n+    ) -> tuple[\"torch.Tensor\", float]:\n+        \"\"\"\n+        Computes the inverse frequencies according to the original RoPE implementation\n+        Args:\n+            config ([`~transformers.PreTrainedConfig`]):\n+                The model configuration.\n+            device (`torch.device`):\n+                The device to use for initialization of the inverse frequencies.\n+            seq_len (`int`, *optional*):\n+                The current sequence length. Unused for this type of RoPE.\n+        Returns:\n+            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n+            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n+        \"\"\"\n+        base = config.rope_parameters[\"rope_theta\"]\n+        dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+\n+        attention_factor = 1.0  # Unused in this type of RoPE\n+\n+        # Compute the inverse frequencies\n         inv_freq = 1.0 / (\n-            self.base\n-            ** (torch.arange(0, self.dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / self.dim)\n+            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n         )\n-        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        # For BC we register cos and sin cached\n-        self.max_seq_len_cached = max_position_embeddings\n+        return inv_freq, attention_factor\n \n     @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n     def forward(self, x, position_ids):\n-        # x: [bs, num_attention_heads, seq_len, head_size]\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n         position_ids_expanded = position_ids[:, None, :].float()\n-        # Force float32 since bfloat16 loses precision on long contexts\n-        # See https://github.com/huggingface/transformers/pull/29285\n-        device_type = x.device.type\n-        device_type = device_type if device_type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos()\n-            sin = emb.sin()\n-        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n-\n-\n-class ChameleonLinearScalingRotaryEmbedding(ChameleonRotaryEmbedding):\n-    \"\"\"ChameleonRotaryEmbedding extended with linear scaling. Credits to the Reddit user /u/kaiokendev\"\"\"\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n \n-    def forward(self, x, position_ids):\n-        # difference to the original RoPE: a scaling factor is applied to the position ids\n-        position_ids = position_ids.float() / self.scaling_factor\n-        cos, sin = super().forward(x, position_ids)\n-        return cos, sin\n-\n-\n-class ChameleonDynamicNTKScalingRotaryEmbedding(ChameleonRotaryEmbedding):\n-    \"\"\"ChameleonRotaryEmbedding extended with Dynamic NTK scaling. Credits to the Reddit users /u/bloc97 and /u/emozilla\"\"\"\n-\n-    def forward(self, x, position_ids):\n-        # difference to the original RoPE: inv_freq is recomputed when the sequence length > original length\n-        seq_len = torch.max(position_ids) + 1\n-        if seq_len > self.max_position_embeddings:\n-            base = self.base * (\n-                (self.scaling_factor * seq_len / self.max_position_embeddings) - (self.scaling_factor - 1)\n-            ) ** (self.dim / (self.dim - 2))\n-            inv_freq = 1.0 / (\n-                base\n-                ** (torch.arange(0, self.dim, 2, dtype=torch.int64).to(device=x.device, dtype=torch.float) / self.dim)\n-            )\n-            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: this may break with compilation\n-\n-        cos, sin = super().forward(x, position_ids)\n-        return cos, sin\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n \n \n # Copied from transformers.models.llama.modeling_llama.rotate_half\n@@ -263,7 +264,7 @@ def __init__(self, config: ChameleonConfig, layer_idx: Optional[int] = None):\n         self.num_key_value_heads = config.num_key_value_heads\n         self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n         self.max_position_embeddings = config.max_position_embeddings\n-        self.rope_theta = config.rope_theta\n+\n         self.is_causal = True\n         self.model_parallel_size = config.model_parallel_size\n         self.scaling = self.head_dim**-0.5\n@@ -280,36 +281,6 @@ def __init__(self, config: ChameleonConfig, layer_idx: Optional[int] = None):\n         self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=config.attention_bias)\n         self.q_norm = ChameleonLayerNorm((self.num_heads, self.head_dim))\n         self.k_norm = ChameleonLayerNorm((self.num_key_value_heads, self.head_dim))\n-        self._init_rope()\n-\n-    # copied from transformers.models.llama.modeling_llama.LlamaAttention._init_rope with Llama->Chameleon\n-    # TODO(joao): add me back asap :)\n-    def _init_rope(self):\n-        if self.config.rope_scaling is None:\n-            self.rotary_emb = ChameleonRotaryEmbedding(\n-                self.head_dim,\n-                max_position_embeddings=self.max_position_embeddings,\n-                base=self.rope_theta,\n-            )\n-        else:\n-            scaling_type = self.config.rope_scaling[\"type\"]\n-            scaling_factor = self.config.rope_scaling[\"factor\"]\n-            if scaling_type == \"linear\":\n-                self.rotary_emb = ChameleonLinearScalingRotaryEmbedding(\n-                    self.head_dim,\n-                    max_position_embeddings=self.max_position_embeddings,\n-                    scaling_factor=scaling_factor,\n-                    base=self.rope_theta,\n-                )\n-            elif scaling_type == \"dynamic\":\n-                self.rotary_emb = ChameleonDynamicNTKScalingRotaryEmbedding(\n-                    self.head_dim,\n-                    max_position_embeddings=self.max_position_embeddings,\n-                    scaling_factor=scaling_factor,\n-                    base=self.rope_theta,\n-                )\n-            else:\n-                raise ValueError(f\"Unknown RoPE scaling type {scaling_type}\")\n \n     def forward(\n         self,\n@@ -320,6 +291,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[torch.Tensor] = None,\n         **kwargs,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         bsz, q_len, _ = hidden_states.size()\n@@ -338,7 +310,7 @@ def forward(\n         key_states = key_states.reshape(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n         value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n \n-        cos, sin = self.rotary_emb(value_states, position_ids)\n+        cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n         if past_key_values is not None:\n@@ -388,6 +360,7 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[torch.Tensor] = None,\n         **kwargs,\n     ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n@@ -422,6 +395,7 @@ def forward(\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n+            position_embeddings=position_embeddings,\n             **kwargs,\n         )\n         hidden_states = residual + hidden_states\n@@ -460,6 +434,7 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[torch.Tensor] = None,\n         **kwargs,\n     ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n@@ -493,6 +468,7 @@ def forward(\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n+            position_embeddings=position_embeddings,\n             **kwargs,\n         )\n         hidden_states = self.input_layernorm(hidden_states)\n@@ -856,6 +832,7 @@ def __init__(self, config: ChameleonConfig):\n         )\n         self.norm = ChameleonRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.vqmodel = ChameleonVQVAE._from_config(config.vq_config)\n+        self.rotary_emb = ChameleonRotaryEmbedding(config=config)\n         self.gradient_checkpointing = False\n \n         # Initialize weights and apply final processing\n@@ -980,6 +957,7 @@ def forward(\n \n         # embed positions\n         hidden_states = inputs_embeds\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids=position_ids)\n \n         # decoder layers\n         all_hidden_states = () if output_hidden_states else None\n@@ -997,6 +975,7 @@ def forward(\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n                 **kwargs,\n             )\n "
        },
        {
            "sha": "ac75ea93c864013de2667ac071fce35798caa381",
            "filename": "src/transformers/models/cohere/configuration_cohere.py",
            "status": "modified",
            "additions": 32,
            "deletions": 64,
            "changes": 96,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fcohere%2Fconfiguration_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fcohere%2Fconfiguration_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fconfiguration_cohere.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -19,8 +19,10 @@\n # limitations under the License.\n \"\"\"Cohere model configuration\"\"\"\n \n+from typing import Optional\n+\n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import rope_config_validation\n+from ...modeling_rope_utils import RopeParameters, standardize_rope_params\n from ...utils import logging\n \n \n@@ -78,45 +80,10 @@ class CohereConfig(PreTrainedConfig):\n             End of stream token id.\n         tie_word_embeddings (`bool`, *optional*, defaults to `True`):\n             Whether to tie weight embeddings\n-        rope_theta (`float`, *optional*, defaults to 10000.0):\n-            The base period of the RoPE embeddings.\n-        rope_scaling (`Dict`, *optional*):\n-            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n-            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n-            accordingly.\n-            Expected contents:\n-                `rope_type` (`str`):\n-                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n-                    'llama3'], with 'default' being the original RoPE implementation.\n-                `factor` (`float`, *optional*):\n-                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n-                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n-                    original maximum pre-trained length.\n-                `original_max_position_embeddings` (`int`, *optional*):\n-                    Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n-                    pretraining.\n-                `attention_factor` (`float`, *optional*):\n-                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n-                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n-                    `factor` field to infer the suggested value.\n-                `beta_fast` (`float`, *optional*):\n-                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n-                    ramp function. If unspecified, it defaults to 32.\n-                `beta_slow` (`float`, *optional*):\n-                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n-                    ramp function. If unspecified, it defaults to 1.\n-                `short_factor` (`list[float]`, *optional*):\n-                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n-                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n-                    size divided by the number of attention heads divided by 2\n-                `long_factor` (`list[float]`, *optional*):\n-                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n-                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n-                    size divided by the number of attention heads divided by 2\n-                `low_freq_factor` (`float`, *optional*):\n-                    Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n-                `high_freq_factor` (`float`, *optional*):\n-                    Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):\n             Whether to use a bias in the query, key, value and output projection layers during self-attention.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n@@ -156,27 +123,26 @@ class CohereConfig(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size=256000,\n-        hidden_size=8192,\n-        intermediate_size=22528,\n-        logit_scale=0.0625,\n-        num_hidden_layers=40,\n-        num_attention_heads=64,\n-        num_key_value_heads=None,\n-        hidden_act=\"silu\",\n-        max_position_embeddings=8192,\n-        initializer_range=0.02,\n-        layer_norm_eps=1e-5,\n-        use_cache=True,\n-        pad_token_id=0,\n-        bos_token_id=5,\n-        eos_token_id=255001,\n-        tie_word_embeddings=True,\n-        rope_theta=10000.0,\n-        rope_scaling=None,\n-        attention_bias=False,\n-        attention_dropout=0.0,\n-        use_qk_norm=False,\n+        vocab_size: Optional[int] = 256000,\n+        hidden_size: Optional[int] = 8192,\n+        intermediate_size: Optional[int] = 22528,\n+        logit_scale: Optional[float] = 0.0625,\n+        num_hidden_layers: Optional[int] = 40,\n+        num_attention_heads: Optional[int] = 64,\n+        num_key_value_heads: Optional[int] = None,\n+        hidden_act: Optional[str] = \"silu\",\n+        max_position_embeddings: Optional[int] = 8192,\n+        initializer_range: Optional[float] = 0.02,\n+        layer_norm_eps: Optional[int] = 1e-5,\n+        use_cache: Optional[bool] = True,\n+        pad_token_id: Optional[int] = 0,\n+        bos_token_id: Optional[int] = 5,\n+        eos_token_id: Optional[int] = 255001,\n+        tie_word_embeddings: Optional[bool] = True,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        attention_bias: Optional[bool] = False,\n+        attention_dropout: Optional[float] = 0.0,\n+        use_qk_norm: Optional[bool] = False,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size\n@@ -196,14 +162,16 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.layer_norm_eps = layer_norm_eps\n         self.use_cache = use_cache\n-        self.rope_theta = rope_theta\n-        self.rope_scaling = rope_scaling\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n         self.use_qk_norm = use_qk_norm\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n \n         # Validate the correctness of rotary position embeddings parameters\n-        rope_config_validation(self)\n+        rope_theta = kwargs.get(\"rope_theta\", 500000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n \n         super().__init__(\n             pad_token_id=pad_token_id,"
        },
        {
            "sha": "71eb4870fbf24933a791290c53b602c50f9a0c6b",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 40,
            "deletions": 11,
            "changes": 51,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -70,20 +70,49 @@ class CohereRotaryEmbedding(nn.Module):\n \n     def __init__(self, config: CohereConfig, device=None):\n         super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n         self.max_seq_len_cached = config.max_position_embeddings\n         self.original_max_seq_len = config.max_position_embeddings\n \n         self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.rope_type = self.config.rope_parameters[\"rope_type\"]\n+        rope_init_fn: Callable = self.compute_default_rope_parameters\n+        if self.rope_type != \"default\":\n+            rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+        inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n+\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n+        self.original_inv_freq = inv_freq\n+\n+    @staticmethod\n+    def compute_default_rope_parameters(\n+        config: Optional[CohereConfig] = None,\n+        device: Optional[\"torch.device\"] = None,\n+        seq_len: Optional[int] = None,\n+    ) -> tuple[\"torch.Tensor\", float]:\n+        \"\"\"\n+        Computes the inverse frequencies according to the original RoPE implementation\n+        Args:\n+            config ([`~transformers.PreTrainedConfig`]):\n+                The model configuration.\n+            device (`torch.device`):\n+                The device to use for initialization of the inverse frequencies.\n+            seq_len (`int`, *optional*):\n+                The current sequence length. Unused for this type of RoPE.\n+        Returns:\n+            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n+            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n+        \"\"\"\n+        base = config.rope_parameters[\"rope_theta\"]\n+        dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+\n+        attention_factor = 1.0  # Unused in this type of RoPE\n+\n+        # Compute the inverse frequencies\n+        inv_freq = 1.0 / (\n+            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n+        )\n+        return inv_freq, attention_factor\n \n     @torch.no_grad()\n     @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n@@ -296,7 +325,7 @@ def forward(\n         past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n@@ -415,16 +444,16 @@ def forward(\n         )\n \n         hidden_states = inputs_embeds\n-        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids=position_ids)\n \n         for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n             hidden_states = decoder_layer(\n                 hidden_states,\n                 attention_mask=causal_mask,\n+                position_embeddings=position_embeddings,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n                 cache_position=cache_position,\n-                position_embeddings=position_embeddings,\n                 **kwargs,\n             )\n "
        },
        {
            "sha": "5147e5638eb2bed571b1f5d3318afb7cdee1e5fc",
            "filename": "src/transformers/models/cohere/modular_cohere.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodular_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodular_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodular_cohere.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -213,7 +213,7 @@ def forward(\n         past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n@@ -260,7 +260,6 @@ def __init__(self, config: CohereConfig):\n         self.layers = nn.ModuleList(\n             [CohereDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n         )\n-        self.rotary_emb = CohereRotaryEmbedding(config=config)\n         self.norm = CohereLayerNorm(hidden_size=(config.hidden_size), eps=config.layer_norm_eps)\n \n "
        },
        {
            "sha": "7bf87307ee1d45bc5bee1a249f21c70e9a80d79d",
            "filename": "src/transformers/models/cohere2/configuration_cohere2.py",
            "status": "modified",
            "additions": 36,
            "deletions": 67,
            "changes": 103,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fcohere2%2Fconfiguration_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fcohere2%2Fconfiguration_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fconfiguration_cohere2.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -19,8 +19,10 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n+from typing import Optional\n+\n from ...configuration_utils import PreTrainedConfig, layer_type_validation\n-from ...modeling_rope_utils import rope_config_validation\n+from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n \n \n class Cohere2Config(PreTrainedConfig):\n@@ -74,45 +76,10 @@ class Cohere2Config(PreTrainedConfig):\n             End of stream token id.\n         tie_word_embeddings (`bool`, *optional*, defaults to `True`):\n             Whether to tie weight embeddings\n-        rope_theta (`float`, *optional*, defaults to 10000.0):\n-            The base period of the RoPE embeddings.\n-        rope_scaling (`Dict`, *optional*):\n-            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n-            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n-            accordingly.\n-            Expected contents:\n-                `rope_type` (`str`):\n-                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n-                    'llama3'], with 'default' being the original RoPE implementation.\n-                `factor` (`float`, *optional*):\n-                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n-                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n-                    original maximum pre-trained length.\n-                `original_max_position_embeddings` (`int`, *optional*):\n-                    Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n-                    pretraining.\n-                `attention_factor` (`float`, *optional*):\n-                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n-                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n-                    `factor` field to infer the suggested value.\n-                `beta_fast` (`float`, *optional*):\n-                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n-                    ramp function. If unspecified, it defaults to 32.\n-                `beta_slow` (`float`, *optional*):\n-                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n-                    ramp function. If unspecified, it defaults to 1.\n-                `short_factor` (`list[float]`, *optional*):\n-                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n-                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n-                    size divided by the number of attention heads divided by 2\n-                `long_factor` (`list[float]`, *optional*):\n-                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n-                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n-                    size divided by the number of attention heads divided by 2\n-                `low_freq_factor` (`float`, *optional*):\n-                    Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n-                `high_freq_factor` (`float`, *optional*):\n-                    Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):\n             Whether to use a bias in the query, key, value and output projection layers during self-attention.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n@@ -155,28 +122,27 @@ class Cohere2Config(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size=256000,\n-        hidden_size=8192,\n-        intermediate_size=22528,\n-        logit_scale=0.0625,\n-        num_hidden_layers=40,\n-        num_attention_heads=64,\n-        num_key_value_heads=None,\n-        hidden_act=\"silu\",\n-        max_position_embeddings=8192,\n-        initializer_range=0.02,\n-        layer_norm_eps=1e-5,\n-        use_cache=True,\n-        pad_token_id=0,\n-        bos_token_id=5,\n-        eos_token_id=255001,\n-        tie_word_embeddings=True,\n-        rope_theta=10000.0,\n-        rope_scaling=None,\n-        attention_bias=False,\n-        attention_dropout=0.0,\n-        sliding_window=4096,\n-        layer_types=None,\n+        vocab_size: Optional[int] = 256000,\n+        hidden_size: Optional[int] = 8192,\n+        intermediate_size: Optional[int] = 22528,\n+        logit_scale: Optional[float] = 0.0625,\n+        num_hidden_layers: Optional[int] = 40,\n+        num_attention_heads: Optional[int] = 64,\n+        num_key_value_heads: Optional[int] = None,\n+        hidden_act: Optional[str] = \"silu\",\n+        max_position_embeddings: Optional[int] = 8192,\n+        initializer_range: Optional[float] = 0.02,\n+        layer_norm_eps: Optional[int] = 1e-5,\n+        use_cache: Optional[int] = True,\n+        pad_token_id: Optional[int] = 0,\n+        bos_token_id: Optional[int] = 5,\n+        eos_token_id: Optional[int] = 255001,\n+        tie_word_embeddings: Optional[bool] = True,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        attention_bias: Optional[bool] = False,\n+        attention_dropout: Optional[float] = 0.0,\n+        sliding_window: Optional[int] = 4096,\n+        layer_types: Optional[list[str]] = None,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size\n@@ -196,18 +162,16 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.layer_norm_eps = layer_norm_eps\n         self.use_cache = use_cache\n-        self.rope_theta = rope_theta\n-        self.rope_scaling = rope_scaling\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n         self.sliding_window = sliding_window\n         self.layer_types = layer_types\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n         # Need to specify head_dim in the config so it can be used in the attention forward functions\n         self.head_dim = hidden_size // num_attention_heads\n \n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_config_validation(self)\n-\n         super().__init__(\n             pad_token_id=pad_token_id,\n             bos_token_id=bos_token_id,\n@@ -228,5 +192,10 @@ def __init__(\n             ]\n         layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n+        # Validate the correctness of rotary position embeddings parameters\n+        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n+        rope_config_validation(self)\n+\n \n __all__ = [\"Cohere2Config\"]"
        },
        {
            "sha": "8a9929dc3ff258957c6b4267c96db4c0a161ed40",
            "filename": "src/transformers/models/cohere2/modeling_cohere2.py",
            "status": "modified",
            "additions": 43,
            "deletions": 12,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -45,20 +45,49 @@ class Cohere2RotaryEmbedding(nn.Module):\n \n     def __init__(self, config: Cohere2Config, device=None):\n         super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n         self.max_seq_len_cached = config.max_position_embeddings\n         self.original_max_seq_len = config.max_position_embeddings\n \n         self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.rope_type = self.config.rope_parameters[\"rope_type\"]\n+        rope_init_fn: Callable = self.compute_default_rope_parameters\n+        if self.rope_type != \"default\":\n+            rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+        inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n+\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n+        self.original_inv_freq = inv_freq\n+\n+    @staticmethod\n+    def compute_default_rope_parameters(\n+        config: Optional[Cohere2Config] = None,\n+        device: Optional[\"torch.device\"] = None,\n+        seq_len: Optional[int] = None,\n+    ) -> tuple[\"torch.Tensor\", float]:\n+        \"\"\"\n+        Computes the inverse frequencies according to the original RoPE implementation\n+        Args:\n+            config ([`~transformers.PreTrainedConfig`]):\n+                The model configuration.\n+            device (`torch.device`):\n+                The device to use for initialization of the inverse frequencies.\n+            seq_len (`int`, *optional*):\n+                The current sequence length. Unused for this type of RoPE.\n+        Returns:\n+            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n+            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n+        \"\"\"\n+        base = config.rope_parameters[\"rope_theta\"]\n+        dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+\n+        attention_factor = 1.0  # Unused in this type of RoPE\n+\n+        # Compute the inverse frequencies\n+        inv_freq = 1.0 / (\n+            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n+        )\n+        return inv_freq, attention_factor\n \n     @torch.no_grad()\n     @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n@@ -181,7 +210,8 @@ def __init__(self, config: Cohere2Config, layer_idx: Optional[int] = None):\n         self.scaling = self.head_dim**-0.5\n         self.attention_dropout = config.attention_dropout\n         self.is_causal = True\n-        self.sliding_window = config.sliding_window if config.layer_types[layer_idx] == \"sliding_attention\" else None\n+        layer_type = config.layer_types[layer_idx] if hasattr(config, \"layer_types\") else None\n+        self.sliding_window = config.sliding_window if layer_type == \"sliding_attention\" else None\n \n         self.q_proj = nn.Linear(\n             config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n@@ -269,7 +299,7 @@ def __init__(self, config: Cohere2Config, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n@@ -343,7 +373,7 @@ def __init__(self, config: Cohere2Config):\n             [Cohere2DecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n         )\n         self.norm = Cohere2LayerNorm(hidden_size=(config.hidden_size), eps=config.layer_norm_eps)\n-        self.rotary_emb = Cohere2RotaryEmbedding(config=config)\n+        self.rotary_emb = Cohere2RotaryEmbedding(config)\n         self.gradient_checkpointing = False\n \n         # Initialize weights and apply final processing\n@@ -399,11 +429,12 @@ def forward(\n         for decoder_layer in self.layers:\n             hidden_states = decoder_layer(\n                 hidden_states,\n-                position_embeddings=position_embeddings,\n                 attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n+                position_embeddings=position_embeddings,\n                 past_key_values=past_key_values,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n+                position_ids=position_ids,\n                 **kwargs,\n             )\n "
        },
        {
            "sha": "dab998730c77074eea75a3735b1e3fff6e661508",
            "filename": "src/transformers/models/cohere2/modular_cohere2.py",
            "status": "modified",
            "additions": 58,
            "deletions": 72,
            "changes": 130,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -24,7 +24,12 @@\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutputWithPast\n-from ...modeling_rope_utils import rope_config_validation\n+from ...modeling_rope_utils import (\n+    RopeParameters,\n+    dynamic_rope_update,\n+    rope_config_validation,\n+    standardize_rope_params,\n+)\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, logging\n@@ -95,45 +100,10 @@ class Cohere2Config(PreTrainedConfig):\n             End of stream token id.\n         tie_word_embeddings (`bool`, *optional*, defaults to `True`):\n             Whether to tie weight embeddings\n-        rope_theta (`float`, *optional*, defaults to 10000.0):\n-            The base period of the RoPE embeddings.\n-        rope_scaling (`Dict`, *optional*):\n-            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n-            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n-            accordingly.\n-            Expected contents:\n-                `rope_type` (`str`):\n-                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n-                    'llama3'], with 'default' being the original RoPE implementation.\n-                `factor` (`float`, *optional*):\n-                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n-                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n-                    original maximum pre-trained length.\n-                `original_max_position_embeddings` (`int`, *optional*):\n-                    Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n-                    pretraining.\n-                `attention_factor` (`float`, *optional*):\n-                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n-                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n-                    `factor` field to infer the suggested value.\n-                `beta_fast` (`float`, *optional*):\n-                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n-                    ramp function. If unspecified, it defaults to 32.\n-                `beta_slow` (`float`, *optional*):\n-                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n-                    ramp function. If unspecified, it defaults to 1.\n-                `short_factor` (`list[float]`, *optional*):\n-                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n-                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n-                    size divided by the number of attention heads divided by 2\n-                `long_factor` (`list[float]`, *optional*):\n-                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n-                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n-                    size divided by the number of attention heads divided by 2\n-                `low_freq_factor` (`float`, *optional*):\n-                    Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n-                `high_freq_factor` (`float`, *optional*):\n-                    Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):\n             Whether to use a bias in the query, key, value and output projection layers during self-attention.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n@@ -176,28 +146,27 @@ class Cohere2Config(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size=256000,\n-        hidden_size=8192,\n-        intermediate_size=22528,\n-        logit_scale=0.0625,\n-        num_hidden_layers=40,\n-        num_attention_heads=64,\n-        num_key_value_heads=None,\n-        hidden_act=\"silu\",\n-        max_position_embeddings=8192,\n-        initializer_range=0.02,\n-        layer_norm_eps=1e-5,\n-        use_cache=True,\n-        pad_token_id=0,\n-        bos_token_id=5,\n-        eos_token_id=255001,\n-        tie_word_embeddings=True,\n-        rope_theta=10000.0,\n-        rope_scaling=None,\n-        attention_bias=False,\n-        attention_dropout=0.0,\n-        sliding_window=4096,\n-        layer_types=None,\n+        vocab_size: Optional[int] = 256000,\n+        hidden_size: Optional[int] = 8192,\n+        intermediate_size: Optional[int] = 22528,\n+        logit_scale: Optional[float] = 0.0625,\n+        num_hidden_layers: Optional[int] = 40,\n+        num_attention_heads: Optional[int] = 64,\n+        num_key_value_heads: Optional[int] = None,\n+        hidden_act: Optional[str] = \"silu\",\n+        max_position_embeddings: Optional[int] = 8192,\n+        initializer_range: Optional[float] = 0.02,\n+        layer_norm_eps: Optional[int] = 1e-5,\n+        use_cache: Optional[int] = True,\n+        pad_token_id: Optional[int] = 0,\n+        bos_token_id: Optional[int] = 5,\n+        eos_token_id: Optional[int] = 255001,\n+        tie_word_embeddings: Optional[bool] = True,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        attention_bias: Optional[bool] = False,\n+        attention_dropout: Optional[float] = 0.0,\n+        sliding_window: Optional[int] = 4096,\n+        layer_types: Optional[list[str]] = None,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size\n@@ -217,18 +186,16 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.layer_norm_eps = layer_norm_eps\n         self.use_cache = use_cache\n-        self.rope_theta = rope_theta\n-        self.rope_scaling = rope_scaling\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n         self.sliding_window = sliding_window\n         self.layer_types = layer_types\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n         # Need to specify head_dim in the config so it can be used in the attention forward functions\n         self.head_dim = hidden_size // num_attention_heads\n \n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_config_validation(self)\n-\n         super().__init__(\n             pad_token_id=pad_token_id,\n             bos_token_id=bos_token_id,\n@@ -249,9 +216,27 @@ def __init__(\n             ]\n         layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n+        # Validate the correctness of rotary position embeddings parameters\n+        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n+        rope_config_validation(self)\n+\n \n class Cohere2RotaryEmbedding(CohereRotaryEmbedding):\n-    pass\n+    @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n+    def forward(self, x, position_ids):\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.repeat_interleave(freqs, 2, dim=-1)  # diff from Llama: we interleave() instead of cat()\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n \n \n class Cohere2LayerNorm(CohereLayerNorm):\n@@ -270,7 +255,8 @@ def __init__(self, config: Cohere2Config, layer_idx: Optional[int] = None):\n         self.scaling = self.head_dim**-0.5\n         self.attention_dropout = config.attention_dropout\n         self.is_causal = True\n-        self.sliding_window = config.sliding_window if config.layer_types[layer_idx] == \"sliding_attention\" else None\n+        layer_type = config.layer_types[layer_idx] if hasattr(config, \"layer_types\") else None\n+        self.sliding_window = config.sliding_window if layer_type == \"sliding_attention\" else None\n \n         self.q_proj = nn.Linear(\n             config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n@@ -338,7 +324,7 @@ def __init__(self, config: Cohere2Config, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n@@ -370,7 +356,6 @@ class Cohere2Model(Gemma2Model):\n     def __init__(self, config: Cohere2Config):\n         super().__init__(config)\n         self.norm = Cohere2LayerNorm(hidden_size=(config.hidden_size), eps=config.layer_norm_eps)\n-        self.rotary_emb = Cohere2RotaryEmbedding(config=config)\n \n     def forward(\n         self,\n@@ -420,11 +405,12 @@ def forward(\n         for decoder_layer in self.layers:\n             hidden_states = decoder_layer(\n                 hidden_states,\n-                position_embeddings=position_embeddings,\n                 attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n+                position_embeddings=position_embeddings,\n                 past_key_values=past_key_values,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n+                position_ids=position_ids,\n                 **kwargs,\n             )\n "
        },
        {
            "sha": "227609c2f1aa007ead9ea51ff469ce168262b0d3",
            "filename": "src/transformers/models/csm/configuration_csm.py",
            "status": "modified",
            "additions": 72,
            "deletions": 140,
            "changes": 212,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fcsm%2Fconfiguration_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fcsm%2Fconfiguration_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fconfiguration_csm.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -13,8 +13,10 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n+from typing import Optional\n+\n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import rope_config_validation\n+from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n from ...utils import logging\n from ..auto.configuration_auto import AutoConfig\n \n@@ -74,45 +76,10 @@ class CsmDepthDecoderConfig(PreTrainedConfig):\n             Beginning of stream token id.\n         eos_token_id (`int`, *optional*):\n             End of stream token id.\n-        rope_theta (`float`, *optional*, defaults to 500000):\n-            The base period of the RoPE embeddings.\n-        rope_scaling (`Dict`, *optional*):\n-            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n-            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n-            accordingly.\n-            Expected contents:\n-                `rope_type` (`str`):\n-                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n-                    'llama3'], with 'default' being the original RoPE implementation.\n-                `factor` (`float`, *optional*):\n-                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n-                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n-                    original maximum pre-trained length.\n-                `original_max_position_embeddings` (`int`, *optional*):\n-                    Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n-                    pretraining.\n-                `attention_factor` (`float`, *optional*):\n-                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n-                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n-                    `factor` field to infer the suggested value.\n-                `beta_fast` (`float`, *optional*):\n-                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n-                    ramp function. If unspecified, it defaults to 32.\n-                `beta_slow` (`float`, *optional*):\n-                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n-                    ramp function. If unspecified, it defaults to 1.\n-                `short_factor` (`list[float]`, *optional*):\n-                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n-                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n-                    size divided by the number of attention heads divided by 2\n-                `long_factor` (`list[float]`, *optional*):\n-                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n-                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n-                    size divided by the number of attention heads divided by 2\n-                `low_freq_factor` (`float`, *optional*):\n-                    Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n-                `high_freq_factor` (`float`, *optional*):\n-                    Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         attention_bias (`bool`, *optional*, defaults to `False`):\n             Whether to use a bias in the query, key, value and output projection layers during self-attention.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n@@ -139,28 +106,27 @@ class CsmDepthDecoderConfig(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        num_codebooks=32,\n-        backbone_hidden_size=2048,\n-        vocab_size=2051,\n-        hidden_size=1024,\n-        intermediate_size=8192,\n-        num_hidden_layers=4,\n-        num_attention_heads=8,\n-        num_key_value_heads=2,\n-        hidden_act=\"silu\",\n-        max_position_embeddings=33,\n-        initializer_range=0.02,\n-        rms_norm_eps=1e-5,\n-        use_cache=True,\n-        pad_token_id=None,\n-        bos_token_id=None,\n-        eos_token_id=None,\n-        rope_theta=500000,\n-        rope_scaling=None,\n-        attention_bias=False,\n-        attention_dropout=0.0,\n-        mlp_bias=False,\n-        head_dim=None,\n+        num_codebooks: Optional[int] = 32,\n+        backbone_hidden_size: Optional[int] = 2048,\n+        vocab_size: Optional[int] = 2051,\n+        hidden_size: Optional[int] = 1024,\n+        intermediate_size: Optional[int] = 8192,\n+        num_hidden_layers: Optional[int] = 4,\n+        num_attention_heads: Optional[int] = 8,\n+        num_key_value_heads: Optional[int] = 2,\n+        hidden_act: Optional[int] = \"silu\",\n+        max_position_embeddings: Optional[int] = 33,\n+        initializer_range: Optional[float] = 0.02,\n+        rms_norm_eps: Optional[int] = 1e-5,\n+        use_cache: Optional[bool] = True,\n+        pad_token_id: Optional[int] = None,\n+        bos_token_id: Optional[int] = None,\n+        eos_token_id: Optional[int] = None,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        attention_bias: Optional[bool] = False,\n+        attention_dropout: Optional[float] = 0.0,\n+        mlp_bias: Optional[bool] = False,\n+        head_dim: Optional[int] = None,\n         **kwargs,\n     ):\n         if kwargs.pop(\"tie_word_embeddings\", False):\n@@ -191,16 +157,17 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.rms_norm_eps = rms_norm_eps\n         self.use_cache = use_cache\n-        self.rope_theta = rope_theta\n-        self.rope_scaling = rope_scaling\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n         self.mlp_bias = mlp_bias\n         self.head_dim = head_dim if head_dim is not None else self.hidden_size // self.num_attention_heads\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n+\n         # Validate the correctness of rotary position embeddings parameters\n-        # BC: if there is a 'type' field, copy it it to 'rope_type'.\n-        if self.rope_scaling is not None and \"type\" in self.rope_scaling:\n-            self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n+        rope_theta = kwargs.get(\"rope_theta\", 500000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n         rope_config_validation(self)\n \n \n@@ -262,45 +229,10 @@ class CsmConfig(PreTrainedConfig):\n             Audio token id in the text input.\n         audio_eos_token_id (`int`, *optional*, defaults to 128003):\n             End of stream token id for audio in the text input.\n-        rope_theta (`float`, *optional*, defaults to 500000):\n-            The base period of the RoPE embeddings.\n-        rope_scaling (`Dict`, *optional*, defaults to `{'factor': 32.0, 'high_freq_factor': 0.5, 'low_freq_factor': 0.125, 'original_max_position_embeddings': 1024, 'rope_type': 'llama3'}`):\n-            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n-            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n-            accordingly.\n-            Expected contents:\n-                `rope_type` (`str`):\n-                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n-                    'llama3'], with 'default' being the original RoPE implementation.\n-                `factor` (`float`, *optional*):\n-                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n-                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n-                    original maximum pre-trained length.\n-                `original_max_position_embeddings` (`int`, *optional*):\n-                    Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n-                    pretraining.\n-                `attention_factor` (`float`, *optional*):\n-                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n-                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n-                    `factor` field to infer the suggested value.\n-                `beta_fast` (`float`, *optional*):\n-                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n-                    ramp function. If unspecified, it defaults to 32.\n-                `beta_slow` (`float`, *optional*):\n-                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n-                    ramp function. If unspecified, it defaults to 1.\n-                `short_factor` (`list[float]`, *optional*):\n-                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n-                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n-                    size divided by the number of attention heads divided by 2\n-                `long_factor` (`list[float]`, *optional*):\n-                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n-                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n-                    size divided by the number of attention heads divided by 2\n-                `low_freq_factor` (`float`, *optional*):\n-                    Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n-                `high_freq_factor` (`float`, *optional*):\n-                    Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         attention_bias (`bool`, *optional*, defaults to `False`):\n             Whether to use a bias in the query, key, value and output projection layers during self-attention.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n@@ -339,35 +271,34 @@ class CsmConfig(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        num_codebooks=32,\n-        vocab_size=2051,\n-        text_vocab_size=128256,\n-        hidden_size=2048,\n-        intermediate_size=8192,\n-        num_hidden_layers=16,\n-        num_attention_heads=32,\n-        num_key_value_heads=8,\n-        hidden_act=\"silu\",\n-        max_position_embeddings=2048,\n-        initializer_range=0.02,\n-        rms_norm_eps=1e-5,\n-        use_cache=True,\n-        pad_token_id=128002,\n-        codebook_pad_token_id=2050,\n-        codebook_eos_token_id=0,\n-        bos_token_id=128000,\n-        eos_token_id=None,\n-        audio_token_id=128002,\n-        audio_eos_token_id=128003,\n-        rope_theta=500000,\n-        rope_scaling=None,\n-        attention_bias=False,\n-        attention_dropout=0.0,\n-        mlp_bias=False,\n-        head_dim=None,\n-        tie_codebooks_embeddings=True,\n-        depth_decoder_config=None,\n-        codec_config=None,\n+        num_codebooks: Optional[int] = 32,\n+        vocab_size: Optional[int] = 2051,\n+        text_vocab_size: Optional[int] = 128256,\n+        hidden_size: Optional[int] = 2048,\n+        intermediate_size: Optional[int] = 8192,\n+        num_hidden_layers: Optional[int] = 16,\n+        num_attention_heads: Optional[int] = 32,\n+        num_key_value_heads: Optional[int] = 8,\n+        hidden_act: Optional[str] = \"silu\",\n+        max_position_embeddings: Optional[int] = 2048,\n+        initializer_range: Optional[float] = 0.02,\n+        rms_norm_eps: Optional[int] = 1e-5,\n+        use_cache: Optional[bool] = True,\n+        pad_token_id: Optional[int] = 128002,\n+        codebook_pad_token_id: Optional[int] = 2050,\n+        codebook_eos_token_id: Optional[int] = 0,\n+        bos_token_id: Optional[int] = 128000,\n+        eos_token_id: Optional[int] = None,\n+        audio_token_id: Optional[int] = 128002,\n+        audio_eos_token_id: Optional[int] = 128003,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        attention_bias: Optional[bool] = False,\n+        attention_dropout: Optional[float] = 0.0,\n+        mlp_bias: Optional[bool] = False,\n+        head_dim: Optional[int] = None,\n+        tie_codebooks_embeddings: Optional[bool] = True,\n+        depth_decoder_config: Optional[dict] = None,\n+        codec_config: Optional[dict] = None,\n         **kwargs,\n     ):\n         if kwargs.pop(\"tie_word_embeddings\", False):\n@@ -413,16 +344,17 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.rms_norm_eps = rms_norm_eps\n         self.use_cache = use_cache\n-        self.rope_theta = rope_theta\n-        self.rope_scaling = rope_scaling\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n         self.mlp_bias = mlp_bias\n         self.head_dim = head_dim if head_dim is not None else self.hidden_size // self.num_attention_heads\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n+\n         # Validate the correctness of rotary position embeddings parameters\n-        # BC: if there is a 'type' field, copy it it to 'rope_type'.\n-        if self.rope_scaling is not None and \"type\" in self.rope_scaling:\n-            self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n+        rope_theta = kwargs.get(\"rope_theta\", 500000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n         rope_config_validation(self)\n \n         super().__init__("
        },
        {
            "sha": "ddbe154af70b39421ecc8e9586b5cee0872f8770",
            "filename": "src/transformers/models/csm/convert_csm.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fcsm%2Fconvert_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fcsm%2Fconvert_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fconvert_csm.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -92,7 +92,7 @@ def write_model(\n     # prepare rope scaling args: the model uses originally\n     # 1 - for the depth decoder\n     # rope_theta=500000,\n-    # rope_scaling={\n+    # rope_parameters={\n     # \t\"factor\": 32.0,\n     # \t\"high_freq_factor\": 4.0,\n     # \t\"low_freq_factor\": 1.0,\n@@ -101,7 +101,7 @@ def write_model(\n     # },\n     # 2 - for the backbone\n     # rope_theta=500000,\n-    # rope_scaling={\n+    # rope_parameters={\n     # \t\"factor\": 32.0,\n     # \t\"high_freq_factor\": 4.0,\n     # \t\"low_freq_factor\": 1.0,\n@@ -114,7 +114,7 @@ def write_model(\n     # Therefore, we convert values to equivalent ones\n \n     depth_decoder_config = CsmDepthDecoderConfig(\n-        rope_scaling={\n+        rope_parameters={\n             \"factor\": 32.0,\n             \"high_freq_factor\": 0.0078125,\n             \"low_freq_factor\": 0.001953125,\n@@ -126,7 +126,7 @@ def write_model(\n     config = CsmConfig(\n         codec_config=codec_model.config,\n         depth_decoder_config=depth_decoder_config,\n-        rope_scaling={\n+        rope_parameters={\n             \"factor\": 32.0,\n             \"high_freq_factor\": 0.5,\n             \"low_freq_factor\": 0.125,"
        },
        {
            "sha": "7d3f87b2953df30b295bef232b856e2ed90f7d47",
            "filename": "src/transformers/models/csm/modeling_csm.py",
            "status": "modified",
            "additions": 43,
            "deletions": 14,
            "changes": 57,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -121,20 +121,49 @@ class CsmRotaryEmbedding(nn.Module):\n \n     def __init__(self, config: CsmConfig, device=None):\n         super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n         self.max_seq_len_cached = config.max_position_embeddings\n         self.original_max_seq_len = config.max_position_embeddings\n \n         self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.rope_type = self.config.rope_parameters[\"rope_type\"]\n+        rope_init_fn: Callable = self.compute_default_rope_parameters\n+        if self.rope_type != \"default\":\n+            rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+        inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n+\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n+        self.original_inv_freq = inv_freq\n+\n+    @staticmethod\n+    def compute_default_rope_parameters(\n+        config: Optional[CsmConfig] = None,\n+        device: Optional[\"torch.device\"] = None,\n+        seq_len: Optional[int] = None,\n+    ) -> tuple[\"torch.Tensor\", float]:\n+        \"\"\"\n+        Computes the inverse frequencies according to the original RoPE implementation\n+        Args:\n+            config ([`~transformers.PreTrainedConfig`]):\n+                The model configuration.\n+            device (`torch.device`):\n+                The device to use for initialization of the inverse frequencies.\n+            seq_len (`int`, *optional*):\n+                The current sequence length. Unused for this type of RoPE.\n+        Returns:\n+            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n+            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n+        \"\"\"\n+        base = config.rope_parameters[\"rope_theta\"]\n+        dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+\n+        attention_factor = 1.0  # Unused in this type of RoPE\n+\n+        # Compute the inverse frequencies\n+        inv_freq = 1.0 / (\n+            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n+        )\n+        return inv_freq, attention_factor\n \n     @torch.no_grad()\n     @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n@@ -269,8 +298,8 @@ def __init__(self, config: CsmConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n-        attention_mask: Optional[torch.Tensor],\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -329,7 +358,7 @@ def forward(\n         past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> torch.Tensor:\n         residual = hidden_states\n@@ -474,7 +503,7 @@ def forward(\n \n         # create position embeddings to be shared across the decoder layers\n         position_ids = cache_position.unsqueeze(0)\n-        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids=position_ids)\n \n         for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n             hidden_states = decoder_layer(\n@@ -714,16 +743,16 @@ def forward(\n         )\n \n         hidden_states = inputs_embeds\n-        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids=position_ids)\n \n         for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n             hidden_states = decoder_layer(\n                 hidden_states,\n                 attention_mask=causal_mask,\n+                position_embeddings=position_embeddings,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n                 cache_position=cache_position,\n-                position_embeddings=position_embeddings,\n                 **kwargs,\n             )\n "
        },
        {
            "sha": "9ecc7017d83f3734e311c542e71ee2c5a40f75be",
            "filename": "src/transformers/models/csm/modular_csm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -223,7 +223,7 @@ def forward(\n \n         # create position embeddings to be shared across the decoder layers\n         position_ids = cache_position.unsqueeze(0)\n-        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids=position_ids)\n \n         for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n             hidden_states = decoder_layer("
        },
        {
            "sha": "765f7f713247fd1651df6e208c51c8959b93110a",
            "filename": "src/transformers/models/cwm/configuration_cwm.py",
            "status": "modified",
            "additions": 18,
            "deletions": 17,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fcwm%2Fconfiguration_cwm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fcwm%2Fconfiguration_cwm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcwm%2Fconfiguration_cwm.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -22,7 +22,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig, layer_type_validation\n-from ...modeling_rope_utils import rope_config_validation\n+from ...modeling_rope_utils import rope_config_validation, standardize_rope_params\n \n \n class CwmConfig(PreTrainedConfig):\n@@ -71,8 +71,6 @@ class CwmConfig(PreTrainedConfig):\n             The id of the *beginning-of-sequence* token.\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie weight embeddings\n-        rope_theta (`float`, *optional*, defaults to 1000000.0):\n-            The base period of the RoPE embeddings.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n             The dropout ratio for the attention probabilities.\n         pretraining_tp (`int`, *optional*, defaults to 1):\n@@ -81,8 +79,10 @@ class CwmConfig(PreTrainedConfig):\n             issue](https://github.com/pytorch/pytorch/issues/76232).\n         mlp_bias (`bool`, *optional*, defaults to `False`):\n             Whether to use a bias in up_proj, down_proj and gate_proj layers in the MLP layers.\n-        rope_scaling (`Dict`, *optional*):\n-            Dictionary containing the scaling configuration for the RoPE embeddings\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         sliding_window (`int`, *optional*, defaults to 8192):\n             Sliding window attention window size.\n         layer_types (`List[str]`, *optional*):\n@@ -126,18 +126,18 @@ def __init__(\n         eos_token_id=[128001, 128008, 128009],\n         bos_token_id: int = 128000,\n         tie_word_embeddings: bool = False,\n-        rope_theta: float = 1_000_000.0,\n         attention_dropout: float = 0.0,\n         pretraining_tp: int = 1,\n         mlp_bias: bool = False,\n-        rope_scaling: Optional[dict] = None,\n+        rope_parameters: Optional[dict] = None,\n         # CWM interleaved sliding window fields\n         sliding_window: int = 8192,\n         layer_types: Optional[list[str]] = None,  # [\"full_attention\"|\"sliding_attention\"] per layer\n         **kwargs,\n     ):\n-        if rope_scaling is None:\n-            rope_scaling = {\n+        if rope_parameters is None:\n+            rope_parameters = {\n+                \"rope_theta\": 1_000_000.0,\n                 \"factor\": 16.0,\n                 \"high_freq_factor\": 4.0,\n                 \"low_freq_factor\": 1.0,\n@@ -154,6 +154,9 @@ def __init__(\n             ]\n         else:\n             layer_type_validation(layer_types, num_hidden_layers)\n+\n+        self.sliding_window = int(sliding_window) if sliding_window else None\n+        self.layer_types = list(layer_types)\n         self.vocab_size = vocab_size\n         self.max_position_embeddings = max_position_embeddings\n         self.hidden_size = hidden_size\n@@ -171,15 +174,16 @@ def __init__(\n         self.rms_norm_eps = rms_norm_eps\n         self.pretraining_tp = pretraining_tp\n         self.use_cache = use_cache\n-        self.rope_theta = rope_theta\n-        self.rope_scaling = rope_scaling\n         self.attention_dropout = attention_dropout\n         self.mlp_bias = mlp_bias\n         self.head_dim = head_dim if head_dim is not None else self.hidden_size // self.num_attention_heads\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n+\n         # Validate the correctness of rotary position embeddings parameters\n-        # BC: if there is a 'type' field, copy it it to 'rope_type'.\n-        if self.rope_scaling is not None and \"type\" in self.rope_scaling:\n-            self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n+        rope_theta = kwargs.get(\"rope_theta\", 1_000_000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n         rope_config_validation(self)\n \n         super().__init__(\n@@ -190,8 +194,5 @@ def __init__(\n             **kwargs,\n         )\n \n-        self.sliding_window = int(sliding_window) if sliding_window else None\n-        self.layer_types = list(layer_types)\n-\n \n __all__ = [\"CwmConfig\"]"
        },
        {
            "sha": "cf4d996b0c49b5b81393a5859bf88b68ab93173c",
            "filename": "src/transformers/models/cwm/modeling_cwm.py",
            "status": "modified",
            "additions": 68,
            "deletions": 38,
            "changes": 106,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fcwm%2Fmodeling_cwm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fcwm%2Fmodeling_cwm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcwm%2Fmodeling_cwm.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -41,6 +41,71 @@\n from .configuration_cwm import CwmConfig\n \n \n+class CwmRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n+    def __init__(self, config: CwmConfig, device=None):\n+        super().__init__()\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+\n+        self.rope_type = self.config.rope_parameters[\"rope_type\"]\n+        rope_init_fn: Callable = self.compute_default_rope_parameters\n+        if self.rope_type != \"default\":\n+            rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+        inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n+\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = inv_freq\n+\n+    @staticmethod\n+    def compute_default_rope_parameters(\n+        config: Optional[CwmConfig] = None,\n+        device: Optional[\"torch.device\"] = None,\n+        seq_len: Optional[int] = None,\n+    ) -> tuple[\"torch.Tensor\", float]:\n+        \"\"\"\n+        Computes the inverse frequencies according to the original RoPE implementation\n+        Args:\n+            config ([`~transformers.PreTrainedConfig`]):\n+                The model configuration.\n+            device (`torch.device`):\n+                The device to use for initialization of the inverse frequencies.\n+            seq_len (`int`, *optional*):\n+                The current sequence length. Unused for this type of RoPE.\n+        Returns:\n+            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n+            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n+        \"\"\"\n+        base = config.rope_parameters[\"rope_theta\"]\n+        dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+\n+        attention_factor = 1.0  # Unused in this type of RoPE\n+\n+        # Compute the inverse frequencies\n+        inv_freq = 1.0 / (\n+            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n+        )\n+        return inv_freq, attention_factor\n+\n+    @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n+    def forward(self, x, position_ids):\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n def rotate_half(x):\n     \"\"\"Rotates half the hidden dims of the input.\"\"\"\n     x1 = x[..., : x.shape[-1] // 2]\n@@ -118,6 +183,7 @@ class CwmAttention(nn.Module):\n \n     def __init__(self, config: CwmConfig, layer_idx: int):\n         super().__init__()\n+        self.layer_type = config.layer_types[layer_idx] if hasattr(config, \"layer_types\") else None\n         self.config = config\n         self.layer_idx = layer_idx\n         self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n@@ -129,7 +195,7 @@ def __init__(self, config: CwmConfig, layer_idx: int):\n         self.k_proj = torch.nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n         self.v_proj = torch.nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n-        self.sliding_window = config.sliding_window if config.layer_types[layer_idx] == \"sliding_attention\" else None\n+        self.sliding_window = config.sliding_window if self.layer_type == \"sliding_attention\" else None\n \n     def forward(\n         self,\n@@ -232,7 +298,7 @@ def forward(\n         past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> torch.Tensor:\n         residual = hidden_states\n@@ -281,42 +347,6 @@ class CwmModelOutputWithPast(BaseModelOutputWithPast):\n     pass\n \n \n-class CwmRotaryEmbedding(nn.Module):\n-    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n-\n-    def __init__(self, config: CwmConfig, device=None):\n-        super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n-        self.max_seq_len_cached = config.max_position_embeddings\n-        self.original_max_seq_len = config.max_position_embeddings\n-\n-        self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n-\n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n-        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n-\n-    @torch.no_grad()\n-    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n-    def forward(self, x, position_ids):\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n-        position_ids_expanded = position_ids[:, None, :].float()\n-\n-        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n-            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos() * self.attention_scaling\n-            sin = emb.sin() * self.attention_scaling\n-\n-        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n-\n-\n @auto_docstring\n class CwmModel(CwmPreTrainedModel):\n     config_class = CwmConfig"
        },
        {
            "sha": "df2a003438a8d0a734518029d872ff9f4ec2ecee",
            "filename": "src/transformers/models/cwm/modular_cwm.py",
            "status": "modified",
            "additions": 21,
            "deletions": 13,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fcwm%2Fmodular_cwm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fcwm%2Fmodular_cwm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcwm%2Fmodular_cwm.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -21,6 +21,7 @@\n from ...configuration_utils import layer_type_validation\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_outputs import BaseModelOutputWithPast\n+from ...modeling_rope_utils import standardize_rope_params\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, logging\n from ..llama.configuration_llama import LlamaConfig\n@@ -30,7 +31,7 @@\n     LlamaModel,\n     LlamaPreTrainedModel,\n )\n-from ..qwen2.modeling_qwen2 import Qwen2Attention\n+from ..qwen2.modeling_qwen2 import Qwen2Attention, Qwen2RotaryEmbedding\n \n \n logger = logging.get_logger(__name__)\n@@ -82,8 +83,6 @@ class CwmConfig(LlamaConfig):\n             The id of the *beginning-of-sequence* token.\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie weight embeddings\n-        rope_theta (`float`, *optional*, defaults to 1000000.0):\n-            The base period of the RoPE embeddings.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n             The dropout ratio for the attention probabilities.\n         pretraining_tp (`int`, *optional*, defaults to 1):\n@@ -92,8 +91,10 @@ class CwmConfig(LlamaConfig):\n             issue](https://github.com/pytorch/pytorch/issues/76232).\n         mlp_bias (`bool`, *optional*, defaults to `False`):\n             Whether to use a bias in up_proj, down_proj and gate_proj layers in the MLP layers.\n-        rope_scaling (`Dict`, *optional*):\n-            Dictionary containing the scaling configuration for the RoPE embeddings\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         sliding_window (`int`, *optional*, defaults to 8192):\n             Sliding window attention window size.\n         layer_types (`List[str]`, *optional*):\n@@ -121,18 +122,18 @@ def __init__(\n         eos_token_id=[128001, 128008, 128009],\n         bos_token_id: int = 128000,\n         tie_word_embeddings: bool = False,\n-        rope_theta: float = 1_000_000.0,\n         attention_dropout: float = 0.0,\n         pretraining_tp: int = 1,\n         mlp_bias: bool = False,\n-        rope_scaling: Optional[dict] = None,\n+        rope_parameters: Optional[dict] = None,\n         # CWM interleaved sliding window fields\n         sliding_window: int = 8192,\n         layer_types: Optional[list[str]] = None,  # [\"full_attention\"|\"sliding_attention\"] per layer\n         **kwargs,\n     ):\n-        if rope_scaling is None:\n-            rope_scaling = {\n+        if rope_parameters is None:\n+            rope_parameters = {\n+                \"rope_theta\": 1_000_000.0,\n                 \"factor\": 16.0,\n                 \"high_freq_factor\": 4.0,\n                 \"low_freq_factor\": 1.0,\n@@ -150,6 +151,9 @@ def __init__(\n         else:\n             layer_type_validation(layer_types, num_hidden_layers)\n \n+        self.sliding_window = int(sliding_window) if sliding_window else None\n+        self.layer_types = list(layer_types)\n+\n         super().__init__(\n             vocab_size=vocab_size,\n             hidden_size=hidden_size,\n@@ -167,10 +171,9 @@ def __init__(\n             eos_token_id=list(eos_token_id),\n             bos_token_id=bos_token_id,\n             tie_word_embeddings=tie_word_embeddings,\n-            rope_theta=rope_theta,\n             attention_bias=False,\n             attention_dropout=attention_dropout,\n-            rope_scaling=rope_scaling,\n+            rope_parameters=rope_parameters,\n             pretraining_tp=pretraining_tp,\n             mlp_bias=mlp_bias,\n             **kwargs,\n@@ -179,8 +182,13 @@ def __init__(\n         # CWM models don't use attention bias, remove it from config\n         del self.attention_bias\n \n-        self.sliding_window = int(sliding_window) if sliding_window else None\n-        self.layer_types = list(layer_types)\n+        # Validate the correctness of rotary position embeddings parameters\n+        rope_theta = kwargs.get(\"rope_theta\", 1_000_000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n+\n+\n+class CwmRotaryEmbedding(Qwen2RotaryEmbedding):\n+    pass\n \n \n class CwmAttention(Qwen2Attention):"
        },
        {
            "sha": "987cb8a8ac06bd434b54d4efed4e557a552b1f7a",
            "filename": "src/transformers/models/dbrx/configuration_dbrx.py",
            "status": "modified",
            "additions": 22,
            "deletions": 13,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fdbrx%2Fconfiguration_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fdbrx%2Fconfiguration_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2Fconfiguration_dbrx.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -17,6 +17,7 @@\n from typing import Any, Optional\n \n from ...configuration_utils import PreTrainedConfig\n+from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n from ...utils import logging\n \n \n@@ -37,8 +38,8 @@ class DbrxAttentionConfig(PreTrainedConfig):\n             The dropout probability for the attention layers.\n         clip_qkv (`float`, *optional*):\n             If set, clip the queries, keys, and values in the attention layer to this value.\n-        kv_n_heads (`int`, *optional*, defaults to 1): For grouped_query_attention only, allow user to specify number of kv heads.\n-        rope_theta (`float`, *optional*, defaults to 10000.0): The base frequency for rope.\n+        kv_n_heads (`int`, *optional*, defaults to 1):\n+            For grouped_query_attention only, allow user to specify number of kv heads.\n     \"\"\"\n \n     base_config_key = \"attn_config\"\n@@ -176,18 +177,19 @@ class DbrxConfig(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        d_model: int = 2048,\n-        n_heads: int = 16,\n-        n_layers: int = 24,\n-        max_seq_len: int = 2048,\n-        vocab_size: int = 32000,\n-        resid_pdrop: float = 0.0,\n-        emb_pdrop: float = 0.0,\n+        d_model: Optional[int] = 2048,\n+        n_heads: Optional[int] = 16,\n+        n_layers: Optional[int] = 24,\n+        max_seq_len: Optional[int] = 2048,\n+        vocab_size: Optional[int] = 32000,\n+        resid_pdrop: Optional[float] = 0.0,\n+        emb_pdrop: Optional[float] = 0.0,\n         attn_config: Optional[DbrxAttentionConfig] = None,\n         ffn_config: Optional[DbrxFFNConfig] = None,\n-        use_cache: bool = True,\n-        initializer_range: float = 0.02,\n-        output_router_logits: bool = False,\n+        use_cache: Optional[bool] = True,\n+        initializer_range: Optional[float] = 0.02,\n+        output_router_logits: Optional[bool] = False,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n         **kwargs: Any,\n     ):\n         if attn_config is None:\n@@ -215,11 +217,18 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.output_router_logits = output_router_logits\n         self.num_key_value_heads = self.attn_config.kv_n_heads\n-        self.rope_theta: float = 10000.0\n         tie_word_embeddings = kwargs.pop(\"tie_word_embeddings\", False)\n         if tie_word_embeddings:\n             raise ValueError(\"tie_word_embeddings is not supported for DBRX models.\")\n \n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n+\n+        # Validate the correctness of rotary position embeddings parameters\n+        standardize_rope_params(self, rope_theta=10000.0)\n+        rope_config_validation(self)\n+\n         super().__init__(tie_word_embeddings=tie_word_embeddings, **kwargs)\n \n "
        },
        {
            "sha": "a3f995d35b95f51f1a0ed118d3cd0ee2deb14858",
            "filename": "src/transformers/models/dbrx/modeling_dbrx.py",
            "status": "modified",
            "additions": 37,
            "deletions": 8,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -44,20 +44,49 @@ class DbrxRotaryEmbedding(nn.Module):\n \n     def __init__(self, config: DbrxConfig, device=None):\n         super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n         self.max_seq_len_cached = config.max_position_embeddings\n         self.original_max_seq_len = config.max_position_embeddings\n \n         self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.rope_type = self.config.rope_parameters[\"rope_type\"]\n+        rope_init_fn: Callable = self.compute_default_rope_parameters\n+        if self.rope_type != \"default\":\n+            rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+        inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n+\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n+        self.original_inv_freq = inv_freq\n+\n+    @staticmethod\n+    def compute_default_rope_parameters(\n+        config: Optional[DbrxConfig] = None,\n+        device: Optional[\"torch.device\"] = None,\n+        seq_len: Optional[int] = None,\n+    ) -> tuple[\"torch.Tensor\", float]:\n+        \"\"\"\n+        Computes the inverse frequencies according to the original RoPE implementation\n+        Args:\n+            config ([`~transformers.PreTrainedConfig`]):\n+                The model configuration.\n+            device (`torch.device`):\n+                The device to use for initialization of the inverse frequencies.\n+            seq_len (`int`, *optional*):\n+                The current sequence length. Unused for this type of RoPE.\n+        Returns:\n+            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n+            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n+        \"\"\"\n+        base = config.rope_parameters[\"rope_theta\"]\n+        dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+\n+        attention_factor = 1.0  # Unused in this type of RoPE\n+\n+        # Compute the inverse frequencies\n+        inv_freq = 1.0 / (\n+            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n+        )\n+        return inv_freq, attention_factor\n \n     @torch.no_grad()\n     @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)"
        },
        {
            "sha": "7e5a8c93feec45c78e7cd8f7e13ce74059b0fdc2",
            "filename": "src/transformers/models/deepseek_v2/configuration_deepseek_v2.py",
            "status": "modified",
            "additions": 45,
            "deletions": 44,
            "changes": 89,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fconfiguration_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fconfiguration_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fconfiguration_deepseek_v2.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -19,9 +19,10 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n+from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import rope_config_validation\n+from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n \n \n class DeepseekV2Config(PreTrainedConfig):\n@@ -66,10 +67,10 @@ class DeepseekV2Config(PreTrainedConfig):\n             End-of-sequence token ID.\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie input and output embeddings.\n-        rope_theta (`float`, *optional*, defaults to 10000.0):\n-            The base period of the Rotary Position Embeddings (RoPE).\n-        rope_scaling (`Dict`, *optional*):\n-            Configuration for scaling RoPE embeddings. Supports `linear` and `dynamic` scaling strategies.\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         attention_bias (`bool`, *optional*, defaults to `False`):\n             Whether to use a bias in the query, key, value, and output projection layers during self-attention.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n@@ -138,40 +139,39 @@ class DeepseekV2Config(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size=32000,\n-        hidden_size=4096,\n-        intermediate_size=11008,\n-        num_hidden_layers=32,\n-        num_attention_heads=32,\n-        num_key_value_heads=None,\n-        hidden_act=\"silu\",\n-        max_position_embeddings=2048,\n-        initializer_range=0.02,\n-        rms_norm_eps=1e-6,\n-        use_cache=True,\n-        pad_token_id=None,\n-        bos_token_id=1,\n-        eos_token_id=2,\n-        tie_word_embeddings=False,\n-        rope_theta=10000.0,\n-        rope_scaling=None,\n-        attention_bias=False,\n-        attention_dropout=0.0,\n-        mlp_bias=False,\n-        first_k_dense_replace=0,\n-        kv_lora_rank=512,\n-        q_lora_rank=1536,\n-        n_group=None,\n-        n_routed_experts=64,\n-        n_shared_experts=2,\n-        qk_nope_head_dim=128,\n-        qk_rope_head_dim=64,\n-        routed_scaling_factor=1.0,\n-        topk_group=None,\n-        topk_method=\"greedy\",\n-        v_head_dim=128,\n-        num_experts_per_tok=None,\n-        moe_intermediate_size=1407,\n+        vocab_size: Optional[int] = 32000,\n+        hidden_size: Optional[int] = 4096,\n+        intermediate_size: Optional[int] = 11008,\n+        num_hidden_layers: Optional[int] = 32,\n+        num_attention_heads: Optional[int] = 32,\n+        num_key_value_heads: Optional[int] = None,\n+        hidden_act: Optional[str] = \"silu\",\n+        max_position_embeddings: Optional[int] = 2048,\n+        initializer_range: Optional[float] = 0.02,\n+        rms_norm_eps: Optional[int] = 1e-6,\n+        use_cache: Optional[bool] = True,\n+        pad_token_id: Optional[int] = None,\n+        bos_token_id: Optional[int] = 1,\n+        eos_token_id: Optional[int] = 2,\n+        tie_word_embeddings: Optional[bool] = False,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        attention_bias: Optional[bool] = False,\n+        attention_dropout: Optional[float] = 0.0,\n+        mlp_bias: Optional[bool] = False,\n+        first_k_dense_replace: Optional[int] = 0,\n+        kv_lora_rank: Optional[int] = 512,\n+        q_lora_rank: Optional[int] = 1536,\n+        n_group: Optional[int] = None,\n+        n_routed_experts: Optional[int] = 64,\n+        n_shared_experts: Optional[int] = 2,\n+        qk_nope_head_dim: Optional[int] = 128,\n+        qk_rope_head_dim: Optional[int] = 64,\n+        routed_scaling_factor: Optional[float] = 1.0,\n+        topk_group: Optional[int] = None,\n+        topk_method: Optional[str] = \"greedy\",\n+        v_head_dim: Optional[int] = 128,\n+        num_experts_per_tok: Optional[int] = None,\n+        moe_intermediate_size: Optional[int] = 1407,\n         **kwargs,\n     ):\n         self.first_k_dense_replace = first_k_dense_replace\n@@ -204,17 +204,18 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.rms_norm_eps = rms_norm_eps\n         self.use_cache = use_cache\n-        self.rope_theta = rope_theta\n-        self.rope_scaling = rope_scaling\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n         self.mlp_bias = mlp_bias\n \n         self.head_dim = qk_rope_head_dim\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n+\n         # Validate the correctness of rotary position embeddings parameters\n-        # BC: if there is a 'type' field, copy it it to 'rope_type'.\n-        if self.rope_scaling is not None and \"type\" in self.rope_scaling:\n-            self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n+        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n         rope_config_validation(self)\n \n         super().__init__("
        },
        {
            "sha": "ae3b6c4431bf05175ec6eb28c6ecfe98470327f2",
            "filename": "src/transformers/models/deepseek_v2/modeling_deepseek_v2.py",
            "status": "modified",
            "additions": 41,
            "deletions": 14,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -166,22 +166,49 @@ class DeepseekV2RotaryEmbedding(nn.Module):\n \n     def __init__(self, config: DeepseekV2Config, device=None):\n         super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        self.rope_type = (\n-            config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-            if config.rope_scaling is not None\n-            else \"default\"\n-        )\n-\n         self.max_seq_len_cached = config.max_position_embeddings\n         self.original_max_seq_len = config.max_position_embeddings\n \n         self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.rope_type = self.config.rope_parameters[\"rope_type\"]\n+        rope_init_fn: Callable = self.compute_default_rope_parameters\n+        if self.rope_type != \"default\":\n+            rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+        inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n+\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n+        self.original_inv_freq = inv_freq\n+\n+    @staticmethod\n+    def compute_default_rope_parameters(\n+        config: Optional[DeepseekV2Config] = None,\n+        device: Optional[\"torch.device\"] = None,\n+        seq_len: Optional[int] = None,\n+    ) -> tuple[\"torch.Tensor\", float]:\n+        \"\"\"\n+        Computes the inverse frequencies according to the original RoPE implementation\n+        Args:\n+            config ([`~transformers.PreTrainedConfig`]):\n+                The model configuration.\n+            device (`torch.device`):\n+                The device to use for initialization of the inverse frequencies.\n+            seq_len (`int`, *optional*):\n+                The current sequence length. Unused for this type of RoPE.\n+        Returns:\n+            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n+            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n+        \"\"\"\n+        base = config.rope_parameters[\"rope_theta\"]\n+        dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+\n+        attention_factor = 1.0  # Unused in this type of RoPE\n+\n+        # Compute the inverse frequencies\n+        inv_freq = 1.0 / (\n+            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n+        )\n+        return inv_freq, attention_factor\n \n     @torch.no_grad()\n     @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n@@ -264,7 +291,7 @@ def __init__(self, config: DeepseekV2Config, layer_idx: Optional[int] = None):\n         self.num_heads = config.num_attention_heads\n         self.head_dim = config.head_dim\n         self.max_position_embeddings = config.max_position_embeddings\n-        self.rope_theta = config.rope_theta\n+\n         self.q_lora_rank = config.q_lora_rank\n         self.qk_rope_head_dim = config.qk_rope_head_dim\n         self.kv_lora_rank = config.kv_lora_rank\n@@ -389,7 +416,7 @@ def forward(\n         past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> torch.Tensor:\n         residual = hidden_states\n@@ -497,16 +524,16 @@ def forward(\n         )\n \n         hidden_states = inputs_embeds\n-        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids=position_ids)\n \n         for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n             hidden_states = decoder_layer(\n                 hidden_states,\n                 attention_mask=causal_mask,\n+                position_embeddings=position_embeddings,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n                 cache_position=cache_position,\n-                position_embeddings=position_embeddings,\n                 **kwargs,\n             )\n "
        },
        {
            "sha": "0a5e1a8b4f065976d52ad27edebce6244572967a",
            "filename": "src/transformers/models/deepseek_v2/modular_deepseek_v2.py",
            "status": "modified",
            "additions": 55,
            "deletions": 52,
            "changes": 107,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodular_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodular_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodular_deepseek_v2.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -22,10 +22,9 @@\n from torch import nn\n \n from ...cache_utils import Cache\n+from ...modeling_rope_utils import RopeParameters, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n-from ...utils import (\n-    logging,\n-)\n+from ...utils import logging\n from ..llama.configuration_llama import LlamaConfig\n from ..llama.modeling_llama import (\n     LlamaDecoderLayer,\n@@ -35,9 +34,9 @@\n     LlamaModel,\n     LlamaPreTrainedModel,\n     LlamaRMSNorm,\n+    LlamaRotaryEmbedding,\n     eager_attention_forward,\n )\n-from ..llama4.modeling_llama4 import Llama4TextRotaryEmbedding\n from ..qwen2_moe.modeling_qwen2_moe import Qwen2MoeExperts\n \n \n@@ -86,10 +85,10 @@ class DeepseekV2Config(LlamaConfig):\n             End-of-sequence token ID.\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie input and output embeddings.\n-        rope_theta (`float`, *optional*, defaults to 10000.0):\n-            The base period of the Rotary Position Embeddings (RoPE).\n-        rope_scaling (`Dict`, *optional*):\n-            Configuration for scaling RoPE embeddings. Supports `linear` and `dynamic` scaling strategies.\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         attention_bias (`bool`, *optional*, defaults to `False`):\n             Whether to use a bias in the query, key, value, and output projection layers during self-attention.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n@@ -153,40 +152,39 @@ class DeepseekV2Config(LlamaConfig):\n \n     def __init__(\n         self,\n-        vocab_size=32000,\n-        hidden_size=4096,\n-        intermediate_size=11008,\n-        num_hidden_layers=32,\n-        num_attention_heads=32,\n-        num_key_value_heads=None,\n-        hidden_act=\"silu\",\n-        max_position_embeddings=2048,\n-        initializer_range=0.02,\n-        rms_norm_eps=1e-6,\n-        use_cache=True,\n-        pad_token_id=None,\n-        bos_token_id=1,\n-        eos_token_id=2,\n-        tie_word_embeddings=False,\n-        rope_theta=10000.0,\n-        rope_scaling=None,\n-        attention_bias=False,\n-        attention_dropout=0.0,\n-        mlp_bias=False,\n-        first_k_dense_replace=0,\n-        kv_lora_rank=512,\n-        q_lora_rank=1536,\n-        n_group=None,\n-        n_routed_experts=64,\n-        n_shared_experts=2,\n-        qk_nope_head_dim=128,\n-        qk_rope_head_dim=64,\n-        routed_scaling_factor=1.0,\n-        topk_group=None,\n-        topk_method=\"greedy\",\n-        v_head_dim=128,\n-        num_experts_per_tok=None,\n-        moe_intermediate_size=1407,\n+        vocab_size: Optional[int] = 32000,\n+        hidden_size: Optional[int] = 4096,\n+        intermediate_size: Optional[int] = 11008,\n+        num_hidden_layers: Optional[int] = 32,\n+        num_attention_heads: Optional[int] = 32,\n+        num_key_value_heads: Optional[int] = None,\n+        hidden_act: Optional[str] = \"silu\",\n+        max_position_embeddings: Optional[int] = 2048,\n+        initializer_range: Optional[float] = 0.02,\n+        rms_norm_eps: Optional[int] = 1e-6,\n+        use_cache: Optional[bool] = True,\n+        pad_token_id: Optional[int] = None,\n+        bos_token_id: Optional[int] = 1,\n+        eos_token_id: Optional[int] = 2,\n+        tie_word_embeddings: Optional[bool] = False,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        attention_bias: Optional[bool] = False,\n+        attention_dropout: Optional[float] = 0.0,\n+        mlp_bias: Optional[bool] = False,\n+        first_k_dense_replace: Optional[int] = 0,\n+        kv_lora_rank: Optional[int] = 512,\n+        q_lora_rank: Optional[int] = 1536,\n+        n_group: Optional[int] = None,\n+        n_routed_experts: Optional[int] = 64,\n+        n_shared_experts: Optional[int] = 2,\n+        qk_nope_head_dim: Optional[int] = 128,\n+        qk_rope_head_dim: Optional[int] = 64,\n+        routed_scaling_factor: Optional[float] = 1.0,\n+        topk_group: Optional[int] = None,\n+        topk_method: Optional[str] = \"greedy\",\n+        v_head_dim: Optional[int] = 128,\n+        num_experts_per_tok: Optional[int] = None,\n+        moe_intermediate_size: Optional[int] = 1407,\n         **kwargs,\n     ):\n         self.first_k_dense_replace = first_k_dense_replace\n@@ -293,15 +291,20 @@ class DeepseekV2RMSNorm(LlamaRMSNorm):\n     pass\n \n \n-class DeepseekV2RotaryEmbedding(Llama4TextRotaryEmbedding):\n-    def __init__(self, config: DeepseekV2Config, device=None):\n-        super().__init__(config=config, device=device)\n-        # BC: \"rope_type\" was originally \"type\"\n-        self.rope_type = (\n-            config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-            if config.rope_scaling is not None\n-            else \"default\"\n-        )\n+class DeepseekV2RotaryEmbedding(LlamaRotaryEmbedding):\n+    @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n+    def forward(self, x, position_ids):\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.to(x.device) @ position_ids_expanded).transpose(1, 2)\n+            freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # Convert to complex representation\n+            freqs_cis = freqs_cis * self.attention_scaling\n+\n+        return freqs_cis\n \n \n class DeepseekV2Attention(nn.Module):\n@@ -316,7 +319,7 @@ def __init__(self, config: DeepseekV2Config, layer_idx: Optional[int] = None):\n         self.num_heads = config.num_attention_heads\n         self.head_dim = config.head_dim\n         self.max_position_embeddings = config.max_position_embeddings\n-        self.rope_theta = config.rope_theta\n+\n         self.q_lora_rank = config.q_lora_rank\n         self.qk_rope_head_dim = config.qk_rope_head_dim\n         self.kv_lora_rank = config.kv_lora_rank"
        },
        {
            "sha": "eed1ea34def46b96df8dbfb5a0f2ddedeb5529ce",
            "filename": "src/transformers/models/deepseek_v3/configuration_deepseek_v3.py",
            "status": "modified",
            "additions": 50,
            "deletions": 52,
            "changes": 102,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fconfiguration_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fconfiguration_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fconfiguration_deepseek_v3.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -16,8 +16,10 @@\n # limitations under the License.\n \"\"\"DeepSeekV3 model configuration\"\"\"\n \n+from typing import Optional\n+\n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import rope_config_validation\n+from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n \n \n DEEPSEEK_PRETRAINED_CONFIG_ARCHIVE_MAP = {}\n@@ -106,13 +108,10 @@ class DeepseekV3Config(PreTrainedConfig):\n             issue](https://github.com/pytorch/pytorch/issues/76232).\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie weight embeddings\n-        rope_theta (`float`, *optional*, defaults to 10000.0):\n-            The base period of the RoPE embeddings.\n-        rope_scaling (`Dict`, *optional*):\n-            Dictionary containing the scaling configuration for the RoPE embeddings. Currently supports two scaling\n-            strategies: linear and dynamic. Their scaling factor must be a float greater than 1. The expected format is\n-            `{\"type\": strategy name, \"factor\": scaling factor}`. When using this flag, don't update\n-            `max_position_embeddings` to the expected new maximum.\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         rope_interleave (`bool`, *optional*, defaults to `True`):\n             Whether to interleave the rotary position embeddings.\n         attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):\n@@ -157,41 +156,40 @@ class DeepseekV3Config(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size=129280,\n-        hidden_size=7168,\n-        intermediate_size=18432,\n-        moe_intermediate_size=2048,\n-        num_hidden_layers=61,\n-        num_attention_heads=128,\n-        num_key_value_heads=128,\n-        n_shared_experts=1,\n-        n_routed_experts=256,\n-        routed_scaling_factor=2.5,\n-        kv_lora_rank=512,\n-        q_lora_rank=1536,\n-        qk_rope_head_dim=64,\n-        v_head_dim=128,\n-        qk_nope_head_dim=128,\n-        n_group=8,\n-        topk_group=4,\n-        num_experts_per_tok=8,\n-        first_k_dense_replace=3,\n-        norm_topk_prob=True,\n-        hidden_act=\"silu\",\n-        max_position_embeddings=4096,\n-        initializer_range=0.02,\n-        rms_norm_eps=1e-6,\n-        use_cache=True,\n-        pad_token_id=None,\n-        bos_token_id=0,\n-        eos_token_id=1,\n-        pretraining_tp=1,\n-        tie_word_embeddings=False,\n-        rope_theta=10000.0,\n-        rope_scaling=None,\n-        rope_interleave=True,\n-        attention_bias=False,\n-        attention_dropout=0.0,\n+        vocab_size: Optional[int] = 129280,\n+        hidden_size: Optional[int] = 7168,\n+        intermediate_size: Optional[int] = 18432,\n+        moe_intermediate_size: Optional[int] = 2048,\n+        num_hidden_layers: Optional[int] = 61,\n+        num_attention_heads: Optional[int] = 128,\n+        num_key_value_heads: Optional[int] = 128,\n+        n_shared_experts: Optional[int] = 1,\n+        n_routed_experts: Optional[int] = 256,\n+        routed_scaling_factor: Optional[float] = 2.5,\n+        kv_lora_rank: Optional[int] = 512,\n+        q_lora_rank: Optional[int] = 1536,\n+        qk_rope_head_dim: Optional[int] = 64,\n+        v_head_dim: Optional[int] = 128,\n+        qk_nope_head_dim: Optional[int] = 128,\n+        n_group: Optional[int] = 8,\n+        topk_group: Optional[int] = 4,\n+        num_experts_per_tok: Optional[int] = 8,\n+        first_k_dense_replace: Optional[int] = 3,\n+        norm_topk_prob: Optional[bool] = True,\n+        hidden_act: Optional[str] = \"silu\",\n+        max_position_embeddings: Optional[int] = 4096,\n+        initializer_range: Optional[float] = 0.02,\n+        rms_norm_eps: Optional[int] = 1e-6,\n+        use_cache: Optional[bool] = True,\n+        pad_token_id: Optional[int] = None,\n+        bos_token_id: Optional[int] = 0,\n+        eos_token_id: Optional[int] = 1,\n+        pretraining_tp: Optional[int] = 1,\n+        tie_word_embeddings: Optional[bool] = False,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_interleave: Optional[bool] = True,\n+        attention_bias: Optional[bool] = False,\n+        attention_dropout: Optional[float] = 0.0,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size\n@@ -228,19 +226,19 @@ def __init__(\n         self.rms_norm_eps = rms_norm_eps\n         self.pretraining_tp = pretraining_tp\n         self.use_cache = use_cache\n-        self.rope_theta = rope_theta\n-        self.rope_scaling = rope_scaling\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n+\n         # Validate the correctness of rotary position embeddings parameters\n-        # BC: if there is a 'type' field, copy it it to 'rope_type'.\n-        if self.rope_scaling is not None and \"type\" in self.rope_scaling:\n-            self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n+        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n \n-        if self.rope_scaling is not None:\n-            for key in [\"beta_fast\", \"beta_slow\", \"factor\"]:\n-                if key in self.rope_scaling:\n-                    self.rope_scaling[key] = float(self.rope_scaling[key])\n+        for key in [\"beta_fast\", \"beta_slow\", \"factor\"]:\n+            if key in self.rope_parameters:\n+                self.rope_parameters[key] = float(self.rope_parameters[key])\n \n         rope_config_validation(self)\n "
        },
        {
            "sha": "cbb63c5216be74bcda23f90d221c8f8c2870c92e",
            "filename": "src/transformers/models/deepseek_v3/modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 44,
            "deletions": 15,
            "changes": 59,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -58,20 +58,49 @@ class DeepseekV3RotaryEmbedding(nn.Module):\n \n     def __init__(self, config: DeepseekV3Config, device=None):\n         super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n         self.max_seq_len_cached = config.max_position_embeddings\n         self.original_max_seq_len = config.max_position_embeddings\n \n         self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.rope_type = self.config.rope_parameters[\"rope_type\"]\n+        rope_init_fn: Callable = self.compute_default_rope_parameters\n+        if self.rope_type != \"default\":\n+            rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+        inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n+\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n+        self.original_inv_freq = inv_freq\n+\n+    @staticmethod\n+    def compute_default_rope_parameters(\n+        config: Optional[DeepseekV3Config] = None,\n+        device: Optional[\"torch.device\"] = None,\n+        seq_len: Optional[int] = None,\n+    ) -> tuple[\"torch.Tensor\", float]:\n+        \"\"\"\n+        Computes the inverse frequencies according to the original RoPE implementation\n+        Args:\n+            config ([`~transformers.PreTrainedConfig`]):\n+                The model configuration.\n+            device (`torch.device`):\n+                The device to use for initialization of the inverse frequencies.\n+            seq_len (`int`, *optional*):\n+                The current sequence length. Unused for this type of RoPE.\n+        Returns:\n+            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n+            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n+        \"\"\"\n+        base = config.rope_parameters[\"rope_theta\"]\n+        dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+\n+        attention_factor = 1.0  # Unused in this type of RoPE\n+\n+        # Compute the inverse frequencies\n+        inv_freq = 1.0 / (\n+            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n+        )\n+        return inv_freq, attention_factor\n \n     @torch.no_grad()\n     @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n@@ -336,7 +365,7 @@ def __init__(self, config: DeepseekV3Config, layer_idx: int):\n         self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n         self.attention_dropout = config.attention_dropout\n         self.num_heads = config.num_attention_heads\n-        self.rope_theta = config.rope_theta\n+\n         self.q_lora_rank = config.q_lora_rank\n         self.qk_rope_head_dim = config.qk_rope_head_dim\n         self.kv_lora_rank = config.kv_lora_rank\n@@ -371,9 +400,9 @@ def __init__(self, config: DeepseekV3Config, layer_idx: int):\n         )\n \n         self.scaling = self.qk_head_dim ** (-0.5)\n-        if self.config.rope_scaling is not None:\n-            mscale_all_dim = self.config.rope_scaling.get(\"mscale_all_dim\", 0)\n-            scaling_factor = self.config.rope_scaling[\"factor\"]\n+        if self.config.rope_parameters.get(\"rope_type\", \"default\") != \"default\":\n+            mscale_all_dim = self.config.rope_parameters.get(\"mscale_all_dim\", 0)\n+            scaling_factor = self.config.rope_parameters[\"factor\"]\n             if mscale_all_dim:\n                 mscale = yarn_get_mscale(scaling_factor, mscale_all_dim)\n                 self.scaling = self.scaling * mscale * mscale\n@@ -470,7 +499,7 @@ def forward(\n         past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> torch.Tensor:\n         residual = hidden_states\n@@ -580,16 +609,16 @@ def forward(\n         )\n \n         hidden_states = inputs_embeds\n-        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids=position_ids)\n \n         for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n             hidden_states = decoder_layer(\n                 hidden_states,\n                 attention_mask=causal_mask,\n+                position_embeddings=position_embeddings,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n                 cache_position=cache_position,\n-                position_embeddings=position_embeddings,\n                 **kwargs,\n             )\n "
        },
        {
            "sha": "3bc9d45e79e95efdabd28f35f982cba12083052a",
            "filename": "src/transformers/models/deepseek_v3/modular_deepseek_v3.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodular_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodular_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodular_deepseek_v3.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -176,7 +176,7 @@ def __init__(self, config: DeepseekV3Config, layer_idx: int):\n         self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n         self.attention_dropout = config.attention_dropout\n         self.num_heads = config.num_attention_heads\n-        self.rope_theta = config.rope_theta\n+\n         self.q_lora_rank = config.q_lora_rank\n         self.qk_rope_head_dim = config.qk_rope_head_dim\n         self.kv_lora_rank = config.kv_lora_rank\n@@ -211,9 +211,9 @@ def __init__(self, config: DeepseekV3Config, layer_idx: int):\n         )\n \n         self.scaling = self.qk_head_dim ** (-0.5)\n-        if self.config.rope_scaling is not None:\n-            mscale_all_dim = self.config.rope_scaling.get(\"mscale_all_dim\", 0)\n-            scaling_factor = self.config.rope_scaling[\"factor\"]\n+        if self.config.rope_parameters.get(\"rope_type\", \"default\") != \"default\":\n+            mscale_all_dim = self.config.rope_parameters.get(\"mscale_all_dim\", 0)\n+            scaling_factor = self.config.rope_parameters[\"factor\"]\n             if mscale_all_dim:\n                 mscale = yarn_get_mscale(scaling_factor, mscale_all_dim)\n                 self.scaling = self.scaling * mscale * mscale"
        },
        {
            "sha": "64545d7abcf6c6ba2ff920c9d13df9ab550200e8",
            "filename": "src/transformers/models/deprecated/open_llama/configuration_open_llama.py",
            "status": "modified",
            "additions": 24,
            "deletions": 22,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fconfiguration_open_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fconfiguration_open_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fconfiguration_open_llama.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -65,14 +65,10 @@ class OpenLlamaConfig(PreTrainedConfig):\n             Whether to tie weight embeddings\n         rope_theta (`float`, *optional*, defaults to 10000.0):\n             The base period of the RoPE embeddings.\n-        rope_scaling (`Dict`, *optional*):\n-            Dictionary containing the scaling configuration for the RoPE embeddings. Currently supports two scaling\n-            strategies: linear and dynamic. Their scaling factor must be a float greater than 1. The expected format is\n-            `{\"type\": strategy name, \"factor\": scaling factor}`. When using this flag, don't update\n-            `max_position_embeddings` to the expected new maximum. See the following thread for more information on how\n-            these scaling strategies behave:\n-            https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/. This is an\n-            experimental feature, subject to breaking API changes in future versions.\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n \n         Example:\n \n@@ -113,7 +109,7 @@ def __init__(\n         use_stable_embedding=True,\n         shared_input_output_embedding=True,\n         rope_theta=10000.0,\n-        rope_scaling=None,\n+        rope_parameters=None,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size\n@@ -134,8 +130,10 @@ def __init__(\n         self.use_stable_embedding = use_stable_embedding\n         self.shared_input_output_embedding = shared_input_output_embedding\n         self.rope_theta = rope_theta\n-        self.rope_scaling = rope_scaling\n-        self._rope_scaling_validation()\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n+        self._rope_parameters_validation()\n \n         super().__init__(\n             pad_token_id=pad_token_id,\n@@ -145,25 +143,29 @@ def __init__(\n             **kwargs,\n         )\n \n-    def _rope_scaling_validation(self):\n+    def _rope_parameters_validation(self):\n         \"\"\"\n-        Validate the `rope_scaling` configuration.\n+        Validate the `rope_parameters` configuration.\n         \"\"\"\n-        if self.rope_scaling is None:\n+        if self.rope_parameters is None:\n             return\n \n-        if not isinstance(self.rope_scaling, dict) or len(self.rope_scaling) != 2:\n+        if not isinstance(self.rope_parameters, dict) or len(self.rope_parameters) != 2:\n             raise ValueError(\n-                f\"`rope_scaling` must be a dictionary with two fields, `type` and `factor`, got {self.rope_scaling}\"\n+                f\"`rope_parameters` must be a dictionary with two fields, `type` and `factor`, got {self.rope_parameters}\"\n             )\n-        rope_scaling_type = self.rope_scaling.get(\"type\", None)\n-        rope_scaling_factor = self.rope_scaling.get(\"factor\", None)\n-        if rope_scaling_type is None or rope_scaling_type not in [\"linear\", \"dynamic\"]:\n+        rope_parameters_type = self.rope_parameters.get(\"type\", None)\n+        rope_parameters_factor = self.rope_parameters.get(\"factor\", None)\n+        if rope_parameters_type is None or rope_parameters_type not in [\"linear\", \"dynamic\"]:\n             raise ValueError(\n-                f\"`rope_scaling`'s type field must be one of ['linear', 'dynamic'], got {rope_scaling_type}\"\n+                f\"`rope_parameters`'s type field must be one of ['linear', 'dynamic'], got {rope_parameters_type}\"\n             )\n-        if rope_scaling_factor is None or not isinstance(rope_scaling_factor, float) or rope_scaling_factor <= 1.0:\n-            raise ValueError(f\"`rope_scaling`'s factor field must be a float > 1, got {rope_scaling_factor}\")\n+        if (\n+            rope_parameters_factor is None\n+            or not isinstance(rope_parameters_factor, float)\n+            or rope_parameters_factor <= 1.0\n+        ):\n+            raise ValueError(f\"`rope_parameters`'s factor field must be a float > 1, got {rope_parameters_factor}\")\n \n \n __all__ = [\"OpenLlamaConfig\"]"
        },
        {
            "sha": "bf39cfca912ac934f9ff9bd37416e4973e42e676",
            "filename": "src/transformers/models/deprecated/open_llama/modeling_open_llama.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fmodeling_open_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fmodeling_open_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fmodeling_open_llama.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -224,7 +224,6 @@ def __init__(self, config: OpenLlamaConfig):\n         self.head_dim = self.hidden_size // self.num_heads\n         self.max_position_embeddings = config.max_position_embeddings\n         self.dropout_prob = config.attention_dropout_prob\n-        self.rope_theta = config.rope_theta\n \n         if (self.head_dim * self.num_heads) != self.hidden_size:\n             raise ValueError(\n@@ -238,15 +237,15 @@ def __init__(self, config: OpenLlamaConfig):\n         self._init_rope()\n \n     def _init_rope(self):\n-        if self.config.rope_scaling is None:\n+        if self.config.rope_parameters is None:\n             self.rotary_emb = OpenLlamaRotaryEmbedding(\n                 self.head_dim,\n                 max_position_embeddings=self.max_position_embeddings,\n                 base=self.rope_theta,\n             )\n         else:\n-            scaling_type = self.config.rope_scaling[\"type\"]\n-            scaling_factor = self.config.rope_scaling[\"factor\"]\n+            scaling_type = self.config.rope_parameters[\"type\"]\n+            scaling_factor = self.config.rope_parameters[\"factor\"]\n             if scaling_type == \"linear\":\n                 self.rotary_emb = OpenLlamaLinearScalingRotaryEmbedding(\n                     self.head_dim,"
        },
        {
            "sha": "b54b5620e5245b59c6ba527f662f6080a2c39bd9",
            "filename": "src/transformers/models/dia/configuration_dia.py",
            "status": "modified",
            "additions": 26,
            "deletions": 96,
            "changes": 122,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fdia%2Fconfiguration_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fdia%2Fconfiguration_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fconfiguration_dia.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -17,7 +17,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import rope_config_validation\n+from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n from ...utils import logging\n \n \n@@ -55,45 +55,10 @@ class DiaEncoderConfig(PreTrainedConfig):\n         hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n             The non-linear activation function (function or string) in the encoder and pooler. If string, `\"gelu\"`,\n             `\"relu\"`, `\"swish\"` and `\"gelu_new\"` are supported.\n-        rope_theta (`float`, *optional*, defaults to 10000.0):\n-            The base period of the RoPE embeddings.\n-        rope_scaling (`dict`, *optional*):\n-            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n-            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n-            accordingly.\n-            Expected contents:\n-                `rope_type` (`str`):\n-                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n-                    'llama3'], with 'default' being the original RoPE implementation.\n-                `factor` (`float`, *optional*):\n-                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n-                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n-                    original maximum pre-trained length.\n-                `original_max_position_embeddings` (`int`, *optional*):\n-                    Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n-                    pretraining.\n-                `attention_factor` (`float`, *optional*):\n-                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n-                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n-                    `factor` field to infer the suggested value.\n-                `beta_fast` (`float`, *optional*):\n-                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n-                    ramp function. If unspecified, it defaults to 32.\n-                `beta_slow` (`float`, *optional*):\n-                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n-                    ramp function. If unspecified, it defaults to 1.\n-                `short_factor` (`List[float]`, *optional*):\n-                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n-                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n-                    size divided by the number of attention heads divided by 2\n-                `long_factor` (`List[float]`, *optional*):\n-                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n-                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n-                    size divided by the number of attention heads divided by 2\n-                `low_freq_factor` (`float`, *optional*):\n-                    Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n-                `high_freq_factor` (`float`, *optional*):\n-                    Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         initializer_range (`float`, *optional*, defaults to 0.02):\n             The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n     \"\"\"\n@@ -112,8 +77,7 @@ def __init__(\n         norm_eps: float = 1e-5,\n         vocab_size: int = 256,\n         hidden_act: str = \"silu\",\n-        rope_theta: float = 10000.0,\n-        rope_scaling: Optional[dict] = None,\n+        rope_parameters: Optional[RopeParameters] = None,\n         initializer_range: float = 0.02,\n         **kwargs,\n     ):\n@@ -127,14 +91,15 @@ def __init__(\n         self.vocab_size = vocab_size\n         self.num_key_value_heads = num_key_value_heads\n         self.hidden_act = hidden_act\n-        self.rope_theta = rope_theta\n-        self.rope_scaling = rope_scaling\n+        self.initializer_range = initializer_range\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n+\n         # Validate the correctness of rotary position embeddings parameters\n-        # BC: if there is a 'type' field, copy it it to 'rope_type'.\n-        if self.rope_scaling is not None and \"type\" in self.rope_scaling:\n-            self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n+        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n         rope_config_validation(self)\n-        self.initializer_range = initializer_range\n         super().__init__(**kwargs)\n \n \n@@ -179,45 +144,10 @@ class DiaDecoderConfig(PreTrainedConfig):\n             `\"swish\"` and `\"gelu_new\"` are supported.\n         num_channels (`int`, *optional*, defaults to 9):\n             Number of channels for the Dia decoder.\n-        rope_theta (`float`, *optional*, defaults to 10000.0):\n-            The base period of the RoPE embeddings.\n-        rope_scaling (`dict`, *optional*):\n-            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n-            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n-            accordingly.\n-            Expected contents:\n-                `rope_type` (`str`):\n-                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n-                    'llama3'], with 'default' being the original RoPE implementation.\n-                `factor` (`float`, *optional*):\n-                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n-                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n-                    original maximum pre-trained length.\n-                `original_max_position_embeddings` (`int`, *optional*):\n-                    Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n-                    pretraining.\n-                `attention_factor` (`float`, *optional*):\n-                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n-                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n-                    `factor` field to infer the suggested value.\n-                `beta_fast` (`float`, *optional*):\n-                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n-                    ramp function. If unspecified, it defaults to 32.\n-                `beta_slow` (`float`, *optional*):\n-                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n-                    ramp function. If unspecified, it defaults to 1.\n-                `short_factor` (`List[float]`, *optional*):\n-                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n-                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n-                    size divided by the number of attention heads divided by 2\n-                `long_factor` (`List[float]`, *optional*):\n-                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n-                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n-                    size divided by the number of attention heads divided by 2\n-                `low_freq_factor` (`float`, *optional*):\n-                    Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n-                `high_freq_factor` (`float`, *optional*):\n-                    Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         initializer_range (`float`, *optional*, defaults to 0.02):\n             The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n         use_cache (`bool`, *optional*, defaults to `True`):\n@@ -245,8 +175,7 @@ def __init__(\n         vocab_size: int = 1028,\n         hidden_act: str = \"silu\",\n         num_channels: int = 9,\n-        rope_theta: float = 10000.0,\n-        rope_scaling: Optional[dict] = None,\n+        rope_parameters: Optional[RopeParameters] = None,\n         initializer_range: float = 0.02,\n         use_cache: bool = True,\n         is_encoder_decoder: bool = True,\n@@ -267,15 +196,16 @@ def __init__(\n         self.vocab_size = vocab_size\n         self.hidden_act = hidden_act\n         self.num_channels = num_channels\n-        self.rope_theta = rope_theta\n-        self.rope_scaling = rope_scaling\n-        # Validate the correctness of rotary position embeddings parameters\n-        # BC: if there is a 'type' field, copy it it to 'rope_type'.\n-        if self.rope_scaling is not None and \"type\" in self.rope_scaling:\n-            self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n-        rope_config_validation(self)\n         self.initializer_range = initializer_range\n         self.use_cache = use_cache\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n+\n+        # Validate the correctness of rotary position embeddings parameters\n+        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n+        rope_config_validation(self)\n         super().__init__(is_encoder_decoder=is_encoder_decoder, **kwargs)\n \n "
        },
        {
            "sha": "6db8b1e9766fce9a7a5632b1b241d0487e178333",
            "filename": "src/transformers/models/dia/modeling_dia.py",
            "status": "modified",
            "additions": 51,
            "deletions": 17,
            "changes": 68,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -133,20 +133,49 @@ class DiaRotaryEmbedding(nn.Module):\n \n     def __init__(self, config: DiaConfig, device=None):\n         super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n         self.max_seq_len_cached = config.max_position_embeddings\n         self.original_max_seq_len = config.max_position_embeddings\n \n         self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.rope_type = self.config.rope_parameters[\"rope_type\"]\n+        rope_init_fn: Callable = self.compute_default_rope_parameters\n+        if self.rope_type != \"default\":\n+            rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+        inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n+\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n+        self.original_inv_freq = inv_freq\n+\n+    @staticmethod\n+    def compute_default_rope_parameters(\n+        config: Optional[DiaConfig] = None,\n+        device: Optional[\"torch.device\"] = None,\n+        seq_len: Optional[int] = None,\n+    ) -> tuple[\"torch.Tensor\", float]:\n+        \"\"\"\n+        Computes the inverse frequencies according to the original RoPE implementation\n+        Args:\n+            config ([`~transformers.PreTrainedConfig`]):\n+                The model configuration.\n+            device (`torch.device`):\n+                The device to use for initialization of the inverse frequencies.\n+            seq_len (`int`, *optional*):\n+                The current sequence length. Unused for this type of RoPE.\n+        Returns:\n+            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n+            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n+        \"\"\"\n+        base = config.rope_parameters[\"rope_theta\"]\n+        dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+\n+        attention_factor = 1.0  # Unused in this type of RoPE\n+\n+        # Compute the inverse frequencies\n+        inv_freq = 1.0 / (\n+            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n+        )\n+        return inv_freq, attention_factor\n \n     @torch.no_grad()\n     @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n@@ -260,8 +289,8 @@ def __init__(self, config: Union[DiaEncoderConfig, DiaDecoderConfig], layer_idx:\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n-        attention_mask: Optional[torch.Tensor],\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -386,7 +415,7 @@ def __init__(self, config: DiaEncoderConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n@@ -418,7 +447,7 @@ def __init__(self, config: DiaEncoderConfig):\n             [DiaEncoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n         )\n         self.norm = DiaRMSNorm(config.hidden_size, eps=config.norm_eps)\n-        self.rotary_embeddings = DiaRotaryEmbedding(config)\n+        self.rotary_emb = DiaRotaryEmbedding(config=config)\n \n     @auto_docstring\n     @can_return_tuple\n@@ -436,13 +465,13 @@ def forward(\n         # Note: We expect right padding and hence always generate\n         # the position ids on the fly to reduce preparation overhead\n         position_ids = torch.arange(input_ids.shape[-1], device=input_ids.device)[None, :]\n-        position_embeddings = self.rotary_embeddings(hidden_states, position_ids)\n \n         attention_mask = create_bidirectional_mask(\n             config=self.config,\n             input_embeds=hidden_states,\n             attention_mask=attention_mask,\n         )\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids=position_ids)\n \n         encoder_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n@@ -453,8 +482,9 @@ def forward(\n \n             layer_outputs = encoder_layer(\n                 hidden_states,\n-                position_embeddings=position_embeddings,\n                 attention_mask=attention_mask,\n+                position_ids=position_ids,\n+                position_embeddings=position_embeddings,\n                 **kwargs,\n             )\n             hidden_states = layer_outputs[0]\n@@ -486,12 +516,13 @@ def __init__(self, config: DiaDecoderConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[EncoderDecoderCache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n         **kwargs,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[torch.Tensor]]:\n         self_attn_cache = past_key_values\n@@ -539,11 +570,11 @@ def __init__(self, config: DiaDecoderConfig):\n         self.num_channels = config.num_channels\n         self.vocab_size = config.vocab_size\n         self.embeddings = DiaMultiChannelEmbedding(config)\n-        self.rotary_embeddings = DiaRotaryEmbedding(config)\n         self.layers = nn.ModuleList(\n             [DiaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n         )\n         self.norm = DiaRMSNorm(config.hidden_size, eps=config.norm_eps)\n+        self.rotary_emb = DiaRotaryEmbedding(config=config)\n \n     @auto_docstring\n     @can_return_tuple\n@@ -578,7 +609,6 @@ def forward(\n \n         # RoPE\n         hidden_states = self.embeddings(input_ids)\n-        position_embeddings = self.rotary_embeddings(hidden_states, position_ids)\n \n         if attention_mask is None and not is_torchdynamo_compiling():\n             # required mask seq length can be calculated via length of past cache\n@@ -598,6 +628,7 @@ def forward(\n             attention_mask=encoder_attention_mask,\n             encoder_hidden_states=encoder_hidden_states,\n         )\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids=position_ids)\n \n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n@@ -609,12 +640,15 @@ def forward(\n \n             layer_outputs = layer(\n                 hidden_states,\n+                # Needs to be an arg in order to function properly\n+                # on inplace operations to be carried (e.g. compile)\n                 position_embeddings,\n                 attention_mask,\n                 encoder_hidden_states,\n                 encoder_attention_mask=encoder_attention_mask,\n                 past_key_values=past_key_values,\n                 cache_position=cache_position,\n+                position_ids=position_ids,\n                 **kwargs,\n             )\n             hidden_states = layer_outputs[0]"
        },
        {
            "sha": "514c40d79816e94e72d344a09e5376f654af0683",
            "filename": "src/transformers/models/dia/modular_dia.py",
            "status": "modified",
            "additions": 12,
            "deletions": 7,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fdia%2Fmodular_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fdia%2Fmodular_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fmodular_dia.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -207,7 +207,7 @@ def __init__(self, config: DiaEncoderConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n@@ -239,7 +239,7 @@ def __init__(self, config: DiaEncoderConfig):\n             [DiaEncoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n         )\n         self.norm = DiaRMSNorm(config.hidden_size, eps=config.norm_eps)\n-        self.rotary_embeddings = DiaRotaryEmbedding(config)\n+        self.rotary_emb = DiaRotaryEmbedding(config=config)\n \n     @auto_docstring\n     @can_return_tuple\n@@ -257,13 +257,13 @@ def forward(\n         # Note: We expect right padding and hence always generate\n         # the position ids on the fly to reduce preparation overhead\n         position_ids = torch.arange(input_ids.shape[-1], device=input_ids.device)[None, :]\n-        position_embeddings = self.rotary_embeddings(hidden_states, position_ids)\n \n         attention_mask = create_bidirectional_mask(\n             config=self.config,\n             input_embeds=hidden_states,\n             attention_mask=attention_mask,\n         )\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids=position_ids)\n \n         encoder_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n@@ -274,8 +274,9 @@ def forward(\n \n             layer_outputs = encoder_layer(\n                 hidden_states,\n-                position_embeddings=position_embeddings,\n                 attention_mask=attention_mask,\n+                position_ids=position_ids,\n+                position_embeddings=position_embeddings,\n                 **kwargs,\n             )\n             hidden_states = layer_outputs[0]\n@@ -307,12 +308,13 @@ def __init__(self, config: DiaDecoderConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[EncoderDecoderCache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n         **kwargs,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[torch.Tensor]]:\n         self_attn_cache = past_key_values\n@@ -360,11 +362,11 @@ def __init__(self, config: DiaDecoderConfig):\n         self.num_channels = config.num_channels\n         self.vocab_size = config.vocab_size\n         self.embeddings = DiaMultiChannelEmbedding(config)\n-        self.rotary_embeddings = DiaRotaryEmbedding(config)\n         self.layers = nn.ModuleList(\n             [DiaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n         )\n         self.norm = DiaRMSNorm(config.hidden_size, eps=config.norm_eps)\n+        self.rotary_emb = DiaRotaryEmbedding(config=config)\n \n     @auto_docstring\n     @can_return_tuple\n@@ -399,7 +401,6 @@ def forward(\n \n         # RoPE\n         hidden_states = self.embeddings(input_ids)\n-        position_embeddings = self.rotary_embeddings(hidden_states, position_ids)\n \n         if attention_mask is None and not is_torchdynamo_compiling():\n             # required mask seq length can be calculated via length of past cache\n@@ -419,6 +420,7 @@ def forward(\n             attention_mask=encoder_attention_mask,\n             encoder_hidden_states=encoder_hidden_states,\n         )\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids=position_ids)\n \n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n@@ -430,12 +432,15 @@ def forward(\n \n             layer_outputs = layer(\n                 hidden_states,\n+                # Needs to be an arg in order to function properly\n+                # on inplace operations to be carried (e.g. compile)\n                 position_embeddings,\n                 attention_mask,\n                 encoder_hidden_states,\n                 encoder_attention_mask=encoder_attention_mask,\n                 past_key_values=past_key_values,\n                 cache_position=cache_position,\n+                position_ids=position_ids,\n                 **kwargs,\n             )\n             hidden_states = layer_outputs[0]"
        },
        {
            "sha": "0eac1f506c722bfe556156a3251ac1c91bcfae8d",
            "filename": "src/transformers/models/diffllama/configuration_diffllama.py",
            "status": "modified",
            "additions": 33,
            "deletions": 66,
            "changes": 99,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fconfiguration_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fconfiguration_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fconfiguration_diffllama.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -17,8 +17,10 @@\n # limitations under the License.\n \"\"\"DiffLlama model configuration\"\"\"\n \n+from typing import Optional\n+\n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import rope_config_validation\n+from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n \n \n class DiffLlamaConfig(PreTrainedConfig):\n@@ -70,45 +72,10 @@ class DiffLlamaConfig(PreTrainedConfig):\n             End of stream token id.\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie weight embeddings\n-        rope_theta (`float`, *optional*, defaults to 10000.0):\n-            The base period of the RoPE embeddings.\n-        rope_scaling (`Dict`, *optional*):\n-            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n-            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n-            accordingly.\n-            Expected contents:\n-                `rope_type` (`str`):\n-                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n-                    'diffllama3'], with 'default' being the original RoPE implementation.\n-                `factor` (`float`, *optional*):\n-                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n-                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n-                    original maximum pre-trained length.\n-                `original_max_position_embeddings` (`int`, *optional*):\n-                    Used with 'dynamic', 'longrope' and 'diffllama3'. The original max position embeddings used during\n-                    pretraining.\n-                `attention_factor` (`float`, *optional*):\n-                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n-                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n-                    `factor` field to infer the suggested value.\n-                `beta_fast` (`float`, *optional*):\n-                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n-                    ramp function. If unspecified, it defaults to 32.\n-                `beta_slow` (`float`, *optional*):\n-                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n-                    ramp function. If unspecified, it defaults to 1.\n-                `short_factor` (`list[float]`, *optional*):\n-                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n-                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n-                    size divided by the number of attention heads divided by 2\n-                `long_factor` (`list[float]`, *optional*):\n-                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n-                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n-                    size divided by the number of attention heads divided by 2\n-                `low_freq_factor` (`float`, *optional*):\n-                    Only used with 'diffllama3'. Scaling factor applied to low frequency components of the RoPE\n-                `high_freq_factor` (`float`, *optional*):\n-                    Only used with 'diffllama3'. Scaling factor applied to high frequency components of the RoPE\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         attention_bias (`bool`, *optional*, defaults to `False`):\n             Whether to use a bias in the query, key, value and output projection layers during self-attention.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n@@ -136,27 +103,26 @@ class DiffLlamaConfig(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size=32000,\n-        hidden_size=2048,\n-        intermediate_size=8192,\n-        num_hidden_layers=16,\n-        num_attention_heads=32,\n-        num_key_value_heads=None,\n-        hidden_act=\"silu\",\n-        max_position_embeddings=2048,\n-        initializer_range=0.02,\n-        rms_norm_eps=1e-5,\n-        use_cache=True,\n-        pad_token_id=None,\n-        bos_token_id=1,\n-        eos_token_id=2,\n-        tie_word_embeddings=False,\n-        rope_theta=10000.0,\n-        rope_scaling=None,\n-        attention_bias=False,\n-        attention_dropout=0.0,\n-        lambda_std_dev=0.1,\n-        head_dim=None,\n+        vocab_size: Optional[int] = 32000,\n+        hidden_size: Optional[int] = 2048,\n+        intermediate_size: Optional[int] = 8192,\n+        num_hidden_layers: Optional[int] = 16,\n+        num_attention_heads: Optional[int] = 32,\n+        num_key_value_heads: Optional[int] = None,\n+        hidden_act: Optional[str] = \"silu\",\n+        max_position_embeddings: Optional[int] = 2048,\n+        initializer_range: Optional[float] = 0.02,\n+        rms_norm_eps: Optional[int] = 1e-5,\n+        use_cache: Optional[bool] = True,\n+        pad_token_id: Optional[int] = None,\n+        bos_token_id: Optional[int] = 1,\n+        eos_token_id: Optional[int] = 2,\n+        tie_word_embeddings: Optional[bool] = False,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        attention_bias: Optional[bool] = False,\n+        attention_dropout: Optional[float] = 0.0,\n+        lambda_std_dev: Optional[float] = 0.1,\n+        head_dim: Optional[int] = None,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size\n@@ -175,16 +141,17 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.rms_norm_eps = rms_norm_eps\n         self.use_cache = use_cache\n-        self.rope_theta = rope_theta\n-        self.rope_scaling = rope_scaling\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n         self.lambda_std_dev = lambda_std_dev\n         self.head_dim = head_dim if head_dim is not None else self.hidden_size // self.num_attention_heads\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n+\n         # Validate the correctness of rotary position embeddings parameters\n-        # BC: if there is a 'type' field, copy it it to 'rope_type'.\n-        if self.rope_scaling is not None and \"type\" in self.rope_scaling:\n-            self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n+        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n         rope_config_validation(self)\n \n         super().__init__("
        },
        {
            "sha": "d82430b623e175f87ad37107fc08b3de49103752",
            "filename": "src/transformers/models/diffllama/modeling_diffllama.py",
            "status": "modified",
            "additions": 70,
            "deletions": 50,
            "changes": 120,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -22,6 +22,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n import math\n+from collections.abc import Callable\n from typing import Optional, Union\n \n import torch\n@@ -67,6 +68,71 @@ def forward(self, x):\n         return down_proj\n \n \n+class DiffLlamaRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n+    def __init__(self, config: DiffLlamaConfig, device=None):\n+        super().__init__()\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+\n+        self.rope_type = self.config.rope_parameters[\"rope_type\"]\n+        rope_init_fn: Callable = self.compute_default_rope_parameters\n+        if self.rope_type != \"default\":\n+            rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+        inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n+\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = inv_freq\n+\n+    @staticmethod\n+    def compute_default_rope_parameters(\n+        config: Optional[DiffLlamaConfig] = None,\n+        device: Optional[\"torch.device\"] = None,\n+        seq_len: Optional[int] = None,\n+    ) -> tuple[\"torch.Tensor\", float]:\n+        \"\"\"\n+        Computes the inverse frequencies according to the original RoPE implementation\n+        Args:\n+            config ([`~transformers.PreTrainedConfig`]):\n+                The model configuration.\n+            device (`torch.device`):\n+                The device to use for initialization of the inverse frequencies.\n+            seq_len (`int`, *optional*):\n+                The current sequence length. Unused for this type of RoPE.\n+        Returns:\n+            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n+            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n+        \"\"\"\n+        base = config.rope_parameters[\"rope_theta\"]\n+        dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+\n+        attention_factor = 1.0  # Unused in this type of RoPE\n+\n+        # Compute the inverse frequencies\n+        inv_freq = 1.0 / (\n+            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n+        )\n+        return inv_freq, attention_factor\n+\n+    @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n+    def forward(self, x, position_ids):\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n def rotate_half(x):\n     \"\"\"Rotates half the hidden dims of the input.\"\"\"\n     x1 = x[..., : x.shape[-1] // 2]\n@@ -139,7 +205,6 @@ def __init__(self, config: DiffLlamaConfig, layer_idx: Optional[int] = None):\n         self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n         # under this are not used\n         self.max_position_embeddings = config.max_position_embeddings\n-        self.rope_theta = config.rope_theta\n         self.is_causal = True\n \n         self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n@@ -261,16 +326,7 @@ def forward(\n         key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n         value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n \n-        if position_embeddings is None:\n-            logger.warning_once(\n-                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n-                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n-                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n-                \"removed and `position_embeddings` will be mandatory.\"\n-            )\n-            cos, sin = self.rotary_emb(value_states, position_ids)\n-        else:\n-            cos, sin = position_embeddings\n+        cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n         if past_key_values is not None:\n@@ -496,7 +552,7 @@ def forward(\n         past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> torch.Tensor:\n         residual = hidden_states\n@@ -549,42 +605,6 @@ def _init_weights(self, module):\n             module.lambda_k2.data.normal_(0, self.config.lambda_std_dev)\n \n \n-class DiffLlamaRotaryEmbedding(nn.Module):\n-    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n-\n-    def __init__(self, config: DiffLlamaConfig, device=None):\n-        super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n-        self.max_seq_len_cached = config.max_position_embeddings\n-        self.original_max_seq_len = config.max_position_embeddings\n-\n-        self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n-\n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n-        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n-\n-    @torch.no_grad()\n-    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n-    def forward(self, x, position_ids):\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n-        position_ids_expanded = position_ids[:, None, :].float()\n-\n-        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n-            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos() * self.attention_scaling\n-            sin = emb.sin() * self.attention_scaling\n-\n-        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n-\n-\n @auto_docstring\n class DiffLlamaModel(DiffLlamaPreTrainedModel):\n     def __init__(self, config: DiffLlamaConfig):\n@@ -644,16 +664,16 @@ def forward(\n         )\n \n         hidden_states = inputs_embeds\n-        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids=position_ids)\n \n         for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n             hidden_states = decoder_layer(\n                 hidden_states,\n                 attention_mask=causal_mask,\n+                position_embeddings=position_embeddings,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n                 cache_position=cache_position,\n-                position_embeddings=position_embeddings,\n                 **kwargs,\n             )\n "
        },
        {
            "sha": "331c7327b681e3cc0edc7a755f634b3f5b0fa343",
            "filename": "src/transformers/models/diffllama/modular_diffllama.py",
            "status": "modified",
            "additions": 6,
            "deletions": 11,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodular_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodular_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodular_diffllama.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -33,6 +33,7 @@\n     LlamaForTokenClassification,\n     LlamaModel,\n     LlamaPreTrainedModel,\n+    LlamaRotaryEmbedding,\n     apply_rotary_pos_emb,\n     repeat_kv,\n )\n@@ -54,6 +55,10 @@ def lambda_init_fn(layer_idx):\n     return 0.8 - 0.6 * math.exp(-0.3 * layer_idx)\n \n \n+class DiffLlamaRotaryEmbedding(LlamaRotaryEmbedding):\n+    pass\n+\n+\n class DiffLlamaAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -76,7 +81,6 @@ def __init__(self, config: DiffLlamaConfig, layer_idx: Optional[int] = None):\n         self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n         # under this are not used\n         self.max_position_embeddings = config.max_position_embeddings\n-        self.rope_theta = config.rope_theta\n         self.is_causal = True\n \n         self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n@@ -198,16 +202,7 @@ def forward(\n         key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n         value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n \n-        if position_embeddings is None:\n-            logger.warning_once(\n-                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n-                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n-                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n-                \"removed and `position_embeddings` will be mandatory.\"\n-            )\n-            cos, sin = self.rotary_emb(value_states, position_ids)\n-        else:\n-            cos, sin = position_embeddings\n+        cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n         if past_key_values is not None:"
        },
        {
            "sha": "844b9519b45ab0f3531421bc8966986a6018f594",
            "filename": "src/transformers/models/doge/configuration_doge.py",
            "status": "modified",
            "additions": 37,
            "deletions": 67,
            "changes": 104,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fdoge%2Fconfiguration_doge.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fdoge%2Fconfiguration_doge.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdoge%2Fconfiguration_doge.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -20,8 +20,10 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n+from typing import Optional\n+\n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import rope_config_validation\n+from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n \n \n class DogeConfig(PreTrainedConfig):\n@@ -56,41 +58,10 @@ class DogeConfig(PreTrainedConfig):\n             Whether the model's input and output word embeddings should be tied.\n         max_position_embeddings (`int`, *optional*, defaults to 2048):\n             The maximum sequence length that this model might ever be used with.\n-        rope_theta (`float`, *optional*, defaults to 10000.0):\n-            The base period of the RoPE embeddings.\n-        rope_scaling (`Dict`, *optional*):\n-            Dictionary containing the scaling configuration for the RoPE embeddings.\n-            NOTE: if you apply new rope type and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value accordingly.\n-            Doge family of small models use `{ 'rope_type': 'dynamic', 'factor': 4.0, 'original_max_position_embeddings': 2048 }` as the default value.\n-            Expected contents:\n-                `rope_type` (`str`):\n-                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope', 'llama3'], with 'default' being the original RoPE implementation.\n-                `factor` (`float`, *optional*):\n-                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings.\n-                    In most scaling types, a `factor` of x will enable the model to handle sequences of length x * original maximum pre-trained length.\n-                `original_max_position_embeddings` (`int`, *optional*):\n-                    Used with 'dynamic', 'longrope' and 'llama3'.\n-                    The original max position embeddings used during pretraining.\n-                `attention_factor` (`float`, *optional*):\n-                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n-                    computation.\n-                    If unspecified, it defaults to value recommended by the implementation, using the `factor` field to infer the suggested value.\n-                `beta_fast` (`float`, *optional*):\n-                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n-                    ramp function. If unspecified, it defaults to 32.\n-                `beta_slow` (`float`, *optional*):\n-                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n-                    ramp function. If unspecified, it defaults to 1.\n-                `short_factor` (`List[float]`, *optional*):\n-                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<`original_max_position_embeddings`).\n-                    Must be a list of numbers with the same length as the hidden size divided by the number of attention heads divided by 2\n-                `long_factor` (`List[float]`, *optional*):\n-                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<`original_max_position_embeddings`).\n-                    Must be a list of numbers with the same length as the hidden size divided by the number of attention heads divided by 2\n-                `low_freq_factor` (`float`, *optional*):\n-                    Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n-                `high_freq_factor` (`float`, *optional*):\n-                    Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         num_attention_heads (`int`, *optional*, defaults to 8):\n             Number of attention heads for each attention layer in the Transformer decoder.\n         num_key_value_heads (`int`, *optional*):\n@@ -166,32 +137,31 @@ class DogeConfig(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size=32768,\n-        hidden_size=1024,\n-        intermediate_size=2048,\n-        num_hidden_layers=32,\n-        hidden_dropout=0.0,\n-        hidden_act=\"silu\",\n-        initializer_range=0.02,\n-        rms_norm_eps=1e-06,\n-        use_cache=True,\n-        tie_word_embeddings=False,\n-        max_position_embeddings=2048,\n-        rope_theta=10000.0,\n-        rope_scaling=None,\n-        num_attention_heads=8,\n-        num_key_value_heads=None,\n-        attention_bias=False,\n-        attention_dropout=0.0,\n-        mlp_bias=False,\n-        sliding_window=None,\n-        keep_window_size=2048,\n-        is_moe=False,\n-        num_experts=16384,\n-        num_experts_per_tok=64,\n-        norm_topk_prob=False,\n-        output_router_logits=False,\n-        router_aux_loss_coef=0.001,\n+        vocab_size: Optional[int] = 32768,\n+        hidden_size: Optional[int] = 1024,\n+        intermediate_size: Optional[int] = 2048,\n+        num_hidden_layers: Optional[int] = 32,\n+        hidden_dropout: Optional[float] = 0.0,\n+        hidden_act: Optional[str] = \"silu\",\n+        initializer_range: Optional[float] = 0.02,\n+        rms_norm_eps: Optional[int] = 1e-06,\n+        use_cache: Optional[bool] = True,\n+        tie_word_embeddings: Optional[bool] = False,\n+        max_position_embeddings: Optional[int] = 2048,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        num_attention_heads: Optional[int] = 8,\n+        num_key_value_heads: Optional[int] = None,\n+        attention_bias: Optional[bool] = False,\n+        attention_dropout: Optional[float] = 0.0,\n+        mlp_bias: Optional[bool] = False,\n+        sliding_window: Optional[int] = None,\n+        keep_window_size: Optional[int] = 2048,\n+        is_moe: Optional[bool] = False,\n+        num_experts: Optional[int] = 16384,\n+        num_experts_per_tok: Optional[int] = 64,\n+        norm_topk_prob: Optional[bool] = False,\n+        output_router_logits: Optional[bool] = False,\n+        router_aux_loss_coef: Optional[float] = 0.001,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size\n@@ -206,8 +176,6 @@ def __init__(\n         self.use_cache = use_cache\n \n         self.max_position_embeddings = max_position_embeddings\n-        self.rope_theta = rope_theta\n-        self.rope_scaling = rope_scaling\n         self.num_attention_heads = num_attention_heads\n         self.num_key_value_heads = num_key_value_heads\n         self.attention_bias = attention_bias\n@@ -221,11 +189,13 @@ def __init__(\n         self.norm_topk_prob = norm_topk_prob\n         self.output_router_logits = output_router_logits\n         self.router_aux_loss_coef = router_aux_loss_coef\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n \n         # Validate the correctness of rotary position embeddings parameters\n-        # BC: if there is a 'type' field, copy it it to 'rope_type'.\n-        if self.rope_scaling is not None and \"type\" in self.rope_scaling:\n-            self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n+        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n         rope_config_validation(self)\n \n         # for backward compatibility"
        },
        {
            "sha": "1ced8dbbdd6308f8198b89b0c213c2f9eb81f5fa",
            "filename": "src/transformers/models/doge/modeling_doge.py",
            "status": "modified",
            "additions": 41,
            "deletions": 13,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodeling_doge.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodeling_doge.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodeling_doge.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -75,20 +75,49 @@ class DogeRotaryEmbedding(nn.Module):\n \n     def __init__(self, config: DogeConfig, device=None):\n         super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n         self.max_seq_len_cached = config.max_position_embeddings\n         self.original_max_seq_len = config.max_position_embeddings\n \n         self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.rope_type = self.config.rope_parameters[\"rope_type\"]\n+        rope_init_fn: Callable = self.compute_default_rope_parameters\n+        if self.rope_type != \"default\":\n+            rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+        inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n+\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n+        self.original_inv_freq = inv_freq\n+\n+    @staticmethod\n+    def compute_default_rope_parameters(\n+        config: Optional[DogeConfig] = None,\n+        device: Optional[\"torch.device\"] = None,\n+        seq_len: Optional[int] = None,\n+    ) -> tuple[\"torch.Tensor\", float]:\n+        \"\"\"\n+        Computes the inverse frequencies according to the original RoPE implementation\n+        Args:\n+            config ([`~transformers.PreTrainedConfig`]):\n+                The model configuration.\n+            device (`torch.device`):\n+                The device to use for initialization of the inverse frequencies.\n+            seq_len (`int`, *optional*):\n+                The current sequence length. Unused for this type of RoPE.\n+        Returns:\n+            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n+            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n+        \"\"\"\n+        base = config.rope_parameters[\"rope_theta\"]\n+        dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+\n+        attention_factor = 1.0  # Unused in this type of RoPE\n+\n+        # Compute the inverse frequencies\n+        inv_freq = 1.0 / (\n+            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n+        )\n+        return inv_freq, attention_factor\n \n     @torch.no_grad()\n     @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n@@ -266,6 +295,7 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n         **kwargs,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         input_shape = hidden_states.shape[:-1]\n@@ -442,7 +472,7 @@ def __init__(self, config: DogeConfig, layer_idx: Optional[int] = None):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -566,19 +596,17 @@ def forward(\n         )\n \n         hidden_states = inputs_embeds\n-\n-        # create position embeddings to be shared across the decoder layers\n-        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids=position_ids)\n \n         for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n             hidden_states = decoder_layer(\n                 hidden_states,\n-                position_embeddings=position_embeddings,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n                 **kwargs,\n             )\n "
        },
        {
            "sha": "52603d99dcd4021aa716326a2505fac1280371db",
            "filename": "src/transformers/models/doge/modular_doge.py",
            "status": "modified",
            "additions": 40,
            "deletions": 69,
            "changes": 109,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodular_doge.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodular_doge.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodular_doge.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -30,10 +30,10 @@\n from ...integrations.flex_attention import compile_friendly_flex_attention\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n-from ...modeling_rope_utils import rope_config_validation\n+from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n from ...modeling_utils import AttentionInterface, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import TransformersKwargs, is_torch_flex_attn_available\n+from ...utils import TransformersKwargs, is_torch_flex_attn_available, logging\n from ...utils.generic import OutputRecorder\n from ..llama.modeling_llama import (\n     LlamaForSequenceClassification,\n@@ -48,6 +48,8 @@\n from ..mixtral.modeling_mixtral import MixtralForCausalLM, MixtralModel\n \n \n+logger = logging.get_logger(__name__)\n+\n if is_torch_flex_attn_available():\n     from torch.nn.attention.flex_attention import BlockMask\n \n@@ -84,41 +86,10 @@ class DogeConfig(PreTrainedConfig):\n             Whether the model's input and output word embeddings should be tied.\n         max_position_embeddings (`int`, *optional*, defaults to 2048):\n             The maximum sequence length that this model might ever be used with.\n-        rope_theta (`float`, *optional*, defaults to 10000.0):\n-            The base period of the RoPE embeddings.\n-        rope_scaling (`Dict`, *optional*):\n-            Dictionary containing the scaling configuration for the RoPE embeddings.\n-            NOTE: if you apply new rope type and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value accordingly.\n-            Doge family of small models use `{ 'rope_type': 'dynamic', 'factor': 4.0, 'original_max_position_embeddings': 2048 }` as the default value.\n-            Expected contents:\n-                `rope_type` (`str`):\n-                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope', 'llama3'], with 'default' being the original RoPE implementation.\n-                `factor` (`float`, *optional*):\n-                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings.\n-                    In most scaling types, a `factor` of x will enable the model to handle sequences of length x * original maximum pre-trained length.\n-                `original_max_position_embeddings` (`int`, *optional*):\n-                    Used with 'dynamic', 'longrope' and 'llama3'.\n-                    The original max position embeddings used during pretraining.\n-                `attention_factor` (`float`, *optional*):\n-                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n-                    computation.\n-                    If unspecified, it defaults to value recommended by the implementation, using the `factor` field to infer the suggested value.\n-                `beta_fast` (`float`, *optional*):\n-                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n-                    ramp function. If unspecified, it defaults to 32.\n-                `beta_slow` (`float`, *optional*):\n-                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n-                    ramp function. If unspecified, it defaults to 1.\n-                `short_factor` (`List[float]`, *optional*):\n-                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<`original_max_position_embeddings`).\n-                    Must be a list of numbers with the same length as the hidden size divided by the number of attention heads divided by 2\n-                `long_factor` (`List[float]`, *optional*):\n-                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<`original_max_position_embeddings`).\n-                    Must be a list of numbers with the same length as the hidden size divided by the number of attention heads divided by 2\n-                `low_freq_factor` (`float`, *optional*):\n-                    Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n-                `high_freq_factor` (`float`, *optional*):\n-                    Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         num_attention_heads (`int`, *optional*, defaults to 8):\n             Number of attention heads for each attention layer in the Transformer decoder.\n         num_key_value_heads (`int`, *optional*):\n@@ -194,32 +165,31 @@ class DogeConfig(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size=32768,\n-        hidden_size=1024,\n-        intermediate_size=2048,\n-        num_hidden_layers=32,\n-        hidden_dropout=0.0,\n-        hidden_act=\"silu\",\n-        initializer_range=0.02,\n-        rms_norm_eps=1e-06,\n-        use_cache=True,\n-        tie_word_embeddings=False,\n-        max_position_embeddings=2048,\n-        rope_theta=10000.0,\n-        rope_scaling=None,\n-        num_attention_heads=8,\n-        num_key_value_heads=None,\n-        attention_bias=False,\n-        attention_dropout=0.0,\n-        mlp_bias=False,\n-        sliding_window=None,\n-        keep_window_size=2048,\n-        is_moe=False,\n-        num_experts=16384,\n-        num_experts_per_tok=64,\n-        norm_topk_prob=False,\n-        output_router_logits=False,\n-        router_aux_loss_coef=0.001,\n+        vocab_size: Optional[int] = 32768,\n+        hidden_size: Optional[int] = 1024,\n+        intermediate_size: Optional[int] = 2048,\n+        num_hidden_layers: Optional[int] = 32,\n+        hidden_dropout: Optional[float] = 0.0,\n+        hidden_act: Optional[str] = \"silu\",\n+        initializer_range: Optional[float] = 0.02,\n+        rms_norm_eps: Optional[int] = 1e-06,\n+        use_cache: Optional[bool] = True,\n+        tie_word_embeddings: Optional[bool] = False,\n+        max_position_embeddings: Optional[int] = 2048,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        num_attention_heads: Optional[int] = 8,\n+        num_key_value_heads: Optional[int] = None,\n+        attention_bias: Optional[bool] = False,\n+        attention_dropout: Optional[float] = 0.0,\n+        mlp_bias: Optional[bool] = False,\n+        sliding_window: Optional[int] = None,\n+        keep_window_size: Optional[int] = 2048,\n+        is_moe: Optional[bool] = False,\n+        num_experts: Optional[int] = 16384,\n+        num_experts_per_tok: Optional[int] = 64,\n+        norm_topk_prob: Optional[bool] = False,\n+        output_router_logits: Optional[bool] = False,\n+        router_aux_loss_coef: Optional[float] = 0.001,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size\n@@ -234,8 +204,6 @@ def __init__(\n         self.use_cache = use_cache\n \n         self.max_position_embeddings = max_position_embeddings\n-        self.rope_theta = rope_theta\n-        self.rope_scaling = rope_scaling\n         self.num_attention_heads = num_attention_heads\n         self.num_key_value_heads = num_key_value_heads\n         self.attention_bias = attention_bias\n@@ -249,11 +217,13 @@ def __init__(\n         self.norm_topk_prob = norm_topk_prob\n         self.output_router_logits = output_router_logits\n         self.router_aux_loss_coef = router_aux_loss_coef\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n \n         # Validate the correctness of rotary position embeddings parameters\n-        # BC: if there is a 'type' field, copy it it to 'rope_type'.\n-        if self.rope_scaling is not None and \"type\" in self.rope_scaling:\n-            self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n+        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n         rope_config_validation(self)\n \n         # for backward compatibility\n@@ -362,6 +332,7 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n         **kwargs,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         input_shape = hidden_states.shape[:-1]\n@@ -526,7 +497,7 @@ def __init__(self, config: DogeConfig, layer_idx: Optional[int] = None):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,"
        },
        {
            "sha": "db524dd5789c981a7adbdeafebe907c7a8cf2691",
            "filename": "src/transformers/models/dots1/configuration_dots1.py",
            "status": "modified",
            "additions": 43,
            "deletions": 34,
            "changes": 77,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fdots1%2Fconfiguration_dots1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fdots1%2Fconfiguration_dots1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdots1%2Fconfiguration_dots1.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -12,7 +12,10 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n+from typing import Optional\n+\n from ...configuration_utils import PreTrainedConfig, layer_type_validation\n+from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n from ...utils import logging\n \n \n@@ -73,10 +76,10 @@ class Dots1Config(PreTrainedConfig):\n             Whether or not the model should return the last key/values attentions. Only relevant if `config.is_decoder=True`.\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie the input and output word embeddings.\n-        rope_theta (`float`, *optional*, defaults to 10000.0):\n-            The base period of the RoPE embeddings.\n-        rope_scaling (`dict`, *optional*):\n-            Dictionary for scaling RoPE embeddings. Supports `{\"type\": strategy name, \"factor\": scaling factor}`.\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         attention_bias (`bool`, *optional*, defaults to `False`):\n             Whether to use a bias in the self-attention projections.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n@@ -136,34 +139,33 @@ class Dots1Config(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size=152064,\n-        hidden_size=4608,\n-        intermediate_size=10944,\n-        moe_intermediate_size=1408,\n-        num_hidden_layers=62,\n-        num_attention_heads=32,\n-        num_key_value_heads=32,\n-        n_shared_experts=None,\n-        n_routed_experts=None,\n-        n_group=1,\n-        topk_group=1,\n-        num_experts_per_tok=None,\n-        first_k_dense_replace=0,\n-        norm_topk_prob=False,\n-        hidden_act=\"silu\",\n-        max_position_embeddings=2048,\n-        initializer_range=0.02,\n-        rms_norm_eps=1e-6,\n-        use_cache=True,\n-        tie_word_embeddings=False,\n-        rope_theta=10000.0,\n-        rope_scaling=None,\n-        attention_bias=False,\n-        attention_dropout=0.0,\n-        routed_scaling_factor=1.0,\n-        sliding_window=4096,\n-        max_window_layers=62,\n-        layer_types=None,\n+        vocab_size: Optional[int] = 152064,\n+        hidden_size: Optional[int] = 4608,\n+        intermediate_size: Optional[int] = 10944,\n+        moe_intermediate_size: Optional[int] = 1408,\n+        num_hidden_layers: Optional[int] = 62,\n+        num_attention_heads: Optional[int] = 32,\n+        num_key_value_heads: Optional[int] = 32,\n+        n_shared_experts: Optional[int] = None,\n+        n_routed_experts: Optional[int] = None,\n+        n_group: Optional[int] = 1,\n+        topk_group: Optional[int] = 1,\n+        num_experts_per_tok: Optional[int] = None,\n+        first_k_dense_replace: Optional[int] = 0,\n+        norm_topk_prob: Optional[bool] = False,\n+        hidden_act: Optional[str] = \"silu\",\n+        max_position_embeddings: Optional[int] = 2048,\n+        initializer_range: Optional[float] = 0.02,\n+        rms_norm_eps: Optional[int] = 1e-6,\n+        use_cache: Optional[bool] = True,\n+        tie_word_embeddings: Optional[bool] = False,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        attention_bias: Optional[bool] = False,\n+        attention_dropout: Optional[float] = 0.0,\n+        routed_scaling_factor: Optional[float] = 1.0,\n+        sliding_window: Optional[int] = 4096,\n+        max_window_layers: Optional[int] = 62,\n+        layer_types: Optional[list[str]] = None,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size\n@@ -187,14 +189,16 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.rms_norm_eps = rms_norm_eps\n         self.use_cache = use_cache\n-        self.rope_theta = rope_theta\n-        self.rope_scaling = rope_scaling\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n         self.routed_scaling_factor = routed_scaling_factor\n         self.sliding_window = sliding_window\n         self.max_window_layers = max_window_layers\n \n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n+\n         self.layer_types = layer_types\n         if self.layer_types is None:\n             self.layer_types = [\n@@ -205,6 +209,11 @@ def __init__(\n             ]\n         layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n+        # Validate the correctness of rotary position embeddings parameters\n+        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n+        rope_config_validation(self)\n+\n         super().__init__(\n             tie_word_embeddings=tie_word_embeddings,\n             **kwargs,"
        },
        {
            "sha": "5c6c6ea450e0237b241835b68f135894a60b57c0",
            "filename": "src/transformers/models/dots1/modeling_dots1.py",
            "status": "modified",
            "additions": 41,
            "deletions": 13,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -67,20 +67,49 @@ class Dots1RotaryEmbedding(nn.Module):\n \n     def __init__(self, config: Dots1Config, device=None):\n         super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n         self.max_seq_len_cached = config.max_position_embeddings\n         self.original_max_seq_len = config.max_position_embeddings\n \n         self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.rope_type = self.config.rope_parameters[\"rope_type\"]\n+        rope_init_fn: Callable = self.compute_default_rope_parameters\n+        if self.rope_type != \"default\":\n+            rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+        inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n+\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n+        self.original_inv_freq = inv_freq\n+\n+    @staticmethod\n+    def compute_default_rope_parameters(\n+        config: Optional[Dots1Config] = None,\n+        device: Optional[\"torch.device\"] = None,\n+        seq_len: Optional[int] = None,\n+    ) -> tuple[\"torch.Tensor\", float]:\n+        \"\"\"\n+        Computes the inverse frequencies according to the original RoPE implementation\n+        Args:\n+            config ([`~transformers.PreTrainedConfig`]):\n+                The model configuration.\n+            device (`torch.device`):\n+                The device to use for initialization of the inverse frequencies.\n+            seq_len (`int`, *optional*):\n+                The current sequence length. Unused for this type of RoPE.\n+        Returns:\n+            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n+            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n+        \"\"\"\n+        base = config.rope_parameters[\"rope_theta\"]\n+        dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+\n+        attention_factor = 1.0  # Unused in this type of RoPE\n+\n+        # Compute the inverse frequencies\n+        inv_freq = 1.0 / (\n+            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n+        )\n+        return inv_freq, attention_factor\n \n     @torch.no_grad()\n     @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n@@ -175,6 +204,7 @@ class Dots1Attention(nn.Module):\n \n     def __init__(self, config: Dots1Config, layer_idx: int):\n         super().__init__()\n+        self.layer_type = config.layer_types[layer_idx] if hasattr(config, \"layer_types\") else None\n         self.config = config\n         self.layer_idx = layer_idx\n         self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n@@ -197,7 +227,7 @@ def __init__(self, config: Dots1Config, layer_idx: int):\n         )\n         self.q_norm = Dots1RMSNorm(self.head_dim, eps=config.rms_norm_eps)  # unlike olmo, only on the head dim!\n         self.k_norm = Dots1RMSNorm(self.head_dim, eps=config.rms_norm_eps)  # thus post q_norm does not need reshape\n-        self.sliding_window = config.sliding_window if config.layer_types[layer_idx] == \"sliding_attention\" else None\n+        self.sliding_window = config.sliding_window if self.layer_type == \"sliding_attention\" else None\n \n     def forward(\n         self,\n@@ -387,7 +417,7 @@ def forward(\n         past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> torch.Tensor:\n         residual = hidden_states\n@@ -506,19 +536,17 @@ def forward(\n                 causal_mask_mapping[\"sliding_attention\"] = create_sliding_window_causal_mask(**mask_kwargs)\n \n         hidden_states = inputs_embeds\n-\n-        # create position embeddings to be shared across the decoder layers\n         position_embeddings = self.rotary_emb(hidden_states, position_ids)\n \n         for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n             hidden_states = decoder_layer(\n                 hidden_states,\n                 attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n+                position_embeddings=position_embeddings,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n-                position_embeddings=position_embeddings,\n                 **kwargs,\n             )\n "
        },
        {
            "sha": "8b57c903dde829f713ef1e2fb5df103cba101793",
            "filename": "src/transformers/models/efficientloftr/configuration_efficientloftr.py",
            "status": "modified",
            "additions": 14,
            "deletions": 20,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fconfiguration_efficientloftr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fconfiguration_efficientloftr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fconfiguration_efficientloftr.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -14,7 +14,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import rope_config_validation\n+from ...modeling_rope_utils import rope_config_validation, standardize_rope_params\n \n \n class EfficientLoFTRConfig(PreTrainedConfig):\n@@ -68,20 +68,13 @@ class EfficientLoFTRConfig(PreTrainedConfig):\n             Kernel size used for the fine feature matching\n         batch_norm_eps (`float`, *optional*, defaults to 1e-05):\n             The epsilon used by the batch normalization layers.\n-        rope_theta (`float`, *optional*, defaults to 10000.0):\n-            The base period of the RoPE embeddings.\n         partial_rotary_factor (`float`, *optional*, defaults to 4.0):\n             Dim factor for the RoPE embeddings, in EfficientLoFTR, frequencies should be generated for\n             the whole hidden_size, so this factor is used to compensate.\n-        rope_scaling (`Dict`, *optional*):\n-            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n-            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n-            accordingly.\n-            Expected contents:\n-                `rope_type` (`str`):\n-                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n-                    'llama3', '2d'], with 'default' being the original RoPE implementation.\n-                `dim` (`int`): The dimension of the RoPE embeddings.\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         fine_matching_slice_dim (`int`, *optional*, defaults to 8):\n             The size of the slice used to divide the fine features for the first and second fine matching stages.\n         fine_matching_regress_temperature (`float`, *optional*, defaults to 10.0):\n@@ -128,9 +121,8 @@ def __init__(\n         coarse_matching_border_removal: int = 2,\n         fine_kernel_size: int = 8,\n         batch_norm_eps: float = 1e-5,\n-        rope_theta: float = 10000.0,\n         partial_rotary_factor: float = 4.0,\n-        rope_scaling: Optional[dict] = None,\n+        rope_parameters: Optional[dict] = None,\n         fine_matching_slice_dim: int = 8,\n         fine_matching_regress_temperature: float = 10.0,\n         initializer_range: float = 0.02,\n@@ -184,14 +176,16 @@ def __init__(\n         self.fine_matching_regress_temperature = fine_matching_regress_temperature\n \n         self.num_key_value_heads = num_attention_heads\n-        self.rope_theta = rope_theta\n-        self.rope_scaling = rope_scaling if rope_scaling is not None else {\"rope_type\": \"default\"}\n-\n-        # for compatibility with \"default\" rope type\n         self.partial_rotary_factor = partial_rotary_factor\n-        rope_config_validation(self)\n-\n         self.initializer_range = initializer_range\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n+\n+        # Validate the correctness of rotary position embeddings parameters\n+        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n+        rope_config_validation(self)\n \n         super().__init__(**kwargs)\n "
        },
        {
            "sha": "07f53e9dee69c34abdffa2868bc8db2e247b7932",
            "filename": "src/transformers/models/efficientloftr/modeling_efficientloftr.py",
            "status": "modified",
            "additions": 48,
            "deletions": 12,
            "changes": 60,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fmodeling_efficientloftr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fmodeling_efficientloftr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fmodeling_efficientloftr.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -84,23 +84,62 @@ def compute_embeddings(inv_freq: torch.Tensor, embed_height: int, embed_width: i\n     return emb\n \n \n+# Copied from transformers.models.llama.modeling_llama.LlamaRotaryEmbedding with Llama->EfficientLoFTR\n class EfficientLoFTRRotaryEmbedding(nn.Module):\n     inv_freq: torch.Tensor  # fix linting for `register_buffer`\n \n+    # Ignore copy\n     def __init__(self, config: EfficientLoFTRConfig, device=None):\n         super().__init__()\n+\n         self.config = config\n-        self.rope_type = config.rope_scaling[\"rope_type\"]\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, _ = self.rope_init_fn(self.config, device)\n-        inv_freq_expanded = inv_freq[None, None, None, :].float().expand(1, 1, 1, -1)\n+        self.rope_type = self.config.rope_parameters[\"rope_type\"]\n+        rope_init_fn: Callable = self.compute_default_rope_parameters\n+        if self.rope_type != \"default\":\n+            rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+        inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n+\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = inv_freq\n+\n+    @staticmethod\n+    # Ignore copy\n+    def compute_default_rope_parameters(\n+        config: Optional[EfficientLoFTRConfig] = None,\n+        device: Optional[\"torch.device\"] = None,\n+        seq_len: Optional[int] = None,\n+    ) -> tuple[\"torch.Tensor\", float]:\n+        \"\"\"\n+        Computes the inverse frequencies according to the original RoPE implementation\n+        Args:\n+            config ([`~transformers.PreTrainedConfig`]):\n+                The model configuration.\n+            device (`torch.device`):\n+                The device to use for initialization of the inverse frequencies.\n+            seq_len (`int`, *optional*):\n+                The current sequence length. Unused for this type of RoPE.\n+        Returns:\n+            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n+            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n+        \"\"\"\n+        base = config.rope_parameters[\"rope_theta\"]\n+        partial_rotary_factor = getattr(config, \"partial_rotary_factor\", 1.0)\n+        head_dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+        dim = int(head_dim * partial_rotary_factor)\n+\n+        attention_factor = 1.0  # Unused in this type of RoPE\n \n-        self.register_buffer(\"inv_freq\", inv_freq_expanded, persistent=False)\n+        # Compute the inverse frequencies\n+        inv_freq = 1.0 / (\n+            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n+        )\n+        return inv_freq, attention_factor\n \n+    # Ignore copy\n     @torch.no_grad()\n     def forward(\n-        self, x: torch.Tensor, position_ids: Optional[tuple[torch.LongTensor, torch.LongTensor]] = None\n+        self, x: torch.Tensor, position_ids: Optional[torch.LongTensor] = None, layer_type=None\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n         feats_height, feats_width = x.shape[-2:]\n         embed_height = (feats_height - self.config.q_aggregation_kernel_size) // self.config.q_aggregation_stride + 1\n@@ -368,9 +407,7 @@ def forward(\n \n         query_states = self.q_proj(hidden_states).view(batch_size, seq_len, -1, dim)\n \n-        is_cross_attention = encoder_hidden_states is not None\n-        current_states = encoder_hidden_states if is_cross_attention else hidden_states\n-\n+        current_states = encoder_hidden_states if encoder_hidden_states is not None else hidden_states\n         key_states = self.k_proj(current_states).view(batch_size, seq_len, -1, dim)\n         value_states = self.v_proj(current_states).view(batch_size, seq_len, -1, self.head_dim).transpose(1, 2)\n \n@@ -480,7 +517,7 @@ def __init__(self, config: EfficientLoFTRConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> torch.Tensor:\n         batch_size, _, embed_dim, height, width = hidden_states.shape\n@@ -515,7 +552,7 @@ def __init__(self, config: EfficientLoFTRConfig):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> torch.Tensor:\n         for layer in self.layers:\n@@ -740,7 +777,6 @@ def forward(\n         coarse_features = self.local_feature_transformer(\n             coarse_features, position_embeddings=position_embeddings, **kwargs\n         )\n-\n         features = (coarse_features,) + tuple(residual_features)\n \n         return BackboneOutput(feature_maps=features)"
        },
        {
            "sha": "634efd227f9e486d863974cec2f09c9462b6a02c",
            "filename": "src/transformers/models/emu3/configuration_emu3.py",
            "status": "modified",
            "additions": 15,
            "deletions": 47,
            "changes": 62,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Femu3%2Fconfiguration_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Femu3%2Fconfiguration_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fconfiguration_emu3.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -14,10 +14,10 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from typing import Any, Optional, Union\n+from typing import Optional, Union\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import rope_config_validation\n+from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n \n \n class Emu3VQVAEConfig(PreTrainedConfig):\n@@ -158,45 +158,10 @@ class Emu3TextConfig(PreTrainedConfig):\n             End of stream token id.\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie weight embeddings\n-        rope_theta (`float`, *optional*, defaults to 1000000.0):\n-            The base period of the RoPE embeddings.\n-        rope_scaling (`Dict`, *optional*):\n-            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n-            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n-            accordingly.\n-            Expected contents:\n-                `rope_type` (`str`):\n-                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n-                    'llama3'], with 'default' being the original RoPE implementation.\n-                `factor` (`float`, *optional*):\n-                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n-                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n-                    original maximum pre-trained length.\n-                `original_max_position_embeddings` (`int`, *optional*):\n-                    Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n-                    pretraining.\n-                `attention_factor` (`float`, *optional*):\n-                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n-                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n-                    `factor` field to infer the suggested value.\n-                `beta_fast` (`float`, *optional*):\n-                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n-                    ramp function. If unspecified, it defaults to 32.\n-                `beta_slow` (`float`, *optional*):\n-                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n-                    ramp function. If unspecified, it defaults to 1.\n-                `short_factor` (`list[float]`, *optional*):\n-                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n-                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n-                    size divided by the number of attention heads divided by 2\n-                `long_factor` (`list[float]`, *optional*):\n-                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n-                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n-                    size divided by the number of attention heads divided by 2\n-                `low_freq_factor` (`float`, *optional*):\n-                    Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n-                `high_freq_factor` (`float`, *optional*):\n-                    Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         mlp_bias (`bool`, *optional*, defaults to `False`):\n             Whether to use a bias in up_proj, down_proj and gate_proj layers in the MLP layers.\n         attention_bias (`bool`, *optional*, defaults to `False`):\n@@ -240,8 +205,7 @@ def __init__(\n         bos_token_id: int = 151849,\n         eos_token_id: int = 151850,\n         tie_word_embeddings: bool = False,\n-        rope_theta: float = 1000000.0,\n-        rope_scaling: Optional[dict[str, Any]] = None,\n+        rope_parameters: Optional[RopeParameters] = None,\n         mlp_bias=False,\n         attention_bias=False,\n         attention_dropout: float = 0.1,\n@@ -258,14 +222,18 @@ def __init__(\n         self.hidden_act = hidden_act\n         self.rms_norm_eps = rms_norm_eps\n         self.use_cache = use_cache\n-        self.rope_theta = rope_theta\n-        self.rope_scaling = rope_scaling\n         self.mlp_bias = mlp_bias\n         self.attention_bias = attention_bias\n         self.initializer_range = initializer_range\n-        rope_config_validation(self)\n-\n         self.attention_dropout = attention_dropout\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n+\n+        # Validate the correctness of rotary position embeddings parameters\n+        rope_theta = kwargs.get(\"rope_theta\", 1000000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n+        rope_config_validation(self)\n \n         super().__init__(\n             pad_token_id=pad_token_id,"
        },
        {
            "sha": "f61cd6a749c6a8e20e5dd6b99a1f9e1160ad71b3",
            "filename": "src/transformers/models/emu3/convert_emu3_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Femu3%2Fconvert_emu3_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Femu3%2Fconvert_emu3_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fconvert_emu3_weights_to_hf.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -284,7 +284,7 @@ def convert_model(vq_model_id, llm_model_id, output_dir, hub_model_id=None, test\n \n     text_config = Emu3TextConfig(\n         max_position_embeddings=model_llm.config.max_position_embeddings,\n-        rope_scaling={\"rope_type\": \"default\"},\n+        rope_parameters={\"rope_type\": \"default\"},\n     )\n     config = Emu3Config(text_config=text_config, vocabulary_map=vocabulary_map)\n "
        },
        {
            "sha": "cdfcad38752fe1dc2e2ccc95d16bb92750b87f76",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 41,
            "deletions": 12,
            "changes": 53,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -145,8 +145,8 @@ def __init__(self, config: Emu3Config, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n-        attention_mask: Optional[torch.Tensor],\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -1112,20 +1112,49 @@ class Emu3RotaryEmbedding(nn.Module):\n \n     def __init__(self, config: Emu3Config, device=None):\n         super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n         self.max_seq_len_cached = config.max_position_embeddings\n         self.original_max_seq_len = config.max_position_embeddings\n \n         self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.rope_type = self.config.rope_parameters[\"rope_type\"]\n+        rope_init_fn: Callable = self.compute_default_rope_parameters\n+        if self.rope_type != \"default\":\n+            rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+        inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n+\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n+        self.original_inv_freq = inv_freq\n+\n+    @staticmethod\n+    def compute_default_rope_parameters(\n+        config: Optional[Emu3Config] = None,\n+        device: Optional[\"torch.device\"] = None,\n+        seq_len: Optional[int] = None,\n+    ) -> tuple[\"torch.Tensor\", float]:\n+        \"\"\"\n+        Computes the inverse frequencies according to the original RoPE implementation\n+        Args:\n+            config ([`~transformers.PreTrainedConfig`]):\n+                The model configuration.\n+            device (`torch.device`):\n+                The device to use for initialization of the inverse frequencies.\n+            seq_len (`int`, *optional*):\n+                The current sequence length. Unused for this type of RoPE.\n+        Returns:\n+            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n+            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n+        \"\"\"\n+        base = config.rope_parameters[\"rope_theta\"]\n+        dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+\n+        attention_factor = 1.0  # Unused in this type of RoPE\n+\n+        # Compute the inverse frequencies\n+        inv_freq = 1.0 / (\n+            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n+        )\n+        return inv_freq, attention_factor\n \n     @torch.no_grad()\n     @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n@@ -1207,16 +1236,16 @@ def forward(\n         )\n \n         hidden_states = inputs_embeds\n-        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids=position_ids)\n \n         for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n             hidden_states = decoder_layer(\n                 hidden_states,\n                 attention_mask=causal_mask,\n+                position_embeddings=position_embeddings,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n                 cache_position=cache_position,\n-                position_embeddings=position_embeddings,\n                 **kwargs,\n             )\n "
        },
        {
            "sha": "03aefe766cf62ad07df4166c49431917f27b4ba2",
            "filename": "src/transformers/models/ernie4_5/configuration_ernie4_5.py",
            "status": "modified",
            "additions": 31,
            "deletions": 64,
            "changes": 95,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fconfiguration_ernie4_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fconfiguration_ernie4_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fconfiguration_ernie4_5.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -13,8 +13,10 @@\n # limitations under the License.\n \"\"\"Ernie 4.5 model configuration\"\"\"\n \n+from typing import Optional\n+\n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import rope_config_validation\n+from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n \n \n class Ernie4_5Config(PreTrainedConfig):\n@@ -66,45 +68,10 @@ class Ernie4_5Config(PreTrainedConfig):\n             End of stream token id.\n         tie_word_embeddings (`bool`, *optional*, defaults to `True`):\n             Whether to tie weight embeddings\n-        rope_theta (`float`, *optional*, defaults to 500000.0):\n-            The base period of the RoPE embeddings.\n-        rope_scaling (`Dict`, *optional*):\n-            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n-            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n-            accordingly.\n-            Expected contents:\n-                `rope_type` (`str`):\n-                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n-                    'llama3'], with 'default' being the original RoPE implementation.\n-                `factor` (`float`, *optional*):\n-                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n-                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n-                    original maximum pre-trained length.\n-                `original_max_position_embeddings` (`int`, *optional*):\n-                    Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n-                    pretraining.\n-                `attention_factor` (`float`, *optional*):\n-                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n-                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n-                    `factor` field to infer the suggested value.\n-                `beta_fast` (`float`, *optional*):\n-                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n-                    ramp function. If unspecified, it defaults to 32.\n-                `beta_slow` (`float`, *optional*):\n-                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n-                    ramp function. If unspecified, it defaults to 1.\n-                `short_factor` (`list[float]`, *optional*):\n-                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n-                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n-                    size divided by the number of attention heads divided by 2\n-                `long_factor` (`list[float]`, *optional*):\n-                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n-                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n-                    size divided by the number of attention heads divided by 2\n-                `low_freq_factor` (`float`, *optional*):\n-                    Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n-                `high_freq_factor` (`float`, *optional*):\n-                    Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         use_bias (`bool`, *optional*, defaults to `False`):\n             Whether to use a bias in any of the projections including mlp and attention for example.\n         head_dim (`int`, *optional*, defaults to 128):\n@@ -143,25 +110,24 @@ class Ernie4_5Config(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size=103424,\n-        hidden_size=1024,\n-        intermediate_size=3072,\n-        num_hidden_layers=18,\n-        num_attention_heads=16,\n-        num_key_value_heads=2,\n-        hidden_act=\"silu\",\n-        max_position_embeddings=131072,\n-        initializer_range=0.02,\n-        rms_norm_eps=1e-05,\n-        use_cache=True,\n-        pad_token_id=0,\n-        bos_token_id=1,\n-        eos_token_id=2,\n-        tie_word_embeddings=True,\n-        rope_theta=500000.0,\n-        rope_scaling=None,\n-        use_bias=False,\n-        head_dim=128,\n+        vocab_size: Optional[int] = 103424,\n+        hidden_size: Optional[int] = 1024,\n+        intermediate_size: Optional[int] = 3072,\n+        num_hidden_layers: Optional[int] = 18,\n+        num_attention_heads: Optional[int] = 16,\n+        num_key_value_heads: Optional[int] = 2,\n+        hidden_act: Optional[str] = \"silu\",\n+        max_position_embeddings: Optional[int] = 131072,\n+        initializer_range: Optional[float] = 0.02,\n+        rms_norm_eps: Optional[int] = 1e-05,\n+        use_cache: Optional[int] = True,\n+        pad_token_id: Optional[int] = 0,\n+        bos_token_id: Optional[int] = 1,\n+        eos_token_id: Optional[int] = 2,\n+        tie_word_embeddings: Optional[bool] = True,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        use_bias: Optional[bool] = False,\n+        head_dim: Optional[int] = 128,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size\n@@ -180,14 +146,15 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.rms_norm_eps = rms_norm_eps\n         self.use_cache = use_cache\n-        self.rope_theta = rope_theta\n-        self.rope_scaling = rope_scaling\n         self.use_bias = use_bias\n         self.head_dim = head_dim if head_dim is not None else self.hidden_size // self.num_attention_heads\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n+\n         # Validate the correctness of rotary position embeddings parameters\n-        # BC: if there is a 'type' field, copy it it to 'rope_type'.\n-        if self.rope_scaling is not None and \"type\" in self.rope_scaling:\n-            self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n+        rope_theta = kwargs.get(\"rope_theta\", 500000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n         rope_config_validation(self)\n \n         super().__init__("
        },
        {
            "sha": "5658c7691c3cbbfa5f4a3e3f61c1d293dce13d25",
            "filename": "src/transformers/models/ernie4_5/modeling_ernie4_5.py",
            "status": "modified",
            "additions": 42,
            "deletions": 13,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fmodeling_ernie4_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fmodeling_ernie4_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fmodeling_ernie4_5.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -44,20 +44,49 @@ class Ernie4_5RotaryEmbedding(nn.Module):\n \n     def __init__(self, config: Ernie4_5Config, device=None):\n         super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n         self.max_seq_len_cached = config.max_position_embeddings\n         self.original_max_seq_len = config.max_position_embeddings\n \n         self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.rope_type = self.config.rope_parameters[\"rope_type\"]\n+        rope_init_fn: Callable = self.compute_default_rope_parameters\n+        if self.rope_type != \"default\":\n+            rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+        inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n+\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n+        self.original_inv_freq = inv_freq\n+\n+    @staticmethod\n+    def compute_default_rope_parameters(\n+        config: Optional[Ernie4_5Config] = None,\n+        device: Optional[\"torch.device\"] = None,\n+        seq_len: Optional[int] = None,\n+    ) -> tuple[\"torch.Tensor\", float]:\n+        \"\"\"\n+        Computes the inverse frequencies according to the original RoPE implementation\n+        Args:\n+            config ([`~transformers.PreTrainedConfig`]):\n+                The model configuration.\n+            device (`torch.device`):\n+                The device to use for initialization of the inverse frequencies.\n+            seq_len (`int`, *optional*):\n+                The current sequence length. Unused for this type of RoPE.\n+        Returns:\n+            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n+            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n+        \"\"\"\n+        base = config.rope_parameters[\"rope_theta\"]\n+        dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+\n+        attention_factor = 1.0  # Unused in this type of RoPE\n+\n+        # Compute the inverse frequencies\n+        inv_freq = 1.0 / (\n+            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n+        )\n+        return inv_freq, attention_factor\n \n     @torch.no_grad()\n     @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n@@ -196,8 +225,8 @@ def __init__(self, config: Ernie4_5Config, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n-        attention_mask: Optional[torch.Tensor],\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -277,7 +306,7 @@ def forward(\n         past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> torch.Tensor:\n         residual = hidden_states\n@@ -381,16 +410,16 @@ def forward(\n         )\n \n         hidden_states = inputs_embeds\n-        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids=position_ids)\n \n         for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n             hidden_states = decoder_layer(\n                 hidden_states,\n                 attention_mask=causal_mask,\n+                position_embeddings=position_embeddings,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n                 cache_position=cache_position,\n-                position_embeddings=position_embeddings,\n                 **kwargs,\n             )\n "
        },
        {
            "sha": "780b07164ec05d65388c572ad121411928a5fe3c",
            "filename": "src/transformers/models/ernie4_5/modular_ernie4_5.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fmodular_ernie4_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fmodular_ernie4_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fmodular_ernie4_5.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -23,12 +23,12 @@\n     LlamaAttention,\n     LlamaForCausalLM,\n     LlamaMLP,\n-    LlamaRotaryEmbedding,\n )\n+from ..olmo.modeling_olmo import OlmoRotaryEmbedding\n from .configuration_ernie4_5 import Ernie4_5Config\n \n \n-class Ernie4_5RotaryEmbedding(LlamaRotaryEmbedding):\n+class Ernie4_5RotaryEmbedding(OlmoRotaryEmbedding):\n     @torch.no_grad()\n     @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n     def forward(self, x, position_ids):"
        },
        {
            "sha": "0fd108a28b4023f147470a7867f2bda1d309f959",
            "filename": "src/transformers/models/ernie4_5_moe/configuration_ernie4_5_moe.py",
            "status": "modified",
            "additions": 39,
            "deletions": 73,
            "changes": 112,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fconfiguration_ernie4_5_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fconfiguration_ernie4_5_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fconfiguration_ernie4_5_moe.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -13,8 +13,10 @@\n # limitations under the License.\n \"\"\"Ernie 4.5 MoE model configuration\"\"\"\n \n+from typing import Optional\n+\n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import rope_config_validation\n+from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n from ...utils import logging\n \n \n@@ -69,45 +71,10 @@ class Ernie4_5_MoeConfig(PreTrainedConfig):\n             relevant if `config.is_decoder=True`.\n         tie_word_embeddings (`bool`, *optional*, defaults to `True`):\n             Whether the model's input and output word embeddings should be tied.\n-        rope_theta (`float`, *optional*, defaults to 500000.0):\n-            The base period of the RoPE embeddings.\n-        rope_scaling (`Dict`, *optional*):\n-            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n-            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n-            accordingly.\n-            Expected contents:\n-                `rope_type` (`str`):\n-                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n-                    'llama3'], with 'default' being the original RoPE implementation.\n-                `factor` (`float`, *optional*):\n-                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n-                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n-                    original maximum pre-trained length.\n-                `original_max_position_embeddings` (`int`, *optional*):\n-                    Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n-                    pretraining.\n-                `attention_factor` (`float`, *optional*):\n-                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n-                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n-                    `factor` field to infer the suggested value.\n-                `beta_fast` (`float`, *optional*):\n-                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n-                    ramp function. If unspecified, it defaults to 32.\n-                `beta_slow` (`float`, *optional*):\n-                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n-                    ramp function. If unspecified, it defaults to 1.\n-                `short_factor` (`list[float]`, *optional*):\n-                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n-                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n-                    size divided by the number of attention heads divided by 2\n-                `long_factor` (`list[float]`, *optional*):\n-                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n-                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n-                    size divided by the number of attention heads divided by 2\n-                `low_freq_factor` (`float`, *optional*):\n-                    Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n-                `high_freq_factor` (`float`, *optional*):\n-                    Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         use_bias (`bool`, *optional*, defaults to `False`):\n             Whether to use a bias in any of the projections including mlp and attention for example.\n         moe_intermediate_size (`int`, *optional*, defaults to 1536):\n@@ -179,34 +146,33 @@ class Ernie4_5_MoeConfig(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size=103424,\n-        pad_token_id=0,\n-        bos_token_id=1,\n-        eos_token_id=2,\n-        hidden_size=2560,\n-        intermediate_size=12288,\n-        num_hidden_layers=28,\n-        num_attention_heads=20,\n-        num_key_value_heads=4,\n-        hidden_act=\"silu\",\n-        max_position_embeddings=131072,\n-        initializer_range=0.02,\n-        rms_norm_eps=1e-5,\n-        use_cache=True,\n-        tie_word_embeddings=True,\n-        rope_theta=500000.0,\n-        rope_scaling=None,\n-        use_bias=False,\n-        moe_intermediate_size=1536,\n-        moe_k=6,\n-        moe_num_experts=64,\n-        moe_num_shared_experts=2,\n-        moe_layer_start_index=1,\n-        moe_layer_end_index=-1,\n-        moe_layer_interval=1,\n-        moe_norm_min=1e-12,\n-        output_router_logits=False,\n-        router_aux_loss_coef=0.001,\n+        vocab_size: Optional[int] = 103424,\n+        pad_token_id: Optional[int] = 0,\n+        bos_token_id: Optional[int] = 1,\n+        eos_token_id: Optional[int] = 2,\n+        hidden_size: Optional[int] = 2560,\n+        intermediate_size: Optional[int] = 12288,\n+        num_hidden_layers: Optional[int] = 28,\n+        num_attention_heads: Optional[int] = 20,\n+        num_key_value_heads: Optional[int] = 4,\n+        hidden_act: Optional[str] = \"silu\",\n+        max_position_embeddings: Optional[int] = 131072,\n+        initializer_range: Optional[float] = 0.02,\n+        rms_norm_eps: Optional[int] = 1e-5,\n+        use_cache: Optional[bool] = True,\n+        tie_word_embeddings: Optional[bool] = True,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        use_bias: Optional[int] = False,\n+        moe_intermediate_size: Optional[int] = 1536,\n+        moe_k: Optional[int] = 6,\n+        moe_num_experts: Optional[int] = 64,\n+        moe_num_shared_experts: Optional[int] = 2,\n+        moe_layer_start_index: Optional[int] = 1,\n+        moe_layer_end_index: Optional[int] = -1,\n+        moe_layer_interval: Optional[int] = 1,\n+        moe_norm_min: Optional[int] = 1e-12,\n+        output_router_logits: Optional[bool] = False,\n+        router_aux_loss_coef: Optional[float] = 0.001,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size\n@@ -221,13 +187,13 @@ def __init__(\n         self.rms_norm_eps = rms_norm_eps\n         self.use_cache = use_cache\n         self.use_bias = use_bias\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n \n-        self.rope_theta = rope_theta\n-        self.rope_scaling = rope_scaling\n         # Validate the correctness of rotary position embeddings parameters\n-        # BC: if there is a 'type' field, move it to 'rope_type'.\n-        if self.rope_scaling is not None and \"type\" in self.rope_scaling:\n-            self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n+        rope_theta = kwargs.get(\"rope_theta\", 500000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n         rope_config_validation(self)\n \n         # MoE arguments"
        },
        {
            "sha": "c2dbd8d436d8b9dc225f71ad327c7c380730077e",
            "filename": "src/transformers/models/ernie4_5_moe/modeling_ernie4_5_moe.py",
            "status": "modified",
            "additions": 40,
            "deletions": 11,
            "changes": 51,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -83,20 +83,49 @@ class Ernie4_5_MoeRotaryEmbedding(nn.Module):\n \n     def __init__(self, config: Ernie4_5_MoeConfig, device=None):\n         super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n         self.max_seq_len_cached = config.max_position_embeddings\n         self.original_max_seq_len = config.max_position_embeddings\n \n         self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.rope_type = self.config.rope_parameters[\"rope_type\"]\n+        rope_init_fn: Callable = self.compute_default_rope_parameters\n+        if self.rope_type != \"default\":\n+            rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+        inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n+\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n+        self.original_inv_freq = inv_freq\n+\n+    @staticmethod\n+    def compute_default_rope_parameters(\n+        config: Optional[Ernie4_5_MoeConfig] = None,\n+        device: Optional[\"torch.device\"] = None,\n+        seq_len: Optional[int] = None,\n+    ) -> tuple[\"torch.Tensor\", float]:\n+        \"\"\"\n+        Computes the inverse frequencies according to the original RoPE implementation\n+        Args:\n+            config ([`~transformers.PreTrainedConfig`]):\n+                The model configuration.\n+            device (`torch.device`):\n+                The device to use for initialization of the inverse frequencies.\n+            seq_len (`int`, *optional*):\n+                The current sequence length. Unused for this type of RoPE.\n+        Returns:\n+            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n+            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n+        \"\"\"\n+        base = config.rope_parameters[\"rope_theta\"]\n+        dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+\n+        attention_factor = 1.0  # Unused in this type of RoPE\n+\n+        # Compute the inverse frequencies\n+        inv_freq = 1.0 / (\n+            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n+        )\n+        return inv_freq, attention_factor\n \n     @torch.no_grad()\n     @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n@@ -218,8 +247,8 @@ def __init__(self, config: Ernie4_5_MoeConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n-        attention_mask: Optional[torch.Tensor],\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -386,7 +415,7 @@ def forward(\n         past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> torch.Tensor:\n         residual = hidden_states"
        },
        {
            "sha": "218bc50ad964d50aae80b44d98666ed355475d61",
            "filename": "src/transformers/models/evolla/configuration_evolla.py",
            "status": "modified",
            "additions": 38,
            "deletions": 39,
            "changes": 77,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fevolla%2Fconfiguration_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fevolla%2Fconfiguration_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fevolla%2Fconfiguration_evolla.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -14,8 +14,10 @@\n # limitations under the License.\n \"\"\"Evolla model configuration\"\"\"\n \n+from typing import Optional\n+\n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import rope_config_validation\n+from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n from ...utils import logging\n \n \n@@ -132,9 +134,7 @@ class EvollaConfig(PreTrainedConfig):\n             just in case (e.g., 512 or 1024 or 2048).\n         rms_norm_eps (`float`, *optional*, defaults to 1e-05):\n             The epsilon value for the RMS-norm layer in the llama model.\n-        rope_theta (`float`, *optional*, defaults to 500000.0):\n-            The threshold value for the RoPE layer in the llama model.\n-        rope_scaling (`float`, *optional*):\n+        rope_parameters (`float`, *optional*):\n             The scaling factor for the RoPE layer in the llama model.\n         attention_bias (`bool`, *optional*, defaults to `False`):\n             Whether to use bias in the attention layer.\n@@ -193,36 +193,35 @@ class EvollaConfig(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        protein_encoder_config=None,\n-        vocab_size=128256,  # llama vocab size\n-        hidden_size=4096,  # llama hidden size\n-        intermediate_size=14336,  # llama intermediate size\n-        num_hidden_layers=32,  # llama num layers\n-        num_attention_heads=32,  # llama num heads\n-        num_key_value_heads=8,  # llama num key-value heads\n-        hidden_act=\"silu\",  # llama activation function\n-        max_position_embeddings=8192,  # llama rope max length\n-        rms_norm_eps=1e-05,\n-        rope_theta=500000.0,\n-        rope_scaling=None,\n-        attention_bias=False,\n-        attention_dropout=0.0,\n-        mlp_bias=False,\n-        aligner_ffn_mult=4,\n-        aligner_enable_bias=True,\n-        aligner_attention_probs_dropout_prob=0.1,\n-        aligner_num_add_layers=8,\n-        resampler_depth=6,\n-        resampler_dim_head=64,\n-        resampler_heads=8,\n-        resampler_num_latents=64,\n-        resampler_ff_mult=4,\n-        initializer_range=0.02,\n-        pad_token_id=None,\n-        bos_token_id=128000,\n-        eos_token_id=128009,\n-        use_cache=False,\n-        tie_word_embeddings=False,\n+        protein_encoder_config: Optional[dict] = None,\n+        vocab_size: Optional[int] = 128256,  # llama vocab size\n+        hidden_size: Optional[int] = 4096,  # llama hidden size\n+        intermediate_size: Optional[int] = 14336,  # llama intermediate size\n+        num_hidden_layers: Optional[int] = 32,  # llama num layers\n+        num_attention_heads: Optional[int] = 32,  # llama num heads\n+        num_key_value_heads: Optional[int] = 8,  # llama num key-value heads\n+        hidden_act: Optional[str] = \"silu\",  # llama activation function\n+        max_position_embeddings: Optional[int] = 8192,  # llama rope max length\n+        rms_norm_eps: Optional[int] = 1e-05,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        attention_bias: Optional[bool] = False,\n+        attention_dropout: Optional[float] = 0.0,\n+        mlp_bias: Optional[bool] = False,\n+        aligner_ffn_mult: Optional[int] = 4,\n+        aligner_enable_bias: Optional[bool] = True,\n+        aligner_attention_probs_dropout_prob: Optional[float] = 0.1,\n+        aligner_num_add_layers: Optional[int] = 8,\n+        resampler_depth: Optional[int] = 6,\n+        resampler_dim_head: Optional[int] = 64,\n+        resampler_heads: Optional[int] = 8,\n+        resampler_num_latents: Optional[int] = 64,\n+        resampler_ff_mult: Optional[int] = 4,\n+        initializer_range: Optional[float] = 0.02,\n+        pad_token_id: Optional[int] = None,\n+        bos_token_id: Optional[int] = 128000,\n+        eos_token_id: Optional[int] = 128009,\n+        use_cache: Optional[bool] = False,\n+        tie_word_embeddings: Optional[bool] = False,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size\n@@ -250,13 +249,13 @@ def __init__(\n         self.resampler_heads = resampler_heads\n         self.resampler_num_latents = resampler_num_latents\n         self.resampler_ff_mult = resampler_ff_mult\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n \n-        self.rope_theta = rope_theta\n-        self.rope_scaling = rope_scaling\n         # Validate the correctness of rotary position embeddings parameters\n-        # BC: if there is a 'type' field, copy it it to 'rope_type'.\n-        if self.rope_scaling is not None and \"type\" in self.rope_scaling:\n-            self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n+        rope_theta = kwargs.get(\"rope_theta\", 500000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n         rope_config_validation(self)\n \n         # Subconfig"
        },
        {
            "sha": "c405df1bb85cad10ccbd2394ddd1b6491d2f058b",
            "filename": "src/transformers/models/evolla/modeling_evolla.py",
            "status": "modified",
            "additions": 43,
            "deletions": 16,
            "changes": 59,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -982,20 +982,49 @@ class EvollaRotaryEmbedding(nn.Module):\n \n     def __init__(self, config: EvollaConfig, device=None):\n         super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n         self.max_seq_len_cached = config.max_position_embeddings\n         self.original_max_seq_len = config.max_position_embeddings\n \n         self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.rope_type = self.config.rope_parameters[\"rope_type\"]\n+        rope_init_fn: Callable = self.compute_default_rope_parameters\n+        if self.rope_type != \"default\":\n+            rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+        inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n+\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n+        self.original_inv_freq = inv_freq\n+\n+    @staticmethod\n+    def compute_default_rope_parameters(\n+        config: Optional[EvollaConfig] = None,\n+        device: Optional[\"torch.device\"] = None,\n+        seq_len: Optional[int] = None,\n+    ) -> tuple[\"torch.Tensor\", float]:\n+        \"\"\"\n+        Computes the inverse frequencies according to the original RoPE implementation\n+        Args:\n+            config ([`~transformers.PreTrainedConfig`]):\n+                The model configuration.\n+            device (`torch.device`):\n+                The device to use for initialization of the inverse frequencies.\n+            seq_len (`int`, *optional*):\n+                The current sequence length. Unused for this type of RoPE.\n+        Returns:\n+            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n+            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n+        \"\"\"\n+        base = config.rope_parameters[\"rope_theta\"]\n+        dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+\n+        attention_factor = 1.0  # Unused in this type of RoPE\n+\n+        # Compute the inverse frequencies\n+        inv_freq = 1.0 / (\n+            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n+        )\n+        return inv_freq, attention_factor\n \n     @torch.no_grad()\n     @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n@@ -1104,8 +1133,8 @@ def __init__(self, config: EvollaConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n-        attention_mask: Optional[torch.Tensor],\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -1164,7 +1193,7 @@ def __init__(self, config: EvollaConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -1268,8 +1297,8 @@ def __init__(self, config: EvollaConfig):\n         )\n \n         self.norm = EvollaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n-        self.rotary_emb = EvollaRotaryEmbedding(config=config)\n         self.gradient_checkpointing = getattr(config, \"gradient_checkpointing\", False)\n+        self.rotary_emb = EvollaRotaryEmbedding(config=config)\n         self.post_init()\n \n     def get_input_embeddings(self):\n@@ -1349,9 +1378,7 @@ def forward(\n         )\n \n         hidden_states = inputs_embeds\n-\n-        # create position embeddings to be shared across the decoder layers\n-        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids=position_ids)\n \n         for decoder_layer in self.layers:\n             hidden_states = decoder_layer(\n@@ -1361,14 +1388,14 @@ def forward(\n                 past_key_values=past_key_values,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n-                position_embeddings=position_embeddings,\n                 protein_kv_states=protein_feats,\n                 structure_kv_states=structure_feats,\n                 msa_kv_states=msa_feats,\n                 protein_batch_mask=protein_batch_mask,\n                 structure_batch_mask=structure_batch_mask,\n                 msa_batch_mask=msa_batch_mask,\n                 query_attn_mask=attention_mask,\n+                position_embeddings=position_embeddings,\n                 **kwargs,\n             )\n "
        },
        {
            "sha": "51d327370ee3c2ed9aaa736c1de30fb3a35712b4",
            "filename": "src/transformers/models/evolla/modular_evolla.py",
            "status": "modified",
            "additions": 4,
            "deletions": 6,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodular_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodular_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodular_evolla.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -669,7 +669,7 @@ def __init__(self, config: EvollaConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -761,8 +761,8 @@ def __init__(self, config: EvollaConfig):\n         )\n \n         self.norm = EvollaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n-        self.rotary_emb = EvollaRotaryEmbedding(config=config)\n         self.gradient_checkpointing = getattr(config, \"gradient_checkpointing\", False)\n+        self.rotary_emb = EvollaRotaryEmbedding(config=config)\n         self.post_init()\n \n     def get_input_embeddings(self):\n@@ -842,9 +842,7 @@ def forward(\n         )\n \n         hidden_states = inputs_embeds\n-\n-        # create position embeddings to be shared across the decoder layers\n-        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids=position_ids)\n \n         for decoder_layer in self.layers:\n             hidden_states = decoder_layer(\n@@ -854,14 +852,14 @@ def forward(\n                 past_key_values=past_key_values,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n-                position_embeddings=position_embeddings,\n                 protein_kv_states=protein_feats,\n                 structure_kv_states=structure_feats,\n                 msa_kv_states=msa_feats,\n                 protein_batch_mask=protein_batch_mask,\n                 structure_batch_mask=structure_batch_mask,\n                 msa_batch_mask=msa_batch_mask,\n                 query_attn_mask=attention_mask,\n+                position_embeddings=position_embeddings,\n                 **kwargs,\n             )\n "
        },
        {
            "sha": "68bdaf5ce9b38d9b3cb050a797a27c23c7a93e12",
            "filename": "src/transformers/models/exaone4/configuration_exaone4.py",
            "status": "modified",
            "additions": 34,
            "deletions": 61,
            "changes": 95,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fexaone4%2Fconfiguration_exaone4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fexaone4%2Fconfiguration_exaone4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fexaone4%2Fconfiguration_exaone4.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -19,7 +19,10 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n+from typing import Optional\n+\n from ...configuration_utils import PreTrainedConfig, layer_type_validation\n+from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n \n \n class Exaone4Config(PreTrainedConfig):\n@@ -69,45 +72,10 @@ class Exaone4Config(PreTrainedConfig):\n             End of stream token id.\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie weight embeddings\n-        rope_theta (`float`, *optional*, defaults to 10000.0):\n-            The base period of the RoPE embeddings.\n-        rope_scaling (`Dict`, *optional*):\n-            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n-            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n-            accordingly.\n-            Expected contents:\n-                `rope_type` (`str`):\n-                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n-                    'llama3'], with 'default' being the original RoPE implementation.\n-                `factor` (`float`, *optional*):\n-                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n-                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n-                    original maximum pre-trained length.\n-                `original_max_position_embeddings` (`int`, *optional*):\n-                    Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n-                    pretraining.\n-                `attention_factor` (`float`, *optional*):\n-                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n-                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n-                    `factor` field to infer the suggested value.\n-                `beta_fast` (`float`, *optional*):\n-                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n-                    ramp function. If unspecified, it defaults to 32.\n-                `beta_slow` (`float`, *optional*):\n-                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n-                    ramp function. If unspecified, it defaults to 1.\n-                `short_factor` (`List[float]`, *optional*):\n-                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n-                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n-                    size divided by the number of attention heads divided by 2\n-                `long_factor` (`List[float]`, *optional*):\n-                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n-                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n-                    size divided by the number of attention heads divided by 2\n-                `low_freq_factor` (`float`, *optional*):\n-                    Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n-                `high_freq_factor` (`float`, *optional*):\n-                    Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n             The dropout ratio for the attention probabilities.\n         sliding_window (`int`, *optional*):\n@@ -161,26 +129,25 @@ class Exaone4Config(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size=102400,\n-        hidden_size=4096,\n-        intermediate_size=16384,\n-        num_hidden_layers=32,\n-        num_attention_heads=32,\n-        num_key_value_heads=32,\n-        hidden_act=\"silu\",\n-        max_position_embeddings=2048,\n-        initializer_range=0.02,\n-        rms_norm_eps=1e-5,\n-        use_cache=True,\n-        bos_token_id=0,\n-        eos_token_id=2,\n-        tie_word_embeddings=False,\n-        rope_theta=10000.0,\n-        rope_scaling=None,\n-        attention_dropout=0.0,\n-        sliding_window=4096,\n-        sliding_window_pattern=4,\n-        layer_types=None,\n+        vocab_size: Optional[int] = 102400,\n+        hidden_size: Optional[int] = 4096,\n+        intermediate_size: Optional[int] = 16384,\n+        num_hidden_layers: Optional[int] = 32,\n+        num_attention_heads: Optional[int] = 32,\n+        num_key_value_heads: Optional[int] = 32,\n+        hidden_act: Optional[str] = \"silu\",\n+        max_position_embeddings: Optional[int] = 2048,\n+        initializer_range: Optional[float] = 0.02,\n+        rms_norm_eps: Optional[int] = 1e-5,\n+        use_cache: Optional[bool] = True,\n+        bos_token_id: Optional[int] = 0,\n+        eos_token_id: Optional[int] = 2,\n+        tie_word_embeddings: Optional[bool] = False,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        attention_dropout: Optional[float] = 0.0,\n+        sliding_window: Optional[int] = 4096,\n+        sliding_window_pattern: Optional[int] = 4,\n+        layer_types: Optional[list[str]] = None,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size\n@@ -195,10 +162,11 @@ def __init__(\n         self.rms_norm_eps = rms_norm_eps\n         self.use_cache = use_cache\n         self.attention_dropout = attention_dropout\n-        self.rope_theta = rope_theta\n-        self.rope_scaling = rope_scaling\n         self.sliding_window = sliding_window\n         self.sliding_window_pattern = sliding_window_pattern\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n \n         self.layer_types = layer_types\n         if self.sliding_window is None:\n@@ -214,6 +182,11 @@ def __init__(\n             self.cache_implementation = \"hybrid\"\n         layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n+        # Validate the correctness of rotary position embeddings parameters\n+        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n+        rope_config_validation(self)\n+\n         super().__init__(\n             bos_token_id=bos_token_id, eos_token_id=eos_token_id, tie_word_embeddings=tie_word_embeddings, **kwargs\n         )"
        },
        {
            "sha": "efc82d192f029fba5a8d77baf3ab9f821b8790e9",
            "filename": "src/transformers/models/exaone4/modeling_exaone4.py",
            "status": "modified",
            "additions": 42,
            "deletions": 12,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodeling_exaone4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodeling_exaone4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodeling_exaone4.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -73,20 +73,49 @@ class Exaone4RotaryEmbedding(nn.Module):\n \n     def __init__(self, config: Exaone4Config, device=None):\n         super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n         self.max_seq_len_cached = config.max_position_embeddings\n         self.original_max_seq_len = config.max_position_embeddings\n \n         self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.rope_type = self.config.rope_parameters[\"rope_type\"]\n+        rope_init_fn: Callable = self.compute_default_rope_parameters\n+        if self.rope_type != \"default\":\n+            rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+        inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n+\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n+        self.original_inv_freq = inv_freq\n+\n+    @staticmethod\n+    def compute_default_rope_parameters(\n+        config: Optional[Exaone4Config] = None,\n+        device: Optional[\"torch.device\"] = None,\n+        seq_len: Optional[int] = None,\n+    ) -> tuple[\"torch.Tensor\", float]:\n+        \"\"\"\n+        Computes the inverse frequencies according to the original RoPE implementation\n+        Args:\n+            config ([`~transformers.PreTrainedConfig`]):\n+                The model configuration.\n+            device (`torch.device`):\n+                The device to use for initialization of the inverse frequencies.\n+            seq_len (`int`, *optional*):\n+                The current sequence length. Unused for this type of RoPE.\n+        Returns:\n+            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n+            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n+        \"\"\"\n+        base = config.rope_parameters[\"rope_theta\"]\n+        dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+\n+        attention_factor = 1.0  # Unused in this type of RoPE\n+\n+        # Compute the inverse frequencies\n+        inv_freq = 1.0 / (\n+            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n+        )\n+        return inv_freq, attention_factor\n \n     @torch.no_grad()\n     @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n@@ -191,7 +220,8 @@ def __init__(self, config: Exaone4Config, layer_idx: int):\n         self.scaling = self.head_dim**-0.5\n         self.sliding_window = config.sliding_window\n         self.sliding_window_pattern = config.sliding_window_pattern\n-        self.is_sliding = config.layer_types[layer_idx] == \"sliding_attention\"\n+        layer_type = config.layer_types[layer_idx] if hasattr(config, \"layer_types\") else None\n+        self.is_sliding = layer_type == \"sliding_attention\"\n \n         self.q_proj = nn.Linear(self.hidden_size, self.num_attention_heads * self.head_dim, bias=False)\n         self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n@@ -208,6 +238,7 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         input_shape = hidden_states.shape[:-1]\n@@ -287,7 +318,7 @@ def forward(\n         past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> torch.Tensor:\n         residual = hidden_states\n@@ -399,19 +430,18 @@ def forward(\n                 causal_mask_mapping[\"sliding_attention\"] = create_sliding_window_causal_mask(**mask_kwargs)\n \n         hidden_states = inputs_embeds\n-\n         position_embeddings = self.rotary_emb(hidden_states, position_ids)\n \n         for i, decoder_layer in enumerate(self.layers):\n             layer_type = self.config.layer_types[i]\n             hidden_states = decoder_layer(\n                 hidden_states,\n-                position_embeddings=position_embeddings,\n                 attention_mask=causal_mask_mapping[layer_type],\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n                 **kwargs,\n             )\n "
        },
        {
            "sha": "d03510d54d46824f6942658e806f0ecf6599d63b",
            "filename": "src/transformers/models/exaone4/modular_exaone4.py",
            "status": "modified",
            "additions": 38,
            "deletions": 66,
            "changes": 104,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodular_exaone4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodular_exaone4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodular_exaone4.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -30,12 +30,14 @@\n     BaseModelOutputWithPast,\n     CausalLMOutputWithPast,\n )\n+from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ...utils import (\n     TransformersKwargs,\n     logging,\n )\n+from ..gemma2.modeling_gemma2 import Gemma2RotaryEmbedding\n from ..llama.modeling_llama import (\n     LlamaForCausalLM,\n     LlamaForQuestionAnswering,\n@@ -44,7 +46,6 @@\n     LlamaModel,\n     LlamaPreTrainedModel,\n     LlamaRMSNorm,\n-    LlamaRotaryEmbedding,\n     apply_rotary_pos_emb,\n     eager_attention_forward,\n )\n@@ -104,45 +105,10 @@ class Exaone4Config(PreTrainedConfig):\n             End of stream token id.\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie weight embeddings\n-        rope_theta (`float`, *optional*, defaults to 10000.0):\n-            The base period of the RoPE embeddings.\n-        rope_scaling (`Dict`, *optional*):\n-            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n-            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n-            accordingly.\n-            Expected contents:\n-                `rope_type` (`str`):\n-                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n-                    'llama3'], with 'default' being the original RoPE implementation.\n-                `factor` (`float`, *optional*):\n-                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n-                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n-                    original maximum pre-trained length.\n-                `original_max_position_embeddings` (`int`, *optional*):\n-                    Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n-                    pretraining.\n-                `attention_factor` (`float`, *optional*):\n-                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n-                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n-                    `factor` field to infer the suggested value.\n-                `beta_fast` (`float`, *optional*):\n-                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n-                    ramp function. If unspecified, it defaults to 32.\n-                `beta_slow` (`float`, *optional*):\n-                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n-                    ramp function. If unspecified, it defaults to 1.\n-                `short_factor` (`List[float]`, *optional*):\n-                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n-                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n-                    size divided by the number of attention heads divided by 2\n-                `long_factor` (`List[float]`, *optional*):\n-                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n-                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n-                    size divided by the number of attention heads divided by 2\n-                `low_freq_factor` (`float`, *optional*):\n-                    Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n-                `high_freq_factor` (`float`, *optional*):\n-                    Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n             The dropout ratio for the attention probabilities.\n         sliding_window (`int`, *optional*):\n@@ -196,26 +162,25 @@ class Exaone4Config(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size=102400,\n-        hidden_size=4096,\n-        intermediate_size=16384,\n-        num_hidden_layers=32,\n-        num_attention_heads=32,\n-        num_key_value_heads=32,\n-        hidden_act=\"silu\",\n-        max_position_embeddings=2048,\n-        initializer_range=0.02,\n-        rms_norm_eps=1e-5,\n-        use_cache=True,\n-        bos_token_id=0,\n-        eos_token_id=2,\n-        tie_word_embeddings=False,\n-        rope_theta=10000.0,\n-        rope_scaling=None,\n-        attention_dropout=0.0,\n-        sliding_window=4096,\n-        sliding_window_pattern=4,\n-        layer_types=None,\n+        vocab_size: Optional[int] = 102400,\n+        hidden_size: Optional[int] = 4096,\n+        intermediate_size: Optional[int] = 16384,\n+        num_hidden_layers: Optional[int] = 32,\n+        num_attention_heads: Optional[int] = 32,\n+        num_key_value_heads: Optional[int] = 32,\n+        hidden_act: Optional[str] = \"silu\",\n+        max_position_embeddings: Optional[int] = 2048,\n+        initializer_range: Optional[float] = 0.02,\n+        rms_norm_eps: Optional[int] = 1e-5,\n+        use_cache: Optional[bool] = True,\n+        bos_token_id: Optional[int] = 0,\n+        eos_token_id: Optional[int] = 2,\n+        tie_word_embeddings: Optional[bool] = False,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        attention_dropout: Optional[float] = 0.0,\n+        sliding_window: Optional[int] = 4096,\n+        sliding_window_pattern: Optional[int] = 4,\n+        layer_types: Optional[list[str]] = None,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size\n@@ -230,10 +195,11 @@ def __init__(\n         self.rms_norm_eps = rms_norm_eps\n         self.use_cache = use_cache\n         self.attention_dropout = attention_dropout\n-        self.rope_theta = rope_theta\n-        self.rope_scaling = rope_scaling\n         self.sliding_window = sliding_window\n         self.sliding_window_pattern = sliding_window_pattern\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n \n         self.layer_types = layer_types\n         if self.sliding_window is None:\n@@ -249,6 +215,11 @@ def __init__(\n             self.cache_implementation = \"hybrid\"\n         layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n+        # Validate the correctness of rotary position embeddings parameters\n+        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n+        rope_config_validation(self)\n+\n         super().__init__(\n             bos_token_id=bos_token_id, eos_token_id=eos_token_id, tie_word_embeddings=tie_word_embeddings, **kwargs\n         )\n@@ -258,7 +229,7 @@ class Exaone4RMSNorm(LlamaRMSNorm):\n     pass\n \n \n-class Exaone4RotaryEmbedding(LlamaRotaryEmbedding):\n+class Exaone4RotaryEmbedding(Gemma2RotaryEmbedding):\n     pass\n \n \n@@ -277,7 +248,8 @@ def __init__(self, config: Exaone4Config, layer_idx: int):\n         self.scaling = self.head_dim**-0.5\n         self.sliding_window = config.sliding_window\n         self.sliding_window_pattern = config.sliding_window_pattern\n-        self.is_sliding = config.layer_types[layer_idx] == \"sliding_attention\"\n+        layer_type = config.layer_types[layer_idx] if hasattr(config, \"layer_types\") else None\n+        self.is_sliding = layer_type == \"sliding_attention\"\n \n         self.q_proj = nn.Linear(self.hidden_size, self.num_attention_heads * self.head_dim, bias=False)\n         self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n@@ -294,6 +266,7 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         input_shape = hidden_states.shape[:-1]\n@@ -412,19 +385,18 @@ def forward(\n                 causal_mask_mapping[\"sliding_attention\"] = create_sliding_window_causal_mask(**mask_kwargs)\n \n         hidden_states = inputs_embeds\n-\n         position_embeddings = self.rotary_emb(hidden_states, position_ids)\n \n         for i, decoder_layer in enumerate(self.layers):\n             layer_type = self.config.layer_types[i]\n             hidden_states = decoder_layer(\n                 hidden_states,\n-                position_embeddings=position_embeddings,\n                 attention_mask=causal_mask_mapping[layer_type],\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n                 **kwargs,\n             )\n "
        },
        {
            "sha": "2a6da686b72e58a791e4b24be610d83c161293ff",
            "filename": "src/transformers/models/falcon/configuration_falcon.py",
            "status": "modified",
            "additions": 38,
            "deletions": 64,
            "changes": 102,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Ffalcon%2Fconfiguration_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Ffalcon%2Fconfiguration_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon%2Fconfiguration_falcon.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -14,7 +14,10 @@\n # limitations under the License.\n \"\"\"Falcon configuration\"\"\"\n \n+from typing import Optional\n+\n from ...configuration_utils import PreTrainedConfig\n+from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n from ...utils import logging\n \n \n@@ -74,45 +77,10 @@ class FalconConfig(PreTrainedConfig):\n         max_position_embeddings (`int`, *optional*, defaults to 2048):\n             The maximum sequence length that this model might ever be used with, when `alibi` is `False`. Pretrained\n             Falcon models with RoPE support up to 2048 tokens.\n-        rope_theta (`float`, *optional*, defaults to 10000.0):\n-            The base period of the RoPE embeddings.\n-        rope_scaling (`Dict`, *optional*):\n-            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n-            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n-            accordingly.\n-            Expected contents:\n-                `rope_type` (`str`):\n-                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n-                    'llama3'], with 'default' being the original RoPE implementation.\n-                `factor` (`float`, *optional*):\n-                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n-                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n-                    original maximum pre-trained length.\n-                `original_max_position_embeddings` (`int`, *optional*):\n-                    Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n-                    pretraining.\n-                `attention_factor` (`float`, *optional*):\n-                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n-                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n-                    `factor` field to infer the suggested value.\n-                `beta_fast` (`float`, *optional*):\n-                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n-                    ramp function. If unspecified, it defaults to 32.\n-                `beta_slow` (`float`, *optional*):\n-                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n-                    ramp function. If unspecified, it defaults to 1.\n-                `short_factor` (`list[float]`, *optional*):\n-                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n-                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n-                    size divided by the number of attention heads divided by 2\n-                `long_factor` (`list[float]`, *optional*):\n-                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n-                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n-                    size divided by the number of attention heads divided by 2\n-                `low_freq_factor` (`float`, *optional*):\n-                    Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n-                `high_freq_factor` (`float`, *optional*):\n-                    Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         bos_token_id (`int`, *optional*, defaults to 11):\n             The id of the \"beginning-of-sequence\" token.\n         eos_token_id (`int`, *optional*, defaults to 11):\n@@ -143,29 +111,28 @@ class FalconConfig(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size=65024,\n-        hidden_size=4544,\n-        num_hidden_layers=32,\n-        num_attention_heads=71,\n-        num_ln_in_parallel_attn=None,\n-        layer_norm_epsilon=1e-5,\n-        initializer_range=0.02,\n-        use_cache=True,\n-        hidden_dropout=0.0,\n-        attention_dropout=0.0,\n-        num_kv_heads=None,\n-        alibi=False,\n-        new_decoder_architecture=False,\n-        multi_query=True,\n-        parallel_attn=True,\n-        bias=False,\n-        max_position_embeddings=2048,\n-        rope_theta=10000.0,\n-        rope_scaling=None,\n-        bos_token_id=11,\n-        eos_token_id=11,\n-        ffn_hidden_size=None,\n-        activation=\"gelu\",\n+        vocab_size: Optional[int] = 65024,\n+        hidden_size: Optional[int] = 4544,\n+        num_hidden_layers: Optional[int] = 32,\n+        num_attention_heads: Optional[int] = 71,\n+        num_ln_in_parallel_attn: Optional[int] = None,\n+        layer_norm_epsilon: Optional[int] = 1e-5,\n+        initializer_range: Optional[float] = 0.02,\n+        use_cache: Optional[bool] = True,\n+        hidden_dropout: Optional[float] = 0.0,\n+        attention_dropout: Optional[float] = 0.0,\n+        num_kv_heads: Optional[int] = None,\n+        alibi: Optional[bool] = False,\n+        new_decoder_architecture: Optional[bool] = False,\n+        multi_query: Optional[bool] = True,\n+        parallel_attn: Optional[bool] = True,\n+        bias: Optional[bool] = False,\n+        max_position_embeddings: Optional[int] = 2048,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        bos_token_id: Optional[int] = 11,\n+        eos_token_id: Optional[int] = 11,\n+        ffn_hidden_size: Optional[int] = None,\n+        activation: Optional[str] = \"gelu\",\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size\n@@ -189,14 +156,21 @@ def __init__(\n         self.bias = bias\n         self.num_ln_in_parallel_attn = num_ln_in_parallel_attn\n         self.max_position_embeddings = max_position_embeddings\n-        self.rope_theta = rope_theta\n-        self.rope_scaling = rope_scaling\n         self.activation = activation\n         if ffn_hidden_size is None:\n             self.ffn_hidden_size = hidden_size * 4\n         else:\n             self.ffn_hidden_size = ffn_hidden_size\n \n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n+\n+        # Validate the correctness of rotary position embeddings parameters\n+        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n+        rope_config_validation(self)\n+\n         super().__init__(bos_token_id=bos_token_id, eos_token_id=eos_token_id, **kwargs)\n \n     @property"
        },
        {
            "sha": "1b89172a19cd6109f1a2ab660143481ad8bfd066",
            "filename": "src/transformers/models/falcon/modeling_falcon.py",
            "status": "modified",
            "additions": 48,
            "deletions": 19,
            "changes": 67,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -15,6 +15,7 @@\n \"\"\"PyTorch Falcon model.\"\"\"\n \n import math\n+from collections.abc import Callable\n from typing import Optional, Union\n \n import torch\n@@ -37,7 +38,10 @@\n     SequenceClassifierOutputWithPast,\n     TokenClassifierOutput,\n )\n-from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n+from ...modeling_rope_utils import (\n+    ROPE_INIT_FUNCTIONS,\n+    dynamic_rope_update,\n+)\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n     auto_docstring,\n@@ -104,20 +108,49 @@ class FalconRotaryEmbedding(nn.Module):\n \n     def __init__(self, config: FalconConfig, device=None):\n         super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n         self.max_seq_len_cached = config.max_position_embeddings\n         self.original_max_seq_len = config.max_position_embeddings\n \n         self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.rope_type = self.config.rope_parameters[\"rope_type\"]\n+        rope_init_fn: Callable = self.compute_default_rope_parameters\n+        if self.rope_type != \"default\":\n+            rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+        inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n+\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n+        self.original_inv_freq = inv_freq\n+\n+    @staticmethod\n+    def compute_default_rope_parameters(\n+        config: Optional[FalconConfig] = None,\n+        device: Optional[\"torch.device\"] = None,\n+        seq_len: Optional[int] = None,\n+    ) -> tuple[\"torch.Tensor\", float]:\n+        \"\"\"\n+        Computes the inverse frequencies according to the original RoPE implementation\n+        Args:\n+            config ([`~transformers.PreTrainedConfig`]):\n+                The model configuration.\n+            device (`torch.device`):\n+                The device to use for initialization of the inverse frequencies.\n+            seq_len (`int`, *optional*):\n+                The current sequence length. Unused for this type of RoPE.\n+        Returns:\n+            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n+            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n+        \"\"\"\n+        base = config.rope_parameters[\"rope_theta\"]\n+        dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+\n+        attention_factor = 1.0  # Unused in this type of RoPE\n+\n+        # Compute the inverse frequencies\n+        inv_freq = 1.0 / (\n+            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n+        )\n+        return inv_freq, attention_factor\n \n     @torch.no_grad()\n     @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n@@ -194,7 +227,7 @@ def __init__(self, config: FalconConfig, layer_idx=None):\n         self.split_size = self.hidden_size\n         self.hidden_dropout = config.hidden_dropout\n         self.max_position_embeddings = config.max_position_embeddings\n-        self.rope_theta = config.rope_theta\n+\n         self.is_causal = True\n         self.layer_idx = layer_idx\n         if layer_idx is None:\n@@ -293,7 +326,7 @@ def forward(\n         use_cache: bool = False,\n         output_attentions: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n     ):\n         fused_qkv = self.query_key_value(hidden_states)  # [batch_size, seq_length, 3 x hidden_size]\n         num_kv_heads = self.num_heads if self.new_decoder_architecture else self.num_kv_heads\n@@ -438,7 +471,7 @@ def forward(\n         use_cache: bool = False,\n         output_attentions: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n     ):\n         fused_qkv = self.query_key_value(hidden_states)  # [batch_size, seq_length, 3 x hidden_size]\n         num_kv_heads = self.num_heads if self.new_decoder_architecture else self.num_kv_heads\n@@ -580,7 +613,7 @@ def forward(\n         use_cache: bool = False,\n         output_attentions: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         **kwargs,\n     ):\n         residual = hidden_states\n@@ -688,10 +721,8 @@ def __init__(self, config: FalconConfig):\n \n         # Final Layer Norm\n         self.ln_f = LayerNorm(self.embed_dim, eps=config.layer_norm_epsilon)\n-\n-        self.rotary_emb = FalconRotaryEmbedding(config=config)\n-\n         self.gradient_checkpointing = False\n+        self.rotary_emb = FalconRotaryEmbedding(config=config)\n \n         # Initialize weights and apply final processing\n         self.post_init()\n@@ -779,9 +810,7 @@ def forward(\n         )\n \n         hidden_states = inputs_embeds\n-\n-        # create position embeddings to be shared across the decoder layers\n-        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids=position_ids)\n \n         all_self_attentions = () if output_attentions else None\n         all_hidden_states = () if output_hidden_states else None"
        },
        {
            "sha": "85a7e76f3901e5110cfb59609b5cfaf9dcf38f87",
            "filename": "src/transformers/models/falcon_h1/configuration_falcon_h1.py",
            "status": "modified",
            "additions": 52,
            "deletions": 47,
            "changes": 99,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fconfiguration_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fconfiguration_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fconfiguration_falcon_h1.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -14,7 +14,10 @@\n # limitations under the License.\n \"\"\"FalconH1 model configuration\"\"\"\n \n+from typing import Optional\n+\n from ...configuration_utils import PreTrainedConfig\n+from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n from ...utils import logging\n \n \n@@ -103,9 +106,7 @@ class FalconH1Config(PreTrainedConfig):\n             Whether to use RMSNorm instead of LayerNorm in the Mamba block\n         projectors_bias (`bool`, *optional*, defaults to `False`):\n             Flag indicating whether or not to use bias in the input and output projections ([\"in_proj\", \"out_proj\"]) of the attention block\n-        rope_theta (`float`, *optional*, defaults to 100000.0):\n-            The theta value used for the RoPE embeddings.\n-        rope_scaling (`float`, *optional*):\n+        rope_parameters (`float`, *optional*):\n             The scaling value used for the RoPE embeddings. If `None`, no scaling is applied.\n         lm_head_multiplier (`float`, *optional*, defaults to 1.0):\n             The multiplier for the LM head. This is used to scale the output of the LM head.\n@@ -133,47 +134,46 @@ class FalconH1Config(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size=128000,\n-        tie_word_embeddings=False,\n-        hidden_size=4096,\n-        intermediate_size=14336,\n-        num_hidden_layers=32,\n-        num_attention_heads=32,\n-        num_key_value_heads=8,\n-        hidden_act=\"silu\",\n-        initializer_range=0.02,\n-        rms_norm_eps=1e-5,\n-        use_cache=True,\n-        num_logits_to_keep=1,\n-        pad_token_id=0,\n-        bos_token_id=1,\n-        eos_token_id=2,\n-        max_position_embeddings=8192,\n-        attention_dropout=0.0,\n-        mamba_d_ssm=1024,\n-        mamba_n_heads=128,\n-        mamba_d_head=\"auto\",\n-        mamba_n_groups=1,\n-        mamba_d_state=256,\n-        mamba_d_conv=4,\n-        mamba_expand=2,\n-        mamba_chunk_size=256,\n-        mamba_conv_bias=True,\n-        mamba_proj_bias=False,\n-        mamba_norm_before_gate=True,\n-        mamba_rms_norm=False,\n-        projectors_bias=False,\n-        rope_theta=100000.0,\n-        rope_scaling=None,\n-        lm_head_multiplier=1.0,\n-        embedding_multiplier=1.0,\n-        mlp_multipliers=None,\n-        key_multiplier=None,\n-        attention_out_multiplier=None,\n-        attention_in_multiplier=None,\n-        ssm_multipliers=None,\n-        ssm_in_multiplier=None,\n-        ssm_out_multiplier=None,\n+        vocab_size: Optional[int] = 128000,\n+        tie_word_embeddings: Optional[bool] = False,\n+        hidden_size: Optional[int] = 4096,\n+        intermediate_size: Optional[int] = 14336,\n+        num_hidden_layers: Optional[int] = 32,\n+        num_attention_heads: Optional[int] = 32,\n+        num_key_value_heads: Optional[int] = 8,\n+        hidden_act: Optional[str] = \"silu\",\n+        initializer_range: Optional[float] = 0.02,\n+        rms_norm_eps: Optional[int] = 1e-5,\n+        use_cache: Optional[int] = True,\n+        num_logits_to_keep: Optional[int] = 1,\n+        pad_token_id: Optional[int] = 0,\n+        bos_token_id: Optional[int] = 1,\n+        eos_token_id: Optional[int] = 2,\n+        max_position_embeddings: Optional[int] = 8192,\n+        attention_dropout: Optional[float] = 0.0,\n+        mamba_d_ssm: Optional[int] = 1024,\n+        mamba_n_heads: Optional[int] = 128,\n+        mamba_d_head: Optional[str] = \"auto\",\n+        mamba_n_groups: Optional[int] = 1,\n+        mamba_d_state: Optional[int] = 256,\n+        mamba_d_conv: Optional[int] = 4,\n+        mamba_expand: Optional[int] = 2,\n+        mamba_chunk_size: Optional[int] = 256,\n+        mamba_conv_bias: Optional[bool] = True,\n+        mamba_proj_bias: Optional[bool] = False,\n+        mamba_norm_before_gate: Optional[bool] = True,\n+        mamba_rms_norm: Optional[bool] = False,\n+        projectors_bias: Optional[bool] = False,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        lm_head_multiplier: Optional[float] = 1.0,\n+        embedding_multiplier: Optional[float] = 1.0,\n+        mlp_multipliers: Optional[int] = None,\n+        key_multiplier: Optional[int] = None,\n+        attention_out_multiplier: Optional[int] = None,\n+        attention_in_multiplier: Optional[int] = None,\n+        ssm_multipliers: Optional[int] = None,\n+        ssm_in_multiplier: Optional[int] = None,\n+        ssm_out_multiplier: Optional[int] = None,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size\n@@ -197,10 +197,15 @@ def __init__(\n \n         self.use_cache = use_cache\n         self.num_logits_to_keep = num_logits_to_keep\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n+\n+        # Validate the correctness of rotary position embeddings parameters\n+        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n+        rope_config_validation(self)\n \n-        self.rope_theta = rope_theta\n-        self.rope_scaling = None\n-        self.rope_scaling = rope_scaling\n         self.projectors_bias = projectors_bias\n         mamba_intermediate = mamba_expand * hidden_size if mamba_d_ssm is None else mamba_d_ssm\n "
        },
        {
            "sha": "451d2f68cb5bf62680b33202743036a8c031b89a",
            "filename": "src/transformers/models/falcon_h1/modeling_falcon_h1.py",
            "status": "modified",
            "additions": 39,
            "deletions": 12,
            "changes": 51,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -227,20 +227,49 @@ class FalconH1RotaryEmbedding(nn.Module):\n \n     def __init__(self, config: FalconH1Config, device=None):\n         super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n         self.max_seq_len_cached = config.max_position_embeddings\n         self.original_max_seq_len = config.max_position_embeddings\n \n         self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.rope_type = self.config.rope_parameters[\"rope_type\"]\n+        rope_init_fn: Callable = self.compute_default_rope_parameters\n+        if self.rope_type != \"default\":\n+            rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+        inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n+\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n+        self.original_inv_freq = inv_freq\n+\n+    @staticmethod\n+    def compute_default_rope_parameters(\n+        config: Optional[FalconH1Config] = None,\n+        device: Optional[\"torch.device\"] = None,\n+        seq_len: Optional[int] = None,\n+    ) -> tuple[\"torch.Tensor\", float]:\n+        \"\"\"\n+        Computes the inverse frequencies according to the original RoPE implementation\n+        Args:\n+            config ([`~transformers.PreTrainedConfig`]):\n+                The model configuration.\n+            device (`torch.device`):\n+                The device to use for initialization of the inverse frequencies.\n+            seq_len (`int`, *optional*):\n+                The current sequence length. Unused for this type of RoPE.\n+        Returns:\n+            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n+            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n+        \"\"\"\n+        base = config.rope_parameters[\"rope_theta\"]\n+        dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+\n+        attention_factor = 1.0  # Unused in this type of RoPE\n+\n+        # Compute the inverse frequencies\n+        inv_freq = 1.0 / (\n+            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n+        )\n+        return inv_freq, attention_factor\n \n     @torch.no_grad()\n     @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n@@ -1085,7 +1114,7 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         **kwargs,\n     ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n@@ -1297,9 +1326,7 @@ def forward(\n             attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n         )\n         mamba_mask = self._update_mamba_mask(attention_mask, cache_position)\n-\n-        # create position embeddings to be shared across the decoder layers\n-        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids=position_ids)\n \n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None"
        },
        {
            "sha": "6dc6da8785679db258dd32ee1e6f8cddc1da9637",
            "filename": "src/transformers/models/falcon_h1/modular_falcon_h1.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodular_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodular_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodular_falcon_h1.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -851,7 +851,7 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         **kwargs,\n     ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n@@ -1063,9 +1063,7 @@ def forward(\n             attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n         )\n         mamba_mask = self._update_mamba_mask(attention_mask, cache_position)\n-\n-        # create position embeddings to be shared across the decoder layers\n-        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids=position_ids)\n \n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None"
        },
        {
            "sha": "0f0f63f2916bbe5ecd504f4c996bb42db4f3940c",
            "filename": "src/transformers/models/flex_olmo/configuration_flex_olmo.py",
            "status": "modified",
            "additions": 35,
            "deletions": 40,
            "changes": 75,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fconfiguration_flex_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fconfiguration_flex_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fconfiguration_flex_olmo.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -19,9 +19,10 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n+from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import rope_config_validation\n+from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n \n \n class FlexOlmoConfig(PreTrainedConfig):\n@@ -73,16 +74,10 @@ class FlexOlmoConfig(PreTrainedConfig):\n             End of stream token id.\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie weight embeddings\n-        rope_theta (`float`, *optional*, defaults to 500000.0):\n-            The base period of the RoPE embeddings.\n-        rope_scaling (`Dict`, *optional*):\n-            Dictionary containing the scaling configuration for the RoPE embeddings. Currently supports two scaling\n-            strategies: linear and dynamic. Their scaling factor must be a float greater than 1. The expected format is\n-            `{\"type\": strategy name, \"factor\": scaling factor}`. When using this flag, don't update\n-            `max_position_embeddings` to the expected new maximum. See the following thread for more information on how\n-            these scaling strategies behave:\n-            https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/. This is an\n-            experimental feature, subject to breaking API changes in future versions.\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):\n             Whether to use a bias in the query, key, value and output projection layers during self-attention.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n@@ -131,30 +126,29 @@ class FlexOlmoConfig(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size=100352,\n-        hidden_size=4096,\n-        intermediate_size=11008,\n-        num_hidden_layers=32,\n-        num_attention_heads=32,\n-        num_key_value_heads=None,\n-        hidden_act=\"silu\",\n-        max_position_embeddings=4096,\n-        initializer_range=0.02,\n-        rms_norm_eps=1e-06,\n-        use_cache=True,\n-        pad_token_id=100277,\n-        bos_token_id=None,\n-        eos_token_id=100257,\n-        tie_word_embeddings=False,\n-        rope_theta=500000.0,\n-        rope_scaling=None,\n-        attention_bias=False,\n-        attention_dropout=0.0,\n-        num_experts_per_tok=5,\n-        num_experts=7,\n-        output_router_logits=False,\n-        router_aux_loss_coef=0.01,\n-        norm_topk_prob=False,\n+        vocab_size: Optional[int] = 100352,\n+        hidden_size: Optional[int] = 4096,\n+        intermediate_size: Optional[int] = 11008,\n+        num_hidden_layers: Optional[int] = 32,\n+        num_attention_heads: Optional[int] = 32,\n+        num_key_value_heads: Optional[int] = None,\n+        hidden_act: Optional[str] = \"silu\",\n+        max_position_embeddings: Optional[int] = 4096,\n+        initializer_range: Optional[float] = 0.02,\n+        rms_norm_eps: Optional[float] = 1e-06,\n+        use_cache: Optional[bool] = True,\n+        pad_token_id: Optional[int] = 100277,\n+        bos_token_id: Optional[int] = None,\n+        eos_token_id: Optional[int] = 100257,\n+        tie_word_embeddings: Optional[bool] = False,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        attention_bias: Optional[bool] = False,\n+        attention_dropout: Optional[float] = 0.0,\n+        num_experts_per_tok: Optional[int] = 5,\n+        num_experts: Optional[int] = 7,\n+        output_router_logits: Optional[bool] = False,\n+        router_aux_loss_coef: Optional[float] = 0.01,\n+        norm_topk_prob: Optional[bool] = False,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size\n@@ -173,19 +167,20 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.rms_norm_eps = rms_norm_eps\n         self.use_cache = use_cache\n-        self.rope_theta = rope_theta\n-        self.rope_scaling = rope_scaling\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n         self.num_experts_per_tok = num_experts_per_tok\n         self.num_experts = num_experts\n         self.output_router_logits = output_router_logits\n         self.router_aux_loss_coef = router_aux_loss_coef\n         self.norm_topk_prob = norm_topk_prob\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n+\n         # Validate the correctness of rotary position embeddings parameters\n-        # BC: if there is a 'type' field, move it to 'rope_type'.\n-        if self.rope_scaling is not None and \"type\" in self.rope_scaling:\n-            self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n+        rope_theta = kwargs.get(\"rope_theta\", 500000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n         rope_config_validation(self)\n \n         super().__init__("
        },
        {
            "sha": "4406e0bf03a998c643ef08c0816005933345385e",
            "filename": "src/transformers/models/flex_olmo/modeling_flex_olmo.py",
            "status": "modified",
            "additions": 39,
            "deletions": 9,
            "changes": 48,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fmodeling_flex_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fmodeling_flex_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fmodeling_flex_olmo.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -66,20 +66,49 @@ class FlexOlmoRotaryEmbedding(nn.Module):\n \n     def __init__(self, config: FlexOlmoConfig, device=None):\n         super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n         self.max_seq_len_cached = config.max_position_embeddings\n         self.original_max_seq_len = config.max_position_embeddings\n \n         self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.rope_type = self.config.rope_parameters[\"rope_type\"]\n+        rope_init_fn: Callable = self.compute_default_rope_parameters\n+        if self.rope_type != \"default\":\n+            rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+        inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n+\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n+        self.original_inv_freq = inv_freq\n+\n+    @staticmethod\n+    def compute_default_rope_parameters(\n+        config: Optional[FlexOlmoConfig] = None,\n+        device: Optional[\"torch.device\"] = None,\n+        seq_len: Optional[int] = None,\n+    ) -> tuple[\"torch.Tensor\", float]:\n+        \"\"\"\n+        Computes the inverse frequencies according to the original RoPE implementation\n+        Args:\n+            config ([`~transformers.PreTrainedConfig`]):\n+                The model configuration.\n+            device (`torch.device`):\n+                The device to use for initialization of the inverse frequencies.\n+            seq_len (`int`, *optional*):\n+                The current sequence length. Unused for this type of RoPE.\n+        Returns:\n+            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n+            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n+        \"\"\"\n+        base = config.rope_parameters[\"rope_theta\"]\n+        dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+\n+        attention_factor = 1.0  # Unused in this type of RoPE\n+\n+        # Compute the inverse frequencies\n+        inv_freq = 1.0 / (\n+            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n+        )\n+        return inv_freq, attention_factor\n \n     @torch.no_grad()\n     @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n@@ -93,7 +122,7 @@ def forward(self, x, position_ids):\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling\n             sin = emb.sin() * self.attention_scaling\n-            return cos, sin\n+        return cos, sin\n \n \n class FlexOlmoMLP(nn.Module):\n@@ -220,6 +249,7 @@ def forward(\n         attention_mask: Optional[torch.Tensor],\n         past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         input_shape = hidden_states.shape[:-1]"
        },
        {
            "sha": "e5c738aa4bc561cc598c496da6d952c2bd66b1a9",
            "filename": "src/transformers/models/flex_olmo/modular_flex_olmo.py",
            "status": "modified",
            "additions": 34,
            "deletions": 36,
            "changes": 70,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fmodular_flex_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fmodular_flex_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fmodular_flex_olmo.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -21,6 +21,7 @@\n from ...cache_utils import Cache, DynamicCache\n from ...masking_utils import create_causal_mask\n from ...modeling_outputs import MoeModelOutputWithPast\n+from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring\n from ...utils.generic import OutputRecorder, check_model_inputs\n@@ -84,16 +85,10 @@ class FlexOlmoConfig(OlmoeConfig):\n             End of stream token id.\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie weight embeddings\n-        rope_theta (`float`, *optional*, defaults to 500000.0):\n-            The base period of the RoPE embeddings.\n-        rope_scaling (`Dict`, *optional*):\n-            Dictionary containing the scaling configuration for the RoPE embeddings. Currently supports two scaling\n-            strategies: linear and dynamic. Their scaling factor must be a float greater than 1. The expected format is\n-            `{\"type\": strategy name, \"factor\": scaling factor}`. When using this flag, don't update\n-            `max_position_embeddings` to the expected new maximum. See the following thread for more information on how\n-            these scaling strategies behave:\n-            https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/. This is an\n-            experimental feature, subject to breaking API changes in future versions.\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):\n             Whether to use a bias in the query, key, value and output projection layers during self-attention.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n@@ -142,30 +137,29 @@ class FlexOlmoConfig(OlmoeConfig):\n \n     def __init__(\n         self,\n-        vocab_size=100352,\n-        hidden_size=4096,\n-        intermediate_size=11008,\n-        num_hidden_layers=32,\n-        num_attention_heads=32,\n-        num_key_value_heads=None,\n-        hidden_act=\"silu\",\n-        max_position_embeddings=4096,\n-        initializer_range=0.02,\n-        rms_norm_eps=1e-06,\n-        use_cache=True,\n-        pad_token_id=100277,\n-        bos_token_id=None,\n-        eos_token_id=100257,\n-        tie_word_embeddings=False,\n-        rope_theta=500000.0,\n-        rope_scaling=None,\n-        attention_bias=False,\n-        attention_dropout=0.0,\n-        num_experts_per_tok=5,\n-        num_experts=7,\n-        output_router_logits=False,\n-        router_aux_loss_coef=0.01,\n-        norm_topk_prob=False,\n+        vocab_size: Optional[int] = 100352,\n+        hidden_size: Optional[int] = 4096,\n+        intermediate_size: Optional[int] = 11008,\n+        num_hidden_layers: Optional[int] = 32,\n+        num_attention_heads: Optional[int] = 32,\n+        num_key_value_heads: Optional[int] = None,\n+        hidden_act: Optional[str] = \"silu\",\n+        max_position_embeddings: Optional[int] = 4096,\n+        initializer_range: Optional[float] = 0.02,\n+        rms_norm_eps: Optional[float] = 1e-06,\n+        use_cache: Optional[bool] = True,\n+        pad_token_id: Optional[int] = 100277,\n+        bos_token_id: Optional[int] = None,\n+        eos_token_id: Optional[int] = 100257,\n+        tie_word_embeddings: Optional[bool] = False,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        attention_bias: Optional[bool] = False,\n+        attention_dropout: Optional[float] = 0.0,\n+        num_experts_per_tok: Optional[int] = 5,\n+        num_experts: Optional[int] = 7,\n+        output_router_logits: Optional[bool] = False,\n+        router_aux_loss_coef: Optional[float] = 0.01,\n+        norm_topk_prob: Optional[bool] = False,\n         **kwargs,\n     ):\n         super().__init__(\n@@ -180,8 +174,7 @@ def __init__(\n             initializer_range=initializer_range,\n             rms_norm_eps=rms_norm_eps,\n             use_cache=use_cache,\n-            rope_theta=rope_theta,\n-            rope_scaling=rope_scaling,\n+            rope_parameters=rope_parameters,\n             attention_bias=attention_bias,\n             attention_dropout=attention_dropout,\n             num_experts_per_tok=num_experts_per_tok,\n@@ -198,6 +191,11 @@ def __init__(\n \n         del self.clip_qkv\n \n+        # Validate the correctness of rotary position embeddings parameters\n+        rope_theta = kwargs.get(\"rope_theta\", 500000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n+        rope_config_validation(self)\n+\n \n # FlexOlmo RMS norm reuses Olmo2 RMS norm, which handles low precision slightly differently than the original Olmoe.\n class FlexOlmoRMSNorm(Olmo2RMSNorm):"
        },
        {
            "sha": "ae3b692cb474d6480d3c36b92847a4879f5b681c",
            "filename": "src/transformers/models/fuyu/configuration_fuyu.py",
            "status": "modified",
            "additions": 40,
            "deletions": 60,
            "changes": 100,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Ffuyu%2Fconfiguration_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Ffuyu%2Fconfiguration_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fconfiguration_fuyu.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -14,7 +14,10 @@\n # limitations under the License.\n \"\"\"Fuyu model configuration\"\"\"\n \n+from typing import Optional\n+\n from ...configuration_utils import PreTrainedConfig\n+from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n from ...utils import logging\n from ..auto import CONFIG_MAPPING, AutoConfig\n \n@@ -64,16 +67,10 @@ class FuyuConfig(PreTrainedConfig):\n             relevant if `config.is_decoder=True`. Whether to tie weight embeddings\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie input and output embeddings.\n-        rope_theta (`float`, *optional*, defaults to 25000.0):\n-            The base period of the RoPE embeddings.\n-        rope_scaling (`Dict`, *optional*):\n-            Dictionary containing the scaling configuration for the RoPE embeddings. Currently supports two scaling\n-            strategies: linear and dynamic. Their scaling factor must be a float greater than 1. The expected format is\n-            `{\"type\": strategy name, \"factor\": scaling factor}`. When using this flag, don't update\n-            `max_position_embeddings` to the expected new maximum. See the following thread for more information on how\n-            these scaling strategies behave:\n-            https://www.reddit.com/r/LocalFuyu/comments/14mrgpr/dynamically_scaled_rope_further_increases/. This is an\n-            experimental feature, subject to breaking API changes in future versions.\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         qk_layernorm (`bool`, *optional*, defaults to `True`):\n             Whether or not to normalize the Queries and Keys after projecting the hidden states\n         hidden_dropout (`float`, *optional*, defaults to 0.0):\n@@ -107,31 +104,30 @@ class FuyuConfig(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size=262144,\n-        hidden_size=4096,\n-        intermediate_size=16384,\n-        num_hidden_layers=36,\n-        num_attention_heads=64,\n-        hidden_act=\"relu2\",\n-        max_position_embeddings=16384,\n-        image_size=300,\n-        patch_size=30,\n-        num_channels=3,\n-        initializer_range=0.02,\n-        layer_norm_eps=1e-5,\n-        use_cache=True,\n-        tie_word_embeddings=False,\n-        rope_theta=25000.0,\n-        rope_scaling=None,\n-        qk_layernorm=True,\n-        hidden_dropout=0.0,\n-        attention_dropout=0.0,\n-        partial_rotary_factor=0.5,\n-        pad_token_id=None,\n-        bos_token_id=1,\n-        eos_token_id=2,\n-        image_token_id=71011,\n-        text_config=None,\n+        vocab_size: Optional[int] = 262144,\n+        hidden_size: Optional[int] = 4096,\n+        intermediate_size: Optional[int] = 16384,\n+        num_hidden_layers: Optional[int] = 36,\n+        num_attention_heads: Optional[int] = 64,\n+        hidden_act: Optional[str] = \"relu2\",\n+        max_position_embeddings: Optional[int] = 16384,\n+        image_size: Optional[int] = 300,\n+        patch_size: Optional[int] = 30,\n+        num_channels: Optional[int] = 3,\n+        initializer_range: Optional[float] = 0.02,\n+        layer_norm_eps: Optional[int] = 1e-5,\n+        use_cache: Optional[bool] = True,\n+        tie_word_embeddings: Optional[bool] = False,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        qk_layernorm: Optional[bool] = True,\n+        hidden_dropout: Optional[float] = 0.0,\n+        attention_dropout: Optional[float] = 0.0,\n+        partial_rotary_factor: Optional[float] = 0.5,\n+        pad_token_id: Optional[int] = None,\n+        bos_token_id: Optional[int] = 1,\n+        eos_token_id: Optional[int] = 2,\n+        image_token_id: Optional[int] = 71011,\n+        text_config: Optional[dict] = None,\n         **kwargs,\n     ):\n         if text_config is None:\n@@ -146,8 +142,7 @@ def __init__(\n                 \"initializer_range\": initializer_range,\n                 \"layer_norm_eps\": layer_norm_eps,\n                 \"use_cache\": use_cache,\n-                \"rope_theta\": rope_theta,\n-                \"rope_scaling\": rope_scaling,\n+                \"rope_parameters\": rope_parameters,\n                 \"qk_layernorm\": qk_layernorm,\n                 \"hidden_dropout\": hidden_dropout,\n                 \"attention_dropout\": attention_dropout,\n@@ -174,14 +169,19 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.layer_norm_eps = layer_norm_eps\n         self.use_cache = use_cache\n-        self.rope_theta = rope_theta\n-        self.rope_scaling = rope_scaling\n         self.qk_layernorm = qk_layernorm\n         self.hidden_dropout = hidden_dropout\n         self.attention_dropout = attention_dropout\n         self.partial_rotary_factor = partial_rotary_factor\n         self.image_token_id = image_token_id\n-        self._rope_scaling_validation()\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n+\n+        # Validate the correctness of rotary position embeddings parameters\n+        rope_theta = kwargs.get(\"rope_theta\", 25000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n+        rope_config_validation(self)\n \n         super().__init__(\n             pad_token_id=pad_token_id,\n@@ -191,25 +191,5 @@ def __init__(\n             **kwargs,\n         )\n \n-    def _rope_scaling_validation(self):\n-        \"\"\"\n-        Validate the `rope_scaling` configuration.\n-        \"\"\"\n-        if self.rope_scaling is None:\n-            return\n-\n-        if not isinstance(self.rope_scaling, dict) or len(self.rope_scaling) != 2:\n-            raise ValueError(\n-                f\"`rope_scaling` must be a dictionary with two fields, `type` and `factor`, got {self.rope_scaling}\"\n-            )\n-        rope_scaling_type = self.rope_scaling.get(\"type\", None)\n-        rope_scaling_factor = self.rope_scaling.get(\"factor\", None)\n-        if rope_scaling_type is None or rope_scaling_type not in [\"linear\", \"dynamic\"]:\n-            raise ValueError(\n-                f\"`rope_scaling`'s type field must be one of ['linear', 'dynamic'], got {rope_scaling_type}\"\n-            )\n-        if rope_scaling_factor is None or not isinstance(rope_scaling_factor, float) or rope_scaling_factor <= 1.0:\n-            raise ValueError(f\"`rope_scaling`'s factor field must be a float > 1, got {rope_scaling_factor}\")\n-\n \n __all__ = [\"FuyuConfig\"]"
        },
        {
            "sha": "986ab2c9aa9469a45a854b6f14574ff642fd9b4f",
            "filename": "src/transformers/models/gemma/configuration_gemma.py",
            "status": "modified",
            "additions": 35,
            "deletions": 31,
            "changes": 66,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fgemma%2Fconfiguration_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fgemma%2Fconfiguration_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fconfiguration_gemma.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -19,7 +19,10 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from ...configuration_utils import PreTrainedConfig, layer_type_validation\n+from typing import Optional\n+\n+from ...configuration_utils import PreTrainedConfig\n+from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n \n \n class GemmaConfig(PreTrainedConfig):\n@@ -72,14 +75,14 @@ class GemmaConfig(PreTrainedConfig):\n             Beginning of stream token id.\n         tie_word_embeddings (`bool`, *optional*, defaults to `True`):\n             Whether to tie weight embeddings\n-        rope_theta (`float`, *optional*, defaults to 10000.0):\n-            The base period of the RoPE embeddings.\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):\n             Whether to use a bias in the query, key, value and output projection layers during self-attention.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n             The dropout ratio for the attention probabilities.\n-        layer_types (`list`, *optional*):\n-            Attention pattern for each layer.\n         use_bidirectional_attention (`bool`, *optional*):\n             If True, the model will attend to all text tokens instead of using a causal mask.\n \n@@ -112,27 +115,26 @@ class GemmaConfig(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size=256000,\n-        hidden_size=3072,\n-        intermediate_size=24576,\n-        num_hidden_layers=28,\n-        num_attention_heads=16,\n-        num_key_value_heads=16,\n-        head_dim=256,\n-        hidden_act=\"gelu_pytorch_tanh\",\n-        max_position_embeddings=8192,\n-        initializer_range=0.02,\n-        rms_norm_eps=1e-6,\n-        use_cache=True,\n-        pad_token_id=0,\n-        eos_token_id=1,\n-        bos_token_id=2,\n-        tie_word_embeddings=True,\n-        rope_theta=10000.0,\n-        attention_bias=False,\n-        attention_dropout=0.0,\n-        layer_types=None,\n-        use_bidirectional_attention=None,\n+        vocab_size: Optional[int] = 256000,\n+        hidden_size: Optional[int] = 3072,\n+        intermediate_size: Optional[int] = 24576,\n+        num_hidden_layers: Optional[int] = 28,\n+        num_attention_heads: Optional[int] = 16,\n+        num_key_value_heads: Optional[int] = 16,\n+        head_dim: Optional[int] = 256,\n+        hidden_act: Optional[str] = \"gelu_pytorch_tanh\",\n+        max_position_embeddings: Optional[int] = 8192,\n+        initializer_range: Optional[float] = 0.02,\n+        rms_norm_eps: Optional[int] = 1e-6,\n+        use_cache: Optional[bool] = True,\n+        pad_token_id: Optional[int] = 0,\n+        eos_token_id: Optional[int] = 1,\n+        bos_token_id: Optional[int] = 2,\n+        tie_word_embeddings: Optional[bool] = True,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        attention_bias: Optional[bool] = False,\n+        attention_dropout: Optional[float] = 0.0,\n+        use_bidirectional_attention: Optional[bool] = None,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size\n@@ -147,15 +149,17 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.rms_norm_eps = rms_norm_eps\n         self.use_cache = use_cache\n-        self.rope_theta = rope_theta\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n         self.use_bidirectional_attention = use_bidirectional_attention\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n \n-        self.layer_types = layer_types\n-        if self.layer_types is None:\n-            self.layer_types = [\"full_attention\" for _ in range(self.num_hidden_layers)]\n-        layer_type_validation(self.layer_types, self.num_hidden_layers)\n+        # Validate the correctness of rotary position embeddings parameters\n+        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n+        rope_config_validation(self)\n \n         super().__init__(\n             pad_token_id=pad_token_id,"
        },
        {
            "sha": "335c2b2cf7b58f49a032ca4c6b15f11e15353691",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 50,
            "deletions": 26,
            "changes": 76,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -84,20 +84,49 @@ class GemmaRotaryEmbedding(nn.Module):\n \n     def __init__(self, config: GemmaConfig, device=None):\n         super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n         self.max_seq_len_cached = config.max_position_embeddings\n         self.original_max_seq_len = config.max_position_embeddings\n \n         self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.rope_type = self.config.rope_parameters[\"rope_type\"]\n+        rope_init_fn: Callable = self.compute_default_rope_parameters\n+        if self.rope_type != \"default\":\n+            rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+        inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n+\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n+        self.original_inv_freq = inv_freq\n+\n+    @staticmethod\n+    def compute_default_rope_parameters(\n+        config: Optional[GemmaConfig] = None,\n+        device: Optional[\"torch.device\"] = None,\n+        seq_len: Optional[int] = None,\n+    ) -> tuple[\"torch.Tensor\", float]:\n+        \"\"\"\n+        Computes the inverse frequencies according to the original RoPE implementation\n+        Args:\n+            config ([`~transformers.PreTrainedConfig`]):\n+                The model configuration.\n+            device (`torch.device`):\n+                The device to use for initialization of the inverse frequencies.\n+            seq_len (`int`, *optional*):\n+                The current sequence length. Unused for this type of RoPE.\n+        Returns:\n+            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n+            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n+        \"\"\"\n+        base = config.rope_parameters[\"rope_theta\"]\n+        dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+\n+        attention_factor = 1.0  # Unused in this type of RoPE\n+\n+        # Compute the inverse frequencies\n+        inv_freq = 1.0 / (\n+            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n+        )\n+        return inv_freq, attention_factor\n \n     @torch.no_grad()\n     @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n@@ -216,8 +245,8 @@ def __init__(self, config: GemmaConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n-        attention_mask: Optional[torch.Tensor],\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -267,7 +296,6 @@ def __init__(self, config: GemmaConfig, layer_idx: int):\n         self.mlp = GemmaMLP(config)\n         self.input_layernorm = GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n-        self.attention_type = config.layer_types[layer_idx]\n \n     def forward(\n         self,\n@@ -277,7 +305,7 @@ def forward(\n         past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> torch.Tensor:\n         residual = hidden_states\n@@ -380,22 +408,18 @@ def forward(\n \n         # It may already have been prepared by e.g. `generate`\n         if not isinstance(causal_mask_mapping := attention_mask, dict):\n-            causal_mask_mapping = {\n-                \"full_attention\": create_causal_mask(\n-                    config=self.config,\n-                    input_embeds=inputs_embeds,\n-                    attention_mask=attention_mask,\n-                    cache_position=cache_position,\n-                    past_key_values=past_key_values,\n-                    position_ids=position_ids,\n-                )\n-            }\n+            causal_mask_mapping = create_causal_mask(\n+                config=self.config,\n+                input_embeds=inputs_embeds,\n+                attention_mask=attention_mask,\n+                cache_position=cache_position,\n+                past_key_values=past_key_values,\n+                position_ids=position_ids,\n+            )\n \n         # embed positions\n         hidden_states = inputs_embeds\n-\n-        # create position embeddings to be shared across the decoder layers\n-        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids=position_ids)\n \n         # normalized\n         # Gemma downcasts the below to float16, causing sqrt(3072)=55.4256 to become 55.5\n@@ -406,7 +430,7 @@ def forward(\n         for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n             hidden_states = decoder_layer(\n                 hidden_states,\n-                attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n+                attention_mask=causal_mask_mapping,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n                 use_cache=use_cache,"
        },
        {
            "sha": "cc4cf066958aa95dd1dbe5ea8569994a95f4b4e7",
            "filename": "src/transformers/models/gemma/modular_gemma.py",
            "status": "modified",
            "additions": 43,
            "deletions": 52,
            "changes": 95,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -20,16 +20,16 @@\n from torch import nn\n \n from ...cache_utils import Cache, DynamicCache\n-from ...configuration_utils import PreTrainedConfig, layer_type_validation\n+from ...configuration_utils import PreTrainedConfig\n from ...masking_utils import create_causal_mask\n from ...modeling_outputs import BaseModelOutputWithPast\n+from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n from ...modeling_utils import PreTrainedModel\n from ...processing_utils import Unpack\n from ...tokenization_utils import AddedToken, PreTrainedTokenizer\n from ...utils import TransformersKwargs, logging\n from ..llama.modeling_llama import (\n     LlamaAttention,\n-    LlamaDecoderLayer,\n     LlamaForCausalLM,\n     LlamaForSequenceClassification,\n     LlamaForTokenClassification,\n@@ -102,14 +102,14 @@ class GemmaConfig(PreTrainedConfig):\n             Beginning of stream token id.\n         tie_word_embeddings (`bool`, *optional*, defaults to `True`):\n             Whether to tie weight embeddings\n-        rope_theta (`float`, *optional*, defaults to 10000.0):\n-            The base period of the RoPE embeddings.\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):\n             Whether to use a bias in the query, key, value and output projection layers during self-attention.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n             The dropout ratio for the attention probabilities.\n-        layer_types (`list`, *optional*):\n-            Attention pattern for each layer.\n         use_bidirectional_attention (`bool`, *optional*):\n             If True, the model will attend to all text tokens instead of using a causal mask.\n \n@@ -142,27 +142,26 @@ class GemmaConfig(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size=256000,\n-        hidden_size=3072,\n-        intermediate_size=24576,\n-        num_hidden_layers=28,\n-        num_attention_heads=16,\n-        num_key_value_heads=16,\n-        head_dim=256,\n-        hidden_act=\"gelu_pytorch_tanh\",\n-        max_position_embeddings=8192,\n-        initializer_range=0.02,\n-        rms_norm_eps=1e-6,\n-        use_cache=True,\n-        pad_token_id=0,\n-        eos_token_id=1,\n-        bos_token_id=2,\n-        tie_word_embeddings=True,\n-        rope_theta=10000.0,\n-        attention_bias=False,\n-        attention_dropout=0.0,\n-        layer_types=None,\n-        use_bidirectional_attention=None,\n+        vocab_size: Optional[int] = 256000,\n+        hidden_size: Optional[int] = 3072,\n+        intermediate_size: Optional[int] = 24576,\n+        num_hidden_layers: Optional[int] = 28,\n+        num_attention_heads: Optional[int] = 16,\n+        num_key_value_heads: Optional[int] = 16,\n+        head_dim: Optional[int] = 256,\n+        hidden_act: Optional[str] = \"gelu_pytorch_tanh\",\n+        max_position_embeddings: Optional[int] = 8192,\n+        initializer_range: Optional[float] = 0.02,\n+        rms_norm_eps: Optional[int] = 1e-6,\n+        use_cache: Optional[bool] = True,\n+        pad_token_id: Optional[int] = 0,\n+        eos_token_id: Optional[int] = 1,\n+        bos_token_id: Optional[int] = 2,\n+        tie_word_embeddings: Optional[bool] = True,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        attention_bias: Optional[bool] = False,\n+        attention_dropout: Optional[float] = 0.0,\n+        use_bidirectional_attention: Optional[bool] = None,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size\n@@ -177,15 +176,17 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.rms_norm_eps = rms_norm_eps\n         self.use_cache = use_cache\n-        self.rope_theta = rope_theta\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n         self.use_bidirectional_attention = use_bidirectional_attention\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n \n-        self.layer_types = layer_types\n-        if self.layer_types is None:\n-            self.layer_types = [\"full_attention\" for _ in range(self.num_hidden_layers)]\n-        layer_type_validation(self.layer_types, self.num_hidden_layers)\n+        # Validate the correctness of rotary position embeddings parameters\n+        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n+        rope_config_validation(self)\n \n         super().__init__(\n             pad_token_id=pad_token_id,\n@@ -392,12 +393,6 @@ def __init__(self, config: GemmaConfig, layer_idx: int):\n         self.is_causal = not getattr(config, \"use_bidirectional_attention\", False)\n \n \n-class GemmaDecoderLayer(LlamaDecoderLayer):\n-    def __init__(self, config: GemmaConfig, layer_idx: int):\n-        super().__init__()\n-        self.attention_type = config.layer_types[layer_idx]\n-\n-\n class GemmaPreTrainedModel(LlamaPreTrainedModel):\n     def _init_weights(self, module):\n         PreTrainedModel._init_weights(self, module)\n@@ -439,22 +434,18 @@ def forward(\n \n         # It may already have been prepared by e.g. `generate`\n         if not isinstance(causal_mask_mapping := attention_mask, dict):\n-            causal_mask_mapping = {\n-                \"full_attention\": create_causal_mask(\n-                    config=self.config,\n-                    input_embeds=inputs_embeds,\n-                    attention_mask=attention_mask,\n-                    cache_position=cache_position,\n-                    past_key_values=past_key_values,\n-                    position_ids=position_ids,\n-                )\n-            }\n+            causal_mask_mapping = create_causal_mask(\n+                config=self.config,\n+                input_embeds=inputs_embeds,\n+                attention_mask=attention_mask,\n+                cache_position=cache_position,\n+                past_key_values=past_key_values,\n+                position_ids=position_ids,\n+            )\n \n         # embed positions\n         hidden_states = inputs_embeds\n-\n-        # create position embeddings to be shared across the decoder layers\n-        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids=position_ids)\n \n         # normalized\n         # Gemma downcasts the below to float16, causing sqrt(3072)=55.4256 to become 55.5\n@@ -465,7 +456,7 @@ def forward(\n         for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n             hidden_states = decoder_layer(\n                 hidden_states,\n-                attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n+                attention_mask=causal_mask_mapping,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n                 use_cache=use_cache,"
        },
        {
            "sha": "7fa77dbb8347e8c4714928b59268b26cece0fdc9",
            "filename": "src/transformers/models/gemma2/configuration_gemma2.py",
            "status": "modified",
            "additions": 40,
            "deletions": 28,
            "changes": 68,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fgemma2%2Fconfiguration_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fgemma2%2Fconfiguration_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fconfiguration_gemma2.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -19,7 +19,10 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n+from typing import Optional\n+\n from ...configuration_utils import PreTrainedConfig, layer_type_validation\n+from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n \n \n class Gemma2Config(PreTrainedConfig):\n@@ -73,8 +76,10 @@ class Gemma2Config(PreTrainedConfig):\n             Beginning of stream token id.\n         tie_word_embeddings (`bool`, *optional*, defaults to `True`):\n             Whether to tie weight embeddings\n-        rope_theta (`float`, *optional*, defaults to 10000.0):\n-            The base period of the RoPE embeddings.\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):\n             Whether to use a bias in the query, key, value and output projection layers during self-attention.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n@@ -121,31 +126,31 @@ class Gemma2Config(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size=256000,\n-        hidden_size=2304,\n-        intermediate_size=9216,\n-        num_hidden_layers=26,\n-        num_attention_heads=8,\n-        num_key_value_heads=4,\n-        head_dim=256,\n-        hidden_activation=\"gelu_pytorch_tanh\",\n-        max_position_embeddings=8192,\n-        initializer_range=0.02,\n-        rms_norm_eps=1e-6,\n-        use_cache=True,\n-        pad_token_id=0,\n-        eos_token_id=1,\n-        bos_token_id=2,\n-        tie_word_embeddings=True,\n-        rope_theta=10000.0,\n-        attention_bias=False,\n-        attention_dropout=0.0,\n-        query_pre_attn_scalar=256,\n-        sliding_window=4096,\n-        layer_types=None,\n-        final_logit_softcapping=30.0,\n-        attn_logit_softcapping=50.0,\n-        use_bidirectional_attention=None,\n+        vocab_size: Optional[int] = 256000,\n+        hidden_size: Optional[int] = 2304,\n+        intermediate_size: Optional[int] = 9216,\n+        num_hidden_layers: Optional[int] = 26,\n+        num_attention_heads: Optional[int] = 8,\n+        num_key_value_heads: Optional[int] = 4,\n+        head_dim: Optional[int] = 256,\n+        hidden_activation: Optional[str] = \"gelu_pytorch_tanh\",\n+        max_position_embeddings: Optional[int] = 8192,\n+        initializer_range: Optional[float] = 0.02,\n+        rms_norm_eps: Optional[int] = 1e-6,\n+        use_cache: Optional[bool] = True,\n+        pad_token_id: Optional[int] = 0,\n+        eos_token_id: Optional[int] = 1,\n+        bos_token_id: Optional[int] = 2,\n+        tie_word_embeddings: Optional[bool] = True,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        attention_bias: Optional[bool] = False,\n+        attention_dropout: Optional[float] = 0.0,\n+        query_pre_attn_scalar: Optional[int] = 256,\n+        sliding_window: Optional[int] = 4096,\n+        layer_types: Optional[list[str]] = None,\n+        final_logit_softcapping: Optional[float] = 30.0,\n+        attn_logit_softcapping: Optional[float] = 50.0,\n+        use_bidirectional_attention: Optional[bool] = None,\n         **kwargs,\n     ):\n         super().__init__(\n@@ -166,7 +171,6 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.rms_norm_eps = rms_norm_eps\n         self.use_cache = use_cache\n-        self.rope_theta = rope_theta\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n         self.hidden_activation = hidden_activation\n@@ -176,12 +180,20 @@ def __init__(\n         self.attn_logit_softcapping = attn_logit_softcapping\n         self.layer_types = layer_types\n         self.use_bidirectional_attention = use_bidirectional_attention\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n \n         if self.layer_types is None:\n             self.layer_types = [\n                 \"sliding_attention\" if bool((i + 1) % 2) else \"full_attention\" for i in range(self.num_hidden_layers)\n             ]\n         layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n+        # Validate the correctness of rotary position embeddings parameters\n+        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n+        rope_config_validation(self)\n+\n \n __all__ = [\"Gemma2Config\"]"
        },
        {
            "sha": "f824053201ad01a2321a2ca058dd49b682b4766c",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 44,
            "deletions": 16,
            "changes": 60,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -88,20 +88,49 @@ class Gemma2RotaryEmbedding(nn.Module):\n \n     def __init__(self, config: Gemma2Config, device=None):\n         super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n         self.max_seq_len_cached = config.max_position_embeddings\n         self.original_max_seq_len = config.max_position_embeddings\n \n         self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.rope_type = self.config.rope_parameters[\"rope_type\"]\n+        rope_init_fn: Callable = self.compute_default_rope_parameters\n+        if self.rope_type != \"default\":\n+            rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+        inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n+\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n+        self.original_inv_freq = inv_freq\n+\n+    @staticmethod\n+    def compute_default_rope_parameters(\n+        config: Optional[Gemma2Config] = None,\n+        device: Optional[\"torch.device\"] = None,\n+        seq_len: Optional[int] = None,\n+    ) -> tuple[\"torch.Tensor\", float]:\n+        \"\"\"\n+        Computes the inverse frequencies according to the original RoPE implementation\n+        Args:\n+            config ([`~transformers.PreTrainedConfig`]):\n+                The model configuration.\n+            device (`torch.device`):\n+                The device to use for initialization of the inverse frequencies.\n+            seq_len (`int`, *optional*):\n+                The current sequence length. Unused for this type of RoPE.\n+        Returns:\n+            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n+            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n+        \"\"\"\n+        base = config.rope_parameters[\"rope_theta\"]\n+        dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+\n+        attention_factor = 1.0  # Unused in this type of RoPE\n+\n+        # Compute the inverse frequencies\n+        inv_freq = 1.0 / (\n+            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n+        )\n+        return inv_freq, attention_factor\n \n     @torch.no_grad()\n     @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n@@ -205,6 +234,7 @@ class Gemma2Attention(nn.Module):\n \n     def __init__(self, config: Gemma2Config, layer_idx: int):\n         super().__init__()\n+        self.layer_type = config.layer_types[layer_idx] if hasattr(config, \"layer_types\") else None\n         self.config = config\n         self.layer_idx = layer_idx\n         self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n@@ -226,13 +256,13 @@ def __init__(self, config: Gemma2Config, layer_idx: int):\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n         self.attn_logit_softcapping = self.config.attn_logit_softcapping\n-        self.sliding_window = config.sliding_window if config.layer_types[layer_idx] == \"sliding_attention\" else None\n+        self.sliding_window = config.sliding_window if self.layer_type == \"sliding_attention\" else None\n \n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n-        attention_mask: Optional[torch.Tensor],\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n@@ -291,7 +321,7 @@ def __init__(self, config: Gemma2Config, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -371,7 +401,7 @@ def __init__(self, config: Gemma2Config):\n             [Gemma2DecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n         )\n         self.norm = Gemma2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n-        self.rotary_emb = Gemma2RotaryEmbedding(config=config)\n+        self.rotary_emb = Gemma2RotaryEmbedding(config)\n         self.gradient_checkpointing = False\n \n         # Initialize weights and apply final processing\n@@ -441,8 +471,6 @@ def forward(\n \n         # embed positions\n         hidden_states = inputs_embeds\n-\n-        # create position embeddings to be shared across the decoder layers\n         position_embeddings = self.rotary_emb(hidden_states, position_ids)\n \n         # normalized\n@@ -461,8 +489,8 @@ def forward(\n \n             layer_outputs = decoder_layer(\n                 hidden_states,\n-                position_embeddings=position_embeddings,\n                 attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n+                position_embeddings=position_embeddings,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n                 output_attentions=output_attentions,"
        },
        {
            "sha": "411c75ac516acbe9fb967a0579128b8ca3eeae78",
            "filename": "src/transformers/models/gemma2/modular_gemma2.py",
            "status": "modified",
            "additions": 82,
            "deletions": 36,
            "changes": 118,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -26,6 +26,13 @@\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n+from ...modeling_rope_utils import (\n+    ROPE_INIT_FUNCTIONS,\n+    RopeParameters,\n+    dynamic_rope_update,\n+    rope_config_validation,\n+    standardize_rope_params,\n+)\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, logging\n@@ -98,8 +105,10 @@ class Gemma2Config(PreTrainedConfig):\n             Beginning of stream token id.\n         tie_word_embeddings (`bool`, *optional*, defaults to `True`):\n             Whether to tie weight embeddings\n-        rope_theta (`float`, *optional*, defaults to 10000.0):\n-            The base period of the RoPE embeddings.\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):\n             Whether to use a bias in the query, key, value and output projection layers during self-attention.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n@@ -146,31 +155,31 @@ class Gemma2Config(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size=256000,\n-        hidden_size=2304,\n-        intermediate_size=9216,\n-        num_hidden_layers=26,\n-        num_attention_heads=8,\n-        num_key_value_heads=4,\n-        head_dim=256,\n-        hidden_activation=\"gelu_pytorch_tanh\",\n-        max_position_embeddings=8192,\n-        initializer_range=0.02,\n-        rms_norm_eps=1e-6,\n-        use_cache=True,\n-        pad_token_id=0,\n-        eos_token_id=1,\n-        bos_token_id=2,\n-        tie_word_embeddings=True,\n-        rope_theta=10000.0,\n-        attention_bias=False,\n-        attention_dropout=0.0,\n-        query_pre_attn_scalar=256,\n-        sliding_window=4096,\n-        layer_types=None,\n-        final_logit_softcapping=30.0,\n-        attn_logit_softcapping=50.0,\n-        use_bidirectional_attention=None,\n+        vocab_size: Optional[int] = 256000,\n+        hidden_size: Optional[int] = 2304,\n+        intermediate_size: Optional[int] = 9216,\n+        num_hidden_layers: Optional[int] = 26,\n+        num_attention_heads: Optional[int] = 8,\n+        num_key_value_heads: Optional[int] = 4,\n+        head_dim: Optional[int] = 256,\n+        hidden_activation: Optional[str] = \"gelu_pytorch_tanh\",\n+        max_position_embeddings: Optional[int] = 8192,\n+        initializer_range: Optional[float] = 0.02,\n+        rms_norm_eps: Optional[int] = 1e-6,\n+        use_cache: Optional[bool] = True,\n+        pad_token_id: Optional[int] = 0,\n+        eos_token_id: Optional[int] = 1,\n+        bos_token_id: Optional[int] = 2,\n+        tie_word_embeddings: Optional[bool] = True,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        attention_bias: Optional[bool] = False,\n+        attention_dropout: Optional[float] = 0.0,\n+        query_pre_attn_scalar: Optional[int] = 256,\n+        sliding_window: Optional[int] = 4096,\n+        layer_types: Optional[list[str]] = None,\n+        final_logit_softcapping: Optional[float] = 30.0,\n+        attn_logit_softcapping: Optional[float] = 50.0,\n+        use_bidirectional_attention: Optional[bool] = None,\n         **kwargs,\n     ):\n         super().__init__(\n@@ -191,7 +200,6 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.rms_norm_eps = rms_norm_eps\n         self.use_cache = use_cache\n-        self.rope_theta = rope_theta\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n         self.hidden_activation = hidden_activation\n@@ -201,13 +209,21 @@ def __init__(\n         self.attn_logit_softcapping = attn_logit_softcapping\n         self.layer_types = layer_types\n         self.use_bidirectional_attention = use_bidirectional_attention\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n \n         if self.layer_types is None:\n             self.layer_types = [\n                 \"sliding_attention\" if bool((i + 1) % 2) else \"full_attention\" for i in range(self.num_hidden_layers)\n             ]\n         layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n+        # Validate the correctness of rotary position embeddings parameters\n+        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n+        rope_config_validation(self)\n+\n \n class Gemma2RMSNorm(GemmaRMSNorm):\n     pass\n@@ -220,7 +236,36 @@ def __init__(self, config):\n \n \n class Gemma2RotaryEmbedding(GemmaRotaryEmbedding):\n-    pass\n+    def __init__(self, config: Gemma2Config, device=None):\n+        nn.Module.__init__()\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+\n+        self.rope_type = self.config.rope_parameters[\"rope_type\"]\n+        rope_init_fn: Callable = self.compute_default_rope_parameters\n+        if self.rope_type != \"default\":\n+            rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+        inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n+\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = inv_freq\n+\n+    @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n+    def forward(self, x, position_ids):\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n \n \n def eager_attention_forward(\n@@ -260,18 +305,20 @@ def eager_attention_forward(\n \n class Gemma2Attention(GemmaAttention):\n     def __init__(self, config: Gemma2Config, layer_idx: int):\n+        self.layer_type = config.layer_types[layer_idx] if hasattr(config, \"layer_types\") else None\n+\n         super().__init__(config, layer_idx)\n         self.attn_logit_softcapping = self.config.attn_logit_softcapping\n         self.attention_dropout = self.config.attention_dropout\n         self.is_causal = not getattr(config, \"use_bidirectional_attention\", False)\n         self.scaling = config.query_pre_attn_scalar**-0.5\n-        self.sliding_window = config.sliding_window if config.layer_types[layer_idx] == \"sliding_attention\" else None\n+        self.sliding_window = config.sliding_window if self.layer_type == \"sliding_attention\" else None\n \n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n-        attention_mask: Optional[torch.Tensor],\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n@@ -330,7 +377,7 @@ def __init__(self, config: Gemma2Config, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -382,6 +429,7 @@ def __init__(self, config: Gemma2Config):\n         self.layers = nn.ModuleList(\n             [Gemma2DecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n         )\n+        self.rotary_emb = Gemma2RotaryEmbedding(config)\n \n     def forward(\n         self,\n@@ -445,8 +493,6 @@ def forward(\n \n         # embed positions\n         hidden_states = inputs_embeds\n-\n-        # create position embeddings to be shared across the decoder layers\n         position_embeddings = self.rotary_emb(hidden_states, position_ids)\n \n         # normalized\n@@ -465,8 +511,8 @@ def forward(\n \n             layer_outputs = decoder_layer(\n                 hidden_states,\n-                position_embeddings=position_embeddings,\n                 attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n+                position_embeddings=position_embeddings,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n                 output_attentions=output_attentions,"
        },
        {
            "sha": "3b9cd24bb46d2c0d04c4f73c0060ef576e9107b5",
            "filename": "src/transformers/models/gemma3/configuration_gemma3.py",
            "status": "modified",
            "additions": 43,
            "deletions": 74,
            "changes": 117,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fgemma3%2Fconfiguration_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fgemma3%2Fconfiguration_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fconfiguration_gemma3.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -22,7 +22,7 @@\n from typing import Any, Optional, Union\n \n from ...configuration_utils import PreTrainedConfig, layer_type_validation\n-from ...modeling_rope_utils import rope_config_validation\n+from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n from ...utils import logging\n from ..siglip import SiglipVisionConfig\n \n@@ -81,8 +81,6 @@ class Gemma3TextConfig(PreTrainedConfig):\n             Beginning of stream token id.\n         tie_word_embeddings (`bool`, *optional*, defaults to `True`):\n             Whether to tie weight embeddings\n-        rope_theta (`float`, *optional*, defaults to 1000000.0):\n-            The base period of the RoPE embeddings.\n         attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):\n             Whether to use a bias in the query, key, value and output projection layers during self-attention.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n@@ -97,45 +95,10 @@ class Gemma3TextConfig(PreTrainedConfig):\n             Scaling factor when applying tanh softcapping on the logits.\n         attn_logit_softcapping (`float`, *optional*):\n             Scaling factor when applying tanh softcapping on the attention scores.\n-        rope_scaling (`Dict`, *optional*):\n-            Dictionary containing the scaling configuration for the RoPE embeddings used in global attention. NOTE: if you apply new rope type\n-            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n-            accordingly.\n-            Expected contents:\n-                `rope_type` (`str`):\n-                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n-                    'llama3'], with 'default' being the original RoPE implementation.\n-                `factor` (`float`, *optional*):\n-                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n-                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n-                    original maximum pre-trained length.\n-                `original_max_position_embeddings` (`int`, *optional*):\n-                    Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n-                    pretraining.\n-                `attention_factor` (`float`, *optional*):\n-                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n-                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n-                    `factor` field to infer the suggested value.\n-                `beta_fast` (`float`, *optional*):\n-                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n-                    ramp function. If unspecified, it defaults to 32.\n-                `beta_slow` (`float`, *optional*):\n-                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n-                    ramp function. If unspecified, it defaults to 1.\n-                `short_factor` (`list[float]`, *optional*):\n-                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n-                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n-                    size divided by the number of attention heads divided by 2\n-                `long_factor` (`list[float]`, *optional*):\n-                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n-                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n-                    size divided by the number of attention heads divided by 2\n-                `low_freq_factor` (`float`, *optional*):\n-                    Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n-                `high_freq_factor` (`float`, *optional*):\n-                    Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n-        rope_local_base_freq (float, *optional*, defaults to 10000.0):\n-            The base period of the RoPE embeddings for local attention.\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         use_bidirectional_attention (`bool`, *optional*, defaults to `False`):\n             If True, the model will attend to all text tokens instead of using a causal mask. This does not change\n             behavior for vision tokens.\n@@ -170,33 +133,31 @@ class Gemma3TextConfig(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size=262_208,\n-        hidden_size=2304,\n-        intermediate_size=9216,\n-        num_hidden_layers=26,\n-        num_attention_heads=8,\n-        num_key_value_heads=4,\n-        head_dim=256,\n-        hidden_activation=\"gelu_pytorch_tanh\",\n-        max_position_embeddings=131_072,\n-        initializer_range=0.02,\n-        rms_norm_eps=1e-6,\n-        use_cache=True,\n-        pad_token_id=0,\n-        eos_token_id=1,\n-        bos_token_id=2,\n-        tie_word_embeddings=True,\n-        rope_theta=1_000_000.0,\n-        attention_bias=False,\n-        attention_dropout=0.0,\n-        query_pre_attn_scalar=256,\n-        sliding_window=4096,\n-        layer_types=None,\n-        final_logit_softcapping=None,\n-        attn_logit_softcapping=None,\n-        rope_scaling=None,\n-        rope_local_base_freq=10_000.0,\n-        use_bidirectional_attention=False,\n+        vocab_size: Optional[int] = 262_208,\n+        hidden_size: Optional[int] = 2304,\n+        intermediate_size: Optional[int] = 9216,\n+        num_hidden_layers: Optional[int] = 26,\n+        num_attention_heads: Optional[int] = 8,\n+        num_key_value_heads: Optional[int] = 4,\n+        head_dim: Optional[int] = 256,\n+        hidden_activation: Optional[str] = \"gelu_pytorch_tanh\",\n+        max_position_embeddings: Optional[int] = 131_072,\n+        initializer_range: Optional[float] = 0.02,\n+        rms_norm_eps: Optional[int] = 1e-6,\n+        use_cache: Optional[bool] = True,\n+        pad_token_id: Optional[int] = 0,\n+        eos_token_id: Optional[int] = 1,\n+        bos_token_id: Optional[int] = 2,\n+        tie_word_embeddings: Optional[bool] = True,\n+        attention_bias: Optional[bool] = False,\n+        attention_dropout: Optional[float] = 0.0,\n+        query_pre_attn_scalar: Optional[int] = 256,\n+        sliding_window: Optional[int] = 4096,\n+        layer_types: Optional[list[str]] = None,\n+        final_logit_softcapping: Optional[float] = None,\n+        attn_logit_softcapping: Optional[float] = None,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        use_bidirectional_attention: Optional[bool] = False,\n         **kwargs,\n     ):\n         super().__init__(\n@@ -217,7 +178,6 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.rms_norm_eps = rms_norm_eps\n         self.use_cache = use_cache\n-        self.rope_theta = rope_theta\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n         self.hidden_activation = hidden_activation\n@@ -226,14 +186,15 @@ def __init__(\n         self.final_logit_softcapping = final_logit_softcapping\n         self.attn_logit_softcapping = attn_logit_softcapping\n         self.layer_types = layer_types\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        if rope_scaling is not None:\n+            rope_parameters = {\"sliding_attention\": {\"rope_type\": \"default\"}, \"full_attention\": rope_scaling}\n+        self.rope_parameters = rope_parameters\n         self.use_bidirectional_attention = use_bidirectional_attention\n         if use_bidirectional_attention:\n             self.sliding_window = (self.sliding_window // 2) + 1  # due to fa we set exclusive bounds\n \n-        self.rope_local_base_freq = rope_local_base_freq\n-        self.rope_scaling = rope_scaling\n-        rope_config_validation(self)\n-\n         # BC -> the pattern used to be a simple int, and it's still present in configs on the Hub\n         self._sliding_window_pattern = kwargs.get(\"sliding_window_pattern\", 6)\n \n@@ -244,6 +205,14 @@ def __init__(\n             ]\n         layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n+        # Validate the correctness of rotary position embeddings parameters\n+        rope_theta = getattr(self, \"rope_theta\", 1_000_000.0)\n+        rope_local_base_freq = getattr(self, \"rope_local_base_freq\", 10000.0)\n+        standardize_rope_params(\n+            self, rope_theta={\"full_attention\": rope_theta, \"sliding_attention\": rope_local_base_freq}\n+        )\n+        rope_config_validation(self)\n+\n \n class Gemma3Config(PreTrainedConfig):\n     r\"\"\""
        },
        {
            "sha": "b4b00dc22ec87e5063ebbe3ebe8955ab1dd3615c",
            "filename": "src/transformers/models/gemma3/convert_gemma3_weights.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fgemma3%2Fconvert_gemma3_weights.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fgemma3%2Fconvert_gemma3_weights.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fconvert_gemma3_weights.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -141,7 +141,7 @@\n             max_position_embeddings=1024,\n             query_pre_attn_scalar=256,\n             sliding_window=512,\n-            rope_scaling=None,\n+            rope_parameters=None,\n             use_bidirectional_attention=True,\n         ),\n         vision_config=None,\n@@ -158,7 +158,7 @@\n             max_position_embeddings=32768,\n             query_pre_attn_scalar=256,\n             sliding_window=512,\n-            rope_scaling=None,\n+            rope_parameters=None,\n         ),\n         vision_config=None,\n     ),\n@@ -190,7 +190,7 @@\n             num_hidden_layers=34,\n             num_key_value_heads=4,\n             sliding_window=1024,\n-            rope_scaling={\"rope_type\": \"linear\", \"factor\": 8.0},  # used for global RoPE only\n+            rope_parameters={\"rope_type\": \"linear\", \"factor\": 8.0},  # used for global RoPE only\n             rope_theta=1_000_000,\n             rope_local_base_freq=10_000,\n             attn_logit_softcapping=None,\n@@ -208,7 +208,7 @@\n             num_hidden_layers=48,\n             num_key_value_heads=8,\n             sliding_window=1024,\n-            rope_scaling={\"rope_type\": \"linear\", \"factor\": 8.0},  # used for global RoPE only\n+            rope_parameters={\"rope_type\": \"linear\", \"factor\": 8.0},  # used for global RoPE only\n             rope_theta=1_000_000,\n             rope_local_base_freq=10_000,\n             attn_logit_softcapping=None,\n@@ -226,7 +226,7 @@\n             num_key_value_heads=16,\n             head_dim=128,\n             sliding_window=1024,\n-            rope_scaling={\"rope_type\": \"linear\", \"factor\": 8.0},  # used for global RoPE only\n+            rope_parameters={\"rope_type\": \"linear\", \"factor\": 8.0},  # used for global RoPE only\n             rope_theta=1_000_000,\n             rope_local_base_freq=10_000,\n             attn_logit_softcapping=None,"
        },
        {
            "sha": "396ae4718412616ddcdace3a1b9487e5f3c44da1",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 71,
            "deletions": 44,
            "changes": 115,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -19,7 +19,6 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-import copy\n from collections.abc import Callable\n from dataclasses import dataclass\n from typing import Optional, Union\n@@ -145,35 +144,80 @@ def extra_repr(self):\n class Gemma3RotaryEmbedding(nn.Module):\n     inv_freq: torch.Tensor  # fix linting for `register_buffer`\n \n-    def __init__(self, config: Gemma3TextConfig, device=None):\n+    def __init__(self, config: Gemma3TextConfig, device=None, layer_type=None):\n         super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n         self.max_seq_len_cached = config.max_position_embeddings\n         self.original_max_seq_len = config.max_position_embeddings\n \n         self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n-        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n+        self.layer_types = list(set(config.layer_types))\n+        self.rope_type = {}\n+        for layer_type in self.layer_types:\n+            rope_params = self.config.rope_parameters[layer_type]\n+            if rope_params is None:\n+                continue\n+\n+            self.rope_type[layer_type] = rope_params[\"rope_type\"]\n+            rope_init_fn: Callable = self.compute_default_rope_parameters\n+            if self.rope_type[layer_type] != \"default\":\n+                rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type[layer_type]]\n+            curr_inv_freq, curr_attention_scaling = rope_init_fn(self.config, device, layer_type=layer_type)\n+            self.register_buffer(f\"{layer_type}_inv_freq\", curr_inv_freq, persistent=False)\n+            setattr(self, f\"{layer_type}_original_inv_freq\", curr_inv_freq)\n+            setattr(self, f\"{layer_type}_attention_scaling\", curr_attention_scaling)\n+\n+    @staticmethod\n+    def compute_default_rope_parameters(\n+        config: Optional[Gemma3TextConfig] = None,\n+        device: Optional[\"torch.device\"] = None,\n+        seq_len: Optional[int] = None,\n+        layer_type: Optional[str] = None,\n+    ) -> tuple[\"torch.Tensor\", float]:\n+        \"\"\"\n+        Computes the inverse frequencies according to the original RoPE implementation\n+        Args:\n+            config ([`~transformers.PreTrainedConfig`]):\n+                The model configuration.\n+            device (`torch.device`):\n+                The device to use for initialization of the inverse frequencies.\n+            seq_len (`int`, *optional*):\n+                The current sequence length. Unused for this type of RoPE.\n+            layer_type (`str`, *optional*):\n+                The current layer type if the model has different RoPE parameters per type.\n+                Should not be used unless `config.layer_types is not None`\n+\n+        Returns:\n+            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n+            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n+        \"\"\"\n+        # For backward compatibility standardize the `rope_parameters_dict` if it uses old format\n+        base = config.rope_parameters[layer_type][\"rope_theta\"]\n+        dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+\n+        attention_factor = 1.0  # Unused in this type of RoPE\n+\n+        # Compute the inverse frequencies\n+        inv_freq = 1.0 / (\n+            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n+        )\n+        return inv_freq, attention_factor\n \n     @torch.no_grad()\n     @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n-    def forward(self, x, position_ids):\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n+    def forward(self, x, position_ids, layer_type=None):\n+        inv_freq = getattr(self, f\"{layer_type}_inv_freq\")\n+        attention_scaling = getattr(self, f\"{layer_type}_attention_scaling\")\n+\n+        inv_freq_expanded = inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n         with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos() * self.attention_scaling\n-            sin = emb.sin() * self.attention_scaling\n+            cos = emb.cos() * attention_scaling\n+            sin = emb.sin() * attention_scaling\n \n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n \n@@ -264,7 +308,7 @@ class Gemma3Attention(nn.Module):\n \n     def __init__(self, config: Gemma3TextConfig, layer_idx: int):\n         super().__init__()\n-        self.is_sliding = config.layer_types[layer_idx] == \"sliding_attention\"\n+        self.layer_type = config.layer_types[layer_idx] if hasattr(config, \"layer_types\") else None\n         self.config = config\n         self.layer_idx = layer_idx\n         self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n@@ -286,16 +330,17 @@ def __init__(self, config: Gemma3TextConfig, layer_idx: int):\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n         self.attn_logit_softcapping = self.config.attn_logit_softcapping\n-        self.sliding_window = config.sliding_window if self.is_sliding else None\n+        self.sliding_window = config.sliding_window if self.layer_type == \"sliding_attention\" else None\n+        self.is_sliding = self.layer_type == \"sliding_attention\"\n \n         self.q_norm = Gemma3RMSNorm(dim=config.head_dim, eps=config.rms_norm_eps)\n         self.k_norm = Gemma3RMSNorm(dim=config.head_dim, eps=config.rms_norm_eps)\n \n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor],\n+        position_embeddings: torch.Tensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n@@ -356,8 +401,7 @@ def __init__(self, config: Gemma3TextConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings_global: torch.Tensor,\n-        position_embeddings_local: torch.Tensor,\n+        position_embeddings: torch.Tensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -370,12 +414,6 @@ def forward(\n \n         hidden_states = self.input_layernorm(hidden_states)\n \n-        # apply global RoPE to non-sliding layer only\n-        if self.self_attn.is_sliding:\n-            position_embeddings = position_embeddings_local\n-        else:\n-            position_embeddings = position_embeddings_global\n-\n         hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             position_embeddings=position_embeddings,\n@@ -468,16 +506,9 @@ def __init__(self, config: Gemma3TextConfig):\n             [Gemma3DecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n         )\n         self.norm = Gemma3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n-        self.rotary_emb = Gemma3RotaryEmbedding(config=config)\n+        self.rotary_emb = Gemma3RotaryEmbedding(config)\n         self.gradient_checkpointing = False\n \n-        # TODO: raushan fix this after RoPE refactor. For now we hack it by reassigning thetas\n-        # when we want to create a local RoPE layer. Config defaults should hold values for global RoPE\n-        config = copy.deepcopy(config)\n-        config.rope_theta = config.rope_local_base_freq\n-        config.rope_scaling = {\"rope_type\": \"default\"}\n-        self.rotary_emb_local = Gemma3RotaryEmbedding(config=config)\n-\n         # Initialize weights and apply final processing\n         self.post_init()\n \n@@ -520,9 +551,7 @@ def forward(\n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n             cache_position = torch.arange(\n-                past_seen_tokens,\n-                past_seen_tokens + inputs_embeds.shape[1],\n-                device=inputs_embeds.device,\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n             )\n \n         if position_ids is None:\n@@ -553,10 +582,9 @@ def forward(\n \n         # embed positions\n         hidden_states = inputs_embeds\n-\n-        # create position embeddings to be shared across the decoder layers\n-        position_embeddings_global = self.rotary_emb(hidden_states, position_ids)\n-        position_embeddings_local = self.rotary_emb_local(hidden_states, position_ids)\n+        position_embeddings = {}\n+        for layer_type in self.config.layer_types:\n+            position_embeddings[layer_type] = self.rotary_emb(hidden_states, position_ids, layer_type)\n \n         # decoder layers\n         all_hidden_states = () if output_hidden_states else None\n@@ -568,9 +596,8 @@ def forward(\n \n             layer_outputs = decoder_layer(\n                 hidden_states,\n-                position_embeddings_global=position_embeddings_global,\n-                position_embeddings_local=position_embeddings_local,\n                 attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n+                position_embeddings=position_embeddings[decoder_layer.attention_type],\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n                 output_attentions=output_attentions,"
        },
        {
            "sha": "a6a867f71ab62123d4756d126911993f477bf090",
            "filename": "src/transformers/models/gemma3/modular_gemma3.py",
            "status": "modified",
            "additions": 135,
            "deletions": 106,
            "changes": 241,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -13,7 +13,6 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-import copy\n from collections.abc import Callable\n from typing import Any, Optional, Union\n \n@@ -26,7 +25,13 @@\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GenericForSequenceClassification, GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, SequenceClassifierOutputWithPast\n-from ...modeling_rope_utils import rope_config_validation\n+from ...modeling_rope_utils import (\n+    ROPE_INIT_FUNCTIONS,\n+    RopeParameters,\n+    dynamic_rope_update,\n+    rope_config_validation,\n+    standardize_rope_params,\n+)\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n@@ -106,8 +111,6 @@ class Gemma3TextConfig(Gemma2Config, PreTrainedConfig):\n             Beginning of stream token id.\n         tie_word_embeddings (`bool`, *optional*, defaults to `True`):\n             Whether to tie weight embeddings\n-        rope_theta (`float`, *optional*, defaults to 1000000.0):\n-            The base period of the RoPE embeddings.\n         attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):\n             Whether to use a bias in the query, key, value and output projection layers during self-attention.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n@@ -122,45 +125,10 @@ class Gemma3TextConfig(Gemma2Config, PreTrainedConfig):\n             Scaling factor when applying tanh softcapping on the logits.\n         attn_logit_softcapping (`float`, *optional*):\n             Scaling factor when applying tanh softcapping on the attention scores.\n-        rope_scaling (`Dict`, *optional*):\n-            Dictionary containing the scaling configuration for the RoPE embeddings used in global attention. NOTE: if you apply new rope type\n-            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n-            accordingly.\n-            Expected contents:\n-                `rope_type` (`str`):\n-                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n-                    'llama3'], with 'default' being the original RoPE implementation.\n-                `factor` (`float`, *optional*):\n-                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n-                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n-                    original maximum pre-trained length.\n-                `original_max_position_embeddings` (`int`, *optional*):\n-                    Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n-                    pretraining.\n-                `attention_factor` (`float`, *optional*):\n-                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n-                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n-                    `factor` field to infer the suggested value.\n-                `beta_fast` (`float`, *optional*):\n-                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n-                    ramp function. If unspecified, it defaults to 32.\n-                `beta_slow` (`float`, *optional*):\n-                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n-                    ramp function. If unspecified, it defaults to 1.\n-                `short_factor` (`list[float]`, *optional*):\n-                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n-                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n-                    size divided by the number of attention heads divided by 2\n-                `long_factor` (`list[float]`, *optional*):\n-                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n-                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n-                    size divided by the number of attention heads divided by 2\n-                `low_freq_factor` (`float`, *optional*):\n-                    Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n-                `high_freq_factor` (`float`, *optional*):\n-                    Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n-        rope_local_base_freq (float, *optional*, defaults to 10000.0):\n-            The base period of the RoPE embeddings for local attention.\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         use_bidirectional_attention (`bool`, *optional*, defaults to `False`):\n             If True, the model will attend to all text tokens instead of using a causal mask. This does not change\n             behavior for vision tokens.\n@@ -180,33 +148,31 @@ class Gemma3TextConfig(Gemma2Config, PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size=262_208,\n-        hidden_size=2304,\n-        intermediate_size=9216,\n-        num_hidden_layers=26,\n-        num_attention_heads=8,\n-        num_key_value_heads=4,\n-        head_dim=256,\n-        hidden_activation=\"gelu_pytorch_tanh\",\n-        max_position_embeddings=131_072,\n-        initializer_range=0.02,\n-        rms_norm_eps=1e-6,\n-        use_cache=True,\n-        pad_token_id=0,\n-        eos_token_id=1,\n-        bos_token_id=2,\n-        tie_word_embeddings=True,\n-        rope_theta=1_000_000.0,\n-        attention_bias=False,\n-        attention_dropout=0.0,\n-        query_pre_attn_scalar=256,\n-        sliding_window=4096,\n-        layer_types=None,\n-        final_logit_softcapping=None,\n-        attn_logit_softcapping=None,\n-        rope_scaling=None,\n-        rope_local_base_freq=10_000.0,\n-        use_bidirectional_attention=False,\n+        vocab_size: Optional[int] = 262_208,\n+        hidden_size: Optional[int] = 2304,\n+        intermediate_size: Optional[int] = 9216,\n+        num_hidden_layers: Optional[int] = 26,\n+        num_attention_heads: Optional[int] = 8,\n+        num_key_value_heads: Optional[int] = 4,\n+        head_dim: Optional[int] = 256,\n+        hidden_activation: Optional[str] = \"gelu_pytorch_tanh\",\n+        max_position_embeddings: Optional[int] = 131_072,\n+        initializer_range: Optional[float] = 0.02,\n+        rms_norm_eps: Optional[int] = 1e-6,\n+        use_cache: Optional[bool] = True,\n+        pad_token_id: Optional[int] = 0,\n+        eos_token_id: Optional[int] = 1,\n+        bos_token_id: Optional[int] = 2,\n+        tie_word_embeddings: Optional[bool] = True,\n+        attention_bias: Optional[bool] = False,\n+        attention_dropout: Optional[float] = 0.0,\n+        query_pre_attn_scalar: Optional[int] = 256,\n+        sliding_window: Optional[int] = 4096,\n+        layer_types: Optional[list[str]] = None,\n+        final_logit_softcapping: Optional[float] = None,\n+        attn_logit_softcapping: Optional[float] = None,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        use_bidirectional_attention: Optional[bool] = False,\n         **kwargs,\n     ):\n         PreTrainedConfig.__init__(\n@@ -227,7 +193,6 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.rms_norm_eps = rms_norm_eps\n         self.use_cache = use_cache\n-        self.rope_theta = rope_theta\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n         self.hidden_activation = hidden_activation\n@@ -236,14 +201,15 @@ def __init__(\n         self.final_logit_softcapping = final_logit_softcapping\n         self.attn_logit_softcapping = attn_logit_softcapping\n         self.layer_types = layer_types\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        if rope_scaling is not None:\n+            rope_parameters = {\"sliding_attention\": {\"rope_type\": \"default\"}, \"full_attention\": rope_scaling}\n+        self.rope_parameters = rope_parameters\n         self.use_bidirectional_attention = use_bidirectional_attention\n         if use_bidirectional_attention:\n             self.sliding_window = (self.sliding_window // 2) + 1  # due to fa we set exclusive bounds\n \n-        self.rope_local_base_freq = rope_local_base_freq\n-        self.rope_scaling = rope_scaling\n-        rope_config_validation(self)\n-\n         # BC -> the pattern used to be a simple int, and it's still present in configs on the Hub\n         self._sliding_window_pattern = kwargs.get(\"sliding_window_pattern\", 6)\n \n@@ -254,6 +220,14 @@ def __init__(\n             ]\n         layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n+        # Validate the correctness of rotary position embeddings parameters\n+        rope_theta = getattr(self, \"rope_theta\", 1_000_000.0)\n+        rope_local_base_freq = getattr(self, \"rope_local_base_freq\", 10000.0)\n+        standardize_rope_params(\n+            self, rope_theta={\"full_attention\": rope_theta, \"sliding_attention\": rope_local_base_freq}\n+        )\n+        rope_config_validation(self)\n+\n \n class Gemma3Config(PreTrainedConfig):\n     r\"\"\"\n@@ -381,17 +355,90 @@ def __init__(self, dim: int, eps: float = 1e-6):\n \n \n class Gemma3RotaryEmbedding(Gemma2RotaryEmbedding):\n-    def __init__(self, config: Gemma3TextConfig, device=None):\n-        super().__init__(config)\n+    def __init__(self, config: Gemma3TextConfig, device=None, layer_type=None):\n+        nn.Module.__init__()\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+\n+        self.layer_types = list(set(config.layer_types))\n+        self.rope_type = {}\n+        for layer_type in self.layer_types:\n+            rope_params = self.config.rope_parameters[layer_type]\n+            if rope_params is None:\n+                continue\n+\n+            self.rope_type[layer_type] = rope_params[\"rope_type\"]\n+            rope_init_fn: Callable = self.compute_default_rope_parameters\n+            if self.rope_type[layer_type] != \"default\":\n+                rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type[layer_type]]\n+            curr_inv_freq, curr_attention_scaling = rope_init_fn(self.config, device, layer_type=layer_type)\n+            self.register_buffer(f\"{layer_type}_inv_freq\", curr_inv_freq, persistent=False)\n+            setattr(self, f\"{layer_type}_original_inv_freq\", curr_inv_freq)\n+            setattr(self, f\"{layer_type}_attention_scaling\", curr_attention_scaling)\n+\n+    @staticmethod\n+    def compute_default_rope_parameters(\n+        config: Optional[Gemma3TextConfig] = None,\n+        device: Optional[\"torch.device\"] = None,\n+        seq_len: Optional[int] = None,\n+        layer_type: Optional[str] = None,\n+    ) -> tuple[\"torch.Tensor\", float]:\n+        \"\"\"\n+        Computes the inverse frequencies according to the original RoPE implementation\n+        Args:\n+            config ([`~transformers.PreTrainedConfig`]):\n+                The model configuration.\n+            device (`torch.device`):\n+                The device to use for initialization of the inverse frequencies.\n+            seq_len (`int`, *optional*):\n+                The current sequence length. Unused for this type of RoPE.\n+            layer_type (`str`, *optional*):\n+                The current layer type if the model has different RoPE parameters per type.\n+                Should not be used unless `config.layer_types is not None`\n+\n+        Returns:\n+            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n+            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n+        \"\"\"\n+        # For backward compatibility standardize the `rope_parameters_dict` if it uses old format\n+        base = config.rope_parameters[layer_type][\"rope_theta\"]\n+        dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+\n+        attention_factor = 1.0  # Unused in this type of RoPE\n+\n+        # Compute the inverse frequencies\n+        inv_freq = 1.0 / (\n+            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n+        )\n+        return inv_freq, attention_factor\n+\n+    @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n+    def forward(self, x, position_ids, layer_type=None):\n+        inv_freq = getattr(self, f\"{layer_type}_inv_freq\")\n+        attention_scaling = getattr(self, f\"{layer_type}_attention_scaling\")\n+\n+        inv_freq_expanded = inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * attention_scaling\n+            sin = emb.sin() * attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n \n \n # Weird way to inherit but otherwise the sliding window gets defined first and can't access `is_sliding`\n class Gemma3Attention(Gemma2Attention):\n     def __init__(self, config: Gemma3TextConfig, layer_idx: int):\n-        self.is_sliding = config.layer_types[layer_idx] == \"sliding_attention\"\n-\n         super().__init__(config, layer_idx)\n-        self.sliding_window = config.sliding_window if self.is_sliding else None\n+        self.sliding_window = config.sliding_window if self.layer_type == \"sliding_attention\" else None\n+        self.is_sliding = self.layer_type == \"sliding_attention\"\n         self.is_causal = not self.config.use_bidirectional_attention\n \n         self.q_norm = Gemma3RMSNorm(dim=config.head_dim, eps=config.rms_norm_eps)\n@@ -400,8 +447,8 @@ def __init__(self, config: Gemma3TextConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor],\n+        position_embeddings: torch.Tensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n@@ -462,8 +509,7 @@ def __init__(self, config: Gemma3TextConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings_global: torch.Tensor,\n-        position_embeddings_local: torch.Tensor,\n+        position_embeddings: torch.Tensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -476,12 +522,6 @@ def forward(\n \n         hidden_states = self.input_layernorm(hidden_states)\n \n-        # apply global RoPE to non-sliding layer only\n-        if self.self_attn.is_sliding:\n-            position_embeddings = position_embeddings_local\n-        else:\n-            position_embeddings = position_embeddings_global\n-\n         hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             position_embeddings=position_embeddings,\n@@ -557,13 +597,6 @@ def __init__(self, config: Gemma3TextConfig):\n             config.vocab_size, config.hidden_size, self.padding_idx, embed_scale=self.config.hidden_size**0.5\n         )\n \n-        # TODO: raushan fix this after RoPE refactor. For now we hack it by reassigning thetas\n-        # when we want to create a local RoPE layer. Config defaults should hold values for global RoPE\n-        config = copy.deepcopy(config)\n-        config.rope_theta = config.rope_local_base_freq\n-        config.rope_scaling = {\"rope_type\": \"default\"}\n-        self.rotary_emb_local = Gemma3RotaryEmbedding(config=config)\n-\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -601,9 +634,7 @@ def forward(\n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n             cache_position = torch.arange(\n-                past_seen_tokens,\n-                past_seen_tokens + inputs_embeds.shape[1],\n-                device=inputs_embeds.device,\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n             )\n \n         if position_ids is None:\n@@ -634,10 +665,9 @@ def forward(\n \n         # embed positions\n         hidden_states = inputs_embeds\n-\n-        # create position embeddings to be shared across the decoder layers\n-        position_embeddings_global = self.rotary_emb(hidden_states, position_ids)\n-        position_embeddings_local = self.rotary_emb_local(hidden_states, position_ids)\n+        position_embeddings = {}\n+        for layer_type in self.config.layer_types:\n+            position_embeddings[layer_type] = self.rotary_emb(hidden_states, position_ids, layer_type)\n \n         # decoder layers\n         all_hidden_states = () if output_hidden_states else None\n@@ -649,9 +679,8 @@ def forward(\n \n             layer_outputs = decoder_layer(\n                 hidden_states,\n-                position_embeddings_global=position_embeddings_global,\n-                position_embeddings_local=position_embeddings_local,\n                 attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n+                position_embeddings=position_embeddings[decoder_layer.attention_type],\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n                 output_attentions=output_attentions,"
        },
        {
            "sha": "cbc0e890d9cc7dba784fceba64c2cd6dd8ef8bfa",
            "filename": "src/transformers/models/gemma3n/configuration_gemma3n.py",
            "status": "modified",
            "additions": 17,
            "deletions": 50,
            "changes": 67,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fconfiguration_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fconfiguration_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fconfiguration_gemma3n.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -23,7 +23,7 @@\n from typing import Any, Optional, Union\n \n from ...configuration_utils import PreTrainedConfig, layer_type_validation\n-from ...modeling_rope_utils import rope_config_validation\n+from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n from ...utils import is_timm_available, logging, requires_backends\n \n \n@@ -90,47 +90,10 @@ class Gemma3nTextConfig(PreTrainedConfig):\n             End of stream token id.\n         bos_token_id (`int`, *optional*, defaults to 2):\n             Beginning of stream token id.\n-        rope_theta (`float`, *optional*, defaults to 1000000.0):\n-            The base period of the RoPE embeddings.\n-        rope_scaling (`Dict`, *optional*):\n-            Dictionary containing the scaling configuration for the RoPE embeddings used in global attention.\n-            NOTE: if you apply new rope type and you expect the model to work on longer `max_position_embeddings`, we\n-            recommend you to update this value accordingly.\n-            Expected contents:\n-                `rope_type` (`str`):\n-                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n-                    'llama3'], with 'default' being the original RoPE implementation.\n-                `factor` (`float`, *optional*):\n-                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n-                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n-                    original maximum pre-trained length.\n-                `original_max_position_embeddings` (`int`, *optional*):\n-                    Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n-                    pretraining.\n-                `attention_factor` (`float`, *optional*):\n-                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n-                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n-                    `factor` field to infer the suggested value.\n-                `beta_fast` (`float`, *optional*):\n-                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n-                    ramp function. If unspecified, it defaults to 32.\n-                `beta_slow` (`float`, *optional*):\n-                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n-                    ramp function. If unspecified, it defaults to 1.\n-                `short_factor` (`List[float]`, *optional*):\n-                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n-                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n-                    size divided by the number of attention heads divided by 2\n-                `long_factor` (`List[float]`, *optional*):\n-                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n-                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n-                    size divided by the number of attention heads divided by 2\n-                `low_freq_factor` (`float`, *optional*):\n-                    Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n-                `high_freq_factor` (`float`, *optional*):\n-                    Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n-        rope_local_base_freq (float, *optional*, defaults to 10000.0):\n-            The base period of the RoPE embeddings for local attention.\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):\n             Whether to use a bias in the query, key, value and output projection layers during self-attention.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n@@ -214,9 +177,7 @@ def __init__(\n         pad_token_id: int = 0,\n         eos_token_id: int = 1,\n         bos_token_id: int = 2,\n-        rope_theta: float = 1_000_000.0,\n-        rope_scaling: Optional[dict[str, Any]] = None,\n-        rope_local_base_freq: float = 10_000.0,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n         attention_bias: bool = False,\n         attention_dropout: float = 0.0,\n         sliding_window: int = 512,\n@@ -258,17 +219,15 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.rms_norm_eps = rms_norm_eps\n         self.use_cache = use_cache\n-        self.rope_theta = rope_theta\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n         self.hidden_activation = hidden_activation\n         self.sliding_window = sliding_window\n         self.final_logit_softcapping = final_logit_softcapping\n         self.layer_types = layer_types\n-\n-        self.rope_local_base_freq = rope_local_base_freq\n-        self.rope_scaling = rope_scaling\n-        rope_config_validation(self)\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n \n         if layer_types is None:\n             self.layer_types = [\n@@ -279,6 +238,14 @@ def __init__(\n \n         layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n+        # Validate the correctness of rotary position embeddings parameters\n+        rope_theta = kwargs.get(\"rope_theta\", 1000000.0)\n+        rope_local_base_freq = kwargs.get(\"rope_local_base_freq\", 100000.0)\n+        standardize_rope_params(\n+            self, rope_theta={\"full_attention\": rope_theta, \"sliding_attention\": rope_local_base_freq}\n+        )\n+        rope_config_validation(self)\n+\n         self.hidden_size_per_layer_input = hidden_size_per_layer_input\n         self.num_kv_shared_layers = num_kv_shared_layers\n "
        },
        {
            "sha": "452860d956f951214978ca9cc366a3296876ad89",
            "filename": "src/transformers/models/gemma3n/modeling_gemma3n.py",
            "status": "modified",
            "additions": 246,
            "deletions": 69,
            "changes": 315,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -19,7 +19,6 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-import copy\n import math\n from collections.abc import Callable, Sequence\n from dataclasses import dataclass\n@@ -1144,42 +1143,6 @@ def scale_corrected_output(self, corrected: torch.Tensor) -> torch.Tensor:\n         return self.forward(corrected)\n \n \n-class Gemma3nTextRotaryEmbedding(nn.Module):\n-    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n-\n-    def __init__(self, config: Gemma3nTextConfig, device=None):\n-        super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n-        self.max_seq_len_cached = config.max_position_embeddings\n-        self.original_max_seq_len = config.max_position_embeddings\n-\n-        self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n-\n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n-        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n-\n-    @torch.no_grad()\n-    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n-    def forward(self, x, position_ids):\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n-        position_ids_expanded = position_ids[:, None, :].float()\n-\n-        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n-            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos() * self.attention_scaling\n-            sin = emb.sin() * self.attention_scaling\n-\n-        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n-\n-\n def rotate_half(x):\n     \"\"\"Rotates half the hidden dims of the input.\"\"\"\n     x1 = x[..., : x.shape[-1] // 2]\n@@ -1269,7 +1232,7 @@ class Gemma3nTextAttention(nn.Module):\n \n     def __init__(self, config: Gemma3nTextConfig, layer_idx: int):\n         super().__init__()\n-        self.is_sliding = config.layer_types[layer_idx] == \"sliding_attention\"\n+        self.layer_type = config.layer_types[layer_idx] if hasattr(config, \"layer_types\") else None\n         self.config = config\n         self.layer_idx = layer_idx\n         self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n@@ -1289,7 +1252,8 @@ def __init__(self, config: Gemma3nTextConfig, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n-        self.sliding_window = config.sliding_window if self.is_sliding else None\n+        self.sliding_window = config.sliding_window if self.layer_type == \"sliding_attention\" else None\n+        self.is_sliding = self.layer_type == \"sliding_attention\"\n \n         self.q_norm = Gemma3nRMSNorm(dim=config.head_dim, eps=config.rms_norm_eps)\n         self.k_norm = Gemma3nRMSNorm(dim=config.head_dim, eps=config.rms_norm_eps)\n@@ -1312,8 +1276,8 @@ def __init__(self, config: Gemma3nTextConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor],\n+        position_embeddings: torch.Tensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n@@ -1322,7 +1286,6 @@ def forward(\n         hidden_shape = (*input_shape, -1, self.config.head_dim)\n \n         cos, sin = position_embeddings\n-\n         query_states = self.q_proj(hidden_states).view(hidden_shape)\n         query_states = self.q_norm(query_states)\n         query_states = apply_rotary_pos_emb(query_states, cos, sin, unsqueeze_dim=2)\n@@ -1408,9 +1371,8 @@ def __init__(self, config: Gemma3nTextConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings_global: torch.Tensor,\n-        position_embeddings_local: torch.Tensor,\n-        per_layer_input: torch.Tensor,\n+        position_embeddings: torch.Tensor = None,\n+        per_layer_input: torch.Tensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -1425,17 +1387,11 @@ def forward(\n         active_prediction_normed = self.input_layernorm(active_prediction)\n         laurel_output = self.laurel(active_prediction_normed)\n \n-        # apply global RoPE to non-sliding layer only\n-        if self.self_attn.is_sliding:\n-            position_embeddings = position_embeddings_local\n-        else:\n-            position_embeddings = position_embeddings_global\n-\n         attn, self_attn_weights = self.self_attn(\n             hidden_states=active_prediction_normed,\n-            position_embeddings=position_embeddings,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n+            position_embeddings=position_embeddings,\n             past_key_values=past_key_values,\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n@@ -1475,6 +1431,156 @@ def forward(\n         return outputs\n \n \n+class Gemma3nMLP(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.hidden_size = config.hidden_size\n+        self.intermediate_size = config.intermediate_size\n+        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n+        self.act_fn = ACT2FN[config.hidden_activation]\n+\n+    def forward(self, x):\n+        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n+        return down_proj\n+\n+\n+class Gemma3nAttention(nn.Module):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+\n+    def __init__(self, config: Gemma3nConfig, layer_idx: int):\n+        super().__init__()\n+        self.layer_type = config.layer_types[layer_idx] if hasattr(config, \"layer_types\") else None\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n+        self.scaling = config.query_pre_attn_scalar**-0.5\n+        self.attention_dropout = self.config.attention_dropout\n+        self.is_causal = not getattr(config, \"use_bidirectional_attention\", False)\n+\n+        self.q_proj = nn.Linear(\n+            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.k_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.v_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.o_proj = nn.Linear(\n+            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n+        )\n+        self.attn_logit_softcapping = self.config.attn_logit_softcapping\n+        self.sliding_window = config.sliding_window if self.layer_type == \"sliding_attention\" else None\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+\n+        cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        if past_key_values is not None:\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=self.attention_dropout if self.training else 0.0,\n+            scaling=self.scaling,\n+            sliding_window=self.sliding_window,\n+            softcap=self.attn_logit_softcapping,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n+\n+\n+class Gemma3nDecoderLayer(GradientCheckpointingLayer):\n+    def __init__(self, config: Gemma3nConfig, layer_idx: int):\n+        super().__init__()\n+        self.hidden_size = config.hidden_size\n+        self.config = config\n+        self.attention_type = config.layer_types[layer_idx]\n+        self.self_attn = Gemma3nAttention(config=config, layer_idx=layer_idx)\n+        self.mlp = Gemma3nMLP(config)\n+        self.input_layernorm = Gemma3nRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.post_attention_layernorm = Gemma3nRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+\n+        self.pre_feedforward_layernorm = Gemma3nRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.post_feedforward_layernorm = Gemma3nRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        output_attentions: Optional[bool] = False,\n+        use_cache: Optional[bool] = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n+    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+        residual = hidden_states\n+\n+        hidden_states = self.input_layernorm(hidden_states)\n+\n+        # Self Attention\n+        hidden_states, self_attn_weights = self.self_attn(\n+            hidden_states=hidden_states,\n+            position_embeddings=position_embeddings,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            output_attentions=output_attentions,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+        hidden_states = self.post_attention_layernorm(hidden_states)\n+        hidden_states = residual + hidden_states\n+\n+        residual = hidden_states\n+        hidden_states = self.pre_feedforward_layernorm(hidden_states)\n+        hidden_states = self.mlp(hidden_states)\n+        hidden_states = self.post_feedforward_layernorm(hidden_states)\n+        hidden_states = residual + hidden_states\n+\n+        outputs = (hidden_states,)\n+\n+        if output_attentions:\n+            outputs += (self_attn_weights,)\n+\n+        return outputs\n+\n+\n @auto_docstring\n class Gemma3nPreTrainedModel(PreTrainedModel):\n     config: Gemma3nConfig\n@@ -1489,8 +1595,8 @@ class Gemma3nPreTrainedModel(PreTrainedModel):\n     _can_compile_fullgraph = True\n     _supports_attention_backend = True\n     _can_record_outputs = {\n-        \"hidden_states\": Gemma3nTextDecoderLayer,\n-        \"attentions\": Gemma3nTextAttention,\n+        \"hidden_states\": Gemma3nDecoderLayer,\n+        \"attentions\": Gemma3nAttention,\n     }\n     input_modalities = [\"image\", \"text\", \"audio\"]\n \n@@ -1504,6 +1610,87 @@ def _init_weights(self, module):\n             module.correct_output_scale.data.zero_()\n \n \n+class Gemma3nRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n+    def __init__(self, config: Gemma3nTextConfig, device=None, layer_type=None):\n+        super().__init__()\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+\n+        self.layer_types = list(set(config.layer_types))\n+        self.rope_type = {}\n+        for layer_type in self.layer_types:\n+            rope_params = self.config.rope_parameters[layer_type]\n+            if rope_params is None:\n+                continue\n+\n+            self.rope_type[layer_type] = rope_params[\"rope_type\"]\n+            rope_init_fn: Callable = self.compute_default_rope_parameters\n+            if self.rope_type[layer_type] != \"default\":\n+                rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type[layer_type]]\n+            curr_inv_freq, curr_attention_scaling = rope_init_fn(self.config, device, layer_type=layer_type)\n+            self.register_buffer(f\"{layer_type}_inv_freq\", curr_inv_freq, persistent=False)\n+            setattr(self, f\"{layer_type}_original_inv_freq\", curr_inv_freq)\n+            setattr(self, f\"{layer_type}_attention_scaling\", curr_attention_scaling)\n+\n+    @staticmethod\n+    def compute_default_rope_parameters(\n+        config: Optional[Gemma3nTextConfig] = None,\n+        device: Optional[\"torch.device\"] = None,\n+        seq_len: Optional[int] = None,\n+        layer_type: Optional[str] = None,\n+    ) -> tuple[\"torch.Tensor\", float]:\n+        \"\"\"\n+        Computes the inverse frequencies according to the original RoPE implementation\n+        Args:\n+            config ([`~transformers.PreTrainedConfig`]):\n+                The model configuration.\n+            device (`torch.device`):\n+                The device to use for initialization of the inverse frequencies.\n+            seq_len (`int`, *optional*):\n+                The current sequence length. Unused for this type of RoPE.\n+            layer_type (`str`, *optional*):\n+                The current layer type if the model has different RoPE parameters per type.\n+                Should not be used unless `config.layer_types is not None`\n+\n+        Returns:\n+            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n+            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n+        \"\"\"\n+        # For backward compatibility standardize the `rope_parameters_dict` if it uses old format\n+        base = config.rope_parameters[layer_type][\"rope_theta\"]\n+        dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+\n+        attention_factor = 1.0  # Unused in this type of RoPE\n+\n+        # Compute the inverse frequencies\n+        inv_freq = 1.0 / (\n+            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n+        )\n+        return inv_freq, attention_factor\n+\n+    @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n+    def forward(self, x, position_ids, layer_type=None):\n+        inv_freq = getattr(self, f\"{layer_type}_inv_freq\")\n+        attention_scaling = getattr(self, f\"{layer_type}_attention_scaling\")\n+\n+        inv_freq_expanded = inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * attention_scaling\n+            sin = emb.sin() * attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n @auto_docstring(custom_intro=\"The base Gemma 3n language model without a language modeling head.\")\n class Gemma3nTextModel(Gemma3nPreTrainedModel):\n     config: Gemma3nTextConfig\n@@ -1523,17 +1710,9 @@ def __init__(self, config: Gemma3nTextConfig):\n         )\n \n         self.norm = Gemma3nRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n-        self.rotary_emb = Gemma3nTextRotaryEmbedding(config=config)\n+        self.rotary_emb = Gemma3nRotaryEmbedding(config)\n         self.gradient_checkpointing = False\n \n-        # TODO (raushan): Fix this after RoPE refactor. For now we hack it by\n-        # reassigning thetas when we want to create a local RoPE layer. Config\n-        # defaults should hold values for global RoPE.\n-        config = copy.deepcopy(config)\n-        config.rope_theta = config.rope_local_base_freq\n-        config.rope_scaling = {\"rope_type\": \"default\"}\n-        self.rotary_emb_local = Gemma3nTextRotaryEmbedding(config=config)\n-\n         self.hidden_size = config.hidden_size\n         self.hidden_size_per_layer_input = config.hidden_size_per_layer_input\n \n@@ -1641,10 +1820,6 @@ def forward(\n         # embed positions\n         hidden_states_0 = inputs_embeds\n \n-        # Initialize RoPE embeddings\n-        position_embeddings_global = self.rotary_emb(hidden_states_0, position_ids)\n-        position_embeddings_local = self.rotary_emb_local(hidden_states_0, position_ids)\n-\n         # Expand hidden_states to support per-layer inputs\n         target_magnitude = torch.mean(hidden_states_0**2, dim=-1, keepdim=True) ** 0.5\n         epsilon_tensor = torch.tensor(1e-5)\n@@ -1660,6 +1835,9 @@ def forward(\n             temp_hidden_states.append(current_hidden_state)\n \n         hidden_states = torch.stack(temp_hidden_states, dim=0)  # [num_altup_inputs, batch, seq_len, hidden_size]\n+        position_embeddings = {}\n+        for layer_type in self.config.layer_types:\n+            position_embeddings[layer_type] = self.rotary_emb(hidden_states, position_ids, layer_type)\n \n         # decoder layers\n         all_hidden_states = () if output_hidden_states else None\n@@ -1674,8 +1852,7 @@ def forward(\n \n             layer_outputs = decoder_layer(\n                 hidden_states,\n-                position_embeddings_global,\n-                position_embeddings_local,\n+                position_embeddings[decoder_layer.attention_type],\n                 per_layer_input,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,"
        },
        {
            "sha": "adbcd029d7c214a3d90a243a5a88869071c9033b",
            "filename": "src/transformers/models/gemma3n/modular_gemma3n.py",
            "status": "modified",
            "additions": 26,
            "deletions": 84,
            "changes": 110,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -13,7 +13,6 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-import copy\n import math\n from collections.abc import Callable, Sequence\n from typing import Any, Optional, Union\n@@ -28,7 +27,7 @@\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutputWithPast\n-from ...modeling_rope_utils import rope_config_validation\n+from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n@@ -37,7 +36,6 @@\n from ..gemma2.modeling_gemma2 import (\n     Gemma2MLP,\n     Gemma2PreTrainedModel,\n-    Gemma2RotaryEmbedding,\n     eager_attention_forward,\n     rotate_half,\n )\n@@ -117,47 +115,10 @@ class Gemma3nTextConfig(Gemma2Config, PreTrainedConfig):\n             End of stream token id.\n         bos_token_id (`int`, *optional*, defaults to 2):\n             Beginning of stream token id.\n-        rope_theta (`float`, *optional*, defaults to 1000000.0):\n-            The base period of the RoPE embeddings.\n-        rope_scaling (`Dict`, *optional*):\n-            Dictionary containing the scaling configuration for the RoPE embeddings used in global attention.\n-            NOTE: if you apply new rope type and you expect the model to work on longer `max_position_embeddings`, we\n-            recommend you to update this value accordingly.\n-            Expected contents:\n-                `rope_type` (`str`):\n-                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n-                    'llama3'], with 'default' being the original RoPE implementation.\n-                `factor` (`float`, *optional*):\n-                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n-                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n-                    original maximum pre-trained length.\n-                `original_max_position_embeddings` (`int`, *optional*):\n-                    Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n-                    pretraining.\n-                `attention_factor` (`float`, *optional*):\n-                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n-                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n-                    `factor` field to infer the suggested value.\n-                `beta_fast` (`float`, *optional*):\n-                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n-                    ramp function. If unspecified, it defaults to 32.\n-                `beta_slow` (`float`, *optional*):\n-                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n-                    ramp function. If unspecified, it defaults to 1.\n-                `short_factor` (`List[float]`, *optional*):\n-                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n-                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n-                    size divided by the number of attention heads divided by 2\n-                `long_factor` (`List[float]`, *optional*):\n-                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n-                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n-                    size divided by the number of attention heads divided by 2\n-                `low_freq_factor` (`float`, *optional*):\n-                    Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n-                `high_freq_factor` (`float`, *optional*):\n-                    Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n-        rope_local_base_freq (float, *optional*, defaults to 10000.0):\n-            The base period of the RoPE embeddings for local attention.\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):\n             Whether to use a bias in the query, key, value and output projection layers during self-attention.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n@@ -226,9 +187,7 @@ def __init__(\n         pad_token_id: int = 0,\n         eos_token_id: int = 1,\n         bos_token_id: int = 2,\n-        rope_theta: float = 1_000_000.0,\n-        rope_scaling: Optional[dict[str, Any]] = None,\n-        rope_local_base_freq: float = 10_000.0,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n         attention_bias: bool = False,\n         attention_dropout: float = 0.0,\n         sliding_window: int = 512,\n@@ -270,17 +229,15 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.rms_norm_eps = rms_norm_eps\n         self.use_cache = use_cache\n-        self.rope_theta = rope_theta\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n         self.hidden_activation = hidden_activation\n         self.sliding_window = sliding_window\n         self.final_logit_softcapping = final_logit_softcapping\n         self.layer_types = layer_types\n-\n-        self.rope_local_base_freq = rope_local_base_freq\n-        self.rope_scaling = rope_scaling\n-        rope_config_validation(self)\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n \n         if layer_types is None:\n             self.layer_types = [\n@@ -291,6 +248,14 @@ def __init__(\n \n         layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n+        # Validate the correctness of rotary position embeddings parameters\n+        rope_theta = kwargs.get(\"rope_theta\", 1000000.0)\n+        rope_local_base_freq = kwargs.get(\"rope_local_base_freq\", 100000.0)\n+        standardize_rope_params(\n+            self, rope_theta={\"full_attention\": rope_theta, \"sliding_attention\": rope_local_base_freq}\n+        )\n+        rope_config_validation(self)\n+\n         self.hidden_size_per_layer_input = hidden_size_per_layer_input\n         self.num_kv_shared_layers = num_kv_shared_layers\n \n@@ -1703,10 +1668,6 @@ def scale_corrected_output(self, corrected: torch.Tensor) -> torch.Tensor:\n         return self.forward(corrected)\n \n \n-class Gemma3nTextRotaryEmbedding(Gemma2RotaryEmbedding):\n-    pass\n-\n-\n def apply_rotary_pos_emb(\n     x: torch.Tensor,\n     cos: torch.Tensor,\n@@ -1762,8 +1723,8 @@ def __init__(self, config: Gemma3nTextConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor],\n+        position_embeddings: torch.Tensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n@@ -1772,7 +1733,6 @@ def forward(\n         hidden_shape = (*input_shape, -1, self.config.head_dim)\n \n         cos, sin = position_embeddings\n-\n         query_states = self.q_proj(hidden_states).view(hidden_shape)\n         query_states = self.q_norm(query_states)\n         query_states = apply_rotary_pos_emb(query_states, cos, sin, unsqueeze_dim=2)\n@@ -1850,9 +1810,8 @@ def __init__(self, config: Gemma3nTextConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings_global: torch.Tensor,\n-        position_embeddings_local: torch.Tensor,\n-        per_layer_input: torch.Tensor,\n+        position_embeddings: torch.Tensor = None,\n+        per_layer_input: torch.Tensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -1867,17 +1826,11 @@ def forward(\n         active_prediction_normed = self.input_layernorm(active_prediction)\n         laurel_output = self.laurel(active_prediction_normed)\n \n-        # apply global RoPE to non-sliding layer only\n-        if self.self_attn.is_sliding:\n-            position_embeddings = position_embeddings_local\n-        else:\n-            position_embeddings = position_embeddings_global\n-\n         attn, self_attn_weights = self.self_attn(\n             hidden_states=active_prediction_normed,\n-            position_embeddings=position_embeddings,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n+            position_embeddings=position_embeddings,\n             past_key_values=past_key_values,\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n@@ -1973,15 +1926,6 @@ def __init__(self, config: Gemma3nTextConfig):\n \n         self.register_buffer(\"per_layer_projection_scale\", torch.tensor(self.hidden_size**-0.5), persistent=False)\n         self.register_buffer(\"per_layer_input_scale\", torch.rsqrt(torch.tensor(2.0)), persistent=False)\n-        self.rotary_emb = Gemma3nTextRotaryEmbedding(config=config)\n-\n-        # TODO (raushan): Fix this after RoPE refactor. For now we hack it by\n-        # reassigning thetas when we want to create a local RoPE layer. Config\n-        # defaults should hold values for global RoPE.\n-        config = copy.deepcopy(config)\n-        config.rope_theta = config.rope_local_base_freq\n-        config.rope_scaling = {\"rope_type\": \"default\"}\n-        self.rotary_emb_local = Gemma3nTextRotaryEmbedding(config=config)\n \n     def get_per_layer_inputs(self, input_ids: torch.LongTensor) -> torch.Tensor:\n         return self.embed_tokens_per_layer(input_ids).reshape(\n@@ -2092,10 +2036,6 @@ def forward(\n         # embed positions\n         hidden_states_0 = inputs_embeds\n \n-        # Initialize RoPE embeddings\n-        position_embeddings_global = self.rotary_emb(hidden_states_0, position_ids)\n-        position_embeddings_local = self.rotary_emb_local(hidden_states_0, position_ids)\n-\n         # Expand hidden_states to support per-layer inputs\n         target_magnitude = torch.mean(hidden_states_0**2, dim=-1, keepdim=True) ** 0.5\n         epsilon_tensor = torch.tensor(1e-5)\n@@ -2111,6 +2051,9 @@ def forward(\n             temp_hidden_states.append(current_hidden_state)\n \n         hidden_states = torch.stack(temp_hidden_states, dim=0)  # [num_altup_inputs, batch, seq_len, hidden_size]\n+        position_embeddings = {}\n+        for layer_type in self.config.layer_types:\n+            position_embeddings[layer_type] = self.rotary_emb(hidden_states, position_ids, layer_type)\n \n         # decoder layers\n         all_hidden_states = () if output_hidden_states else None\n@@ -2125,8 +2068,7 @@ def forward(\n \n             layer_outputs = decoder_layer(\n                 hidden_states,\n-                position_embeddings_global,\n-                position_embeddings_local,\n+                position_embeddings[decoder_layer.attention_type],\n                 per_layer_input,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,"
        },
        {
            "sha": "63685ce767292ffaaaed7f049d3742512191ce3e",
            "filename": "src/transformers/models/glm/configuration_glm.py",
            "status": "modified",
            "additions": 35,
            "deletions": 23,
            "changes": 58,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fglm%2Fconfiguration_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fglm%2Fconfiguration_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fconfiguration_glm.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -14,7 +14,10 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n+from typing import Optional\n+\n from ...configuration_utils import PreTrainedConfig\n+from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n \n \n class GlmConfig(PreTrainedConfig):\n@@ -63,8 +66,10 @@ class GlmConfig(PreTrainedConfig):\n             relevant if `config.is_decoder=True`.\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie weight embeddings\n-        rope_theta (`float`, *optional*, defaults to 10000.0):\n-            The base period of the RoPE embeddings.\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         pad_token_id (`int`, *optional*, defaults to 151329):\n             Padding token id.\n         eos_token_id (`int` | `list`, *optional*, defaults to `[151329, 151336, 151338]`):\n@@ -101,26 +106,26 @@ class GlmConfig(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size=151552,\n-        hidden_size=4096,\n-        intermediate_size=13696,\n-        num_hidden_layers=40,\n-        num_attention_heads=32,\n-        num_key_value_heads=2,\n-        partial_rotary_factor=0.5,\n-        head_dim=128,\n-        hidden_act=\"silu\",\n-        attention_dropout=0.0,\n-        max_position_embeddings=131072,\n-        initializer_range=0.02,\n-        rms_norm_eps=0.00000015625,\n-        use_cache=True,\n-        tie_word_embeddings=False,\n-        rope_theta=10000.0,\n-        pad_token_id=151329,\n-        eos_token_id=[151329, 151336, 151338],\n-        bos_token_id=None,\n-        attention_bias=True,\n+        vocab_size: Optional[int] = 151552,\n+        hidden_size: Optional[int] = 4096,\n+        intermediate_size: Optional[int] = 13696,\n+        num_hidden_layers: Optional[int] = 40,\n+        num_attention_heads: Optional[int] = 32,\n+        num_key_value_heads: Optional[int] = 2,\n+        partial_rotary_factor: Optional[float] = 0.5,\n+        head_dim: Optional[int] = 128,\n+        hidden_act: Optional[str] = \"silu\",\n+        attention_dropout: Optional[float] = 0.0,\n+        max_position_embeddings: Optional[int] = 131072,\n+        initializer_range: Optional[float] = 0.02,\n+        rms_norm_eps: Optional[float] = 0.00000015625,\n+        use_cache: Optional[bool] = True,\n+        tie_word_embeddings: Optional[bool] = False,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        pad_token_id: Optional[int] = 151329,\n+        eos_token_id: Optional[list[int]] = [151329, 151336, 151338],\n+        bos_token_id: Optional[int] = None,\n+        attention_bias: Optional[bool] = True,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size\n@@ -136,9 +141,16 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.rms_norm_eps = rms_norm_eps\n         self.use_cache = use_cache\n-        self.rope_theta = rope_theta\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n+\n+        # Validate the correctness of rotary position embeddings parameters\n+        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n+        rope_config_validation(self)\n \n         super().__init__(\n             pad_token_id=pad_token_id,"
        },
        {
            "sha": "f72268465ece4935ebd3104067f6a5ea4b76600c",
            "filename": "src/transformers/models/glm/modeling_glm.py",
            "status": "modified",
            "additions": 72,
            "deletions": 41,
            "changes": 113,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -62,6 +62,73 @@ def forward(self, hidden_states: torch.FloatTensor) -> torch.FloatTensor:\n         return self.down_proj(up_states)\n \n \n+class GlmRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n+    def __init__(self, config: GlmConfig, device=None):\n+        super().__init__()\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+\n+        self.rope_type = self.config.rope_parameters[\"rope_type\"]\n+        rope_init_fn: Callable = self.compute_default_rope_parameters\n+        if self.rope_type != \"default\":\n+            rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+        inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n+\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = inv_freq\n+\n+    @staticmethod\n+    def compute_default_rope_parameters(\n+        config: Optional[GlmConfig] = None,\n+        device: Optional[\"torch.device\"] = None,\n+        seq_len: Optional[int] = None,\n+    ) -> tuple[\"torch.Tensor\", float]:\n+        \"\"\"\n+        Computes the inverse frequencies according to the original RoPE implementation\n+        Args:\n+            config ([`~transformers.PreTrainedConfig`]):\n+                The model configuration.\n+            device (`torch.device`):\n+                The device to use for initialization of the inverse frequencies.\n+            seq_len (`int`, *optional*):\n+                The current sequence length. Unused for this type of RoPE.\n+        Returns:\n+            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n+            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n+        \"\"\"\n+        base = config.rope_parameters[\"rope_theta\"]\n+        partial_rotary_factor = getattr(config, \"partial_rotary_factor\", 1.0)\n+        head_dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+        dim = int(head_dim * partial_rotary_factor)\n+\n+        attention_factor = 1.0  # Unused in this type of RoPE\n+\n+        # Compute the inverse frequencies\n+        inv_freq = 1.0 / (\n+            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n+        )\n+        return inv_freq, attention_factor\n+\n+    @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n+    def forward(self, x, position_ids):\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     \"\"\"\n     This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n@@ -176,8 +243,8 @@ def __init__(self, config: GlmConfig, layer_idx: Optional[int] = None):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n-        attention_mask: Optional[torch.Tensor],\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -238,42 +305,6 @@ def extra_repr(self):\n         return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n \n \n-class GlmRotaryEmbedding(nn.Module):\n-    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n-\n-    def __init__(self, config: GlmConfig, device=None):\n-        super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n-        self.max_seq_len_cached = config.max_position_embeddings\n-        self.original_max_seq_len = config.max_position_embeddings\n-\n-        self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n-\n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n-        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n-\n-    @torch.no_grad()\n-    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n-    def forward(self, x, position_ids):\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n-        position_ids_expanded = position_ids[:, None, :].float()\n-\n-        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n-            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos() * self.attention_scaling\n-            sin = emb.sin() * self.attention_scaling\n-\n-        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n-\n-\n class GlmDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: GlmConfig, layer_idx: int):\n         super().__init__()\n@@ -293,7 +324,7 @@ def forward(\n         past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> torch.Tensor:\n         residual = hidden_states\n@@ -397,16 +428,16 @@ def forward(\n         )\n \n         hidden_states = inputs_embeds\n-        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids=position_ids)\n \n         for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n             hidden_states = decoder_layer(\n                 hidden_states,\n                 attention_mask=causal_mask,\n+                position_embeddings=position_embeddings,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n                 cache_position=cache_position,\n-                position_embeddings=position_embeddings,\n                 **kwargs,\n             )\n "
        },
        {
            "sha": "059cb296c9727b5c616bb958c8fec539357c747f",
            "filename": "src/transformers/models/glm/modular_glm.py",
            "status": "modified",
            "additions": 35,
            "deletions": 0,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fglm%2Fmodular_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fglm%2Fmodular_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodular_glm.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -24,6 +24,7 @@\n     LlamaForCausalLM,\n     LlamaForSequenceClassification,\n     LlamaForTokenClassification,\n+    LlamaRotaryEmbedding,\n )\n from ..phi3.modeling_phi3 import Phi3MLP\n from .configuration_glm import GlmConfig\n@@ -38,6 +39,40 @@ class GlmMLP(Phi3MLP):\n     pass\n \n \n+class GlmRotaryEmbedding(LlamaRotaryEmbedding):\n+    @staticmethod\n+    def compute_default_rope_parameters(\n+        config: Optional[GlmConfig] = None,\n+        device: Optional[\"torch.device\"] = None,\n+        seq_len: Optional[int] = None,\n+    ) -> tuple[\"torch.Tensor\", float]:\n+        \"\"\"\n+        Computes the inverse frequencies according to the original RoPE implementation\n+        Args:\n+            config ([`~transformers.PreTrainedConfig`]):\n+                The model configuration.\n+            device (`torch.device`):\n+                The device to use for initialization of the inverse frequencies.\n+            seq_len (`int`, *optional*):\n+                The current sequence length. Unused for this type of RoPE.\n+        Returns:\n+            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n+            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n+        \"\"\"\n+        base = config.rope_parameters[\"rope_theta\"]\n+        partial_rotary_factor = getattr(config, \"partial_rotary_factor\", 1.0)\n+        head_dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+        dim = int(head_dim * partial_rotary_factor)\n+\n+        attention_factor = 1.0  # Unused in this type of RoPE\n+\n+        # Compute the inverse frequencies\n+        inv_freq = 1.0 / (\n+            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n+        )\n+        return inv_freq, attention_factor\n+\n+\n def rotate_half(x):\n     \"\"\"Rotates half the hidden dims of the input.\"\"\"\n     x1 = x[..., 0::2]"
        },
        {
            "sha": "026658fa07936184c26e97a45d906cee354373ac",
            "filename": "src/transformers/models/glm4/configuration_glm4.py",
            "status": "modified",
            "additions": 37,
            "deletions": 24,
            "changes": 61,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fglm4%2Fconfiguration_glm4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fglm4%2Fconfiguration_glm4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4%2Fconfiguration_glm4.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -14,7 +14,10 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n+from typing import Optional\n+\n from ...configuration_utils import PreTrainedConfig\n+from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n \n \n class Glm4Config(PreTrainedConfig):\n@@ -45,7 +48,8 @@ class Glm4Config(PreTrainedConfig):\n             by meanpooling all the original heads within that group. For more details, check out [this\n             paper](https://huggingface.co/papers/2305.13245). If it is not specified, will default to\n             `num_attention_heads`.\n-        partial_rotary_factor (`float`, *optional*, defaults to 0.5): The factor of the partial rotary position.\n+        partial_rotary_factor (`float`, *optional*, defaults to 0.5):\n+            The factor of the partial rotary position.\n         head_dim (`int`, *optional*, defaults to 128):\n             The attention head dimension.\n         hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n@@ -63,8 +67,10 @@ class Glm4Config(PreTrainedConfig):\n             relevant if `config.is_decoder=True`.\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie weight embeddings\n-        rope_theta (`float`, *optional*, defaults to 10000.0):\n-            The base period of the RoPE embeddings.\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         pad_token_id (`int`, *optional*, defaults to 151329):\n             Padding token id.\n         eos_token_id (`int` | `list`, *optional*, defaults to `[151329, 151336, 151338]`):\n@@ -101,26 +107,26 @@ class Glm4Config(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size=151552,\n-        hidden_size=4096,\n-        intermediate_size=13696,\n-        num_hidden_layers=40,\n-        num_attention_heads=32,\n-        num_key_value_heads=2,\n-        partial_rotary_factor=0.5,\n-        head_dim=128,\n-        hidden_act=\"silu\",\n-        attention_dropout=0.0,\n-        max_position_embeddings=131072,\n-        initializer_range=0.02,\n-        rms_norm_eps=0.00000015625,\n-        use_cache=True,\n-        tie_word_embeddings=False,\n-        rope_theta=10000.0,\n-        pad_token_id=151329,\n-        eos_token_id=[151329, 151336, 151338],\n-        bos_token_id=None,\n-        attention_bias=True,\n+        vocab_size: Optional[int] = 151552,\n+        hidden_size: Optional[int] = 4096,\n+        intermediate_size: Optional[int] = 13696,\n+        num_hidden_layers: Optional[int] = 40,\n+        num_attention_heads: Optional[int] = 32,\n+        num_key_value_heads: Optional[int] = 2,\n+        partial_rotary_factor: Optional[float] = 0.5,\n+        head_dim: Optional[int] = 128,\n+        hidden_act: Optional[str] = \"silu\",\n+        attention_dropout: Optional[float] = 0.0,\n+        max_position_embeddings: Optional[int] = 131072,\n+        initializer_range: Optional[float] = 0.02,\n+        rms_norm_eps: Optional[float] = 0.00000015625,\n+        use_cache: Optional[bool] = True,\n+        tie_word_embeddings: Optional[bool] = False,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        pad_token_id: Optional[int] = 151329,\n+        eos_token_id: Optional[list[int]] = [151329, 151336, 151338],\n+        bos_token_id: Optional[int] = None,\n+        attention_bias: Optional[bool] = True,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size\n@@ -136,9 +142,16 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.rms_norm_eps = rms_norm_eps\n         self.use_cache = use_cache\n-        self.rope_theta = rope_theta\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n+\n+        # Validate the correctness of rotary position embeddings parameters\n+        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n+        rope_config_validation(self)\n \n         super().__init__(\n             pad_token_id=pad_token_id,"
        },
        {
            "sha": "935a722fd1db4d101691157f95ca76c2d50ad66a",
            "filename": "src/transformers/models/glm4/modeling_glm4.py",
            "status": "modified",
            "additions": 65,
            "deletions": 34,
            "changes": 99,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -83,7 +83,7 @@ def forward(\n         past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         residual = hidden_states\n@@ -225,8 +225,8 @@ def __init__(self, config: Glm4Config, layer_idx: Optional[int] = None):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n-        attention_mask: Optional[torch.Tensor],\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -266,46 +266,56 @@ def forward(\n         return attn_output, attn_weights\n \n \n-@use_kernel_forward_from_hub(\"RMSNorm\")\n-class Glm4RMSNorm(nn.Module):\n-    def __init__(self, hidden_size, eps=1e-6):\n-        \"\"\"\n-        Glm4RMSNorm is equivalent to T5LayerNorm\n-        \"\"\"\n-        super().__init__()\n-        self.weight = nn.Parameter(torch.ones(hidden_size))\n-        self.variance_epsilon = eps\n-\n-    def forward(self, hidden_states):\n-        input_dtype = hidden_states.dtype\n-        hidden_states = hidden_states.to(torch.float32)\n-        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n-        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n-        return self.weight * hidden_states.to(input_dtype)\n-\n-    def extra_repr(self):\n-        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n-\n-\n class Glm4RotaryEmbedding(nn.Module):\n     inv_freq: torch.Tensor  # fix linting for `register_buffer`\n \n     def __init__(self, config: Glm4Config, device=None):\n         super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n         self.max_seq_len_cached = config.max_position_embeddings\n         self.original_max_seq_len = config.max_position_embeddings\n \n         self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.rope_type = self.config.rope_parameters[\"rope_type\"]\n+        rope_init_fn: Callable = self.compute_default_rope_parameters\n+        if self.rope_type != \"default\":\n+            rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+        inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n+\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n+        self.original_inv_freq = inv_freq\n+\n+    @staticmethod\n+    def compute_default_rope_parameters(\n+        config: Optional[Glm4Config] = None,\n+        device: Optional[\"torch.device\"] = None,\n+        seq_len: Optional[int] = None,\n+    ) -> tuple[\"torch.Tensor\", float]:\n+        \"\"\"\n+        Computes the inverse frequencies according to the original RoPE implementation\n+        Args:\n+            config ([`~transformers.PreTrainedConfig`]):\n+                The model configuration.\n+            device (`torch.device`):\n+                The device to use for initialization of the inverse frequencies.\n+            seq_len (`int`, *optional*):\n+                The current sequence length. Unused for this type of RoPE.\n+        Returns:\n+            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n+            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n+        \"\"\"\n+        base = config.rope_parameters[\"rope_theta\"]\n+        partial_rotary_factor = getattr(config, \"partial_rotary_factor\", 1.0)\n+        head_dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+        dim = int(head_dim * partial_rotary_factor)\n+\n+        attention_factor = 1.0  # Unused in this type of RoPE\n+\n+        # Compute the inverse frequencies\n+        inv_freq = 1.0 / (\n+            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n+        )\n+        return inv_freq, attention_factor\n \n     @torch.no_grad()\n     @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n@@ -323,6 +333,27 @@ def forward(self, x, position_ids):\n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n \n \n+@use_kernel_forward_from_hub(\"RMSNorm\")\n+class Glm4RMSNorm(nn.Module):\n+    def __init__(self, hidden_size, eps=1e-6):\n+        \"\"\"\n+        Glm4RMSNorm is equivalent to T5LayerNorm\n+        \"\"\"\n+        super().__init__()\n+        self.weight = nn.Parameter(torch.ones(hidden_size))\n+        self.variance_epsilon = eps\n+\n+    def forward(self, hidden_states):\n+        input_dtype = hidden_states.dtype\n+        hidden_states = hidden_states.to(torch.float32)\n+        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n+        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n+        return self.weight * hidden_states.to(input_dtype)\n+\n+    def extra_repr(self):\n+        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n+\n+\n @auto_docstring\n class Glm4PreTrainedModel(PreTrainedModel):\n     config: Glm4Config\n@@ -401,16 +432,16 @@ def forward(\n         )\n \n         hidden_states = inputs_embeds\n-        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids=position_ids)\n \n         for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n             hidden_states = decoder_layer(\n                 hidden_states,\n                 attention_mask=causal_mask,\n+                position_embeddings=position_embeddings,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n                 cache_position=cache_position,\n-                position_embeddings=position_embeddings,\n                 **kwargs,\n             )\n "
        },
        {
            "sha": "ff03376eb43525d100a52ed189562f4643cc6ea0",
            "filename": "src/transformers/models/glm4/modular_glm4.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodular_glm4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodular_glm4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodular_glm4.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -58,7 +58,7 @@ def forward(\n         past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         residual = hidden_states"
        },
        {
            "sha": "a35dec5f4e3fa94e5b44e74dc0396c87d21d40f6",
            "filename": "src/transformers/models/glm4_moe/configuration_glm4_moe.py",
            "status": "modified",
            "additions": 39,
            "deletions": 72,
            "changes": 111,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fconfiguration_glm4_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fconfiguration_glm4_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fconfiguration_glm4_moe.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -19,8 +19,10 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n+from typing import Optional\n+\n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import rope_config_validation\n+from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n \n \n class Glm4MoeConfig(PreTrainedConfig):\n@@ -68,45 +70,10 @@ class Glm4MoeConfig(PreTrainedConfig):\n             relevant if `config.is_decoder=True`.\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether the model's input and output word embeddings should be tied.\n-        rope_theta (`float`, *optional*, defaults to 10000.0):\n-            The base period of the RoPE embeddings.\n-        rope_scaling (`Dict`, *optional*):\n-            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n-            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n-            accordingly.\n-            Expected contents:\n-                `rope_type` (`str`):\n-                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n-                    'llama3'], with 'default' being the original RoPE implementation.\n-                `factor` (`float`, *optional*):\n-                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n-                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n-                    original maximum pre-trained length.\n-                `original_max_position_embeddings` (`int`, *optional*):\n-                    Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n-                    pretraining.\n-                `attention_factor` (`float`, *optional*):\n-                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n-                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n-                    `factor` field to infer the suggested value.\n-                `beta_fast` (`float`, *optional*):\n-                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n-                    ramp function. If unspecified, it defaults to 32.\n-                `beta_slow` (`float`, *optional*):\n-                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n-                    ramp function. If unspecified, it defaults to 1.\n-                `short_factor` (`list[float]`, *optional*):\n-                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n-                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n-                    size divided by the number of attention heads divided by 2\n-                `long_factor` (`list[float]`, *optional*):\n-                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n-                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n-                    size divided by the number of attention heads divided by 2\n-                `low_freq_factor` (`float`, *optional*):\n-                    Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n-                `high_freq_factor` (`float`, *optional*):\n-                    Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):\n             Whether to use a bias in the query, key, value and output projection layers during self-attention.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n@@ -172,33 +139,32 @@ class Glm4MoeConfig(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size=151552,\n-        hidden_size=4096,\n-        intermediate_size=10944,\n-        num_hidden_layers=46,\n-        num_attention_heads=96,\n-        partial_rotary_factor=0.5,\n-        num_key_value_heads=8,\n-        hidden_act=\"silu\",\n-        max_position_embeddings=131072,\n-        initializer_range=0.02,\n-        rms_norm_eps=1e-5,\n-        use_cache=True,\n-        tie_word_embeddings=False,\n-        rope_theta=10000.0,\n-        rope_scaling=None,\n-        attention_bias=False,\n-        attention_dropout=0.0,\n-        moe_intermediate_size=1408,\n-        num_experts_per_tok=8,\n-        n_shared_experts=1,\n-        n_routed_experts=128,\n-        routed_scaling_factor=1.0,\n-        n_group=1,\n-        topk_group=1,\n-        first_k_dense_replace=1,\n-        norm_topk_prob=True,\n-        use_qk_norm=False,\n+        vocab_size: Optional[int] = 151552,\n+        hidden_size: Optional[int] = 4096,\n+        intermediate_size: Optional[int] = 10944,\n+        num_hidden_layers: Optional[int] = 46,\n+        num_attention_heads: Optional[int] = 96,\n+        partial_rotary_factor: Optional[float] = 0.5,\n+        num_key_value_heads: Optional[int] = 8,\n+        hidden_act: Optional[str] = \"silu\",\n+        max_position_embeddings: Optional[int] = 131072,\n+        initializer_range: Optional[float] = 0.02,\n+        rms_norm_eps: Optional[int] = 1e-5,\n+        use_cache: Optional[bool] = True,\n+        tie_word_embeddings: Optional[bool] = False,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        attention_bias: Optional[bool] = False,\n+        attention_dropout: Optional[float] = 0.0,\n+        moe_intermediate_size: Optional[int] = 1408,\n+        num_experts_per_tok: Optional[int] = 8,\n+        n_shared_experts: Optional[int] = 1,\n+        n_routed_experts: Optional[int] = 128,\n+        routed_scaling_factor: Optional[float] = 1.0,\n+        n_group: Optional[int] = 1,\n+        topk_group: Optional[int] = 1,\n+        first_k_dense_replace: Optional[int] = 1,\n+        norm_topk_prob: Optional[bool] = True,\n+        use_qk_norm: Optional[bool] = False,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size\n@@ -214,14 +180,15 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.rms_norm_eps = rms_norm_eps\n         self.use_cache = use_cache\n-        self.rope_theta = rope_theta\n-        self.rope_scaling = rope_scaling\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n+\n         # Validate the correctness of rotary position embeddings parameters\n-        # BC: if there is a 'type' field, move it to 'rope_type'.\n-        if self.rope_scaling is not None and \"type\" in self.rope_scaling:\n-            self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n+        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n         rope_config_validation(self)\n \n         # MoE arguments"
        },
        {
            "sha": "00afc27bf236e54630b836389dd0ef3cacc95d1c",
            "filename": "src/transformers/models/glm4_moe/modeling_glm4_moe.py",
            "status": "modified",
            "additions": 71,
            "deletions": 40,
            "changes": 111,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -42,6 +42,73 @@\n from .configuration_glm4_moe import Glm4MoeConfig\n \n \n+class Glm4MoeRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n+    def __init__(self, config: Glm4MoeConfig, device=None):\n+        super().__init__()\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+\n+        self.rope_type = self.config.rope_parameters[\"rope_type\"]\n+        rope_init_fn: Callable = self.compute_default_rope_parameters\n+        if self.rope_type != \"default\":\n+            rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+        inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n+\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = inv_freq\n+\n+    @staticmethod\n+    def compute_default_rope_parameters(\n+        config: Optional[Glm4MoeConfig] = None,\n+        device: Optional[\"torch.device\"] = None,\n+        seq_len: Optional[int] = None,\n+    ) -> tuple[\"torch.Tensor\", float]:\n+        \"\"\"\n+        Computes the inverse frequencies according to the original RoPE implementation\n+        Args:\n+            config ([`~transformers.PreTrainedConfig`]):\n+                The model configuration.\n+            device (`torch.device`):\n+                The device to use for initialization of the inverse frequencies.\n+            seq_len (`int`, *optional*):\n+                The current sequence length. Unused for this type of RoPE.\n+        Returns:\n+            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n+            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n+        \"\"\"\n+        base = config.rope_parameters[\"rope_theta\"]\n+        partial_rotary_factor = getattr(config, \"partial_rotary_factor\", 1.0)\n+        head_dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+        dim = int(head_dim * partial_rotary_factor)\n+\n+        attention_factor = 1.0  # Unused in this type of RoPE\n+\n+        # Compute the inverse frequencies\n+        inv_freq = 1.0 / (\n+            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n+        )\n+        return inv_freq, attention_factor\n+\n+    @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n+    def forward(self, x, position_ids):\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     \"\"\"\n     This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n@@ -135,7 +202,7 @@ def __init__(self, config: Glm4MoeConfig, layer_idx: Optional[int] = None):\n         self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n         self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n         self.scaling = self.head_dim**-0.5\n-        self.rope_scaling = config.rope_scaling\n+        self.rope_parameters = config.rope_parameters\n         self.attention_dropout = config.attention_dropout\n         self.is_causal = True\n \n@@ -376,7 +443,7 @@ def forward(\n         past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> torch.Tensor:\n         residual = hidden_states\n@@ -425,42 +492,6 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n \n \n-class Glm4MoeRotaryEmbedding(nn.Module):\n-    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n-\n-    def __init__(self, config: Glm4MoeConfig, device=None):\n-        super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n-        self.max_seq_len_cached = config.max_position_embeddings\n-        self.original_max_seq_len = config.max_position_embeddings\n-\n-        self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n-\n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n-        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n-\n-    @torch.no_grad()\n-    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n-    def forward(self, x, position_ids):\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n-        position_ids_expanded = position_ids[:, None, :].float()\n-\n-        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n-            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos() * self.attention_scaling\n-            sin = emb.sin() * self.attention_scaling\n-\n-        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n-\n-\n @auto_docstring\n class Glm4MoeModel(Glm4MoePreTrainedModel):\n     _keys_to_ignore_on_load_unexpected = [r\"model\\.layers\\.92.*\", r\"model\\.layers\\.46.*\"]\n@@ -522,16 +553,16 @@ def forward(\n         )\n \n         hidden_states = inputs_embeds\n-        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids=position_ids)\n \n         for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n             hidden_states = decoder_layer(\n                 hidden_states,\n                 attention_mask=causal_mask,\n+                position_embeddings=position_embeddings,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n                 cache_position=cache_position,\n-                position_embeddings=position_embeddings,\n                 **kwargs,\n             )\n "
        },
        {
            "sha": "db1f22e58e45053fb1bf67176069f2cd6ffb3e97",
            "filename": "src/transformers/models/glm4_moe/modular_glm4_moe.py",
            "status": "modified",
            "additions": 43,
            "deletions": 73,
            "changes": 116,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodular_glm4_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodular_glm4_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodular_glm4_moe.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -20,7 +20,7 @@\n from torch import nn\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import rope_config_validation\n+from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n from ...utils import logging\n from ..cohere.modeling_cohere import CohereAttention\n from ..deepseek_v3.modeling_deepseek_v3 import (\n@@ -32,6 +32,7 @@\n     DeepseekV3RMSNorm,\n     DeepseekV3TopkRouter,\n )\n+from ..glm.modeling_glm import GlmRotaryEmbedding\n from ..gpt_neox.modeling_gpt_neox import apply_rotary_pos_emb  # noqa\n \n \n@@ -83,45 +84,10 @@ class Glm4MoeConfig(PreTrainedConfig):\n             relevant if `config.is_decoder=True`.\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether the model's input and output word embeddings should be tied.\n-        rope_theta (`float`, *optional*, defaults to 10000.0):\n-            The base period of the RoPE embeddings.\n-        rope_scaling (`Dict`, *optional*):\n-            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n-            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n-            accordingly.\n-            Expected contents:\n-                `rope_type` (`str`):\n-                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n-                    'llama3'], with 'default' being the original RoPE implementation.\n-                `factor` (`float`, *optional*):\n-                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n-                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n-                    original maximum pre-trained length.\n-                `original_max_position_embeddings` (`int`, *optional*):\n-                    Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n-                    pretraining.\n-                `attention_factor` (`float`, *optional*):\n-                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n-                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n-                    `factor` field to infer the suggested value.\n-                `beta_fast` (`float`, *optional*):\n-                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n-                    ramp function. If unspecified, it defaults to 32.\n-                `beta_slow` (`float`, *optional*):\n-                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n-                    ramp function. If unspecified, it defaults to 1.\n-                `short_factor` (`list[float]`, *optional*):\n-                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n-                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n-                    size divided by the number of attention heads divided by 2\n-                `long_factor` (`list[float]`, *optional*):\n-                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n-                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n-                    size divided by the number of attention heads divided by 2\n-                `low_freq_factor` (`float`, *optional*):\n-                    Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n-                `high_freq_factor` (`float`, *optional*):\n-                    Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):\n             Whether to use a bias in the query, key, value and output projection layers during self-attention.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n@@ -187,33 +153,32 @@ class Glm4MoeConfig(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size=151552,\n-        hidden_size=4096,\n-        intermediate_size=10944,\n-        num_hidden_layers=46,\n-        num_attention_heads=96,\n-        partial_rotary_factor=0.5,\n-        num_key_value_heads=8,\n-        hidden_act=\"silu\",\n-        max_position_embeddings=131072,\n-        initializer_range=0.02,\n-        rms_norm_eps=1e-5,\n-        use_cache=True,\n-        tie_word_embeddings=False,\n-        rope_theta=10000.0,\n-        rope_scaling=None,\n-        attention_bias=False,\n-        attention_dropout=0.0,\n-        moe_intermediate_size=1408,\n-        num_experts_per_tok=8,\n-        n_shared_experts=1,\n-        n_routed_experts=128,\n-        routed_scaling_factor=1.0,\n-        n_group=1,\n-        topk_group=1,\n-        first_k_dense_replace=1,\n-        norm_topk_prob=True,\n-        use_qk_norm=False,\n+        vocab_size: Optional[int] = 151552,\n+        hidden_size: Optional[int] = 4096,\n+        intermediate_size: Optional[int] = 10944,\n+        num_hidden_layers: Optional[int] = 46,\n+        num_attention_heads: Optional[int] = 96,\n+        partial_rotary_factor: Optional[float] = 0.5,\n+        num_key_value_heads: Optional[int] = 8,\n+        hidden_act: Optional[str] = \"silu\",\n+        max_position_embeddings: Optional[int] = 131072,\n+        initializer_range: Optional[float] = 0.02,\n+        rms_norm_eps: Optional[int] = 1e-5,\n+        use_cache: Optional[bool] = True,\n+        tie_word_embeddings: Optional[bool] = False,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        attention_bias: Optional[bool] = False,\n+        attention_dropout: Optional[float] = 0.0,\n+        moe_intermediate_size: Optional[int] = 1408,\n+        num_experts_per_tok: Optional[int] = 8,\n+        n_shared_experts: Optional[int] = 1,\n+        n_routed_experts: Optional[int] = 128,\n+        routed_scaling_factor: Optional[float] = 1.0,\n+        n_group: Optional[int] = 1,\n+        topk_group: Optional[int] = 1,\n+        first_k_dense_replace: Optional[int] = 1,\n+        norm_topk_prob: Optional[bool] = True,\n+        use_qk_norm: Optional[bool] = False,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size\n@@ -229,14 +194,15 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.rms_norm_eps = rms_norm_eps\n         self.use_cache = use_cache\n-        self.rope_theta = rope_theta\n-        self.rope_scaling = rope_scaling\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n+\n         # Validate the correctness of rotary position embeddings parameters\n-        # BC: if there is a 'type' field, move it to 'rope_type'.\n-        if self.rope_scaling is not None and \"type\" in self.rope_scaling:\n-            self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n+        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n         rope_config_validation(self)\n \n         # MoE arguments\n@@ -257,6 +223,10 @@ def __init__(\n         )\n \n \n+class Glm4MoeRotaryEmbedding(GlmRotaryEmbedding):\n+    pass\n+\n+\n class Glm4MoeAttention(CohereAttention):\n     def __init__(self, config: Glm4MoeConfig, layer_idx: Optional[int] = None):\n         nn.Module.__init__(self)\n@@ -265,7 +235,7 @@ def __init__(self, config: Glm4MoeConfig, layer_idx: Optional[int] = None):\n         self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n         self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n         self.scaling = self.head_dim**-0.5\n-        self.rope_scaling = config.rope_scaling\n+        self.rope_parameters = config.rope_parameters\n         self.attention_dropout = config.attention_dropout\n         self.is_causal = True\n "
        },
        {
            "sha": "e316c14079bdb7f83a012be2a333ee08c9da5cb7",
            "filename": "src/transformers/models/glm4v/configuration_glm4v.py",
            "status": "modified",
            "additions": 28,
            "deletions": 44,
            "changes": 72,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fglm4v%2Fconfiguration_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fglm4v%2Fconfiguration_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fconfiguration_glm4v.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -18,8 +18,10 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n+from typing import Optional\n+\n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import rope_config_validation\n+from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n \n \n class Glm4vVisionConfig(PreTrainedConfig):\n@@ -161,29 +163,12 @@ class Glm4vTextConfig(PreTrainedConfig):\n             relevant if `config.is_decoder=True`.\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether the model's input and output word embeddings should be tied.\n-        rope_theta (`float`, *optional*, defaults to 10000.0):\n-            The base period of the RoPE embeddings.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n             The dropout ratio for the attention probabilities.\n-        rope_scaling (`Dict`, *optional*):\n-            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n-            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n-            accordingly.\n-            Expected contents:\n-                `rope_type` (`str`):\n-                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n-                    'llama3'], with 'default' being the original RoPE implementation.\n-                `factor` (`float`, *optional*):\n-                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n-                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n-                    original maximum pre-trained length.\n-                `original_max_position_embeddings` (`int`, *optional*):\n-                    Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n-                    pretraining.\n-                `attention_factor` (`float`, *optional*):\n-                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n-                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n-                    `factor` field to infer the suggested value.\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         image_token_id (`int`, *optional*):\n             Token index used as placeholder for image embeddings.\n         video_token_id (`int`, *optional*):\n@@ -222,23 +207,22 @@ class Glm4vTextConfig(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size=151552,\n-        hidden_size=4096,\n-        intermediate_size=13696,\n-        num_hidden_layers=40,\n-        num_attention_heads=32,\n-        num_key_value_heads=2,\n-        hidden_act=\"silu\",\n-        max_position_embeddings=32768,\n-        initializer_range=0.02,\n-        rms_norm_eps=1e-05,\n-        use_cache=True,\n-        tie_word_embeddings=False,\n-        rope_theta=10000.0,\n-        attention_dropout=0.0,\n-        rope_scaling=None,\n-        image_token_id=None,\n-        video_token_id=None,\n+        vocab_size: Optional[int] = 151552,\n+        hidden_size: Optional[int] = 4096,\n+        intermediate_size: Optional[int] = 13696,\n+        num_hidden_layers: Optional[int] = 40,\n+        num_attention_heads: Optional[int] = 32,\n+        num_key_value_heads: Optional[int] = 2,\n+        hidden_act: Optional[str] = \"silu\",\n+        max_position_embeddings: Optional[int] = 32768,\n+        initializer_range: Optional[float] = 0.02,\n+        rms_norm_eps: Optional[int] = 1e-05,\n+        use_cache: Optional[bool] = True,\n+        tie_word_embeddings: Optional[bool] = False,\n+        attention_dropout: Optional[float] = 0.0,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        image_token_id: Optional[int] = None,\n+        video_token_id: Optional[int] = None,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size\n@@ -257,14 +241,14 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.rms_norm_eps = rms_norm_eps\n         self.use_cache = use_cache\n-        self.rope_theta = rope_theta\n         self.attention_dropout = attention_dropout\n-        self.rope_scaling = rope_scaling\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n \n         # Validate the correctness of rotary position embeddings parameters\n-        # BC: if there is a 'type' field, move it to 'rope_type'.\n-        if self.rope_scaling is not None and \"type\" in self.rope_scaling:\n-            self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n+        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n         rope_config_validation(self, ignore_keys={\"mrope_section\"})\n         self.image_token_id = image_token_id\n         self.video_token_id = video_token_id"
        },
        {
            "sha": "966ef9506027e61b28c3fc9d9693c81e87f9f1fc",
            "filename": "src/transformers/models/glm4v/convert_glm4v_mgt_weights_to_hf.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fglm4v%2Fconvert_glm4v_mgt_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fglm4v%2Fconvert_glm4v_mgt_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fconvert_glm4v_mgt_weights_to_hf.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -626,8 +626,8 @@ def merge_tp_weights(model_path, output_path, vllm_config_path=None):\n         }\n         hf_config[\"vision_config\"] = vision_config\n \n-    if \"rope_scaling\" in model_config:\n-        hf_config[\"rope_scaling\"] = model_config[\"rope_scaling\"]\n+    if \"rope_parameters\" in model_config:\n+        hf_config[\"rope_parameters\"] = model_config[\"rope_parameters\"]\n \n     config_path = os.path.join(output_path, \"config.json\")\n     with open(config_path, \"w\") as f:"
        },
        {
            "sha": "147e18b7e78ecb26a3544d52bf483b95e870931d",
            "filename": "src/transformers/models/glm4v/modeling_glm4v.py",
            "status": "modified",
            "additions": 46,
            "deletions": 18,
            "changes": 64,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -391,25 +391,56 @@ class Glm4vTextRotaryEmbedding(nn.Module):\n \n     def __init__(self, config: Glm4vTextConfig, device=None):\n         super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n         self.max_seq_len_cached = config.max_position_embeddings\n         self.original_max_seq_len = config.max_position_embeddings\n \n         self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.rope_type = self.config.rope_parameters[\"rope_type\"]\n+        rope_init_fn: Callable = self.compute_default_rope_parameters\n+        if self.rope_type != \"default\":\n+            rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+        inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n+\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n+        self.original_inv_freq = inv_freq\n+\n+    @staticmethod\n+    def compute_default_rope_parameters(\n+        config: Optional[Glm4vTextConfig] = None,\n+        device: Optional[\"torch.device\"] = None,\n+        seq_len: Optional[int] = None,\n+    ) -> tuple[\"torch.Tensor\", float]:\n+        \"\"\"\n+        Computes the inverse frequencies according to the original RoPE implementation\n+        Args:\n+            config ([`~transformers.PreTrainedConfig`]):\n+                The model configuration.\n+            device (`torch.device`):\n+                The device to use for initialization of the inverse frequencies.\n+            seq_len (`int`, *optional*):\n+                The current sequence length. Unused for this type of RoPE.\n+        Returns:\n+            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n+            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n+        \"\"\"\n+        base = config.rope_parameters[\"rope_theta\"]\n+        partial_rotary_factor = getattr(config, \"partial_rotary_factor\", 1.0)\n+        head_dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+        dim = int(head_dim * partial_rotary_factor)\n+\n+        attention_factor = 1.0  # Unused in this type of RoPE\n+\n+        # Compute the inverse frequencies\n+        inv_freq = 1.0 / (\n+            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n+        )\n+        return inv_freq, attention_factor\n \n     @torch.no_grad()\n     @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n     def forward(self, x, position_ids):\n-        # In contrast to other models, Glm4vText has different position ids for the grids\n+        # In contrast to other models, GLM4V different position ids for the grids\n         # So we expand the inv_freq to shape (3, ...)\n         inv_freq_expanded = self.inv_freq[None, None, :, None].float().expand(3, position_ids.shape[1], -1, 1)\n         position_ids_expanded = position_ids[:, :, None, :].float()  # shape (3, bs, 1, positions)\n@@ -506,7 +537,7 @@ def __init__(self, config: Glm4vTextConfig, layer_idx: Optional[int] = None):\n         self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n         self.is_causal = True\n         self.attention_dropout = config.attention_dropout\n-        self.rope_scaling = config.rope_scaling\n+        self.rope_parameters = config.rope_parameters\n         self.scaling = self.head_dim**-0.5\n \n         self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=True)\n@@ -517,9 +548,8 @@ def __init__(self, config: Glm4vTextConfig, layer_idx: Optional[int] = None):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n@@ -536,7 +566,7 @@ def forward(\n \n         cos, sin = position_embeddings\n         query_states, key_states = apply_multimodal_rotary_pos_emb(  # diff with Llama\n-            query_states, key_states, cos, sin, self.rope_scaling[\"mrope_section\"]\n+            query_states, key_states, cos, sin, self.rope_parameters[\"mrope_section\"]\n         )\n \n         if past_key_values is not None:\n@@ -596,7 +626,7 @@ def __init__(self, config: Glm4vTextConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -872,18 +902,16 @@ def forward(\n         causal_mask = create_causal_mask(**mask_kwargs)\n \n         hidden_states = inputs_embeds\n-\n-        # create position embeddings to be shared across the decoder layers\n-        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids=position_ids)\n \n         for decoder_layer in self.layers:\n             layer_outputs = decoder_layer(\n                 hidden_states,\n-                position_embeddings=position_embeddings,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n                 cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n                 **kwargs,\n             )\n             hidden_states = layer_outputs"
        },
        {
            "sha": "b34fef15b6422805dd9cac8f673b4e7b54e5b4a6",
            "filename": "src/transformers/models/glm4v/modular_glm4v.py",
            "status": "modified",
            "additions": 49,
            "deletions": 57,
            "changes": 106,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -31,14 +31,14 @@\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast\n-from ...modeling_rope_utils import rope_config_validation\n+from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_torchdynamo_compiling, logging\n from ...utils.generic import check_model_inputs\n from ...video_utils import VideoInput\n-from ..glm4.modeling_glm4 import Glm4MLP, Glm4RMSNorm, eager_attention_forward\n+from ..glm4.modeling_glm4 import Glm4MLP, Glm4RMSNorm, Glm4RotaryEmbedding, eager_attention_forward\n from ..qwen2_5_vl.modeling_qwen2_5_vl import (\n     Qwen2_5_VisionPatchEmbed,\n     Qwen2_5_VisionRotaryEmbedding,\n@@ -48,7 +48,6 @@\n     Qwen2_5_VLModel,\n     Qwen2_5_VLModelOutputWithPast,\n     Qwen2_5_VLPreTrainedModel,\n-    Qwen2_5_VLRotaryEmbedding,\n     Qwen2_5_VLTextModel,\n     Qwen2_5_VLVisionAttention,\n     Qwen2_5_VLVisionBlock,\n@@ -201,29 +200,12 @@ class Glm4vTextConfig(PreTrainedConfig):\n             relevant if `config.is_decoder=True`.\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether the model's input and output word embeddings should be tied.\n-        rope_theta (`float`, *optional*, defaults to 10000.0):\n-            The base period of the RoPE embeddings.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n             The dropout ratio for the attention probabilities.\n-        rope_scaling (`Dict`, *optional*):\n-            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n-            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n-            accordingly.\n-            Expected contents:\n-                `rope_type` (`str`):\n-                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n-                    'llama3'], with 'default' being the original RoPE implementation.\n-                `factor` (`float`, *optional*):\n-                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n-                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n-                    original maximum pre-trained length.\n-                `original_max_position_embeddings` (`int`, *optional*):\n-                    Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n-                    pretraining.\n-                `attention_factor` (`float`, *optional*):\n-                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n-                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n-                    `factor` field to infer the suggested value.\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         image_token_id (`int`, *optional*):\n             Token index used as placeholder for image embeddings.\n         video_token_id (`int`, *optional*):\n@@ -262,23 +244,22 @@ class Glm4vTextConfig(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size=151552,\n-        hidden_size=4096,\n-        intermediate_size=13696,\n-        num_hidden_layers=40,\n-        num_attention_heads=32,\n-        num_key_value_heads=2,\n-        hidden_act=\"silu\",\n-        max_position_embeddings=32768,\n-        initializer_range=0.02,\n-        rms_norm_eps=1e-05,\n-        use_cache=True,\n-        tie_word_embeddings=False,\n-        rope_theta=10000.0,\n-        attention_dropout=0.0,\n-        rope_scaling=None,\n-        image_token_id=None,\n-        video_token_id=None,\n+        vocab_size: Optional[int] = 151552,\n+        hidden_size: Optional[int] = 4096,\n+        intermediate_size: Optional[int] = 13696,\n+        num_hidden_layers: Optional[int] = 40,\n+        num_attention_heads: Optional[int] = 32,\n+        num_key_value_heads: Optional[int] = 2,\n+        hidden_act: Optional[str] = \"silu\",\n+        max_position_embeddings: Optional[int] = 32768,\n+        initializer_range: Optional[float] = 0.02,\n+        rms_norm_eps: Optional[int] = 1e-05,\n+        use_cache: Optional[bool] = True,\n+        tie_word_embeddings: Optional[bool] = False,\n+        attention_dropout: Optional[float] = 0.0,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        image_token_id: Optional[int] = None,\n+        video_token_id: Optional[int] = None,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size\n@@ -297,14 +278,14 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.rms_norm_eps = rms_norm_eps\n         self.use_cache = use_cache\n-        self.rope_theta = rope_theta\n         self.attention_dropout = attention_dropout\n-        self.rope_scaling = rope_scaling\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n \n         # Validate the correctness of rotary position embeddings parameters\n-        # BC: if there is a 'type' field, move it to 'rope_type'.\n-        if self.rope_scaling is not None and \"type\" in self.rope_scaling:\n-            self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n+        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n         rope_config_validation(self, ignore_keys={\"mrope_section\"})\n         self.image_token_id = image_token_id\n         self.video_token_id = video_token_id\n@@ -538,8 +519,22 @@ def __init__(self, config) -> None:\n         self.mlp = Glm4VisionMlp(config, bias=False)\n \n \n-class Glm4vTextRotaryEmbedding(Qwen2_5_VLRotaryEmbedding):\n-    pass\n+class Glm4vTextRotaryEmbedding(Glm4RotaryEmbedding):\n+    # Ignore copy\n+    def forward(self, x, position_ids):\n+        # In contrast to other models, GLM4V different position ids for the grids\n+        # So we expand the inv_freq to shape (3, ...)\n+        inv_freq_expanded = self.inv_freq[None, None, :, None].float().expand(3, position_ids.shape[1], -1, 1)\n+        position_ids_expanded = position_ids[:, :, None, :].float()  # shape (3, bs, 1, positions)\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(2, 3)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n \n \n def rotate_half_llm(x):\n@@ -624,7 +619,7 @@ def __init__(self, config: Glm4vTextConfig, layer_idx: Optional[int] = None):\n         self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n         self.is_causal = True\n         self.attention_dropout = config.attention_dropout\n-        self.rope_scaling = config.rope_scaling\n+        self.rope_parameters = config.rope_parameters\n         self.scaling = self.head_dim**-0.5\n \n         self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=True)\n@@ -635,9 +630,8 @@ def __init__(self, config: Glm4vTextConfig, layer_idx: Optional[int] = None):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n@@ -654,7 +648,7 @@ def forward(\n \n         cos, sin = position_embeddings\n         query_states, key_states = apply_multimodal_rotary_pos_emb(  # diff with Llama\n-            query_states, key_states, cos, sin, self.rope_scaling[\"mrope_section\"]\n+            query_states, key_states, cos, sin, self.rope_parameters[\"mrope_section\"]\n         )\n \n         if past_key_values is not None:\n@@ -700,7 +694,7 @@ def __init__(self, config: Glm4vTextConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -935,18 +929,16 @@ def forward(\n         causal_mask = create_causal_mask(**mask_kwargs)\n \n         hidden_states = inputs_embeds\n-\n-        # create position embeddings to be shared across the decoder layers\n-        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids=position_ids)\n \n         for decoder_layer in self.layers:\n             layer_outputs = decoder_layer(\n                 hidden_states,\n-                position_embeddings=position_embeddings,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n                 cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n                 **kwargs,\n             )\n             hidden_states = layer_outputs"
        },
        {
            "sha": "dc09238012433a3acd941726af29d263d5b0556b",
            "filename": "src/transformers/models/glm4v_moe/configuration_glm4v_moe.py",
            "status": "modified",
            "additions": 39,
            "deletions": 54,
            "changes": 93,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fconfiguration_glm4v_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fconfiguration_glm4v_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fconfiguration_glm4v_moe.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -18,8 +18,10 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n+from typing import Optional\n+\n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import rope_config_validation\n+from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n \n \n class Glm4vMoeVisionConfig(PreTrainedConfig):\n@@ -162,27 +164,10 @@ class Glm4vMoeTextConfig(PreTrainedConfig):\n             relevant if `config.is_decoder=True`.\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether the model's input and output word embeddings should be tied.\n-        rope_theta (`float`, *optional*, defaults to 10000.0):\n-            The base period of the RoPE embeddings.\n-        rope_scaling (`Dict`, *optional*):\n-            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n-            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n-            accordingly.\n-            Expected contents:\n-                `rope_type` (`str`):\n-                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n-                    'llama3'], with 'default' being the original RoPE implementation.\n-                `factor` (`float`, *optional*):\n-                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n-                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n-                    original maximum pre-trained length.\n-                `original_max_position_embeddings` (`int`, *optional*):\n-                    Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n-                    pretraining.\n-                `attention_factor` (`float`, *optional*):\n-                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n-                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n-                    `factor` field to infer the suggested value.\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         attention_bias (`bool`, defaults to `True`, *optional*, defaults to `True`):\n             Whether to use a bias in the query, key, value and output projection layers during self-attention.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n@@ -244,33 +229,32 @@ class Glm4vMoeTextConfig(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size=151424,\n-        hidden_size=4096,\n-        intermediate_size=10944,\n-        num_hidden_layers=46,\n-        num_attention_heads=96,\n-        partial_rotary_factor=0.5,\n-        num_key_value_heads=8,\n-        hidden_act=\"silu\",\n-        max_position_embeddings=65536,\n-        initializer_range=0.02,\n-        rms_norm_eps=1e-5,\n-        use_cache=True,\n-        tie_word_embeddings=False,\n-        rope_theta=10000.0,\n-        rope_scaling=None,\n-        attention_bias=True,\n-        attention_dropout=0.0,\n-        moe_intermediate_size=1408,\n-        num_experts_per_tok=8,\n-        n_shared_experts=1,\n-        n_routed_experts=128,\n-        routed_scaling_factor=1.0,\n-        n_group=1,\n-        topk_group=1,\n-        first_k_dense_replace=1,\n-        norm_topk_prob=True,\n-        router_aux_loss_coef=0.0001,\n+        vocab_size: Optional[int] = 151424,\n+        hidden_size: Optional[int] = 4096,\n+        intermediate_size: Optional[int] = 10944,\n+        num_hidden_layers: Optional[int] = 46,\n+        num_attention_heads: Optional[int] = 96,\n+        partial_rotary_factor: Optional[float] = 0.5,\n+        num_key_value_heads: Optional[int] = 8,\n+        hidden_act: Optional[str] = \"silu\",\n+        max_position_embeddings: Optional[int] = 65536,\n+        initializer_range: Optional[float] = 0.02,\n+        rms_norm_eps: Optional[int] = 1e-5,\n+        use_cache: Optional[bool] = True,\n+        tie_word_embeddings: Optional[bool] = False,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        attention_bias: Optional[bool] = True,\n+        attention_dropout: Optional[float] = 0.0,\n+        moe_intermediate_size: Optional[int] = 1408,\n+        num_experts_per_tok: Optional[int] = 8,\n+        n_shared_experts: Optional[int] = 1,\n+        n_routed_experts: Optional[int] = 128,\n+        routed_scaling_factor: Optional[float] = 1.0,\n+        n_group: Optional[int] = 1,\n+        topk_group: Optional[int] = 1,\n+        first_k_dense_replace: Optional[int] = 1,\n+        norm_topk_prob: Optional[bool] = True,\n+        router_aux_loss_coef: Optional[float] = 0.0001,\n         **kwargs,\n     ):\n         super().__init__(tie_word_embeddings=tie_word_embeddings, **kwargs)\n@@ -287,14 +271,15 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.rms_norm_eps = rms_norm_eps\n         self.use_cache = use_cache\n-        self.rope_theta = rope_theta\n-        self.rope_scaling = rope_scaling\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n+\n         # Validate the correctness of rotary position embeddings parameters\n-        # BC: if there is a 'type' field, move it to 'rope_type'.\n-        if self.rope_scaling is not None and \"type\" in self.rope_scaling:\n-            self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n+        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n         rope_config_validation(self, ignore_keys={\"mrope_section\"})\n \n         # MoE arguments"
        },
        {
            "sha": "40e6704fb7580d70ca27e610fbefb6c1e4135683",
            "filename": "src/transformers/models/glm4v_moe/modeling_glm4v_moe.py",
            "status": "modified",
            "additions": 68,
            "deletions": 37,
            "changes": 105,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -65,30 +65,85 @@ def extra_repr(self):\n         return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n \n \n+@dataclass\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Base class for Llava outputs, with hidden states and attentions.\n+    \"\"\"\n+)\n+class Glm4vMoeModelOutputWithPast(ModelOutput):\n+    r\"\"\"\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n+    rope_deltas (`torch.LongTensor` of shape `(batch_size, )`, *optional*):\n+        The rope index difference between sequence length and multimodal rope.\n+    \"\"\"\n+\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n+    past_key_values: Optional[Cache] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n+    attentions: Optional[tuple[torch.FloatTensor]] = None\n+    rope_deltas: Optional[torch.LongTensor] = None\n+\n+\n class Glm4vMoeTextRotaryEmbedding(nn.Module):\n     inv_freq: torch.Tensor  # fix linting for `register_buffer`\n \n-    def __init__(self, config: Glm4vMoeTextConfig, device=None):\n+    def __init__(self, config: Glm4vMoeTextConfig, device=None, layer_type=None):\n         super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n         self.max_seq_len_cached = config.max_position_embeddings\n         self.original_max_seq_len = config.max_position_embeddings\n \n         self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.rope_type = self.config.rope_parameters[\"rope_type\"]\n+        rope_init_fn: Callable = self.compute_default_rope_parameters\n+        if self.rope_type != \"default\":\n+            rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+        inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n+\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n+        self.original_inv_freq = inv_freq\n+\n+    @staticmethod\n+    def compute_default_rope_parameters(\n+        config: Optional[Glm4vMoeTextConfig] = None,\n+        device: Optional[\"torch.device\"] = None,\n+        seq_len: Optional[int] = None,\n+    ) -> tuple[\"torch.Tensor\", float]:\n+        \"\"\"\n+        Computes the inverse frequencies according to the original RoPE implementation\n+        Args:\n+            config ([`~transformers.PreTrainedConfig`]):\n+                The model configuration.\n+            device (`torch.device`):\n+                The device to use for initialization of the inverse frequencies.\n+            seq_len (`int`, *optional*):\n+                The current sequence length. Unused for this type of RoPE.\n+        Returns:\n+            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n+            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n+        \"\"\"\n+        base = config.rope_parameters[\"rope_theta\"]\n+        partial_rotary_factor = getattr(config, \"partial_rotary_factor\", 1.0)\n+        head_dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+        dim = int(head_dim * partial_rotary_factor)\n+\n+        attention_factor = 1.0  # Unused in this type of RoPE\n+\n+        # Compute the inverse frequencies\n+        inv_freq = 1.0 / (\n+            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n+        )\n+        return inv_freq, attention_factor\n \n     @torch.no_grad()\n     @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n     def forward(self, x, position_ids):\n-        # In contrast to other models, Glm4vMoeText has different position ids for the grids\n+        # In contrast to other models, GLM4V_MOE different position ids for the grids\n         # So we expand the inv_freq to shape (3, ...)\n         inv_freq_expanded = self.inv_freq[None, None, :, None].float().expand(3, position_ids.shape[1], -1, 1)\n         position_ids_expanded = position_ids[:, :, None, :].float()  # shape (3, bs, 1, positions)\n@@ -103,30 +158,6 @@ def forward(self, x, position_ids):\n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n \n \n-@dataclass\n-@auto_docstring(\n-    custom_intro=\"\"\"\n-    Base class for Llava outputs, with hidden states and attentions.\n-    \"\"\"\n-)\n-class Glm4vMoeModelOutputWithPast(ModelOutput):\n-    r\"\"\"\n-    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n-\n-        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n-        `past_key_values` input) to speed up sequential decoding.\n-    rope_deltas (`torch.LongTensor` of shape `(batch_size, )`, *optional*):\n-        The rope index difference between sequence length and multimodal rope.\n-    \"\"\"\n-\n-    last_hidden_state: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[Cache] = None\n-    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n-    attentions: Optional[tuple[torch.FloatTensor]] = None\n-    rope_deltas: Optional[torch.LongTensor] = None\n-\n-\n def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     \"\"\"\n     This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n@@ -248,7 +279,7 @@ def __init__(self, config: Glm4vMoeTextConfig, layer_idx: Optional[int] = None):\n             config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n         )\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n-        self.rope_scaling = config.rope_scaling\n+        self.rope_parameters = config.rope_parameters\n \n     def forward(\n         self,\n@@ -272,7 +303,7 @@ def forward(\n \n         cos, sin = position_embeddings\n         query_states, key_states = apply_multimodal_rotary_pos_emb(  # diff with Llama\n-            query_states, key_states, cos, sin, self.rope_scaling[\"mrope_section\"]\n+            query_states, key_states, cos, sin, self.rope_parameters[\"mrope_section\"]\n         )\n \n         if past_key_values is not None:\n@@ -470,7 +501,7 @@ def forward(\n         past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> torch.Tensor:\n         residual = hidden_states"
        },
        {
            "sha": "62b5fee670df49b7abbdde27214172505448e591",
            "filename": "src/transformers/models/glm4v_moe/modular_glm4v_moe.py",
            "status": "modified",
            "additions": 76,
            "deletions": 60,
            "changes": 136,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodular_glm4v_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodular_glm4v_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodular_glm4v_moe.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -23,7 +23,7 @@\n from ...masking_utils import create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import MoeModelOutputWithPast\n-from ...modeling_rope_utils import rope_config_validation\n+from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, logging\n@@ -65,10 +65,6 @@ class Glm4vMoeRMSNorm(Glm4MoeRMSNorm):\n     pass\n \n \n-class Glm4vMoeTextRotaryEmbedding(Glm4vTextRotaryEmbedding):\n-    pass\n-\n-\n class Glm4vMoeTextConfig(Glm4MoeConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Glm4vMoeModel`]. It is used to instantiate a\n@@ -112,27 +108,10 @@ class Glm4vMoeTextConfig(Glm4MoeConfig):\n             relevant if `config.is_decoder=True`.\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether the model's input and output word embeddings should be tied.\n-        rope_theta (`float`, *optional*, defaults to 10000.0):\n-            The base period of the RoPE embeddings.\n-        rope_scaling (`Dict`, *optional*):\n-            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n-            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n-            accordingly.\n-            Expected contents:\n-                `rope_type` (`str`):\n-                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n-                    'llama3'], with 'default' being the original RoPE implementation.\n-                `factor` (`float`, *optional*):\n-                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n-                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n-                    original maximum pre-trained length.\n-                `original_max_position_embeddings` (`int`, *optional*):\n-                    Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n-                    pretraining.\n-                `attention_factor` (`float`, *optional*):\n-                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n-                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n-                    `factor` field to infer the suggested value.\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         attention_bias (`bool`, defaults to `True`, *optional*, defaults to `True`):\n             Whether to use a bias in the query, key, value and output projection layers during self-attention.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n@@ -191,33 +170,32 @@ class Glm4vMoeTextConfig(Glm4MoeConfig):\n \n     def __init__(\n         self,\n-        vocab_size=151424,\n-        hidden_size=4096,\n-        intermediate_size=10944,\n-        num_hidden_layers=46,\n-        num_attention_heads=96,\n-        partial_rotary_factor=0.5,\n-        num_key_value_heads=8,\n-        hidden_act=\"silu\",\n-        max_position_embeddings=65536,\n-        initializer_range=0.02,\n-        rms_norm_eps=1e-5,\n-        use_cache=True,\n-        tie_word_embeddings=False,\n-        rope_theta=10000.0,\n-        rope_scaling=None,\n-        attention_bias=True,\n-        attention_dropout=0.0,\n-        moe_intermediate_size=1408,\n-        num_experts_per_tok=8,\n-        n_shared_experts=1,\n-        n_routed_experts=128,\n-        routed_scaling_factor=1.0,\n-        n_group=1,\n-        topk_group=1,\n-        first_k_dense_replace=1,\n-        norm_topk_prob=True,\n-        router_aux_loss_coef=0.0001,\n+        vocab_size: Optional[int] = 151424,\n+        hidden_size: Optional[int] = 4096,\n+        intermediate_size: Optional[int] = 10944,\n+        num_hidden_layers: Optional[int] = 46,\n+        num_attention_heads: Optional[int] = 96,\n+        partial_rotary_factor: Optional[float] = 0.5,\n+        num_key_value_heads: Optional[int] = 8,\n+        hidden_act: Optional[str] = \"silu\",\n+        max_position_embeddings: Optional[int] = 65536,\n+        initializer_range: Optional[float] = 0.02,\n+        rms_norm_eps: Optional[int] = 1e-5,\n+        use_cache: Optional[bool] = True,\n+        tie_word_embeddings: Optional[bool] = False,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        attention_bias: Optional[bool] = True,\n+        attention_dropout: Optional[float] = 0.0,\n+        moe_intermediate_size: Optional[int] = 1408,\n+        num_experts_per_tok: Optional[int] = 8,\n+        n_shared_experts: Optional[int] = 1,\n+        n_routed_experts: Optional[int] = 128,\n+        routed_scaling_factor: Optional[float] = 1.0,\n+        n_group: Optional[int] = 1,\n+        topk_group: Optional[int] = 1,\n+        first_k_dense_replace: Optional[int] = 1,\n+        norm_topk_prob: Optional[bool] = True,\n+        router_aux_loss_coef: Optional[float] = 0.0001,\n         **kwargs,\n     ):\n         PreTrainedConfig.__init__(self, tie_word_embeddings=tie_word_embeddings, **kwargs)\n@@ -234,14 +212,15 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.rms_norm_eps = rms_norm_eps\n         self.use_cache = use_cache\n-        self.rope_theta = rope_theta\n-        self.rope_scaling = rope_scaling\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n+\n         # Validate the correctness of rotary position embeddings parameters\n-        # BC: if there is a 'type' field, move it to 'rope_type'.\n-        if self.rope_scaling is not None and \"type\" in self.rope_scaling:\n-            self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n+        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n         rope_config_validation(self, ignore_keys={\"mrope_section\"})\n \n         # MoE arguments\n@@ -371,10 +350,47 @@ def apply_multimodal_rotary_pos_emb(q, k, cos, sin, mrope_section, unsqueeze_dim\n     return q_embed, k_embed\n \n \n+class Glm4vMoeTextRotaryEmbedding(Glm4vTextRotaryEmbedding):\n+    def __init__(self, config: Glm4vMoeTextConfig, device=None, layer_type=None):\n+        super().__init__(config, device=device, layer_type=layer_type)\n+\n+    @staticmethod\n+    def compute_default_rope_parameters(\n+        config: Optional[Glm4vMoeTextConfig] = None,\n+        device: Optional[\"torch.device\"] = None,\n+        seq_len: Optional[int] = None,\n+    ) -> tuple[\"torch.Tensor\", float]:\n+        \"\"\"\n+        Computes the inverse frequencies according to the original RoPE implementation\n+        Args:\n+            config ([`~transformers.PreTrainedConfig`]):\n+                The model configuration.\n+            device (`torch.device`):\n+                The device to use for initialization of the inverse frequencies.\n+            seq_len (`int`, *optional*):\n+                The current sequence length. Unused for this type of RoPE.\n+        Returns:\n+            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n+            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n+        \"\"\"\n+        base = config.rope_parameters[\"rope_theta\"]\n+        partial_rotary_factor = getattr(config, \"partial_rotary_factor\", 1.0)\n+        head_dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+        dim = int(head_dim * partial_rotary_factor)\n+\n+        attention_factor = 1.0  # Unused in this type of RoPE\n+\n+        # Compute the inverse frequencies\n+        inv_freq = 1.0 / (\n+            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n+        )\n+        return inv_freq, attention_factor\n+\n+\n class Glm4vMoeTextAttention(Glm4Attention):\n     def __init__(self, config: Glm4vMoeTextConfig, layer_idx: Optional[int] = None):\n         super().__init__(config, layer_idx)\n-        self.rope_scaling = config.rope_scaling\n+        self.rope_parameters = config.rope_parameters\n \n     def forward(\n         self,\n@@ -398,7 +414,7 @@ def forward(\n \n         cos, sin = position_embeddings\n         query_states, key_states = apply_multimodal_rotary_pos_emb(  # diff with Llama\n-            query_states, key_states, cos, sin, self.rope_scaling[\"mrope_section\"]\n+            query_states, key_states, cos, sin, self.rope_parameters[\"mrope_section\"]\n         )\n \n         if past_key_values is not None:"
        },
        {
            "sha": "35d2f5baa0dd5a03c7e17745740ceb797a6f959d",
            "filename": "src/transformers/models/got_ocr2/configuration_got_ocr2.py",
            "status": "modified",
            "additions": 8,
            "deletions": 6,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fconfiguration_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fconfiguration_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fconfiguration_got_ocr2.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -20,6 +20,8 @@\n # limitations under the License.\n \n \n+from typing import Optional\n+\n from ...configuration_utils import PreTrainedConfig\n from ..auto import CONFIG_MAPPING, AutoConfig\n \n@@ -160,11 +162,11 @@ class GotOcr2Config(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vision_config=None,\n-        text_config=None,\n-        image_token_index=151859,\n-        image_seq_length=576,\n-        pad_token_id=-1,\n+        vision_config: Optional[dict] = None,\n+        text_config: Optional[dict] = None,\n+        image_token_index: Optional[int] = 151859,\n+        image_seq_length: Optional[int] = 576,\n+        pad_token_id: Optional[int] = -1,\n         **kwargs,\n     ):\n         self.image_token_index = image_token_index\n@@ -196,7 +198,7 @@ def __init__(\n                 use_cache=True,\n                 tie_word_embeddings=True,\n                 rope_theta=1000000.0,\n-                rope_scaling=None,\n+                rope_parameters=None,\n                 use_sliding_window=False,\n                 sliding_window=4096,\n                 max_window_layers=21,"
        },
        {
            "sha": "1b56eff7729d020d526e960d443113a18f39f307",
            "filename": "src/transformers/models/got_ocr2/modular_got_ocr2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -181,11 +181,11 @@ class GotOcr2Config(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vision_config=None,\n-        text_config=None,\n-        image_token_index=151859,\n-        image_seq_length=576,\n-        pad_token_id=-1,\n+        vision_config: Optional[dict] = None,\n+        text_config: Optional[dict] = None,\n+        image_token_index: Optional[int] = 151859,\n+        image_seq_length: Optional[int] = 576,\n+        pad_token_id: Optional[int] = -1,\n         **kwargs,\n     ):\n         self.image_token_index = image_token_index\n@@ -217,7 +217,7 @@ def __init__(\n                 use_cache=True,\n                 tie_word_embeddings=True,\n                 rope_theta=1000000.0,\n-                rope_scaling=None,\n+                rope_parameters=None,\n                 use_sliding_window=False,\n                 sliding_window=4096,\n                 max_window_layers=21,"
        },
        {
            "sha": "1a2dafdf2668c06573bcd9dcf22fe8c329942894",
            "filename": "src/transformers/models/gpt_neox/configuration_gpt_neox.py",
            "status": "modified",
            "additions": 36,
            "deletions": 68,
            "changes": 104,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fconfiguration_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fconfiguration_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fconfiguration_gpt_neox.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -14,8 +14,10 @@\n # limitations under the License.\n \"\"\"GPTNeoX model configuration\"\"\"\n \n+from typing import Optional\n+\n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import rope_config_validation\n+from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n from ...utils import logging\n \n \n@@ -50,8 +52,6 @@ class GPTNeoXConfig(PreTrainedConfig):\n             `\"relu\"`, `\"selu\"` and `\"gelu_new\"` are supported.\n         rotary_pct (`float`, *optional*, defaults to 0.25):\n             percentage of hidden dimensions to allocate to rotary embeddings\n-        rotary_emb_base (`int`, *optional*, defaults to 10000)\n-            base for computing rotary embeddings frequency\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n             The dropout ratio probability of the attention score.\n         hidden_dropout (`float`, *optional*, defaults to 0.0):\n@@ -74,43 +74,10 @@ class GPTNeoXConfig(PreTrainedConfig):\n         use_parallel_residual (`bool`, *optional*, defaults to `True`):\n             Whether to use a \"parallel\" formulation in each Transformer layer, which can provide a slight training\n             speedup at large scales (e.g. 20B).\n-        rope_scaling (`Dict`, *optional*):\n-            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n-            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n-            accordingly.\n-            Expected contents:\n-                `rope_type` (`str`):\n-                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n-                    'llama3'], with 'default' being the original RoPE implementation.\n-                `factor` (`float`, *optional*):\n-                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n-                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n-                    original maximum pre-trained length.\n-                `original_max_position_embeddings` (`int`, *optional*):\n-                    Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n-                    pretraining.\n-                `attention_factor` (`float`, *optional*):\n-                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n-                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n-                    `factor` field to infer the suggested value.\n-                `beta_fast` (`float`, *optional*):\n-                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n-                    ramp function. If unspecified, it defaults to 32.\n-                `beta_slow` (`float`, *optional*):\n-                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n-                    ramp function. If unspecified, it defaults to 1.\n-                `short_factor` (`list[float]`, *optional*):\n-                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n-                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n-                    size divided by the number of attention heads divided by 2\n-                `long_factor` (`list[float]`, *optional*):\n-                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n-                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n-                    size divided by the number of attention heads divided by 2\n-                `low_freq_factor` (`float`, *optional*):\n-                    Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n-                `high_freq_factor` (`float`, *optional*):\n-                    Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         attention_bias (`bool`, *optional*, defaults to `True`):\n             Whether to use a bias in the query, key, value and output projection layers during self-attention.\n \n@@ -146,27 +113,26 @@ class GPTNeoXConfig(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size=50432,\n-        hidden_size=6144,\n-        num_hidden_layers=44,\n-        num_attention_heads=64,\n-        intermediate_size=24576,\n-        hidden_act=\"gelu\",\n-        rotary_pct=0.25,\n-        rotary_emb_base=10000,\n-        attention_dropout=0.0,\n-        hidden_dropout=0.0,\n-        classifier_dropout=0.1,\n-        max_position_embeddings=2048,\n-        initializer_range=0.02,\n-        layer_norm_eps=1e-5,\n-        use_cache=True,\n-        bos_token_id=0,\n-        eos_token_id=2,\n-        tie_word_embeddings=False,\n-        use_parallel_residual=True,\n-        rope_scaling=None,\n-        attention_bias=True,\n+        vocab_size: Optional[int] = 50432,\n+        hidden_size: Optional[int] = 6144,\n+        num_hidden_layers: Optional[int] = 44,\n+        num_attention_heads: Optional[int] = 64,\n+        intermediate_size: Optional[int] = 24576,\n+        hidden_act: Optional[str] = \"gelu\",\n+        rotary_pct: Optional[float] = 0.25,\n+        attention_dropout: Optional[float] = 0.0,\n+        hidden_dropout: Optional[float] = 0.0,\n+        classifier_dropout: Optional[float] = 0.1,\n+        max_position_embeddings: Optional[int] = 2048,\n+        initializer_range: Optional[float] = 0.02,\n+        layer_norm_eps: Optional[int] = 1e-5,\n+        use_cache: Optional[bool] = True,\n+        bos_token_id: Optional[int] = 0,\n+        eos_token_id: Optional[int] = 2,\n+        tie_word_embeddings: Optional[bool] = False,\n+        use_parallel_residual: Optional[bool] = True,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        attention_bias: Optional[bool] = True,\n         **kwargs,\n     ):\n         super().__init__(bos_token_id=bos_token_id, eos_token_id=eos_token_id, **kwargs)\n@@ -179,8 +145,6 @@ def __init__(\n         self.hidden_act = hidden_act\n         self.rotary_pct = rotary_pct\n         self.partial_rotary_factor = rotary_pct\n-        self.rotary_emb_base = rotary_emb_base\n-        self.rope_theta = rotary_emb_base\n         self.attention_dropout = attention_dropout\n         self.hidden_dropout = hidden_dropout\n         self.classifier_dropout = classifier_dropout\n@@ -189,14 +153,18 @@ def __init__(\n         self.use_cache = use_cache\n         self.tie_word_embeddings = tie_word_embeddings\n         self.use_parallel_residual = use_parallel_residual\n-        self.rope_scaling = rope_scaling\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n         self.attention_bias = attention_bias\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n+\n         # Validate the correctness of rotary position embeddings parameters\n-        # BC: if there is a 'type' field, move it to 'rope_type'.\n-        if self.rope_scaling is not None and \"type\" in self.rope_scaling:\n-            self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n+        rope_theta = kwargs.get(\"rotary_emb_base\", 10000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n         rope_config_validation(self)\n-\n         if self.hidden_size % self.num_attention_heads != 0:\n             raise ValueError(\n                 \"The hidden size is not divisible by the number of attention heads! Make sure to update them!\""
        },
        {
            "sha": "719ec08ce3e67313f1d1aa71bcaecfd5f4a81f9b",
            "filename": "src/transformers/models/gpt_neox/modeling_gpt_neox.py",
            "status": "modified",
            "additions": 72,
            "deletions": 45,
            "changes": 117,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -49,6 +49,73 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n+class GPTNeoXRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n+    def __init__(self, config: GPTNeoXConfig, device=None):\n+        super().__init__()\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+\n+        self.rope_type = self.config.rope_parameters[\"rope_type\"]\n+        rope_init_fn: Callable = self.compute_default_rope_parameters\n+        if self.rope_type != \"default\":\n+            rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+        inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n+\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = inv_freq\n+\n+    @staticmethod\n+    def compute_default_rope_parameters(\n+        config: Optional[GPTNeoXConfig] = None,\n+        device: Optional[\"torch.device\"] = None,\n+        seq_len: Optional[int] = None,\n+    ) -> tuple[\"torch.Tensor\", float]:\n+        \"\"\"\n+        Computes the inverse frequencies according to the original RoPE implementation\n+        Args:\n+            config ([`~transformers.PreTrainedConfig`]):\n+                The model configuration.\n+            device (`torch.device`):\n+                The device to use for initialization of the inverse frequencies.\n+            seq_len (`int`, *optional*):\n+                The current sequence length. Unused for this type of RoPE.\n+        Returns:\n+            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n+            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n+        \"\"\"\n+        base = config.rope_parameters[\"rope_theta\"]\n+        partial_rotary_factor = getattr(config, \"partial_rotary_factor\", 1.0)\n+        head_dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+        dim = int(head_dim * partial_rotary_factor)\n+\n+        attention_factor = 1.0  # Unused in this type of RoPE\n+\n+        # Compute the inverse frequencies\n+        inv_freq = 1.0 / (\n+            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n+        )\n+        return inv_freq, attention_factor\n+\n+    @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n+    def forward(self, x, position_ids):\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n def rotate_half(x):\n     \"\"\"Rotates half the hidden dims of the input.\"\"\"\n     x1 = x[..., : x.shape[-1] // 2]\n@@ -140,9 +207,8 @@ def forward(\n         hidden_states: torch.FloatTensor,\n         attention_mask: torch.FloatTensor,\n         layer_past: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ):\n         input_shape = hidden_states.shape[:-1]\n@@ -207,7 +273,7 @@ def forward(\n         layer_past: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ):\n         attn_output, attn_weights = self.attention(\n@@ -216,7 +282,6 @@ def forward(\n             position_ids=position_ids,\n             layer_past=layer_past,\n             use_cache=use_cache,\n-            output_attentions=output_attentions,\n             cache_position=cache_position,\n             position_embeddings=position_embeddings,\n             **kwargs,\n@@ -245,42 +310,6 @@ def forward(\n         return outputs\n \n \n-class GPTNeoXRotaryEmbedding(nn.Module):\n-    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n-\n-    def __init__(self, config: GPTNeoXConfig, device=None):\n-        super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n-        self.max_seq_len_cached = config.max_position_embeddings\n-        self.original_max_seq_len = config.max_position_embeddings\n-\n-        self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n-\n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n-        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n-\n-    @torch.no_grad()\n-    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n-    def forward(self, x, position_ids):\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n-        position_ids_expanded = position_ids[:, None, :].float()\n-\n-        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n-            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos() * self.attention_scaling\n-            sin = emb.sin() * self.attention_scaling\n-\n-        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n-\n-\n @use_kernel_forward_from_hub(\"RMSNorm\")\n class GPTNeoXRMSNorm(nn.Module):\n     def __init__(self, hidden_size, eps=1e-6):\n@@ -321,7 +350,7 @@ def forward(\n         past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> torch.Tensor:\n         residual = hidden_states\n@@ -439,9 +468,7 @@ def forward(\n         )\n \n         hidden_states = self.emb_dropout(inputs_embeds)\n-\n-        # create position embeddings to be shared across the decoder layers\n-        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids=position_ids)\n \n         all_attentions = () if output_attentions else None\n         all_hidden_states = () if output_hidden_states else None\n@@ -455,9 +482,9 @@ def forward(\n                 position_ids=position_ids,\n                 layer_past=past_key_values,\n                 use_cache=use_cache,\n+                position_embeddings=position_embeddings,\n                 output_attentions=output_attentions,\n                 cache_position=cache_position,\n-                position_embeddings=position_embeddings,\n                 **kwargs,\n             )\n             hidden_states = outputs[0]"
        },
        {
            "sha": "dfd87782536368ac2bfa2663ee51c81d187c6abf",
            "filename": "src/transformers/models/gpt_neox/modular_gpt_neox.py",
            "status": "modified",
            "additions": 39,
            "deletions": 12,
            "changes": 51,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodular_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodular_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodular_gpt_neox.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -21,6 +21,7 @@\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n from ..llama.modeling_llama import LlamaModel, LlamaPreTrainedModel, LlamaRotaryEmbedding, rotate_half\n+from .configuration_gpt_neox import GPTNeoXConfig\n \n \n logger = logging.get_logger(__name__)\n@@ -40,6 +41,40 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n+class GPTNeoXRotaryEmbedding(LlamaRotaryEmbedding):\n+    @staticmethod\n+    def compute_default_rope_parameters(\n+        config: Optional[GPTNeoXConfig] = None,\n+        device: Optional[\"torch.device\"] = None,\n+        seq_len: Optional[int] = None,\n+    ) -> tuple[\"torch.Tensor\", float]:\n+        \"\"\"\n+        Computes the inverse frequencies according to the original RoPE implementation\n+        Args:\n+            config ([`~transformers.PreTrainedConfig`]):\n+                The model configuration.\n+            device (`torch.device`):\n+                The device to use for initialization of the inverse frequencies.\n+            seq_len (`int`, *optional*):\n+                The current sequence length. Unused for this type of RoPE.\n+        Returns:\n+            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n+            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n+        \"\"\"\n+        base = config.rope_parameters[\"rope_theta\"]\n+        partial_rotary_factor = getattr(config, \"partial_rotary_factor\", 1.0)\n+        head_dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+        dim = int(head_dim * partial_rotary_factor)\n+\n+        attention_factor = 1.0  # Unused in this type of RoPE\n+\n+        # Compute the inverse frequencies\n+        inv_freq = 1.0 / (\n+            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n+        )\n+        return inv_freq, attention_factor\n+\n+\n def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n \n@@ -124,9 +159,8 @@ def forward(\n         hidden_states: torch.FloatTensor,\n         attention_mask: torch.FloatTensor,\n         layer_past: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ):\n         input_shape = hidden_states.shape[:-1]\n@@ -191,7 +225,7 @@ def forward(\n         layer_past: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ):\n         attn_output, attn_weights = self.attention(\n@@ -200,7 +234,6 @@ def forward(\n             position_ids=position_ids,\n             layer_past=layer_past,\n             use_cache=use_cache,\n-            output_attentions=output_attentions,\n             cache_position=cache_position,\n             position_embeddings=position_embeddings,\n             **kwargs,\n@@ -229,10 +262,6 @@ def forward(\n         return outputs\n \n \n-class GPTNeoXRotaryEmbedding(LlamaRotaryEmbedding):\n-    pass\n-\n-\n class GPTNeoXPreTrainedModel(LlamaPreTrainedModel):\n     base_model_prefix = \"gpt_neox\"\n     _no_split_modules = [\"GPTNeoXLayer\"]\n@@ -318,9 +347,7 @@ def forward(\n         )\n \n         hidden_states = self.emb_dropout(inputs_embeds)\n-\n-        # create position embeddings to be shared across the decoder layers\n-        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids=position_ids)\n \n         all_attentions = () if output_attentions else None\n         all_hidden_states = () if output_hidden_states else None\n@@ -334,9 +361,9 @@ def forward(\n                 position_ids=position_ids,\n                 layer_past=past_key_values,\n                 use_cache=use_cache,\n+                position_embeddings=position_embeddings,\n                 output_attentions=output_attentions,\n                 cache_position=cache_position,\n-                position_embeddings=position_embeddings,\n                 **kwargs,\n             )\n             hidden_states = outputs[0]"
        },
        {
            "sha": "f09bc8810da0c3830ae16c766d57c7b9a2cce1b0",
            "filename": "src/transformers/models/gpt_neox_japanese/configuration_gpt_neox_japanese.py",
            "status": "modified",
            "additions": 32,
            "deletions": 63,
            "changes": 95,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fconfiguration_gpt_neox_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fconfiguration_gpt_neox_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fconfiguration_gpt_neox_japanese.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -14,8 +14,10 @@\n # limitations under the License.\n \"\"\"GPTNeoX Japanese model configuration\"\"\"\n \n+from typing import Optional\n+\n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import rope_config_validation\n+from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n from ...utils import logging\n \n \n@@ -49,8 +51,6 @@ class GPTNeoXJapaneseConfig(PreTrainedConfig):\n             The non-linear activation function (function or string) in the encoder and pooler.\n         rotary_pct (`float`, *optional*, defaults to 1.00):\n             percentage of hidden dimensions to allocate to rotary embeddings\n-        rotary_emb_base (`int`, *optional*, defaults to 10000)\n-            base for computing rotary embeddings frequency\n         max_position_embeddings (`int`, *optional*, defaults to 2048):\n             The maximum sequence length that this model might ever be used with.\n         initializer_range (`float`, *optional*, defaults to 0.02):\n@@ -60,43 +60,10 @@ class GPTNeoXJapaneseConfig(PreTrainedConfig):\n         use_cache (`bool`, *optional*, defaults to `True`):\n             Whether or not the model should return the last key/values attentions (not used by all models). Only\n             relevant if `config.is_decoder=True`.\n-        rope_scaling (`Dict`, *optional*):\n-            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n-            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n-            accordingly.\n-            Expected contents:\n-                `rope_type` (`str`):\n-                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n-                    'llama3'], with 'default' being the original RoPE implementation.\n-                `factor` (`float`, *optional*):\n-                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n-                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n-                    original maximum pre-trained length.\n-                `original_max_position_embeddings` (`int`, *optional*):\n-                    Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n-                    pretraining.\n-                `attention_factor` (`float`, *optional*):\n-                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n-                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n-                    `factor` field to infer the suggested value.\n-                `beta_fast` (`float`, *optional*):\n-                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n-                    ramp function. If unspecified, it defaults to 32.\n-                `beta_slow` (`float`, *optional*):\n-                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n-                    ramp function. If unspecified, it defaults to 1.\n-                `short_factor` (`list[float]`, *optional*):\n-                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n-                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n-                    size divided by the number of attention heads divided by 2\n-                `long_factor` (`list[float]`, *optional*):\n-                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n-                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n-                    size divided by the number of attention heads divided by 2\n-                `low_freq_factor` (`float`, *optional*):\n-                    Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n-                `high_freq_factor` (`float`, *optional*):\n-                    Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         attention_dropout (`float`, *optional*, defaults to 0.1):\n             The dropout ratio for the attention.\n         hidden_dropout (`float`, *optional*, defaults to 0.0):\n@@ -120,23 +87,22 @@ class GPTNeoXJapaneseConfig(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size=32000,\n-        hidden_size=2560,\n-        num_hidden_layers=32,\n-        num_attention_heads=32,\n-        intermediate_multiple_size=4,\n-        hidden_act=\"gelu\",\n-        rotary_pct=1.00,\n-        rotary_emb_base=10000,\n-        max_position_embeddings=2048,\n-        initializer_range=0.02,\n-        layer_norm_eps=1e-5,\n-        use_cache=True,\n-        bos_token_id=31996,\n-        eos_token_id=31999,\n-        rope_scaling=None,\n-        attention_dropout=0.1,\n-        hidden_dropout=0.0,\n+        vocab_size: Optional[int] = 32000,\n+        hidden_size: Optional[int] = 2560,\n+        num_hidden_layers: Optional[int] = 32,\n+        num_attention_heads: Optional[int] = 32,\n+        intermediate_multiple_size: Optional[int] = 4,\n+        hidden_act: Optional[str] = \"gelu\",\n+        rotary_pct: Optional[float] = 1.00,\n+        max_position_embeddings: Optional[int] = 2048,\n+        initializer_range: Optional[float] = 0.02,\n+        layer_norm_eps: Optional[int] = 1e-5,\n+        use_cache: Optional[bool] = True,\n+        bos_token_id: Optional[int] = 31996,\n+        eos_token_id: Optional[int] = 31999,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        attention_dropout: Optional[float] = 0.1,\n+        hidden_dropout: Optional[float] = 0.0,\n         **kwargs,\n     ):\n         super().__init__(bos_token_id=bos_token_id, eos_token_id=eos_token_id, **kwargs)\n@@ -149,18 +115,21 @@ def __init__(\n         self.hidden_act = hidden_act\n         self.rotary_pct = rotary_pct\n         self.partial_rotary_factor = rotary_pct\n-        self.rotary_emb_base = rotary_emb_base\n-        self.rope_theta = rotary_emb_base\n         self.initializer_range = initializer_range\n         self.layer_norm_eps = layer_norm_eps\n         self.use_cache = use_cache\n-        self.rope_scaling = rope_scaling\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n         self.attention_dropout = attention_dropout\n         self.hidden_dropout = hidden_dropout\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n+\n         # Validate the correctness of rotary position embeddings parameters\n-        # BC: if there is a 'type' field, move it to 'rope_type'.\n-        if self.rope_scaling is not None and \"type\" in self.rope_scaling:\n-            self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n+        rope_theta = kwargs.get(\"rotary_emb_base\", 10000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n         rope_config_validation(self)\n \n "
        },
        {
            "sha": "5120929f9b4b7af7dc4471b5cf874286d50d35ac",
            "filename": "src/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py",
            "status": "modified",
            "additions": 105,
            "deletions": 79,
            "changes": 184,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -15,6 +15,7 @@\n \"\"\"PyTorch GPTNeoX model.\"\"\"\n \n import math\n+from collections.abc import Callable\n from typing import Optional, Union\n \n import torch\n@@ -67,6 +68,107 @@ def _init_weights(self, module):\n                 module.dense_bias.data.zero_()\n \n \n+# Copied from transformers.models.llama.modeling_llama.LlamaRotaryEmbedding with Llama->GPTNeoXJapanese\n+class GPTNeoXJapaneseRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n+    def __init__(self, config: GPTNeoXJapaneseConfig, device=None):\n+        super().__init__()\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+\n+        self.rope_type = self.config.rope_parameters[\"rope_type\"]\n+        rope_init_fn: Callable = self.compute_default_rope_parameters\n+        if self.rope_type != \"default\":\n+            rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+        inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n+\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = inv_freq\n+\n+    @staticmethod\n+    def compute_default_rope_parameters(\n+        config: Optional[GPTNeoXJapaneseConfig] = None,\n+        device: Optional[\"torch.device\"] = None,\n+        seq_len: Optional[int] = None,\n+    ) -> tuple[\"torch.Tensor\", float]:\n+        \"\"\"\n+        Computes the inverse frequencies according to the original RoPE implementation\n+        Args:\n+            config ([`~transformers.PreTrainedConfig`]):\n+                The model configuration.\n+            device (`torch.device`):\n+                The device to use for initialization of the inverse frequencies.\n+            seq_len (`int`, *optional*):\n+                The current sequence length. Unused for this type of RoPE.\n+        Returns:\n+            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n+            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n+        \"\"\"\n+        base = config.rope_parameters[\"rope_theta\"]\n+        dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+\n+        attention_factor = 1.0  # Unused in this type of RoPE\n+\n+        # Compute the inverse frequencies\n+        inv_freq = 1.0 / (\n+            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n+        )\n+        return inv_freq, attention_factor\n+\n+    @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n+    def forward(self, x, position_ids):\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n+def rotate_half(x):\n+    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n+    x1 = x[..., : x.shape[-1] // 2]\n+    x2 = x[..., x.shape[-1] // 2 :]\n+    return torch.cat((-x2, x1), dim=-1)\n+\n+\n+# Copied from transformers.models.llama.modeling_llama.apply_rotary_pos_emb\n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n+    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n+\n+    Args:\n+        q (`torch.Tensor`): The query tensor.\n+        k (`torch.Tensor`): The key tensor.\n+        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n+        sin (`torch.Tensor`): The sine part of the rotary embedding.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n+        unsqueeze_dim (`int`, *optional*, defaults to 1):\n+            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n+            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n+            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n+            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n+            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n+            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n+    Returns:\n+        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n+    \"\"\"\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n+    q_embed = (q * cos) + (rotate_half(q) * sin)\n+    k_embed = (k * cos) + (rotate_half(k) * sin)\n+    return q_embed, k_embed\n+\n+\n class GPTNeoXJapaneseAttention(nn.Module):\n     def __init__(self, config, use_bias=False, layer_idx=None):\n         super().__init__()\n@@ -82,8 +184,6 @@ def __init__(self, config, use_bias=False, layer_idx=None):\n \n         self.layer_idx = layer_idx\n         self.rotary_ndims = int(self.head_size * config.rotary_pct)\n-        self.rope_theta = config.rotary_emb_base\n-        self.rotary_emb = GPTNeoXJapaneseRotaryEmbedding(config=config)\n         self.attention_dropout = nn.Dropout(config.attention_dropout)\n         self.norm_factor = math.sqrt(self.head_size)\n \n@@ -102,7 +202,7 @@ def forward(\n         use_cache: Optional[bool] = False,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n     ):\n         # Compute QKV\n         # Attention heads [batch, seq_len, hidden_size]\n@@ -212,78 +312,6 @@ def _attn(self, query, key, value, attention_mask=None):\n         return attn_output, attn_weights\n \n \n-# Copied from transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXRotaryEmbedding with GPTNeoX->GPTNeoXJapanese\n-class GPTNeoXJapaneseRotaryEmbedding(nn.Module):\n-    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n-\n-    def __init__(self, config: GPTNeoXJapaneseConfig, device=None):\n-        super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n-        self.max_seq_len_cached = config.max_position_embeddings\n-        self.original_max_seq_len = config.max_position_embeddings\n-\n-        self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n-\n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n-        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n-\n-    @torch.no_grad()\n-    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n-    def forward(self, x, position_ids):\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n-        position_ids_expanded = position_ids[:, None, :].float()\n-\n-        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n-            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos() * self.attention_scaling\n-            sin = emb.sin() * self.attention_scaling\n-\n-        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n-\n-\n-def rotate_half(x):\n-    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n-    x1 = x[..., : x.shape[-1] // 2]\n-    x2 = x[..., x.shape[-1] // 2 :]\n-    return torch.cat((-x2, x1), dim=-1)\n-\n-\n-# Copied from transformers.models.llama.modeling_llama.apply_rotary_pos_emb\n-def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n-    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n-\n-    Args:\n-        q (`torch.Tensor`): The query tensor.\n-        k (`torch.Tensor`): The key tensor.\n-        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n-        sin (`torch.Tensor`): The sine part of the rotary embedding.\n-        position_ids (`torch.Tensor`, *optional*):\n-            Deprecated and unused.\n-        unsqueeze_dim (`int`, *optional*, defaults to 1):\n-            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n-            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n-            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n-            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n-            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n-            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n-    Returns:\n-        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n-    \"\"\"\n-    cos = cos.unsqueeze(unsqueeze_dim)\n-    sin = sin.unsqueeze(unsqueeze_dim)\n-    q_embed = (q * cos) + (rotate_half(q) * sin)\n-    k_embed = (k * cos) + (rotate_half(k) * sin)\n-    return q_embed, k_embed\n-\n-\n def bias_dropout_add(x: Tensor, bias: Tensor, residual: Optional[Tensor], prob: float, training: bool) -> Tensor:\n     \"\"\"add bias to x, apply dropout and residual connection\n \n@@ -343,7 +371,7 @@ def forward(\n         layer_past: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n     ):\n         residual = hidden_states\n         ln_out = self.input_layernorm(hidden_states)\n@@ -457,9 +485,7 @@ def forward(\n         )\n \n         hidden_states = inputs_embeds\n-\n-        # create position embeddings to be shared across the decoder layers\n-        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids=position_ids)\n \n         all_attentions = () if output_attentions else None\n         all_hidden_states = () if output_hidden_states else None"
        },
        {
            "sha": "d7e714079e39e1e34f8c967170ba5c5347f23e09",
            "filename": "src/transformers/models/gpt_oss/configuration_gpt_oss.py",
            "status": "modified",
            "additions": 29,
            "deletions": 28,
            "changes": 57,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fconfiguration_gpt_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fconfiguration_gpt_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fconfiguration_gpt_oss.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -14,8 +14,10 @@\n # limitations under the License.\n \"\"\"openai model configuration\"\"\"\n \n+from typing import Optional\n+\n from ...configuration_utils import PreTrainedConfig, layer_type_validation\n-from ...modeling_rope_utils import rope_config_validation\n+from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n \n \n class GptOssConfig(PreTrainedConfig):\n@@ -47,35 +49,34 @@ class GptOssConfig(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        num_hidden_layers: int = 36,\n-        num_local_experts: int = 128,\n-        vocab_size: int = 201088,\n-        hidden_size: int = 2880,\n-        intermediate_size: int = 2880,\n-        head_dim: int = 64,\n-        num_attention_heads: int = 64,\n-        num_key_value_heads: int = 8,\n-        sliding_window: int = 128,\n-        rope_theta: float = 150000.0,\n-        tie_word_embeddings=False,\n-        hidden_act: str = \"silu\",\n-        initializer_range: float = 0.02,\n-        max_position_embeddings=131072,\n-        rms_norm_eps: float = 1e-5,\n-        rope_scaling={\n+        num_hidden_layers: Optional[int] = 36,\n+        num_local_experts: Optional[int] = 128,\n+        vocab_size: Optional[int] = 201088,\n+        hidden_size: Optional[int] = 2880,\n+        intermediate_size: Optional[int] = 2880,\n+        head_dim: Optional[int] = 64,\n+        num_attention_heads: Optional[int] = 64,\n+        num_key_value_heads: Optional[int] = 8,\n+        sliding_window: Optional[int] = 128,\n+        tie_word_embeddings: Optional[bool] = False,\n+        hidden_act: Optional[str] = \"silu\",\n+        initializer_range: Optional[float] = 0.02,\n+        max_position_embeddings: Optional[int] = 131072,\n+        rms_norm_eps: Optional[float] = 1e-5,\n+        rope_parameters: Optional[RopeParameters] = {\n             \"rope_type\": \"yarn\",\n             \"factor\": 32.0,\n             \"beta_fast\": 32.0,\n             \"beta_slow\": 1.0,\n             \"truncate\": False,\n             \"original_max_position_embeddings\": 4096,\n         },\n-        attention_dropout: float = 0.0,\n-        num_experts_per_tok=4,\n-        router_aux_loss_coef: float = 0.9,\n-        output_router_logits=False,\n-        use_cache=True,\n-        layer_types=None,\n+        attention_dropout: Optional[float] = 0.0,\n+        num_experts_per_tok: Optional[int] = 4,\n+        router_aux_loss_coef: Optional[float] = 0.9,\n+        output_router_logits: Optional[bool] = False,\n+        use_cache: Optional[bool] = True,\n+        layer_types: Optional[list[str]] = None,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size\n@@ -94,8 +95,6 @@ def __init__(\n         self.hidden_act = hidden_act\n         self.initializer_range = initializer_range\n         self.rms_norm_eps = rms_norm_eps\n-        self.rope_theta = rope_theta\n-        self.rope_scaling = rope_scaling\n         self.attention_dropout = attention_dropout\n         self.head_dim = head_dim if head_dim is not None else self.hidden_size // self.num_attention_heads\n         self.layer_types = layer_types\n@@ -110,11 +109,13 @@ def __init__(\n         self.router_aux_loss_coef = router_aux_loss_coef\n         self.output_router_logits = output_router_logits\n         self.use_cache = use_cache\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n \n         # Validate the correctness of rotary position embeddings parameters\n-        # BC: if there is a 'type' field, copy it it to 'rope_type'.\n-        if self.rope_scaling is not None and \"type\" in self.rope_scaling:\n-            self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n+        rope_theta = kwargs.get(\"rope_theta\", 150000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n         rope_config_validation(self)\n \n         super().__init__("
        },
        {
            "sha": "a5f3cce78a4096849e959e9a120e83ca53a96618",
            "filename": "src/transformers/models/gpt_oss/convert_gpt_oss_weights_to_hf.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fconvert_gpt_oss_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fconvert_gpt_oss_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fconvert_gpt_oss_weights_to_hf.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -157,18 +157,18 @@ def write_model(\n     original_config = json.loads((Path(input_base_path) / \"config.json\").read_text())\n \n     num_local_experts = original_config.pop(\"num_experts\")\n-    rope_scaling = {\n+    rope_parameters = {\n         \"beta_fast\": float(original_config.pop(\"rope_ntk_beta\")),\n         \"beta_slow\": float(original_config.pop(\"rope_ntk_alpha\")),\n-        \"factor\": float(original_config.pop(\"rope_scaling_factor\")),\n+        \"factor\": float(original_config.pop(\"rope_parameters_factor\")),\n         \"rope_type\": \"yarn\",\n         \"truncate\": False,\n         \"original_max_position_embeddings\": 4096,\n     }\n \n     config = GptOssConfig(\n         num_local_experts=num_local_experts,\n-        rope_scaling=rope_scaling,\n+        rope_parameters=rope_parameters,\n         eos_token_id=eos_token_id,\n         pad_token_id=pad_token_id,\n         **original_config,"
        },
        {
            "sha": "92688a0ab3415e813a54b289fb6d0c11f33a0f13",
            "filename": "src/transformers/models/gpt_oss/modeling_gpt_oss.py",
            "status": "modified",
            "additions": 43,
            "deletions": 11,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -176,20 +176,49 @@ class GptOssRotaryEmbedding(nn.Module):\n \n     def __init__(self, config: GptOssConfig, device=None):\n         super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n         self.max_seq_len_cached = config.max_position_embeddings\n         self.original_max_seq_len = config.max_position_embeddings\n \n         self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.rope_type = self.config.rope_parameters[\"rope_type\"]\n+        rope_init_fn: Callable = self.compute_default_rope_parameters\n+        if self.rope_type != \"default\":\n+            rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+        inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n+\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n+        self.original_inv_freq = inv_freq\n+\n+    @staticmethod\n+    def compute_default_rope_parameters(\n+        config: Optional[GptOssConfig] = None,\n+        device: Optional[\"torch.device\"] = None,\n+        seq_len: Optional[int] = None,\n+    ) -> tuple[\"torch.Tensor\", float]:\n+        \"\"\"\n+        Computes the inverse frequencies according to the original RoPE implementation\n+        Args:\n+            config ([`~transformers.PreTrainedConfig`]):\n+                The model configuration.\n+            device (`torch.device`):\n+                The device to use for initialization of the inverse frequencies.\n+            seq_len (`int`, *optional*):\n+                The current sequence length. Unused for this type of RoPE.\n+        Returns:\n+            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n+            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n+        \"\"\"\n+        base = config.rope_parameters[\"rope_theta\"]\n+        dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+\n+        attention_factor = 1.0  # Unused in this type of RoPE\n+\n+        # Compute the inverse frequencies\n+        inv_freq = 1.0 / (\n+            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n+        )\n+        return inv_freq, attention_factor\n \n     @torch.no_grad()\n     @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n@@ -275,6 +304,7 @@ class GptOssAttention(nn.Module):\n \n     def __init__(self, config: GptOssConfig, layer_idx: int):\n         super().__init__()\n+        self.layer_type = config.layer_types[layer_idx] if hasattr(config, \"layer_types\") else None\n         self.config = config\n         self.layer_idx = layer_idx\n         self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n@@ -294,7 +324,7 @@ def __init__(self, config: GptOssConfig, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n-        self.sliding_window = config.sliding_window if config.layer_types[layer_idx] == \"sliding_attention\" else None\n+        self.sliding_window = config.sliding_window if self.layer_type == \"sliding_attention\" else None\n         self.sinks = nn.Parameter(torch.empty(config.num_attention_heads))\n \n     def forward(\n@@ -304,6 +334,7 @@ def forward(\n         attention_mask: Optional[torch.Tensor],\n         past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n         input_shape = hidden_states.shape[:-1]\n@@ -333,6 +364,7 @@ def forward(\n             dropout=0.0 if not self.training else self.attention_dropout,\n             scaling=self.scaling,\n             sliding_window=self.sliding_window,\n+            position_ids=position_ids,\n             s_aux=self.sinks,  # diff with Llama\n             **kwargs,\n         )\n@@ -360,7 +392,7 @@ def forward(\n         past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> torch.Tensor:\n         residual = hidden_states\n@@ -505,11 +537,11 @@ def forward(\n             hidden_states = decoder_layer(\n                 hidden_states,\n                 attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n+                position_embeddings=position_embeddings,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n-                position_embeddings=position_embeddings,\n                 **kwargs,\n             )\n         hidden_states = self.norm(hidden_states)"
        },
        {
            "sha": "11e21245f41e6f8006ad255f1429467afa33c7fc",
            "filename": "src/transformers/models/gpt_oss/modular_gpt_oss.py",
            "status": "modified",
            "additions": 8,
            "deletions": 5,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodular_gpt_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodular_gpt_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodular_gpt_oss.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -38,7 +38,6 @@\n     LlamaDecoderLayer,\n     LlamaPreTrainedModel,\n     LlamaRMSNorm,\n-    LlamaRotaryEmbedding,\n     repeat_kv,\n )\n from ..mixtral.modeling_mixtral import (\n@@ -47,7 +46,7 @@\n     MixtralForTokenClassification,\n     MixtralModel,\n )\n-from ..qwen2.modeling_qwen2 import Qwen2Attention\n+from ..qwen2.modeling_qwen2 import Qwen2Attention, Qwen2RotaryEmbedding\n from .configuration_gpt_oss import GptOssConfig\n \n \n@@ -170,7 +169,9 @@ def forward(self, hidden_states):\n         return routed_out, router_scores\n \n \n-class GptOssRotaryEmbedding(LlamaRotaryEmbedding):\n+class GptOssRotaryEmbedding(Qwen2RotaryEmbedding):\n+    pass\n+\n     @torch.no_grad()\n     @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n     def forward(self, x, position_ids):\n@@ -262,6 +263,7 @@ def forward(\n         attention_mask: Optional[torch.Tensor],\n         past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n         input_shape = hidden_states.shape[:-1]\n@@ -291,6 +293,7 @@ def forward(\n             dropout=0.0 if not self.training else self.attention_dropout,\n             scaling=self.scaling,\n             sliding_window=self.sliding_window,\n+            position_ids=position_ids,\n             s_aux=self.sinks,  # diff with Llama\n             **kwargs,\n         )\n@@ -318,7 +321,7 @@ def forward(\n         past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> torch.Tensor:\n         residual = hidden_states\n@@ -435,11 +438,11 @@ def forward(\n             hidden_states = decoder_layer(\n                 hidden_states,\n                 attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n+                position_embeddings=position_embeddings,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n-                position_embeddings=position_embeddings,\n                 **kwargs,\n             )\n         hidden_states = self.norm(hidden_states)"
        },
        {
            "sha": "65c04c3a67e109a851c073b4beb1f983e4646346",
            "filename": "src/transformers/models/granite/configuration_granite.py",
            "status": "modified",
            "additions": 38,
            "deletions": 37,
            "changes": 75,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fgranite%2Fconfiguration_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fgranite%2Fconfiguration_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fconfiguration_granite.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -19,8 +19,10 @@\n # limitations under the License.\n \"\"\"Granite model configuration\"\"\"\n \n+from typing import Optional\n+\n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import rope_config_validation\n+from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n from ...utils import logging\n \n \n@@ -76,16 +78,10 @@ class GraniteConfig(PreTrainedConfig):\n             End of stream token id.\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie weight embeddings\n-        rope_theta (`float`, *optional*, defaults to 10000.0):\n-            The base period of the RoPE embeddings.\n-        rope_scaling (`Dict`, *optional*):\n-            Dictionary containing the scaling configuration for the RoPE embeddings. Currently supports two scaling\n-            strategies: linear and dynamic. Their scaling factor must be a float greater than 1. The expected format is\n-            `{\"type\": strategy name, \"factor\": scaling factor}`. When using this flag, don't update\n-            `max_position_embeddings` to the expected new maximum. See the following thread for more information on how\n-            these scaling strategies behave:\n-            https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/. This is an\n-            experimental feature, subject to breaking API changes in future versions.\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         attention_bias (`bool`, *optional*, defaults to `False`):\n             Whether to use a bias in the query, key, value and output projection layers during self-attention.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n@@ -130,30 +126,29 @@ class GraniteConfig(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size=32000,\n-        hidden_size=4096,\n-        intermediate_size=11008,\n-        num_hidden_layers=32,\n-        num_attention_heads=32,\n-        num_key_value_heads=None,\n-        hidden_act=\"silu\",\n-        max_position_embeddings=2048,\n-        initializer_range=0.02,\n-        rms_norm_eps=1e-6,\n-        use_cache=True,\n-        pad_token_id=None,\n-        bos_token_id=1,\n-        eos_token_id=2,\n-        tie_word_embeddings=False,\n-        rope_theta=10000.0,\n-        rope_scaling=None,\n-        attention_bias=False,\n-        attention_dropout=0.0,\n-        mlp_bias=False,\n-        embedding_multiplier=1.0,\n-        logits_scaling=1.0,\n-        residual_multiplier=1.0,\n-        attention_multiplier=1.0,\n+        vocab_size: Optional[int] = 32000,\n+        hidden_size: Optional[int] = 4096,\n+        intermediate_size: Optional[int] = 11008,\n+        num_hidden_layers: Optional[int] = 32,\n+        num_attention_heads: Optional[int] = 32,\n+        num_key_value_heads: Optional[int] = None,\n+        hidden_act: Optional[str] = \"silu\",\n+        max_position_embeddings: Optional[int] = 2048,\n+        initializer_range: Optional[float] = 0.02,\n+        rms_norm_eps: Optional[int] = 1e-6,\n+        use_cache: Optional[bool] = True,\n+        pad_token_id: Optional[int] = None,\n+        bos_token_id: Optional[int] = 1,\n+        eos_token_id: Optional[int] = 2,\n+        tie_word_embeddings: Optional[bool] = False,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        attention_bias: Optional[bool] = False,\n+        attention_dropout: Optional[float] = 0.0,\n+        mlp_bias: Optional[bool] = False,\n+        embedding_multiplier: Optional[float] = 1.0,\n+        logits_scaling: Optional[float] = 1.0,\n+        residual_multiplier: Optional[float] = 1.0,\n+        attention_multiplier: Optional[float] = 1.0,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size\n@@ -172,8 +167,6 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.rms_norm_eps = rms_norm_eps\n         self.use_cache = use_cache\n-        self.rope_theta = rope_theta\n-        self.rope_scaling = rope_scaling\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n         self.mlp_bias = mlp_bias\n@@ -182,6 +175,14 @@ def __init__(\n         self.logits_scaling = logits_scaling\n         self.residual_multiplier = residual_multiplier\n         self.attention_multiplier = attention_multiplier\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n+\n+        # Validate the correctness of rotary position embeddings parameters\n+        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n+        rope_config_validation(self)\n \n         super().__init__(\n             pad_token_id=pad_token_id,"
        },
        {
            "sha": "bf64a382700b38aa33e153cd32acf6464f0ac02d",
            "filename": "src/transformers/models/granite/modeling_granite.py",
            "status": "modified",
            "additions": 41,
            "deletions": 14,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -144,8 +144,8 @@ def __init__(self, config: GraniteConfig, layer_idx: Optional[int] = None):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n-        attention_mask: Optional[torch.Tensor],\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -242,7 +242,7 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         **kwargs,\n     ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n@@ -323,20 +323,49 @@ class GraniteRotaryEmbedding(nn.Module):\n \n     def __init__(self, config: GraniteConfig, device=None):\n         super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n         self.max_seq_len_cached = config.max_position_embeddings\n         self.original_max_seq_len = config.max_position_embeddings\n \n         self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.rope_type = self.config.rope_parameters[\"rope_type\"]\n+        rope_init_fn: Callable = self.compute_default_rope_parameters\n+        if self.rope_type != \"default\":\n+            rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+        inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n+\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n+        self.original_inv_freq = inv_freq\n+\n+    @staticmethod\n+    def compute_default_rope_parameters(\n+        config: Optional[GraniteConfig] = None,\n+        device: Optional[\"torch.device\"] = None,\n+        seq_len: Optional[int] = None,\n+    ) -> tuple[\"torch.Tensor\", float]:\n+        \"\"\"\n+        Computes the inverse frequencies according to the original RoPE implementation\n+        Args:\n+            config ([`~transformers.PreTrainedConfig`]):\n+                The model configuration.\n+            device (`torch.device`):\n+                The device to use for initialization of the inverse frequencies.\n+            seq_len (`int`, *optional*):\n+                The current sequence length. Unused for this type of RoPE.\n+        Returns:\n+            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n+            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n+        \"\"\"\n+        base = config.rope_parameters[\"rope_theta\"]\n+        dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+\n+        attention_factor = 1.0  # Unused in this type of RoPE\n+\n+        # Compute the inverse frequencies\n+        inv_freq = 1.0 / (\n+            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n+        )\n+        return inv_freq, attention_factor\n \n     @torch.no_grad()\n     @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n@@ -430,9 +459,7 @@ def forward(\n         )\n \n         hidden_states = inputs_embeds\n-\n-        # create position embeddings to be shared across the decoder layers\n-        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids=position_ids)\n \n         # decoder layers\n         all_hidden_states = () if output_hidden_states else None"
        },
        {
            "sha": "1be9e360067978df5a472567922dc573473ec1ed",
            "filename": "src/transformers/models/granite/modular_granite.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodular_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodular_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodular_granite.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -59,7 +59,7 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         **kwargs,\n     ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n@@ -183,9 +183,7 @@ def forward(\n         )\n \n         hidden_states = inputs_embeds\n-\n-        # create position embeddings to be shared across the decoder layers\n-        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids=position_ids)\n \n         # decoder layers\n         all_hidden_states = () if output_hidden_states else None"
        },
        {
            "sha": "f1263f0806304c83c5506292482c613f6d428b71",
            "filename": "src/transformers/models/granitemoe/configuration_granitemoe.py",
            "status": "modified",
            "additions": 41,
            "deletions": 40,
            "changes": 81,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fconfiguration_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fconfiguration_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fconfiguration_granitemoe.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -19,8 +19,10 @@\n # limitations under the License.\n \"\"\"GraniteMoe model configuration\"\"\"\n \n+from typing import Optional\n+\n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import rope_config_validation\n+from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n from ...utils import logging\n \n \n@@ -76,16 +78,10 @@ class GraniteMoeConfig(PreTrainedConfig):\n             End of stream token id.\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie weight embeddings\n-        rope_theta (`float`, *optional*, defaults to 10000.0):\n-            The base period of the RoPE embeddings.\n-        rope_scaling (`Dict`, *optional*):\n-            Dictionary containing the scaling configuration for the RoPE embeddings. Currently supports two scaling\n-            strategies: linear and dynamic. Their scaling factor must be a float greater than 1. The expected format is\n-            `{\"type\": strategy name, \"factor\": scaling factor}`. When using this flag, don't update\n-            `max_position_embeddings` to the expected new maximum. See the following thread for more information on how\n-            these scaling strategies behave:\n-            https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/. This is an\n-            experimental feature, subject to breaking API changes in future versions.\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         attention_bias (`bool`, *optional*, defaults to `False`):\n             Whether to use a bias in the query, key, value and output projection layers during self-attention.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n@@ -119,33 +115,32 @@ class GraniteMoeConfig(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size=32000,\n-        hidden_size=4096,\n-        intermediate_size=11008,\n-        num_hidden_layers=32,\n-        num_attention_heads=32,\n-        num_key_value_heads=None,\n-        hidden_act=\"silu\",\n-        max_position_embeddings=2048,\n-        initializer_range=0.02,\n-        rms_norm_eps=1e-6,\n-        use_cache=True,\n-        pad_token_id=None,\n-        bos_token_id=1,\n-        eos_token_id=2,\n-        tie_word_embeddings=False,\n-        rope_theta=10000.0,\n-        rope_scaling=None,\n-        attention_bias=False,\n-        attention_dropout=0.0,\n-        embedding_multiplier=1.0,\n-        logits_scaling=1.0,\n-        residual_multiplier=1.0,\n-        attention_multiplier=1.0,\n-        num_local_experts=8,\n-        num_experts_per_tok=2,\n-        output_router_logits=False,\n-        router_aux_loss_coef=0.001,\n+        vocab_size: Optional[int] = 32000,\n+        hidden_size: Optional[int] = 4096,\n+        intermediate_size: Optional[int] = 11008,\n+        num_hidden_layers: Optional[int] = 32,\n+        num_attention_heads: Optional[int] = 32,\n+        num_key_value_heads: Optional[int] = None,\n+        hidden_act: Optional[str] = \"silu\",\n+        max_position_embeddings: Optional[int] = 2048,\n+        initializer_range: Optional[float] = 0.02,\n+        rms_norm_eps: Optional[int] = 1e-6,\n+        use_cache: Optional[bool] = True,\n+        pad_token_id: Optional[int] = None,\n+        bos_token_id: Optional[int] = 1,\n+        eos_token_id: Optional[int] = 2,\n+        tie_word_embeddings: Optional[bool] = False,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        attention_bias: Optional[bool] = False,\n+        attention_dropout: Optional[float] = 0.0,\n+        embedding_multiplier: Optional[float] = 1.0,\n+        logits_scaling: Optional[float] = 1.0,\n+        residual_multiplier: Optional[float] = 1.0,\n+        attention_multiplier: Optional[float] = 1.0,\n+        num_local_experts: Optional[int] = 8,\n+        num_experts_per_tok: Optional[int] = 2,\n+        output_router_logits: Optional[bool] = False,\n+        router_aux_loss_coef: Optional[float] = 0.001,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size\n@@ -164,8 +159,14 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.rms_norm_eps = rms_norm_eps\n         self.use_cache = use_cache\n-        self.rope_theta = rope_theta\n-        self.rope_scaling = rope_scaling\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n+\n+        # Validate the correctness of rotary position embeddings parameters\n+        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n+        rope_config_validation(self)\n \n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout"
        },
        {
            "sha": "0eefadc9a1b9a600964f98e803f409af95601547",
            "filename": "src/transformers/models/granitemoe/modeling_granitemoe.py",
            "status": "modified",
            "additions": 40,
            "deletions": 11,
            "changes": 51,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -67,20 +67,49 @@ class GraniteMoeRotaryEmbedding(nn.Module):\n \n     def __init__(self, config: GraniteMoeConfig, device=None):\n         super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n         self.max_seq_len_cached = config.max_position_embeddings\n         self.original_max_seq_len = config.max_position_embeddings\n \n         self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.rope_type = self.config.rope_parameters[\"rope_type\"]\n+        rope_init_fn: Callable = self.compute_default_rope_parameters\n+        if self.rope_type != \"default\":\n+            rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+        inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n+\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n+        self.original_inv_freq = inv_freq\n+\n+    @staticmethod\n+    def compute_default_rope_parameters(\n+        config: Optional[GraniteMoeConfig] = None,\n+        device: Optional[\"torch.device\"] = None,\n+        seq_len: Optional[int] = None,\n+    ) -> tuple[\"torch.Tensor\", float]:\n+        \"\"\"\n+        Computes the inverse frequencies according to the original RoPE implementation\n+        Args:\n+            config ([`~transformers.PreTrainedConfig`]):\n+                The model configuration.\n+            device (`torch.device`):\n+                The device to use for initialization of the inverse frequencies.\n+            seq_len (`int`, *optional*):\n+                The current sequence length. Unused for this type of RoPE.\n+        Returns:\n+            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n+            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n+        \"\"\"\n+        base = config.rope_parameters[\"rope_theta\"]\n+        dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+\n+        attention_factor = 1.0  # Unused in this type of RoPE\n+\n+        # Compute the inverse frequencies\n+        inv_freq = 1.0 / (\n+            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n+        )\n+        return inv_freq, attention_factor\n \n     @torch.no_grad()\n     @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n@@ -336,8 +365,8 @@ def __init__(self, config: GraniteMoeConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n-        attention_mask: Optional[torch.Tensor],\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -394,7 +423,7 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         **kwargs,\n     ) -> torch.Tensor:\n         residual = hidden_states"
        },
        {
            "sha": "3c5b73ebf899cb05b8e26c26cbaf781d1e749fbf",
            "filename": "src/transformers/models/granitemoe/modular_granitemoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodular_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodular_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodular_granitemoe.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -114,7 +114,7 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         **kwargs,\n     ) -> torch.Tensor:\n         residual = hidden_states"
        },
        {
            "sha": "55e1546fa435caba012ee43eb25ead5c4e9a78fe",
            "filename": "src/transformers/models/granitemoehybrid/configuration_granitemoehybrid.py",
            "status": "modified",
            "additions": 52,
            "deletions": 50,
            "changes": 102,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fconfiguration_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fconfiguration_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fconfiguration_granitemoehybrid.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -15,7 +15,10 @@\n # limitations under the License.\n \"\"\"GraniteMoeHybrid model configuration\"\"\"\n \n+from typing import Optional\n+\n from ...configuration_utils import PreTrainedConfig\n+from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n from ...utils import logging\n \n \n@@ -70,16 +73,10 @@ class GraniteMoeHybridConfig(PreTrainedConfig):\n             End of stream token id.\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie weight embeddings\n-        rope_theta (`float`, *optional*, defaults to 10000.0):\n-            The base period of the RoPE embeddings.\n-        rope_scaling (`Dict`, *optional*):\n-            Dictionary containing the scaling configuration for the RoPE embeddings. Currently supports two scaling\n-            strategies: linear and dynamic. Their scaling factor must be a float greater than 1. The expected format is\n-            `{\"type\": strategy name, \"factor\": scaling factor}`. When using this flag, don't update\n-            `max_position_embeddings` to the expected new maximum. See the following thread for more information on how\n-            these scaling strategies behave:\n-            https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/. This is an\n-            experimental feature, subject to breaking API changes in future versions.\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         attention_bias (`bool`, *optional*, defaults to `False`):\n             Whether to use a bias in the query, key, value and output projection layers during self-attention.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n@@ -135,44 +132,43 @@ class GraniteMoeHybridConfig(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size=32000,\n-        hidden_size=4096,\n-        intermediate_size=11008,\n-        num_hidden_layers=32,\n-        num_attention_heads=32,\n-        num_key_value_heads=None,\n-        hidden_act=\"silu\",\n-        max_position_embeddings=2048,\n-        initializer_range=0.02,\n-        rms_norm_eps=1e-6,\n-        use_cache=True,\n-        pad_token_id=None,\n-        bos_token_id=1,\n-        eos_token_id=2,\n-        tie_word_embeddings=False,\n-        rope_theta=10000.0,\n-        rope_scaling=None,\n-        attention_bias=False,\n-        attention_dropout=0.0,\n-        embedding_multiplier=1.0,\n-        logits_scaling=1.0,\n-        residual_multiplier=1.0,\n-        attention_multiplier=1.0,\n-        num_local_experts=8,\n-        num_experts_per_tok=2,\n-        output_router_logits=False,\n-        router_aux_loss_coef=0.001,\n-        shared_intermediate_size=1024,\n-        layer_types=None,\n-        mamba_n_heads=128,\n-        mamba_n_groups=1,\n-        mamba_d_state=256,\n-        mamba_d_head=\"auto\",\n-        mamba_d_conv=4,\n-        mamba_expand=2,\n-        mamba_chunk_size=256,\n-        mamba_conv_bias=True,\n-        mamba_proj_bias=False,\n+        vocab_size: Optional[int] = 32000,\n+        hidden_size: Optional[int] = 4096,\n+        intermediate_size: Optional[int] = 11008,\n+        num_hidden_layers: Optional[int] = 32,\n+        num_attention_heads: Optional[int] = 32,\n+        num_key_value_heads: Optional[int] = None,\n+        hidden_act: Optional[str] = \"silu\",\n+        max_position_embeddings: Optional[int] = 2048,\n+        initializer_range: Optional[float] = 0.02,\n+        rms_norm_eps: Optional[int] = 1e-6,\n+        use_cache: Optional[bool] = True,\n+        pad_token_id: Optional[int] = None,\n+        bos_token_id: Optional[int] = 1,\n+        eos_token_id: Optional[int] = 2,\n+        tie_word_embeddings: Optional[bool] = False,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        attention_bias: Optional[bool] = False,\n+        attention_dropout: Optional[float] = 0.0,\n+        embedding_multiplier: Optional[float] = 1.0,\n+        logits_scaling: Optional[float] = 1.0,\n+        residual_multiplier: Optional[float] = 1.0,\n+        attention_multiplier: Optional[float] = 1.0,\n+        num_local_experts: Optional[int] = 8,\n+        num_experts_per_tok: Optional[int] = 2,\n+        output_router_logits: Optional[bool] = False,\n+        router_aux_loss_coef: Optional[float] = 0.001,\n+        shared_intermediate_size: Optional[int] = 1024,\n+        layer_types: Optional[list[str]] = None,\n+        mamba_n_heads: Optional[int] = 128,\n+        mamba_n_groups: Optional[int] = 1,\n+        mamba_d_state: Optional[int] = 256,\n+        mamba_d_head: Optional[str] = \"auto\",\n+        mamba_d_conv: Optional[int] = 4,\n+        mamba_expand: Optional[int] = 2,\n+        mamba_chunk_size: Optional[int] = 256,\n+        mamba_conv_bias: Optional[bool] = True,\n+        mamba_proj_bias: Optional[bool] = False,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size\n@@ -191,8 +187,6 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.rms_norm_eps = rms_norm_eps\n         self.use_cache = use_cache\n-        self.rope_theta = rope_theta\n-        self.rope_scaling = rope_scaling\n         self.attention_bias = attention_bias\n         self.embedding_multiplier = embedding_multiplier\n         self.logits_scaling = logits_scaling\n@@ -204,6 +198,14 @@ def __init__(\n         self.output_router_logits = output_router_logits\n         self.router_aux_loss_coef = router_aux_loss_coef\n         self.shared_intermediate_size = shared_intermediate_size\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n+\n+        # Validate the correctness of rotary position embeddings parameters\n+        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n+        rope_config_validation(self)\n \n         mamba_intermediate = mamba_expand * hidden_size\n "
        },
        {
            "sha": "0b296f26ac2224e0f681c5b4774d36cb74525ade",
            "filename": "src/transformers/models/granitemoehybrid/modeling_granitemoehybrid.py",
            "status": "modified",
            "additions": 74,
            "deletions": 36,
            "changes": 110,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -860,6 +860,71 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return hidden_states\n \n \n+class GraniteMoeHybridRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n+    def __init__(self, config: GraniteMoeHybridConfig, device=None):\n+        super().__init__()\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+\n+        self.rope_type = self.config.rope_parameters[\"rope_type\"]\n+        rope_init_fn: Callable = self.compute_default_rope_parameters\n+        if self.rope_type != \"default\":\n+            rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+        inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n+\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = inv_freq\n+\n+    @staticmethod\n+    def compute_default_rope_parameters(\n+        config: Optional[GraniteMoeHybridConfig] = None,\n+        device: Optional[\"torch.device\"] = None,\n+        seq_len: Optional[int] = None,\n+    ) -> tuple[\"torch.Tensor\", float]:\n+        \"\"\"\n+        Computes the inverse frequencies according to the original RoPE implementation\n+        Args:\n+            config ([`~transformers.PreTrainedConfig`]):\n+                The model configuration.\n+            device (`torch.device`):\n+                The device to use for initialization of the inverse frequencies.\n+            seq_len (`int`, *optional*):\n+                The current sequence length. Unused for this type of RoPE.\n+        Returns:\n+            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n+            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n+        \"\"\"\n+        base = config.rope_parameters[\"rope_theta\"]\n+        dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+\n+        attention_factor = 1.0  # Unused in this type of RoPE\n+\n+        # Compute the inverse frequencies\n+        inv_freq = 1.0 / (\n+            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n+        )\n+        return inv_freq, attention_factor\n+\n+    @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n+    def forward(self, x, position_ids):\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n class GraniteFlashAttentionKwargs(TypedDict, total=False):\n     \"\"\"\n     Keyword arguments for advanced Flash Attention, causal-conv1d, and mamba_ssm kernel usage.\n@@ -1078,6 +1143,7 @@ def forward(\n         past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         **kwargs: Unpack[GraniteFlashAttentionKwargs],\n     ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         residual = hidden_states\n@@ -1098,6 +1164,7 @@ def forward(\n                 past_key_values=past_key_values,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n                 **kwargs,\n             )\n \n@@ -1146,42 +1213,6 @@ def _init_weights(self, module):\n             module.weight.data.fill_(1.0)\n \n \n-class GraniteMoeHybridRotaryEmbedding(nn.Module):\n-    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n-\n-    def __init__(self, config: GraniteMoeHybridConfig, device=None):\n-        super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n-        self.max_seq_len_cached = config.max_position_embeddings\n-        self.original_max_seq_len = config.max_position_embeddings\n-\n-        self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n-\n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n-        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n-\n-    @torch.no_grad()\n-    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n-    def forward(self, x, position_ids):\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n-        position_ids_expanded = position_ids[:, None, :].float()\n-\n-        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n-            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos() * self.attention_scaling\n-            sin = emb.sin() * self.attention_scaling\n-\n-        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n-\n-\n @auto_docstring\n class GraniteMoeHybridModel(GraniteMoeHybridPreTrainedModel):\n     def __init__(self, config: GraniteMoeHybridConfig):\n@@ -1207,6 +1238,7 @@ def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -1227,6 +1259,9 @@ def forward(\n                 past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n             )\n \n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n         causal_mask = create_causal_mask(\n             self.config,\n             inputs_embeds,\n@@ -1238,6 +1273,8 @@ def forward(\n \n         # embed positions\n         hidden_states = inputs_embeds\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n         for decoder_layer in self.layers:\n             # Depending on the layer type we opt for 2D base attention mask (Mamba) or 4D causal mask (Attention)\n             layer_mask = mamba_mask if decoder_layer.layer_type == \"mamba\" else causal_mask\n@@ -1248,6 +1285,7 @@ def forward(\n                 past_key_values=past_key_values,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n                 **kwargs,\n             )\n         hidden_states = self.norm(hidden_states)"
        },
        {
            "sha": "f1b8a5bfb11097f23a1e5d7354f272f86fe2387f",
            "filename": "src/transformers/models/granitemoehybrid/modular_granitemoehybrid.py",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodular_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodular_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodular_granitemoehybrid.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -28,6 +28,7 @@\n from ...utils.generic import check_model_inputs\n from ..bamba.configuration_bamba import BambaConfig\n from ..bamba.modeling_bamba import BambaMixer, BambaRMSNormGated, HybridMambaAttentionDynamicCache\n+from ..gemma2.modeling_gemma2 import Gemma2RotaryEmbedding\n from ..granitemoeshared.modeling_granitemoeshared import (\n     GraniteFlashAttentionKwargs,\n     GraniteMoeSharedAttention,\n@@ -102,6 +103,10 @@ def __init__(self, config: GraniteMoeHybridConfig):\n         super().__init__(config)\n \n \n+class GraniteMoeHybridRotaryEmbedding(Gemma2RotaryEmbedding):\n+    pass\n+\n+\n class GraniteMoeHybridDecoderLayer(GraniteMoeSharedDecoderLayer):\n     def __init__(self, config: GraniteMoeHybridConfig, layer_idx: int):\n         super().__init__(config, layer_idx)\n@@ -127,6 +132,7 @@ def forward(\n         past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         **kwargs: Unpack[GraniteFlashAttentionKwargs],\n     ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         residual = hidden_states\n@@ -147,6 +153,7 @@ def forward(\n                 past_key_values=past_key_values,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n                 **kwargs,\n             )\n \n@@ -193,6 +200,7 @@ def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -213,6 +221,9 @@ def forward(\n                 past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n             )\n \n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n         causal_mask = create_causal_mask(\n             self.config,\n             inputs_embeds,\n@@ -224,6 +235,8 @@ def forward(\n \n         # embed positions\n         hidden_states = inputs_embeds\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n         for decoder_layer in self.layers:\n             # Depending on the layer type we opt for 2D base attention mask (Mamba) or 4D causal mask (Attention)\n             layer_mask = mamba_mask if decoder_layer.layer_type == \"mamba\" else causal_mask\n@@ -234,6 +247,7 @@ def forward(\n                 past_key_values=past_key_values,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n                 **kwargs,\n             )\n         hidden_states = self.norm(hidden_states)"
        },
        {
            "sha": "00f87604bf519d468c41fdbdeab86e5bb00e0890",
            "filename": "src/transformers/models/granitemoeshared/configuration_granitemoeshared.py",
            "status": "modified",
            "additions": 42,
            "deletions": 41,
            "changes": 83,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fconfiguration_granitemoeshared.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fconfiguration_granitemoeshared.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fconfiguration_granitemoeshared.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -19,8 +19,10 @@\n # limitations under the License.\n \"\"\"GraniteMoeShared model configuration\"\"\"\n \n+from typing import Optional\n+\n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import rope_config_validation\n+from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n from ...utils import logging\n \n \n@@ -76,16 +78,10 @@ class GraniteMoeSharedConfig(PreTrainedConfig):\n             End of stream token id.\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie weight embeddings\n-        rope_theta (`float`, *optional*, defaults to 10000.0):\n-            The base period of the RoPE embeddings.\n-        rope_scaling (`Dict`, *optional*):\n-            Dictionary containing the scaling configuration for the RoPE embeddings. Currently supports two scaling\n-            strategies: linear and dynamic. Their scaling factor must be a float greater than 1. The expected format is\n-            `{\"type\": strategy name, \"factor\": scaling factor}`. When using this flag, don't update\n-            `max_position_embeddings` to the expected new maximum. See the following thread for more information on how\n-            these scaling strategies behave:\n-            https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/. This is an\n-            experimental feature, subject to breaking API changes in future versions.\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         attention_bias (`bool`, *optional*, defaults to `False`):\n             Whether to use a bias in the query, key, value and output projection layers during self-attention.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n@@ -121,34 +117,33 @@ class GraniteMoeSharedConfig(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size=32000,\n-        hidden_size=4096,\n-        intermediate_size=11008,\n-        num_hidden_layers=32,\n-        num_attention_heads=32,\n-        num_key_value_heads=None,\n-        hidden_act=\"silu\",\n-        max_position_embeddings=2048,\n-        initializer_range=0.02,\n-        rms_norm_eps=1e-6,\n-        use_cache=True,\n-        pad_token_id=None,\n-        bos_token_id=1,\n-        eos_token_id=2,\n-        tie_word_embeddings=False,\n-        rope_theta=10000.0,\n-        rope_scaling=None,\n-        attention_bias=False,\n-        attention_dropout=0.0,\n-        embedding_multiplier=1.0,\n-        logits_scaling=1.0,\n-        residual_multiplier=1.0,\n-        attention_multiplier=1.0,\n-        num_local_experts=8,\n-        num_experts_per_tok=2,\n-        output_router_logits=False,\n-        router_aux_loss_coef=0.001,\n-        shared_intermediate_size=0,\n+        vocab_size: Optional[int] = 32000,\n+        hidden_size: Optional[int] = 4096,\n+        intermediate_size: Optional[int] = 11008,\n+        num_hidden_layers: Optional[int] = 32,\n+        num_attention_heads: Optional[int] = 32,\n+        num_key_value_heads: Optional[int] = None,\n+        hidden_act: Optional[str] = \"silu\",\n+        max_position_embeddings: Optional[int] = 2048,\n+        initializer_range: Optional[float] = 0.02,\n+        rms_norm_eps: Optional[int] = 1e-6,\n+        use_cache: Optional[bool] = True,\n+        pad_token_id: Optional[int] = None,\n+        bos_token_id: Optional[int] = 1,\n+        eos_token_id: Optional[int] = 2,\n+        tie_word_embeddings: Optional[bool] = False,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        attention_bias: Optional[bool] = False,\n+        attention_dropout: Optional[float] = 0.0,\n+        embedding_multiplier: Optional[float] = 1.0,\n+        logits_scaling: Optional[float] = 1.0,\n+        residual_multiplier: Optional[float] = 1.0,\n+        attention_multiplier: Optional[float] = 1.0,\n+        num_local_experts: Optional[int] = 8,\n+        num_experts_per_tok: Optional[int] = 2,\n+        output_router_logits: Optional[bool] = False,\n+        router_aux_loss_coef: Optional[float] = 0.001,\n+        shared_intermediate_size: Optional[int] = 0,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size\n@@ -167,10 +162,16 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.rms_norm_eps = rms_norm_eps\n         self.use_cache = use_cache\n-        self.rope_theta = rope_theta\n-        self.rope_scaling = rope_scaling\n         # this model has rope embedding type, hardcoded for BC\n         self.position_embedding_type = \"rope\"\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n+\n+        # Validate the correctness of rotary position embeddings parameters\n+        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n+        rope_config_validation(self)\n \n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout"
        },
        {
            "sha": "8b15697220063c2718dba2db6797da97d0ea4cf4",
            "filename": "src/transformers/models/granitemoeshared/modeling_granitemoeshared.py",
            "status": "modified",
            "additions": 39,
            "deletions": 10,
            "changes": 49,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -355,8 +355,8 @@ def __init__(self, config: GraniteMoeSharedConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n-        attention_mask: Optional[torch.Tensor],\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -479,20 +479,49 @@ class GraniteMoeSharedRotaryEmbedding(nn.Module):\n \n     def __init__(self, config: GraniteMoeSharedConfig, device=None):\n         super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n         self.max_seq_len_cached = config.max_position_embeddings\n         self.original_max_seq_len = config.max_position_embeddings\n \n         self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.rope_type = self.config.rope_parameters[\"rope_type\"]\n+        rope_init_fn: Callable = self.compute_default_rope_parameters\n+        if self.rope_type != \"default\":\n+            rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+        inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n+\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n+        self.original_inv_freq = inv_freq\n+\n+    @staticmethod\n+    def compute_default_rope_parameters(\n+        config: Optional[GraniteMoeSharedConfig] = None,\n+        device: Optional[\"torch.device\"] = None,\n+        seq_len: Optional[int] = None,\n+    ) -> tuple[\"torch.Tensor\", float]:\n+        \"\"\"\n+        Computes the inverse frequencies according to the original RoPE implementation\n+        Args:\n+            config ([`~transformers.PreTrainedConfig`]):\n+                The model configuration.\n+            device (`torch.device`):\n+                The device to use for initialization of the inverse frequencies.\n+            seq_len (`int`, *optional*):\n+                The current sequence length. Unused for this type of RoPE.\n+        Returns:\n+            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n+            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n+        \"\"\"\n+        base = config.rope_parameters[\"rope_theta\"]\n+        dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+\n+        attention_factor = 1.0  # Unused in this type of RoPE\n+\n+        # Compute the inverse frequencies\n+        inv_freq = 1.0 / (\n+            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n+        )\n+        return inv_freq, attention_factor\n \n     @torch.no_grad()\n     @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)"
        },
        {
            "sha": "db7ccaf185aec785e688c41022c787601b12d70f",
            "filename": "src/transformers/models/helium/configuration_helium.py",
            "status": "modified",
            "additions": 35,
            "deletions": 23,
            "changes": 58,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fhelium%2Fconfiguration_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fhelium%2Fconfiguration_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhelium%2Fconfiguration_helium.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -14,7 +14,10 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n+from typing import Optional\n+\n from ...configuration_utils import PreTrainedConfig\n+from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n \n \n class HeliumConfig(PreTrainedConfig):\n@@ -63,8 +66,10 @@ class HeliumConfig(PreTrainedConfig):\n             relevant if `config.is_decoder=True`.\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie weight embeddings\n-        rope_theta (`float`, *optional*, defaults to 100000.0):\n-            The base period of the RoPE embeddings.\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         pad_token_id (`int`, *optional*, defaults to 3):\n             Padding token id.\n         eos_token_id (`int` | `list`, *optional*, defaults to 2):\n@@ -105,26 +110,26 @@ class HeliumConfig(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size=48000,\n-        hidden_size=2560,\n-        intermediate_size=7040,\n-        num_hidden_layers=24,\n-        num_attention_heads=20,\n-        num_key_value_heads=20,\n-        head_dim=128,\n-        hidden_act=\"silu\",\n-        attention_dropout=0.0,\n-        max_position_embeddings=4096,\n-        initializer_range=0.02,\n-        rms_norm_eps=1e-8,\n-        use_cache=True,\n-        tie_word_embeddings=False,\n-        rope_theta=100000.0,\n-        pad_token_id=3,\n-        eos_token_id=2,\n-        bos_token_id=1,\n-        attention_bias=False,\n-        mlp_bias=False,\n+        vocab_size: Optional[int] = 48000,\n+        hidden_size: Optional[int] = 2560,\n+        intermediate_size: Optional[int] = 7040,\n+        num_hidden_layers: Optional[int] = 24,\n+        num_attention_heads: Optional[int] = 20,\n+        num_key_value_heads: Optional[int] = 20,\n+        head_dim: Optional[int] = 128,\n+        hidden_act: Optional[str] = \"silu\",\n+        attention_dropout: Optional[float] = 0.0,\n+        max_position_embeddings: Optional[int] = 4096,\n+        initializer_range: Optional[float] = 0.02,\n+        rms_norm_eps: Optional[int] = 1e-8,\n+        use_cache: Optional[bool] = True,\n+        tie_word_embeddings: Optional[bool] = False,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        pad_token_id: Optional[int] = 3,\n+        eos_token_id: Optional[int] = 2,\n+        bos_token_id: Optional[int] = 1,\n+        attention_bias: Optional[bool] = False,\n+        mlp_bias: Optional[bool] = False,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size\n@@ -139,10 +144,17 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.rms_norm_eps = rms_norm_eps\n         self.use_cache = use_cache\n-        self.rope_theta = rope_theta\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n         self.mlp_bias = mlp_bias\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n+\n+        # Validate the correctness of rotary position embeddings parameters\n+        rope_theta = kwargs.get(\"rope_theta\", 100000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n+        rope_config_validation(self)\n \n         super().__init__(\n             pad_token_id=pad_token_id,"
        },
        {
            "sha": "a1d0a09e848f63f96d90d2017d1844cbe39d6a30",
            "filename": "src/transformers/models/helium/modeling_helium.py",
            "status": "modified",
            "additions": 43,
            "deletions": 14,
            "changes": 57,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -66,20 +66,49 @@ class HeliumRotaryEmbedding(nn.Module):\n \n     def __init__(self, config: HeliumConfig, device=None):\n         super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n         self.max_seq_len_cached = config.max_position_embeddings\n         self.original_max_seq_len = config.max_position_embeddings\n \n         self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.rope_type = self.config.rope_parameters[\"rope_type\"]\n+        rope_init_fn: Callable = self.compute_default_rope_parameters\n+        if self.rope_type != \"default\":\n+            rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+        inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n+\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n+        self.original_inv_freq = inv_freq\n+\n+    @staticmethod\n+    def compute_default_rope_parameters(\n+        config: Optional[HeliumConfig] = None,\n+        device: Optional[\"torch.device\"] = None,\n+        seq_len: Optional[int] = None,\n+    ) -> tuple[\"torch.Tensor\", float]:\n+        \"\"\"\n+        Computes the inverse frequencies according to the original RoPE implementation\n+        Args:\n+            config ([`~transformers.PreTrainedConfig`]):\n+                The model configuration.\n+            device (`torch.device`):\n+                The device to use for initialization of the inverse frequencies.\n+            seq_len (`int`, *optional*):\n+                The current sequence length. Unused for this type of RoPE.\n+        Returns:\n+            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n+            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n+        \"\"\"\n+        base = config.rope_parameters[\"rope_theta\"]\n+        dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+\n+        attention_factor = 1.0  # Unused in this type of RoPE\n+\n+        # Compute the inverse frequencies\n+        inv_freq = 1.0 / (\n+            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n+        )\n+        return inv_freq, attention_factor\n \n     @torch.no_grad()\n     @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n@@ -218,8 +247,8 @@ def __init__(self, config: HeliumConfig, layer_idx: Optional[int] = None):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n-        attention_mask: Optional[torch.Tensor],\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -278,7 +307,7 @@ def forward(\n         past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> torch.Tensor:\n         residual = hidden_states\n@@ -335,7 +364,7 @@ def __init__(self, config: HeliumConfig):\n             [HeliumDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n         )\n         self.norm = HeliumRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n-        self.rotary_emb = HeliumRotaryEmbedding(config)\n+        self.rotary_emb = HeliumRotaryEmbedding(config=config)\n         self.gradient_checkpointing = False\n \n         # Initialize weights and apply final processing\n@@ -382,16 +411,16 @@ def forward(\n         )\n \n         hidden_states = inputs_embeds\n-        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids=position_ids)\n \n         for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n             hidden_states = decoder_layer(\n                 hidden_states,\n                 attention_mask=causal_mask,\n+                position_embeddings=position_embeddings,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n                 cache_position=cache_position,\n-                position_embeddings=position_embeddings,\n                 **kwargs,\n             )\n "
        },
        {
            "sha": "79d995720e30d495f10dfc74c0a470a7bd4fe2d6",
            "filename": "src/transformers/models/helium/modular_helium.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodular_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodular_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodular_helium.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -121,7 +121,6 @@ def __init__(self, config: HeliumConfig):\n             [HeliumDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n         )\n         self.norm = HeliumRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n-        self.rotary_emb = HeliumRotaryEmbedding(config)\n         self.gradient_checkpointing = False\n \n         # Initialize weights and apply final processing"
        },
        {
            "sha": "29dd3ac34f98d72f5a523529be3d34cdb221a660",
            "filename": "src/transformers/models/hunyuan_v1_dense/configuration_hunyuan_v1_dense.py",
            "status": "modified",
            "additions": 59,
            "deletions": 56,
            "changes": 115,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fconfiguration_hunyuan_v1_dense.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fconfiguration_hunyuan_v1_dense.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fconfiguration_hunyuan_v1_dense.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -14,8 +14,11 @@\n # limitations under the License.\n \"\"\"HunYuanDenseV1 model configuration\"\"\"\n \n-from transformers.configuration_utils import PreTrainedConfig\n-from transformers.utils import logging\n+from typing import Optional\n+\n+from ...configuration_utils import PreTrainedConfig\n+from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n@@ -79,16 +82,10 @@ class HunYuanDenseV1Config(PreTrainedConfig):\n             issue](https://github.com/pytorch/pytorch/issues/76232).\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie weight embeddings\n-        rope_theta (`float`, *optional*, defaults to 10000.0):\n-            The base period of the RoPE embeddings.\n-        rope_scaling (`Dict`, *optional*):\n-            Dictionary containing the scaling configuration for the RoPE embeddings. Currently supports two scaling\n-            strategies: linear and dynamic. Their scaling factor must be a float greater than 1. The expected format is\n-            `{\"type\": strategy name, \"factor\": scaling factor}`. When using this flag, don't update\n-            `max_position_embeddings` to the expected new maximum. See the following thread for more information on how\n-            these scaling strategies behave:\n-            https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/. This is an\n-            experimental feature, subject to breaking API changes in future versions.\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):\n             Whether to use a bias in the query, key, value and output projection layers during self-attention.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n@@ -102,28 +99,27 @@ class HunYuanDenseV1Config(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size=290943,\n-        hidden_size=4096,\n-        intermediate_size: int = 11008,\n-        num_hidden_layers=32,\n-        num_attention_heads=32,\n-        num_key_value_heads=None,\n-        hidden_act=\"silu\",\n-        max_position_embeddings=2048,\n-        initializer_range=0.02,\n-        rms_norm_eps=1e-5,\n-        use_cache=True,\n-        pad_token_id=0,\n-        bos_token_id=1,\n-        eos_token_id=2,\n-        eod_token_id=3,\n-        pretraining_tp=1,\n-        tie_word_embeddings=False,\n-        rope_theta=10000.0,\n-        rope_scaling=None,\n-        attention_bias=False,\n-        attention_dropout=0.0,\n-        head_dim=None,\n+        vocab_size: Optional[int] = 290943,\n+        hidden_size: Optional[int] = 4096,\n+        intermediate_size: Optional[int] = 11008,\n+        num_hidden_layers: Optional[int] = 32,\n+        num_attention_heads: Optional[int] = 32,\n+        num_key_value_heads: Optional[int] = None,\n+        hidden_act: Optional[str] = \"silu\",\n+        max_position_embeddings: Optional[int] = 2048,\n+        initializer_range: Optional[float] = 0.02,\n+        rms_norm_eps: Optional[float] = 1e-5,\n+        use_cache: Optional[bool] = True,\n+        pad_token_id: Optional[int] = 0,\n+        bos_token_id: Optional[int] = 1,\n+        eos_token_id: Optional[int] = 2,\n+        eod_token_id: Optional[int] = 3,\n+        pretraining_tp: Optional[int] = 1,\n+        tie_word_embeddings: Optional[bool] = False,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        attention_bias: Optional[bool] = False,\n+        attention_dropout: Optional[float] = 0.0,\n+        head_dim: Optional[int] = None,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size\n@@ -143,11 +139,16 @@ def __init__(\n         self.rms_norm_eps = rms_norm_eps\n         self.pretraining_tp = pretraining_tp\n         self.use_cache = use_cache\n-        self.rope_theta = rope_theta\n-        self.rope_scaling = rope_scaling\n-        # self._rope_scaling_validation()   # TODO: Need validation?\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n+\n+        # Validate the correctness of rotary position embeddings parameters\n+        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n+        rope_config_validation(self)  # TODO needs model-specific validation?\n \n         super().__init__(\n             pad_token_id=pad_token_id,\n@@ -157,33 +158,35 @@ def __init__(\n             **kwargs,\n         )\n \n-    def _rope_scaling_validation(self):\n+    def _rope_parameters_validation(self):\n         \"\"\"\n-        Validate the `rope_scaling` configuration.\n+        Validate the `rope_parameters` configuration.\n         \"\"\"\n-        if self.rope_scaling is None:\n+        if self.rope_parameters is None:\n             return\n \n-        if not isinstance(self.rope_scaling, dict) or len(self.rope_scaling) != 2:\n+        if not isinstance(self.rope_parameters, dict) or len(self.rope_parameters) != 2:\n             raise ValueError(\n-                \"`rope_scaling` must be a dictionary with with two fields, `type` and `factor` or `type` and `alpha`, \"\n-                f\"got {self.rope_scaling}\"\n+                \"`rope_parameters` must be a dictionary with with two fields, `type` and `factor` or `type` and `alpha`, \"\n+                f\"got {self.rope_parameters}\"\n             )\n-        rope_scaling_type = self.rope_scaling.get(\"type\", None)\n-        rope_scaling_factor = self.rope_scaling.get(\"factor\", None)\n-        rope_scaling_alpha = self.rope_scaling.get(\"alpha\", None)\n-        if rope_scaling_type is None or rope_scaling_type not in [\"linear\", \"dynamic\"]:\n+        rope_parameters_type = self.rope_parameters.get(\"type\", None)\n+        rope_parameters_factor = self.rope_parameters.get(\"factor\", None)\n+        rope_parameters_alpha = self.rope_parameters.get(\"alpha\", None)\n+        if rope_parameters_type is None or rope_parameters_type not in [\"linear\", \"dynamic\"]:\n             raise ValueError(\n-                f\"`rope_scaling`'s type field must be one of ['linear', 'dynamic'], got {rope_scaling_type}\"\n+                f\"`rope_parameters`'s type field must be one of ['linear', 'dynamic'], got {rope_parameters_type}\"\n             )\n-        if rope_scaling_factor is None and rope_scaling_alpha is None:\n-            raise ValueError(\"`rope_scaling`'s factor or alpha field must be have one, got both of none\")\n-        if rope_scaling_factor is not None:\n-            if not isinstance(rope_scaling_factor, float) or rope_scaling_factor <= 1.0:\n-                raise ValueError(f\"`rope_scaling`'s factor field must be a float > 1.0, got {rope_scaling_factor}\")\n-        if rope_scaling_alpha is not None:\n-            if not isinstance(rope_scaling_alpha, float) or rope_scaling_alpha <= 1.0:\n-                raise ValueError(f\"`rope_scaling`'s alpha field must be a float > 1.0, got {rope_scaling_alpha}\")\n+        if rope_parameters_factor is None and rope_parameters_alpha is None:\n+            raise ValueError(\"`rope_parameters`'s factor or alpha field must be have one, got both of none\")\n+        if rope_parameters_factor is not None:\n+            if not isinstance(rope_parameters_factor, float) or rope_parameters_factor <= 1.0:\n+                raise ValueError(\n+                    f\"`rope_parameters`'s factor field must be a float > 1.0, got {rope_parameters_factor}\"\n+                )\n+        if rope_parameters_alpha is not None:\n+            if not isinstance(rope_parameters_alpha, float) or rope_parameters_alpha <= 1.0:\n+                raise ValueError(f\"`rope_parameters`'s alpha field must be a float > 1.0, got {rope_parameters_alpha}\")\n \n \n __all__ = [\"HunYuanDenseV1Config\"]"
        },
        {
            "sha": "e3a55c296f6fc128f2fe2c7ed1610eb3f8f5698d",
            "filename": "src/transformers/models/hunyuan_v1_dense/modeling_hunyuan_v1_dense.py",
            "status": "modified",
            "additions": 47,
            "deletions": 15,
            "changes": 62,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fmodeling_hunyuan_v1_dense.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fmodeling_hunyuan_v1_dense.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fmodeling_hunyuan_v1_dense.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -246,7 +246,7 @@ def forward(\n         past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> torch.Tensor:\n         residual = hidden_states\n@@ -307,27 +307,59 @@ class HunYuanDenseV1RotaryEmbedding(nn.Module):\n \n     def __init__(self, config: HunYuanDenseV1Config, device=None):\n         super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n         self.max_seq_len_cached = config.max_position_embeddings\n         self.original_max_seq_len = config.max_position_embeddings\n \n         self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n-        if self.rope_type == \"dynamic\" and config.rope_scaling[\"alpha\"]:\n-            # DynamicNTKAlphaRotary\n+\n+        self.rope_type = self.config.rope_parameters[\"rope_type\"]\n+\n+        # Diff from Llama - DynamicNTKAlphaRotary\n+        if self.rope_type == \"dynamic\" and self.config.rope_parameters.get(\"alpha\"):\n             self.dim = config.head_dim\n-            base = config.rope_theta * config.rope_scaling.get(\"alpha\") ** (self.dim / (self.dim - 2))\n-            inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2).float().to(device) / self.dim))\n+            base = self.config.rope_parameters[\"rope_theta\"] * self.config.rope_parameters[\"alpha\"] ** (\n+                self.config.head_dim / (self.config.head_dim - 2)\n+            )\n+            inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2).float().to(device) / self.config.head_dim))\n             self.attention_scaling = 1.0\n         else:\n-            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+            rope_init_fn: Callable = self.compute_default_rope_parameters\n+            if self.rope_type != \"default\":\n+                rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+            inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n+        self.original_inv_freq = inv_freq\n+\n+    @staticmethod\n+    def compute_default_rope_parameters(\n+        config: Optional[HunYuanDenseV1Config] = None,\n+        device: Optional[\"torch.device\"] = None,\n+        seq_len: Optional[int] = None,\n+    ) -> tuple[\"torch.Tensor\", float]:\n+        \"\"\"\n+        Computes the inverse frequencies according to the original RoPE implementation\n+        Args:\n+            config ([`~transformers.PreTrainedConfig`]):\n+                The model configuration.\n+            device (`torch.device`):\n+                The device to use for initialization of the inverse frequencies.\n+            seq_len (`int`, *optional*):\n+                The current sequence length. Unused for this type of RoPE.\n+        Returns:\n+            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n+            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n+        \"\"\"\n+        base = config.rope_parameters[\"rope_theta\"]\n+        dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+\n+        attention_factor = 1.0  # Unused in this type of RoPE\n+\n+        # Compute the inverse frequencies\n+        inv_freq = 1.0 / (\n+            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n+        )\n+        return inv_freq, attention_factor\n \n     @torch.no_grad()\n     @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n@@ -404,16 +436,16 @@ def forward(\n         )\n \n         hidden_states = inputs_embeds\n-        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids=position_ids)\n \n         for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n             hidden_states = decoder_layer(\n                 hidden_states,\n                 attention_mask=causal_mask,\n+                position_embeddings=position_embeddings,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n                 cache_position=cache_position,\n-                position_embeddings=position_embeddings,\n                 **kwargs,\n             )\n "
        },
        {
            "sha": "31a03ac05cc710efb7901640a0aa0dc8a4f42c24",
            "filename": "src/transformers/models/hunyuan_v1_dense/modular_hunyuan_v1_dense.py",
            "status": "modified",
            "additions": 18,
            "deletions": 32,
            "changes": 50,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fmodular_hunyuan_v1_dense.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fmodular_hunyuan_v1_dense.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fmodular_hunyuan_v1_dense.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -25,7 +25,7 @@\n     logging,\n )\n \n-from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs\n@@ -38,6 +38,7 @@\n     LlamaModel,\n     LlamaPreTrainedModel,\n     LlamaRMSNorm,\n+    LlamaRotaryEmbedding,\n     apply_rotary_pos_emb,\n     eager_attention_forward,\n )\n@@ -131,47 +132,32 @@ def _init_weights(self, module):\n                 module.weight.data[module.padding_idx].zero_()\n \n \n-class HunYuanDenseV1RotaryEmbedding(nn.Module):\n-    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n-\n+class HunYuanDenseV1RotaryEmbedding(LlamaRotaryEmbedding):\n     def __init__(self, config: HunYuanDenseV1Config, device=None):\n-        super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n+        nn.Module.__init__()\n         self.max_seq_len_cached = config.max_position_embeddings\n         self.original_max_seq_len = config.max_position_embeddings\n \n         self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n-        if self.rope_type == \"dynamic\" and config.rope_scaling[\"alpha\"]:\n-            # DynamicNTKAlphaRotary\n+\n+        self.rope_type = self.config.rope_parameters[\"rope_type\"]\n+\n+        # Diff from Llama - DynamicNTKAlphaRotary\n+        if self.rope_type == \"dynamic\" and self.config.rope_parameters.get(\"alpha\"):\n             self.dim = config.head_dim\n-            base = config.rope_theta * config.rope_scaling.get(\"alpha\") ** (self.dim / (self.dim - 2))\n-            inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2).float().to(device) / self.dim))\n+            base = self.config.rope_parameters[\"rope_theta\"] * self.config.rope_parameters[\"alpha\"] ** (\n+                self.config.head_dim / (self.config.head_dim - 2)\n+            )\n+            inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2).float().to(device) / self.config.head_dim))\n             self.attention_scaling = 1.0\n         else:\n-            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+            rope_init_fn: Callable = self.compute_default_rope_parameters\n+            if self.rope_type != \"default\":\n+                rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+            inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n-\n-    @torch.no_grad()\n-    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n-    def forward(self, x, position_ids):\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n-        position_ids_expanded = position_ids[:, None, :].float()\n-\n-        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n-            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos() * self.attention_scaling\n-            sin = emb.sin() * self.attention_scaling\n-\n-        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+        self.original_inv_freq = inv_freq\n \n \n class HunYuanDenseV1Model(LlamaModel):"
        },
        {
            "sha": "497a5674f4f3b93c55f322a40d745c55a657c0ae",
            "filename": "src/transformers/models/hunyuan_v1_moe/configuration_hunyuan_v1_moe.py",
            "status": "modified",
            "additions": 59,
            "deletions": 58,
            "changes": 117,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fconfiguration_hunyuan_v1_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fconfiguration_hunyuan_v1_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fconfiguration_hunyuan_v1_moe.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -14,10 +14,11 @@\n # limitations under the License.\n \"\"\"HunYuanMoEV1 model configuration\"\"\"\n \n-from typing import Union\n+from typing import Optional, Union\n \n-from transformers.configuration_utils import PreTrainedConfig\n-from transformers.utils import logging\n+from ...configuration_utils import PreTrainedConfig\n+from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n@@ -83,16 +84,10 @@ class HunYuanMoEV1Config(PreTrainedConfig):\n             issue](https://github.com/pytorch/pytorch/issues/76232).\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie weight embeddings\n-        rope_theta (`float`, *optional*, defaults to 10000.0):\n-            The base period of the RoPE embeddings.\n-        rope_scaling (`Dict`, *optional*):\n-            Dictionary containing the scaling configuration for the RoPE embeddings. Currently supports two scaling\n-            strategies: linear and dynamic. Their scaling factor must be a float greater than 1. The expected format is\n-            `{\"type\": strategy name, \"factor\": scaling factor}`. When using this flag, don't update\n-            `max_position_embeddings` to the expected new maximum. See the following thread for more information on how\n-            these scaling strategies behave:\n-            https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/. This is an\n-            experimental feature, subject to breaking API changes in future versions.\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):\n             Whether to use a bias in the query, key, value and output projection layers during self-attention.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n@@ -114,31 +109,30 @@ class HunYuanMoEV1Config(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size=290943,\n-        hidden_size=4096,\n-        intermediate_size: int = 11008,\n-        num_hidden_layers=32,\n-        num_attention_heads=32,\n-        num_key_value_heads=None,\n-        hidden_act=\"silu\",\n-        max_position_embeddings=2048,\n-        initializer_range=0.02,\n-        rms_norm_eps=1e-5,\n-        use_cache=True,\n-        pad_token_id=0,\n-        bos_token_id=1,\n-        eos_token_id=2,\n-        eod_token_id=3,\n-        sep_token_id=4,\n-        pretraining_tp=1,\n-        tie_word_embeddings=False,\n-        rope_theta=10000.0,\n-        rope_scaling=None,\n-        attention_bias=False,\n-        attention_dropout=0.0,\n+        vocab_size: Optional[int] = 290943,\n+        hidden_size: Optional[int] = 4096,\n+        intermediate_size: Optional[int] = 11008,\n+        num_hidden_layers: Optional[int] = 32,\n+        num_attention_heads: Optional[int] = 32,\n+        num_key_value_heads: Optional[int] = None,\n+        hidden_act: Optional[str] = \"silu\",\n+        max_position_embeddings: Optional[int] = 2048,\n+        initializer_range: Optional[float] = 0.02,\n+        rms_norm_eps: Optional[float] = 1e-5,\n+        use_cache: Optional[bool] = True,\n+        pad_token_id: Optional[int] = 0,\n+        bos_token_id: Optional[int] = 1,\n+        eos_token_id: Optional[int] = 2,\n+        eod_token_id: Optional[int] = 3,\n+        sep_token_id: Optional[int] = 4,\n+        pretraining_tp: Optional[int] = 1,\n+        tie_word_embeddings: Optional[bool] = False,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        attention_bias: Optional[bool] = False,\n+        attention_dropout: Optional[float] = 0.0,\n         num_experts: Union[int, list] = 1,\n         moe_topk: Union[int, list] = 1,\n-        head_dim=None,\n+        head_dim: Optional[int] = None,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size\n@@ -161,11 +155,16 @@ def __init__(\n         self.rms_norm_eps = rms_norm_eps\n         self.pretraining_tp = pretraining_tp\n         self.use_cache = use_cache\n-        self.rope_theta = rope_theta\n-        self.rope_scaling = rope_scaling\n-        # self._rope_scaling_validation()   # TODO: Need validation?\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n+\n+        # Validate the correctness of rotary position embeddings parameters\n+        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n+        rope_config_validation(self)\n \n         super().__init__(\n             pad_token_id=pad_token_id,\n@@ -176,33 +175,35 @@ def __init__(\n             **kwargs,\n         )\n \n-    def _rope_scaling_validation(self):\n+    def _rope_parameters_validation(self):\n         \"\"\"\n-        Validate the `rope_scaling` configuration.\n+        Validate the `rope_parameters` configuration.\n         \"\"\"\n-        if self.rope_scaling is None:\n+        if self.rope_parameters is None:\n             return\n \n-        if not isinstance(self.rope_scaling, dict) or len(self.rope_scaling) != 2:\n+        if not isinstance(self.rope_parameters, dict) or len(self.rope_parameters) != 2:\n             raise ValueError(\n-                \"`rope_scaling` must be a dictionary with with two fields, `type` and `factor` or `type` and `alpha`, \"\n-                f\"got {self.rope_scaling}\"\n+                \"`rope_parameters` must be a dictionary with with two fields, `type` and `factor` or `type` and `alpha`, \"\n+                f\"got {self.rope_parameters}\"\n             )\n-        rope_scaling_type = self.rope_scaling.get(\"type\", None)\n-        rope_scaling_factor = self.rope_scaling.get(\"factor\", None)\n-        rope_scaling_alpha = self.rope_scaling.get(\"alpha\", None)\n-        if rope_scaling_type is None or rope_scaling_type not in [\"linear\", \"dynamic\"]:\n+        rope_parameters_type = self.rope_parameters.get(\"type\", None)\n+        rope_parameters_factor = self.rope_parameters.get(\"factor\", None)\n+        rope_parameters_alpha = self.rope_parameters.get(\"alpha\", None)\n+        if rope_parameters_type is None or rope_parameters_type not in [\"linear\", \"dynamic\"]:\n             raise ValueError(\n-                f\"`rope_scaling`'s type field must be one of ['linear', 'dynamic'], got {rope_scaling_type}\"\n+                f\"`rope_parameters`'s type field must be one of ['linear', 'dynamic'], got {rope_parameters_type}\"\n             )\n-        if rope_scaling_factor is None and rope_scaling_alpha is None:\n-            raise ValueError(\"`rope_scaling`'s factor or alpha field must be have one, got both of none\")\n-        if rope_scaling_factor is not None:\n-            if not isinstance(rope_scaling_factor, float) or rope_scaling_factor <= 1.0:\n-                raise ValueError(f\"`rope_scaling`'s factor field must be a float > 1.0, got {rope_scaling_factor}\")\n-        if rope_scaling_alpha is not None:\n-            if not isinstance(rope_scaling_alpha, float) or rope_scaling_alpha <= 1.0:\n-                raise ValueError(f\"`rope_scaling`'s alpha field must be a float > 1.0, got {rope_scaling_alpha}\")\n+        if rope_parameters_factor is None and rope_parameters_alpha is None:\n+            raise ValueError(\"`rope_parameters`'s factor or alpha field must be have one, got both of none\")\n+        if rope_parameters_factor is not None:\n+            if not isinstance(rope_parameters_factor, float) or rope_parameters_factor <= 1.0:\n+                raise ValueError(\n+                    f\"`rope_parameters`'s factor field must be a float > 1.0, got {rope_parameters_factor}\"\n+                )\n+        if rope_parameters_alpha is not None:\n+            if not isinstance(rope_parameters_alpha, float) or rope_parameters_alpha <= 1.0:\n+                raise ValueError(f\"`rope_parameters`'s alpha field must be a float > 1.0, got {rope_parameters_alpha}\")\n \n \n __all__ = [\"HunYuanMoEV1Config\"]"
        },
        {
            "sha": "a1fded6bdf778d2df241298dc8778f316fcabb50",
            "filename": "src/transformers/models/hunyuan_v1_moe/modeling_hunyuan_v1_moe.py",
            "status": "modified",
            "additions": 47,
            "deletions": 15,
            "changes": 62,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fmodeling_hunyuan_v1_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fmodeling_hunyuan_v1_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fmodeling_hunyuan_v1_moe.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -325,7 +325,7 @@ def forward(\n         past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> torch.Tensor:\n         residual = hidden_states\n@@ -385,27 +385,59 @@ class HunYuanMoEV1RotaryEmbedding(nn.Module):\n \n     def __init__(self, config: HunYuanMoEV1Config, device=None):\n         super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n         self.max_seq_len_cached = config.max_position_embeddings\n         self.original_max_seq_len = config.max_position_embeddings\n \n         self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n-        if self.rope_type == \"dynamic\" and config.rope_scaling[\"alpha\"]:\n-            # DynamicNTKAlphaRotary\n+\n+        self.rope_type = self.config.rope_parameters[\"rope_type\"]\n+\n+        # Diff from Llama - DynamicNTKAlphaRotary\n+        if self.rope_type == \"dynamic\" and self.config.rope_parameters.get(\"alpha\"):\n             self.dim = config.head_dim\n-            base = config.rope_theta * config.rope_scaling.get(\"alpha\") ** (self.dim / (self.dim - 2))\n-            inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2).float().to(device) / self.dim))\n+            base = self.config.rope_parameters[\"rope_theta\"] * self.config.rope_parameters[\"alpha\"] ** (\n+                self.config.head_dim / (self.config.head_dim - 2)\n+            )\n+            inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2).float().to(device) / self.config.head_dim))\n             self.attention_scaling = 1.0\n         else:\n-            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+            rope_init_fn: Callable = self.compute_default_rope_parameters\n+            if self.rope_type != \"default\":\n+                rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+            inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n \n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n+        self.original_inv_freq = inv_freq\n+\n+    @staticmethod\n+    def compute_default_rope_parameters(\n+        config: Optional[HunYuanMoEV1Config] = None,\n+        device: Optional[\"torch.device\"] = None,\n+        seq_len: Optional[int] = None,\n+    ) -> tuple[\"torch.Tensor\", float]:\n+        \"\"\"\n+        Computes the inverse frequencies according to the original RoPE implementation\n+        Args:\n+            config ([`~transformers.PreTrainedConfig`]):\n+                The model configuration.\n+            device (`torch.device`):\n+                The device to use for initialization of the inverse frequencies.\n+            seq_len (`int`, *optional*):\n+                The current sequence length. Unused for this type of RoPE.\n+        Returns:\n+            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n+            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n+        \"\"\"\n+        base = config.rope_parameters[\"rope_theta\"]\n+        dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+\n+        attention_factor = 1.0  # Unused in this type of RoPE\n+\n+        # Compute the inverse frequencies\n+        inv_freq = 1.0 / (\n+            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n+        )\n+        return inv_freq, attention_factor\n \n     @torch.no_grad()\n     @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n@@ -482,16 +514,16 @@ def forward(\n         )\n \n         hidden_states = inputs_embeds\n-        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids=position_ids)\n \n         for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n             hidden_states = decoder_layer(\n                 hidden_states,\n                 attention_mask=causal_mask,\n+                position_embeddings=position_embeddings,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n                 cache_position=cache_position,\n-                position_embeddings=position_embeddings,\n                 **kwargs,\n             )\n "
        },
        {
            "sha": "06269fedf784781d4e300e1dfc543ec3e8a25f41",
            "filename": "src/transformers/models/hunyuan_v1_moe/modular_hunyuan_v1_moe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 42,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fmodular_hunyuan_v1_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fmodular_hunyuan_v1_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fmodular_hunyuan_v1_moe.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -26,10 +26,10 @@\n     logging,\n )\n \n-from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs\n+from ..hunyuan_v1_dense.modeling_hunyuan_v1_dense import HunYuanDenseV1RotaryEmbedding\n from ..llama.modeling_llama import (\n     LlamaAttention,\n     LlamaDecoderLayer,\n@@ -189,47 +189,8 @@ def _init_weights(self, module):\n                 module.weight.data[module.padding_idx].zero_()\n \n \n-class HunYuanMoEV1RotaryEmbedding(nn.Module):\n-    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n-\n-    def __init__(self, config: HunYuanMoEV1Config, device=None):\n-        super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n-        self.max_seq_len_cached = config.max_position_embeddings\n-        self.original_max_seq_len = config.max_position_embeddings\n-\n-        self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n-        if self.rope_type == \"dynamic\" and config.rope_scaling[\"alpha\"]:\n-            # DynamicNTKAlphaRotary\n-            self.dim = config.head_dim\n-            base = config.rope_theta * config.rope_scaling.get(\"alpha\") ** (self.dim / (self.dim - 2))\n-            inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2).float().to(device) / self.dim))\n-            self.attention_scaling = 1.0\n-        else:\n-            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n-\n-        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n-\n-    @torch.no_grad()\n-    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n-    def forward(self, x, position_ids):\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n-        position_ids_expanded = position_ids[:, None, :].float()\n-\n-        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n-            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos() * self.attention_scaling\n-            sin = emb.sin() * self.attention_scaling\n-\n-        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+class HunYuanMoEV1RotaryEmbedding(HunYuanDenseV1RotaryEmbedding):\n+    pass\n \n \n class HunYuanMoEV1Model(LlamaModel):"
        },
        {
            "sha": "9f5367d1c01c4d8fed9f560681c64ff761315506",
            "filename": "src/transformers/models/jetmoe/configuration_jetmoe.py",
            "status": "modified",
            "additions": 35,
            "deletions": 24,
            "changes": 59,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fconfiguration_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fconfiguration_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fconfiguration_jetmoe.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -14,7 +14,10 @@\n # limitations under the License.\n \"\"\"JetMoe model configuration\"\"\"\n \n+from typing import Optional\n+\n from ...configuration_utils import PreTrainedConfig\n+from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n from ...utils import logging\n \n \n@@ -70,8 +73,10 @@ class JetMoeConfig(PreTrainedConfig):\n             The id of the \"end-of-sequence\" token.\n         tie_word_embeddings (`bool`, *optional*, defaults to `True`):\n             Whether the model's input and output word embeddings should be tied.\n-        rope_theta (`float`, *optional*, defaults to 10000.0):\n-            The base period of the RoPE embeddings.\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         rms_norm_eps (`float`, *optional*, defaults to 1e-06):\n             The epsilon used by the rms normalization layers.\n         initializer_range (`float`, *optional*, defaults to 0.01):\n@@ -98,26 +103,26 @@ class JetMoeConfig(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size=32000,\n-        hidden_size=2048,\n-        num_hidden_layers=12,\n-        num_key_value_heads=16,\n-        kv_channels=128,\n-        intermediate_size=5632,\n-        max_position_embeddings=4096,\n-        activation_function=\"silu\",\n-        num_local_experts=8,\n-        num_experts_per_tok=2,\n-        output_router_logits=False,\n-        aux_loss_coef=0.01,\n-        use_cache=True,\n-        bos_token_id=1,\n-        eos_token_id=2,\n-        tie_word_embeddings=True,\n-        rope_theta=10000.0,\n-        rms_norm_eps=1e-6,\n-        initializer_range=0.01,\n-        attention_dropout=0.0,\n+        vocab_size: Optional[int] = 32000,\n+        hidden_size: Optional[int] = 2048,\n+        num_hidden_layers: Optional[int] = 12,\n+        num_key_value_heads: Optional[int] = 16,\n+        kv_channels: Optional[int] = 128,\n+        intermediate_size: Optional[int] = 5632,\n+        max_position_embeddings: Optional[int] = 4096,\n+        activation_function: Optional[str] = \"silu\",\n+        num_local_experts: Optional[int] = 8,\n+        num_experts_per_tok: Optional[int] = 2,\n+        output_router_logits: Optional[bool] = False,\n+        aux_loss_coef: Optional[float] = 0.01,\n+        use_cache: Optional[bool] = True,\n+        bos_token_id: Optional[int] = 1,\n+        eos_token_id: Optional[int] = 2,\n+        tie_word_embeddings: Optional[bool] = True,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rms_norm_eps: Optional[int] = 1e-6,\n+        initializer_range: Optional[float] = 0.01,\n+        attention_dropout: Optional[float] = 0.0,\n         **kwargs,\n     ):\n         if num_experts_per_tok > num_local_experts:\n@@ -141,9 +146,15 @@ def __init__(\n \n         self.bos_token_id = bos_token_id\n         self.eos_token_id = eos_token_id\n-\n-        self.rope_theta = rope_theta\n         self.rms_norm_eps = rms_norm_eps\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n+\n+        # Validate the correctness of rotary position embeddings parameters\n+        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n+        rope_config_validation(self)\n \n         super().__init__(\n             bos_token_id=bos_token_id, eos_token_id=eos_token_id, tie_word_embeddings=tie_word_embeddings, **kwargs"
        },
        {
            "sha": "1beb7be7626ce33069923f79335ea185d3af5781",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 38,
            "deletions": 9,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -70,20 +70,49 @@ class JetMoeRotaryEmbedding(nn.Module):\n \n     def __init__(self, config: JetMoeConfig, device=None):\n         super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n         self.max_seq_len_cached = config.max_position_embeddings\n         self.original_max_seq_len = config.max_position_embeddings\n \n         self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.rope_type = self.config.rope_parameters[\"rope_type\"]\n+        rope_init_fn: Callable = self.compute_default_rope_parameters\n+        if self.rope_type != \"default\":\n+            rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+        inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n+\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n+        self.original_inv_freq = inv_freq\n+\n+    @staticmethod\n+    def compute_default_rope_parameters(\n+        config: Optional[JetMoeConfig] = None,\n+        device: Optional[\"torch.device\"] = None,\n+        seq_len: Optional[int] = None,\n+    ) -> tuple[\"torch.Tensor\", float]:\n+        \"\"\"\n+        Computes the inverse frequencies according to the original RoPE implementation\n+        Args:\n+            config ([`~transformers.PreTrainedConfig`]):\n+                The model configuration.\n+            device (`torch.device`):\n+                The device to use for initialization of the inverse frequencies.\n+            seq_len (`int`, *optional*):\n+                The current sequence length. Unused for this type of RoPE.\n+        Returns:\n+            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n+            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n+        \"\"\"\n+        base = config.rope_parameters[\"rope_theta\"]\n+        dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+\n+        attention_factor = 1.0  # Unused in this type of RoPE\n+\n+        # Compute the inverse frequencies\n+        inv_freq = 1.0 / (\n+            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n+        )\n+        return inv_freq, attention_factor\n \n     @torch.no_grad()\n     @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n@@ -509,7 +538,7 @@ def forward(\n         past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> torch.Tensor:\n         residual = hidden_states"
        },
        {
            "sha": "d994388969e342a3dd967c238ac992ab6f082582",
            "filename": "src/transformers/models/jetmoe/modular_jetmoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodular_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodular_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodular_jetmoe.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -394,7 +394,7 @@ def forward(\n         past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> torch.Tensor:\n         residual = hidden_states"
        },
        {
            "sha": "d43856daa96af2de5e7c37b78a202e71dce609de",
            "filename": "src/transformers/models/kyutai_speech_to_text/configuration_kyutai_speech_to_text.py",
            "status": "modified",
            "additions": 38,
            "deletions": 26,
            "changes": 64,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fconfiguration_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fconfiguration_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fconfiguration_kyutai_speech_to_text.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -13,7 +13,10 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.s\n \n+from typing import Optional\n+\n from ...configuration_utils import PreTrainedConfig\n+from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n from ...utils import logging\n from ..auto.configuration_auto import AutoConfig\n \n@@ -56,8 +59,10 @@ class KyutaiSpeechToTextConfig(PreTrainedConfig):\n         max_position_embeddings (`int`, *optional*, defaults to 750):\n             The maximum sequence length that this model might ever be used with. Typically, set this to something large\n             just in case (e.g., 512 or 1024 or 2048).\n-        rope_theta (`float`, *optional*, defaults to 100000.0):\n-            The base period of the RoPE embeddings.\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n             The non-linear activation function (function or string) in the decoder.\n         head_dim (`int`, *optional*, defaults to `hidden_size // num_attention_heads`):\n@@ -117,29 +122,29 @@ class KyutaiSpeechToTextConfig(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        codebook_vocab_size=2049,\n-        vocab_size=4001,\n-        hidden_size=2048,\n-        num_hidden_layers=48,\n-        num_attention_heads=32,\n-        num_key_value_heads=None,\n-        max_position_embeddings=750,\n-        rope_theta=100000.0,\n-        hidden_act=\"silu\",\n-        head_dim=None,\n-        initializer_range=0.02,\n-        use_cache=True,\n-        sliding_window=375,\n-        attention_dropout=0.0,\n-        ffn_dim=11264,\n-        rms_norm_eps=1e-8,\n-        num_codebooks=32,\n-        audio_bos_token_id=2048,\n-        audio_pad_token_id=69569,\n-        tie_word_embeddings=False,\n-        pad_token_id=3,\n-        bos_token_id=48000,\n-        codec_config=None,\n+        codebook_vocab_size: Optional[int] = 2049,\n+        vocab_size: Optional[int] = 4001,\n+        hidden_size: Optional[int] = 2048,\n+        num_hidden_layers: Optional[int] = 48,\n+        num_attention_heads: Optional[int] = 32,\n+        num_key_value_heads: Optional[int] = None,\n+        max_position_embeddings: Optional[int] = 750,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        hidden_act: Optional[str] = \"silu\",\n+        head_dim: Optional[int] = None,\n+        initializer_range: Optional[float] = 0.02,\n+        use_cache: Optional[bool] = True,\n+        sliding_window: Optional[int] = 375,\n+        attention_dropout: Optional[float] = 0.0,\n+        ffn_dim: Optional[int] = 11264,\n+        rms_norm_eps: Optional[int] = 1e-8,\n+        num_codebooks: Optional[int] = 32,\n+        audio_bos_token_id: Optional[int] = 2048,\n+        audio_pad_token_id: Optional[int] = 69569,\n+        tie_word_embeddings: Optional[bool] = False,\n+        pad_token_id: Optional[int] = 3,\n+        bos_token_id: Optional[int] = 48000,\n+        codec_config: Optional[dict] = None,\n         **kwargs,\n     ):\n         if codec_config is None:\n@@ -175,10 +180,17 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.rms_norm_eps = rms_norm_eps\n         self.use_cache = use_cache\n-        self.rope_theta = rope_theta\n         self.attention_dropout = attention_dropout\n         self.head_dim = head_dim if head_dim is not None else self.hidden_size // self.num_attention_heads\n         self.sliding_window = sliding_window\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n+\n+        # Validate the correctness of rotary position embeddings parameters\n+        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n+        rope_config_validation(self)\n \n         super().__init__(\n             pad_token_id=pad_token_id, bos_token_id=bos_token_id, tie_word_embeddings=tie_word_embeddings, **kwargs"
        },
        {
            "sha": "e3f9824de41d057bd871488bd39fc2d8a1b25244",
            "filename": "src/transformers/models/kyutai_speech_to_text/modeling_kyutai_speech_to_text.py",
            "status": "modified",
            "additions": 38,
            "deletions": 9,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -21,6 +21,7 @@\n \n import math\n import types\n+from collections.abc import Callable\n from typing import Optional, Union\n \n import torch\n@@ -272,20 +273,49 @@ class KyutaiSpeechToTextRotaryEmbedding(nn.Module):\n \n     def __init__(self, config: KyutaiSpeechToTextConfig, device=None):\n         super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n         self.max_seq_len_cached = config.max_position_embeddings\n         self.original_max_seq_len = config.max_position_embeddings\n \n         self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.rope_type = self.config.rope_parameters[\"rope_type\"]\n+        rope_init_fn: Callable = self.compute_default_rope_parameters\n+        if self.rope_type != \"default\":\n+            rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+        inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n+\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n+        self.original_inv_freq = inv_freq\n+\n+    @staticmethod\n+    def compute_default_rope_parameters(\n+        config: Optional[KyutaiSpeechToTextConfig] = None,\n+        device: Optional[\"torch.device\"] = None,\n+        seq_len: Optional[int] = None,\n+    ) -> tuple[\"torch.Tensor\", float]:\n+        \"\"\"\n+        Computes the inverse frequencies according to the original RoPE implementation\n+        Args:\n+            config ([`~transformers.PreTrainedConfig`]):\n+                The model configuration.\n+            device (`torch.device`):\n+                The device to use for initialization of the inverse frequencies.\n+            seq_len (`int`, *optional*):\n+                The current sequence length. Unused for this type of RoPE.\n+        Returns:\n+            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n+            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n+        \"\"\"\n+        base = config.rope_parameters[\"rope_theta\"]\n+        dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+\n+        attention_factor = 1.0  # Unused in this type of RoPE\n+\n+        # Compute the inverse frequencies\n+        inv_freq = 1.0 / (\n+            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n+        )\n+        return inv_freq, attention_factor\n \n     @torch.no_grad()\n     @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n@@ -426,7 +456,6 @@ def __init__(\n         # rotary embeddings are not used in the depth decoder\n         self.rotary_emb = None\n         if use_rope:\n-            self.rope_theta = config.rope_theta\n             self.rotary_emb = KyutaiSpeechToTextRotaryEmbedding(config)\n \n     def forward("
        },
        {
            "sha": "4999f6ab433fe335cf64510f4312986294ff1d4d",
            "filename": "src/transformers/models/lfm2/configuration_lfm2.py",
            "status": "modified",
            "additions": 33,
            "deletions": 23,
            "changes": 56,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Flfm2%2Fconfiguration_lfm2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Flfm2%2Fconfiguration_lfm2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2%2Fconfiguration_lfm2.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -14,6 +14,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n+from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n \n \n class Lfm2Config(PreTrainedConfig):\n@@ -65,8 +66,10 @@ class Lfm2Config(PreTrainedConfig):\n             End of stream token id.\n         tie_word_embeddings (`bool`, *optional*, defaults to `True`):\n             Whether to tie weight embeddings\n-        rope_theta (`float`, *optional*, defaults to 1000000.0):\n-            The base period of the RoPE embeddings.\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         conv_bias (`bool`, *optional*, defaults to `False`):\n             Whether to use bias in the conv layers.\n         conv_L_cache (`int`, *optional*, defaults to 3):\n@@ -100,34 +103,33 @@ class Lfm2Config(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size: int = 65536,\n-        hidden_size: int = 2560,\n-        intermediate_size: int = 12288,\n-        num_hidden_layers: int = 32,\n-        num_attention_heads: int = 32,\n-        num_key_value_heads: int = 8,\n-        max_position_embeddings: int = 128_000,\n-        initializer_range: float = 0.02,\n-        norm_eps: float = 0.00001,\n-        use_cache: bool = True,\n-        pad_token_id: int = 0,\n-        bos_token_id: int = 1,\n-        eos_token_id: int = 2,\n-        tie_word_embeddings: bool = True,\n-        rope_theta: float = 1000000.0,\n-        conv_bias: bool = False,\n-        conv_L_cache: int = 3,\n-        block_multiple_of: int = 256,\n-        block_ffn_dim_multiplier: float = 1.0,\n-        block_auto_adjust_ff_dim: bool = True,\n+        vocab_size: Optional[int] = 65536,\n+        hidden_size: Optional[int] = 2560,\n+        intermediate_size: Optional[int] = 12288,\n+        num_hidden_layers: Optional[int] = 32,\n+        num_attention_heads: Optional[int] = 32,\n+        num_key_value_heads: Optional[int] = 8,\n+        max_position_embeddings: Optional[int] = 128_000,\n+        initializer_range: Optional[float] = 0.02,\n+        norm_eps: Optional[float] = 0.00001,\n+        use_cache: Optional[bool] = True,\n+        pad_token_id: Optional[int] = 0,\n+        bos_token_id: Optional[int] = 1,\n+        eos_token_id: Optional[int] = 2,\n+        tie_word_embeddings: Optional[bool] = True,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        conv_bias: Optional[bool] = False,\n+        conv_L_cache: Optional[int] = 3,\n+        block_multiple_of: Optional[int] = 256,\n+        block_ffn_dim_multiplier: Optional[float] = 1.0,\n+        block_auto_adjust_ff_dim: Optional[bool] = True,\n         full_attn_idxs: Optional[list[int]] = None,\n         layer_types: Optional[list[str]] = None,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size\n         self.hidden_size = hidden_size\n         self.num_hidden_layers = num_hidden_layers\n-        self.rope_theta = kwargs.get(\"theta\", rope_theta)  # to fit original config keys\n         self.max_position_embeddings = max_position_embeddings\n         self.use_cache = use_cache\n         self.norm_eps = norm_eps\n@@ -146,12 +148,20 @@ def __init__(\n         self.block_multiple_of = block_multiple_of\n         self.block_ffn_dim_multiplier = block_ffn_dim_multiplier\n         self.block_auto_adjust_ff_dim = block_auto_adjust_ff_dim\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n \n         self.layer_types = layer_types\n         if self.layer_types is None:\n             full_attn_idxs = full_attn_idxs if full_attn_idxs is not None else list(range(num_hidden_layers))\n             self.layer_types = [\"full_attention\" if i in full_attn_idxs else \"conv\" for i in range(num_hidden_layers)]\n \n+        # Validate the correctness of rotary position embeddings parameters\n+        rope_theta = kwargs.get(\"theta\", kwargs.get(\"rope_theta\", 1000000.0))\n+        standardize_rope_params(self, rope_theta=rope_theta)\n+        rope_config_validation(self)\n+\n         tie_word_embeddings = kwargs.get(\"tie_embedding\", tie_word_embeddings)  # to fit original config keys\n         super().__init__(\n             pad_token_id=pad_token_id,"
        },
        {
            "sha": "737e0f53255d60f0bb288388b60345d8658f6339",
            "filename": "src/transformers/models/lfm2/modeling_lfm2.py",
            "status": "modified",
            "additions": 42,
            "deletions": 12,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -71,20 +71,49 @@ class Lfm2RotaryEmbedding(nn.Module):\n \n     def __init__(self, config: Lfm2Config, device=None):\n         super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n         self.max_seq_len_cached = config.max_position_embeddings\n         self.original_max_seq_len = config.max_position_embeddings\n \n         self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.rope_type = self.config.rope_parameters[\"rope_type\"]\n+        rope_init_fn: Callable = self.compute_default_rope_parameters\n+        if self.rope_type != \"default\":\n+            rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+        inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n+\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n+        self.original_inv_freq = inv_freq\n+\n+    @staticmethod\n+    def compute_default_rope_parameters(\n+        config: Optional[Lfm2Config] = None,\n+        device: Optional[\"torch.device\"] = None,\n+        seq_len: Optional[int] = None,\n+    ) -> tuple[\"torch.Tensor\", float]:\n+        \"\"\"\n+        Computes the inverse frequencies according to the original RoPE implementation\n+        Args:\n+            config ([`~transformers.PreTrainedConfig`]):\n+                The model configuration.\n+            device (`torch.device`):\n+                The device to use for initialization of the inverse frequencies.\n+            seq_len (`int`, *optional*):\n+                The current sequence length. Unused for this type of RoPE.\n+        Returns:\n+            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n+            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n+        \"\"\"\n+        base = config.rope_parameters[\"rope_theta\"]\n+        dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+\n+        attention_factor = 1.0  # Unused in this type of RoPE\n+\n+        # Compute the inverse frequencies\n+        inv_freq = 1.0 / (\n+            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n+        )\n+        return inv_freq, attention_factor\n \n     @torch.no_grad()\n     @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n@@ -353,6 +382,7 @@ def forward(\n         attention_mask: Optional[torch.Tensor],\n         past_key_values: Optional[Lfm2HybridConvCache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n         **kwargs,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         input_shape = hidden_states.shape[:-1]\n@@ -526,7 +556,7 @@ def __init__(self, config: Lfm2Config, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Lfm2HybridConvCache] = None,\n@@ -586,8 +616,8 @@ def __init__(self, config: Lfm2Config):\n         self.layers = nn.ModuleList(\n             [Lfm2DecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n         )\n+        self.rotary_emb = Lfm2RotaryEmbedding(config=config)\n         self.gradient_checkpointing = False\n-        self.pos_emb = Lfm2RotaryEmbedding(config)\n         self.embedding_norm = Lfm2RMSNorm(config.hidden_size, eps=config.norm_eps)\n \n         # Initialize weights and apply final processing\n@@ -637,17 +667,17 @@ def forward(\n         )\n \n         hidden_states = inputs_embeds\n-        position_embeddings = self.pos_emb(hidden_states, position_ids)\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids=position_ids)\n \n         # decoder layers\n         for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n             hidden_states = decoder_layer(\n                 hidden_states,\n                 attention_mask=causal_mask,\n+                position_embeddings=position_embeddings,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n                 cache_position=cache_position,\n-                position_embeddings=position_embeddings,\n                 **kwargs,\n             )\n "
        },
        {
            "sha": "778a5c505dbda7d5ef59ba8de618fadf5a5b7e86",
            "filename": "src/transformers/models/lfm2/modular_lfm2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 7,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodular_lfm2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodular_lfm2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodular_lfm2.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -26,13 +26,13 @@\n from ...utils import TransformersKwargs, logging\n from ...utils.import_utils import is_causal_conv1d_available\n from ..bamba.modeling_bamba import apply_mask_to_padding_states\n+from ..gemma2.modeling_gemma2 import Gemma2RotaryEmbedding\n from ..llama.modeling_llama import (\n     LlamaAttention,\n     LlamaForCausalLM,\n     LlamaModel,\n     LlamaPreTrainedModel,\n     LlamaRMSNorm,\n-    LlamaRotaryEmbedding,\n     apply_rotary_pos_emb,\n     eager_attention_forward,\n )\n@@ -56,7 +56,7 @@ class Lfm2RMSNorm(LlamaRMSNorm):\n     pass\n \n \n-class Lfm2RotaryEmbedding(LlamaRotaryEmbedding):\n+class Lfm2RotaryEmbedding(Gemma2RotaryEmbedding):\n     pass\n \n \n@@ -233,6 +233,7 @@ def forward(\n         attention_mask: Optional[torch.Tensor],\n         past_key_values: Optional[Lfm2HybridConvCache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n         **kwargs,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         input_shape = hidden_states.shape[:-1]\n@@ -391,7 +392,7 @@ def __init__(self, config: Lfm2Config, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Lfm2HybridConvCache] = None,\n@@ -429,10 +430,8 @@ class Lfm2PreTrainedModel(LlamaPreTrainedModel):\n class Lfm2Model(LlamaModel):\n     def __init__(self, config: Lfm2Config):\n         super().__init__(config)\n-        self.pos_emb = Lfm2RotaryEmbedding(config)\n         self.embedding_norm = Lfm2RMSNorm(config.hidden_size, eps=config.norm_eps)\n         del self.norm\n-        del self.rotary_emb\n \n     def forward(\n         self,\n@@ -476,17 +475,17 @@ def forward(\n         )\n \n         hidden_states = inputs_embeds\n-        position_embeddings = self.pos_emb(hidden_states, position_ids)\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids=position_ids)\n \n         # decoder layers\n         for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n             hidden_states = decoder_layer(\n                 hidden_states,\n                 attention_mask=causal_mask,\n+                position_embeddings=position_embeddings,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n                 cache_position=cache_position,\n-                position_embeddings=position_embeddings,\n                 **kwargs,\n             )\n "
        },
        {
            "sha": "f65af16d77b6b58bc4ea72ae8a6365f2d39ce6b2",
            "filename": "src/transformers/models/lfm2_moe/configuration_lfm2_moe.py",
            "status": "modified",
            "additions": 18,
            "deletions": 8,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fconfiguration_lfm2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fconfiguration_lfm2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fconfiguration_lfm2_moe.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -13,18 +13,19 @@\n # limitations under the License.\n from typing import Optional\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n+from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n \n \n-class Lfm2MoeConfig(PretrainedConfig):\n+class Lfm2MoeConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Lfm2MoeModel`]. It is used to instantiate a LFM2 Moe\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of the LFM2-8B-A1B model.\n     e.g. [LiquidAI/LFM2-8B-A1B](https://huggingface.co/LiquidAI/LFM2-8B-A1B)\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:\n@@ -47,8 +48,10 @@ class Lfm2MoeConfig(PretrainedConfig):\n             End of stream token id.\n         tie_word_embeddings (`bool`, *optional*, defaults to `True`):\n             Whether to tie weight embeddings\n-        rope_theta (`float`, *optional*, defaults to 1000000.0):\n-            The base period of the RoPE embeddings.\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         max_position_embeddings (`int`, *optional*, defaults to 128000):\n             The maximum sequence length that this model might ever be used with.\n         use_cache (`bool`, *optional*, defaults to `True`):\n@@ -112,7 +115,7 @@ def __init__(\n         bos_token_id: int = 1,\n         eos_token_id: int = 2,\n         tie_word_embeddings: bool = True,\n-        rope_theta: float = 1000000.0,\n+        rope_parameters: RopeParameters = None,\n         max_position_embeddings: int = 128_000,\n         use_cache: bool = True,\n         norm_eps: float = 0.00001,\n@@ -133,7 +136,9 @@ def __init__(\n         self.hidden_size = hidden_size\n         self.intermediate_size = intermediate_size\n         self.num_hidden_layers = num_hidden_layers\n-        self.rope_theta = rope_theta\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n         self.max_position_embeddings = max_position_embeddings\n         self.use_cache = use_cache\n         self.norm_eps = norm_eps\n@@ -156,6 +161,11 @@ def __init__(\n         self.norm_topk_prob = norm_topk_prob\n         self.layer_types = layer_types\n \n+        # Validate the correctness of rotary position embeddings parameters\n+        rope_theta = kwargs.get(\"theta\", kwargs.get(\"rope_theta\", 1000000.0))\n+        standardize_rope_params(self, rope_theta=rope_theta)\n+        rope_config_validation(self)\n+\n         tie_word_embeddings = kwargs.get(\"tie_embedding\", tie_word_embeddings)  # to fit original config keys\n         super().__init__(\n             pad_token_id=pad_token_id,"
        },
        {
            "sha": "05f2f5389322a2ed5f7d1b2a1d114d06013e3d6e",
            "filename": "src/transformers/models/lfm2_moe/modeling_lfm2_moe.py",
            "status": "modified",
            "additions": 40,
            "deletions": 10,
            "changes": 50,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fmodeling_lfm2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fmodeling_lfm2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fmodeling_lfm2_moe.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -71,20 +71,49 @@ class Lfm2MoeRotaryEmbedding(nn.Module):\n \n     def __init__(self, config: Lfm2MoeConfig, device=None):\n         super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n         self.max_seq_len_cached = config.max_position_embeddings\n         self.original_max_seq_len = config.max_position_embeddings\n \n         self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.rope_type = self.config.rope_parameters[\"rope_type\"]\n+        rope_init_fn: Callable = self.compute_default_rope_parameters\n+        if self.rope_type != \"default\":\n+            rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+        inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n+\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n+        self.original_inv_freq = inv_freq\n+\n+    @staticmethod\n+    def compute_default_rope_parameters(\n+        config: Optional[Lfm2MoeConfig] = None,\n+        device: Optional[\"torch.device\"] = None,\n+        seq_len: Optional[int] = None,\n+    ) -> tuple[\"torch.Tensor\", float]:\n+        \"\"\"\n+        Computes the inverse frequencies according to the original RoPE implementation\n+        Args:\n+            config ([`~transformers.PreTrainedConfig`]):\n+                The model configuration.\n+            device (`torch.device`):\n+                The device to use for initialization of the inverse frequencies.\n+            seq_len (`int`, *optional*):\n+                The current sequence length. Unused for this type of RoPE.\n+        Returns:\n+            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n+            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n+        \"\"\"\n+        base = config.rope_parameters[\"rope_theta\"]\n+        dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+\n+        attention_factor = 1.0  # Unused in this type of RoPE\n+\n+        # Compute the inverse frequencies\n+        inv_freq = 1.0 / (\n+            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n+        )\n+        return inv_freq, attention_factor\n \n     @torch.no_grad()\n     @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n@@ -416,6 +445,7 @@ def forward(\n         attention_mask: Optional[torch.Tensor],\n         past_key_values: Optional[Lfm2MoeHybridConvCache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n         **kwargs,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         input_shape = hidden_states.shape[:-1]\n@@ -593,7 +623,7 @@ def __init__(self, config: Lfm2MoeConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Lfm2MoeHybridConvCache] = None,\n@@ -704,7 +734,7 @@ def forward(\n         )\n \n         hidden_states = inputs_embeds\n-        position_embeddings = self.pos_emb(hidden_states, position_ids)\n+        position_embeddings = self.pos_emb(hidden_states, position_ids=position_ids)\n \n         # decoder layers\n         for decoder_layer in self.layers[: self.config.num_hidden_layers]:"
        },
        {
            "sha": "5d5b1b08d8eaab91b9b94e846426124a0fed2cb5",
            "filename": "src/transformers/models/lfm2_moe/modular_lfm2_moe.py",
            "status": "modified",
            "additions": 11,
            "deletions": 4,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fmodular_lfm2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fmodular_lfm2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fmodular_lfm2_moe.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -21,8 +21,15 @@\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, logging\n from ...utils.import_utils import is_causal_conv1d_available\n-from ..lfm2.modeling_lfm2 import Lfm2Attention, Lfm2DecoderLayer, Lfm2HybridConvCache, Lfm2MLP, Lfm2ShortConv\n-from ..llama.modeling_llama import LlamaForCausalLM, LlamaPreTrainedModel, LlamaRMSNorm, LlamaRotaryEmbedding\n+from ..lfm2.modeling_lfm2 import (\n+    Lfm2Attention,\n+    Lfm2DecoderLayer,\n+    Lfm2HybridConvCache,\n+    Lfm2MLP,\n+    Lfm2RotaryEmbedding,\n+    Lfm2ShortConv,\n+)\n+from ..llama.modeling_llama import LlamaForCausalLM, LlamaPreTrainedModel, LlamaRMSNorm\n from ..mixtral.modeling_mixtral import MixtralModel\n from ..qwen2_moe.modeling_qwen2_moe import Qwen2MoeExperts\n from .configuration_lfm2_moe import Lfm2MoeConfig\n@@ -45,7 +52,7 @@ class Lfm2MoeRMSNorm(LlamaRMSNorm):\n     pass\n \n \n-class Lfm2MoeRotaryEmbedding(LlamaRotaryEmbedding):\n+class Lfm2MoeRotaryEmbedding(Lfm2RotaryEmbedding):\n     pass\n \n \n@@ -175,7 +182,7 @@ def forward(\n         )\n \n         hidden_states = inputs_embeds\n-        position_embeddings = self.pos_emb(hidden_states, position_ids)\n+        position_embeddings = self.pos_emb(hidden_states, position_ids=position_ids)\n \n         # decoder layers\n         for decoder_layer in self.layers[: self.config.num_hidden_layers]:"
        },
        {
            "sha": "1477c2dbc5ab88e7d3654bfc7d7a0fc1d5d03f3a",
            "filename": "src/transformers/models/lightglue/modular_lightglue.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodular_lightglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodular_lightglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodular_lightglue.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -363,6 +363,10 @@ def forward(\n \n \n class LightGlueAttention(LlamaAttention):\n+    def __init__(self, config: LightGlueConfig, layer_idx: int):\n+        super().__init__()\n+        del self.rotary_emb\n+\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "3b2543983e06a03c4dbb58007094a7038c44866f",
            "filename": "src/transformers/models/llama/configuration_llama.py",
            "status": "modified",
            "additions": 34,
            "deletions": 67,
            "changes": 101,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fllama%2Fconfiguration_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fllama%2Fconfiguration_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fconfiguration_llama.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -19,8 +19,10 @@\n # limitations under the License.\n \"\"\"LLaMA model configuration\"\"\"\n \n+from typing import Optional\n+\n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import rope_config_validation\n+from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n \n \n class LlamaConfig(PreTrainedConfig):\n@@ -79,45 +81,10 @@ class LlamaConfig(PreTrainedConfig):\n             results. Please refer to [this issue](https://github.com/pytorch/pytorch/issues/76232).\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie weight embeddings\n-        rope_theta (`float`, *optional*, defaults to 10000.0):\n-            The base period of the RoPE embeddings.\n-        rope_scaling (`Dict`, *optional*):\n-            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n-            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n-            accordingly.\n-            Expected contents:\n-                `rope_type` (`str`):\n-                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n-                    'llama3'], with 'default' being the original RoPE implementation.\n-                `factor` (`float`, *optional*):\n-                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n-                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n-                    original maximum pre-trained length.\n-                `original_max_position_embeddings` (`int`, *optional*):\n-                    Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n-                    pretraining.\n-                `attention_factor` (`float`, *optional*):\n-                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n-                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n-                    `factor` field to infer the suggested value.\n-                `beta_fast` (`float`, *optional*):\n-                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n-                    ramp function. If unspecified, it defaults to 32.\n-                `beta_slow` (`float`, *optional*):\n-                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n-                    ramp function. If unspecified, it defaults to 1.\n-                `short_factor` (`list[float]`, *optional*):\n-                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n-                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n-                    size divided by the number of attention heads divided by 2\n-                `long_factor` (`list[float]`, *optional*):\n-                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n-                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n-                    size divided by the number of attention heads divided by 2\n-                `low_freq_factor` (`float`, *optional*):\n-                    Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n-                `high_freq_factor` (`float`, *optional*):\n-                    Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         attention_bias (`bool`, *optional*, defaults to `False`):\n             Whether to use a bias in the query, key, value and output projection layers during self-attention.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n@@ -160,28 +127,27 @@ class LlamaConfig(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size=32000,\n-        hidden_size=4096,\n-        intermediate_size=11008,\n-        num_hidden_layers=32,\n-        num_attention_heads=32,\n-        num_key_value_heads=None,\n-        hidden_act=\"silu\",\n-        max_position_embeddings=2048,\n-        initializer_range=0.02,\n-        rms_norm_eps=1e-6,\n-        use_cache=True,\n-        pad_token_id=None,\n-        bos_token_id=1,\n-        eos_token_id=2,\n-        pretraining_tp=1,\n-        tie_word_embeddings=False,\n-        rope_theta=10000.0,\n-        rope_scaling=None,\n-        attention_bias=False,\n-        attention_dropout=0.0,\n-        mlp_bias=False,\n-        head_dim=None,\n+        vocab_size: Optional[int] = 32000,\n+        hidden_size: Optional[int] = 4096,\n+        intermediate_size: Optional[int] = 11008,\n+        num_hidden_layers: Optional[int] = 32,\n+        num_attention_heads: Optional[int] = 32,\n+        num_key_value_heads: Optional[int] = None,\n+        hidden_act: Optional[str] = \"silu\",\n+        max_position_embeddings: Optional[int] = 2048,\n+        initializer_range: Optional[float] = 0.02,\n+        rms_norm_eps: Optional[int] = 1e-6,\n+        use_cache: Optional[bool] = True,\n+        pad_token_id: Optional[int] = None,\n+        bos_token_id: Optional[int] = 1,\n+        eos_token_id: Optional[int] = 2,\n+        pretraining_tp: Optional[int] = 1,\n+        tie_word_embeddings: Optional[bool] = False,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        attention_bias: Optional[bool] = False,\n+        attention_dropout: Optional[float] = 0.0,\n+        mlp_bias: Optional[bool] = False,\n+        head_dim: Optional[int] = None,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size\n@@ -201,16 +167,17 @@ def __init__(\n         self.rms_norm_eps = rms_norm_eps\n         self.pretraining_tp = pretraining_tp\n         self.use_cache = use_cache\n-        self.rope_theta = rope_theta\n-        self.rope_scaling = rope_scaling\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n         self.mlp_bias = mlp_bias\n         self.head_dim = head_dim if head_dim is not None else self.hidden_size // self.num_attention_heads\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n+\n         # Validate the correctness of rotary position embeddings parameters\n-        # BC: if there is a 'type' field, copy it it to 'rope_type'.\n-        if self.rope_scaling is not None and \"type\" in self.rope_scaling:\n-            self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n+        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n         rope_config_validation(self)\n \n         super().__init__("
        },
        {
            "sha": "191426910ec1f07583384063eb17da21913e098b",
            "filename": "src/transformers/models/llama/convert_llama_weights_to_hf.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fllama%2Fconvert_llama_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fllama%2Fconvert_llama_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fconvert_llama_weights_to_hf.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -375,15 +375,15 @@ def permute(w, n_heads, dim1=dim, dim2=dim):\n             eos_token_id = 2\n \n         if llama_version in [\"3.1\", \"3.2\", \"Guard-3\"]:\n-            rope_scaling = {\n+            rope_parameters = {\n                 \"factor\": 32.0 if llama_version == \"3.2\" else 8.0,\n                 \"low_freq_factor\": 1.0,\n                 \"high_freq_factor\": 4.0,\n                 \"original_max_position_embeddings\": 8192,\n                 \"rope_type\": \"llama3\",\n             }\n         else:\n-            rope_scaling = None\n+            rope_parameters = None\n \n         config = LlamaConfig(\n             hidden_size=dim,\n@@ -394,7 +394,7 @@ def permute(w, n_heads, dim1=dim, dim2=dim):\n             num_key_value_heads=num_key_value_heads,\n             vocab_size=vocab_size,\n             rope_theta=base,\n-            rope_scaling=rope_scaling,\n+            rope_parameters=rope_parameters,\n             max_position_embeddings=max_position_embeddings,\n             bos_token_id=bos_token_id,\n             eos_token_id=eos_token_id,"
        },
        {
            "sha": "3d8340091bee9229bee961718c3466dc9427d29e",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 42,
            "deletions": 13,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -75,20 +75,49 @@ class LlamaRotaryEmbedding(nn.Module):\n \n     def __init__(self, config: LlamaConfig, device=None):\n         super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n         self.max_seq_len_cached = config.max_position_embeddings\n         self.original_max_seq_len = config.max_position_embeddings\n \n         self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.rope_type = self.config.rope_parameters[\"rope_type\"]\n+        rope_init_fn: Callable = self.compute_default_rope_parameters\n+        if self.rope_type != \"default\":\n+            rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+        inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n+\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n+        self.original_inv_freq = inv_freq\n+\n+    @staticmethod\n+    def compute_default_rope_parameters(\n+        config: Optional[LlamaConfig] = None,\n+        device: Optional[\"torch.device\"] = None,\n+        seq_len: Optional[int] = None,\n+    ) -> tuple[\"torch.Tensor\", float]:\n+        \"\"\"\n+        Computes the inverse frequencies according to the original RoPE implementation\n+        Args:\n+            config ([`~transformers.PreTrainedConfig`]):\n+                The model configuration.\n+            device (`torch.device`):\n+                The device to use for initialization of the inverse frequencies.\n+            seq_len (`int`, *optional*):\n+                The current sequence length. Unused for this type of RoPE.\n+        Returns:\n+            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n+            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n+        \"\"\"\n+        base = config.rope_parameters[\"rope_theta\"]\n+        dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+\n+        attention_factor = 1.0  # Unused in this type of RoPE\n+\n+        # Compute the inverse frequencies\n+        inv_freq = 1.0 / (\n+            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n+        )\n+        return inv_freq, attention_factor\n \n     @torch.no_grad()\n     @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n@@ -223,8 +252,8 @@ def __init__(self, config: LlamaConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n-        attention_mask: Optional[torch.Tensor],\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -283,7 +312,7 @@ def forward(\n         past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> torch.Tensor:\n         residual = hidden_states\n@@ -387,16 +416,16 @@ def forward(\n         )\n \n         hidden_states = inputs_embeds\n-        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids=position_ids)\n \n         for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n             hidden_states = decoder_layer(\n                 hidden_states,\n                 attention_mask=causal_mask,\n+                position_embeddings=position_embeddings,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n                 cache_position=cache_position,\n-                position_embeddings=position_embeddings,\n                 **kwargs,\n             )\n "
        },
        {
            "sha": "7d457cf8523c44a0e433dad92ece09a6b5de6650",
            "filename": "src/transformers/models/llama4/configuration_llama4.py",
            "status": "modified",
            "additions": 43,
            "deletions": 65,
            "changes": 108,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fllama4%2Fconfiguration_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fllama4%2Fconfiguration_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fconfiguration_llama4.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -15,8 +15,10 @@\n # limitations under the License.\n \n import warnings\n+from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig, layer_type_validation\n+from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n from ...utils import logging\n \n \n@@ -66,7 +68,8 @@ class Llama4VisionConfig(PreTrainedConfig):\n         multi_modal_projector_bias (`int`, *optional*, defaults to `False`): TODO\n         projector_dropout (`int`, *optional*, defaults to 0.0): TODO\n         attention_dropout (`int`, *optional*, defaults to 0.0): TODO\n-        rope_theta (`int`, *optional*, defaults to 10000): TODO\n+        rope_parameters (`RopeParameters`, *optional*):\n+            RoPE Parameters\n     \"\"\"\n \n     base_model_tp_plan = {\n@@ -83,25 +86,25 @@ class Llama4VisionConfig(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        hidden_size: int = 768,\n-        hidden_act: str = \"gelu\",\n-        num_hidden_layers: int = 34,\n-        num_attention_heads: int = 16,\n-        num_channels: int = 3,\n-        intermediate_size: int = 5632,\n-        vision_output_dim: int = 7680,\n-        image_size: int = 448,\n-        patch_size: int = 14,\n-        norm_eps: float = 1e-5,\n-        vision_feature_select_strategy=\"default\",\n-        initializer_range: float = 0.02,\n-        pixel_shuffle_ratio=0.5,\n-        projector_input_dim=4096,\n-        projector_output_dim=4096,\n-        multi_modal_projector_bias=False,\n-        projector_dropout=0.0,\n-        attention_dropout=0.0,\n-        rope_theta=10000,\n+        hidden_size: Optional[int] = 768,\n+        hidden_act: Optional[str] = \"gelu\",\n+        num_hidden_layers: Optional[int] = 34,\n+        num_attention_heads: Optional[int] = 16,\n+        num_channels: Optional[int] = 3,\n+        intermediate_size: Optional[int] = 5632,\n+        vision_output_dim: Optional[int] = 7680,\n+        image_size: Optional[int] = 448,\n+        patch_size: Optional[int] = 14,\n+        norm_eps: Optional[float] = 1e-5,\n+        vision_feature_select_strategy: Optional[str] = \"default\",\n+        initializer_range: Optional[float] = 0.02,\n+        pixel_shuffle_ratio: Optional[float] = 0.5,\n+        projector_input_dim: Optional[int] = 4096,\n+        projector_output_dim: Optional[int] = 4096,\n+        multi_modal_projector_bias: Optional[bool] = False,\n+        projector_dropout: Optional[float] = 0.0,\n+        attention_dropout: Optional[float] = 0.0,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n         **kwargs,\n     ):\n         self.hidden_size = hidden_size\n@@ -122,9 +125,14 @@ def __init__(\n         self.projector_dropout = projector_dropout\n         self.attention_dropout = attention_dropout\n         self.vision_feature_select_strategy = vision_feature_select_strategy\n-        self.rope_theta = rope_theta\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n \n-        self._vision_feature_layer = kwargs.get(\"vision_feature_layer\", -1)\n+        # Validate the correctness of rotary position embeddings parameters\n+        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n+        rope_config_validation(self)\n \n         @property\n         def vision_feature_layer(self):\n@@ -187,8 +195,6 @@ class Llama4TextConfig(PreTrainedConfig):\n             The id of the end of sentence token.\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie weight embeddings\n-        rope_theta (`float`, *optional*, defaults to `500000.0`):\n-            The base period of the RoPE embeddings.\n         attention_dropout (`int`, *optional*, defaults to 0.0): TODO\n         num_experts_per_tok (`int`, *optional*, defaults to 1): TODO\n         num_local_experts (`int`, *optional*, defaults to 16): TODO\n@@ -198,43 +204,10 @@ class Llama4TextConfig(PreTrainedConfig):\n         output_router_logits (`int`, *optional*, defaults to `False`): TODO\n         router_aux_loss_coef (`int`, *optional*, defaults to 0.001): TODO\n         router_jitter_noise (`int`, *optional*, defaults to 0.0): TODO\n-        rope_scaling (`Dict`, *optional*):\n-            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n-            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n-            accordingly.\n-            Expected contents:\n-                `rope_type` (`str`):\n-                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n-                    'llama3'], with 'default' being the original RoPE implementation.\n-                `factor` (`float`, *optional*):\n-                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n-                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n-                    original maximum pre-trained length.\n-                `original_max_position_embeddings` (`int`, *optional*):\n-                    Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n-                    pretraining.\n-                `attention_factor` (`float`, *optional*):\n-                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n-                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n-                    `factor` field to infer the suggested value.\n-                `beta_fast` (`float`, *optional*):\n-                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n-                    ramp function. If unspecified, it defaults to 32.\n-                `beta_slow` (`float`, *optional*):\n-                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n-                    ramp function. If unspecified, it defaults to 1.\n-                `short_factor` (`list[float]`, *optional*):\n-                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n-                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n-                    size divided by the number of attention heads divided by 2\n-                `long_factor` (`list[float]`, *optional*):\n-                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n-                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n-                    size divided by the number of attention heads divided by 2\n-                `low_freq_factor` (`float`, *optional*):\n-                    Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n-                `high_freq_factor` (`float`, *optional*):\n-                    Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n             <TODO>\n             <TODO>\n         no_rope_layers (`list[int]`, *optional*):\n@@ -308,7 +281,6 @@ def __init__(\n         bos_token_id=1,\n         eos_token_id=2,\n         tie_word_embeddings=False,\n-        rope_theta=500000,\n         attention_dropout=0.0,\n         num_experts_per_tok=1,\n         num_local_experts=16,\n@@ -318,7 +290,7 @@ def __init__(\n         output_router_logits=False,\n         router_aux_loss_coef=0.001,\n         router_jitter_noise=0.0,\n-        rope_scaling=None,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n         no_rope_layers=None,\n         no_rope_layer_interval=4,\n         attention_chunk_size=8192,\n@@ -345,7 +317,6 @@ def __init__(\n         self.intermediate_size_mlp = intermediate_size_mlp\n         self.num_hidden_layers = num_hidden_layers\n         self.num_attention_heads = num_attention_heads\n-        self.rope_scaling = rope_scaling\n         self.attention_bias = False\n         # for backward compatibility\n         if num_key_value_heads is None:\n@@ -356,10 +327,12 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.rms_norm_eps = rms_norm_eps\n         self.use_cache = use_cache\n-        self.rope_theta = rope_theta\n         self.attention_dropout = attention_dropout\n         self.head_dim = head_dim if head_dim is not None else self.hidden_size // self.num_attention_heads\n         self.use_qk_norm = use_qk_norm\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n \n         self.num_experts_per_tok = num_experts_per_tok\n         self.num_local_experts = num_local_experts\n@@ -393,6 +366,11 @@ def __init__(\n             ]\n         layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n+        # Validate the correctness of rotary position embeddings parameters\n+        rope_theta = kwargs.get(\"rope_theta\", 500000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n+        rope_config_validation(self)\n+\n \n class Llama4Config(PreTrainedConfig):\n     r\"\"\""
        },
        {
            "sha": "1cba9c678584d5755b0be0175b955ee2ebe818ee",
            "filename": "src/transformers/models/llama4/convert_llama4_weights_to_hf.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fllama4%2Fconvert_llama4_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fllama4%2Fconvert_llama4_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fconvert_llama4_weights_to_hf.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -243,14 +243,14 @@ def write_model(\n     config_kwargs = {}\n     if params[\"use_scaled_rope\"]:\n         # some constants from original code\n-        rope_scaling = {\n+        rope_parameters = {\n             \"rope_type\": \"llama3\",\n-            \"factor\": params.get(\"rope_scaling_factor\", 8.0),\n+            \"factor\": params.get(\"rope_parameters_factor\", 8.0),\n             \"low_freq_factor\": 1.0,\n             \"high_freq_factor\": params.get(\"rope_high_freq_factor\", 4.0),\n             \"original_max_position_embeddings\": 8192,\n         }\n-        config_kwargs.update({\"rope_scaling\": rope_scaling})\n+        config_kwargs.update({\"rope_parameters\": rope_parameters})\n \n     if attention_chunk_size is None:\n         config_kwargs.update({\"cache_implementation\": \"static\"})"
        },
        {
            "sha": "6b012a5b096afa180080ab9060729f0668862d90",
            "filename": "src/transformers/models/llama4/modeling_llama4.py",
            "status": "modified",
            "additions": 45,
            "deletions": 8,
            "changes": 53,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -32,7 +32,10 @@\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPast, CausalLMOutputWithPast, ModelOutput\n-from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n+from ...modeling_rope_utils import (\n+    ROPE_INIT_FUNCTIONS,\n+    dynamic_rope_update,\n+)\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n@@ -166,24 +169,58 @@ def forward(self, hidden_states):\n         return out, router_logits\n \n \n+# Copied from transformers.models.llama.modeling_llama.LlamaRotaryEmbedding with Llama->Llama4Text\n class Llama4TextRotaryEmbedding(nn.Module):\n     inv_freq: torch.Tensor  # fix linting for `register_buffer`\n \n+    # Ignore copy\n     def __init__(self, config: Llama4TextConfig, device=None):\n         super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        self.rope_type = \"llama3\" if config.rope_scaling is not None else \"default\"\n-\n         self.max_seq_len_cached = config.max_position_embeddings\n         self.original_max_seq_len = config.max_position_embeddings\n \n         self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.rope_type = self.config.rope_parameters[\"rope_type\"]\n+        rope_init_fn: Callable = self.compute_default_rope_parameters\n+        if self.rope_type != \"default\":\n+            rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+        inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n+\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n+        self.original_inv_freq = inv_freq\n+\n+    @staticmethod\n+    def compute_default_rope_parameters(\n+        config: Optional[Llama4TextConfig] = None,\n+        device: Optional[\"torch.device\"] = None,\n+        seq_len: Optional[int] = None,\n+    ) -> tuple[\"torch.Tensor\", float]:\n+        \"\"\"\n+        Computes the inverse frequencies according to the original RoPE implementation\n+        Args:\n+            config ([`~transformers.PreTrainedConfig`]):\n+                The model configuration.\n+            device (`torch.device`):\n+                The device to use for initialization of the inverse frequencies.\n+            seq_len (`int`, *optional*):\n+                The current sequence length. Unused for this type of RoPE.\n+        Returns:\n+            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n+            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n+        \"\"\"\n+        base = config.rope_parameters[\"rope_theta\"]\n+        dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+\n+        attention_factor = 1.0  # Unused in this type of RoPE\n+\n+        # Compute the inverse frequencies\n+        inv_freq = 1.0 / (\n+            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n+        )\n+        return inv_freq, attention_factor\n \n+    # Ignore copy\n     @torch.no_grad()\n     @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n     def forward(self, x, position_ids):\n@@ -394,7 +431,7 @@ def forward(\n         past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         residual = hidden_states"
        },
        {
            "sha": "7933cb5bb0dc516aafc97c8f93e6a713db5cd8fd",
            "filename": "src/transformers/models/longcat_flash/configuration_longcat_flash.py",
            "status": "modified",
            "additions": 46,
            "deletions": 45,
            "changes": 91,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fconfiguration_longcat_flash.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fconfiguration_longcat_flash.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fconfiguration_longcat_flash.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -15,8 +15,10 @@\n \n \"\"\"LongCat Flash model configuration\"\"\"\n \n+from typing import Optional\n+\n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import rope_config_validation\n+from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n \n \n class LongcatFlashConfig(PreTrainedConfig):\n@@ -69,12 +71,11 @@ class LongcatFlashConfig(PreTrainedConfig):\n             End of stream token id.\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie input and output embeddings.\n-        rope_theta (`float`, *optional*, defaults to 10000000.0):\n-            The base period of the RoPE embeddings.\n-        rope_scaling (`Dict`, *optional*):\n+        rope_parameters (`RopeParameters`, *optional*):\n             Dictionary containing the scaling configuration for the RoPE embeddings. Currently supports two scaling\n             strategies: linear and dynamic. Their scaling factor must be a float greater than 1. The expected format is\n-            `{\"type\": strategy name, \"factor\": scaling factor}`.\n+            `{\"type\": strategy name, \"factor\": scaling factor}`. When using this flag, don't update\n+            `max_position_embeddings` to the expected new maximum.\n         attention_bias (`bool`, *optional*, defaults to `False`):\n             Whether to use a bias in the query, key, value and output projection layers during self-attention.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n@@ -141,38 +142,37 @@ class LongcatFlashConfig(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size=131072,\n-        hidden_size=6144,\n-        num_hidden_layers=56,\n-        num_layers=28,\n-        num_attention_heads=64,\n-        num_key_value_heads=None,\n-        hidden_act=\"silu\",\n-        max_position_embeddings=131072,\n-        initializer_range=0.02,\n-        rms_norm_eps=1e-5,\n-        use_cache=True,\n-        pad_token_id=None,\n-        bos_token_id=1,\n-        eos_token_id=2,\n-        tie_word_embeddings=False,\n-        rope_theta=10000000.0,\n-        rope_scaling=None,\n-        attention_bias=False,\n-        attention_dropout=0.0,\n-        ffn_hidden_size=12288,\n-        q_lora_rank=1536,\n-        kv_lora_rank=512,\n-        qk_nope_head_dim=128,\n-        qk_rope_head_dim=64,\n-        head_dim=64,\n-        v_head_dim=128,\n-        qk_head_dim=None,\n-        moe_topk=12,\n-        n_routed_experts=512,\n-        zero_expert_num=256,\n-        expert_ffn_hidden_size=2048,\n-        routed_scaling_factor=6.0,\n+        vocab_size: Optional[int] = 131072,\n+        hidden_size: Optional[int] = 6144,\n+        num_hidden_layers: Optional[int] = 56,\n+        num_layers: Optional[int] = 28,\n+        num_attention_heads: Optional[int] = 64,\n+        num_key_value_heads: Optional[int] = None,\n+        hidden_act: Optional[str] = \"silu\",\n+        max_position_embeddings: Optional[int] = 131072,\n+        initializer_range: Optional[float] = 0.02,\n+        rms_norm_eps: Optional[float] = 1e-5,\n+        use_cache: Optional[bool] = True,\n+        pad_token_id: Optional[int] = None,\n+        bos_token_id: Optional[int] = 1,\n+        eos_token_id: Optional[int] = 2,\n+        tie_word_embeddings: Optional[bool] = False,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        attention_bias: Optional[bool] = False,\n+        attention_dropout: Optional[float] = 0.0,\n+        ffn_hidden_size: Optional[int] = 12288,\n+        q_lora_rank: Optional[int] = 1536,\n+        kv_lora_rank: Optional[int] = 512,\n+        qk_nope_head_dim: Optional[int] = 128,\n+        qk_rope_head_dim: Optional[int] = 64,\n+        head_dim: Optional[int] = 64,\n+        v_head_dim: Optional[int] = 128,\n+        qk_head_dim: Optional[int] = None,\n+        moe_topk: Optional[int] = 12,\n+        n_routed_experts: Optional[int] = 512,\n+        zero_expert_num: Optional[int] = 256,\n+        expert_ffn_hidden_size: Optional[int] = 2048,\n+        routed_scaling_factor: Optional[float] = 6.0,\n         **kwargs,\n     ):\n         if num_key_value_heads is None:\n@@ -192,8 +192,6 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.rms_norm_eps = rms_norm_eps\n         self.use_cache = use_cache\n-        self.rope_theta = rope_theta\n-        self.rope_scaling = rope_scaling\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n \n@@ -212,14 +210,17 @@ def __init__(\n         self.zero_expert_num = zero_expert_num\n         self.expert_ffn_hidden_size = expert_ffn_hidden_size\n         self.routed_scaling_factor = routed_scaling_factor\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n \n-        if self.rope_scaling is not None and \"type\" in self.rope_scaling:\n-            self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n+        # Validate the correctness of rotary position embeddings parameters\n+        rope_theta = kwargs.get(\"rope_theta\", 10000000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n \n-        if self.rope_scaling is not None:\n-            for key in [\"beta_fast\", \"beta_slow\", \"factor\"]:\n-                if key in self.rope_scaling:\n-                    self.rope_scaling[key] = float(self.rope_scaling[key])\n+        for key in [\"beta_fast\", \"beta_slow\", \"factor\"]:\n+            if key in self.rope_parameters:\n+                self.rope_parameters[key] = float(self.rope_parameters[key])\n \n         rope_config_validation(self)\n "
        },
        {
            "sha": "c082eb43ee4de5b02adb9273539fde1cd11d530a",
            "filename": "src/transformers/models/longcat_flash/modeling_longcat_flash.py",
            "status": "modified",
            "additions": 41,
            "deletions": 12,
            "changes": 53,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fmodeling_longcat_flash.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fmodeling_longcat_flash.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fmodeling_longcat_flash.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -69,20 +69,49 @@ class LongcatFlashRotaryEmbedding(nn.Module):\n \n     def __init__(self, config: LongcatFlashConfig, device=None):\n         super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n         self.max_seq_len_cached = config.max_position_embeddings\n         self.original_max_seq_len = config.max_position_embeddings\n \n         self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.rope_type = self.config.rope_parameters[\"rope_type\"]\n+        rope_init_fn: Callable = self.compute_default_rope_parameters\n+        if self.rope_type != \"default\":\n+            rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+        inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n+\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n+        self.original_inv_freq = inv_freq\n+\n+    @staticmethod\n+    def compute_default_rope_parameters(\n+        config: Optional[LongcatFlashConfig] = None,\n+        device: Optional[\"torch.device\"] = None,\n+        seq_len: Optional[int] = None,\n+    ) -> tuple[\"torch.Tensor\", float]:\n+        \"\"\"\n+        Computes the inverse frequencies according to the original RoPE implementation\n+        Args:\n+            config ([`~transformers.PreTrainedConfig`]):\n+                The model configuration.\n+            device (`torch.device`):\n+                The device to use for initialization of the inverse frequencies.\n+            seq_len (`int`, *optional*):\n+                The current sequence length. Unused for this type of RoPE.\n+        Returns:\n+            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n+            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n+        \"\"\"\n+        base = config.rope_parameters[\"rope_theta\"]\n+        dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+\n+        attention_factor = 1.0  # Unused in this type of RoPE\n+\n+        # Compute the inverse frequencies\n+        inv_freq = 1.0 / (\n+            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n+        )\n+        return inv_freq, attention_factor\n \n     @torch.no_grad()\n     @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n@@ -291,7 +320,7 @@ def __init__(self, config, layer_idx: int):\n         self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n         self.attention_dropout = config.attention_dropout\n         self.num_heads = config.num_attention_heads\n-        self.rope_theta = config.rope_theta\n+\n         self.q_lora_rank = config.q_lora_rank\n         self.qk_rope_head_dim = config.qk_rope_head_dim\n         self.kv_lora_rank = config.kv_lora_rank\n@@ -326,9 +355,9 @@ def __init__(self, config, layer_idx: int):\n         )\n \n         self.scaling = self.qk_head_dim ** (-0.5)\n-        if self.config.rope_scaling is not None:\n-            mscale_all_dim = self.config.rope_scaling.get(\"mscale_all_dim\", 0)\n-            scaling_factor = self.config.rope_scaling[\"factor\"]\n+        if self.config.rope_parameters.get(\"rope_type\", \"default\") != \"default\":\n+            mscale_all_dim = self.config.rope_parameters.get(\"mscale_all_dim\", 0)\n+            scaling_factor = self.config.rope_parameters[\"factor\"]\n             if mscale_all_dim:\n                 mscale = yarn_get_mscale(scaling_factor, mscale_all_dim)\n                 self.scaling = self.scaling * mscale * mscale"
        },
        {
            "sha": "7332212730160c083f2d1df16568871a75bc18c6",
            "filename": "src/transformers/models/mimi/configuration_mimi.py",
            "status": "modified",
            "additions": 52,
            "deletions": 41,
            "changes": 93,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fmimi%2Fconfiguration_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fmimi%2Fconfiguration_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmimi%2Fconfiguration_mimi.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -15,10 +15,12 @@\n \"\"\"Mimi model configuration\"\"\"\n \n import math\n+from typing import Optional\n \n import numpy as np\n \n from ...configuration_utils import PreTrainedConfig\n+from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n from ...utils import logging\n \n \n@@ -113,8 +115,10 @@ class MimiConfig(PreTrainedConfig):\n             relevant if `config.is_decoder=True`.\n         use_streaming (`bool`, *optional*, defaults to `False`):\n             Whether to use streaming mode. If `True`, the model encode method will return the padding cache that can be used in a subsequent call to the encode method.\n-        rope_theta (`float`, *optional*, defaults to 10000.0):\n-            The base period of the RoPE embeddings.\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         sliding_window (`int`, *optional*, defaults to 250):\n             Sliding window attention window size. If not specified, will default to `250`.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n@@ -142,44 +146,44 @@ class MimiConfig(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        sampling_rate=24_000,\n-        frame_rate=None,\n-        audio_channels=1,\n-        hidden_size=512,\n-        num_filters=64,\n-        num_residual_layers=1,\n-        upsampling_ratios=None,\n-        kernel_size=7,\n-        last_kernel_size=3,\n-        residual_kernel_size=3,\n-        dilation_growth_rate=2,\n-        use_causal_conv=True,\n-        pad_mode=\"constant\",\n-        compress=2,\n-        trim_right_ratio=1.0,\n-        codebook_size=2048,\n-        codebook_dim=256,\n-        num_quantizers=32,\n-        use_conv_shortcut=False,\n-        vector_quantization_hidden_dimension=256,\n-        num_semantic_quantizers=1,\n-        upsample_groups=512,\n-        num_hidden_layers=8,\n-        intermediate_size=2048,\n-        num_attention_heads=8,\n-        num_key_value_heads=8,\n-        head_dim=None,\n-        hidden_act=\"gelu\",\n-        max_position_embeddings=8000,\n-        initializer_range=0.02,\n-        norm_eps=1e-5,\n-        use_cache=False,\n-        use_streaming=False,\n-        rope_theta=10000.0,\n-        sliding_window=250,\n-        attention_dropout=0.0,\n-        layer_scale_initial_scale=0.01,\n-        attention_bias=False,\n+        sampling_rate: Optional[int] = 24_000,\n+        frame_rate: Optional[int] = None,\n+        audio_channels: Optional[int] = 1,\n+        hidden_size: Optional[int] = 512,\n+        num_filters: Optional[int] = 64,\n+        num_residual_layers: Optional[int] = 1,\n+        upsampling_ratios: Optional[list[int]] = None,\n+        kernel_size: Optional[int] = 7,\n+        last_kernel_size: Optional[int] = 3,\n+        residual_kernel_size: Optional[int] = 3,\n+        dilation_growth_rate: Optional[int] = 2,\n+        use_causal_conv: Optional[bool] = True,\n+        pad_mode: Optional[str] = \"constant\",\n+        compress: Optional[int] = 2,\n+        trim_right_ratio: Optional[float] = 1.0,\n+        codebook_size: Optional[int] = 2048,\n+        codebook_dim: Optional[int] = 256,\n+        num_quantizers: Optional[int] = 32,\n+        use_conv_shortcut: Optional[bool] = False,\n+        vector_quantization_hidden_dimension: Optional[int] = 256,\n+        num_semantic_quantizers: Optional[int] = 1,\n+        upsample_groups: Optional[int] = 512,\n+        num_hidden_layers: Optional[int] = 8,\n+        intermediate_size: Optional[int] = 2048,\n+        num_attention_heads: Optional[int] = 8,\n+        num_key_value_heads: Optional[int] = 8,\n+        head_dim: Optional[int] = None,\n+        hidden_act: Optional[str] = \"gelu\",\n+        max_position_embeddings: Optional[int] = 8000,\n+        initializer_range: Optional[float] = 0.02,\n+        norm_eps: Optional[int] = 1e-5,\n+        use_cache: Optional[bool] = False,\n+        use_streaming: Optional[bool] = False,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        sliding_window: Optional[int] = 250,\n+        attention_dropout: Optional[float] = 0.0,\n+        layer_scale_initial_scale: Optional[float] = 0.01,\n+        attention_bias: Optional[bool] = False,\n         **kwargs,\n     ):\n         self.sampling_rate = sampling_rate\n@@ -212,12 +216,19 @@ def __init__(\n         self.norm_eps = norm_eps\n         self.use_cache = use_cache\n         self.use_streaming = use_streaming\n-        self.rope_theta = rope_theta\n         self.sliding_window = sliding_window\n         self.attention_dropout = attention_dropout\n         self.head_dim = head_dim or hidden_size // num_attention_heads\n         self.layer_scale_initial_scale = layer_scale_initial_scale\n         self.attention_bias = attention_bias\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n+\n+        # Validate the correctness of rotary position embeddings parameters\n+        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n+        rope_config_validation(self)\n \n         # Handle backward compatibility for frame_rate:\n         # If frame_rate is explicitly provided, use it (backward compatibility)"
        },
        {
            "sha": "83bcbd857a0dc857780d6d82b2b1910beab3846f",
            "filename": "src/transformers/models/mimi/modeling_mimi.py",
            "status": "modified",
            "additions": 40,
            "deletions": 10,
            "changes": 50,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -15,6 +15,7 @@\n \"\"\"PyTorch Mimi model.\"\"\"\n \n import math\n+from collections.abc import Callable\n from dataclasses import dataclass\n from typing import Optional, Union\n \n@@ -500,26 +501,55 @@ def forward(self, x: torch.Tensor):\n         return self.scale * x\n \n \n-# Copied from transformers.models.mistral.modeling_mistral.MistralRotaryEmbedding with Mistral->Mimi\n+# Copied from transformers.models.llama.modeling_llama.LlamaRotaryEmbedding with Llama->Mimi\n class MimiRotaryEmbedding(nn.Module):\n     inv_freq: torch.Tensor  # fix linting for `register_buffer`\n \n     def __init__(self, config: MimiConfig, device=None):\n         super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n         self.max_seq_len_cached = config.max_position_embeddings\n         self.original_max_seq_len = config.max_position_embeddings\n \n         self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.rope_type = self.config.rope_parameters[\"rope_type\"]\n+        rope_init_fn: Callable = self.compute_default_rope_parameters\n+        if self.rope_type != \"default\":\n+            rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+        inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n+\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n+        self.original_inv_freq = inv_freq\n+\n+    @staticmethod\n+    def compute_default_rope_parameters(\n+        config: Optional[MimiConfig] = None,\n+        device: Optional[\"torch.device\"] = None,\n+        seq_len: Optional[int] = None,\n+    ) -> tuple[\"torch.Tensor\", float]:\n+        \"\"\"\n+        Computes the inverse frequencies according to the original RoPE implementation\n+        Args:\n+            config ([`~transformers.PreTrainedConfig`]):\n+                The model configuration.\n+            device (`torch.device`):\n+                The device to use for initialization of the inverse frequencies.\n+            seq_len (`int`, *optional*):\n+                The current sequence length. Unused for this type of RoPE.\n+        Returns:\n+            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n+            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n+        \"\"\"\n+        base = config.rope_parameters[\"rope_theta\"]\n+        dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+\n+        attention_factor = 1.0  # Unused in this type of RoPE\n+\n+        # Compute the inverse frequencies\n+        inv_freq = 1.0 / (\n+            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n+        )\n+        return inv_freq, attention_factor\n \n     @torch.no_grad()\n     @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n@@ -625,7 +655,7 @@ def __init__(self, config: MimiConfig, layer_idx: Optional[int] = None):\n         self.num_key_value_heads = config.num_key_value_heads\n         self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n         self.max_position_embeddings = config.max_position_embeddings\n-        self.rope_theta = config.rope_theta\n+\n         self.is_causal = True\n         self.scaling = 1 / math.sqrt(config.head_dim)\n "
        },
        {
            "sha": "8c4737cc5b67303504d407d0af8f07dbd9b45adb",
            "filename": "src/transformers/models/minimax/configuration_minimax.py",
            "status": "modified",
            "additions": 63,
            "deletions": 48,
            "changes": 111,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fminimax%2Fconfiguration_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fminimax%2Fconfiguration_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fminimax%2Fconfiguration_minimax.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -19,7 +19,11 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n+\n+from typing import Optional\n+\n from ...configuration_utils import PreTrainedConfig, layer_type_validation\n+from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n \n \n class MiniMaxConfig(PreTrainedConfig):\n@@ -75,8 +79,6 @@ class MiniMaxConfig(PreTrainedConfig):\n             The id of the \"end-of-sequence\" token.\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether the model's input and output word embeddings should be tied.\n-        rope_theta (`float`, *optional*, defaults to 1000000.0):\n-            The base period of the RoPE embeddings.\n         sliding_window (`int`, *optional*):\n             Sliding window attention window size. If not specified, will default to `4096`.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n@@ -93,6 +95,10 @@ class MiniMaxConfig(PreTrainedConfig):\n             The aux loss factor for the total loss.\n         router_jitter_noise (`float`, *optional*, defaults to 0.0):\n             Amount of noise to add to the router.\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         layer_types (`list`, *optional*):\n             Attention pattern for each layer.\n         block_size (`int`, *optional*, defaults to 256):\n@@ -147,48 +153,40 @@ class MiniMaxConfig(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size=32000,\n-        hidden_size=4096,\n-        intermediate_size=14336,\n-        num_hidden_layers=32,\n-        num_attention_heads=32,\n-        num_key_value_heads=8,\n-        head_dim=None,\n-        hidden_act=\"silu\",\n-        max_position_embeddings=4096 * 32,\n-        initializer_range=0.02,\n-        rms_norm_eps=1e-5,\n-        use_cache=True,\n-        pad_token_id=None,\n-        bos_token_id=1,\n-        eos_token_id=2,\n-        tie_word_embeddings=False,\n-        rope_theta=1e6,\n-        sliding_window=None,\n-        attention_dropout=0.0,\n-        num_experts_per_tok=2,\n-        num_local_experts=8,\n-        output_router_logits=False,\n-        router_aux_loss_coef=0.001,\n-        router_jitter_noise=0.0,\n-        layer_types=None,\n-        block_size=256,\n-        full_attn_alpha_factor=1,\n-        full_attn_beta_factor=1,\n-        linear_attn_alpha_factor=1,\n-        linear_attn_beta_factor=1,\n-        mlp_alpha_factor=1,\n-        mlp_beta_factor=1,\n+        vocab_size: Optional[int] = 32000,\n+        hidden_size: Optional[int] = 4096,\n+        intermediate_size: Optional[int] = 14336,\n+        num_hidden_layers: Optional[int] = 32,\n+        num_attention_heads: Optional[int] = 32,\n+        num_key_value_heads: Optional[int] = 8,\n+        head_dim: Optional[int] = None,\n+        hidden_act: Optional[str] = \"silu\",\n+        max_position_embeddings: Optional[int] = 4096 * 32,\n+        initializer_range: Optional[float] = 0.02,\n+        rms_norm_eps: Optional[int] = 1e-5,\n+        use_cache: Optional[bool] = True,\n+        pad_token_id: Optional[int] = None,\n+        bos_token_id: Optional[int] = 1,\n+        eos_token_id: Optional[int] = 2,\n+        tie_word_embeddings: Optional[bool] = False,\n+        sliding_window: Optional[int] = None,\n+        attention_dropout: Optional[float] = 0.0,\n+        num_experts_per_tok: Optional[int] = 2,\n+        num_local_experts: Optional[int] = 8,\n+        output_router_logits: Optional[bool] = False,\n+        router_aux_loss_coef: Optional[float] = 0.001,\n+        router_jitter_noise: Optional[float] = 0.0,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        layer_types: Optional[list[str]] = None,\n+        block_size: Optional[int] = 256,\n+        full_attn_alpha_factor: Optional[int] = 1,\n+        full_attn_beta_factor: Optional[int] = 1,\n+        linear_attn_alpha_factor: Optional[int] = 1,\n+        linear_attn_beta_factor: Optional[int] = 1,\n+        mlp_alpha_factor: Optional[int] = 1,\n+        mlp_beta_factor: Optional[int] = 1,\n         **kwargs,\n     ):\n-        self.layer_types = layer_types\n-        self.block_size = block_size\n-        self.full_attn_alpha_factor = full_attn_alpha_factor\n-        self.full_attn_beta_factor = full_attn_beta_factor\n-        self.linear_attn_alpha_factor = linear_attn_alpha_factor\n-        self.linear_attn_beta_factor = linear_attn_beta_factor\n-        self.mlp_alpha_factor = mlp_alpha_factor\n-        self.mlp_beta_factor = mlp_beta_factor\n         self.vocab_size = vocab_size\n         self.max_position_embeddings = max_position_embeddings\n         self.hidden_size = hidden_size\n@@ -206,7 +204,6 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.rms_norm_eps = rms_norm_eps\n         self.use_cache = use_cache\n-        self.rope_theta = rope_theta\n         self.attention_dropout = attention_dropout\n         self.head_dim = head_dim\n \n@@ -215,18 +212,36 @@ def __init__(\n         self.output_router_logits = output_router_logits\n         self.router_aux_loss_coef = router_aux_loss_coef\n         self.router_jitter_noise = router_jitter_noise\n+\n+        self.layer_types = layer_types\n+        self.block_size = block_size\n+        self.full_attn_alpha_factor = full_attn_alpha_factor\n+        self.full_attn_beta_factor = full_attn_beta_factor\n+        self.linear_attn_alpha_factor = linear_attn_alpha_factor\n+        self.linear_attn_beta_factor = linear_attn_beta_factor\n+        self.mlp_alpha_factor = mlp_alpha_factor\n+        self.mlp_beta_factor = mlp_beta_factor\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n+\n+        if self.layer_types is None:\n+            self.layer_types = [\n+                \"full_attention\" if bool((i + 1) % 2) else \"linear_attention\" for i in range(self.num_hidden_layers)\n+            ]\n+        layer_type_validation(self.layer_types, self.num_hidden_layers)\n+\n+        # Validate the correctness of rotary position embeddings parameters\n+        rope_theta = getattr(self, \"rope_theta\", 1000000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n+        rope_config_validation(self)\n         super().__init__(\n             pad_token_id=pad_token_id,\n             bos_token_id=bos_token_id,\n             eos_token_id=eos_token_id,\n             tie_word_embeddings=tie_word_embeddings,\n             **kwargs,\n         )\n-        if self.layer_types is None:\n-            self.layer_types = [\n-                \"full_attention\" if bool((i + 1) % 2) else \"linear_attention\" for i in range(self.num_hidden_layers)\n-            ]\n-        layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n \n __all__ = [\"MiniMaxConfig\"]"
        },
        {
            "sha": "84573a213a9f4288a26e0bbf4c4e157afa080657",
            "filename": "src/transformers/models/minimax/modeling_minimax.py",
            "status": "modified",
            "additions": 68,
            "deletions": 41,
            "changes": 109,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -253,6 +253,71 @@ def forward(\n         return attn_output, attn_weights_inter\n \n \n+class MiniMaxRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n+    def __init__(self, config: MiniMaxConfig, device=None):\n+        super().__init__()\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+\n+        self.rope_type = self.config.rope_parameters[\"rope_type\"]\n+        rope_init_fn: Callable = self.compute_default_rope_parameters\n+        if self.rope_type != \"default\":\n+            rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+        inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n+\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = inv_freq\n+\n+    @staticmethod\n+    def compute_default_rope_parameters(\n+        config: Optional[MiniMaxConfig] = None,\n+        device: Optional[\"torch.device\"] = None,\n+        seq_len: Optional[int] = None,\n+    ) -> tuple[\"torch.Tensor\", float]:\n+        \"\"\"\n+        Computes the inverse frequencies according to the original RoPE implementation\n+        Args:\n+            config ([`~transformers.PreTrainedConfig`]):\n+                The model configuration.\n+            device (`torch.device`):\n+                The device to use for initialization of the inverse frequencies.\n+            seq_len (`int`, *optional*):\n+                The current sequence length. Unused for this type of RoPE.\n+        Returns:\n+            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n+            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n+        \"\"\"\n+        base = config.rope_parameters[\"rope_theta\"]\n+        dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+\n+        attention_factor = 1.0  # Unused in this type of RoPE\n+\n+        # Compute the inverse frequencies\n+        inv_freq = 1.0 / (\n+            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n+        )\n+        return inv_freq, attention_factor\n+\n+    @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n+    def forward(self, x, position_ids):\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n def rotate_half(x):\n     \"\"\"Rotates half the hidden dims of the input.\"\"\"\n     x1 = x[..., : x.shape[-1] // 2]\n@@ -478,7 +543,7 @@ def __init__(self, config: MiniMaxConfig, layer_idx: int):\n         self.post_attention_layernorm = MiniMaxRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n         self.layer_idx = layer_idx\n-        self.layer_type = config.layer_types[layer_idx]\n+        self.layer_type = config.layer_types[layer_idx] if hasattr(config, \"layer_types\") else None\n         self.mlp_alpha_factor = config.mlp_alpha_factor\n         self.mlp_beta_factor = config.mlp_beta_factor\n \n@@ -494,7 +559,7 @@ def __init__(self, config: MiniMaxConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -542,42 +607,6 @@ class MiniMaxPreTrainedModel(PreTrainedModel):\n     }\n \n \n-class MiniMaxRotaryEmbedding(nn.Module):\n-    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n-\n-    def __init__(self, config: MiniMaxConfig, device=None):\n-        super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n-        self.max_seq_len_cached = config.max_position_embeddings\n-        self.original_max_seq_len = config.max_position_embeddings\n-\n-        self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n-\n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n-        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n-\n-    @torch.no_grad()\n-    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n-    def forward(self, x, position_ids):\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n-        position_ids_expanded = position_ids[:, None, :].float()\n-\n-        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n-            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos() * self.attention_scaling\n-            sin = emb.sin() * self.attention_scaling\n-\n-        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n-\n-\n @auto_docstring\n class MiniMaxModel(MiniMaxPreTrainedModel):\n     def __init__(self, config: MiniMaxConfig):\n@@ -640,8 +669,6 @@ def forward(\n         )\n \n         hidden_states = inputs_embeds\n-\n-        # create position embeddings to be shared across the decoder layers\n         position_embeddings = self.rotary_emb(hidden_states, position_ids)\n \n         for decoder_layer in self.layers:\n@@ -653,8 +680,8 @@ def forward(\n \n             hidden_states = decoder_layer(\n                 hidden_states,\n-                position_embeddings=position_embeddings,\n                 attention_mask=input_attention_mask,\n+                position_embeddings=position_embeddings,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n                 use_cache=use_cache,"
        },
        {
            "sha": "50a42c9d5cec0dee43d301ecd9abbbc91f224572",
            "filename": "src/transformers/models/minimax/modular_minimax.py",
            "status": "modified",
            "additions": 110,
            "deletions": 20,
            "changes": 130,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodular_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodular_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodular_minimax.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -23,15 +23,16 @@\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n-from ...configuration_utils import layer_type_validation\n+from ...configuration_utils import PreTrainedConfig, layer_type_validation\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import MoeModelOutputWithPast\n+from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, logging\n from ...utils.generic import OutputRecorder, check_model_inputs\n-from ..mixtral.configuration_mixtral import MixtralConfig\n+from ..gemma2.modeling_gemma2 import Gemma2RotaryEmbedding\n from ..mixtral.modeling_mixtral import (\n     MixtralAttention,\n     MixtralDecoderLayer,\n@@ -49,7 +50,7 @@\n logger = logging.get_logger(__name__)\n \n \n-class MiniMaxConfig(MixtralConfig):\n+class MiniMaxConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`MiniMaxModel`]. It is used to instantiate an\n     MiniMax model according to the specified arguments, defining the model architecture. Instantiating a configuration\n@@ -102,8 +103,6 @@ class MiniMaxConfig(MixtralConfig):\n             The id of the \"end-of-sequence\" token.\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether the model's input and output word embeddings should be tied.\n-        rope_theta (`float`, *optional*, defaults to 1000000.0):\n-            The base period of the RoPE embeddings.\n         sliding_window (`int`, *optional*):\n             Sliding window attention window size. If not specified, will default to `4096`.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n@@ -120,6 +119,10 @@ class MiniMaxConfig(MixtralConfig):\n             The aux loss factor for the total loss.\n         router_jitter_noise (`float`, *optional*, defaults to 0.0):\n             Amount of noise to add to the router.\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         layer_types (`list`, *optional*):\n             Attention pattern for each layer.\n         block_size (`int`, *optional*, defaults to 256):\n@@ -151,18 +154,89 @@ class MiniMaxConfig(MixtralConfig):\n     >>> configuration = model.config\n     ```\"\"\"\n \n+    model_type = \"minimax\"\n+    keys_to_ignore_at_inference = [\"past_key_values\"]\n+    base_model_tp_plan = {\n+        \"layers.*.self_attn.q_proj\": \"colwise\",\n+        \"layers.*.self_attn.k_proj\": \"colwise\",\n+        \"layers.*.self_attn.v_proj\": \"colwise\",\n+        \"layers.*.self_attn.o_proj\": \"rowwise\",\n+        \"layers.*.block_sparse_moe.gate\": \"colwise_rep\",  # we need to replicate here to correctly route experts\n+        \"layers.*.block_sparse_moe.experts.*.w1\": \"colwise\",\n+        \"layers.*.block_sparse_moe.experts.*.w2\": \"rowwise\",\n+        \"layers.*.block_sparse_moe.experts.*.w3\": \"colwise\",\n+    }\n+    base_model_pp_plan = {\n+        \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n+        \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n+        \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n+    }\n+    attribute_map = {\n+        \"num_experts\": \"num_local_experts\",\n+    }\n+\n     def __init__(\n         self,\n-        layer_types=None,\n-        block_size=256,\n-        full_attn_alpha_factor=1,\n-        full_attn_beta_factor=1,\n-        linear_attn_alpha_factor=1,\n-        linear_attn_beta_factor=1,\n-        mlp_alpha_factor=1,\n-        mlp_beta_factor=1,\n-        **super_kwargs,\n+        vocab_size: Optional[int] = 32000,\n+        hidden_size: Optional[int] = 4096,\n+        intermediate_size: Optional[int] = 14336,\n+        num_hidden_layers: Optional[int] = 32,\n+        num_attention_heads: Optional[int] = 32,\n+        num_key_value_heads: Optional[int] = 8,\n+        head_dim: Optional[int] = None,\n+        hidden_act: Optional[str] = \"silu\",\n+        max_position_embeddings: Optional[int] = 4096 * 32,\n+        initializer_range: Optional[float] = 0.02,\n+        rms_norm_eps: Optional[int] = 1e-5,\n+        use_cache: Optional[bool] = True,\n+        pad_token_id: Optional[int] = None,\n+        bos_token_id: Optional[int] = 1,\n+        eos_token_id: Optional[int] = 2,\n+        tie_word_embeddings: Optional[bool] = False,\n+        sliding_window: Optional[int] = None,\n+        attention_dropout: Optional[float] = 0.0,\n+        num_experts_per_tok: Optional[int] = 2,\n+        num_local_experts: Optional[int] = 8,\n+        output_router_logits: Optional[bool] = False,\n+        router_aux_loss_coef: Optional[float] = 0.001,\n+        router_jitter_noise: Optional[float] = 0.0,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        layer_types: Optional[list[str]] = None,\n+        block_size: Optional[int] = 256,\n+        full_attn_alpha_factor: Optional[int] = 1,\n+        full_attn_beta_factor: Optional[int] = 1,\n+        linear_attn_alpha_factor: Optional[int] = 1,\n+        linear_attn_beta_factor: Optional[int] = 1,\n+        mlp_alpha_factor: Optional[int] = 1,\n+        mlp_beta_factor: Optional[int] = 1,\n+        **kwargs,\n     ):\n+        self.vocab_size = vocab_size\n+        self.max_position_embeddings = max_position_embeddings\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.sliding_window = sliding_window\n+\n+        # for backward compatibility\n+        if num_key_value_heads is None:\n+            num_key_value_heads = num_attention_heads\n+\n+        self.num_key_value_heads = num_key_value_heads\n+        self.hidden_act = hidden_act\n+        self.initializer_range = initializer_range\n+        self.rms_norm_eps = rms_norm_eps\n+        self.use_cache = use_cache\n+        self.attention_dropout = attention_dropout\n+        self.head_dim = head_dim\n+\n+        self.num_experts_per_tok = num_experts_per_tok\n+        self.num_local_experts = num_local_experts\n+        self.output_router_logits = output_router_logits\n+        self.router_aux_loss_coef = router_aux_loss_coef\n+        self.router_jitter_noise = router_jitter_noise\n+\n         self.layer_types = layer_types\n         self.block_size = block_size\n         self.full_attn_alpha_factor = full_attn_alpha_factor\n@@ -171,14 +245,28 @@ def __init__(\n         self.linear_attn_beta_factor = linear_attn_beta_factor\n         self.mlp_alpha_factor = mlp_alpha_factor\n         self.mlp_beta_factor = mlp_beta_factor\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n \n-        super().__init__(**super_kwargs)\n         if self.layer_types is None:\n             self.layer_types = [\n                 \"full_attention\" if bool((i + 1) % 2) else \"linear_attention\" for i in range(self.num_hidden_layers)\n             ]\n         layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n+        # Validate the correctness of rotary position embeddings parameters\n+        rope_theta = getattr(self, \"rope_theta\", 1000000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n+        rope_config_validation(self)\n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n+\n \n class MiniMaxRMSNorm(MixtralRMSNorm):\n     pass\n@@ -368,6 +456,10 @@ def forward(\n         return attn_output, attn_weights_inter\n \n \n+class MiniMaxRotaryEmbedding(Gemma2RotaryEmbedding):\n+    pass\n+\n+\n class MiniMaxAttention(MixtralAttention):\n     pass\n \n@@ -381,7 +473,7 @@ def __init__(self, config: MiniMaxConfig, layer_idx: int):\n         super().__init__(config, layer_idx)\n \n         self.layer_idx = layer_idx\n-        self.layer_type = config.layer_types[layer_idx]\n+        self.layer_type = config.layer_types[layer_idx] if hasattr(config, \"layer_types\") else None\n         self.mlp_alpha_factor = config.mlp_alpha_factor\n         self.mlp_beta_factor = config.mlp_beta_factor\n \n@@ -397,7 +489,7 @@ def __init__(self, config: MiniMaxConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -480,8 +572,6 @@ def forward(\n         )\n \n         hidden_states = inputs_embeds\n-\n-        # create position embeddings to be shared across the decoder layers\n         position_embeddings = self.rotary_emb(hidden_states, position_ids)\n \n         for decoder_layer in self.layers:\n@@ -493,8 +583,8 @@ def forward(\n \n             hidden_states = decoder_layer(\n                 hidden_states,\n-                position_embeddings=position_embeddings,\n                 attention_mask=input_attention_mask,\n+                position_embeddings=position_embeddings,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n                 use_cache=use_cache,"
        },
        {
            "sha": "3f286cd69a9fe943fc6031f65dde09d237953a9e",
            "filename": "src/transformers/models/ministral/configuration_ministral.py",
            "status": "modified",
            "additions": 35,
            "deletions": 23,
            "changes": 58,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fministral%2Fconfiguration_ministral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fministral%2Fconfiguration_ministral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fministral%2Fconfiguration_ministral.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -4,7 +4,10 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_ministral.py file directly. One of our CI enforces this.\n #                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+from typing import Optional\n+\n from ...configuration_utils import PreTrainedConfig\n+from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n \n \n class MinistralConfig(PreTrainedConfig):\n@@ -61,8 +64,10 @@ class MinistralConfig(PreTrainedConfig):\n             The id of the \"end-of-sequence\" token.\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether the model's input and output word embeddings should be tied.\n-        rope_theta (`float`, *optional*, defaults to 10000.0):\n-            The base period of the RoPE embeddings.\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         sliding_window (`int`, *optional*, defaults to 4096):\n             Sliding window attention window size. If not specified, will default to `4096`.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n@@ -103,26 +108,26 @@ class MinistralConfig(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size=32000,\n-        hidden_size=4096,\n-        intermediate_size=14336,\n-        num_hidden_layers=32,\n-        num_attention_heads=32,\n-        num_key_value_heads=8,\n-        head_dim=None,\n-        hidden_act=\"silu\",\n-        max_position_embeddings=4096 * 32,\n-        initializer_range=0.02,\n-        rms_norm_eps=1e-6,\n-        use_cache=True,\n-        pad_token_id=None,\n-        bos_token_id=1,\n-        eos_token_id=2,\n-        tie_word_embeddings=False,\n-        rope_theta=10000.0,\n-        sliding_window=4096,\n-        attention_dropout=0.0,\n-        layer_types=None,\n+        vocab_size: Optional[int] = 32000,\n+        hidden_size: Optional[int] = 4096,\n+        intermediate_size: Optional[int] = 14336,\n+        num_hidden_layers: Optional[int] = 32,\n+        num_attention_heads: Optional[int] = 32,\n+        num_key_value_heads: Optional[int] = 8,\n+        head_dim: Optional[int] = None,\n+        hidden_act: Optional[str] = \"silu\",\n+        max_position_embeddings: Optional[int] = 4096 * 32,\n+        initializer_range: Optional[float] = 0.02,\n+        rms_norm_eps: Optional[float] = 1e-6,\n+        use_cache: Optional[bool] = True,\n+        pad_token_id: Optional[int] = None,\n+        bos_token_id: Optional[int] = 1,\n+        eos_token_id: Optional[int] = 2,\n+        tie_word_embeddings: Optional[bool] = False,\n+        rope_parameters: Optional[RopeParameters] = None,\n+        sliding_window: Optional[int] = 4096,\n+        attention_dropout: Optional[float] = 0.0,\n+        layer_types: Optional[list[str]] = None,\n         **kwargs,\n     ):\n         super().__init__(\n@@ -150,14 +155,21 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.rms_norm_eps = rms_norm_eps\n         self.use_cache = use_cache\n-        self.rope_theta = rope_theta\n         self.attention_dropout = attention_dropout\n         self.layer_types = layer_types\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n \n         if self.layer_types is None:\n             self.layer_types = [\n                 \"sliding_attention\" if self.sliding_window is not None else \"full_attention\"\n             ] * num_hidden_layers\n \n+        # Validate the correctness of rotary position embeddings parameters\n+        rope_theta = getattr(self, \"rope_theta\", 10000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n+        rope_config_validation(self)\n+\n \n __all__ = [\"MinistralConfig\"]"
        },
        {
            "sha": "239d2fc2047bbe55b21772f5549094bb34397da7",
            "filename": "src/transformers/models/ministral/modeling_ministral.py",
            "status": "modified",
            "additions": 40,
            "deletions": 12,
            "changes": 52,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fministral%2Fmodeling_ministral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fministral%2Fmodeling_ministral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fministral%2Fmodeling_ministral.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -124,6 +124,7 @@ class MinistralAttention(nn.Module):\n \n     def __init__(self, config, layer_idx: int):\n         super().__init__()\n+        self.layer_type = config.layer_types[layer_idx] if hasattr(config, \"layer_types\") else None\n         self.config = config\n         self.layer_idx = layer_idx\n         self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n@@ -136,7 +137,7 @@ def __init__(self, config, layer_idx: int):\n         self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n         self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n-        self.sliding_window = config.sliding_window if config.layer_types[layer_idx] == \"sliding_attention\" else None\n+        self.sliding_window = config.sliding_window if self.layer_type == \"sliding_attention\" else None\n \n     def forward(\n         self,\n@@ -224,7 +225,7 @@ def forward(\n         past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> torch.Tensor:\n         residual = hidden_states\n@@ -274,20 +275,49 @@ class MinistralRotaryEmbedding(nn.Module):\n \n     def __init__(self, config: MinistralConfig, device=None):\n         super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n         self.max_seq_len_cached = config.max_position_embeddings\n         self.original_max_seq_len = config.max_position_embeddings\n \n         self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.rope_type = self.config.rope_parameters[\"rope_type\"]\n+        rope_init_fn: Callable = self.compute_default_rope_parameters\n+        if self.rope_type != \"default\":\n+            rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+        inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n+\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n+        self.original_inv_freq = inv_freq\n+\n+    @staticmethod\n+    def compute_default_rope_parameters(\n+        config: Optional[MinistralConfig] = None,\n+        device: Optional[\"torch.device\"] = None,\n+        seq_len: Optional[int] = None,\n+    ) -> tuple[\"torch.Tensor\", float]:\n+        \"\"\"\n+        Computes the inverse frequencies according to the original RoPE implementation\n+        Args:\n+            config ([`~transformers.PreTrainedConfig`]):\n+                The model configuration.\n+            device (`torch.device`):\n+                The device to use for initialization of the inverse frequencies.\n+            seq_len (`int`, *optional*):\n+                The current sequence length. Unused for this type of RoPE.\n+        Returns:\n+            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n+            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n+        \"\"\"\n+        base = config.rope_parameters[\"rope_theta\"]\n+        dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+\n+        attention_factor = 1.0  # Unused in this type of RoPE\n+\n+        # Compute the inverse frequencies\n+        inv_freq = 1.0 / (\n+            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n+        )\n+        return inv_freq, attention_factor\n \n     @torch.no_grad()\n     @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n@@ -372,8 +402,6 @@ def forward(\n             }\n \n         hidden_states = inputs_embeds\n-\n-        # create position embeddings to be shared across the decoder layers\n         position_embeddings = self.rotary_emb(hidden_states, position_ids)\n \n         for decoder_layer in self.layers[: self.config.num_hidden_layers]:"
        },
        {
            "sha": "f79600e829741c1533e08a8f26a73839a1ac2618",
            "filename": "src/transformers/models/ministral/modular_ministral.py",
            "status": "modified",
            "additions": 33,
            "deletions": 25,
            "changes": 58,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fministral%2Fmodular_ministral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fministral%2Fmodular_ministral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fministral%2Fmodular_ministral.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -7,6 +7,7 @@\n from ...configuration_utils import PreTrainedConfig\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_outputs import BaseModelOutputWithPast\n+from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring\n from ...utils.generic import check_model_inputs\n@@ -80,8 +81,10 @@ class MinistralConfig(MistralConfig, PreTrainedConfig):\n             The id of the \"end-of-sequence\" token.\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether the model's input and output word embeddings should be tied.\n-        rope_theta (`float`, *optional*, defaults to 10000.0):\n-            The base period of the RoPE embeddings.\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         sliding_window (`int`, *optional*, defaults to 4096):\n             Sliding window attention window size. If not specified, will default to `4096`.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n@@ -106,26 +109,26 @@ class MinistralConfig(MistralConfig, PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size=32000,\n-        hidden_size=4096,\n-        intermediate_size=14336,\n-        num_hidden_layers=32,\n-        num_attention_heads=32,\n-        num_key_value_heads=8,\n-        head_dim=None,\n-        hidden_act=\"silu\",\n-        max_position_embeddings=4096 * 32,\n-        initializer_range=0.02,\n-        rms_norm_eps=1e-6,\n-        use_cache=True,\n-        pad_token_id=None,\n-        bos_token_id=1,\n-        eos_token_id=2,\n-        tie_word_embeddings=False,\n-        rope_theta=10000.0,\n-        sliding_window=4096,\n-        attention_dropout=0.0,\n-        layer_types=None,\n+        vocab_size: Optional[int] = 32000,\n+        hidden_size: Optional[int] = 4096,\n+        intermediate_size: Optional[int] = 14336,\n+        num_hidden_layers: Optional[int] = 32,\n+        num_attention_heads: Optional[int] = 32,\n+        num_key_value_heads: Optional[int] = 8,\n+        head_dim: Optional[int] = None,\n+        hidden_act: Optional[str] = \"silu\",\n+        max_position_embeddings: Optional[int] = 4096 * 32,\n+        initializer_range: Optional[float] = 0.02,\n+        rms_norm_eps: Optional[float] = 1e-6,\n+        use_cache: Optional[bool] = True,\n+        pad_token_id: Optional[int] = None,\n+        bos_token_id: Optional[int] = 1,\n+        eos_token_id: Optional[int] = 2,\n+        tie_word_embeddings: Optional[bool] = False,\n+        rope_parameters: Optional[RopeParameters] = None,\n+        sliding_window: Optional[int] = 4096,\n+        attention_dropout: Optional[float] = 0.0,\n+        layer_types: Optional[list[str]] = None,\n         **kwargs,\n     ):\n         PreTrainedConfig.__init__(\n@@ -154,15 +157,22 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.rms_norm_eps = rms_norm_eps\n         self.use_cache = use_cache\n-        self.rope_theta = rope_theta\n         self.attention_dropout = attention_dropout\n         self.layer_types = layer_types\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n \n         if self.layer_types is None:\n             self.layer_types = [\n                 \"sliding_attention\" if self.sliding_window is not None else \"full_attention\"\n             ] * num_hidden_layers\n \n+        # Validate the correctness of rotary position embeddings parameters\n+        rope_theta = getattr(self, \"rope_theta\", 10000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n+        rope_config_validation(self)\n+\n \n class MinistralMLP(Qwen2MLP):\n     pass\n@@ -247,8 +257,6 @@ def forward(\n             }\n \n         hidden_states = inputs_embeds\n-\n-        # create position embeddings to be shared across the decoder layers\n         position_embeddings = self.rotary_emb(hidden_states, position_ids)\n \n         for decoder_layer in self.layers[: self.config.num_hidden_layers]:"
        },
        {
            "sha": "e17bd2a65423fc2b073faaeed548d3d0c34d68f8",
            "filename": "src/transformers/models/mistral/configuration_mistral.py",
            "status": "modified",
            "additions": 35,
            "deletions": 22,
            "changes": 57,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fmistral%2Fconfiguration_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fmistral%2Fconfiguration_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fconfiguration_mistral.py?ref=10de06dacef06605be3296f776d4122697637d6c",
            "patch": "@@ -14,7 +14,10 @@\n # limitations under the License.\n \"\"\"Mistral model configuration\"\"\"\n \n+from typing import Optional\n+\n from ...configuration_utils import PreTrainedConfig\n+from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n from ...utils import logging\n \n \n@@ -75,8 +78,10 @@ class MistralConfig(PreTrainedConfig):\n             The id of the \"end-of-sequence\" token.\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether the model's input and output word embeddings should be tied.\n-        rope_theta (`float`, *optional*, defaults to 10000.0):\n-            The base period of the RoPE embeddings.\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         sliding_window (`int`, *optional*, defaults to 4096):\n             Sliding window attention window size. If not specified, will default to `4096`.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n@@ -115,25 +120,25 @@ class MistralConfig(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size=32000,\n-        hidden_size=4096,\n-        intermediate_size=14336,\n-        num_hidden_layers=32,\n-        num_attention_heads=32,\n-        num_key_value_heads=8,\n-        head_dim=None,\n-        hidden_act=\"silu\",\n-        max_position_embeddings=4096 * 32,\n-        initializer_range=0.02,\n-        rms_norm_eps=1e-6,\n-        use_cache=True,\n-        pad_token_id=None,\n-        bos_token_id=1,\n-        eos_token_id=2,\n-        tie_word_embeddings=False,\n-        rope_theta=10000.0,\n-        sliding_window=4096,\n-        attention_dropout=0.0,\n+        vocab_size: Optional[int] = 32000,\n+        hidden_size: Optional[int] = 4096,\n+        intermediate_size: Optional[int] = 14336,\n+        num_hidden_layers: Optional[int] = 32,\n+        num_attention_heads: Optional[int] = 32,\n+        num_key_value_heads: Optional[int] = 8,\n+        head_dim: Optional[int] = None,\n+        hidden_act: Optional[str] = \"silu\",\n+        max_position_embeddings: Optional[int] = 4096 * 32,\n+        initializer_range: Optional[float] = 0.02,\n+        rms_norm_eps: Optional[int] = 1e-6,\n+        use_cache: Optional[bool] = True,\n+        pad_token_id: Optional[int] = None,\n+        bos_token_id: Optional[int] = 1,\n+        eos_token_id: Optional[int] = 2,\n+        tie_word_embeddings: Optional[bool] = False,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        sliding_window: Optional[int] = 4096,\n+        attention_dropout: Optional[float] = 0.0,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size\n@@ -154,14 +159,22 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.rms_norm_eps = rms_norm_eps\n         self.use_cache = use_cache\n-        self.rope_theta = rope_theta\n         self.attention_dropout = attention_dropout\n \n         if \"layer_types\" in kwargs:\n             logger.warning_once(\n                 \"Detected Mistral model with layer_types. Consider using AutoModel or Ministral classes instead to enable alternating attention compatibility.\"\n             )\n \n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n+\n+        # Validate the correctness of rotary position embeddings parameters\n+        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n+        rope_config_validation(self)\n+\n         super().__init__(\n             pad_token_id=pad_token_id,\n             bos_token_id=bos_token_id,"
        },
        {
            "sha": "ab3cae55bb6e330dd3aa5b8e199fce117621ca61",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "709ff855c399b55d1d21a4722247c374c260ac0c",
            "filename": "src/transformers/models/mistral/modular_mistral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "9a8e2280c252150b72c1a57df163651e3ed0c423",
            "filename": "src/transformers/models/mixtral/configuration_mixtral.py",
            "status": "modified",
            "additions": 40,
            "deletions": 27,
            "changes": 67,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fmixtral%2Fconfiguration_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fmixtral%2Fconfiguration_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fconfiguration_mixtral.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "a8fa4ed5619d83c0a9685a194ba194cbf28084a8",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 68,
            "deletions": 41,
            "changes": 109,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "58a9100d388e48d76eae0e24c3bfc8627dd5d627",
            "filename": "src/transformers/models/mixtral/modular_mixtral.py",
            "status": "modified",
            "additions": 7,
            "deletions": 9,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "85be3701a84ae9c7e55e6c42211ca3d7d02f40c1",
            "filename": "src/transformers/models/mllama/configuration_mllama.py",
            "status": "modified",
            "additions": 13,
            "deletions": 44,
            "changes": 57,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fmllama%2Fconfiguration_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fmllama%2Fconfiguration_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fconfiguration_mllama.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "13729fc3bbf4fc82eb9b7e7c6a6e63541a333197",
            "filename": "src/transformers/models/mllama/convert_mllama_weights_to_hf.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fmllama%2Fconvert_mllama_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fmllama%2Fconvert_mllama_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fconvert_mllama_weights_to_hf.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "c3c1930e386e97b67a8808912160802558dbc45d",
            "filename": "src/transformers/models/mllama/modeling_mllama.py",
            "status": "modified",
            "additions": 49,
            "deletions": 10,
            "changes": 59,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "6d378425284d1611c7c2361c16d1e294709a983e",
            "filename": "src/transformers/models/modernbert/configuration_modernbert.py",
            "status": "modified",
            "additions": 65,
            "deletions": 45,
            "changes": 110,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fconfiguration_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fconfiguration_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fconfiguration_modernbert.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "c363eaefcf3c312be27572b3a56ee82c152e8e0c",
            "filename": "src/transformers/models/modernbert/modeling_modernbert.py",
            "status": "modified",
            "additions": 79,
            "deletions": 21,
            "changes": 100,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "9e535d345f2f8a163192d4a9e5f7f8ed69853821",
            "filename": "src/transformers/models/modernbert/modular_modernbert.py",
            "status": "modified",
            "additions": 97,
            "deletions": 55,
            "changes": 152,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "cc17f6ce6711da42c58ecd421d42dbed65fbf989",
            "filename": "src/transformers/models/modernbert_decoder/configuration_modernbert_decoder.py",
            "status": "modified",
            "additions": 49,
            "deletions": 38,
            "changes": 87,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fconfiguration_modernbert_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fconfiguration_modernbert_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fconfiguration_modernbert_decoder.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "bb5c8dad9fa4084055970b435fe113dd97d6e06a",
            "filename": "src/transformers/models/modernbert_decoder/modeling_modernbert_decoder.py",
            "status": "modified",
            "additions": 65,
            "deletions": 30,
            "changes": 95,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodeling_modernbert_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodeling_modernbert_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodeling_modernbert_decoder.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "ffa7da7c130a7433728230ed6322bb502e97a84a",
            "filename": "src/transformers/models/modernbert_decoder/modular_modernbert_decoder.py",
            "status": "modified",
            "additions": 54,
            "deletions": 55,
            "changes": 109,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodular_modernbert_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodular_modernbert_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodular_modernbert_decoder.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "5237cd4e3d8cf180fc49c667f4e5620e4c8fece0",
            "filename": "src/transformers/models/moonshine/configuration_moonshine.py",
            "status": "modified",
            "additions": 35,
            "deletions": 67,
            "changes": 102,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fconfiguration_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fconfiguration_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fconfiguration_moonshine.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "63b93f9c2651912cfd55b8c733bdc39230451205",
            "filename": "src/transformers/models/moonshine/modeling_moonshine.py",
            "status": "modified",
            "additions": 71,
            "deletions": 40,
            "changes": 111,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "1e035bdb87c6b86b0e828f655ca677a2689d8657",
            "filename": "src/transformers/models/moonshine/modular_moonshine.py",
            "status": "modified",
            "additions": 40,
            "deletions": 74,
            "changes": 114,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "8d2e2aef339ab4628ebcc71bec34af535119978f",
            "filename": "src/transformers/models/moshi/configuration_moshi.py",
            "status": "modified",
            "additions": 33,
            "deletions": 21,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fmoshi%2Fconfiguration_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fmoshi%2Fconfiguration_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fconfiguration_moshi.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "01c89ecb52cc62c510d9a929b19a23f376265f40",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 39,
            "deletions": 10,
            "changes": 49,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "084a674fc345cd09e2a2ee76a978b4f4fa1b978b",
            "filename": "src/transformers/models/nemotron/configuration_nemotron.py",
            "status": "modified",
            "additions": 36,
            "deletions": 26,
            "changes": 62,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fnemotron%2Fconfiguration_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fnemotron%2Fconfiguration_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fconfiguration_nemotron.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "1c8c7eca861f9322b15e5ae1cc7cb2d407937eae",
            "filename": "src/transformers/models/nemotron/modeling_nemotron.py",
            "status": "modified",
            "additions": 53,
            "deletions": 23,
            "changes": 76,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "6a1fb4f96526ccaebb256d8177a84ea7716e9f11",
            "filename": "src/transformers/models/olmo/configuration_olmo.py",
            "status": "modified",
            "additions": 33,
            "deletions": 52,
            "changes": 85,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Folmo%2Fconfiguration_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Folmo%2Fconfiguration_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fconfiguration_olmo.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "6a3432c31d1887486a4f7d7289787591f568820b",
            "filename": "src/transformers/models/olmo/modeling_olmo.py",
            "status": "modified",
            "additions": 88,
            "deletions": 38,
            "changes": 126,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "4f2796837cb5faabf23f7bbf5e2e983200786777",
            "filename": "src/transformers/models/olmo/modular_olmo.py",
            "status": "modified",
            "additions": 40,
            "deletions": 16,
            "changes": 56,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Folmo%2Fmodular_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Folmo%2Fmodular_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodular_olmo.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "2b4af4c7523c948174e22a597167dbb73a89d13c",
            "filename": "src/transformers/models/olmo2/configuration_olmo2.py",
            "status": "modified",
            "additions": 52,
            "deletions": 52,
            "changes": 104,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Folmo2%2Fconfiguration_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Folmo2%2Fconfiguration_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fconfiguration_olmo2.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "7315661282c9c296cb650a34cad0eece190d40e7",
            "filename": "src/transformers/models/olmo2/modeling_olmo2.py",
            "status": "modified",
            "additions": 88,
            "deletions": 38,
            "changes": 126,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "74eddd2d5af4a4bfa5ae04d5e0a0da49270949b5",
            "filename": "src/transformers/models/olmo2/modular_olmo2.py",
            "status": "modified",
            "additions": 50,
            "deletions": 36,
            "changes": 86,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodular_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodular_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodular_olmo2.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "08762d09ff615e1a220797815ee6d54f5aeb8529",
            "filename": "src/transformers/models/olmo3/configuration_olmo3.py",
            "status": "modified",
            "additions": 41,
            "deletions": 78,
            "changes": 119,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Folmo3%2Fconfiguration_olmo3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Folmo3%2Fconfiguration_olmo3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo3%2Fconfiguration_olmo3.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "2888f787399b428faa7eac2fcadc0a14eaf59f2f",
            "filename": "src/transformers/models/olmo3/modeling_olmo3.py",
            "status": "modified",
            "additions": 44,
            "deletions": 26,
            "changes": 70,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Folmo3%2Fmodeling_olmo3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Folmo3%2Fmodeling_olmo3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo3%2Fmodeling_olmo3.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "ab1b63752721f0ec35b83779516efb40b22adc89",
            "filename": "src/transformers/models/olmo3/modular_olmo3.py",
            "status": "modified",
            "additions": 59,
            "deletions": 120,
            "changes": 179,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Folmo3%2Fmodular_olmo3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Folmo3%2Fmodular_olmo3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo3%2Fmodular_olmo3.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "5dae49098a29f17d47b843559ee024b46d447145",
            "filename": "src/transformers/models/olmoe/configuration_olmoe.py",
            "status": "modified",
            "additions": 37,
            "deletions": 41,
            "changes": 78,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Folmoe%2Fconfiguration_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Folmoe%2Fconfiguration_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fconfiguration_olmoe.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "3403b2e59667856be7b366bfb28ff9fb8e9f5c68",
            "filename": "src/transformers/models/olmoe/modeling_olmoe.py",
            "status": "modified",
            "additions": 38,
            "deletions": 9,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "0d079f76a797a6df37f8be0ab059c69a5503b26e",
            "filename": "src/transformers/models/perception_lm/convert_perception_lm_weights_to_hf.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fconvert_perception_lm_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fconvert_perception_lm_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fconvert_perception_lm_weights_to_hf.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "5c2452526635f38e4e19c39265a56094e1e4f3fd",
            "filename": "src/transformers/models/persimmon/configuration_persimmon.py",
            "status": "modified",
            "additions": 32,
            "deletions": 65,
            "changes": 97,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fconfiguration_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fconfiguration_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fconfiguration_persimmon.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "205d5b1fc1d798b562b63eed7639fc7bf599f049",
            "filename": "src/transformers/models/persimmon/modeling_persimmon.py",
            "status": "modified",
            "additions": 49,
            "deletions": 21,
            "changes": 70,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "427b453db981d2bdb695b444aa15fe378405f1be",
            "filename": "src/transformers/models/phi/configuration_phi.py",
            "status": "modified",
            "additions": 33,
            "deletions": 66,
            "changes": 99,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fphi%2Fconfiguration_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fphi%2Fconfiguration_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fconfiguration_phi.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "3fb8de6e32e3db183a0b2ae3d1cbaf233187106e",
            "filename": "src/transformers/models/phi/modeling_phi.py",
            "status": "modified",
            "additions": 70,
            "deletions": 40,
            "changes": 110,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "3ecc9ba9d4f7a9815cf69da76f412a2b210a24ce",
            "filename": "src/transformers/models/phi/modular_phi.py",
            "status": "modified",
            "additions": 37,
            "deletions": 8,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fphi%2Fmodular_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fphi%2Fmodular_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodular_phi.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "ed096dd8a319a6780c9448c190e9d996484e72b0",
            "filename": "src/transformers/models/phi3/configuration_phi3.py",
            "status": "modified",
            "additions": 79,
            "deletions": 79,
            "changes": 158,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fphi3%2Fconfiguration_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fphi3%2Fconfiguration_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fconfiguration_phi3.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "d1ebf1ea99c0b88728b9d1b7d0cb15421c4f9d9c",
            "filename": "src/transformers/models/phi3/modeling_phi3.py",
            "status": "modified",
            "additions": 70,
            "deletions": 39,
            "changes": 109,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "5b0d5f76d69c9aad5c63e0bcc5495ff7c1b623ae",
            "filename": "src/transformers/models/phi3/modular_phi3.py",
            "status": "modified",
            "additions": 7,
            "deletions": 2,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodular_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodular_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodular_phi3.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "c9ea706b2c4c1a2d20637ce03ce49de3fa9ec361",
            "filename": "src/transformers/models/phi4_multimodal/configuration_phi4_multimodal.py",
            "status": "modified",
            "additions": 80,
            "deletions": 81,
            "changes": 161,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fconfiguration_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fconfiguration_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fconfiguration_phi4_multimodal.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "aebf09174575b47952aa1209f96e15261b88c173",
            "filename": "src/transformers/models/phi4_multimodal/modeling_phi4_multimodal.py",
            "status": "modified",
            "additions": 69,
            "deletions": 39,
            "changes": 108,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "17458f141f127284888aa3b5ae5b4033bf0eca25",
            "filename": "src/transformers/models/phi4_multimodal/modular_phi4_multimodal.py",
            "status": "modified",
            "additions": 32,
            "deletions": 42,
            "changes": 74,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "8af5508daf936eba7b58b560f9b48db7453615c0",
            "filename": "src/transformers/models/phimoe/configuration_phimoe.py",
            "status": "modified",
            "additions": 52,
            "deletions": 50,
            "changes": 102,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fphimoe%2Fconfiguration_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fphimoe%2Fconfiguration_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fconfiguration_phimoe.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "5f974d51e8c46ca3829d5ce553ea1649de0f2e19",
            "filename": "src/transformers/models/phimoe/modeling_phimoe.py",
            "status": "modified",
            "additions": 62,
            "deletions": 24,
            "changes": 86,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "59f5761987b9c31ed7cdbbcc6896b325e55ab835",
            "filename": "src/transformers/models/phimoe/modular_phimoe.py",
            "status": "modified",
            "additions": 26,
            "deletions": 19,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodular_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodular_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodular_phimoe.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "3e2098adcd943b719fe9a106d292b5ac5a1ebaba",
            "filename": "src/transformers/models/pixtral/configuration_pixtral.py",
            "status": "modified",
            "additions": 24,
            "deletions": 14,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fpixtral%2Fconfiguration_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fpixtral%2Fconfiguration_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fconfiguration_pixtral.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "0f237c86beacc9addb193dfcd455c8dfa97dbc20",
            "filename": "src/transformers/models/pixtral/modeling_pixtral.py",
            "status": "modified",
            "additions": 46,
            "deletions": 11,
            "changes": 57,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "418b18027350163eb290117e0f10e94601558b3c",
            "filename": "src/transformers/models/qwen2/configuration_qwen2.py",
            "status": "modified",
            "additions": 33,
            "deletions": 66,
            "changes": 99,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fqwen2%2Fconfiguration_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fqwen2%2Fconfiguration_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fconfiguration_qwen2.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "59e038eb2552f550bc74ce68d3e68d3e7253e436",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 69,
            "deletions": 41,
            "changes": 110,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "b06d3182b273313b0661365babeb4b3e16ba11f8",
            "filename": "src/transformers/models/qwen2/modular_qwen2.py",
            "status": "modified",
            "additions": 8,
            "deletions": 4,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodular_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodular_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodular_qwen2.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "69a4e3e0c66fa178e539d09ae3f6cf2454af3a4c",
            "filename": "src/transformers/models/qwen2_5_omni/configuration_qwen2_5_omni.py",
            "status": "modified",
            "additions": 55,
            "deletions": 114,
            "changes": 169,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fconfiguration_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fconfiguration_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fconfiguration_qwen2_5_omni.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "80b23721431d0d8c1ac9c6c8340d722def93a666",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 108,
            "deletions": 49,
            "changes": 157,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "c37b321ce38b2837ef286de55466c0bd048a7e56",
            "filename": "src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py",
            "status": "modified",
            "additions": 76,
            "deletions": 145,
            "changes": 221,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "ad556be59a8a89edea9a82deacc9941ba1596338",
            "filename": "src/transformers/models/qwen2_5_vl/configuration_qwen2_5_vl.py",
            "status": "modified",
            "additions": 33,
            "deletions": 69,
            "changes": 102,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fconfiguration_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fconfiguration_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fconfiguration_qwen2_5_vl.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "f9ad678c7d3e42b6f1ed61ad611eed105e6cc3eb",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 48,
            "deletions": 23,
            "changes": 71,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "f45577e91516b6e9cf55b659b3ba01aa81cb50ea",
            "filename": "src/transformers/models/qwen2_moe/configuration_qwen2_moe.py",
            "status": "modified",
            "additions": 44,
            "deletions": 76,
            "changes": 120,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fconfiguration_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fconfiguration_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fconfiguration_qwen2_moe.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "55af7da767ce017307d1cc5b7d201d0c188586f6",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 40,
            "deletions": 13,
            "changes": 53,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "56c100f94b9355c728984e00d305f7231f80a123",
            "filename": "src/transformers/models/qwen2_moe/modular_qwen2_moe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodular_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodular_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodular_qwen2_moe.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "043ae4ee993ed1fb3edda4cceca16378cb7b25ae",
            "filename": "src/transformers/models/qwen2_vl/configuration_qwen2_vl.py",
            "status": "modified",
            "additions": 32,
            "deletions": 69,
            "changes": 101,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fconfiguration_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fconfiguration_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fconfiguration_qwen2_vl.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "e29829b31e760e55bedc6572b8520cb2cd07b044",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 49,
            "deletions": 23,
            "changes": 72,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "90edaff6aaa3e46055b8adbadbfc97bd7cef0055",
            "filename": "src/transformers/models/qwen3/configuration_qwen3.py",
            "status": "modified",
            "additions": 35,
            "deletions": 68,
            "changes": 103,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fqwen3%2Fconfiguration_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fqwen3%2Fconfiguration_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3%2Fconfiguration_qwen3.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "1973de1b19ef356e60cd11452fd195350a73879a",
            "filename": "src/transformers/models/qwen3/modeling_qwen3.py",
            "status": "modified",
            "additions": 69,
            "deletions": 41,
            "changes": 110,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "2f113785a94e408cd70b6ccdb51a24201814625d",
            "filename": "src/transformers/models/qwen3/modular_qwen3.py",
            "status": "modified",
            "additions": 9,
            "deletions": 18,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodular_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodular_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodular_qwen3.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "e5003c5091185ecc5d578c572ab5db16933aa4ae",
            "filename": "src/transformers/models/qwen3_moe/configuration_qwen3_moe.py",
            "status": "modified",
            "additions": 38,
            "deletions": 71,
            "changes": 109,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fconfiguration_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fconfiguration_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fconfiguration_qwen3_moe.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "7c0a7cb72a774197175d0ffe06abe2704ef95245",
            "filename": "src/transformers/models/qwen3_moe/modeling_qwen3_moe.py",
            "status": "modified",
            "additions": 40,
            "deletions": 13,
            "changes": 53,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "87a4bbfa9625d9b4720bc11f8cdb36df278b8614",
            "filename": "src/transformers/models/qwen3_moe/modular_qwen3_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodular_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodular_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodular_qwen3_moe.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "da6dde8c9db7020f88f8484b9602713bfec91c1d",
            "filename": "src/transformers/models/qwen3_next/configuration_qwen3_next.py",
            "status": "modified",
            "additions": 47,
            "deletions": 76,
            "changes": 123,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fconfiguration_qwen3_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fconfiguration_qwen3_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fconfiguration_qwen3_next.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "036d2a30a55eb1bf9c3dc729c8541486d4e56a1d",
            "filename": "src/transformers/models/qwen3_next/modeling_qwen3_next.py",
            "status": "modified",
            "additions": 39,
            "deletions": 10,
            "changes": 49,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodeling_qwen3_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodeling_qwen3_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodeling_qwen3_next.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "1e9962797775ea804c97630540fdf6f3fc4ab681",
            "filename": "src/transformers/models/qwen3_next/modular_qwen3_next.py",
            "status": "modified",
            "additions": 33,
            "deletions": 5,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodular_qwen3_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodular_qwen3_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodular_qwen3_next.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "281c2a2bf509d8e173a22203d65601a62e819514",
            "filename": "src/transformers/models/qwen3_omni_moe/configuration_qwen3_omni_moe.py",
            "status": "modified",
            "additions": 119,
            "deletions": 222,
            "changes": 341,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fconfiguration_qwen3_omni_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fconfiguration_qwen3_omni_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fconfiguration_qwen3_omni_moe.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "4ce6408dbb3e3ba5a5e4d34fd1f88e3c42dbeeb9",
            "filename": "src/transformers/models/qwen3_omni_moe/modeling_qwen3_omni_moe.py",
            "status": "modified",
            "additions": 163,
            "deletions": 80,
            "changes": 243,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "9fdcac86ca121660eef776031f026406ea0e00fd",
            "filename": "src/transformers/models/qwen3_omni_moe/modular_qwen3_omni_moe.py",
            "status": "modified",
            "additions": 101,
            "deletions": 102,
            "changes": 203,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodular_qwen3_omni_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodular_qwen3_omni_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodular_qwen3_omni_moe.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "f4228ddb3f879351d559392d6729daeebb7fd551",
            "filename": "src/transformers/models/qwen3_vl/configuration_qwen3_vl.py",
            "status": "modified",
            "additions": 29,
            "deletions": 59,
            "changes": 88,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fconfiguration_qwen3_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fconfiguration_qwen3_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fconfiguration_qwen3_vl.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "d41cfa4b090ef760283c3d5ac9fa4884d849bb4d",
            "filename": "src/transformers/models/qwen3_vl/modeling_qwen3_vl.py",
            "status": "modified",
            "additions": 54,
            "deletions": 23,
            "changes": 77,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodeling_qwen3_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodeling_qwen3_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodeling_qwen3_vl.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "5d1c88d03bc4df8847239c9bcfbdf3af65f50051",
            "filename": "src/transformers/models/qwen3_vl/modular_qwen3_vl.py",
            "status": "modified",
            "additions": 31,
            "deletions": 75,
            "changes": 106,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodular_qwen3_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodular_qwen3_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodular_qwen3_vl.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "eab77fa368a2cc06e7c35c3c171daa96a7763309",
            "filename": "src/transformers/models/qwen3_vl_moe/configuration_qwen3_vl_moe.py",
            "status": "modified",
            "additions": 35,
            "deletions": 72,
            "changes": 107,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fconfiguration_qwen3_vl_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fconfiguration_qwen3_vl_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fconfiguration_qwen3_vl_moe.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "264902c2d8a4d62a65c3fa1368055916ade20325",
            "filename": "src/transformers/models/qwen3_vl_moe/modeling_qwen3_vl_moe.py",
            "status": "modified",
            "additions": 55,
            "deletions": 24,
            "changes": 79,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodeling_qwen3_vl_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodeling_qwen3_vl_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodeling_qwen3_vl_moe.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "c0c4be2ddb68529fdfbb80eeff185f8cbea4d25d",
            "filename": "src/transformers/models/qwen3_vl_moe/modular_qwen3_vl_moe.py",
            "status": "modified",
            "additions": 32,
            "deletions": 72,
            "changes": 104,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodular_qwen3_vl_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodular_qwen3_vl_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodular_qwen3_vl_moe.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "3a3aca4ddacd753633411f81d84ae6d92157da04",
            "filename": "src/transformers/models/recurrent_gemma/configuration_recurrent_gemma.py",
            "status": "modified",
            "additions": 38,
            "deletions": 25,
            "changes": 63,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fconfiguration_recurrent_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fconfiguration_recurrent_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fconfiguration_recurrent_gemma.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "3b6041c9d0463c0f68205b2df9110aa9fd7c1bdd",
            "filename": "src/transformers/models/recurrent_gemma/modeling_recurrent_gemma.py",
            "status": "modified",
            "additions": 61,
            "deletions": 20,
            "changes": 81,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "7961646ae2d8a45bd63b1b6ed9b9d7f98ad2d498",
            "filename": "src/transformers/models/seed_oss/configuration_seed_oss.py",
            "status": "modified",
            "additions": 36,
            "deletions": 69,
            "changes": 105,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fseed_oss%2Fconfiguration_seed_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fseed_oss%2Fconfiguration_seed_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseed_oss%2Fconfiguration_seed_oss.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "7e645e3ce052de5166933a5965a4fb8cd1f7375d",
            "filename": "src/transformers/models/seed_oss/modeling_seed_oss.py",
            "status": "modified",
            "additions": 40,
            "deletions": 11,
            "changes": 51,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fseed_oss%2Fmodeling_seed_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fseed_oss%2Fmodeling_seed_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseed_oss%2Fmodeling_seed_oss.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "d5b31541ac0c626f2bf67dd580a03c52d552ac53",
            "filename": "src/transformers/models/shieldgemma2/convert_shieldgemma2_weights_orbax_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fshieldgemma2%2Fconvert_shieldgemma2_weights_orbax_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fshieldgemma2%2Fconvert_shieldgemma2_weights_orbax_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fshieldgemma2%2Fconvert_shieldgemma2_weights_orbax_to_hf.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "2ffdf53008c6d554815a14e8ce824e47a37134c0",
            "filename": "src/transformers/models/smollm3/configuration_smollm3.py",
            "status": "modified",
            "additions": 32,
            "deletions": 69,
            "changes": 101,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fconfiguration_smollm3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fconfiguration_smollm3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fconfiguration_smollm3.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "e11c1138b490d0cdd2a406973431916a9d49ed99",
            "filename": "src/transformers/models/smollm3/modeling_smollm3.py",
            "status": "modified",
            "additions": 67,
            "deletions": 40,
            "changes": 107,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodeling_smollm3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodeling_smollm3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodeling_smollm3.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "fe8bcb52080ddcb9156a6a2b50af545f3e1083e3",
            "filename": "src/transformers/models/smollm3/modular_smollm3.py",
            "status": "modified",
            "additions": 35,
            "deletions": 70,
            "changes": 105,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodular_smollm3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodular_smollm3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodular_smollm3.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "9beed377ad6904d0316f58270392cc8258f4c3c6",
            "filename": "src/transformers/models/stablelm/configuration_stablelm.py",
            "status": "modified",
            "additions": 34,
            "deletions": 67,
            "changes": 101,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fstablelm%2Fconfiguration_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fstablelm%2Fconfiguration_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstablelm%2Fconfiguration_stablelm.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "6698273cfae358c628c30481a12167606fb5a628",
            "filename": "src/transformers/models/stablelm/modeling_stablelm.py",
            "status": "modified",
            "additions": 95,
            "deletions": 272,
            "changes": 367,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "9d87dd6eefa107dd1a2d62f65e85a27c63aeaa0b",
            "filename": "src/transformers/models/starcoder2/configuration_starcoder2.py",
            "status": "modified",
            "additions": 32,
            "deletions": 65,
            "changes": 97,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fconfiguration_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fconfiguration_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fconfiguration_starcoder2.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "6b93c18a3d17fc304f30bbcd39b29cbe9fa0e246",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 58,
            "deletions": 30,
            "changes": 88,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "2c0a27f81bdf7d633f82bed43dd0c8047d59b1bb",
            "filename": "src/transformers/models/starcoder2/modular_starcoder2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodular_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodular_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodular_starcoder2.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "bea8916d3e6bd30b8757ca5f2244bf673ca8b778",
            "filename": "src/transformers/models/t5gemma/configuration_t5gemma.py",
            "status": "modified",
            "additions": 43,
            "deletions": 33,
            "changes": 76,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fconfiguration_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fconfiguration_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fconfiguration_t5gemma.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "81ba072a2c726e235f7823e0c8cb9f58207b7430",
            "filename": "src/transformers/models/t5gemma/modeling_t5gemma.py",
            "status": "modified",
            "additions": 50,
            "deletions": 19,
            "changes": 69,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "6d49e5c241ad78c62b903fa83ad727d3e28d3bc7",
            "filename": "src/transformers/models/t5gemma/modular_t5gemma.py",
            "status": "modified",
            "additions": 46,
            "deletions": 42,
            "changes": 88,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "d50cf5ed93d7b2faa328371d437aa2802e227f06",
            "filename": "src/transformers/models/vaultgemma/configuration_vaultgemma.py",
            "status": "modified",
            "additions": 39,
            "deletions": 27,
            "changes": 66,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fvaultgemma%2Fconfiguration_vaultgemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fvaultgemma%2Fconfiguration_vaultgemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvaultgemma%2Fconfiguration_vaultgemma.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "51071e59997ba275c554bdb9556333b63e387f19",
            "filename": "src/transformers/models/vaultgemma/modeling_vaultgemma.py",
            "status": "modified",
            "additions": 43,
            "deletions": 15,
            "changes": 58,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fvaultgemma%2Fmodeling_vaultgemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fvaultgemma%2Fmodeling_vaultgemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvaultgemma%2Fmodeling_vaultgemma.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "e8b3a4ee6773d5a3001fe5b2e54cb2bab3b66c0a",
            "filename": "src/transformers/models/vaultgemma/modular_vaultgemma.py",
            "status": "modified",
            "additions": 30,
            "deletions": 27,
            "changes": 57,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fvaultgemma%2Fmodular_vaultgemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fvaultgemma%2Fmodular_vaultgemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvaultgemma%2Fmodular_vaultgemma.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "a144fbd589cff28a794d29f9fd2e55f08f66a778",
            "filename": "src/transformers/models/zamba/modeling_zamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "40e30822cf596ad9868eb2f1e028593d6a327aa4",
            "filename": "src/transformers/models/zamba2/configuration_zamba2.py",
            "status": "modified",
            "additions": 52,
            "deletions": 42,
            "changes": 94,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fzamba2%2Fconfiguration_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fzamba2%2Fconfiguration_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fconfiguration_zamba2.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "8f6efc7dbe1cf13aa979c725697d958c9d9b1971",
            "filename": "src/transformers/models/zamba2/modeling_zamba2.py",
            "status": "modified",
            "additions": 44,
            "deletions": 14,
            "changes": 58,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "b884e2b38e4a6c024a1e9d72fb0560bca115de6f",
            "filename": "src/transformers/models/zamba2/modular_zamba2.py",
            "status": "modified",
            "additions": 7,
            "deletions": 9,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodular_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodular_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodular_zamba2.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "5c25223428d631ab004f509d1763dd521eabc0f2",
            "filename": "tests/causal_lm_tester.py",
            "status": "modified",
            "additions": 19,
            "deletions": 9,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/tests%2Fcausal_lm_tester.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/tests%2Fcausal_lm_tester.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fcausal_lm_tester.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "5f53bd2d4cfb4e57dbc79d13a832e23d01d818f4",
            "filename": "tests/models/blt/test_modeling_blt.py",
            "status": "modified",
            "additions": 7,
            "deletions": 45,
            "changes": 52,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/tests%2Fmodels%2Fblt%2Ftest_modeling_blt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/tests%2Fmodels%2Fblt%2Ftest_modeling_blt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblt%2Ftest_modeling_blt.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "35ede458045ddb14152eb000319621322d57ad7a",
            "filename": "tests/models/chameleon/test_modeling_chameleon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 33,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/tests%2Fmodels%2Fchameleon%2Ftest_modeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/tests%2Fmodels%2Fchameleon%2Ftest_modeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fchameleon%2Ftest_modeling_chameleon.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "20fdb42a38900a3d050528756976e56fbb9bef2d",
            "filename": "tests/models/colqwen2/test_modeling_colqwen2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/tests%2Fmodels%2Fcolqwen2%2Ftest_modeling_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/tests%2Fmodels%2Fcolqwen2%2Ftest_modeling_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcolqwen2%2Ftest_modeling_colqwen2.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "344a653dd8abb9341fa23aa48e2fb2d3dde8ca93",
            "filename": "tests/models/cwm/test_configuration_cwm.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/tests%2Fmodels%2Fcwm%2Ftest_configuration_cwm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/tests%2Fmodels%2Fcwm%2Ftest_configuration_cwm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcwm%2Ftest_configuration_cwm.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "eaed2878bb3345ea01ce0570efa0294952ae38d1",
            "filename": "tests/models/cwm/test_modeling_cwm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/tests%2Fmodels%2Fcwm%2Ftest_modeling_cwm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/tests%2Fmodels%2Fcwm%2Ftest_modeling_cwm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcwm%2Ftest_modeling_cwm.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "5a9bdab52b6b91db90b3e855bf100c7c5691e794",
            "filename": "tests/models/dbrx/test_modeling_dbrx.py",
            "status": "modified",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/tests%2Fmodels%2Fdbrx%2Ftest_modeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/tests%2Fmodels%2Fdbrx%2Ftest_modeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdbrx%2Ftest_modeling_dbrx.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "5301ac7c1ff8e43283c91c745512ad52c28b5dd6",
            "filename": "tests/models/deepseek_v2/test_modeling_deepseek_v2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/tests%2Fmodels%2Fdeepseek_v2%2Ftest_modeling_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/tests%2Fmodels%2Fdeepseek_v2%2Ftest_modeling_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeepseek_v2%2Ftest_modeling_deepseek_v2.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "a5c696aed1de1d15f042b1def5ddd2a7bbd3eba4",
            "filename": "tests/models/deepseek_v3/test_modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 102,
            "changes": 103,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "90f72ab989c23528e46bf4f6810938e644186df0",
            "filename": "tests/models/diffllama/test_modeling_diffllama.py",
            "status": "modified",
            "additions": 11,
            "deletions": 111,
            "changes": 122,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/tests%2Fmodels%2Fdiffllama%2Ftest_modeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/tests%2Fmodels%2Fdiffllama%2Ftest_modeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdiffllama%2Ftest_modeling_diffllama.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "013315894067c2e27ad2ad8194b05ace005bf096",
            "filename": "tests/models/emu3/test_modeling_emu3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 42,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/tests%2Fmodels%2Femu3%2Ftest_modeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/tests%2Fmodels%2Femu3%2Ftest_modeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Femu3%2Ftest_modeling_emu3.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "cc23cd8ac67c2a1c5428ffb8f6e484efc2671432",
            "filename": "tests/models/gemma3/test_modeling_gemma3.py",
            "status": "modified",
            "additions": 97,
            "deletions": 0,
            "changes": 97,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "c279d98ac3c8842a74dc0c52d2da474afc724aee",
            "filename": "tests/models/gemma3n/test_modeling_gemma3n.py",
            "status": "modified",
            "additions": 91,
            "deletions": 0,
            "changes": 91,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/tests%2Fmodels%2Fgemma3n%2Ftest_modeling_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/tests%2Fmodels%2Fgemma3n%2Ftest_modeling_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3n%2Ftest_modeling_gemma3n.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "ddfd332e93471576cfed2c9e8fc58fbc00536a67",
            "filename": "tests/models/glm4v/test_modeling_glm4v.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/tests%2Fmodels%2Fglm4v%2Ftest_modeling_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/tests%2Fmodels%2Fglm4v%2Ftest_modeling_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fglm4v%2Ftest_modeling_glm4v.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "f291a868ab17e2eda6c0731eaaf36a7f11666d5c",
            "filename": "tests/models/glm4v_moe/test_modeling_glm4v_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/tests%2Fmodels%2Fglm4v_moe%2Ftest_modeling_glm4v_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/tests%2Fmodels%2Fglm4v_moe%2Ftest_modeling_glm4v_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fglm4v_moe%2Ftest_modeling_glm4v_moe.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "b64e3d2f912dca30ff8ca22b1effafc8f2b187ad",
            "filename": "tests/models/gpt_neox/test_modeling_gpt_neox.py",
            "status": "modified",
            "additions": 1,
            "deletions": 85,
            "changes": 86,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/tests%2Fmodels%2Fgpt_neox%2Ftest_modeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/tests%2Fmodels%2Fgpt_neox%2Ftest_modeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt_neox%2Ftest_modeling_gpt_neox.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "4afa595d6a1652553958415d6ab1e4228d0c125c",
            "filename": "tests/models/granite/test_modeling_granite.py",
            "status": "modified",
            "additions": 1,
            "deletions": 104,
            "changes": 105,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/tests%2Fmodels%2Fgranite%2Ftest_modeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/tests%2Fmodels%2Fgranite%2Ftest_modeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgranite%2Ftest_modeling_granite.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        },
        {
            "sha": "7d6bc3f6d21c16708bb8e57c0c15a0c141e38d32",
            "filename": "tests/models/granitemoe/test_modeling_granitemoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 104,
            "changes": 105,
            "blob_url": "https://github.com/huggingface/transformers/blob/10de06dacef06605be3296f776d4122697637d6c/tests%2Fmodels%2Fgranitemoe%2Ftest_modeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10de06dacef06605be3296f776d4122697637d6c/tests%2Fmodels%2Fgranitemoe%2Ftest_modeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgranitemoe%2Ftest_modeling_granitemoe.py?ref=10de06dacef06605be3296f776d4122697637d6c"
        }
    ],
    "stats": {
        "total": 23162,
        "additions": 12117,
        "deletions": 11045
    }
}