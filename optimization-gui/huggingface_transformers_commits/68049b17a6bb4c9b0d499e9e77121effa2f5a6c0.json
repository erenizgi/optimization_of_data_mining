{
    "author": "hikari-kubota-fixstars",
    "message": "Fix Megatron-LM tokenizer path (#33344)\n\n* Change Megatron-LM tokenizer path\r\n\r\n* Add version check\r\n\r\n* Fix code formatting issues\r\n\r\n* Check module importability using importlib.util\r\n\r\n* Fix code formatting issues\r\n\r\n* Use packaging library\r\n\r\n* Trigger CircleCI",
    "sha": "68049b17a6bb4c9b0d499e9e77121effa2f5a6c0",
    "files": [
        {
            "sha": "502aa78263649a4b69d9ece8532af4f4faf161f8",
            "filename": "src/transformers/models/megatron_gpt2/checkpoint_reshaping_and_interoperability.py",
            "status": "modified",
            "additions": 12,
            "deletions": 3,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/68049b17a6bb4c9b0d499e9e77121effa2f5a6c0/src%2Ftransformers%2Fmodels%2Fmegatron_gpt2%2Fcheckpoint_reshaping_and_interoperability.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/68049b17a6bb4c9b0d499e9e77121effa2f5a6c0/src%2Ftransformers%2Fmodels%2Fmegatron_gpt2%2Fcheckpoint_reshaping_and_interoperability.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmegatron_gpt2%2Fcheckpoint_reshaping_and_interoperability.py?ref=68049b17a6bb4c9b0d499e9e77121effa2f5a6c0",
            "patch": "@@ -13,13 +13,15 @@\n # limitations under the License.\n \n import argparse\n+import importlib.util\n import json\n import os\n import re\n import sys\n import types\n \n import torch\n+from packaging import version\n \n from transformers import AutoTokenizer, GPT2Config\n from transformers.modeling_utils import WEIGHTS_INDEX_NAME, WEIGHTS_NAME, shard_checkpoint\n@@ -606,9 +608,16 @@ def convert_checkpoint_from_transformers_to_megatron(args):\n     if args.megatron_path is not None:\n         sys.path.insert(0, args.megatron_path)\n \n-    try:\n-        from megatron.tokenizer.tokenizer import _vocab_size_with_padding\n-    except ModuleNotFoundError:\n+    megatron_exists = importlib.util.find_spec(\"megatron\") is not None\n+    if megatron_exists:\n+        from megatron.core import package_info\n+\n+        if version.parse(package_info.__version__) >= version.parse(\"0.6.0\"):\n+            from megatron.training.tokenizer.tokenizer import _vocab_size_with_padding\n+        else:\n+            from megatron.tokenizer.tokenizer import _vocab_size_with_padding\n+\n+    else:\n         print(\"Unable to import Megatron, please specify the path to Megatron using --megatron-path. Exiting.\")\n         exit(1)\n "
        }
    ],
    "stats": {
        "total": 15,
        "additions": 12,
        "deletions": 3
    }
}