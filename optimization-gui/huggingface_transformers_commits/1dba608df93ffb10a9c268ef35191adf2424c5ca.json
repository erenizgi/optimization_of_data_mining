{
    "author": "ArthurZucker",
    "message": "[`modular`] fixes!  (#33820)\n\n* fix converter for function definitions\r\n\r\n* small changes\r\n\r\n* no prints\r\n\r\n* style",
    "sha": "1dba608df93ffb10a9c268ef35191adf2424c5ca",
    "files": [
        {
            "sha": "3c7848e69569cc99c8b62f1e03ba3a17e62e0120",
            "filename": "examples/modular-transformers/configuration_my_new_model.py",
            "status": "modified",
            "additions": 9,
            "deletions": 8,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/1dba608df93ffb10a9c268ef35191adf2424c5ca/examples%2Fmodular-transformers%2Fconfiguration_my_new_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1dba608df93ffb10a9c268ef35191adf2424c5ca/examples%2Fmodular-transformers%2Fconfiguration_my_new_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fconfiguration_my_new_model.py?ref=1dba608df93ffb10a9c268ef35191adf2424c5ca",
            "patch": "@@ -1,8 +1,8 @@\n #           ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n-#               This file was automatically generated from <path_to_diff_file.py>.\n+#               This file was automatically generated from <path_to_modular_file.py>.\n #         Do NOT edit this file manually as any edits will be overwritten by the generation of\n-#         the file from the diff. If any change should be done, please apply the change to the\n-#                           diff.py file directly. One of our CI enforces this\n+#         the file from the modular. If any change should be done, please apply the change to the\n+#                           modular_xxx.py file directly. One of our CI enforces this\n #           ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n \n from ...configuration_utils import PretrainedConfig\n@@ -111,8 +111,6 @@ class MyNewModelConfig(PretrainedConfig):\n             Whether to use a bias in up_proj, down_proj and gate_proj layers in the MLP layers.\n         head_dim (`int`, *optional*):\n             The attention head dimension. If None, it will default to hidden_size // num_heads\n-        new_param (`int`, *optional*, defaults to `False`):\n-            A fun new parameter\n \n     ```python\n     >>> from transformers import MyNewModelModel, MyNewModelConfig\n@@ -125,7 +123,10 @@ class MyNewModelConfig(PretrainedConfig):\n \n     >>> # Accessing the model configuration\n     >>> configuration = model.config\n-    ```\"\"\"\n+    ```\n+        new_param (`int`, *optional*, defaults to `False`):\n+            A fun new parameter\n+    \"\"\"\n \n     model_type = \"my_new_model\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n@@ -178,12 +179,14 @@ def __init__(\n         self.rope_scaling = rope_scaling\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n+        self.mlp_bias = mlp_bias\n         self.head_dim = head_dim if head_dim is not None else self.hidden_size // self.num_attention_heads\n         # Validate the correctness of rotary position embeddings parameters\n         # BC: if there is a 'type' field, copy it it to 'rope_type'.\n         if self.rope_scaling is not None and \"type\" in self.rope_scaling:\n             self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n         rope_config_validation(self)\n+        self.new_param = new_param\n \n         super().__init__(\n             pad_token_id=pad_token_id,\n@@ -192,5 +195,3 @@ def __init__(\n             tie_word_embeddings=tie_word_embeddings,\n             **kwargs,\n         )\n-        self.mlp_bias = mlp_bias\n-        self.new_param = new_param"
        },
        {
            "sha": "5fef1cecc702d71640d502f548afa6c756ad652a",
            "filename": "examples/modular-transformers/configuration_my_new_model2.py",
            "status": "modified",
            "additions": 106,
            "deletions": 4,
            "changes": 110,
            "blob_url": "https://github.com/huggingface/transformers/blob/1dba608df93ffb10a9c268ef35191adf2424c5ca/examples%2Fmodular-transformers%2Fconfiguration_my_new_model2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1dba608df93ffb10a9c268ef35191adf2424c5ca/examples%2Fmodular-transformers%2Fconfiguration_my_new_model2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fconfiguration_my_new_model2.py?ref=1dba608df93ffb10a9c268ef35191adf2424c5ca",
            "patch": "@@ -1,15 +1,116 @@\n #           ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n-#               This file was automatically generated from <path_to_diff_file.py>.\n+#               This file was automatically generated from <path_to_modular_file.py>.\n #         Do NOT edit this file manually as any edits will be overwritten by the generation of\n-#         the file from the diff. If any change should be done, please apply the change to the\n-#                           diff.py file directly. One of our CI enforces this\n+#         the file from the modular. If any change should be done, please apply the change to the\n+#                           modular_xxx.py file directly. One of our CI enforces this\n #           ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n+\n from ...configuration_utils import PretrainedConfig\n from ...modeling_rope_utils import rope_config_validation\n \n \n class MyNewModel2Config(PretrainedConfig):\n     r\"\"\"\n+    This is the configuration class to store the configuration of a [`MyNewModel2Model`]. It is used to instantiate an MyNewModel2\n+    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n+    defaults will yield a similar configuration to that of the MyNewModel2-7B.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+\n+    Args:\n+        vocab_size (`int`, *optional*, defaults to 32000):\n+            Vocabulary size of the MyNewModel2 model. Defines the number of different tokens that can be represented by the\n+            `inputs_ids` passed when calling [`MyNewModel2Model`]\n+        hidden_size (`int`, *optional*, defaults to 4096):\n+            Dimension of the hidden representations.\n+        intermediate_size (`int`, *optional*, defaults to 11008):\n+            Dimension of the MLP representations.\n+        num_hidden_layers (`int`, *optional*, defaults to 32):\n+            Number of hidden layers in the Transformer decoder.\n+        num_attention_heads (`int`, *optional*, defaults to 32):\n+            Number of attention heads for each attention layer in the Transformer decoder.\n+        num_key_value_heads (`int`, *optional*):\n+            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n+            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n+            `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n+            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n+            by meanpooling all the original heads within that group. For more details checkout [this\n+            paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n+            `num_attention_heads`.\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n+            The non-linear activation function (function or string) in the decoder.\n+        max_position_embeddings (`int`, *optional*, defaults to 2048):\n+            The maximum sequence length that this model might ever be used with. MyNewModel2 1 supports up to 2048 tokens,\n+            MyNewModel2 2 up to 4096, CodeMyNewModel2 up to 16384.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        rms_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            The epsilon used by the rms normalization layers.\n+        use_cache (`bool`, *optional*, defaults to `True`):\n+            Whether or not the model should return the last key/values attentions (not used by all models). Only\n+            relevant if `config.is_decoder=True`.\n+        pad_token_id (`int`, *optional*):\n+            Padding token id.\n+        bos_token_id (`int`, *optional*, defaults to 1):\n+            Beginning of stream token id.\n+        eos_token_id (`int`, *optional*, defaults to 2):\n+            End of stream token id.\n+        pretraining_tp (`int`, *optional*, defaults to 1):\n+            Experimental feature. Tensor parallelism rank used during pretraining. Please refer to [this\n+            document](https://huggingface.co/docs/transformers/main/perf_train_gpu_many#tensor-parallelism) to\n+            understand more about it. This value is necessary to ensure exact reproducibility of the pretraining\n+            results. Please refer to [this issue](https://github.com/pytorch/pytorch/issues/76232).\n+        tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n+            Whether to tie weight embeddings\n+        rope_theta (`float`, *optional*, defaults to 10000.0):\n+            The base period of the RoPE embeddings.\n+        rope_scaling (`Dict`, *optional*):\n+            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n+            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n+            accordingly.\n+            Expected contents:\n+                `rope_type` (`str`):\n+                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n+                    'my_new_model23'], with 'default' being the original RoPE implementation.\n+                `factor` (`float`, *optional*):\n+                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n+                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n+                    original maximum pre-trained length.\n+                `original_max_position_embeddings` (`int`, *optional*):\n+                    Used with 'dynamic', 'longrope' and 'my_new_model23'. The original max position embeddings used during\n+                    pretraining.\n+                `attention_factor` (`float`, *optional*):\n+                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n+                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n+                    `factor` field to infer the suggested value.\n+                `beta_fast` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 32.\n+                `beta_slow` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 1.\n+                `short_factor` (`List[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `long_factor` (`List[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `low_freq_factor` (`float`, *optional*):\n+                    Only used with 'my_new_model23'. Scaling factor applied to low frequency components of the RoPE\n+                `high_freq_factor` (`float`, *optional*):\n+                    Only used with 'my_new_model23'. Scaling factor applied to high frequency components of the RoPE\n+        attention_bias (`bool`, *optional*, defaults to `False`):\n+            Whether to use a bias in the query, key, value and output projection layers during self-attention.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        mlp_bias (`bool`, *optional*, defaults to `False`):\n+            Whether to use a bias in up_proj, down_proj and gate_proj layers in the MLP layers.\n+        head_dim (`int`, *optional*):\n+            The attention head dimension. If None, it will default to hidden_size // num_heads\n     This is the configuration class to store the configuration of a [`GemmaModel`]. It is used to instantiate an Gemma\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of the Gemma-7B.\n@@ -20,6 +121,7 @@ class MyNewModel2Config(PretrainedConfig):\n         vocab_size (`int`, *optional*, defaults to 256000):\n             Vocabulary size of the Gemma model. Defines the number of different tokens that can be represented by the\n             `inputs_ids` passed when calling [`GemmaModel`]\n+\n     ```python\n     >>> from transformers import GemmaModel, GemmaConfig\n     >>> # Initializing a Gemma gemma-7b style configuration\n@@ -83,7 +185,7 @@ def __init__(\n         self.mlp_bias = mlp_bias\n         self.head_dim = head_dim if head_dim is not None else self.hidden_size // self.num_attention_heads\n         # Validate the correctness of rotary position embeddings parameters\n-        # BC: if there is a 'type' field, move it to 'rope_type'.\n+        # BC: if there is a 'type' field, copy it it to 'rope_type'.\n         if self.rope_scaling is not None and \"type\" in self.rope_scaling:\n             self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n         rope_config_validation(self)"
        },
        {
            "sha": "8bc8ef52cee62f612798732bed5ab14a3c58b9b0",
            "filename": "examples/modular-transformers/configuration_new_model.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/1dba608df93ffb10a9c268ef35191adf2424c5ca/examples%2Fmodular-transformers%2Fconfiguration_new_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1dba608df93ffb10a9c268ef35191adf2424c5ca/examples%2Fmodular-transformers%2Fconfiguration_new_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fconfiguration_new_model.py?ref=1dba608df93ffb10a9c268ef35191adf2424c5ca",
            "patch": "@@ -1,12 +1,12 @@\n #           ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n-#               This file was automatically generated from <path_to_diff_file.py>.\n+#               This file was automatically generated from <path_to_modular_file.py>.\n #         Do NOT edit this file manually as any edits will be overwritten by the generation of\n-#         the file from the diff. If any change should be done, please apply the change to the\n-#                           diff.py file directly. One of our CI enforces this\n+#         the file from the modular. If any change should be done, please apply the change to the\n+#                           modular_xxx.py file directly. One of our CI enforces this\n #           ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n # Example where we only want to overwrite the defaults of an init\n \n-from transformers import PretrainedConfig\n+from ...configuration_utils import PretrainedConfig\n \n \n class NewModelConfig(PretrainedConfig):"
        },
        {
            "sha": "49666ab1154d33c0d06930c01aa6d43b4a963d74",
            "filename": "examples/modular-transformers/convert_examples.sh",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1dba608df93ffb10a9c268ef35191adf2424c5ca/examples%2Fmodular-transformers%2Fconvert_examples.sh",
            "raw_url": "https://github.com/huggingface/transformers/raw/1dba608df93ffb10a9c268ef35191adf2424c5ca/examples%2Fmodular-transformers%2Fconvert_examples.sh",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fconvert_examples.sh?ref=1dba608df93ffb10a9c268ef35191adf2424c5ca",
            "patch": "@@ -5,6 +5,6 @@ for file in examples/modular-transformers/modular_*; do\n     # Check if it's a regular file\n     if [ -f \"$file\" ]; then\n         # Call the Python script with the file name as an argument\n-        python utils/diff_model_converter.py --files_to_parse \"$file\"\n+        python utils/modular_model_converter.py --files_to_parse \"$file\"\n     fi\n done\n\\ No newline at end of file"
        },
        {
            "sha": "c67787fbd8a69ee837f8e05e64eba03afdd02131",
            "filename": "examples/modular-transformers/modeling_dummy.py",
            "status": "modified",
            "additions": 28,
            "deletions": 30,
            "changes": 58,
            "blob_url": "https://github.com/huggingface/transformers/blob/1dba608df93ffb10a9c268ef35191adf2424c5ca/examples%2Fmodular-transformers%2Fmodeling_dummy.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1dba608df93ffb10a9c268ef35191adf2424c5ca/examples%2Fmodular-transformers%2Fmodeling_dummy.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_dummy.py?ref=1dba608df93ffb10a9c268ef35191adf2424c5ca",
            "patch": "@@ -1,11 +1,11 @@\n #           ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n-#               This file was automatically generated from <path_to_diff_file.py>.\n+#               This file was automatically generated from <path_to_modular_file.py>.\n #         Do NOT edit this file manually as any edits will be overwritten by the generation of\n-#         the file from the diff. If any change should be done, please apply the change to the\n-#                           diff.py file directly. One of our CI enforces this\n+#         the file from the modular. If any change should be done, please apply the change to the\n+#                           modular_xxx.py file directly. One of our CI enforces this\n #           ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n+\n import math\n-from math import log\n from typing import List, Optional, Tuple, Union\n \n import torch\n@@ -31,11 +31,6 @@\n from .configuration_dummy import DummyConfig\n \n \n-def _pre_process_input(input_ids):\n-    print(log(input_ids))\n-    return input_ids\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -129,7 +124,7 @@ def __init__(\n         if config is None:\n             logger.warning_once(\n                 \"`DummyRotaryEmbedding` can now be fully parameterized by passing the model config through the \"\n-                \"`config` argument. All other arguments will be removed in v4.45\"\n+                \"`config` argument. All other arguments will be removed in v4.46\"\n             )\n             self.rope_kwargs = {\n                 \"rope_type\": rope_type,\n@@ -201,8 +196,8 @@ def forward(self, x, position_ids):\n \n def rotate_half(x):\n     \"\"\"Rotates half the hidden dims of the input.\"\"\"\n-    x1 = x[..., : x.shape[-1] // 2]\n-    x2 = x[..., x.shape[-1] // 2 :]\n+    x1 = x[..., : x.shape[-1] // 4]\n+    x2 = x[..., x.shape[-1] // 4 :]\n     return torch.cat((-x2, x1), dim=-1)\n \n \n@@ -308,7 +303,7 @@ def __init__(self, config: DummyConfig, layer_idx: Optional[int] = None):\n         self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n         self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=config.attention_bias)\n \n-        # TODO (joao): remove in v4.45 (RoPE is computed in the model, not in the decoder layers)\n+        # TODO (joao): remove in v4.46 (RoPE is computed in the model, not in the decoder layers)\n         self.rotary_emb = DummyRotaryEmbedding(config=self.config)\n \n     def forward(\n@@ -320,7 +315,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.45\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n         **kwargs,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         bsz, q_len, _ = hidden_states.size()\n@@ -355,7 +350,7 @@ def forward(\n             logger.warning_once(\n                 \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n                 \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n-                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.45 `position_ids` will be \"\n+                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n                 \"removed and `position_embeddings` will be mandatory.\"\n             )\n             cos, sin = self.rotary_emb(value_states, position_ids)\n@@ -428,7 +423,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.45\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         if isinstance(past_key_value, StaticCache):\n             raise ValueError(\n@@ -455,7 +450,7 @@ def forward(\n             logger.warning_once(\n                 \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n                 \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n-                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.45 `position_ids` will be \"\n+                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n                 \"removed and `position_embeddings` will be mandatory.\"\n             )\n             cos, sin = self.rotary_emb(value_states, position_ids)\n@@ -541,7 +536,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.45\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n         **kwargs,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         if output_attentions:\n@@ -575,7 +570,7 @@ def forward(\n             logger.warning_once(\n                 \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n                 \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n-                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.45 `position_ids` will be \"\n+                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n                 \"removed and `position_embeddings` will be mandatory.\"\n             )\n             cos, sin = self.rotary_emb(value_states, position_ids)\n@@ -650,7 +645,7 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.45\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n         **kwargs,\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n@@ -796,7 +791,8 @@ def _init_weights(self, module):\n             returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n \n             Two formats are allowed:\n-            - a [`~cache_utils.Cache`] instance;\n+            - a [`~cache_utils.Cache`] instance, see our\n+            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n             - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n             shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n             cache format.\n@@ -877,7 +873,6 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n     ) -> Union[Tuple, BaseModelOutputWithPast]:\n-        input_ids = _pre_process_input(input_ids)\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -899,16 +894,19 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n+        # kept for BC (non `Cache` `past_key_values` inputs)\n         return_legacy_cache = False\n-        if (\n-            use_cache and not isinstance(past_key_values, Cache) and not self.training\n-        ):  # kept for BC (non `Cache` `past_key_values` inputs)\n+        if use_cache and not isinstance(past_key_values, Cache):\n             return_legacy_cache = True\n-            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n-            logger.warning_once(\n-                \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. \"\n-                \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/internal/generation_utils#transformers.Cache)\"\n-            )\n+            if past_key_values is None:\n+                past_key_values = DynamicCache()\n+            else:\n+                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n+                logger.warning_once(\n+                    \"We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and \"\n+                    \"will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class \"\n+                    \"(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\"\n+                )\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "611d7be961f7e419b3cf7cc6bed759cd996b7591",
            "filename": "examples/modular-transformers/modeling_dummy_bert.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/1dba608df93ffb10a9c268ef35191adf2424c5ca/examples%2Fmodular-transformers%2Fmodeling_dummy_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1dba608df93ffb10a9c268ef35191adf2424c5ca/examples%2Fmodular-transformers%2Fmodeling_dummy_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_dummy_bert.py?ref=1dba608df93ffb10a9c268ef35191adf2424c5ca",
            "patch": "@@ -1,8 +1,8 @@\n #           ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n-#               This file was automatically generated from <path_to_diff_file.py>.\n+#               This file was automatically generated from <path_to_modular_file.py>.\n #         Do NOT edit this file manually as any edits will be overwritten by the generation of\n-#         the file from the diff. If any change should be done, please apply the change to the\n-#                           diff.py file directly. One of our CI enforces this\n+#         the file from the modular. If any change should be done, please apply the change to the\n+#                           modular_xxx.py file directly. One of our CI enforces this\n #           ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n import math\n import os\n@@ -1027,6 +1027,7 @@ def forward(\n \n         if not return_dict:\n             return (sequence_output, pooled_output) + encoder_outputs[1:]\n+        return super().forward(input_ids)\n \n         return BaseModelOutputWithPoolingAndCrossAttentions(\n             last_hidden_state=sequence_output,"
        },
        {
            "sha": "5484b3890fbdc4f9b576195c9698cfcb83cf43d6",
            "filename": "examples/modular-transformers/modeling_my_new_model2.py",
            "status": "modified",
            "additions": 155,
            "deletions": 159,
            "changes": 314,
            "blob_url": "https://github.com/huggingface/transformers/blob/1dba608df93ffb10a9c268ef35191adf2424c5ca/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1dba608df93ffb10a9c268ef35191adf2424c5ca/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py?ref=1dba608df93ffb10a9c268ef35191adf2424c5ca",
            "patch": "@@ -1,8 +1,8 @@\n #           ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n-#               This file was automatically generated from <path_to_diff_file.py>.\n+#               This file was automatically generated from <path_to_modular_file.py>.\n #         Do NOT edit this file manually as any edits will be overwritten by the generation of\n-#         the file from the diff. If any change should be done, please apply the change to the\n-#                           diff.py file directly. One of our CI enforces this\n+#         the file from the modular. If any change should be done, please apply the change to the\n+#                           modular_xxx.py file directly. One of our CI enforces this\n #           ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n import math\n from typing import List, Optional, Tuple, Union\n@@ -30,63 +30,6 @@\n from .configuration_my_new_model2 import MyNewModel2Config\n \n \n-logger = logging.get_logger(__name__)\n-\n-\n-# Copied from transformers.models.llama.modeling_llama._prepare_4d_causal_attention_mask_with_cache_position\n-def _prepare_4d_causal_attention_mask_with_cache_position(\n-    attention_mask: torch.Tensor,\n-    sequence_length: int,\n-    target_length: int,\n-    dtype: torch.dtype,\n-    device: torch.device,\n-    min_dtype: float,\n-    cache_position: torch.Tensor,\n-    batch_size: int,\n-):\n-    \"\"\"\n-    Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-    `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-    Args:\n-        attention_mask (`torch.Tensor`):\n-            A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape `(batch_size, 1, query_length, key_value_length)`.\n-        sequence_length (`int`):\n-            The sequence length being processed.\n-        target_length (`int`):\n-            The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.\n-        dtype (`torch.dtype`):\n-            The dtype to use for the 4D attention mask.\n-        device (`torch.device`):\n-            The device to plcae the 4D attention mask on.\n-        min_dtype (`float`):\n-            The minimum value representable with the dtype `dtype`.\n-        cache_position (`torch.Tensor`):\n-            Indices depicting the position of the input sequence tokens in the sequence.\n-        batch_size (`torch.Tensor`):\n-            Batch size.\n-    \"\"\"\n-    if attention_mask is not None and attention_mask.dim() == 4:\n-        # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-        causal_mask = attention_mask\n-    else:\n-        causal_mask = torch.full((sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device)\n-        if sequence_length != 1:\n-            causal_mask = torch.triu(causal_mask, diagonal=1)\n-        causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n-        causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-        if attention_mask is not None:\n-            causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-            mask_length = attention_mask.shape[-1]\n-            padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n-            padding_mask = padding_mask == 0\n-            causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                padding_mask, min_dtype\n-            )\n-\n-    return causal_mask\n-\n-\n class MyNewModel2RMSNorm(nn.Module):\n     def __init__(self, dim: int, eps: float = 1e-6):\n         super().__init__()\n@@ -107,6 +50,9 @@ def extra_repr(self):\n         return f\"{tuple(self.weight.shape)}, eps={self.eps}\"\n \n \n+logger = logging.get_logger(__name__)\n+\n+\n class MyNewModel2RotaryEmbedding(nn.Module):\n     def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n         super().__init__()\n@@ -305,6 +251,94 @@ def forward(\n         return attn_output, attn_weights, past_key_value\n \n \n+class MyNewModel2SdpaAttention(MyNewModel2Attention):\n+    \"\"\"\n+    MyNewModel2 attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n+    `MyNewModel2Attention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n+    SDPA API.\n+    \"\"\"\n+\n+    # Adapted from MyNewModel2Attention.forward\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_value: Optional[Cache] = None,\n+        output_attentions: bool = False,\n+        use_cache: bool = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+        if output_attentions:\n+            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n+            logger.warning_once(\n+                \"MyNewModel2Model is using MyNewModel2SdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n+                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+            )\n+            return super().forward(\n+                hidden_states=hidden_states,\n+                attention_mask=attention_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_value,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+            )\n+\n+        bsz, q_len, _ = hidden_states.size()\n+\n+        query_states = self.q_proj(hidden_states)\n+        key_states = self.k_proj(hidden_states)\n+        value_states = self.v_proj(hidden_states)\n+\n+        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n+        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+\n+        cos, sin = self.rotary_emb(value_states, position_ids)\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        if past_key_value is not None:\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        key_states = repeat_kv(key_states, self.num_key_value_groups)\n+        value_states = repeat_kv(value_states, self.num_key_value_groups)\n+\n+        causal_mask = attention_mask\n+        if attention_mask is not None:\n+            causal_mask = causal_mask[:, :, :, : key_states.shape[-2]]\n+\n+        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n+        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n+        if query_states.device.type == \"cuda\" and causal_mask is not None:\n+            query_states = query_states.contiguous()\n+            key_states = key_states.contiguous()\n+            value_states = value_states.contiguous()\n+\n+        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n+        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n+        is_causal = True if causal_mask is None and q_len > 1 else False\n+\n+        attn_output = torch.nn.functional.scaled_dot_product_attention(\n+            query_states,\n+            key_states,\n+            value_states,\n+            attn_mask=causal_mask,\n+            dropout_p=self.attention_dropout if self.training else 0.0,\n+            is_causal=is_causal,\n+        )\n+\n+        attn_output = attn_output.transpose(1, 2).contiguous()\n+        attn_output = attn_output.view(bsz, q_len, -1)\n+\n+        attn_output = self.o_proj(attn_output)\n+\n+        return attn_output, None, past_key_value\n+\n+\n class MyNewModel2FlashAttention2(MyNewModel2Attention):\n     \"\"\"\n     MyNewModel2 flash attention module. This module inherits from `MyNewModel2Attention` as the weights of the module stays\n@@ -405,7 +439,6 @@ def forward(\n             is_causal=self.is_causal,\n             use_top_left_mask=self._flash_attn_uses_top_left_mask,\n         )\n-\n         attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()\n         attn_output = self.o_proj(attn_output)\n \n@@ -415,92 +448,57 @@ def forward(\n         return attn_output, attn_weights, past_key_value\n \n \n-class MyNewModel2SdpaAttention(MyNewModel2Attention):\n-    \"\"\"\n-    MyNewModel2 attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n-    `MyNewModel2Attention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n-    SDPA API.\n+def _prepare_4d_causal_attention_mask_with_cache_position(\n+    attention_mask: torch.Tensor,\n+    sequence_length: int,\n+    target_length: int,\n+    dtype: torch.dtype,\n+    device: torch.device,\n+    min_dtype: float,\n+    cache_position: torch.Tensor,\n+    batch_size: int,\n+):\n     \"\"\"\n+    Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+    `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n \n-    # Adapted from MyNewModel2Attention.forward\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        if output_attentions:\n-            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n-            logger.warning_once(\n-                \"MyNewModel2Model is using MyNewModel2SdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n-                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states=hidden_states,\n-                attention_mask=attention_mask,\n-                position_ids=position_ids,\n-                past_key_value=past_key_value,\n-                output_attentions=output_attentions,\n-                use_cache=use_cache,\n-                cache_position=cache_position,\n-            )\n-\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-\n-        cos, sin = self.rotary_emb(value_states, position_ids)\n-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n-\n-        if past_key_value is not None:\n-            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n-            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n-\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-\n+    Args:\n+        attention_mask (`torch.Tensor`):\n+            A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape `(batch_size, 1, query_length, key_value_length)`.\n+        sequence_length (`int`):\n+            The sequence length being processed.\n+        target_length (`int`):\n+            The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.\n+        dtype (`torch.dtype`):\n+            The dtype to use for the 4D attention mask.\n+        device (`torch.device`):\n+            The device to plcae the 4D attention mask on.\n+        min_dtype (`float`):\n+            The minimum value representable with the dtype `dtype`.\n+        cache_position (`torch.Tensor`):\n+            Indices depicting the position of the input sequence tokens in the sequence.\n+        batch_size (`torch.Tensor`):\n+            Batch size.\n+    \"\"\"\n+    if attention_mask is not None and attention_mask.dim() == 4:\n+        # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n         causal_mask = attention_mask\n+    else:\n+        causal_mask = torch.full((sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device)\n+        if sequence_length != 1:\n+            causal_mask = torch.triu(causal_mask, diagonal=1)\n+        causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+        causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n         if attention_mask is not None:\n-            causal_mask = causal_mask[:, :, :, : key_states.shape[-2]]\n-\n-        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n-        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n-        if query_states.device.type == \"cuda\" and causal_mask is not None:\n-            query_states = query_states.contiguous()\n-            key_states = key_states.contiguous()\n-            value_states = value_states.contiguous()\n-\n-        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n-        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        is_causal = True if causal_mask is None and q_len > 1 else False\n-\n-        attn_output = torch.nn.functional.scaled_dot_product_attention(\n-            query_states,\n-            key_states,\n-            value_states,\n-            attn_mask=causal_mask,\n-            dropout_p=self.attention_dropout if self.training else 0.0,\n-            is_causal=is_causal,\n-        )\n-\n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-        attn_output = attn_output.view(bsz, q_len, -1)\n-\n-        attn_output = self.o_proj(attn_output)\n+            causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+            mask_length = attention_mask.shape[-1]\n+            padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+            padding_mask = padding_mask == 0\n+            causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                padding_mask, min_dtype\n+            )\n \n-        return attn_output, None, past_key_value\n+    return causal_mask\n \n \n MY_NEW_MODEL2_ATTENTION_CLASSES = {\n@@ -514,11 +512,9 @@ class MyNewModel2DecoderLayer(nn.Module):\n     def __init__(self, config: MyNewModel2Config, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n-\n         self.self_attn = MY_NEW_MODEL2_ATTENTION_CLASSES[config._attn_implementation](\n             config=config, layer_idx=layer_idx\n         )\n-\n         self.mlp = MyNewModel2MLP(config)\n         self.input_layernorm = MyNewModel2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = MyNewModel2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n@@ -673,7 +669,8 @@ def _init_weights(self, module):\n             returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n \n             Two formats are allowed:\n-            - a [`~cache_utils.Cache`] instance;\n+            - a [`~cache_utils.Cache`] instance, see our\n+            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n             - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n             shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n             cache format.\n@@ -774,12 +771,19 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n+        # kept for BC (non `Cache` `past_key_values` inputs)\n         return_legacy_cache = False  # noqa: F841\n-        if (\n-            use_cache and not isinstance(past_key_values, Cache) and not self.training\n-        ):  # kept for BC (non `Cache` `past_key_values` inputs)\n+        if use_cache and not isinstance(past_key_values, Cache):\n             return_legacy_cache = True  # noqa: F841\n-            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n+            if past_key_values is None:\n+                past_key_values = DynamicCache()\n+            else:\n+                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n+                logger.warning_once(\n+                    \"We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and \"\n+                    \"will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class \"\n+                    \"(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\"\n+                )\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n@@ -802,15 +806,6 @@ def forward(\n         # See https://github.com/huggingface/transformers/pull/29402\n         normalizer = torch.tensor(self.config.hidden_size**0.5, dtype=hidden_states.dtype)\n         hidden_states = hidden_states * normalizer\n-        if (\n-            use_cache and not isinstance(past_key_values, Cache) and not self.training\n-        ):  # kept for BC (non `Cache` `past_key_values` inputs)\n-            return_legacy_cache = True\n-            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n-            logger.warning_once(\n-                \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. \"\n-                \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/internal/generation_utils#transformers.Cache)\"\n-            )\n \n         # decoder layers\n         all_hidden_states = () if output_hidden_states else None\n@@ -922,6 +917,7 @@ def _update_causal_mask(\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n         )\n+\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n@@ -970,7 +966,7 @@ def set_input_embeddings(self, value):\n     @add_start_docstrings_to_model_forward(MY_NEW_MODEL2_INPUTS_DOCSTRING)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,"
        },
        {
            "sha": "fb64ba4d8566655604001f44bea2ed34363b7d3b",
            "filename": "examples/modular-transformers/modular_dummy.py",
            "status": "modified",
            "additions": 6,
            "deletions": 36,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/1dba608df93ffb10a9c268ef35191adf2424c5ca/examples%2Fmodular-transformers%2Fmodular_dummy.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1dba608df93ffb10a9c268ef35191adf2424c5ca/examples%2Fmodular-transformers%2Fmodular_dummy.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodular_dummy.py?ref=1dba608df93ffb10a9c268ef35191adf2424c5ca",
            "patch": "@@ -1,45 +1,15 @@\n-from math import log\n-from typing import List, Optional, Tuple, Union\n-\n import torch\n \n-from transformers.modeling_outputs import CausalLMOutputWithPast\n from transformers.models.llama.modeling_llama import LlamaModel\n \n-from ...cache_utils import Cache\n-\n \n-def _pre_process_input(input_ids):\n-    print(log(input_ids))\n-    return input_ids\n+def rotate_half(x):\n+    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n+    x1 = x[..., : x.shape[-1] // 4]\n+    x2 = x[..., x.shape[-1] // 4 :]\n+    return torch.cat((-x2, x1), dim=-1)\n \n \n # example where we need some deps and some functions\n class DummyModel(LlamaModel):\n-    def forward(\n-        self,\n-        input_ids: torch.LongTensor = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n-    ) -> Union[Tuple, CausalLMOutputWithPast]:\n-        input_ids = _pre_process_input(input_ids)\n-\n-        return super().forward(\n-            None,\n-            attention_mask,\n-            position_ids,\n-            past_key_values,\n-            inputs_embeds,\n-            use_cache,\n-            output_attentions,\n-            output_hidden_states,\n-            return_dict,\n-            cache_position,\n-        )\n+    pass"
        },
        {
            "sha": "1bfc1230a913494b0a6a74a27ec7f38342a4fa8b",
            "filename": "utils/modular_model_converter.py",
            "status": "modified",
            "additions": 9,
            "deletions": 2,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/1dba608df93ffb10a9c268ef35191adf2424c5ca/utils%2Fmodular_model_converter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1dba608df93ffb10a9c268ef35191adf2424c5ca/utils%2Fmodular_model_converter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fmodular_model_converter.py?ref=1dba608df93ffb10a9c268ef35191adf2424c5ca",
            "patch": "@@ -537,7 +537,7 @@ def __init__(self, python_module, new_name, given_old_name=None, given_new_name=\n             \"feature_extractor\": {},\n         }\n         self.match_patterns = \"|\".join(self.files.keys())\n-        self.all_functions = {}\n+        self.all_definitions = {}\n \n     def visit_ImportFrom(self, node: cst.ImportFrom) -> None:\n         \"\"\"When visiting imports from `transformers.models.xxx` we need to:\n@@ -647,6 +647,7 @@ def leave_ClassDef(self, original_node, updated_node):\n                 node = class_finder.global_nodes.get(dependency, None)\n                 if node is not None:\n                     if dependency not in file_to_update:\n+                        node = self.all_definitions.get(dependency, node)\n                         start_insert_idx -= 1\n                         file_to_update[dependency] = {\"insert_idx\": start_insert_idx, \"node\": node}\n                     elif dependency not in self.inserted_deps:\n@@ -683,6 +684,12 @@ def leave_ClassDef(self, original_node, updated_node):\n             self.files[\"modeling\"][class_name] = {\"insert_idx\": self.global_scope_index, \"node\": updated_node}\n         return updated_node\n \n+    def leave_FunctionDef(self, original_node, node):\n+        parent_node = self.get_metadata(cst.metadata.ParentNodeProvider, original_node)\n+        if m.matches(parent_node, m.Module()):\n+            self.all_definitions[node.name.value] = node\n+        return node\n+\n     def leave_If(self, original_node, node):\n         parent_node = self.get_metadata(cst.metadata.ParentNodeProvider, original_node)\n         if m.matches(parent_node, m.Module()):\n@@ -757,7 +764,7 @@ def save_modeling_file(modular_file, converted_file):\n     parser = argparse.ArgumentParser()\n     parser.add_argument(\n         \"--files_to_parse\",\n-        default=[\"all\"],\n+        default=[\"examples/modular-transformers/modular_dummy.py\"],\n         nargs=\"+\",\n         help=\"A list of `modular_xxxx` files that should be converted to single model file\",\n     )"
        }
    ],
    "stats": {
        "total": 569,
        "additions": 322,
        "deletions": 247
    }
}