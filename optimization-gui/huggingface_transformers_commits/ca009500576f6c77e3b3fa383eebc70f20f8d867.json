{
    "author": "itsskofficial",
    "message": "added logic for deleting adapters once loaded (#34650)\n\n* added logic for deleting adapters once loaded\r\n\r\n* updated to the latest version of transformers, merged utility function into the source\r\n\r\n* updated with missing check\r\n\r\n* added peft version check\r\n\r\n* Apply suggestions from code review\r\n\r\nCo-authored-by: Anton Vlasjuk <73884904+vasqu@users.noreply.github.com>\r\n\r\n* changes according to reviewer\r\n\r\n* added test for deleting adapter(s)\r\n\r\n* styling changes\r\n\r\n* styling changes in test\r\n\r\n* removed redundant code\r\n\r\n* formatted my contributions with ruff\r\n\r\n* optimized error handling\r\n\r\n* ruff formatted with correct config\r\n\r\n* resolved formatting issues\r\n\r\n---------\r\n\r\nCo-authored-by: Anton Vlasjuk <73884904+vasqu@users.noreply.github.com>",
    "sha": "ca009500576f6c77e3b3fa383eebc70f20f8d867",
    "files": [
        {
            "sha": "791528a629acf431264634cfc7ae351604377388",
            "filename": "src/transformers/integrations/peft.py",
            "status": "modified",
            "additions": 62,
            "deletions": 0,
            "changes": 62,
            "blob_url": "https://github.com/huggingface/transformers/blob/ca009500576f6c77e3b3fa383eebc70f20f8d867/src%2Ftransformers%2Fintegrations%2Fpeft.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ca009500576f6c77e3b3fa383eebc70f20f8d867/src%2Ftransformers%2Fintegrations%2Fpeft.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fpeft.py?ref=ca009500576f6c77e3b3fa383eebc70f20f8d867",
            "patch": "@@ -11,6 +11,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n+\n import importlib\n import inspect\n import warnings\n@@ -525,3 +526,64 @@ def _dispatch_accelerate_model(\n             offload_dir=offload_folder,\n             **dispatch_model_kwargs,\n         )\n+\n+    def delete_adapter(self, adapter_names: Union[List[str], str]) -> None:\n+        \"\"\"\n+        Delete an adapter's LoRA layers from the underlying model.\n+\n+        Args:\n+            adapter_names (`Union[List[str], str]`):\n+                The name(s) of the adapter(s) to delete.\n+\n+        Example:\n+\n+        ```py\n+        from diffusers import AutoPipelineForText2Image\n+        import torch\n+\n+        pipeline = AutoPipelineForText2Image.from_pretrained(\n+            \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16\n+        ).to(\"cuda\")\n+        pipeline.load_lora_weights(\n+            \"jbilcke-hf/sdxl-cinematic-1\", weight_name=\"pytorch_lora_weights.safetensors\", adapter_names=\"cinematic\"\n+        )\n+        pipeline.delete_adapters(\"cinematic\")\n+        ```\n+        \"\"\"\n+\n+        check_peft_version(min_version=MIN_PEFT_VERSION)\n+\n+        if not self._hf_peft_config_loaded:\n+            raise ValueError(\"No adapter loaded. Please load an adapter first.\")\n+\n+        from peft.tuners.tuners_utils import BaseTunerLayer\n+\n+        if isinstance(adapter_names, str):\n+            adapter_names = [adapter_names]\n+\n+        # Check that all adapter names are present in the config\n+        missing_adapters = [name for name in adapter_names if name not in self.peft_config]\n+        if missing_adapters:\n+            raise ValueError(\n+                f\"The following adapter(s) are not present and cannot be deleted: {', '.join(missing_adapters)}\"\n+            )\n+\n+        for adapter_name in adapter_names:\n+            for module in self.modules():\n+                if isinstance(module, BaseTunerLayer):\n+                    if hasattr(module, \"delete_adapter\"):\n+                        module.delete_adapter(adapter_name)\n+                    else:\n+                        raise ValueError(\n+                            \"The version of PEFT you are using is not compatible, please use a version that is greater than 0.6.1\"\n+                        )\n+\n+            # For transformers integration - we need to pop the adapter from the config\n+            if getattr(self, \"_hf_peft_config_loaded\", False) and hasattr(self, \"peft_config\"):\n+                self.peft_config.pop(adapter_name, None)\n+\n+        # In case all adapters are deleted, we need to delete the config\n+        # and make sure to set the flag to False\n+        if len(self.peft_config) == 0:\n+            del self.peft_config\n+            self._hf_peft_config_loaded = False"
        },
        {
            "sha": "6d6330d3d4f6bab572f9169b21eac460efce04ae",
            "filename": "tests/peft_integration/test_peft_integration.py",
            "status": "modified",
            "additions": 64,
            "deletions": 1,
            "changes": 65,
            "blob_url": "https://github.com/huggingface/transformers/blob/ca009500576f6c77e3b3fa383eebc70f20f8d867/tests%2Fpeft_integration%2Ftest_peft_integration.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ca009500576f6c77e3b3fa383eebc70f20f8d867/tests%2Fpeft_integration%2Ftest_peft_integration.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpeft_integration%2Ftest_peft_integration.py?ref=ca009500576f6c77e3b3fa383eebc70f20f8d867",
            "patch": "@@ -350,7 +350,6 @@ def test_peft_add_multi_adapter(self):\n                 self.assertFalse(\n                     torch.allclose(logits_adapter_1.logits, logits_adapter_mixed.logits, atol=1e-6, rtol=1e-6)\n                 )\n-\n                 self.assertFalse(\n                     torch.allclose(logits_adapter_2.logits, logits_adapter_mixed.logits, atol=1e-6, rtol=1e-6)\n                 )\n@@ -359,6 +358,70 @@ def test_peft_add_multi_adapter(self):\n                 with self.assertRaises(ValueError), tempfile.TemporaryDirectory() as tmpdirname:\n                     model.save_pretrained(tmpdirname)\n \n+    def test_delete_adapter(self):\n+        \"\"\"\n+        Enhanced test for `delete_adapter` to handle multiple adapters,\n+        edge cases, and proper error handling.\n+        \"\"\"\n+        from peft import LoraConfig\n+\n+        for model_id in self.transformers_test_model_ids:\n+            for transformers_class in self.transformers_test_model_classes:\n+                model = transformers_class.from_pretrained(model_id).to(torch_device)\n+\n+                # Add multiple adapters\n+                peft_config_1 = LoraConfig(init_lora_weights=False)\n+                peft_config_2 = LoraConfig(init_lora_weights=False)\n+                model.add_adapter(peft_config_1, adapter_name=\"adapter_1\")\n+                model.add_adapter(peft_config_2, adapter_name=\"adapter_2\")\n+\n+                # Ensure adapters were added\n+                self.assertIn(\"adapter_1\", model.peft_config)\n+                self.assertIn(\"adapter_2\", model.peft_config)\n+\n+                # Delete a single adapter\n+                model.delete_adapter(\"adapter_1\")\n+                self.assertNotIn(\"adapter_1\", model.peft_config)\n+                self.assertIn(\"adapter_2\", model.peft_config)\n+\n+                # Delete remaining adapter\n+                model.delete_adapter(\"adapter_2\")\n+                self.assertNotIn(\"adapter_2\", model.peft_config)\n+                self.assertFalse(model._hf_peft_config_loaded)\n+\n+                # Re-add adapters for edge case tests\n+                model.add_adapter(peft_config_1, adapter_name=\"adapter_1\")\n+                model.add_adapter(peft_config_2, adapter_name=\"adapter_2\")\n+\n+                # Attempt to delete multiple adapters at once\n+                model.delete_adapter([\"adapter_1\", \"adapter_2\"])\n+                self.assertNotIn(\"adapter_1\", model.peft_config)\n+                self.assertNotIn(\"adapter_2\", model.peft_config)\n+                self.assertFalse(model._hf_peft_config_loaded)\n+\n+                # Test edge cases\n+                with self.assertRaisesRegex(ValueError, \"The following adapter\\\\(s\\\\) are not present\"):\n+                    model.delete_adapter(\"nonexistent_adapter\")\n+\n+                with self.assertRaisesRegex(ValueError, \"The following adapter\\\\(s\\\\) are not present\"):\n+                    model.delete_adapter([\"adapter_1\", \"nonexistent_adapter\"])\n+\n+                # Deleting with an empty list or None should not raise errors\n+                model.add_adapter(peft_config_1, adapter_name=\"adapter_1\")\n+                model.add_adapter(peft_config_2, adapter_name=\"adapter_2\")\n+                model.delete_adapter([])  # No-op\n+                self.assertIn(\"adapter_1\", model.peft_config)\n+                self.assertIn(\"adapter_2\", model.peft_config)\n+\n+                model.delete_adapter(None)  # No-op\n+                self.assertIn(\"adapter_1\", model.peft_config)\n+                self.assertIn(\"adapter_2\", model.peft_config)\n+\n+                # Deleting duplicate adapter names in the list\n+                model.delete_adapter([\"adapter_1\", \"adapter_1\"])\n+                self.assertNotIn(\"adapter_1\", model.peft_config)\n+                self.assertIn(\"adapter_2\", model.peft_config)\n+\n     @require_torch_gpu\n     @require_bitsandbytes\n     def test_peft_from_pretrained_kwargs(self):"
        }
    ],
    "stats": {
        "total": 127,
        "additions": 126,
        "deletions": 1
    }
}