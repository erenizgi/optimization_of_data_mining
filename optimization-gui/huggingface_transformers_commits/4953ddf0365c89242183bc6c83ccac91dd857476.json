{
    "author": "avishaiElmakies",
    "message": "Add position ids in forward pass to opt model (#33121)\n\n* start working on adding position ids\r\n\r\n* add docs\r\n\r\n* Refactor modeling_biogpt.py and modeling_opt.py for code consistency\r\n\r\n* fix 2 PR comments\r\n\r\n* move position_ids to end of args\r\n\r\n* remove trailing white space\r\n\r\n* add comment with TODO\r\n\r\n* bug fix gradient checkpointing\r\n\r\n* fixup\r\n\r\n* missed on position_ids\r\n\r\n* remove _attention_to_position_ids and refactor embedding class\r\n\r\n* remove redundent code\r\n\r\n---------\r\n\r\nCo-authored-by: Avishai Elmakies <avishai.elma@cs.huji.ac.il>",
    "sha": "4953ddf0365c89242183bc6c83ccac91dd857476",
    "files": [
        {
            "sha": "8158cf814aae6cb12575c01b4b942a0ab9ae9e78",
            "filename": "src/transformers/models/biogpt/modeling_biogpt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/4953ddf0365c89242183bc6c83ccac91dd857476/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4953ddf0365c89242183bc6c83ccac91dd857476/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py?ref=4953ddf0365c89242183bc6c83ccac91dd857476",
            "patch": "@@ -47,7 +47,8 @@\n _CONFIG_FOR_DOC = \"BioGptConfig\"\n \n \n-# Copied from transformers.models.opt.modeling_opt.OPTLearnedPositionalEmbedding with OPT->BioGpt\n+# copied from transformers.models.opt.modeling_opt.OPTLearnedPositionalEmbedding with OPT->BioGpt\n+# TODO @ArthurZucker bring copied from back\n class BioGptLearnedPositionalEmbedding(nn.Embedding):\n     \"\"\"\n     This module learns positional embeddings up to a fixed maximum size."
        },
        {
            "sha": "b1dbdbe5d912eb207bd89f0bd331d98adf5bdde4",
            "filename": "src/transformers/models/opt/modeling_opt.py",
            "status": "modified",
            "additions": 53,
            "deletions": 10,
            "changes": 63,
            "blob_url": "https://github.com/huggingface/transformers/blob/4953ddf0365c89242183bc6c83ccac91dd857476/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4953ddf0365c89242183bc6c83ccac91dd857476/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py?ref=4953ddf0365c89242183bc6c83ccac91dd857476",
            "patch": "@@ -72,17 +72,21 @@ def __init__(self, num_embeddings: int, embedding_dim: int):\n         self.offset = 2\n         super().__init__(num_embeddings + self.offset, embedding_dim)\n \n-    def forward(self, attention_mask: torch.LongTensor, past_key_values_length: int = 0):\n+    def forward(\n+        self,\n+        attention_mask: torch.LongTensor,\n+        past_key_values_length: int = 0,\n+        position_ids: Optional[torch.LongTensor] = None,\n+    ):\n         \"\"\"`input_ids_shape` is expected to be [bsz x seqlen].\"\"\"\n-        attention_mask = attention_mask.long()\n-\n-        # create positions depending on attention_mask\n-        positions = (torch.cumsum(attention_mask, dim=1).type_as(attention_mask) * attention_mask).long() - 1\n \n-        # cut positions if `past_key_values_length` is > 0\n-        positions = positions[:, past_key_values_length:]\n+        if position_ids is None:\n+            position_ids = torch.cumsum(attention_mask, dim=1)\n+            position_ids = (position_ids * attention_mask - 1).long()\n+            # cut positions if `past_key_values_length` is > 0\n+            position_ids = position_ids[:, past_key_values_length:]\n \n-        return super().forward(positions + self.offset)\n+        return super().forward(position_ids + self.offset)\n \n \n class OPTAttention(nn.Module):\n@@ -128,6 +132,8 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n+        # isn't needed in normal attention, but needed in flash attention so to keep the signature same\n+        position_ids: Optional[torch.Tensor] = None,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n@@ -265,6 +271,7 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n+        position_ids: Optional[torch.Tensor] = None,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n@@ -346,6 +353,7 @@ def forward(\n             value_states,\n             attention_mask,\n             query_length,\n+            position_ids=position_ids,\n             dropout=attn_dropout,\n             is_causal=self.is_causal,\n             use_top_left_mask=self._flash_attn_uses_top_left_mask,\n@@ -392,6 +400,7 @@ def forward(\n         past_key_value: Optional[Tuple[torch.Tensor]] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n+        position_ids: Optional[torch.LongTensor] = None,\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n         Args:\n@@ -419,6 +428,7 @@ def forward(\n         hidden_states, self_attn_weights, present_key_value = self.self_attn(\n             hidden_states=hidden_states,\n             past_key_value=past_key_value,\n+            position_ids=position_ids,\n             attention_mask=attention_mask,\n             layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n@@ -561,6 +571,11 @@ def _init_weights(self, module):\n             more detail.\n         return_dict (`bool`, *optional*):\n             Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n+            config.n_positions - 1]`. for padding use -1.\n+\n+            [What are position IDs?](../glossary#position-ids)\n \"\"\"\n \n \n@@ -627,6 +642,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n     ) -> Union[Tuple, BaseModelOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -674,6 +690,11 @@ def forward(\n                 for more detail.\n             return_dict (`bool`, *optional*):\n                 Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+            position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+                Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n+                config.n_positions - 1]`. for padding use -1.\n+\n+                [What are position IDs?](../glossary#position-ids)\n         \"\"\"\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -724,7 +745,13 @@ def forward(\n                 attention_mask, input_shape, inputs_embeds, past_key_values_length\n             )\n \n-        pos_embeds = self.embed_positions(attention_mask, past_key_values_length)\n+        if position_ids is None:\n+            position_ids = torch.cumsum(attention_mask, dim=1)\n+            position_ids = (position_ids * attention_mask - 1).long()\n+            # cut positions if `past_key_values_length` is > 0\n+            position_ids = position_ids[:, past_key_values_length:]\n+\n+        pos_embeds = self.embed_positions(attention_mask, past_key_values_length, position_ids=position_ids)\n \n         if self.project_in is not None:\n             inputs_embeds = self.project_in(inputs_embeds)\n@@ -773,11 +800,13 @@ def forward(\n                     None,\n                     output_attentions,\n                     use_cache,\n+                    position_ids,\n                 )\n             else:\n                 layer_outputs = decoder_layer(\n                     hidden_states,\n                     attention_mask=causal_attention_mask,\n+                    position_ids=position_ids,\n                     layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n                     past_key_value=past_key_value,\n                     output_attentions=output_attentions,\n@@ -851,6 +880,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n     ) -> Union[Tuple, BaseModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -863,6 +893,7 @@ def forward(\n         decoder_outputs = self.decoder(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n+            position_ids=position_ids,\n             head_mask=head_mask,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n@@ -927,6 +958,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -982,6 +1014,11 @@ def forward(\n                 for more detail.\n             return_dict (`bool`, *optional*):\n                 Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+            position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+                Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n+                config.n_positions - 1]`. for padding use -1.\n+\n+                [What are position IDs?](../glossary#position-ids)\n \n         Returns:\n \n@@ -1012,6 +1049,7 @@ def forward(\n         outputs = self.model.decoder(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n+            position_ids=position_ids,\n             head_mask=head_mask,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n@@ -1047,7 +1085,7 @@ def forward(\n         )\n \n     def prepare_inputs_for_generation(\n-        self, input_ids, past_key_values=None, attention_mask=None, inputs_embeds=None, **kwargs\n+        self, input_ids, past_key_values=None, attention_mask=None, inputs_embeds=None, position_ids=None, **kwargs\n     ):\n         if past_key_values is not None:\n             past_length = past_key_values[0][0].shape[2]\n@@ -1072,6 +1110,7 @@ def prepare_inputs_for_generation(\n                 \"past_key_values\": past_key_values,\n                 \"use_cache\": kwargs.get(\"use_cache\"),\n                 \"attention_mask\": attention_mask,\n+                \"position_ids\": position_ids,\n             }\n         )\n         return model_inputs\n@@ -1131,6 +1170,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n     ) -> Union[Tuple, SequenceClassifierOutputWithPast]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n@@ -1144,6 +1184,7 @@ def forward(\n             input_ids,\n             past_key_values=past_key_values,\n             attention_mask=attention_mask,\n+            position_ids=position_ids,\n             head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n@@ -1248,6 +1289,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n     ) -> Union[Tuple, QuestionAnsweringModelOutput]:\n         r\"\"\"\n         start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n@@ -1298,6 +1340,7 @@ def forward(\n             input_ids,\n             past_key_values=past_key_values,\n             attention_mask=attention_mask,\n+            position_ids=position_ids,\n             head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,"
        }
    ],
    "stats": {
        "total": 66,
        "additions": 55,
        "deletions": 11
    }
}