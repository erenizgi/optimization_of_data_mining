{
    "author": "jerryzh168",
    "message": "Preserve requires_grad in pre quantized model (#37354)\n\n* Preserve requires_grad in pre quantized model\n\nSummary:\ndiscovered this when running lm-eval for some models, current\ncode will set requires_grad to True always\n\nTest Plan:\nlm_eval --model hf --model_args pretrained=jerryzh168/phi4-torchao-gguf-q4_k --tasks hellaswag --device cuda:0 --batch_size 8\n\nReviewers:\n\nSubscribers:\n\nTasks:\n\nTags:\n\n* ruff format\n\n---------\n\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>",
    "sha": "08e3217bafddc5d11ce0e7369bcfaaabe5501ba5",
    "files": [
        {
            "sha": "e5e02f5387ad7eb0f899c706b8d082fe5a49d669",
            "filename": "src/transformers/quantizers/quantizer_torchao.py",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/08e3217bafddc5d11ce0e7369bcfaaabe5501ba5/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08e3217bafddc5d11ce0e7369bcfaaabe5501ba5/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py?ref=08e3217bafddc5d11ce0e7369bcfaaabe5501ba5",
            "patch": "@@ -230,12 +230,16 @@ def create_quantized_param(\n \n         module, tensor_name = get_module_from_name(model, param_name)\n         if self.pre_quantized:\n-            module._parameters[tensor_name] = torch.nn.Parameter(param_value.to(device=target_device))\n+            module._parameters[tensor_name] = torch.nn.Parameter(\n+                param_value.to(device=target_device), requires_grad=param_value.requires_grad\n+            )\n             if isinstance(module, nn.Linear):\n                 module.extra_repr = types.MethodType(_linear_extra_repr, module)\n         else:\n             assert isinstance(self.quantization_config, TorchAoConfig)\n-            module._parameters[tensor_name] = torch.nn.Parameter(param_value).to(device=target_device)\n+            module._parameters[tensor_name] = torch.nn.Parameter(\n+                param_value, requires_grad=param_value.requires_grad\n+            ).to(device=target_device)\n             quantize_(module, self.quantization_config.get_apply_tensor_subclass())\n \n     def _process_model_after_weight_loading(self, model, **kwargs):"
        }
    ],
    "stats": {
        "total": 8,
        "additions": 6,
        "deletions": 2
    }
}