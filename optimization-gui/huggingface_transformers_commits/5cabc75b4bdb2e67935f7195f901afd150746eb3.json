{
    "author": "d223302",
    "message": "Add compute_loss_func to Seq2SeqTrainer (#35136)",
    "sha": "5cabc75b4bdb2e67935f7195f901afd150746eb3",
    "files": [
        {
            "sha": "5cd89b3701cff6e2303e98bf63a1ad97c8af6eef",
            "filename": "src/transformers/trainer_seq2seq.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5cabc75b4bdb2e67935f7195f901afd150746eb3/src%2Ftransformers%2Ftrainer_seq2seq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5cabc75b4bdb2e67935f7195f901afd150746eb3/src%2Ftransformers%2Ftrainer_seq2seq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer_seq2seq.py?ref=5cabc75b4bdb2e67935f7195f901afd150746eb3",
            "patch": "@@ -64,6 +64,7 @@ def __init__(\n             Union[\"PreTrainedTokenizerBase\", \"BaseImageProcessor\", \"FeatureExtractionMixin\", \"ProcessorMixin\"]\n         ] = None,\n         model_init: Optional[Callable[[], \"PreTrainedModel\"]] = None,\n+        compute_loss_func: Optional[Callable] = None,\n         compute_metrics: Optional[Callable[[\"EvalPrediction\"], Dict]] = None,\n         callbacks: Optional[List[\"TrainerCallback\"]] = None,\n         optimizers: Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR] = (None, None),\n@@ -77,6 +78,7 @@ def __init__(\n             eval_dataset=eval_dataset,\n             processing_class=processing_class,\n             model_init=model_init,\n+            compute_loss_func=compute_loss_func,\n             compute_metrics=compute_metrics,\n             callbacks=callbacks,\n             optimizers=optimizers,"
        }
    ],
    "stats": {
        "total": 2,
        "additions": 2,
        "deletions": 0
    }
}