{
    "author": "ydshieh",
    "message": "Mark `LongformerModelTest::test_attention_outputs` as flaky (#40655)\n\n* fix\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "5ea5c8179b60095e4a211aec4d4e9d73f277c2f3",
    "files": [
        {
            "sha": "19f6b3de0d903e8a43d1615ca1748d626569df56",
            "filename": "tests/models/longformer/test_modeling_longformer.py",
            "status": "modified",
            "additions": 16,
            "deletions": 1,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ea5c8179b60095e4a211aec4d4e9d73f277c2f3/tests%2Fmodels%2Flongformer%2Ftest_modeling_longformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ea5c8179b60095e4a211aec4d4e9d73f277c2f3/tests%2Fmodels%2Flongformer%2Ftest_modeling_longformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flongformer%2Ftest_modeling_longformer.py?ref=5ea5c8179b60095e4a211aec4d4e9d73f277c2f3",
            "patch": "@@ -16,7 +16,14 @@\n import unittest\n \n from transformers import LongformerConfig, is_torch_available\n-from transformers.testing_utils import require_sentencepiece, require_tokenizers, require_torch, slow, torch_device\n+from transformers.testing_utils import (\n+    is_flaky,\n+    require_sentencepiece,\n+    require_tokenizers,\n+    require_torch,\n+    slow,\n+    torch_device,\n+)\n \n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import ModelTesterMixin, ids_tensor, random_attention_mask\n@@ -355,6 +362,14 @@ def setUp(self):\n         self.model_tester = LongformerModelTester(self)\n         self.config_tester = ConfigTester(self, config_class=LongformerConfig, hidden_size=37)\n \n+    # Without this, 0.01% failure rate.\n+    @is_flaky(\n+        max_attempts=2,\n+        description=\"When `inputs_dict['attention_mask'][:, -1]` is all `0`s, we get shorter length along the last dimension of the output's `attentions`.\",\n+    )\n+    def test_attention_outputs(self):\n+        super().test_attention_outputs()\n+\n     def test_config(self):\n         self.config_tester.run_common_tests()\n "
        }
    ],
    "stats": {
        "total": 17,
        "additions": 16,
        "deletions": 1
    }
}