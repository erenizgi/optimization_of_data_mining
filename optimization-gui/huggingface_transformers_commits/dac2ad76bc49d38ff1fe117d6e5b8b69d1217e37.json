{
    "author": "Rocketknight1",
    "message": "Fix Qwen3OmniMoE weight init (#42531)\n\n* module.router -> module.gate\n\n* i am not smart today",
    "sha": "dac2ad76bc49d38ff1fe117d6e5b8b69d1217e37",
    "files": [
        {
            "sha": "a04ddc842e0cb86e97bdf77dc7d0861c8ec60fc6",
            "filename": "src/transformers/models/qwen3_omni_moe/modeling_qwen3_omni_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/dac2ad76bc49d38ff1fe117d6e5b8b69d1217e37/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dac2ad76bc49d38ff1fe117d6e5b8b69d1217e37/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py?ref=dac2ad76bc49d38ff1fe117d6e5b8b69d1217e37",
            "patch": "@@ -84,7 +84,7 @@ def _init_weights(self, module):\n         if isinstance(module, Qwen3OmniMoeThinkerTextSparseMoeBlock):\n             init.normal_(module.experts.gate_up_proj, mean=0.0, std=std)\n             init.normal_(module.experts.down_proj, mean=0.0, std=std)\n-            init.normal_(module.router.weight, mean=0.0, std=std)\n+            init.normal_(module.gate.weight, mean=0.0, std=std)\n \n \n def _get_feat_extract_output_lengths(input_lengths):"
        },
        {
            "sha": "26cd94112e442a18e65a852ffac6af55c6714181",
            "filename": "src/transformers/models/qwen3_omni_moe/modular_qwen3_omni_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/dac2ad76bc49d38ff1fe117d6e5b8b69d1217e37/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodular_qwen3_omni_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dac2ad76bc49d38ff1fe117d6e5b8b69d1217e37/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodular_qwen3_omni_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodular_qwen3_omni_moe.py?ref=dac2ad76bc49d38ff1fe117d6e5b8b69d1217e37",
            "patch": "@@ -899,7 +899,7 @@ def _init_weights(self, module):\n         if isinstance(module, Qwen3OmniMoeThinkerTextSparseMoeBlock):\n             init.normal_(module.experts.gate_up_proj, mean=0.0, std=std)\n             init.normal_(module.experts.down_proj, mean=0.0, std=std)\n-            init.normal_(module.router.weight, mean=0.0, std=std)\n+            init.normal_(module.gate.weight, mean=0.0, std=std)\n \n \n class Qwen3OmniMoePreTrainedModelForConditionalGeneration(Qwen2_5OmniPreTrainedModelForConditionalGeneration):"
        }
    ],
    "stats": {
        "total": 4,
        "additions": 2,
        "deletions": 2
    }
}