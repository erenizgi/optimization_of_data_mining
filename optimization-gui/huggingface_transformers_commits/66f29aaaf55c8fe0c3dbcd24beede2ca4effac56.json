{
    "author": "threewebcode",
    "message": "chore: enhance messages in docstrings (#36525)\n\nchore: enhance the message in docstrings",
    "sha": "66f29aaaf55c8fe0c3dbcd24beede2ca4effac56",
    "files": [
        {
            "sha": "6a98b1b2ff6a8be4731e6486ca0f377d988f86e1",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/66f29aaaf55c8fe0c3dbcd24beede2ca4effac56/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/66f29aaaf55c8fe0c3dbcd24beede2ca4effac56/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=66f29aaaf55c8fe0c3dbcd24beede2ca4effac56",
            "patch": "@@ -212,10 +212,10 @@ class QuantizedCacheConfig(CacheConfig):\n             Size of the quantization group, should be a divisor of the model's hidden dimension.\n             Defaults to 64.\n         residual_length (`Optional[int]`, *optional*, defaults to 128):\n-            Length of the residual cache which will always be stored in original presicion.\n+            Length of the residual cache which will always be stored in original precision.\n             Defaults to 128.\n         compute_dtype (`torch.dtype`, *optional*, defaults to `torch.float16`):\n-            The defualt dtype used for computations in the model. Keys and Values will be cast to this dtype after dequantization.\n+            The default dtype used for computations in the model. Keys and Values will be cast to this dtype after dequantization.\n         device (`str`, *optional*, defaults to `\"cpu\"`):\n             Device on which to perform computations, should be same as the model's device.\n     \"\"\"\n@@ -1074,7 +1074,7 @@ class StaticCache(Cache):\n         dtype (`torch.dtype`, *optional*, defaults to `torch.float32`):\n             The default `dtype` to use when initializing the layer.\n         layer_device_map(`Dict[int, Union[str, torch.device, int]]]`, `optional`):\n-            Mapping between the layers and its device. This is required when you are manually initializing the cache and the model is splitted between differents gpus.\n+            Mapping between the layers and its device. This is required when you are manually initializing the cache and the model is splitted between different gpus.\n             You can know which layers mapped to which device by checking the associated device_map: `model.hf_device_map`.\n \n \n@@ -1267,7 +1267,7 @@ class SlidingWindowCache(StaticCache):\n         dtype (`torch.dtype`, *optional*, defaults to `torch.float32`):\n             The default `dtype` to use when initializing the layer.\n         layer_device_map(`Dict[int, Union[str, torch.device, int]]]`, `optional`):\n-            Mapping between the layers and its device. This is required when you are manually initializing the cache and the model is splitted between differents gpus.\n+            Mapping between the layers and its device. This is required when you are manually initializing the cache and the model is splitted between different gpus.\n             You can know which layers mapped to which device by checking the associated device_map: `model.hf_device_map`.\n \n     Example:\n@@ -1579,7 +1579,7 @@ class HybridCache(Cache):\n         dtype (torch.dtype, *optional*, defaults to `torch.float32`):\n             The default `dtype` to use when initializing the layer.\n         layer_device_map(`Dict[int, Union[str, torch.device, int]]]`, `optional`):\n-            Mapping between the layers and its device. This is required when you are manually initializing the cache and the model is splitted between differents gpus.\n+            Mapping between the layers and its device. This is required when you are manually initializing the cache and the model is splitted between different gpus.\n             You can know which layers mapped to which device by checking the associated device_map: `model.hf_device_map`.\n \n     Example:\n@@ -1929,7 +1929,7 @@ class OffloadedStaticCache(StaticCache):\n         offload_device (`Union[str, torch.device]`, *optional*, defaults to `cpu`):\n             The device to offload to. Defaults to CPU.\n         layer_device_map (`Dict[int, Union[str, torch.device, int]]`, *optional*):\n-            Mapping between the layers and its device. This is required when you are manually initializing the cache and the model is splitted between differents gpus.\n+            Mapping between the layers and its device. This is required when you are manually initializing the cache and the model is splitted between different gpus.\n             You can know which layers mapped to which device by checking the associated device_map: `model.hf_device_map`.\n \n     Attributes:"
        },
        {
            "sha": "0c3c22deeb914add589c55f682d45f665301110b",
            "filename": "src/transformers/tokenization_utils_base.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/66f29aaaf55c8fe0c3dbcd24beede2ca4effac56/src%2Ftransformers%2Ftokenization_utils_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/66f29aaaf55c8fe0c3dbcd24beede2ca4effac56/src%2Ftransformers%2Ftokenization_utils_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_base.py?ref=66f29aaaf55c8fe0c3dbcd24beede2ca4effac56",
            "patch": "@@ -1184,7 +1184,7 @@ def _set_model_specific_special_tokens(self, special_tokens: List[str]):\n         \"\"\"\n         Adds new special tokens to the \"SPECIAL_TOKENS_ATTRIBUTES\" list which will be part\n         of \"self.special_tokens\" and saved as a special token in tokenizer's config.\n-        This allows us to dynamically add new model-type specific tokens after initilizing the tokenizer.\n+        This allows us to dynamically add new model-type specific tokens after initializing the tokenizer.\n         For example: if the model tokenizers is multimodal, we can support special image or audio tokens.\n         \"\"\"\n         self.SPECIAL_TOKENS_ATTRIBUTES = self.SPECIAL_TOKENS_ATTRIBUTES + list(special_tokens.keys())\n@@ -1199,7 +1199,7 @@ def _set_model_specific_special_tokens(self, special_tokens: List[str]):\n             add_special_tokens (`bool`, *optional*, defaults to `True`):\n                 Whether or not to add special tokens when encoding the sequences. This will use the underlying\n                 `PretrainedTokenizerBase.build_inputs_with_special_tokens` function, which defines which tokens are\n-                automatically added to the input ids. This is usefull if you want to add `bos` or `eos` tokens\n+                automatically added to the input ids. This is useful if you want to add `bos` or `eos` tokens\n                 automatically.\n             padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n                 Activates and controls padding. Accepts the following values:\n@@ -2474,7 +2474,7 @@ def save_pretrained(\n         # no typefields, this way old fast and slow can load it\n         tokenizer_config = self.convert_added_tokens(tokenizer_config, add_type_field=True, save=True)\n \n-        # Process added tokens seperatly: allows previous versions to ignore it!\n+        # Process added tokens separately: allows previous versions to ignore it!\n         added_tokens = {}\n         for key, value in self.added_tokens_decoder.items():\n             added_tokens[key] = value.__getstate__()"
        }
    ],
    "stats": {
        "total": 18,
        "additions": 9,
        "deletions": 9
    }
}