{
    "author": "jerryzh168",
    "message": "Add support for `weights_only` flag when loading state_dict (#32481)\n\n* Add support for `weights_only` flag when loading state_dict\r\n\r\nSummary:\r\nThis is to enable loading a state_dict with wrapper tensor subclasses (used in torchao to\r\nfor quantized weights)\r\n\r\nTest Plan:\r\ntested locally with torchao weights, also need https://github.com/huggingface/transformers/pull/32306:\r\n```\r\nimport torch\r\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\r\nfrom transformers import TorchAoConfig\r\nfrom torchao.utils import benchmark_model\r\nimport torchao\r\n\r\nDEVICE_TYPE = \"cuda\"\r\n\r\ndef init_model_and_benchmark(model_id, torch_dtype=torch.bfloat16, quantization_config=None):\r\n    tokenizer = AutoTokenizer.from_pretrained(model_id)\r\n    if quantization_config is not None:\r\n        model = AutoModelForCausalLM.from_pretrained(model_id, device_map=DEVICE_TYPE, torch_dtype=torch.\\bfloat16, quantization_config=quantization_config)\r\n    else:\r\n        model = AutoModelForCausalLM.from_pretrained(model_id, device_map=DEVICE_TYPE, torch_dtype=torch.\\bfloat16, weights_only=False)\r\n\r\n    # sanity check: run the model\r\n    input_text = \"What are we having for dinner?\"\r\n    input_ids = tokenizer(input_text, return_tensors=\"pt\").to(DEVICE_TYPE)\r\n    output = model.generate(**input_ids, max_new_tokens=1000)\r\n    print(tokenizer.decode(output[0], skip_special_tokens=True))\r\n\r\n    NUM_WARMUP = 1\r\n    NUM_RUNS = 5\r\n\r\n    if quantization_config is not None:\r\n        torchao.quantization.utils.recommended_inductor_config_setter()\r\n\r\n    model = torch.compile(model, mode=\"max-autotune\")\r\n\r\n    benchmark_model(model.generate, NUM_WARMUP, kwargs=input_ids, device_type=DEVICE_TYPE)\r\n    print(\"running benchmark\")\r\n    results = benchmark_model(model.generate, NUM_RUNS, kwargs=input_ids, device_type=DEVICE_TYPE)\r\n    return model, results\r\n\r\nmodel_id = \"jerryzh168/test-model\"\r\ntorchao.quantization.utils.recommended_inductor_config_setter()\r\nbf16_model, bf16_time = init_model_and_benchmark(model_id)\r\nprint(f\"bf16: {bf16_time}\")\r\n```\r\n\r\nReviewers:\r\n\r\nSubscribers:\r\n\r\nTasks:\r\n\r\nTags:\r\n\r\n* format",
    "sha": "15a4d24805a8d003ab7ce0758a7ede4142f011a9",
    "files": [
        {
            "sha": "127f60d557779218c11bd462806f60e7934f7ff5",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 17,
            "deletions": 5,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/15a4d24805a8d003ab7ce0758a7ede4142f011a9/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/15a4d24805a8d003ab7ce0758a7ede4142f011a9/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=15a4d24805a8d003ab7ce0758a7ede4142f011a9",
            "patch": "@@ -544,6 +544,7 @@ def load_state_dict(\n     checkpoint_file: Union[str, os.PathLike],\n     is_quantized: bool = False,\n     map_location: Optional[Union[str, torch.device]] = None,\n+    weights_only: bool = True,\n ):\n     \"\"\"\n     Reads a PyTorch checkpoint file, returning properly formatted errors if they arise.\n@@ -580,7 +581,7 @@ def load_state_dict(\n             and is_zipfile(checkpoint_file)\n         ):\n             extra_args = {\"mmap\": True}\n-        weights_only_kwarg = {\"weights_only\": True} if is_torch_greater_or_equal_than_1_13 else {}\n+        weights_only_kwarg = {\"weights_only\": weights_only} if is_torch_greater_or_equal_than_1_13 else {}\n         return torch.load(\n             checkpoint_file,\n             map_location=map_location,\n@@ -3009,6 +3010,7 @@ def from_pretrained(\n         token: Optional[Union[str, bool]] = None,\n         revision: str = \"main\",\n         use_safetensors: bool = None,\n+        weights_only: bool = True,\n         **kwargs,\n     ) -> \"PreTrainedModel\":\n         r\"\"\"\n@@ -3196,6 +3198,11 @@ def from_pretrained(\n                 Whether or not to use `safetensors` checkpoints. Defaults to `None`. If not specified and `safetensors`\n                 is not installed, it will be set to `False`.\n \n+            weights_only (`bool`, *optional*, defaults to `True`):\n+                Indicates whether unpickler should be restricted to loading only tensors, primitive types,\n+                dictionaries and any types added via torch.serialization.add_safe_globals().\n+                When set to False, we can load wrapper tensor subclass weights.\n+\n             kwargs (remaining dictionary of keyword arguments, *optional*):\n                 Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,\n                 `output_attentions=True`). Behaves differently depending on whether a `config` is provided or\n@@ -3831,7 +3838,7 @@ def from_pretrained(\n         if from_pt:\n             if not is_sharded and state_dict is None:\n                 # Time to load the checkpoint\n-                state_dict = load_state_dict(resolved_archive_file)\n+                state_dict = load_state_dict(resolved_archive_file, weights_only=weights_only)\n \n             # set dtype to instantiate the model under:\n             # 1. If torch_dtype is not None, we use that dtype\n@@ -3852,7 +3859,7 @@ def from_pretrained(\n                             elif not is_sharded:\n                                 torch_dtype = get_state_dict_dtype(state_dict)\n                             else:\n-                                one_state_dict = load_state_dict(resolved_archive_file[0])\n+                                one_state_dict = load_state_dict(resolved_archive_file[0], weights_only=weights_only)\n                                 torch_dtype = get_state_dict_dtype(one_state_dict)\n                                 del one_state_dict  # free CPU memory\n                             logger.info(\n@@ -4052,6 +4059,7 @@ def from_pretrained(\n                 hf_quantizer=hf_quantizer,\n                 keep_in_fp32_modules=keep_in_fp32_modules,\n                 gguf_path=gguf_path,\n+                weights_only=weights_only,\n             )\n \n         # make sure token embedding weights are still tied if needed\n@@ -4157,6 +4165,7 @@ def _load_pretrained_model(\n         hf_quantizer=None,\n         keep_in_fp32_modules=None,\n         gguf_path=None,\n+        weights_only=True,\n     ):\n         is_safetensors = False\n         is_quantized = hf_quantizer is not None\n@@ -4514,7 +4523,9 @@ def _find_mismatched_keys(\n                     and hf_quantizer.quantization_config.quant_type == \"int4_weight_only\"\n                 ):\n                     map_location = torch.device([d for d in device_map.values() if d not in [\"cpu\", \"disk\"]][0])\n-                state_dict = load_state_dict(shard_file, is_quantized=is_quantized, map_location=map_location)\n+                state_dict = load_state_dict(\n+                    shard_file, is_quantized=is_quantized, map_location=map_location, weights_only=weights_only\n+                )\n \n                 # Mistmatched keys contains tuples key/shape1/shape2 of weights in the checkpoint that have a shape not\n                 # matching the weights in the model.\n@@ -4667,6 +4678,7 @@ def _load_pretrained_model_low_mem(\n         start_prefix=\"\",\n         hf_quantizer=None,\n         pretrained_model_name_or_path=None,\n+        weights_only=True,\n     ):\n         \"\"\"\n         This is an experimental function that loads the model using ~1.x model size CPU memory\n@@ -4687,7 +4699,7 @@ def _load_pretrained_model_low_mem(\n         \"\"\"\n \n         _move_model_to_meta(model, loaded_state_dict_keys, start_prefix)\n-        state_dict = load_state_dict(resolved_archive_file)\n+        state_dict = load_state_dict(resolved_archive_file, weights_only=weights_only)\n         expected_keys = loaded_state_dict_keys  # plug for missing expected_keys. TODO: replace with proper keys\n         error_msgs = _load_state_dict_into_meta_model(\n             model,"
        }
    ],
    "stats": {
        "total": 22,
        "additions": 17,
        "deletions": 5
    }
}