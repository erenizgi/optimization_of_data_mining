{
    "author": "yonigozlan",
    "message": "Add support for post-processing kwargs in image-text-to-text pipeline (#35374)\n\n* fix error and improve pipeline\r\n\r\n* add processing_kwargs to apply_chat_template\r\n\r\n* change default post_process kwarg to args\r\n\r\n* Fix slow tests\r\n\r\n* fix copies",
    "sha": "9f51dc25357bcde280a02b59e80b66248b018ca4",
    "files": [
        {
            "sha": "52db91a6f95fcf0e0caed4dd4df7486053922840",
            "filename": "src/transformers/models/fuyu/processing_fuyu.py",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/9f51dc25357bcde280a02b59e80b66248b018ca4/src%2Ftransformers%2Fmodels%2Ffuyu%2Fprocessing_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9f51dc25357bcde280a02b59e80b66248b018ca4/src%2Ftransformers%2Fmodels%2Ffuyu%2Fprocessing_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fprocessing_fuyu.py?ref=9f51dc25357bcde280a02b59e80b66248b018ca4",
            "patch": "@@ -682,14 +682,18 @@ def tokens_to_points(tokens, original_size):\n \n         return results\n \n-    def post_process_image_text_to_text(self, generated_outputs):\n+    def post_process_image_text_to_text(self, generated_outputs, skip_special_tokens=True, **kwargs):\n         \"\"\"\n         Post-processes the output of `FuyuForConditionalGeneration` to only return the text output.\n \n         Args:\n             generated_outputs (`torch.Tensor` or `np.ndarray`):\n                 The output of the model. The output is expected to be a tensor of shape `(batch_size, sequence_length)`\n                 containing the token ids of the generated sequences.\n+            skip_special_tokens (`bool`, *optional*, defaults to `True`):\n+                Whether or not to remove special tokens in the output. Argument passed to the tokenizer's `batch_decode` method.\n+            **kwargs:\n+                Additional arguments to be passed to the tokenizer's `batch_decode method`.\n \n         Returns:\n             `List[str]`: The decoded text output.\n@@ -706,7 +710,7 @@ def post_process_image_text_to_text(self, generated_outputs):\n         for i, seq in enumerate(unpadded_output_sequences):\n             padded_output_sequences[i, : len(seq)] = torch.tensor(seq)\n \n-        return self.batch_decode(padded_output_sequences, skip_special_tokens=True)\n+        return self.batch_decode(padded_output_sequences, skip_special_tokens=skip_special_tokens, **kwargs)\n \n     def batch_decode(self, *args, **kwargs):\n         \"\"\""
        },
        {
            "sha": "3d1d28622f33afda4dfc7fca23e23b94c63eabde",
            "filename": "src/transformers/models/kosmos2/processing_kosmos2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/9f51dc25357bcde280a02b59e80b66248b018ca4/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fprocessing_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9f51dc25357bcde280a02b59e80b66248b018ca4/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fprocessing_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fprocessing_kosmos2.py?ref=9f51dc25357bcde280a02b59e80b66248b018ca4",
            "patch": "@@ -428,19 +428,23 @@ def post_process_generation(self, text, cleanup_and_extract=True):\n             return clean_text_and_extract_entities_with_bboxes(caption)\n         return caption\n \n-    def post_process_image_text_to_text(self, generated_outputs):\n+    def post_process_image_text_to_text(self, generated_outputs, skip_special_tokens=True, **kwargs):\n         \"\"\"\n         Post-process the output of the model to decode the text.\n \n         Args:\n             generated_outputs (`torch.Tensor` or `np.ndarray`):\n                 The output of the model `generate` function. The output is expected to be a tensor of shape `(batch_size, sequence_length)`\n                 or `(sequence_length,)`.\n+            skip_special_tokens (`bool`, *optional*, defaults to `True`):\n+                Whether or not to remove special tokens in the output. Argument passed to the tokenizer's `batch_decode` method.\n+            **kwargs:\n+                Additional arguments to be passed to the tokenizer's `batch_decode method`.\n \n         Returns:\n             `List[str]`: The decoded text.\n         \"\"\"\n-        generated_texts = self.batch_decode(generated_outputs, skip_special_tokens=True)\n+        generated_texts = self.batch_decode(generated_outputs, skip_special_tokens=skip_special_tokens, **kwargs)\n         return [self.post_process_generation(text, cleanup_and_extract=False) for text in generated_texts]\n \n     @property"
        },
        {
            "sha": "8e845ffd3a75b12123eed7840e20e3f33990e099",
            "filename": "src/transformers/models/mllama/processing_mllama.py",
            "status": "modified",
            "additions": 13,
            "deletions": 2,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/9f51dc25357bcde280a02b59e80b66248b018ca4/src%2Ftransformers%2Fmodels%2Fmllama%2Fprocessing_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9f51dc25357bcde280a02b59e80b66248b018ca4/src%2Ftransformers%2Fmodels%2Fmllama%2Fprocessing_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fprocessing_mllama.py?ref=9f51dc25357bcde280a02b59e80b66248b018ca4",
            "patch": "@@ -346,20 +346,31 @@ def decode(self, *args, **kwargs):\n         \"\"\"\n         return self.tokenizer.decode(*args, **kwargs)\n \n-    def post_process_image_text_to_text(self, generated_outputs):\n+    def post_process_image_text_to_text(\n+        self, generated_outputs, skip_special_tokens=True, clean_up_tokenization_spaces=False, **kwargs\n+    ):\n         \"\"\"\n         Post-process the output of the model to decode the text.\n \n         Args:\n             generated_outputs (`torch.Tensor` or `np.ndarray`):\n                 The output of the model `generate` function. The output is expected to be a tensor of shape `(batch_size, sequence_length)`\n                 or `(sequence_length,)`.\n+            skip_special_tokens (`bool`, *optional*, defaults to `True`):\n+                Whether or not to remove special tokens in the output. Argument passed to the tokenizer's `batch_decode` method.\n+            Clean_up_tokenization_spaces (`bool`, *optional*, defaults to `False`):\n+                Whether or not to clean up the tokenization spaces. Argument passed to the tokenizer's `batch_decode` method.\n+            **kwargs:\n+                Additional arguments to be passed to the tokenizer's `batch_decode method`.\n \n         Returns:\n             `List[str]`: The decoded text.\n         \"\"\"\n         return self.tokenizer.batch_decode(\n-            generated_outputs, skip_special_tokens=True, clean_up_tokenization_spaces=False\n+            generated_outputs,\n+            skip_special_tokens=skip_special_tokens,\n+            clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n+            **kwargs,\n         )\n \n     @property"
        },
        {
            "sha": "6357debbe26e2b67395411e10eeee65b64cd4400",
            "filename": "src/transformers/models/qwen2_5_vl/processing_qwen2_5_vl.py",
            "status": "modified",
            "additions": 13,
            "deletions": 2,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/9f51dc25357bcde280a02b59e80b66248b018ca4/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fprocessing_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9f51dc25357bcde280a02b59e80b66248b018ca4/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fprocessing_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fprocessing_qwen2_5_vl.py?ref=9f51dc25357bcde280a02b59e80b66248b018ca4",
            "patch": "@@ -192,20 +192,31 @@ def decode(self, *args, **kwargs):\n         \"\"\"\n         return self.tokenizer.decode(*args, **kwargs)\n \n-    def post_process_image_text_to_text(self, generated_outputs):\n+    def post_process_image_text_to_text(\n+        self, generated_outputs, skip_special_tokens=True, clean_up_tokenization_spaces=False, **kwargs\n+    ):\n         \"\"\"\n         Post-process the output of the model to decode the text.\n \n         Args:\n             generated_outputs (`torch.Tensor` or `np.ndarray`):\n                 The output of the model `generate` function. The output is expected to be a tensor of shape `(batch_size, sequence_length)`\n                 or `(sequence_length,)`.\n+            skip_special_tokens (`bool`, *optional*, defaults to `True`):\n+                Whether or not to remove special tokens in the output. Argument passed to the tokenizer's `batch_decode` method.\n+            Clean_up_tokenization_spaces (`bool`, *optional*, defaults to `False`):\n+                Whether or not to clean up the tokenization spaces. Argument passed to the tokenizer's `batch_decode` method.\n+            **kwargs:\n+                Additional arguments to be passed to the tokenizer's `batch_decode method`.\n \n         Returns:\n             `List[str]`: The decoded text.\n         \"\"\"\n         return self.tokenizer.batch_decode(\n-            generated_outputs, skip_special_tokens=True, clean_up_tokenization_spaces=False\n+            generated_outputs,\n+            skip_special_tokens=skip_special_tokens,\n+            clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n+            **kwargs,\n         )\n \n     @property"
        },
        {
            "sha": "90720ad586e29a0980fcb126b89a9c5a909af60e",
            "filename": "src/transformers/models/qwen2_vl/processing_qwen2_vl.py",
            "status": "modified",
            "additions": 13,
            "deletions": 2,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/9f51dc25357bcde280a02b59e80b66248b018ca4/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fprocessing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9f51dc25357bcde280a02b59e80b66248b018ca4/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fprocessing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fprocessing_qwen2_vl.py?ref=9f51dc25357bcde280a02b59e80b66248b018ca4",
            "patch": "@@ -170,20 +170,31 @@ def decode(self, *args, **kwargs):\n         \"\"\"\n         return self.tokenizer.decode(*args, **kwargs)\n \n-    def post_process_image_text_to_text(self, generated_outputs):\n+    def post_process_image_text_to_text(\n+        self, generated_outputs, skip_special_tokens=True, clean_up_tokenization_spaces=False, **kwargs\n+    ):\n         \"\"\"\n         Post-process the output of the model to decode the text.\n \n         Args:\n             generated_outputs (`torch.Tensor` or `np.ndarray`):\n                 The output of the model `generate` function. The output is expected to be a tensor of shape `(batch_size, sequence_length)`\n                 or `(sequence_length,)`.\n+            skip_special_tokens (`bool`, *optional*, defaults to `True`):\n+                Whether or not to remove special tokens in the output. Argument passed to the tokenizer's `batch_decode` method.\n+            Clean_up_tokenization_spaces (`bool`, *optional*, defaults to `False`):\n+                Whether or not to clean up the tokenization spaces. Argument passed to the tokenizer's `batch_decode` method.\n+            **kwargs:\n+                Additional arguments to be passed to the tokenizer's `batch_decode method`.\n \n         Returns:\n             `List[str]`: The decoded text.\n         \"\"\"\n         return self.tokenizer.batch_decode(\n-            generated_outputs, skip_special_tokens=True, clean_up_tokenization_spaces=False\n+            generated_outputs,\n+            skip_special_tokens=skip_special_tokens,\n+            clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n+            **kwargs,\n         )\n \n     @property"
        },
        {
            "sha": "6b743997f5ee632171613ed07969b89799155a73",
            "filename": "src/transformers/pipelines/image_text_to_text.py",
            "status": "modified",
            "additions": 26,
            "deletions": 8,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/9f51dc25357bcde280a02b59e80b66248b018ca4/src%2Ftransformers%2Fpipelines%2Fimage_text_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9f51dc25357bcde280a02b59e80b66248b018ca4/src%2Ftransformers%2Fpipelines%2Fimage_text_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fimage_text_to_text.py?ref=9f51dc25357bcde280a02b59e80b66248b018ca4",
            "patch": "@@ -14,6 +14,7 @@\n # limitations under the License.\n \n import enum\n+from collections.abc import Iterable  # pylint: disable=g-importing-member\n from typing import Dict, List, Optional, Union\n \n from ..processing_utils import ProcessingKwargs, Unpack\n@@ -71,6 +72,8 @@ def retrieve_images_in_messages(\n     \"\"\"\n     if images is None:\n         images = []\n+    elif not isinstance(images, Iterable):\n+        images = [images]\n     idx_images = 0\n     retrieved_images = []\n     for message in messages:\n@@ -188,14 +191,15 @@ def _sanitize_parameters(\n         return_full_text=None,\n         return_tensors=None,\n         return_type=None,\n+        clean_up_tokenization_spaces=None,\n+        stop_sequence=None,\n         continue_final_message=None,\n         **kwargs: Unpack[ProcessingKwargs],\n     ):\n         forward_kwargs = {}\n         preprocess_params = {}\n         postprocess_params = {}\n-\n-        preprocess_params[\"processing_kwargs\"] = kwargs\n+        preprocess_params.update(kwargs)\n \n         if timeout is not None:\n             preprocess_params[\"timeout\"] = timeout\n@@ -226,7 +230,16 @@ def _sanitize_parameters(\n             postprocess_params[\"return_type\"] = return_type\n         if continue_final_message is not None:\n             postprocess_params[\"continue_final_message\"] = continue_final_message\n-\n+        if clean_up_tokenization_spaces is not None:\n+            postprocess_params[\"clean_up_tokenization_spaces\"] = clean_up_tokenization_spaces\n+        if stop_sequence is not None:\n+            stop_sequence_ids = self.processor.tokenizer.encode(stop_sequence, add_special_tokens=False)\n+            if len(stop_sequence_ids) > 1:\n+                logger.warning_once(\n+                    \"Stopping on a multiple token sequence is not yet supported on transformers. The first token of\"\n+                    \" the stop sequence will be used as the stop sequence string in the interim.\"\n+                )\n+            generate_kwargs[\"eos_token_id\"] = stop_sequence_ids[0]\n         return preprocess_params, forward_kwargs, postprocess_params\n \n     def __call__(\n@@ -264,6 +277,8 @@ def __call__(\n             return_full_text (`bool`, *optional*, defaults to `True`):\n                 If set to `False` only added text is returned, otherwise the full text is returned. Cannot be\n                 specified at the same time as `return_text`.\n+            clean_up_tokenization_spaces (`bool`, *optional*, defaults to `True`):\n+                Whether or not to clean up the potential extra spaces in the text output.\n             continue_final_message( `bool`, *optional*): This indicates that you want the model to continue the\n                 last message in the input chat rather than starting a new one, allowing you to \"prefill\" its response.\n                 By default this is `True` when the final message in the input chat has the `assistant` role and\n@@ -315,7 +330,7 @@ def __call__(\n \n         return super().__call__({\"images\": images, \"text\": text}, **kwargs)\n \n-    def preprocess(self, inputs=None, timeout=None, continue_final_message=None, processing_kwargs=None):\n+    def preprocess(self, inputs=None, timeout=None, continue_final_message=None, **processing_kwargs):\n         # In case we only have text inputs\n         if isinstance(inputs, (list, tuple, str)):\n             images = None\n@@ -332,6 +347,7 @@ def preprocess(self, inputs=None, timeout=None, continue_final_message=None, pro\n                     add_generation_prompt=not continue_final_message,\n                     continue_final_message=continue_final_message,\n                     return_tensors=self.framework,\n+                    **processing_kwargs,\n                 )\n                 inputs_text = inputs\n                 images = inputs.images\n@@ -340,7 +356,7 @@ def preprocess(self, inputs=None, timeout=None, continue_final_message=None, pro\n                 inputs_text = inputs[\"text\"]\n                 images = inputs[\"images\"]\n \n-            images = load_images(images)\n+            images = load_images(images, timeout=timeout)\n \n         # if batched text inputs, we set padding to True unless specified otherwise\n         if isinstance(text, (list, tuple)) and len(text) > 1:\n@@ -363,7 +379,9 @@ def _forward(self, model_inputs, generate_kwargs=None):\n \n         return {\"generated_sequence\": generated_sequence, \"prompt_text\": prompt_text, \"input_ids\": input_ids}\n \n-    def postprocess(self, model_outputs, return_type=ReturnType.FULL_TEXT, continue_final_message=None):\n+    def postprocess(\n+        self, model_outputs, return_type=ReturnType.FULL_TEXT, continue_final_message=None, **postprocess_kwargs\n+    ):\n         input_texts = model_outputs[\"prompt_text\"]\n         input_texts = [input_texts] if isinstance(input_texts, (str, Chat)) else input_texts\n         generated_sequence = model_outputs[\"generated_sequence\"]\n@@ -375,8 +393,8 @@ def postprocess(self, model_outputs, return_type=ReturnType.FULL_TEXT, continue_\n             ]\n \n         # Decode inputs and outputs the same way to remove input text from generated text if present\n-        generated_texts = self.processor.post_process_image_text_to_text(generated_sequence)\n-        decoded_inputs = self.processor.post_process_image_text_to_text(input_ids)\n+        generated_texts = self.processor.post_process_image_text_to_text(generated_sequence, **postprocess_kwargs)\n+        decoded_inputs = self.processor.post_process_image_text_to_text(input_ids, **postprocess_kwargs)\n \n         # Force consistent behavior for including the input text in the output\n         if return_type in {ReturnType.NEW_TEXT, ReturnType.FULL_TEXT}:"
        },
        {
            "sha": "990a237d0a36d8ab812a7095d220e5e42a251d62",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/9f51dc25357bcde280a02b59e80b66248b018ca4/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9f51dc25357bcde280a02b59e80b66248b018ca4/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=9f51dc25357bcde280a02b59e80b66248b018ca4",
            "patch": "@@ -1392,19 +1392,23 @@ def apply_chat_template(\n                 return out[\"input_ids\"]\n         return prompt\n \n-    def post_process_image_text_to_text(self, generated_outputs):\n+    def post_process_image_text_to_text(self, generated_outputs, skip_special_tokens=True, **kwargs):\n         \"\"\"\n         Post-process the output of a vlm to decode the text.\n \n         Args:\n             generated_outputs (`torch.Tensor` or `np.ndarray`):\n                 The output of the model `generate` function. The output is expected to be a tensor of shape `(batch_size, sequence_length)`\n                 or `(sequence_length,)`.\n+            skip_special_tokens (`bool`, *optional*, defaults to `True`):\n+                Whether or not to remove special tokens in the output. Argument passed to the tokenizer's `batch_decode` method.\n+            **kwargs:\n+                Additional arguments to be passed to the tokenizer's `batch_decode method`.\n \n         Returns:\n             `List[str]`: The decoded text.\n         \"\"\"\n-        return self.tokenizer.batch_decode(generated_outputs, skip_special_tokens=True)\n+        return self.tokenizer.batch_decode(generated_outputs, skip_special_tokens=skip_special_tokens, **kwargs)\n \n \n def _validate_images_text_input_order(images, text):"
        },
        {
            "sha": "903e90919c2cec4517fff0a7554ff6c2475688e0",
            "filename": "tests/pipelines/test_pipelines_image_text_to_text.py",
            "status": "modified",
            "additions": 8,
            "deletions": 21,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/9f51dc25357bcde280a02b59e80b66248b018ca4/tests%2Fpipelines%2Ftest_pipelines_image_text_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9f51dc25357bcde280a02b59e80b66248b018ca4/tests%2Fpipelines%2Ftest_pipelines_image_text_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_image_text_to_text.py?ref=9f51dc25357bcde280a02b59e80b66248b018ca4",
            "patch": "@@ -124,7 +124,7 @@ def test_model_pt_chat_template(self):\n                 ],\n             }\n         ]\n-        outputs = pipe([image_ny, image_chicago], text=messages)\n+        outputs = pipe([image_ny, image_chicago], text=messages, return_full_text=False, max_new_tokens=10)\n         self.assertEqual(\n             outputs,\n             [\n@@ -139,20 +139,7 @@ def test_model_pt_chat_template(self):\n                             ],\n                         }\n                     ],\n-                    \"generated_text\": [\n-                        {\n-                            \"role\": \"user\",\n-                            \"content\": [\n-                                {\"type\": \"text\", \"text\": \"Whatâ€™s the difference between these two images?\"},\n-                                {\"type\": \"image\"},\n-                                {\"type\": \"image\"},\n-                            ],\n-                        },\n-                        {\n-                            \"role\": \"assistant\",\n-                            \"content\": \"The first image shows a statue of the Statue of Liberty in the foreground, while the second image shows\",\n-                        },\n-                    ],\n+                    \"generated_text\": \"The first image shows a statue of Liberty in the\",\n                 }\n             ],\n         )\n@@ -179,7 +166,7 @@ def test_model_pt_chat_template_continue_final_message(self):\n                 ],\n             },\n         ]\n-        outputs = pipe(text=messages)\n+        outputs = pipe(text=messages, max_new_tokens=10)\n         self.assertEqual(\n             outputs,\n             [\n@@ -213,7 +200,7 @@ def test_model_pt_chat_template_continue_final_message(self):\n                             \"content\": [\n                                 {\n                                     \"type\": \"text\",\n-                                    \"text\": \"There is a dog and a person in the image. The dog is sitting on the sand, and the person is sitting on\",\n+                                    \"text\": \"There is a dog and a person in the image. The dog is sitting\",\n                                 }\n                             ],\n                         },\n@@ -238,7 +225,7 @@ def test_model_pt_chat_template_new_text(self):\n                 ],\n             }\n         ]\n-        outputs = pipe(text=messages, return_full_text=False)\n+        outputs = pipe(text=messages, return_full_text=False, max_new_tokens=10)\n         self.assertEqual(\n             outputs,\n             [\n@@ -255,15 +242,15 @@ def test_model_pt_chat_template_new_text(self):\n                             ],\n                         }\n                     ],\n-                    \"generated_text\": \"In the image, a woman is sitting on the sandy beach, her legs crossed in a relaxed manner\",\n+                    \"generated_text\": \"In the image, a woman is sitting on the\",\n                 }\n             ],\n         )\n \n     @slow\n     @require_torch\n     def test_model_pt_chat_template_image_url(self):\n-        pipe = pipeline(\"image-text-to-text\", model=\"llava-hf/llava-onevision-qwen2-0.5b-ov-hf\")\n+        pipe = pipeline(\"image-text-to-text\", model=\"llava-hf/llava-interleave-qwen-0.5b-hf\")\n         messages = [\n             {\n                 \"role\": \"user\",\n@@ -279,7 +266,7 @@ def test_model_pt_chat_template_image_url(self):\n             }\n         ]\n         outputs = pipe(text=messages, return_full_text=False, max_new_tokens=10)[0][\"generated_text\"]\n-        self.assertEqual(outputs, \"The image captures the iconic Statue of Liberty, a\")\n+        self.assertEqual(outputs, \"A statue of liberty in the foreground of a city\")\n \n     @slow\n     @require_torch"
        }
    ],
    "stats": {
        "total": 132,
        "additions": 91,
        "deletions": 41
    }
}