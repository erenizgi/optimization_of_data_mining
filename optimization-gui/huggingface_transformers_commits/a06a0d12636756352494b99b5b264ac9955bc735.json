{
    "author": "maximizemaxwell",
    "message": "ğŸŒ [i18n-KO] Translated bert.md to Korean  (#34627)\n\n* Translated bert.md, Need additional check\r\n\r\n* Translation 2nd ver, changed _toctree.yml\r\n\r\n* Fixed Typo\r\n\r\n* Update bert.md\r\n\r\nCo-authored-by: YONGSANG <71686691+4N3MONE@users.noreply.github.com>\r\n\r\n* Update bert.md\r\n\r\nCo-authored-by: YONGSANG <71686691+4N3MONE@users.noreply.github.com>\r\n\r\n* Update bert.md\r\n\r\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\r\n\r\n* Update bert.md\r\n\r\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\r\n\r\n---------\r\n\r\nCo-authored-by: YONGSANG <71686691+4N3MONE@users.noreply.github.com>\r\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "a06a0d12636756352494b99b5b264ac9955bc735",
    "files": [
        {
            "sha": "0cafd918af540991e0be8a7060b0642ac9ee7bee",
            "filename": "docs/source/ko/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a06a0d12636756352494b99b5b264ac9955bc735/docs%2Fsource%2Fko%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/a06a0d12636756352494b99b5b264ac9955bc735/docs%2Fsource%2Fko%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2F_toctree.yml?ref=a06a0d12636756352494b99b5b264ac9955bc735",
            "patch": "@@ -326,8 +326,8 @@\n         title: BARThez\n       - local: model_doc/bartpho\n         title: BARTpho\n-      - local: in_translation\n-        title: (ë²ˆì—­ì¤‘) BERT\n+      - local: model_doc/bert\n+        title: BERT\n       - local: in_translation\n         title: (ë²ˆì—­ì¤‘) BertGeneration\n       - local: model_doc/bert-japanese"
        },
        {
            "sha": "531d3e3dd6394766531b64adacf280d6392576c4",
            "filename": "docs/source/ko/model_doc/bert.md",
            "status": "added",
            "additions": 340,
            "deletions": 0,
            "changes": 340,
            "blob_url": "https://github.com/huggingface/transformers/blob/a06a0d12636756352494b99b5b264ac9955bc735/docs%2Fsource%2Fko%2Fmodel_doc%2Fbert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a06a0d12636756352494b99b5b264ac9955bc735/docs%2Fsource%2Fko%2Fmodel_doc%2Fbert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fbert.md?ref=a06a0d12636756352494b99b5b264ac9955bc735",
            "patch": "@@ -0,0 +1,340 @@\n+<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# BERT[[BERT]]\n+\n+<div class=\"flex flex-wrap space-x-1\">\n+<a href=\"https://huggingface.co/models?filter=bert\">\n+<img alt=\"Models\" src=\"https://img.shields.io/badge/All_model_pages-bert-blueviolet\">\n+</a>\n+<a href=\"https://huggingface.co/spaces/docs-demos/bert-base-uncased\">\n+<img alt=\"Spaces\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue\">\n+</a>\n+</div>\n+\n+## ê°œìš”[[Overview]]\n+\n+BERT ëª¨ë¸ì€ Jacob Devlin. Ming-Wei Chang, Kenton Lee, Kristina Touranovaê°€ ì œì•ˆí•œ ë…¼ë¬¸ [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)ì—ì„œ ì†Œê°œë˜ì—ˆìŠµë‹ˆë‹¤. BERTëŠ” ì‚¬ì „ í•™ìŠµëœ ì–‘ë°©í–¥ íŠ¸ëœìŠ¤í¬ë¨¸ë¡œ,  Toronto Book Corpusì™€ Wikipediaë¡œ êµ¬ì„±ëœ ëŒ€ê·œëª¨ ì½”í¼ìŠ¤ì—ì„œ ë§ˆìŠ¤í‚¹ëœ ì–¸ì–´ ëª¨ë¸ë§ê³¼ ë‹¤ìŒ ë¬¸ì¥ ì˜ˆì¸¡(Next Sentence Prediction) ëª©í‘œë¥¼ ê²°í•©í•´ í•™ìŠµë˜ì—ˆìŠµë‹ˆë‹¤.\n+\n+í•´ë‹¹ ë…¼ë¬¸ì˜ ì´ˆë¡ì…ë‹ˆë‹¤:\n+\n+*ìš°ë¦¬ëŠ” BERT(Bidirectional Encoder Representations from Transformers)ë¼ëŠ” ìƒˆë¡œìš´ ì–¸ì–´ í‘œí˜„ ëª¨ë¸ì„ ì†Œê°œí•©ë‹ˆë‹¤. ìµœê·¼ì˜ ë‹¤ë¥¸ ì–¸ì–´ í‘œí˜„ ëª¨ë¸ë“¤ê³¼ ë‹¬ë¦¬, BERTëŠ” ëª¨ë“  ê³„ì¸µì—ì„œ ì–‘ë°©í–¥ìœ¼ë¡œ ì–‘ìª½ ë¬¸ë§¥ì„ ì¡°ê±´ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ ë¹„ì§€ë„ í•™ìŠµëœ í…ìŠ¤íŠ¸ì—ì„œ ê¹Šì´ ìˆëŠ” ì–‘ë°©í–¥ í‘œí˜„ì„ ì‚¬ì „ í•™ìŠµí•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. ê·¸ ê²°ê³¼, ì‚¬ì „ í•™ìŠµëœ BERT ëª¨ë¸ì€ ì¶”ê°€ì ì¸ ì¶œë ¥ ê³„ì¸µ í•˜ë‚˜ë§Œìœ¼ë¡œ ì§ˆë¬¸ ì‘ë‹µ, ì–¸ì–´ ì¶”ë¡ ê³¼ ê°™ì€ ë‹¤ì–‘í•œ ì‘ì—…ì—ì„œ ë¯¸ì„¸ ì¡°ì •ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, íŠ¹ì • ì‘ì—…ì„ ìœ„í•´ ì•„í‚¤í…ì²˜ë¥¼ ìˆ˜ì •í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤.*\n+\n+*BERTëŠ” ê°œë…ì ìœ¼ë¡œ ë‹¨ìˆœí•˜ë©´ì„œë„ ì‹¤ì¦ì ìœ¼ë¡œ ê°•ë ¥í•œ ëª¨ë¸ì…ë‹ˆë‹¤. BERTëŠ” 11ê°œì˜ ìì—°ì–´ ì²˜ë¦¬ ê³¼ì œì—ì„œ ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆìœ¼ë©°, GLUE ì ìˆ˜ë¥¼ 80.5% (7.7% í¬ì¸íŠ¸ ì ˆëŒ€ ê°œì„ )ë¡œ, MultiNLI ì •í™•ë„ë¥¼ 86.7% (4.6% í¬ì¸íŠ¸ ì ˆëŒ€ ê°œì„ ), SQuAD v1.1 ì§ˆë¬¸ ì‘ë‹µ í…ŒìŠ¤íŠ¸ì—ì„œ F1 ì ìˆ˜ë¥¼ 93.2 (1.5% í¬ì¸íŠ¸ ì ˆëŒ€ ê°œì„ )ë¡œ, SQuAD v2.0ì—ì„œ F1 ì ìˆ˜ë¥¼ 83.1 (5.1% í¬ì¸íŠ¸ ì ˆëŒ€ ê°œì„ )ë¡œ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤.*\n+\n+ì´ ëª¨ë¸ì€ [thomwolf](https://huggingface.co/thomwolf)ê°€ ê¸°ì—¬í•˜ì˜€ìŠµë‹ˆë‹¤. ì›ë³¸ ì½”ë“œëŠ” [ì—¬ê¸°](https://github.com/google-research/bert)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+## ì‚¬ìš© íŒ[[Usage tips]]\n+\n+- BERTëŠ” ì ˆëŒ€ ìœ„ì¹˜ ì„ë² ë”©ì„ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ì´ë¯€ë¡œ ì…ë ¥ì„ ì™¼ìª½ì´ ì•„ë‹ˆë¼ ì˜¤ë¥¸ìª½ì—ì„œ íŒ¨ë”©í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ìœ¼ë¡œ ê¶Œì¥ë©ë‹ˆë‹¤.\n+- BERTëŠ” ë§ˆìŠ¤í‚¹ëœ ì–¸ì–´ ëª¨ë¸(MLM)ê³¼ Next Sentence Prediction(NSP) ëª©í‘œë¡œ í•™ìŠµë˜ì—ˆìŠµë‹ˆë‹¤. ì´ëŠ” ë§ˆìŠ¤í‚¹ëœ í† í° ì˜ˆì¸¡ê³¼ ì „ë°˜ì ì¸ ìì—°ì–´ ì´í•´(NLU)ì— ë›°ì–´ë‚˜ì§€ë§Œ, í…ìŠ¤íŠ¸ ìƒì„±ì—ëŠ” ìµœì í™”ë˜ì–´ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.    \n+- BERTì˜ ì‚¬ì „ í•™ìŠµ ê³¼ì •ì—ì„œëŠ” ì…ë ¥ ë°ì´í„°ë¥¼ ë¬´ì‘ìœ„ë¡œ ë§ˆìŠ¤í‚¹í•˜ì—¬ ì¼ë¶€ í† í°ì„ ë§ˆìŠ¤í‚¹í•©ë‹ˆë‹¤. ì „ì²´ í† í° ì¤‘ ì•½ 15%ê°€ ë‹¤ìŒê³¼ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ë§ˆìŠ¤í‚¹ë©ë‹ˆë‹¤:\n+\n+    * 80% í™•ë¥ ë¡œ ë§ˆìŠ¤í¬ í† í°ìœ¼ë¡œ ëŒ€ì²´\n+    * 10% í™•ë¥ ë¡œ ì„ì˜ì˜ ë‹¤ë¥¸ í† í°ìœ¼ë¡œ ëŒ€ì²´\n+    * 10% í™•ë¥ ë¡œ ì›ë˜ í† í° ê·¸ëŒ€ë¡œ ìœ ì§€\n+\n+- ëª¨ë¸ì˜ ì£¼ìš” ëª©í‘œëŠ” ì›ë³¸ ë¬¸ì¥ì„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì´ì§€ë§Œ, ë‘ ë²ˆì§¸ ëª©í‘œê°€ ìˆìŠµë‹ˆë‹¤: ì…ë ¥ìœ¼ë¡œ ë¬¸ì¥ Aì™€ B (ì‚¬ì´ì—ëŠ” êµ¬ë¶„ í† í°ì´ ìˆìŒ)ê°€ ì£¼ì–´ì§‘ë‹ˆë‹¤. ì´ ë¬¸ì¥ ìŒì´ ì—°ì†ë  í™•ë¥ ì€ 50%ì´ë©°, ë‚˜ë¨¸ì§€ 50%ëŠ” ì„œë¡œ ë¬´ê´€í•œ ë¬¸ì¥ë“¤ì…ë‹ˆë‹¤. ëª¨ë¸ì€ ì´ ë‘ ë¬¸ì¥ì´ ì•„ë‹Œì§€ë¥¼ ì˜ˆì¸¡í•´ì•¼ í•©ë‹ˆë‹¤.\n+\n+### Scaled Dot Product Attention(SDPA) ì‚¬ìš©í•˜ê¸° [[Using Scaled Dot Product Attention (SDPA)]]\n+\n+PytorchëŠ” `torch.nn.functional`ì˜ ì¼ë¶€ë¡œ Scaled Dot Product Attention(SDPA) ì—°ì‚°ìë¥¼ ê¸°ë³¸ì ìœ¼ë¡œ ì œê³µí•©ë‹ˆë‹¤. ì´ í•¨ìˆ˜ëŠ” ì…ë ¥ê³¼ í•˜ë“œì›¨ì–´ì— ë”°ë¼ ì—¬ëŸ¬ êµ¬í˜„ ë°©ì‹ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [ê³µì‹ ë¬¸ì„œ](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)ë‚˜ [GPU Inference](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#pytorch-scaled-dot-product-attention)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+`torch>=2.1.1`ì—ì„œëŠ” êµ¬í˜„ì´ ê°€ëŠ¥í•œ ê²½ìš° SDPAê°€ ê¸°ë³¸ì ìœ¼ë¡œ ì‚¬ìš©ë˜ì§€ë§Œ, `from_pretrained()`í•¨ìˆ˜ì—ì„œ `attn_implementation=\"sdpa\"`ë¥¼ ì„¤ì •í•˜ì—¬ SDPAë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì‚¬ìš©í•˜ë„ë¡ ì§€ì •í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n+\n+```\n+from transformers import BertModel\n+\n+model = BertModel.from_pretrained(\"bert-base-uncased\", torch_dtype=torch.float16, attn_implementation=\"sdpa\")\n+...\n+```\n+\n+ìµœì  ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•´ ëª¨ë¸ì„ ë°˜ì •ë°€ë„(ì˜ˆ: `torch.float16` ë˜ëŠ” `torch.bfloat16`)ë¡œ ë¶ˆëŸ¬ì˜¤ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.\n+\n+ë¡œì»¬ ë²¤ì¹˜ë§ˆí¬ (A100-80GB, CPUx12, RAM 96.6GB, PyTorch 2.2.0, OS Ubuntu 22.04)ì—ì„œ `float16`ì„ ì‚¬ìš©í•´ í•™ìŠµ ë° ì¶”ë¡ ì„ ìˆ˜í–‰í•œ ê²°ê³¼, ë‹¤ìŒê³¼ ê°™ì€ ì†ë„ í–¥ìƒì´ ê´€ì°°ë˜ì—ˆìŠµë‹ˆë‹¤.\n+\n+#### í•™ìŠµ [[Training]]\n+\n+|batch_size|seq_len|Time per batch (eager - s)|Time per batch (sdpa - s)|Speedup (%)|Eager peak mem (MB)|sdpa peak mem (MB)|Mem saving (%)|\n+|----------|-------|--------------------------|-------------------------|-----------|-------------------|------------------|--------------|\n+|4         |256    |0.023                     |0.017                    |35.472     |939.213            |764.834           |22.800        |\n+|4         |512    |0.023                     |0.018                    |23.687     |1970.447           |1227.162          |60.569        |\n+|8         |256    |0.023                     |0.018                    |23.491     |1594.295           |1226.114          |30.028        |\n+|8         |512    |0.035                     |0.025                    |43.058     |3629.401           |2134.262          |70.054        |\n+|16        |256    |0.030                     |0.024                    |25.583     |2874.426           |2134.262          |34.680        |\n+|16        |512    |0.064                     |0.044                    |46.223     |6964.659           |3961.013          |75.830        |\n+\n+#### ì¶”ë¡  [[Inference]]\n+\n+|batch_size|seq_len|Per token latency eager (ms)|Per token latency SDPA (ms)|Speedup (%)|Mem eager (MB)|Mem BT (MB)|Mem saved (%)|\n+|----------|-------|----------------------------|---------------------------|-----------|--------------|-----------|-------------|\n+|1         |128    |5.736                       |4.987                      |15.022     |282.661       |282.924    |-0.093       |\n+|1         |256    |5.689                       |4.945                      |15.055     |298.686       |298.948    |-0.088       |\n+|2         |128    |6.154                       |4.982                      |23.521     |314.523       |314.785    |-0.083       |\n+|2         |256    |6.201                       |4.949                      |25.303     |347.546       |347.033    |0.148        |\n+|4         |128    |6.049                       |4.987                      |21.305     |378.895       |379.301    |-0.107       |\n+|4         |256    |6.285                       |5.364                      |17.166     |443.209       |444.382    |-0.264       |\n+\n+\n+\n+## ìë£Œ[[Resources]]\n+\n+BERTë¥¼ ì‹œì‘í•˜ëŠ” ë° ë„ì›€ì´ ë˜ëŠ” Hugging Faceì™€ community ìë£Œ ëª©ë¡(ğŸŒë¡œ í‘œì‹œë¨) ì…ë‹ˆë‹¤. ì—¬ê¸°ì— í¬í•¨ë  ìë£Œë¥¼ ì œì¶œí•˜ê³  ì‹¶ë‹¤ë©´ PR(Pull Request)ë¥¼ ì—´ì–´ì£¼ì„¸ìš”. ë¦¬ë·° í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤! ìë£ŒëŠ” ê¸°ì¡´ ìë£Œë¥¼ ë³µì œí•˜ëŠ” ëŒ€ì‹  ìƒˆë¡œìš´ ë‚´ìš©ì„ ë‹´ê³  ìˆì–´ì•¼ í•©ë‹ˆë‹¤.\n+\n+<PipelineTag pipeline=\"text-classification\"/>\n+\n+- [BERT í…ìŠ¤íŠ¸ ë¶„ë¥˜ (ë‹¤ë¥¸ ì–¸ì–´ë¡œ)](https://www.philschmid.de/bert-text-classification-in-a-different-language)ì— ëŒ€í•œ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸.\n+- [ë‹¤ì¤‘ ë ˆì´ë¸” í…ìŠ¤íŠ¸ ë¶„ë¥˜ë¥¼ ìœ„í•œ BERT (ë° ê´€ë ¨ ëª¨ë¸) ë¯¸ì„¸ ì¡°ì •](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/BERT/Fine_tuning_BERT_(and_friends)_for_multi_label_text_classification.ipynb)ì— ëŒ€í•œ ë…¸íŠ¸ë¶.\n+- [PyTorchë¥¼ ì´ìš©í•´ BERTë¥¼ ë‹¤ì¤‘ ë ˆì´ë¸” ë¶„ë¥˜ë¥¼ ìœ„í•´ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ë°©ë²•](httê¸°ps://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_multi_label_classification.ipynb)ì— ëŒ€í•œ ë…¸íŠ¸ë¶. ğŸŒ\n+- [BERTë¡œ EncoderDecoder ëª¨ë¸ì„ warm-startí•˜ì—¬ ìš”ì•½í•˜ê¸°](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/BERT2BERT_for_CNN_Dailymail.ipynb)ì— ëŒ€í•œ ë…¸íŠ¸ë¶.\n+- [`BertForSequenceClassification`]ì´  [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification)ì™€ [ë…¸íŠ¸ë¶](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification.ipynb)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.\n+- [`TFBertForSequenceClassification`]ì´ [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/text-classification)ì™€ [ë…¸íŠ¸ë¶](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification-tf.ipynb)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.\n+- [`FlaxBertForSequenceClassification`]ì´ [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/flax/text-classification)ì™€ [ë…¸íŠ¸ë¶](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification_flax.ipynb)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.\n+- [í…ìŠ¤íŠ¸ ë¶„ë¥˜ ì‘ì—… ê°€ì´ë“œ](../tasks/sequence_classification)\n+\n+<PipelineTag pipeline=\"token-classification\"/>\n+\n+- [Kerasì™€ í•¨ê»˜ Hugging Face Transformersë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹„ì˜ë¦¬ BERTë¥¼ ê°œì²´ëª… ì¸ì‹(NER)ìš©ìœ¼ë¡œ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ë°©ë²•](https://www.philschmid.de/huggingface-transformers-keras-tf)ì— ëŒ€í•œ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸.\n+- [BERTë¥¼ ê°œì²´ëª… ì¸ì‹ì„ ìœ„í•´ ë¯¸ì„¸ ì¡°ì •í•˜ê¸°](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/BERT/Custom_Named_Entity_Recognition_with_BERT_only_first_wordpiece.ipynb)ì— ëŒ€í•œ ë…¸íŠ¸ë¶. ê° ë‹¨ì–´ì˜ ì²« ë²ˆì§¸ wordpieceì—ë§Œ ë ˆì´ë¸”ì„ ì§€ì •í•˜ì—¬ í•™ìŠµí•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤. ëª¨ë“  wordpieceì— ë ˆì´ë¸”ì„ ì „íŒŒí•˜ëŠ” ë°©ë²•ì€ [ì´ ë²„ì „](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/BERT/Custom_Named_Entity_Recognition_with_BERT.ipynb)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+- [`BertForTokenClassification`]ì´  [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/pytorch/token-classification)ì™€  [ë…¸íŠ¸ë¶](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification.ipynb)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.\n+- [`TFBertForTokenClassification`]ì´ [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/token-classification)ì™€ [ë…¸íŠ¸ë¶](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification-tf.ipynb)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.\n+- [`FlaxBertForTokenClassification`]ì´ [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/flax/token-classification)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.\n+- ğŸ¤— Hugging Face ì½”ìŠ¤ì˜ [í† í° ë¶„ë¥˜ ì±•í„°](https://huggingface.co/course/chapter7/2?fw=pt).\n+- [í† í° ë¶„ë¥˜ ì‘ì—… ê°€ì´ë“œ](../tasks/token_classification)\n+\n+<PipelineTag pipeline=\"fill-mask\"/>\n+\n+- [`BertForMaskedLM`]ì´ [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling#robertabertdistilbert-and-masked-language-modeling)ì™€ [ë…¸íŠ¸ë¶](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.\n+- [`TFBertForMaskedLM`]ì´ [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/language-modeling#run_mlmpy) ì™€ [ë…¸íŠ¸ë¶](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.\n+- [`FlaxBertForMaskedLM`]ì´ [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/flax/language-modeling#masked-language-modeling)ì™€ [ë…¸íŠ¸ë¶](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/masked_language_modeling_flax.ipynb)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.\n+- ğŸ¤— Hugging Face ì½”ìŠ¤ì˜ [ë§ˆìŠ¤í‚¹ëœ ì–¸ì–´ ëª¨ë¸ë§ ì±•í„°](https://huggingface.co/course/chapter7/3?fw=pt).\n+- [ë§ˆìŠ¤í‚¹ëœ ì–¸ì–´ ëª¨ë¸ë§ ì‘ì—… ê°€ì´ë“œ](../tasks/masked_language_modeling)\n+\n+<PipelineTag pipeline=\"question-answering\"/>\n+\n+- [`BertForQuestionAnswering`]ì´ [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering)ì™€ [ë…¸íŠ¸ë¶](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering.ipynb)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.\n+- [`TFBertForQuestionAnswering`]ì´ [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/question-answering) ì™€ [ë…¸íŠ¸ë¶](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering-tf.ipynb)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.\n+- [`FlaxBertForQuestionAnswering`]ì´ [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/flax/question-answering)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.\n+- ğŸ¤— Hugging Face ì½”ìŠ¤ì˜ [ì§ˆë¬¸ ë‹µë³€ ì±•í„°](https://huggingface.co/course/chapter7/7?fw=pt).\n+- [ì§ˆë¬¸ ë‹µë³€ ì‘ì—… ê°€ì´ë“œ](../tasks/question_answering)\n+\n+**ë‹¤ì¤‘ ì„ íƒ**\n+- [`BertForMultipleChoice`]ì´ [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/pytorch/multiple-choice)ì™€ [ë…¸íŠ¸ë¶](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice.ipynb)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.\n+- [`TFBertForMultipleChoice`]ì´ [ì—ì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/multiple-choice)ì™€ [ë…¸íŠ¸ë¶](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice-tf.ipynb)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.\n+- [ë‹¤ì¤‘ ì„ íƒ ì‘ì—… ê°€ì´ë“œ](../tasks/multiple_choice)\n+\n+âš¡ï¸ **ì¶”ë¡ **\n+- [Hugging Face Transformersì™€ AWS Inferentiaë¥¼ ì‚¬ìš©í•˜ì—¬ BERT ì¶”ë¡ ì„ ê°€ì†í™”í•˜ëŠ” ë°©ë²•](https://huggingface.co/blog/bert-inferentia-sagemaker)ì— ëŒ€í•œ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸.\n+- [GPUì—ì„œ DeepSpeed-Inferenceë¡œ BERT ì¶”ë¡ ì„ ê°€ì†í™”í•˜ëŠ” ë°©ë²•](https://www.philschmid.de/bert-deepspeed-inference)ì— ëŒ€í•œ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸.\n+\n+âš™ï¸ **ì‚¬ì „ í•™ìŠµ**\n+- [Hugging Face Optimumìœ¼ë¡œ Transformersë¥¼ ONMXë¡œ ë³€í™˜í•˜ëŠ” ë°©ë²•](https://www.philschmid.de/pre-training-bert-habana)ì— ëŒ€í•œ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸.\n+\n+ğŸš€ **ë°°í¬**\n+- [Hugging Face Optimumìœ¼ë¡œ Transformersë¥¼ ONMXë¡œ ë³€í™˜í•˜ëŠ” ë°©ë²•](https://www.philschmid.de/convert-transformers-to-onnx)ì— ëŒ€í•œ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸.\n+- [AWSì—ì„œ Hugging Face Transformersë¥¼ ìœ„í•œ Habana Gaudi ë”¥ëŸ¬ë‹ í™˜ê²½ ì„¤ì • ë°©ë²•](https://www.philschmid.de/getting-started-habana-gaudi#conclusion)ì— ëŒ€í•œ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸.\n+- [Hugging Face Transformers, Amazon SageMaker ë° Terraform ëª¨ë“ˆì„ ì´ìš©í•œ BERT ìë™ í™•ì¥](https://www.philschmid.de/terraform-huggingface-amazon-sagemaker-advanced)ì— ëŒ€í•œ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸.\n+- [Hugging Face, AWS Lambda, Dockerë¥¼ í™œìš©í•˜ì—¬ ì„œë²„ë¦¬ìŠ¤ BERT ì„¤ì •í•˜ëŠ” ë°©ë²•](https://www.philschmid.de/serverless-bert-with-huggingface-aws-lambda-docker)ì— ëŒ€í•œ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸.\n+- [Amazon SageMakerì™€ Training Compilerë¥¼ ì‚¬ìš©í•˜ì—¬ Hugging Face Transformersì—ì„œ BERT ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ë°©ë²•](https://www.philschmid.de/huggingface-amazon-sagemaker-training-compiler)ì— ëŒ€í•œ ë¸”ë¡œê·¸.\n+- [Amazon SageMakerë¥¼ ì‚¬ìš©í•œ Transformersì™€ BERTì˜ ì‘ì—…ë³„ ì§€ì‹ ì¦ë¥˜](https://www.philschmid.de/knowledge-distillation-bert-transformers)ì— ëŒ€í•œ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸.\n+\n+## BertConfig\n+\n+[[autodoc]] BertConfig\n+    - all\n+\n+## BertTokenizer\n+\n+[[autodoc]] BertTokenizer\n+    - build_inputs_with_special_tokens\n+    - get_special_tokens_mask\n+    - create_token_type_ids_from_sequences\n+    - save_vocabulary\n+\n+<frameworkcontent>\n+<pt>\n+\n+## BertTokenizerFast\n+\n+[[autodoc]] BertTokenizerFast\n+\n+</pt>\n+<tf>\n+\n+## TFBertTokenizer\n+\n+[[autodoc]] TFBertTokenizer\n+\n+</tf>\n+</frameworkcontent>\n+\n+## Bert specific outputs\n+\n+[[autodoc]] models.bert.modeling_bert.BertForPreTrainingOutput\n+\n+[[autodoc]] models.bert.modeling_tf_bert.TFBertForPreTrainingOutput\n+\n+[[autodoc]] models.bert.modeling_flax_bert.FlaxBertForPreTrainingOutput\n+\n+\n+<frameworkcontent>\n+<pt>\n+\n+## BertModel\n+\n+[[autodoc]] BertModel\n+    - forward\n+\n+## BertForPreTraining\n+\n+[[autodoc]] BertForPreTraining\n+    - forward\n+\n+## BertLMHeadModel\n+\n+[[autodoc]] BertLMHeadModel\n+    - forward\n+\n+## BertForMaskedLM\n+\n+[[autodoc]] BertForMaskedLM\n+    - forward\n+\n+## BertForNextSentencePrediction\n+\n+[[autodoc]] BertForNextSentencePrediction\n+    - forward\n+\n+## BertForSequenceClassification\n+\n+[[autodoc]] BertForSequenceClassification\n+    - forward\n+\n+## BertForMultipleChoice\n+\n+[[autodoc]] BertForMultipleChoice\n+    - forward\n+\n+## BertForTokenClassification\n+\n+[[autodoc]] BertForTokenClassification\n+    - forward\n+\n+## BertForQuestionAnswering\n+\n+[[autodoc]] BertForQuestionAnswering\n+    - forward\n+\n+</pt>\n+<tf>\n+\n+## TFBertModel\n+\n+[[autodoc]] TFBertModel\n+    - call\n+\n+## TFBertForPreTraining\n+\n+[[autodoc]] TFBertForPreTraining\n+    - call\n+\n+## TFBertModelLMHeadModel\n+\n+[[autodoc]] TFBertLMHeadModel\n+    - call\n+\n+## TFBertForMaskedLM\n+\n+[[autodoc]] TFBertForMaskedLM\n+    - call\n+\n+## TFBertForNextSentencePrediction\n+\n+[[autodoc]] TFBertForNextSentencePrediction\n+    - call\n+\n+## TFBertForSequenceClassification\n+\n+[[autodoc]] TFBertForSequenceClassification\n+    - call\n+\n+## TFBertForMultipleChoice\n+\n+[[autodoc]] TFBertForMultipleChoice\n+    - call\n+\n+## TFBertForTokenClassification\n+\n+[[autodoc]] TFBertForTokenClassification\n+    - call\n+\n+## TFBertForQuestionAnswering\n+\n+[[autodoc]] TFBertForQuestionAnswering\n+    - call\n+\n+</tf>\n+<jax>\n+\n+## FlaxBertModel\n+\n+[[autodoc]] FlaxBertModel\n+    - __call__\n+\n+## FlaxBertForPreTraining\n+\n+[[autodoc]] FlaxBertForPreTraining\n+    - __call__\n+\n+## FlaxBertForCausalLM\n+\n+[[autodoc]] FlaxBertForCausalLM\n+    - __call__\n+\n+## FlaxBertForMaskedLM\n+\n+[[autodoc]] FlaxBertForMaskedLM\n+    - __call__\n+\n+## FlaxBertForNextSentencePrediction\n+\n+[[autodoc]] FlaxBertForNextSentencePrediction\n+    - __call__\n+\n+## FlaxBertForSequenceClassification\n+\n+[[autodoc]] FlaxBertForSequenceClassification\n+    - __call__\n+\n+## FlaxBertForMultipleChoice\n+\n+[[autodoc]] FlaxBertForMultipleChoice\n+    - __call__\n+\n+## FlaxBertForTokenClassification\n+\n+[[autodoc]] FlaxBertForTokenClassification\n+    - __call__\n+\n+## FlaxBertForQuestionAnswering\n+\n+[[autodoc]] FlaxBertForQuestionAnswering\n+    - __call__\n+\n+</jax>\n+</frameworkcontent>\n+\n+"
        }
    ],
    "stats": {
        "total": 344,
        "additions": 342,
        "deletions": 2
    }
}