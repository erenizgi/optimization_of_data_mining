{
    "author": "zucchini-nlp",
    "message": "Fix helper fn for new processor config format (#42085)\n\n* fix the helper fn for new processor config format\n\n* change the priority order\n\n* maybe we need to explicitly load and then decide\n\n* Apply suggestions from code review\n\nCo-authored-by: Pablo Montalvo <39954772+molbap@users.noreply.github.com>\n\n* use helper fn for json decoding\n\n---------\n\nCo-authored-by: Pablo Montalvo <39954772+molbap@users.noreply.github.com>",
    "sha": "5150dac727fa30fb066f5b495bb30539ef9a6a5f",
    "files": [
        {
            "sha": "844d9a215914feb4f83e86bd5097c3cd48c771f6",
            "filename": "src/transformers/feature_extraction_utils.py",
            "status": "modified",
            "additions": 45,
            "deletions": 32,
            "changes": 77,
            "blob_url": "https://github.com/huggingface/transformers/blob/5150dac727fa30fb066f5b495bb30539ef9a6a5f/src%2Ftransformers%2Ffeature_extraction_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5150dac727fa30fb066f5b495bb30539ef9a6a5f/src%2Ftransformers%2Ffeature_extraction_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ffeature_extraction_utils.py?ref=5150dac727fa30fb066f5b495bb30539ef9a6a5f",
            "patch": "@@ -39,6 +39,7 @@\n     is_torch_dtype,\n     logging,\n     requires_backends,\n+    safe_load_json_file,\n )\n from .utils.hub import cached_file\n \n@@ -427,35 +428,42 @@ def get_feature_extractor_dict(\n             feature_extractor_file = os.path.join(pretrained_model_name_or_path, FEATURE_EXTRACTOR_NAME)\n         if os.path.isfile(pretrained_model_name_or_path):\n             resolved_feature_extractor_file = pretrained_model_name_or_path\n+            resolved_processor_file = None\n             is_local = True\n         elif is_remote_url(pretrained_model_name_or_path):\n             feature_extractor_file = pretrained_model_name_or_path\n+            resolved_processor_file = None\n             resolved_feature_extractor_file = download_url(pretrained_model_name_or_path)\n         else:\n             feature_extractor_file = FEATURE_EXTRACTOR_NAME\n             try:\n                 # Load from local folder or from cache or download from model Hub and cache\n-                resolved_feature_extractor_files = [\n-                    resolved_file\n-                    for filename in [feature_extractor_file, PROCESSOR_NAME]\n-                    if (\n-                        resolved_file := cached_file(\n-                            pretrained_model_name_or_path,\n-                            filename=filename,\n-                            cache_dir=cache_dir,\n-                            force_download=force_download,\n-                            proxies=proxies,\n-                            local_files_only=local_files_only,\n-                            subfolder=subfolder,\n-                            token=token,\n-                            user_agent=user_agent,\n-                            revision=revision,\n-                            _raise_exceptions_for_missing_entries=False,\n-                        )\n-                    )\n-                    is not None\n-                ]\n-                resolved_feature_extractor_file = resolved_feature_extractor_files[0]\n+                resolved_processor_file = cached_file(\n+                    pretrained_model_name_or_path,\n+                    filename=PROCESSOR_NAME,\n+                    cache_dir=cache_dir,\n+                    force_download=force_download,\n+                    proxies=proxies,\n+                    local_files_only=local_files_only,\n+                    token=token,\n+                    user_agent=user_agent,\n+                    revision=revision,\n+                    subfolder=subfolder,\n+                    _raise_exceptions_for_missing_entries=False,\n+                )\n+                resolved_feature_extractor_file = cached_file(\n+                    pretrained_model_name_or_path,\n+                    filename=feature_extractor_file,\n+                    cache_dir=cache_dir,\n+                    force_download=force_download,\n+                    proxies=proxies,\n+                    local_files_only=local_files_only,\n+                    token=token,\n+                    user_agent=user_agent,\n+                    revision=revision,\n+                    subfolder=subfolder,\n+                    _raise_exceptions_for_missing_entries=False,\n+                )\n             except OSError:\n                 # Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\n                 # the original exception.\n@@ -469,19 +477,24 @@ def get_feature_extractor_dict(\n                     f\" directory containing a {FEATURE_EXTRACTOR_NAME} file\"\n                 )\n \n-        try:\n-            # Load feature_extractor dict\n-            with open(resolved_feature_extractor_file, encoding=\"utf-8\") as reader:\n-                text = reader.read()\n-            feature_extractor_dict = json.loads(text)\n-            if \"audio_processor\" in feature_extractor_dict:\n-                feature_extractor_dict = feature_extractor_dict[\"audio_processor\"]\n-            else:\n-                feature_extractor_dict = feature_extractor_dict.get(\"feature_extractor\", feature_extractor_dict)\n+        # Load feature_extractor dict. Priority goes as (nested config if found -> image processor config)\n+        # We are downloading both configs because almost all models have a `processor_config.json` but\n+        # not all of these are nested. We need to check if it was saved recebtly as nested or if it is legacy style\n+        feature_extractor_dict = None\n+        if resolved_processor_file is not None:\n+            processor_dict = safe_load_json_file(resolved_processor_file)\n+            if \"feature_extractor\" in processor_dict or \"audio_processor\" in processor_dict:\n+                feature_extractor_dict = processor_dict.get(\"feature_extractor\", processor_dict.get(\"audio_processor\"))\n+\n+        if resolved_feature_extractor_file is not None and feature_extractor_dict is None:\n+            feature_extractor_dict = safe_load_json_file(resolved_feature_extractor_file)\n \n-        except json.JSONDecodeError:\n+        if feature_extractor_dict is None:\n             raise OSError(\n-                f\"It looks like the config file at '{resolved_feature_extractor_file}' is not a valid JSON file.\"\n+                f\"Can't load feature extractor for '{pretrained_model_name_or_path}'. If you were trying to load\"\n+                \" it from 'https://huggingface.co/models', make sure you don't have a local directory with the\"\n+                f\" same name. Otherwise, make sure '{pretrained_model_name_or_path}' is the correct path to a\"\n+                f\" directory containing a {feature_extractor_file} file\"\n             )\n \n         if is_local:"
        },
        {
            "sha": "564af6be80816d5181363c1c32f07efac92886f8",
            "filename": "src/transformers/image_processing_base.py",
            "status": "modified",
            "additions": 45,
            "deletions": 30,
            "changes": 75,
            "blob_url": "https://github.com/huggingface/transformers/blob/5150dac727fa30fb066f5b495bb30539ef9a6a5f/src%2Ftransformers%2Fimage_processing_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5150dac727fa30fb066f5b495bb30539ef9a6a5f/src%2Ftransformers%2Fimage_processing_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_processing_base.py?ref=5150dac727fa30fb066f5b495bb30539ef9a6a5f",
            "patch": "@@ -32,6 +32,7 @@\n     is_offline_mode,\n     is_remote_url,\n     logging,\n+    safe_load_json_file,\n )\n from .utils.hub import cached_file\n \n@@ -280,35 +281,41 @@ def get_image_processor_dict(\n             image_processor_file = os.path.join(pretrained_model_name_or_path, image_processor_filename)\n         if os.path.isfile(pretrained_model_name_or_path):\n             resolved_image_processor_file = pretrained_model_name_or_path\n+            resolved_processor_file = None\n             is_local = True\n         elif is_remote_url(pretrained_model_name_or_path):\n             image_processor_file = pretrained_model_name_or_path\n+            resolved_processor_file = None\n             resolved_image_processor_file = download_url(pretrained_model_name_or_path)\n         else:\n             image_processor_file = image_processor_filename\n             try:\n-                # Load from local folder or from cache or download from model Hub and cache\n-                resolved_image_processor_files = [\n-                    resolved_file\n-                    for filename in [image_processor_file, PROCESSOR_NAME]\n-                    if (\n-                        resolved_file := cached_file(\n-                            pretrained_model_name_or_path,\n-                            filename=filename,\n-                            cache_dir=cache_dir,\n-                            force_download=force_download,\n-                            proxies=proxies,\n-                            local_files_only=local_files_only,\n-                            token=token,\n-                            user_agent=user_agent,\n-                            revision=revision,\n-                            subfolder=subfolder,\n-                            _raise_exceptions_for_missing_entries=False,\n-                        )\n-                    )\n-                    is not None\n-                ]\n-                resolved_image_processor_file = resolved_image_processor_files[0]\n+                resolved_processor_file = cached_file(\n+                    pretrained_model_name_or_path,\n+                    filename=PROCESSOR_NAME,\n+                    cache_dir=cache_dir,\n+                    force_download=force_download,\n+                    proxies=proxies,\n+                    local_files_only=local_files_only,\n+                    token=token,\n+                    user_agent=user_agent,\n+                    revision=revision,\n+                    subfolder=subfolder,\n+                    _raise_exceptions_for_missing_entries=False,\n+                )\n+                resolved_image_processor_file = cached_file(\n+                    pretrained_model_name_or_path,\n+                    filename=image_processor_file,\n+                    cache_dir=cache_dir,\n+                    force_download=force_download,\n+                    proxies=proxies,\n+                    local_files_only=local_files_only,\n+                    token=token,\n+                    user_agent=user_agent,\n+                    revision=revision,\n+                    subfolder=subfolder,\n+                    _raise_exceptions_for_missing_entries=False,\n+                )\n             except OSError:\n                 # Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\n                 # the original exception.\n@@ -322,16 +329,24 @@ def get_image_processor_dict(\n                     f\" directory containing a {image_processor_filename} file\"\n                 )\n \n-        try:\n-            # Load image_processor dict\n-            with open(resolved_image_processor_file, encoding=\"utf-8\") as reader:\n-                text = reader.read()\n-            image_processor_dict = json.loads(text)\n-            image_processor_dict = image_processor_dict.get(\"image_processor\", image_processor_dict)\n+        # Load image_processor dict. Priority goes as (nested config if found -> image processor config)\n+        # We are downloading both configs because almost all models have a `processor_config.json` but\n+        # not all of these are nested. We need to check if it was saved recebtly as nested or if it is legacy style\n+        image_processor_dict = None\n+        if resolved_processor_file is not None:\n+            processor_dict = safe_load_json_file(resolved_processor_file)\n+            if \"image_processor\" in processor_dict:\n+                image_processor_dict = processor_dict[\"image_processor\"]\n+\n+        if resolved_image_processor_file is not None and image_processor_dict is None:\n+            image_processor_dict = safe_load_json_file(resolved_image_processor_file)\n \n-        except json.JSONDecodeError:\n+        if image_processor_dict is None:\n             raise OSError(\n-                f\"It looks like the config file at '{resolved_image_processor_file}' is not a valid JSON file.\"\n+                f\"Can't load image processor for '{pretrained_model_name_or_path}'. If you were trying to load\"\n+                \" it from 'https://huggingface.co/models', make sure you don't have a local directory with the\"\n+                f\" same name. Otherwise, make sure '{pretrained_model_name_or_path}' is the correct path to a\"\n+                f\" directory containing a {image_processor_filename} file\"\n             )\n \n         if is_local:"
        },
        {
            "sha": "c458208b90272627919b9294a8862bcf98622b08",
            "filename": "src/transformers/models/auto/feature_extraction_auto.py",
            "status": "modified",
            "additions": 32,
            "deletions": 11,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/5150dac727fa30fb066f5b495bb30539ef9a6a5f/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5150dac727fa30fb066f5b495bb30539ef9a6a5f/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py?ref=5150dac727fa30fb066f5b495bb30539ef9a6a5f",
            "patch": "@@ -15,7 +15,6 @@\n \"\"\"AutoFeatureExtractor class.\"\"\"\n \n import importlib\n-import json\n import os\n from collections import OrderedDict\n from typing import Optional, Union\n@@ -24,7 +23,7 @@\n from ...configuration_utils import PreTrainedConfig\n from ...dynamic_module_utils import get_class_from_dynamic_module, resolve_trust_remote_code\n from ...feature_extraction_utils import FeatureExtractionMixin\n-from ...utils import CONFIG_NAME, FEATURE_EXTRACTOR_NAME, cached_file, logging\n+from ...utils import CONFIG_NAME, FEATURE_EXTRACTOR_NAME, PROCESSOR_NAME, cached_file, logging, safe_load_json_file\n from .auto_factory import _LazyAutoMapping\n from .configuration_auto import (\n     CONFIG_MAPPING_NAMES,\n@@ -175,9 +174,10 @@ def get_feature_extractor_config(\n     feature_extractor.save_pretrained(\"feature-extractor-test\")\n     feature_extractor_config = get_feature_extractor_config(\"feature-extractor-test\")\n     ```\"\"\"\n-    resolved_config_file = cached_file(\n+    # Load with a priority given to the nested processor config, if available in repo\n+    resolved_processor_file = cached_file(\n         pretrained_model_name_or_path,\n-        FEATURE_EXTRACTOR_NAME,\n+        filename=PROCESSOR_NAME,\n         cache_dir=cache_dir,\n         force_download=force_download,\n         proxies=proxies,\n@@ -186,16 +186,37 @@ def get_feature_extractor_config(\n         local_files_only=local_files_only,\n         _raise_exceptions_for_gated_repo=False,\n         _raise_exceptions_for_missing_entries=False,\n-        _raise_exceptions_for_connection_errors=False,\n     )\n-    if resolved_config_file is None:\n-        logger.info(\n-            \"Could not locate the feature extractor configuration file, will try to use the model config instead.\"\n-        )\n+    resolved_feature_extractor_file = cached_file(\n+        pretrained_model_name_or_path,\n+        filename=FEATURE_EXTRACTOR_NAME,\n+        cache_dir=cache_dir,\n+        force_download=force_download,\n+        proxies=proxies,\n+        token=token,\n+        revision=revision,\n+        local_files_only=local_files_only,\n+        _raise_exceptions_for_gated_repo=False,\n+        _raise_exceptions_for_missing_entries=False,\n+    )\n+\n+    # An empty list if none of the possible files is found in the repo\n+    if not resolved_feature_extractor_file and not resolved_processor_file:\n+        logger.info(\"Could not locate the feature extractor configuration file.\")\n         return {}\n \n-    with open(resolved_config_file, encoding=\"utf-8\") as reader:\n-        return json.load(reader)\n+    # Load feature_extractor dict. Priority goes as (nested config if found -> feature extractor config)\n+    # We are downloading both configs because almost all models have a `processor_config.json` but\n+    # not all of these are nested. We need to check if it was saved recently as nested or if it is legacy style\n+    feature_extractor_dict = {}\n+    if resolved_processor_file is not None:\n+        processor_dict = safe_load_json_file(resolved_processor_file)\n+        if \"feature_extractor\" in processor_dict:\n+            feature_extractor_dict = processor_dict[\"feature_extractor\"]\n+\n+    if resolved_feature_extractor_file is not None and feature_extractor_dict is None:\n+        feature_extractor_dict = safe_load_json_file(resolved_feature_extractor_file)\n+    return feature_extractor_dict\n \n \n class AutoFeatureExtractor:"
        },
        {
            "sha": "29282d276366a3d6130cbab2a982cd5f8dd0c29e",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 34,
            "deletions": 10,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/5150dac727fa30fb066f5b495bb30539ef9a6a5f/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5150dac727fa30fb066f5b495bb30539ef9a6a5f/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=5150dac727fa30fb066f5b495bb30539ef9a6a5f",
            "patch": "@@ -15,7 +15,6 @@\n \"\"\"AutoImageProcessor class.\"\"\"\n \n import importlib\n-import json\n import os\n import warnings\n from collections import OrderedDict\n@@ -29,12 +28,14 @@\n from ...utils import (\n     CONFIG_NAME,\n     IMAGE_PROCESSOR_NAME,\n+    PROCESSOR_NAME,\n     cached_file,\n     is_timm_config_dict,\n     is_timm_local_checkpoint,\n     is_torchvision_available,\n     is_vision_available,\n     logging,\n+    safe_load_json_file,\n )\n from ...utils.import_utils import requires\n from .auto_factory import _LazyAutoMapping\n@@ -319,9 +320,10 @@ def get_image_processor_config(\n     image_processor.save_pretrained(\"image-processor-test\")\n     image_processor_config = get_image_processor_config(\"image-processor-test\")\n     ```\"\"\"\n-    resolved_config_file = cached_file(\n+    # Load with a priority given to the nested processor config, if available in repo\n+    resolved_processor_file = cached_file(\n         pretrained_model_name_or_path,\n-        IMAGE_PROCESSOR_NAME,\n+        filename=PROCESSOR_NAME,\n         cache_dir=cache_dir,\n         force_download=force_download,\n         proxies=proxies,\n@@ -330,16 +332,38 @@ def get_image_processor_config(\n         local_files_only=local_files_only,\n         _raise_exceptions_for_gated_repo=False,\n         _raise_exceptions_for_missing_entries=False,\n-        _raise_exceptions_for_connection_errors=False,\n     )\n-    if resolved_config_file is None:\n-        logger.info(\n-            \"Could not locate the image processor configuration file, will try to use the model config instead.\"\n-        )\n+    resolved_image_processor_file = cached_file(\n+        pretrained_model_name_or_path,\n+        filename=IMAGE_PROCESSOR_NAME,\n+        cache_dir=cache_dir,\n+        force_download=force_download,\n+        proxies=proxies,\n+        token=token,\n+        revision=revision,\n+        local_files_only=local_files_only,\n+        _raise_exceptions_for_gated_repo=False,\n+        _raise_exceptions_for_missing_entries=False,\n+    )\n+\n+    # An empty list if none of the possible files is found in the repo\n+    if not resolved_image_processor_file and not resolved_processor_file:\n+        logger.info(\"Could not locate the image processor configuration file.\")\n         return {}\n \n-    with open(resolved_config_file, encoding=\"utf-8\") as reader:\n-        return json.load(reader)\n+    # Load image_processor dict. Priority goes as (nested config if found -> image processor config)\n+    # We are downloading both configs because almost all models have a `processor_config.json` but\n+    # not all of these are nested. We need to check if it was saved recently as nested or if it is legacy style\n+    image_processor_dict = {}\n+    if resolved_processor_file is not None:\n+        processor_dict = safe_load_json_file(resolved_processor_file)\n+        if \"image_processor\" in processor_dict:\n+            image_processor_dict = processor_dict[\"image_processor\"]\n+\n+    if resolved_image_processor_file is not None and image_processor_dict is None:\n+        image_processor_dict = safe_load_json_file(resolved_image_processor_file)\n+\n+    return image_processor_dict\n \n \n def _warning_fast_image_processor_available(fast_class):"
        },
        {
            "sha": "9a396c4b9557baf5a7061e144d3b1d2df7d85153",
            "filename": "src/transformers/models/auto/video_processing_auto.py",
            "status": "modified",
            "additions": 52,
            "deletions": 9,
            "changes": 61,
            "blob_url": "https://github.com/huggingface/transformers/blob/5150dac727fa30fb066f5b495bb30539ef9a6a5f/src%2Ftransformers%2Fmodels%2Fauto%2Fvideo_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5150dac727fa30fb066f5b495bb30539ef9a6a5f/src%2Ftransformers%2Fmodels%2Fauto%2Fvideo_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fvideo_processing_auto.py?ref=5150dac727fa30fb066f5b495bb30539ef9a6a5f",
            "patch": "@@ -15,15 +15,23 @@\n \"\"\"AutoVideoProcessor class.\"\"\"\n \n import importlib\n-import json\n import os\n from collections import OrderedDict\n from typing import TYPE_CHECKING, Optional, Union\n \n # Build the list of all video processors\n from ...configuration_utils import PreTrainedConfig\n from ...dynamic_module_utils import get_class_from_dynamic_module, resolve_trust_remote_code\n-from ...utils import CONFIG_NAME, VIDEO_PROCESSOR_NAME, cached_file, is_torchvision_available, logging\n+from ...utils import (\n+    CONFIG_NAME,\n+    IMAGE_PROCESSOR_NAME,\n+    PROCESSOR_NAME,\n+    VIDEO_PROCESSOR_NAME,\n+    cached_file,\n+    is_torchvision_available,\n+    logging,\n+    safe_load_json_file,\n+)\n from ...utils.import_utils import requires\n from ...video_processing_utils import BaseVideoProcessor\n from .auto_factory import _LazyAutoMapping\n@@ -168,24 +176,59 @@ def get_video_processor_config(\n     video_processor.save_pretrained(\"video-processor-test\")\n     video_processor = get_video_processor_config(\"video-processor-test\")\n     ```\"\"\"\n-    resolved_config_file = cached_file(\n+    # Load with a priority given to the nested processor config, if available in repo\n+    resolved_processor_file = cached_file(\n         pretrained_model_name_or_path,\n-        VIDEO_PROCESSOR_NAME,\n+        filename=PROCESSOR_NAME,\n         cache_dir=cache_dir,\n         force_download=force_download,\n         proxies=proxies,\n         token=token,\n         revision=revision,\n         local_files_only=local_files_only,\n+        _raise_exceptions_for_gated_repo=False,\n+        _raise_exceptions_for_missing_entries=False,\n     )\n-    if resolved_config_file is None:\n-        logger.info(\n-            \"Could not locate the video processor configuration file, will try to use the model config instead.\"\n+    resolved_video_processor_files = [\n+        resolved_file\n+        for filename in [VIDEO_PROCESSOR_NAME, IMAGE_PROCESSOR_NAME]\n+        if (\n+            resolved_file := cached_file(\n+                pretrained_model_name_or_path,\n+                filename=filename,\n+                cache_dir=cache_dir,\n+                force_download=force_download,\n+                proxies=proxies,\n+                token=token,\n+                revision=revision,\n+                local_files_only=local_files_only,\n+                _raise_exceptions_for_gated_repo=False,\n+                _raise_exceptions_for_missing_entries=False,\n+                _raise_exceptions_for_connection_errors=False,\n+            )\n         )\n+        is not None\n+    ]\n+    resolved_video_processor_file = resolved_video_processor_files[0] if resolved_video_processor_files else None\n+\n+    # An empty list if none of the possible files is found in the repo\n+    if not resolved_video_processor_file and not resolved_processor_file:\n+        logger.info(\"Could not locate the video processor configuration file.\")\n         return {}\n \n-    with open(resolved_config_file, encoding=\"utf-8\") as reader:\n-        return json.load(reader)\n+    # Load video_processor dict. Priority goes as (nested config if found -> video processor config -> image processor config)\n+    # We are downloading both configs because almost all models have a `processor_config.json` but\n+    # not all of these are nested. We need to check if it was saved recebtly as nested or if it is legacy style\n+    video_processor_dict = {}\n+    if resolved_processor_file is not None:\n+        processor_dict = safe_load_json_file(resolved_processor_file)\n+        if \"video_processor\" in processor_dict:\n+            video_processor_dict = processor_dict[\"video_processor\"]\n+\n+    if resolved_video_processor_file is not None and video_processor_dict is None:\n+        video_processor_dict = safe_load_json_file(resolved_video_processor_file)\n+\n+    return video_processor_dict\n \n \n @requires(backends=(\"vision\", \"torchvision\"))"
        },
        {
            "sha": "38b5db8f48931a1cd1ded95b138e44e90f805148",
            "filename": "src/transformers/utils/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/5150dac727fa30fb066f5b495bb30539ef9a6a5f/src%2Ftransformers%2Futils%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5150dac727fa30fb066f5b495bb30539ef9a6a5f/src%2Ftransformers%2Futils%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2F__init__.py?ref=5150dac727fa30fb066f5b495bb30539ef9a6a5f",
            "patch": "@@ -63,6 +63,7 @@\n     is_torch_dtype,\n     is_torch_tensor,\n     reshape,\n+    safe_load_json_file,\n     squeeze,\n     strtobool,\n     tensor_size,"
        },
        {
            "sha": "00cc581b1ac18ce1702bc291084102cba72fc697",
            "filename": "src/transformers/utils/generic.py",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/5150dac727fa30fb066f5b495bb30539ef9a6a5f/src%2Ftransformers%2Futils%2Fgeneric.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5150dac727fa30fb066f5b495bb30539ef9a6a5f/src%2Ftransformers%2Futils%2Fgeneric.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fgeneric.py?ref=5150dac727fa30fb066f5b495bb30539ef9a6a5f",
            "patch": "@@ -221,6 +221,17 @@ def to_numpy(obj):\n     return obj\n \n \n+def safe_load_json_file(json_file: str):\n+    \"A helper to load safe config files and raise a proper error message if it wasn't serialized correctly\"\n+    try:\n+        with open(json_file, encoding=\"utf-8\") as reader:\n+            text = reader.read()\n+        config_dict = json.loads(text)\n+    except json.JSONDecodeError:\n+        raise OSError(f\"It looks like the config file at '{json_file}' is not a valid JSON file.\")\n+    return config_dict\n+\n+\n class ModelOutput(OrderedDict):\n     \"\"\"\n     Base class for all model outputs as dataclass. Has a `__getitem__` that allows indexing by integer or slice (like a"
        },
        {
            "sha": "eeb04eeb3adbdebe14fb2ad2c4331dbd24f1a2ab",
            "filename": "src/transformers/video_processing_utils.py",
            "status": "modified",
            "additions": 38,
            "deletions": 12,
            "changes": 50,
            "blob_url": "https://github.com/huggingface/transformers/blob/5150dac727fa30fb066f5b495bb30539ef9a6a5f/src%2Ftransformers%2Fvideo_processing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5150dac727fa30fb066f5b495bb30539ef9a6a5f/src%2Ftransformers%2Fvideo_processing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fvideo_processing_utils.py?ref=5150dac727fa30fb066f5b495bb30539ef9a6a5f",
            "patch": "@@ -50,6 +50,7 @@\n     is_torchcodec_available,\n     is_torchvision_v2_available,\n     logging,\n+    safe_load_json_file,\n )\n from .utils.hub import cached_file\n from .utils.import_utils import requires\n@@ -635,18 +636,33 @@ def get_video_processor_dict(\n         is_local = os.path.isdir(pretrained_model_name_or_path)\n         if os.path.isfile(pretrained_model_name_or_path):\n             resolved_video_processor_file = pretrained_model_name_or_path\n+            resolved_processor_file = None\n             is_local = True\n         elif is_remote_url(pretrained_model_name_or_path):\n             video_processor_file = pretrained_model_name_or_path\n+            resolved_processor_file = None\n             resolved_video_processor_file = download_url(pretrained_model_name_or_path)\n         else:\n             video_processor_file = VIDEO_PROCESSOR_NAME\n             try:\n                 # Try to load with a new config name first and if not successful try with the old file name\n-                # NOTE: we will gradually change to saving all processor configs as nested dict in PROCESSOR_NAME\n+                # NOTE: we save all processor configs as nested dict in PROCESSOR_NAME from v5, which is the standard\n+                resolved_processor_file = cached_file(\n+                    pretrained_model_name_or_path,\n+                    filename=PROCESSOR_NAME,\n+                    cache_dir=cache_dir,\n+                    force_download=force_download,\n+                    proxies=proxies,\n+                    local_files_only=local_files_only,\n+                    token=token,\n+                    user_agent=user_agent,\n+                    revision=revision,\n+                    subfolder=subfolder,\n+                    _raise_exceptions_for_missing_entries=False,\n+                )\n                 resolved_video_processor_files = [\n                     resolved_file\n-                    for filename in [VIDEO_PROCESSOR_NAME, IMAGE_PROCESSOR_NAME, PROCESSOR_NAME]\n+                    for filename in [video_processor_file, IMAGE_PROCESSOR_NAME]\n                     if (\n                         resolved_file := cached_file(\n                             pretrained_model_name_or_path,\n@@ -664,7 +680,9 @@ def get_video_processor_dict(\n                     )\n                     is not None\n                 ]\n-                resolved_video_processor_file = resolved_video_processor_files[0]\n+                resolved_video_processor_file = (\n+                    resolved_video_processor_files[0] if resolved_video_processor_files else None\n+                )\n             except OSError:\n                 # Raise any OS error raise by `cached_file`. It will have a helpful error message adapted to\n                 # the original exception.\n@@ -675,19 +693,27 @@ def get_video_processor_dict(\n                     f\"Can't load video processor for '{pretrained_model_name_or_path}'. If you were trying to load\"\n                     \" it from 'https://huggingface.co/models', make sure you don't have a local directory with the\"\n                     f\" same name. Otherwise, make sure '{pretrained_model_name_or_path}' is the correct path to a\"\n-                    f\" directory containing a {VIDEO_PROCESSOR_NAME} file\"\n+                    f\" directory containing a {video_processor_file} file\"\n                 )\n \n-        try:\n-            # Load video_processor dict\n-            with open(resolved_video_processor_file, \"r\", encoding=\"utf-8\") as reader:\n-                text = reader.read()\n-            video_processor_dict = json.loads(text)\n-            video_processor_dict = video_processor_dict.get(\"video_processor\", video_processor_dict)\n+        # Load video_processor dict. Priority goes as (nested config if found -> video processor config -> image processor config)\n+        # We are downloading both configs because almost all models have a `processor_config.json` but\n+        # not all of these are nested. We need to check if it was saved recebtly as nested or if it is legacy style\n+        video_processor_dict = None\n+        if resolved_processor_file is not None:\n+            processor_dict = safe_load_json_file(resolved_processor_file)\n+            if \"video_processor\" in processor_dict:\n+                video_processor_dict = processor_dict[\"video_processor\"]\n+\n+        if resolved_video_processor_file is not None and video_processor_dict is None:\n+            video_processor_dict = safe_load_json_file(resolved_video_processor_file)\n \n-        except json.JSONDecodeError:\n+        if video_processor_dict is None:\n             raise OSError(\n-                f\"It looks like the config file at '{resolved_video_processor_file}' is not a valid JSON file.\"\n+                f\"Can't load video processor for '{pretrained_model_name_or_path}'. If you were trying to load\"\n+                \" it from 'https://huggingface.co/models', make sure you don't have a local directory with the\"\n+                f\" same name. Otherwise, make sure '{pretrained_model_name_or_path}' is the correct path to a\"\n+                f\" directory containing a {video_processor_file} file\"\n             )\n \n         if is_local:"
        },
        {
            "sha": "829942d84859a28ea5a07b637f7e50a6bf1a3c84",
            "filename": "tests/models/auto/test_processor_auto.py",
            "status": "modified",
            "additions": 49,
            "deletions": 0,
            "changes": 49,
            "blob_url": "https://github.com/huggingface/transformers/blob/5150dac727fa30fb066f5b495bb30539ef9a6a5f/tests%2Fmodels%2Fauto%2Ftest_processor_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5150dac727fa30fb066f5b495bb30539ef9a6a5f/tests%2Fmodels%2Fauto%2Ftest_processor_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fauto%2Ftest_processor_auto.py?ref=5150dac727fa30fb066f5b495bb30539ef9a6a5f",
            "patch": "@@ -33,15 +33,22 @@\n     AutoFeatureExtractor,\n     AutoProcessor,\n     AutoTokenizer,\n+    BaseVideoProcessor,\n     BertTokenizer,\n+    FeatureExtractionMixin,\n+    ImageProcessingMixin,\n     LlamaTokenizer,\n+    LlavaOnevisionVideoProcessor,\n     LlavaProcessor,\n     ProcessorMixin,\n     SiglipImageProcessor,\n     Wav2Vec2Config,\n     Wav2Vec2FeatureExtractor,\n     Wav2Vec2Processor,\n )\n+from transformers.models.auto.feature_extraction_auto import get_feature_extractor_config\n+from transformers.models.auto.image_processing_auto import get_image_processor_config\n+from transformers.models.auto.video_processing_auto import get_video_processor_config\n from transformers.testing_utils import TOKEN, TemporaryHubRepo, get_tests_dir, is_staging_test\n from transformers.tokenization_utils import TOKENIZER_CONFIG_FILE\n from transformers.utils import (\n@@ -107,6 +114,48 @@ def test_processor_from_local_directory_from_extractor_config(self):\n \n         self.assertIsInstance(processor, Wav2Vec2Processor)\n \n+    def test_subcomponent_get_config_dict_saved_as_nested_config(self):\n+        \"\"\"\n+        Tests that we can get config dict of a subcomponents of a processor,\n+        even if they were saved as nested dict in `processor_config.json`\n+        \"\"\"\n+        # Test feature extractor first\n+        with tempfile.TemporaryDirectory() as tmpdirname:\n+            processor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n+            processor.save_pretrained(tmpdirname)\n+\n+            config_dict_1 = get_feature_extractor_config(tmpdirname)\n+            feature_extractor_1 = Wav2Vec2FeatureExtractor(**config_dict_1)\n+            self.assertIsInstance(feature_extractor_1, Wav2Vec2FeatureExtractor)\n+\n+            config_dict_2, _ = FeatureExtractionMixin.get_feature_extractor_dict(tmpdirname)\n+            feature_extractor_2 = Wav2Vec2FeatureExtractor(**config_dict_2)\n+            self.assertIsInstance(feature_extractor_2, Wav2Vec2FeatureExtractor)\n+            self.assertEqual(config_dict_1, config_dict_2)\n+\n+        # Test image and video processors next\n+        with tempfile.TemporaryDirectory() as tmpdirname:\n+            processor = AutoProcessor.from_pretrained(\"llava-hf/llava-onevision-qwen2-0.5b-ov-hf\")\n+            processor.save_pretrained(tmpdirname)\n+\n+            config_dict_1 = get_image_processor_config(tmpdirname)\n+            image_processor_1 = SiglipImageProcessor(**config_dict_1)\n+            self.assertIsInstance(image_processor_1, SiglipImageProcessor)\n+\n+            config_dict_2, _ = ImageProcessingMixin.get_image_processor_dict(tmpdirname)\n+            image_processor_2 = SiglipImageProcessor(**config_dict_2)\n+            self.assertIsInstance(image_processor_2, SiglipImageProcessor)\n+            self.assertEqual(config_dict_1, config_dict_2)\n+\n+            config_dict_1 = get_video_processor_config(tmpdirname)\n+            video_processor_1 = LlavaOnevisionVideoProcessor(**config_dict_1)\n+            self.assertIsInstance(video_processor_1, LlavaOnevisionVideoProcessor)\n+\n+            config_dict_2, _ = BaseVideoProcessor.get_video_processor_dict(tmpdirname)\n+            video_processor_2 = LlavaOnevisionVideoProcessor(**config_dict_2)\n+            self.assertIsInstance(video_processor_2, LlavaOnevisionVideoProcessor)\n+            self.assertEqual(config_dict_1, config_dict_2)\n+\n     def test_processor_from_processor_class(self):\n         with tempfile.TemporaryDirectory() as tmpdirname:\n             feature_extractor = Wav2Vec2FeatureExtractor()"
        }
    ],
    "stats": {
        "total": 411,
        "additions": 307,
        "deletions": 104
    }
}