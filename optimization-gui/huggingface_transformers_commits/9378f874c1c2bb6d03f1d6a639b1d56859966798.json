{
    "author": "SunMarc",
    "message": "[Trainer] Fix DP loss (#40799)\n\n* fix\n\n* style\n\n* Fix fp16\n\n* style\n\n---------\n\nCo-authored-by: Matej Sirovatka <54212263+S1ro1@users.noreply.github.com>",
    "sha": "9378f874c1c2bb6d03f1d6a639b1d56859966798",
    "files": [
        {
            "sha": "24e1730608d23cddcf6edb15caa66dcc698d484b",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 10,
            "deletions": 7,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/9378f874c1c2bb6d03f1d6a639b1d56859966798/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9378f874c1c2bb6d03f1d6a639b1d56859966798/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=9378f874c1c2bb6d03f1d6a639b1d56859966798",
            "patch": "@@ -2483,8 +2483,7 @@ def _inner_training_loop(\n                 model, self.optimizer, self.lr_scheduler = self.accelerator.prepare(\n                     self.model, self.optimizer, self.lr_scheduler\n                 )\n-        elif self.args.optim in [OptimizerNames.LOMO, OptimizerNames.ADALOMO]:\n-            # In this case we are in DDP + LOMO, which should be supported\n+        else:\n             self.optimizer = self.accelerator.prepare(self.optimizer)\n \n         if self.is_fsdp_enabled:\n@@ -3783,7 +3782,7 @@ def log(self, logs: dict[str, float], start_time: Optional[float] = None) -> Non\n         \"\"\"\n         if self.state.epoch is not None:\n             logs[\"epoch\"] = self.state.epoch\n-        if self.args.include_num_input_tokens_seen:\n+        if self.args.include_num_input_tokens_seen != \"no\":\n             logs[\"num_input_tokens_seen\"] = self.state.num_input_tokens_seen\n             if start_time is not None:\n                 logs.update(speed_metrics(\"train\", start_time, num_tokens=self.state.num_input_tokens_seen))\n@@ -4143,7 +4142,7 @@ def compute_loss(\n             and (self.model_accepts_loss_kwargs or self.compute_loss_func)\n             and num_items_in_batch is not None\n         ):\n-            loss *= self.accelerator.num_processes\n+            loss *= self.accelerator.num_processes if self.args.n_gpu <= 1 else self.args.n_gpu\n \n         return (loss, outputs) if return_outputs else loss\n \n@@ -5617,15 +5616,19 @@ def get_batch_samples(\n                 pass\n \n         if num_items_in_batch is not None:\n-            if self.args.average_tokens_across_devices:\n+            if self.args.average_tokens_across_devices and self.args.world_size >= 1:\n                 num_items_in_batch = self.accelerator.gather(num_items_in_batch.to(device)).sum()\n+            elif self.args.n_gpu >= 1:\n+                # In DP case, if we don't average, we need to divide by the number of gpu. This is the simplest approximation.\n+                # Otherwise, we would have to scatter labels and calculate num_items_in_batch for each gpu.\n+                num_items_in_batch = num_items_in_batch // self.args.n_gpu\n \n             if torch.is_tensor(num_items_in_batch):\n                 num_items_in_batch = num_items_in_batch.to(device)\n \n                 if self.args.n_gpu > 1 and num_items_in_batch.dim() == 0:\n-                    # In the DataParallel case, convert the scalar tensor into a 1-dim tensor\n-                    num_items_in_batch = num_items_in_batch.unsqueeze(0)\n+                    # In the DataParallel case, convert the scalar tensor into a 2-dim tensor with the same value repeated\n+                    num_items_in_batch = num_items_in_batch.unsqueeze(0).expand(self.args.n_gpu, -1)\n                 # Divide by number of devices with the same batch\n                 if pc := getattr(self.accelerator, \"parallelism_config\", None):\n                     num_items_in_batch = num_items_in_batch // pc.non_data_parallel_size"
        },
        {
            "sha": "b232dcb76454c4c7ac3433d7e1479d873ffe22bb",
            "filename": "src/transformers/training_args.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/9378f874c1c2bb6d03f1d6a639b1d56859966798/src%2Ftransformers%2Ftraining_args.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9378f874c1c2bb6d03f1d6a639b1d56859966798/src%2Ftransformers%2Ftraining_args.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftraining_args.py?ref=9378f874c1c2bb6d03f1d6a639b1d56859966798",
            "patch": "@@ -1790,18 +1790,6 @@ def __post_init__(self):\n         if self.framework == \"pt\" and is_torch_available():\n             self.device\n \n-        # Disable average tokens when using single device\n-        if self.average_tokens_across_devices:\n-            try:\n-                if self.world_size == 1:\n-                    logger.info(\n-                        \"average_tokens_across_devices is True but world size is 1. Setting it to False automatically.\"\n-                    )\n-                    self.average_tokens_across_devices = False\n-            except ImportError as e:\n-                logger.warning(f\"Can not specify world size due to {e}. Turn average_tokens_across_devices to False.\")\n-                self.average_tokens_across_devices = False\n-\n         if self.torchdynamo is not None:\n             warnings.warn(\n                 \"`torchdynamo` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use\""
        },
        {
            "sha": "29558f50bba1a2e656d575403c7fb720f8ab353a",
            "filename": "tests/trainer/test_trainer.py",
            "status": "modified",
            "additions": 12,
            "deletions": 2,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/9378f874c1c2bb6d03f1d6a639b1d56859966798/tests%2Ftrainer%2Ftest_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9378f874c1c2bb6d03f1d6a639b1d56859966798/tests%2Ftrainer%2Ftest_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer.py?ref=9378f874c1c2bb6d03f1d6a639b1d56859966798",
            "patch": "@@ -1270,6 +1270,18 @@ def test_adafactor_lr_none(self):\n             self.assertFalse(torch.allclose(trainer.model.b, b))\n             self.assertGreater(trainer.optimizer.state_dict()[\"param_groups\"][0][\"lr\"], 0)\n \n+    @require_torch_fp16\n+    @require_torch_accelerator\n+    def test_mixed_fp16(self):\n+        # very basic test\n+        with tempfile.TemporaryDirectory() as tmp_dir:\n+            trainer = get_regression_trainer(learning_rate=0.1, fp16=True, logging_steps=1, output_dir=tmp_dir)\n+            trainer.train()\n+            self.check_trained_model(trainer.model, atol=ATOL, rtol=RTOL)\n+            log_0 = trainer.state.log_history[:-1][0]\n+            # check that the grads were properly clipped due to the grad scaler. Otherwise, we get huge values\n+            self.assertEqual(log_0[\"grad_norm\"] < 100, True)\n+\n     @require_torch_bf16\n     @require_torch_accelerator\n     def test_mixed_bf16(self):\n@@ -1286,8 +1298,6 @@ def test_mixed_bf16(self):\n                     learning_rate=0.1, bf16=True, half_precision_backend=\"apex\", output_dir=tmp_dir\n                 )\n \n-        # will add more specific tests once there are some bugs to fix\n-\n     @require_torch_gpu\n     @require_torch_tf32\n     def test_tf32(self):"
        }
    ],
    "stats": {
        "total": 43,
        "additions": 22,
        "deletions": 21
    }
}