{
    "author": "yao-matrix",
    "message": "extend gemma3n integration ut cases on XPU (#41071)\n\nSigned-off-by: Yao, Matrix <matrix.yao@intel.com>",
    "sha": "1dd22a234cdbec840d139871720447baf809158b",
    "files": [
        {
            "sha": "2012bb9c8ed409722fd9ad6903c8077d2370a948",
            "filename": "tests/models/gemma3n/test_modeling_gemma3n.py",
            "status": "modified",
            "additions": 52,
            "deletions": 11,
            "changes": 63,
            "blob_url": "https://github.com/huggingface/transformers/blob/1dd22a234cdbec840d139871720447baf809158b/tests%2Fmodels%2Fgemma3n%2Ftest_modeling_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1dd22a234cdbec840d139871720447baf809158b/tests%2Fmodels%2Fgemma3n%2Ftest_modeling_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3n%2Ftest_modeling_gemma3n.py?ref=1dd22a234cdbec840d139871720447baf809158b",
            "patch": "@@ -37,10 +37,12 @@\n     is_torch_available,\n )\n from transformers.testing_utils import (\n+    Expectations,\n     cleanup,\n+    require_deterministic_for_xpu,\n     require_read_token,\n     require_torch,\n-    require_torch_gpu,\n+    require_torch_accelerator,\n     set_config_for_less_flaky_test,\n     set_model_for_less_flaky_test,\n     slow,\n@@ -217,8 +219,6 @@ def test_feature_extractor(self):\n         self.assertEqual(input_features.shape, self.expected_input_features_shape)\n         np.testing.assert_allclose(input_features[0, 0, :5], self.expected_input_features_slice, rtol=1e-5, atol=1e-5)\n \n-        print(input_features[0, 0, :5])\n-\n         input_features_mask = audio_inputs[\"input_features_mask\"]\n         self.assertEqual(input_features_mask.shape, self.expected_input_features_mask_shape)\n         # The second audio sample is shorter (22 frames vs 48), so its mask should become False at index 22\n@@ -235,8 +235,6 @@ def test_audio_encoder(self):\n         with torch.no_grad():\n             encoder_output, encoder_mask = model(**inputs_dict)\n \n-        print(encoder_output[0, 0, :5])\n-\n         # Check output encodings\n         self.assertEqual(encoder_output.shape, self.expected_encoder_output_shape)\n         torch.testing.assert_close(\n@@ -745,7 +743,7 @@ def test_automodelforcausallm(self):\n \n \n @slow\n-@require_torch_gpu\n+@require_torch_accelerator\n @require_read_token\n class Gemma3nIntegrationTest(unittest.TestCase):\n     def setUp(self):\n@@ -766,7 +764,7 @@ def setUp(self):\n         audio_ds = load_dataset(\n             \"etechgrid/28.5k_wavfiles_dataset\", \"default\", data_files=\"wav_dataset/103-1240-0000.wav\"\n         )\n-        self.audio_file_path = audio_ds[\"train\"][0][\"audio\"].metadata.path\n+        self.audio_file_path = audio_ds[\"train\"][0][\"audio\"][\"path\"]\n         cleanup(torch_device, gc_collect=True)\n \n     def tearDown(self):\n@@ -869,7 +867,17 @@ def test_model_4b_batch(self):\n         output = model.generate(**inputs, max_new_tokens=30, do_sample=False)\n         output_text = self.processor.batch_decode(output, skip_special_tokens=True)\n \n-        EXPECTED_TEXTS = ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nThe image shows a brown and white cow standing on a sandy beach next to a clear blue ocean. The cow is facing the viewer with its head slightly', \"user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAre these images identical?\\nmodel\\nNo, the images are not identical. \\n\\nHere's a breakdown of the differences:\\n\\n* **Subject:** The first image features a cow\"]  # fmt: skip\n+        # fmt: off\n+        EXPECTATIONS = Expectations(\n+            {\n+                (\"cuda\", None): ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nThe image shows a brown and white cow standing on a sandy beach next to a clear blue ocean. The cow is facing the viewer with its head slightly', \"user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAre these images identical?\\nmodel\\nNo, the images are not identical. \\n\\nHere's a breakdown of the differences:\\n\\n* **Subject:** The first image features a cow\"],\n+                (\"xpu\", None): ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nThe image shows a brown and white cow standing on a sandy beach next to a turquoise ocean. The cow is facing the viewer with its head slightly turned', \"user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAre these images identical?\\nmodel\\nNo, the images are not identical. \\n\\nHere's a breakdown of the differences:\\n\\n* **Subject:** The first image features a cow\"],\n+            }\n+        )\n+\n+        EXPECTED_TEXTS = EXPECTATIONS.get_expectation()\n+        # fmt: on\n+\n         self.assertEqual(output_text, EXPECTED_TEXTS)\n \n     def test_model_4b_image(self):\n@@ -891,10 +899,22 @@ def test_model_4b_image(self):\n         output_text = self.processor.batch_decode(output, skip_special_tokens=True)\n \n         EXPECTED_NUM_IMAGES = 1  # Gemma3n does not support crops\n-        EXPECTED_TEXTS = ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nThe image shows a brown and white cow standing on a sandy beach next to a clear blue ocean. The cow is facing the viewer with its head slightly']  # fmt: skip\n+\n+        # fmt: off\n+        EXPECTATIONS = Expectations(\n+            {\n+                (\"cuda\", None): ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nThe image shows a brown and white cow standing on a sandy beach next to a clear blue ocean. The cow is facing the viewer with its head slightly'],\n+                (\"xpu\", None): ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nThe image shows a brown and white cow standing on a sandy beach next to a clear blue ocean. The cow is facing the viewer with its head slightly'],\n+            }\n+        )\n+\n+        EXPECTED_TEXTS = EXPECTATIONS.get_expectation()\n+        # fmt: on\n+\n         self.assertEqual(len(inputs[\"pixel_values\"]), EXPECTED_NUM_IMAGES)\n         self.assertEqual(output_text, EXPECTED_TEXTS)\n \n+    @require_deterministic_for_xpu\n     def test_model_4b_multiimage(self):\n         model_id = \"Google/gemma-3n-E4B-it\"\n \n@@ -928,7 +948,17 @@ def test_model_4b_multiimage(self):\n         output = model.generate(**inputs, max_new_tokens=30, do_sample=False)\n         output_text = self.processor.batch_decode(output, skip_special_tokens=True)\n \n-        EXPECTED_TEXTS = ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat do you see here?\\nmodel\\nIn the image, I see a street scene in what appears to be a Chinatown district. Here are some key elements:\\n\\n* **A prominent red']  # fmt: skip\n+        # fmt: off\n+        EXPECTATIONS = Expectations(\n+            {\n+                (\"cuda\", None): ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat do you see here?\\nmodel\\nIn the image, I see a street scene in what appears to be a Chinatown district. Here are some key elements:\\n\\n* **A prominent red'],\n+                (\"xpu\", None): ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat do you see here?\\nmodel\\nIn the image, I see a street scene in what appears to be a Chinatown district. Here are the key elements:\\n\\n* **A prominent red'],\n+            }\n+        )\n+\n+        EXPECTED_TEXTS = EXPECTATIONS.get_expectation()\n+        # fmt: on\n+\n         self.assertEqual(output_text, EXPECTED_TEXTS)\n \n     @unittest.skip(\"For now, using a gemma model with the 3n class is not supported\")\n@@ -978,6 +1008,7 @@ def test_generation_beyond_sliding_window(self, attn_implementation: str):\n         EXPECTED_COMPLETIONS = [\" and I think it's a nice place to visit. This is a nice place. This is\", \", green, yellow, orange, purple, pink, brown, black, white.\\n\\nHere'\"]  # fmt: skip\n         self.assertEqual(output_text, EXPECTED_COMPLETIONS)\n \n+    @require_deterministic_for_xpu\n     def test_generation_beyond_sliding_window_with_generation_config(self):\n         \"\"\"Same as `test_generation_beyond_sliding_window`, but passing a GenerationConfig. Regression test for #36684 --\n         ensures `cache_implementation='hybrid'` is correctly inherited from the base `model.generation_config`.\n@@ -1003,5 +1034,15 @@ def test_generation_beyond_sliding_window_with_generation_config(self):\n         ]\n         output_text = tokenizer.batch_decode(out)\n \n-        EXPECTED_COMPLETIONS = [\" and I am glad to be here. This is a nice place. This is a nice place.\", \", green, yellow, purple, orange, pink, brown, black, white.\\n\\nHere are\"]  # fmt: skip\n+        # fmt: off\n+        EXPECTATIONS = Expectations(\n+            {\n+                (\"cuda\", None): [\" and I am glad to be here. This is a nice place. This is a nice place.\", \", green, yellow, purple, orange, pink, brown, black, white.\\n\\nHere are\"],\n+                (\"xpu\", None): [\" and I think it is very nice. I think it is nice. This is a nice place.\", \", green, yellow, purple, orange, pink, brown, black, white.\\n\\nHere are\"],\n+            }\n+        )\n+\n+        EXPECTED_COMPLETIONS = EXPECTATIONS.get_expectation()\n+        # fmt: on\n+\n         self.assertEqual(output_text, EXPECTED_COMPLETIONS)"
        }
    ],
    "stats": {
        "total": 63,
        "additions": 52,
        "deletions": 11
    }
}