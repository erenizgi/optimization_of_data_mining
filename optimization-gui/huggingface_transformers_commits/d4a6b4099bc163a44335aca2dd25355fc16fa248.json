{
    "author": "SunMarc",
    "message": "Revert checkpoint tmp dir (#36112)\n\n* Revert \"Fix OS err (#36094)\"\r\n\r\nThis reverts commit ba29a439adbe6f371710d0514659127264ae24b3.\r\n\r\n* Revert \"Save checkpoint to temporary directory to handle partial saves during failures (#35580)\"\r\n\r\nThis reverts commit 20d17358c468b7aefca9e54c3461eb88d1ee34f9.",
    "sha": "d4a6b4099bc163a44335aca2dd25355fc16fa248",
    "files": [
        {
            "sha": "74129a7e5c7fc41463f88865a7bf9c4c9ccf7717",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 21,
            "deletions": 33,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/d4a6b4099bc163a44335aca2dd25355fc16fa248/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d4a6b4099bc163a44335aca2dd25355fc16fa248/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=d4a6b4099bc163a44335aca2dd25355fc16fa248",
            "patch": "@@ -18,7 +18,6 @@\n \n import contextlib\n import copy\n-import errno\n import functools\n import glob\n import importlib.metadata\n@@ -3129,42 +3128,31 @@ def _save_checkpoint(self, model, trial):\n             self.store_flos()\n \n         run_dir = self._get_output_dir(trial=trial)\n-        checkpoint_dir = os.path.join(run_dir, checkpoint_folder)\n-        with tempfile.TemporaryDirectory(prefix=f\"tmp-{PREFIX_CHECKPOINT_DIR}-\", dir=run_dir) as output_dir:\n-            self.save_model(output_dir, _internal_call=True)\n+        output_dir = os.path.join(run_dir, checkpoint_folder)\n+        self.save_model(output_dir, _internal_call=True)\n \n-            if not self.args.save_only_model:\n-                # Save optimizer and scheduler\n-                self._save_optimizer_and_scheduler(output_dir)\n-                # Save RNG state\n-                self._save_rng_state(output_dir)\n+        if not self.args.save_only_model:\n+            # Save optimizer and scheduler\n+            self._save_optimizer_and_scheduler(output_dir)\n+            # Save RNG state\n+            self._save_rng_state(output_dir)\n \n-            # Save the Trainer state\n-            if self.args.should_save:\n-                # Update `ExportableState` callbacks and `TrainerControl` state to where we are currently\n-                for cb in [\n-                    cb for cb in self.callback_handler.callbacks + [self.control] if isinstance(cb, ExportableState)\n-                ]:\n-                    cb_name = cb.__class__.__name__\n-                    cb_state = cb.state()\n-                    if isinstance(self.state.stateful_callbacks[cb_name], list):\n-                        self.state.stateful_callbacks[cb_name].append(cb_state)\n-                    else:\n-                        self.state.stateful_callbacks[cb_name] = cb_state\n-                self.state.save_to_json(os.path.join(output_dir, TRAINER_STATE_NAME))\n-\n-                if os.path.exists(output_dir):\n-                    try:\n-                        os.renames(output_dir, checkpoint_dir)\n-                    except OSError as e:\n-                        if e.errno in [errno.ENOTEMPTY, errno.EEXIST]:  # Directory/File already exists\n-                            shutil.rmtree(checkpoint_dir)\n-                            os.renames(output_dir, checkpoint_dir)\n-                        else:\n-                            raise\n+        # Save the Trainer state\n+        if self.args.should_save:\n+            # Update `ExportableState` callbacks and `TrainerControl` state to where we are currently\n+            for cb in [\n+                cb for cb in self.callback_handler.callbacks + [self.control] if isinstance(cb, ExportableState)\n+            ]:\n+                cb_name = cb.__class__.__name__\n+                cb_state = cb.state()\n+                if isinstance(self.state.stateful_callbacks[cb_name], list):\n+                    self.state.stateful_callbacks[cb_name].append(cb_state)\n+                else:\n+                    self.state.stateful_callbacks[cb_name] = cb_state\n+            self.state.save_to_json(os.path.join(output_dir, TRAINER_STATE_NAME))\n \n         if self.args.push_to_hub:\n-            self._push_from_checkpoint(checkpoint_dir)\n+            self._push_from_checkpoint(output_dir)\n \n         # Maybe delete some older checkpoints.\n         if self.args.should_save:"
        }
    ],
    "stats": {
        "total": 54,
        "additions": 21,
        "deletions": 33
    }
}