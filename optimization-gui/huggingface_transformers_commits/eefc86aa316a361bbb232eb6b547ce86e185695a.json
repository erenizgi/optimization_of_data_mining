{
    "author": "Cyrilvallez",
    "message": "Fix tensor parallel with non-floating dtypes (#37790)\n\nfix",
    "sha": "eefc86aa316a361bbb232eb6b547ce86e185695a",
    "files": [
        {
            "sha": "f73fcdea5a6c1f12eb38b5ef30679a72c1fba0b5",
            "filename": "src/transformers/integrations/tensor_parallel.py",
            "status": "modified",
            "additions": 6,
            "deletions": 11,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/eefc86aa316a361bbb232eb6b547ce86e185695a/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eefc86aa316a361bbb232eb6b547ce86e185695a/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py?ref=eefc86aa316a361bbb232eb6b547ce86e185695a",
            "patch": "@@ -307,8 +307,7 @@ def partition_tensor(self, param, empty_param, param_type, param_casting_dtype,\n             parameter = parameter.contiguous()\n         if self.use_dtensor:\n             parameter = DTensor.from_local(parameter, device_mesh, shard, run_check=False)\n-        requires_grad = True if parameter.is_floating_point() else False\n-        return nn.Parameter(parameter, requires_grad=requires_grad)\n+        return nn.Parameter(parameter, requires_grad=parameter.is_floating_point())\n \n     @staticmethod\n     def _prepare_output_fn(output_layouts, use_local_output, mod, outputs, device_mesh):\n@@ -330,8 +329,7 @@ def partition_tensor(self, param, empty_param, param_type, param_casting_dtype,\n             parameter = parameter.contiguous()\n         if self.use_dtensor:\n             parameter = DTensor.from_local(parameter, device_mesh, [Shard(-2)], run_check=False)\n-        requires_grad = True if parameter.is_floating_point() else False\n-        return nn.Parameter(parameter, requires_grad=requires_grad)\n+        return nn.Parameter(parameter, requires_grad=parameter.is_floating_point())\n \n \n class RowwiseParallel(TensorParallelLayer):\n@@ -383,8 +381,7 @@ def partition_tensor(self, param, empty_param, param_type, param_casting_dtype,\n             parameter = parameter.contiguous()\n         if self.use_dtensor:\n             parameter = DTensor.from_local(parameter, device_mesh, shard, run_check=False)\n-        requires_grad = True if parameter.is_floating_point() else False\n-        return nn.Parameter(parameter, requires_grad=requires_grad)\n+        return nn.Parameter(parameter, requires_grad=parameter.is_floating_point())\n \n     @staticmethod\n     def _prepare_input_fn(input_layouts, desired_input_layouts, mod, inputs, device_mesh):\n@@ -446,8 +443,7 @@ def partition_tensor(self, param, empty_param, param_type, param_casting_dtype,\n             parameter = parameter.contiguous()\n         if self.use_dtensor:\n             parameter = DTensor.from_local(parameter, device_mesh, [Shard(-1)], run_check=False)\n-        requires_grad = True if parameter.is_floating_point() else False\n-        return nn.Parameter(parameter, requires_grad=requires_grad)\n+        return nn.Parameter(parameter, requires_grad=parameter.is_floating_point())\n \n \n class SequenceParallel(TensorParallelLayer):\n@@ -531,8 +527,7 @@ def partition_tensor(self, param, empty_param, param_type, param_casting_dtype,\n             parameter = parameter.contiguous()\n         if self.use_dtensor:\n             parameter = DTensor.from_local(parameter, device_mesh, [Replicate()], run_check=False)\n-        requires_grad = True if parameter.is_floating_point() else False\n-        return nn.Parameter(parameter, requires_grad=requires_grad)\n+        return nn.Parameter(parameter, requires_grad=parameter.is_floating_point())\n \n \n SUPPORTED_TP_STYLES = {\n@@ -671,7 +666,7 @@ def shard_and_distribute_module(\n     # SUPER IMPORTANT we have to use setattr\n     # otherwise loading is crazy slow\n     if not isinstance(param, torch.nn.Parameter):\n-        param = torch.nn.Parameter(param)\n+        param = torch.nn.Parameter(param, requires_grad=param.is_floating_point())\n     setattr(module_to_tp, param_type, param)\n     # module_to_tp.load_state_dict({param_type: param}, strict=False, assign=True)\n     return param"
        }
    ],
    "stats": {
        "total": 17,
        "additions": 6,
        "deletions": 11
    }
}