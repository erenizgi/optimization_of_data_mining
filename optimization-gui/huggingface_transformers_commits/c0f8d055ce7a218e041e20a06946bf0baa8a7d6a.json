{
    "author": "stevhliu",
    "message": "[docs] Redesign (#31757)\n\n* toctree\n\n* not-doctested.txt\n\n* collapse sections\n\n* feedback\n\n* update\n\n* rewrite get started sections\n\n* fixes\n\n* fix\n\n* loading models\n\n* fix\n\n* customize models\n\n* share\n\n* fix link\n\n* contribute part 1\n\n* contribute pt 2\n\n* fix toctree\n\n* tokenization pt 1\n\n* Add new model (#32615)\n\n* v1 - working version\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* rename to correct name\n\n* fix title\n\n* fixup\n\n* rename files\n\n* fix\n\n* add copied from on tests\n\n* rename to `FalconMamba` everywhere and fix bugs\n\n* fix quantization + accelerate\n\n* fix copies\n\n* add `torch.compile` support\n\n* fix tests\n\n* fix tests and add slow tests\n\n* copies on config\n\n* merge the latest changes\n\n* fix tests\n\n* add few lines about instruct\n\n* Apply suggestions from code review\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* fix\n\n* fix tests\n\n---------\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* \"to be not\" -> \"not to be\" (#32636)\n\n* \"to be not\" -> \"not to be\"\n\n* Update sam.md\n\n* Update trainer.py\n\n* Update modeling_utils.py\n\n* Update test_modeling_utils.py\n\n* Update test_modeling_utils.py\n\n* fix hfoption tag\n\n* tokenization pt. 2\n\n* image processor\n\n* fix toctree\n\n* backbones\n\n* feature extractor\n\n* fix file name\n\n* processor\n\n* update not-doctested\n\n* update\n\n* make style\n\n* fix toctree\n\n* revision\n\n* make fixup\n\n* fix toctree\n\n* fix\n\n* make style\n\n* fix hfoption tag\n\n* pipeline\n\n* pipeline gradio\n\n* pipeline web server\n\n* add pipeline\n\n* fix toctree\n\n* not-doctested\n\n* prompting\n\n* llm optims\n\n* fix toctree\n\n* fixes\n\n* cache\n\n* text generation\n\n* fix\n\n* chat pipeline\n\n* chat stuff\n\n* xla\n\n* torch.compile\n\n* cpu inference\n\n* toctree\n\n* gpu inference\n\n* agents and tools\n\n* gguf/tiktoken\n\n* finetune\n\n* toctree\n\n* trainer\n\n* trainer pt 2\n\n* optims\n\n* optimizers\n\n* accelerate\n\n* parallelism\n\n* fsdp\n\n* update\n\n* distributed cpu\n\n* hardware training\n\n* gpu training\n\n* gpu training 2\n\n* peft\n\n* distrib debug\n\n* deepspeed 1\n\n* deepspeed 2\n\n* chat toctree\n\n* quant pt 1\n\n* quant pt 2\n\n* fix toctree\n\n* fix\n\n* fix\n\n* quant pt 3\n\n* quant pt 4\n\n* serialization\n\n* torchscript\n\n* scripts\n\n* tpu\n\n* review\n\n* model addition timeline\n\n* modular\n\n* more reviews\n\n* reviews\n\n* fix toctree\n\n* reviews reviews\n\n* continue reviews\n\n* more reviews\n\n* modular transformers\n\n* more review\n\n* zamba2\n\n* fix\n\n* all frameworks\n\n* pytorch\n\n* supported model frameworks\n\n* flashattention\n\n* rm check_table\n\n* not-doctested.txt\n\n* rm check_support_list.py\n\n* feedback\n\n* updates/feedback\n\n* review\n\n* feedback\n\n* fix\n\n* update\n\n* feedback\n\n* updates\n\n* update\n\n---------\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\nCo-authored-by: Quentin GallouÃ©dec <45557362+qgallouedec@users.noreply.github.com>",
    "sha": "c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
    "files": [
        {
            "sha": "108e319d2cc72642f61f73265a992eba4953bbef",
            "filename": ".circleci/config.yml",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/.circleci%2Fconfig.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/.circleci%2Fconfig.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.circleci%2Fconfig.yml?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -178,8 +178,7 @@ jobs:\n             - store_artifacts:\n                   path: ~/transformers/installed.txt\n             - run: python utils/check_copies.py\n-            - run: python utils/check_modular_conversion.py --num_workers 4\n-            - run: python utils/check_table.py\n+            - run: python utils/check_modular_conversion.py\n             - run: python utils/check_dummies.py\n             - run: python utils/check_repo.py\n             - run: python utils/check_inits.py\n@@ -189,7 +188,6 @@ jobs:\n             - run: make deps_table_check_updated\n             - run: python utils/update_metadata.py --check-only\n             - run: python utils/check_docstrings.py\n-            - run: python utils/check_support_list.py\n \n workflows:\n     version: 2"
        },
        {
            "sha": "21152e985082ace05f8cc083b8161f4298309b28",
            "filename": "Makefile",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/Makefile",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/Makefile",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/Makefile?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -37,7 +37,6 @@ autogenerate_code: deps_table_update\n repo-consistency:\n \tpython utils/check_copies.py\n \tpython utils/check_modular_conversion.py\n-\tpython utils/check_table.py\n \tpython utils/check_dummies.py\n \tpython utils/check_repo.py\n \tpython utils/check_inits.py\n@@ -46,7 +45,6 @@ repo-consistency:\n \tpython utils/check_doctest_list.py\n \tpython utils/update_metadata.py --check-only\n \tpython utils/check_docstrings.py\n-\tpython utils/check_support_list.py\n \n # this target runs checks on all files\n \n@@ -82,7 +80,6 @@ fixup: modified_only_fixup extra_style_checks autogenerate_code repo-consistency\n fix-copies:\n \tpython utils/check_copies.py --fix_and_overwrite\n \tpython utils/check_modular_conversion.py  --fix_and_overwrite\n-\tpython utils/check_table.py --fix_and_overwrite\n \tpython utils/check_dummies.py --fix_and_overwrite\n \tpython utils/check_doctest_list.py --fix_and_overwrite\n \tpython utils/check_docstrings.py --fix_and_overwrite"
        },
        {
            "sha": "055b1d0844a0914ec7e4f6a8ab2f3e08ff00401d",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 292,
            "deletions": 284,
            "changes": 576,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -1,292 +1,307 @@\n-- sections:\n+- title: Get started\n+  sections:\n   - local: index\n-    title: ðŸ¤— Transformers\n-  - local: quicktour\n-    title: Quick tour\n+    title: Transformers\n   - local: installation\n     title: Installation\n-  - local: add_new_model\n-    title: Adding a new model to `transformers`\n-  title: Get started\n-- sections:\n-  - local: pipeline_tutorial\n-    title: Run inference with pipelines\n-  - local: autoclass_tutorial\n-    title: Write portable code with AutoClass\n-  - local: preprocessing\n-    title: Preprocess data\n-  - local: training\n-    title: Fine-tune a pretrained model\n-  - local: run_scripts\n-    title: Train with a script\n-  - local: accelerate\n-    title: Set up distributed training with ðŸ¤— Accelerate\n-  - local: peft\n-    title: Load and train adapters with ðŸ¤— PEFT\n-  - local: model_sharing\n-    title: Share your model\n-  - local: agents\n-    title: Agents 101\n-  - local: agents_advanced\n-    title: Agents, supercharged - Multi-agents, External tools, and more\n-  - local: llm_tutorial\n-    title: Generation with LLMs\n-  - local: conversations\n-    title: Chatting with Transformers\n-  title: Tutorials\n-- sections:\n-  - isExpanded: false\n-    sections:\n-    - local: tasks/sequence_classification\n-      title: Text classification\n-    - local: tasks/token_classification\n-      title: Token classification\n-    - local: tasks/question_answering\n-      title: Question answering\n-    - local: tasks/language_modeling\n-      title: Causal language modeling\n-    - local: tasks/masked_language_modeling\n-      title: Masked language modeling\n-    - local: tasks/translation\n-      title: Translation\n-    - local: tasks/summarization\n-      title: Summarization\n-    - local: tasks/multiple_choice\n-      title: Multiple choice\n-    title: Natural Language Processing\n-  - isExpanded: false\n+  - local: quicktour\n+    title: Quickstart\n+- title: Base classes\n+  isExpanded: False\n+  sections:\n+  - title: Models\n     sections:\n-    - local: tasks/audio_classification\n-      title: Audio classification\n-    - local: tasks/asr\n-      title: Automatic speech recognition\n-    title: Audio\n-  - isExpanded: false\n+    - local: models\n+      title: Loading models\n+    - local: custom_models\n+      title: Customizing models\n+    - local: how_to_hack_models\n+      title: Customizing model components\n+    - local: model_sharing\n+      title: Sharing\n+    - local: add_new_model\n+      title: Adding a new model to Transformers\n+    - local: modular_transformers\n+      title: Modular Transformers\n+    - local: task_summary\n+      title: What ðŸ¤— Transformers can do\n+    - local: tasks_explained\n+      title: How ðŸ¤— Transformers solve tasks\n+    - local: model_summary\n+      title: The Transformer model family\n+    - local: attention\n+      title: Attention mechanisms\n+  - title: Preprocessors\n     sections:\n-    - local: tasks/image_classification\n-      title: Image classification\n-    - local: tasks/semantic_segmentation\n-      title: Image segmentation\n-    - local: tasks/video_classification\n-      title: Video classification\n-    - local: tasks/object_detection\n-      title: Object detection\n-    - local: tasks/zero_shot_object_detection\n-      title: Zero-shot object detection\n-    - local: tasks/zero_shot_image_classification\n-      title: Zero-shot image classification\n-    - local: tasks/monocular_depth_estimation\n-      title: Depth estimation\n-    - local: tasks/image_to_image\n-      title: Image-to-Image\n-    - local: tasks/image_feature_extraction\n-      title: Image Feature Extraction\n-    - local: tasks/mask_generation\n-      title: Mask Generation\n-    - local: tasks/keypoint_detection\n-      title: Keypoint Detection\n-    - local: tasks/knowledge_distillation_for_image_classification\n-      title: Knowledge Distillation for Computer Vision\n-    title: Computer Vision\n-  - isExpanded: false\n+    - local: fast_tokenizers\n+      title: Tokenizers\n+    - local: image_processors\n+      title: Image processors\n+    - local: backbones\n+      title: Backbones\n+    - local: feature_extractors\n+      title: Feature extractors\n+    - local: processors\n+      title: Processors\n+    - local: tokenizer_summary\n+      title: Summary of the tokenizers\n+    - local: pad_truncation\n+      title: Padding and truncation\n+- title: Inference\n+  isExpanded: False\n+  sections:\n+  - title: Pipeline API\n     sections:\n-    - local: tasks/image_captioning\n-      title: Image captioning\n-    - local: tasks/document_question_answering\n-      title: Document Question Answering\n-    - local: tasks/visual_question_answering\n-      title: Visual Question Answering\n-    - local: tasks/text-to-speech\n-      title: Text to speech\n-    - local: tasks/image_text_to_text\n-      title: Image-text-to-text\n-    - local: tasks/video_text_to_text\n-      title: Video-text-to-text\n-    title: Multimodal\n-  - isExpanded: false\n+    - local: pipeline_tutorial\n+      title: Pipeline\n+    - local: pipeline_gradio\n+      title: Machine learning apps\n+    - local: pipeline_webserver\n+      title: Web server inference\n+    - local: add_new_pipeline\n+      title: Adding a new pipeline\n+  - title: LLMs\n     sections:\n+    - local: llm_tutorial\n+      title: Text generation\n     - local: generation_strategies\n-      title: Customize the generation strategy\n+      title: Generation strategies\n+    - local: generation_features\n+      title: Generation features\n+    - local: tasks/prompting\n+      title: Prompt engineering\n+    - local: llm_optims\n+      title: Optimizing inference\n     - local: kv_cache\n-      title: Best Practices for Generation with Cache\n-    title: Generation\n-  - isExpanded: false\n+      title: KV cache strategies\n+    - local: cache_explanation\n+      title: Caching\n+    - local: llm_tutorial_optimization\n+      title: Getting the most out of LLMs\n+    - local: perplexity\n+      title: Perplexity of fixed-length models\n+  - title: Chat with models\n     sections:\n-    - local: chat_template_basics\n-      title: Getting Started with Chat Templates for Text LLMs\n-    - local: chat_template_multimodal\n-      title: Multimodal Chat Templates for Vision and Audio LLMs\n-    - local: chat_template_tools_and_documents\n-      title: Expanding Chat Templates with Tools and Documents\n-    - local: chat_template_advanced\n-      title: Advanced Usage and Customizing Your Chat Templates\n-    title: Chat Templates\n-  - isExpanded: false\n+    - local: conversations\n+      title: Chat basics\n+    - local: chat_templating\n+      title: Templates\n+    - local: chat_templating_multimodal\n+      title: Multimodal templates\n+    - local: chat_templating_writing\n+      title: Template writing\n+    - local: chat_extras\n+      title: Tools and RAG\n+  - title: Optimization\n     sections:\n-    - local: tasks/idefics\n-      title: Image tasks with IDEFICS\n-    - local: tasks/prompting\n-      title: LLM prompting guide\n-    title: Prompting\n-  title: Task Guides\n-- sections:\n-  - local: fast_tokenizers\n-    title: Use fast tokenizers from ðŸ¤— Tokenizers\n-  - local: multilingual\n-    title: Run inference with multilingual models\n-  - local: create_a_model\n-    title: Use model-specific APIs\n-  - local: custom_models\n-    title: Share a custom model\n-  - local: trainer\n-    title: Trainer\n-  - local: sagemaker\n-    title: Run training on Amazon SageMaker\n-  - local: serialization\n-    title: Export to ONNX\n-  - local: tflite\n-    title: Export to TFLite\n-  - local: torchscript\n-    title: Export to TorchScript\n-  - local: notebooks\n-    title: Notebooks with examples\n-  - local: community\n-    title: Community resources\n-  - local: troubleshooting\n-    title: Troubleshoot\n-  - local: gguf\n-    title: Interoperability with GGUF files\n-  - local: tiktoken\n-    title: Interoperability with TikToken files\n-  - local: modular_transformers\n-    title: Modularity in `transformers`\n-  - local: how_to_hack_models\n-    title: Model Hacking (overwriting a class to your usage)\n-  title: Developer guides\n-- sections:\n+    - local: perf_torch_compile\n+      title: torch.compile\n+    - local: perf_infer_gpu_one\n+      title: GPU\n+    - local: perf_infer_gpu_multi\n+      title: Distributed GPU inference\n+    - local: perf_infer_cpu\n+      title: CPU\n+    - local: tf_xla\n+      title: XLA\n+  - local: agents\n+    title: Agents\n+  - local: tools\n+    title: Tools\n+- title: Training\n+  isExpanded: False\n+  sections:\n+  - title: Trainer API\n+    sections:\n+    - local: trainer\n+      title: Trainer\n+    - local: training\n+      title: Fine-tuning\n+    - local: optimizers\n+      title: Optimizers\n+    - local: hpo_train\n+      title: Hyperparameter search\n+  - title: Distributed training\n+    sections:\n+    - local: gpu_selection\n+      title: GPU selection\n+    - local: accelerate\n+      title: Accelerate\n+    - local: fsdp\n+      title: FullyShardedDataParallel\n+    - local: deepspeed\n+      title: DeepSpeed\n+    - local: debugging\n+      title: Multi-GPU debugging\n+    - local: perf_train_cpu_many\n+      title: Distributed CPUs\n+    - local: perf_train_gpu_many\n+      title: Parallelism methods\n+  - title: Hardware\n+    sections:\n+    - local: perf_train_gpu_one\n+      title: GPU\n+    - local: perf_train_cpu\n+      title: CPU\n+    - local: perf_train_tpu_tf\n+      title: TPU\n+    - local: perf_train_special\n+      title: Apple Silicon\n+    - local: perf_hardware\n+      title: Build your own machine\n+  - local: peft\n+    title: PEFT\n+  - local: model_memory_anatomy\n+    title: Model training anatomy\n+- title: Quantization\n+  isExpanded: False\n+  sections:\n   - local: quantization/overview\n-    title: Getting started\n-  - local: quantization/bitsandbytes\n-    title: bitsandbytes\n-  - local: quantization/gptq\n-    title: GPTQ\n-  - local: quantization/awq\n-    title: AWQ\n+    title: Overview\n   - local: quantization/aqlm\n     title: AQLM\n-  - local: quantization/vptq\n-    title: SpQR\n-  - local: quantization/spqr\n-    title: VPTQ\n-  - local: quantization/quanto\n-    title: Quanto\n+  - local: quantization/awq\n+    title: AWQ\n+  - local: quantization/bitnet\n+    title: BitNet\n+  - local: quantization/bitsandbytes\n+    title: bitsandbytes\n+  - local: quantization/compressed_tensors\n+    title: compressed-tensors\n   - local: quantization/eetq\n     title: EETQ\n+  - local: quantization/fbgemm_fp8\n+    title: FBGEMM\n+  - local: quantization/finegrained_fp8\n+    title: Fine-grained FP8\n+  - local: gguf\n+    title: GGUF\n+  - local: quantization/gptq\n+    title: GPTQ\n   - local: quantization/higgs\n     title: HIGGS\n   - local: quantization/hqq\n     title: HQQ\n-  - local: quantization/fbgemm_fp8\n-    title: FBGEMM_FP8\n   - local: quantization/optimum\n     title: Optimum\n+  - local: quantization/quanto\n+    title: Quanto\n   - local: quantization/torchao\n-    title: TorchAO\n-  - local: quantization/bitnet\n-    title: BitNet\n-  - local: quantization/compressed_tensors\n-    title: compressed-tensors\n-  - local: quantization/finegrained_fp8\n-    title: Fine-grained FP8\n+    title: torchao\n+  - local: quantization/spqr\n+    title: SpQR\n+  - local: quantization/vptq\n+    title: VPTQ\n   - local: quantization/contribute\n-    title: Contribute new quantization method\n-  title: Quantization Methods\n-- sections:\n-  - local: performance\n-    title: Overview\n-  - local: llm_optims\n-    title: LLM inference optimization\n-  - sections:\n-    - local: perf_train_gpu_one\n-      title: Methods and tools for efficient training on a single GPU\n-    - local: perf_train_gpu_many\n-      title: Multiple GPUs and parallelism\n-    - local: fsdp\n-      title: Fully Sharded Data Parallel\n-    - local: deepspeed\n-      title: DeepSpeed\n-    - local: perf_train_cpu\n-      title: Efficient training on CPU\n-    - local: perf_train_cpu_many\n-      title: Distributed CPU training\n-    - local: perf_train_tpu_tf\n-      title: Training on TPU with TensorFlow\n-    - local: perf_train_special\n-      title: PyTorch training on Apple silicon\n-    - local: perf_hardware\n-      title: Custom hardware for training\n-    - local: hpo_train\n-      title: Hyperparameter Search using Trainer API\n-    title: Efficient training techniques\n-  - sections:\n-    - local: perf_infer_cpu\n-      title: CPU inference\n-    - local: perf_infer_gpu_one\n-      title: GPU inference\n-    - local: perf_infer_gpu_multi\n-      title: Multi-GPU inference\n-    title: Optimizing inference\n-  - local: big_models\n-    title: Instantiate a big model\n-  - local: debugging\n-    title: Debugging\n-  - local: tf_xla\n-    title: XLA Integration for TensorFlow Models\n-  - local: perf_torch_compile\n-    title: Optimize inference using `torch.compile()`\n-  title: Performance and scalability\n-- sections:\n+    title: Contribute\n+- title: Export to production\n+  isExpanded: False\n+  sections:\n+  - local: serialization\n+    title: ONNX\n+  - local: tflite\n+    title: LiteRT\n+  - local: executorch\n+    title: ExecuTorch\n+  - local: torchscript\n+    title: TorchScript\n+- title: Resources\n+  isExpanded: False\n+  sections:\n+  - title: Task recipes\n+    sections:\n+    - title: Natural language processing\n+      sections:\n+      - local: tasks/sequence_classification\n+        title: Text classification\n+      - local: tasks/token_classification\n+        title: Token classification\n+      - local: tasks/question_answering\n+        title: Question answering\n+      - local: tasks/language_modeling\n+        title: Causal language modeling\n+      - local: tasks/masked_language_modeling\n+        title: Masked language modeling\n+      - local: tasks/translation\n+        title: Translation\n+      - local: tasks/summarization\n+        title: Summarization\n+      - local: tasks/multiple_choice\n+        title: Multiple choice\n+    - title: Audio\n+      sections:\n+      - local: tasks/audio_classification\n+        title: Audio classification\n+      - local: tasks/asr\n+        title: Automatic speech recognition\n+    - title: Computer vision\n+      sections:\n+      - local: tasks/image_classification\n+        title: Image classification\n+      - local: tasks/semantic_segmentation\n+        title: Image segmentation\n+      - local: tasks/video_classification\n+        title: Video classification\n+      - local: tasks/object_detection\n+        title: Object detection\n+      - local: tasks/zero_shot_object_detection\n+        title: Zero-shot object detection\n+      - local: tasks/zero_shot_image_classification\n+        title: Zero-shot image classification\n+      - local: tasks/monocular_depth_estimation\n+        title: Depth estimation\n+      - local: tasks/image_to_image\n+        title: Image-to-Image\n+      - local: tasks/image_feature_extraction\n+        title: Image Feature Extraction\n+      - local: tasks/mask_generation\n+        title: Mask Generation\n+      - local: tasks/keypoint_detection\n+        title: Keypoint detection\n+      - local: tasks/knowledge_distillation_for_image_classification\n+        title: Knowledge Distillation for Computer Vision\n+    - title: Multimodal\n+      sections:\n+      - local: tasks/image_captioning\n+        title: Image captioning\n+      - local: tasks/document_question_answering\n+        title: Document Question Answering\n+      - local: tasks/visual_question_answering\n+        title: Visual Question Answering\n+      - local: tasks/text-to-speech\n+        title: Text to speech\n+      - local: tasks/idefics\n+        title: Image tasks with IDEFICS\n+      - local: tasks/image_text_to_text\n+        title: Image-text-to-text\n+      - local: tasks/video_text_to_text\n+        title: Video-text-to-text\n+  - local: run_scripts\n+    title: Training scripts\n+  - local: glossary\n+    title: Glossary\n+  - local: philosophy\n+    title: Philosophy\n+  - local: notebooks\n+    title: Notebooks with examples\n+  - local: community\n+    title: Community resources\n+  - local: troubleshooting\n+    title: Troubleshoot\n+- title: Contribute\n+  isExpanded: False\n+  sections:\n   - local: contributing\n-    title: How to contribute to ðŸ¤— Transformers?\n-  - local: add_new_model\n-    title: How to add a model to ðŸ¤— Transformers?\n-  - local: add_new_pipeline\n-    title: How to add a pipeline to ðŸ¤— Transformers?\n+    title: Contribute to Transformers\n   - local: testing\n-    title: Testing\n+    title: Transformers model tests\n   - local: pr_checks\n-    title: Checks on a Pull Request\n-  title: Contribute\n-- sections:\n-  - local: philosophy\n-    title: Philosophy\n-  - local: glossary\n-    title: Glossary\n-  - local: task_summary\n-    title: What ðŸ¤— Transformers can do\n-  - local: tasks_explained\n-    title: How ðŸ¤— Transformers solve tasks\n-  - local: model_summary\n-    title: The Transformer model family\n-  - local: tokenizer_summary\n-    title: Summary of the tokenizers\n-  - local: attention\n-    title: Attention mechanisms\n-  - local: pad_truncation\n-    title: Padding and truncation\n-  - local: bertology\n-    title: BERTology\n-  - local: perplexity\n-    title: Perplexity of fixed-length models\n-  - local: pipeline_webserver\n-    title: Pipelines for webserver inference\n-  - local: model_memory_anatomy\n-    title: Model training anatomy\n-  - local: llm_tutorial_optimization\n-    title: Getting the most out of LLMs\n-  title: Conceptual guides\n-- sections:\n-  - sections:\n+    title: Pull request checks\n+- title: API\n+  isExpanded: False\n+  sections:\n+  - title: Main classes\n+    sections:\n     - local: main_classes/agent\n       title: Agents and Tools\n     - local: model_doc/auto\n@@ -313,6 +328,8 @@\n       title: Optimization\n     - local: main_classes/output\n       title: Model outputs\n+    - local: main_classes/peft\n+      title: PEFT\n     - local: main_classes/pipelines\n       title: Pipelines\n     - local: main_classes/processors\n@@ -331,9 +348,9 @@\n       title: Feature Extractor\n     - local: main_classes/image_processor\n       title: Image Processor\n-    title: Main Classes\n-  - sections:\n-    - isExpanded: false\n+  - title: Models\n+    sections:\n+    - title: Text models\n       sections:\n       - local: model_doc/albert\n         title: ALBERT\n@@ -643,8 +660,7 @@\n         title: Zamba\n       - local: model_doc/zamba2\n         title: Zamba2\n-      title: Text models\n-    - isExpanded: false\n+    - title: Vision models\n       sections:\n       - local: model_doc/beit\n         title: BEiT\n@@ -772,8 +788,7 @@\n         title: YOLOS\n       - local: model_doc/zoedepth\n         title: ZoeDepth\n-      title: Vision models\n-    - isExpanded: false\n+    - title: Audio models\n       sections:\n       - local: model_doc/audio-spectrogram-transformer\n         title: Audio Spectrogram Transformer\n@@ -843,17 +858,15 @@\n         title: XLS-R\n       - local: model_doc/xlsr_wav2vec2\n         title: XLSR-Wav2Vec2\n-      title: Audio models\n-    - isExpanded: false\n+    - title: Video models\n       sections:\n       - local: model_doc/timesformer\n         title: TimeSformer\n       - local: model_doc/videomae\n         title: VideoMAE\n       - local: model_doc/vivit\n         title: ViViT\n-      title: Video models\n-    - isExpanded: false\n+    - title: Multimodal models\n       sections:\n       - local: model_doc/align\n         title: ALIGN\n@@ -995,15 +1008,13 @@\n         title: VisualBERT\n       - local: model_doc/xclip\n         title: X-CLIP\n-      title: Multimodal models\n-    - isExpanded: false\n+    - title: Reinforcement learning models\n       sections:\n       - local: model_doc/decision_transformer\n         title: Decision Transformer\n       - local: model_doc/trajectory_transformer\n         title: Trajectory Transformer\n-      title: Reinforcement learning models\n-    - isExpanded: false\n+    - title: Time series models\n       sections:\n       - local: model_doc/autoformer\n         title: Autoformer\n@@ -1015,14 +1026,12 @@\n         title: PatchTST\n       - local: model_doc/time_series_transformer\n         title: Time Series Transformer\n-      title: Time series models\n-    - isExpanded: false\n+    - title: Graph models\n       sections:\n       - local: model_doc/graphormer\n         title: Graphormer\n-      title: Graph models\n-    title: Models\n-  - sections:\n+  - title: Internal helpers\n+    sections:\n     - local: internal/modeling_utils\n       title: Custom Layers and Utilities\n     - local: internal/pipelines_utils\n@@ -1041,5 +1050,4 @@\n       title: General Utilities\n     - local: internal/time_series_utils\n       title: Utilities for Time Series\n-    title: Internal Helpers\n-  title: API\n+      \n\\ No newline at end of file"
        },
        {
            "sha": "c0ad46f8ac91fba157953fcba5143e437681d196",
            "filename": "docs/source/en/accelerate.md",
            "status": "modified",
            "additions": 114,
            "deletions": 85,
            "changes": 199,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Faccelerate.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Faccelerate.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Faccelerate.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -1,4 +1,4 @@\n-<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n+<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n \n Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n the License. You may obtain a copy of the License at\n@@ -14,123 +14,152 @@ rendered properly in your Markdown viewer.\n \n -->\n \n-# Distributed training with ðŸ¤— Accelerate\n+# Accelerate\n \n-As models get bigger, parallelism has emerged as a strategy for training larger models on limited hardware and accelerating training speed by several orders of magnitude. At Hugging Face, we created the [ðŸ¤— Accelerate](https://huggingface.co/docs/accelerate) library to help users easily train a ðŸ¤— Transformers model on any type of distributed setup, whether it is multiple GPU's on one machine or multiple GPU's across several machines. In this tutorial, learn how to customize your native PyTorch training loop to enable training in a distributed environment.\n+[Accelerate](https://hf.co/docs/accelerate/index) is a library designed to simplify distributed training on any type of setup with PyTorch by uniting the most common frameworks ([Fully Sharded Data Parallel (FSDP)](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/) and [DeepSpeed](https://www.deepspeed.ai/)) for it into a single interface. [`Trainer`] is powered by Accelerate under the hood, enabling loading big models and distributed training.\n \n-## Setup\n-\n-Get started by installing ðŸ¤— Accelerate:\n+This guide will show you two ways to use Accelerate with Transformers, using FSDP as the backend. The first method demonstrates distributed training with [`Trainer`], and the second method demonstrates adapting a PyTorch training loop. For more detailed information about Accelerate, please refer to the [documentation](https://hf.co/docs/accelerate/index).\n \n ```bash\n pip install accelerate\n ```\n \n-Then import and create an [`~accelerate.Accelerator`] object. The [`~accelerate.Accelerator`] will automatically detect your type of distributed setup and initialize all the necessary components for training. You don't need to explicitly place your model on a device.\n+Start by running [accelerate config](https://hf.co/docs/accelerate/main/en/package_reference/cli#accelerate-config) in the command line to answer a series of prompts about your training system. This creates and saves a configuration file to help Accelerate correctly set up training based on your setup.\n \n-```py\n->>> from accelerate import Accelerator\n+```bash\n+accelerate config\n+```\n \n->>> accelerator = Accelerator()\n+Depending on your setup and the answers you provide, an example configuration file for distributing training with FSDP on one machine with two GPUs may look like the following.\n+\n+```yaml\n+compute_environment: LOCAL_MACHINE\n+debug: false\n+distributed_type: FSDP\n+downcast_bf16: 'no'\n+fsdp_config:\n+  fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP\n+  fsdp_backward_prefetch_policy: BACKWARD_PRE\n+  fsdp_forward_prefetch: false\n+  fsdp_cpu_ram_efficient_loading: true\n+  fsdp_offload_params: false\n+  fsdp_sharding_strategy: FULL_SHARD\n+  fsdp_state_dict_type: SHARDED_STATE_DICT\n+  fsdp_sync_module_states: true\n+  fsdp_transformer_layer_cls_to_wrap: BertLayer\n+  fsdp_use_orig_params: true\n+machine_rank: 0\n+main_training_function: main\n+mixed_precision: bf16\n+num_machines: 1\n+num_processes: 2\n+rdzv_backend: static\n+same_network: true\n+tpu_env: []\n+tpu_use_cluster: false\n+tpu_use_sudo: false\n+use_cpu: false\n ```\n \n-## Prepare to accelerate\n+## Trainer\n \n-The next step is to pass all the relevant training objects to the [`~accelerate.Accelerator.prepare`] method. This includes your training and evaluation DataLoaders, a model and an optimizer:\n+Pass the path to the saved configuration file to [`TrainingArguments`], and from there, pass your [`TrainingArguments`] to [`Trainer`].\n \n ```py\n->>> train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(\n-...     train_dataloader, eval_dataloader, model, optimizer\n-... )\n+from transformers import TrainingArguments, Trainer\n+\n+training_args = TrainingArguments(\n+    output_dir=\"your-model\",\n+    learning_rate=2e-5,\n+    per_device_train_batch_size=16,\n+    per_device_eval_batch_size=16,\n+    num_train_epochs=2,\n+    fsdp_config=\"path/to/fsdp_config\",\n+    fsdp_strategy=\"full_shard\",\n+    weight_decay=0.01,\n+    eval_strategy=\"epoch\",\n+    save_strategy=\"epoch\",\n+    load_best_model_at_end=True,\n+    push_to_hub=True,\n+)\n+\n+trainer = Trainer(\n+    model=model,\n+    args=training_args,\n+    train_dataset=dataset[\"train\"],\n+    eval_dataset=dataset[\"test\"],\n+    processing_class=tokenizer,\n+    data_collator=data_collator,\n+    compute_metrics=compute_metrics,\n+)\n+\n+trainer.train()\n ```\n \n-## Backward\n+## Native PyTorch\n \n-The last addition is to replace the typical `loss.backward()` in your training loop with ðŸ¤— Accelerate's [`~accelerate.Accelerator.backward`] method:\n+Accelerate can also be added to any PyTorch training loop to enable distributed training. The [`~accelerate.Accelerator`] is the main entry point for adapting your PyTorch code to work with Accelerate. It automatically detects your distributed training setup and initializes all the necessary components for training. You don't need to explicitly place your model on a device because [`~accelerate.Accelerator`] knows which device to move your model to.\n \n ```py\n->>> for epoch in range(num_epochs):\n-...     for batch in train_dataloader:\n-...         outputs = model(**batch)\n-...         loss = outputs.loss\n-...         accelerator.backward(loss)\n-\n-...         optimizer.step()\n-...         lr_scheduler.step()\n-...         optimizer.zero_grad()\n-...         progress_bar.update(1)\n+from accelerate import Accelerator\n+\n+accelerator = Accelerator()\n+device = accelerator.device\n ```\n \n-As you can see in the following code, you only need to add four additional lines of code to your training loop to enable distributed training!\n+All PyTorch objects (model, optimizer, scheduler, dataloaders) should be passed to the [`~accelerate.Accelerator.prepare`] method now. This method moves your model to the appropriate device or devices, adapts the optimizer and scheduler to use [`~accelerate.optimizer.AcceleratedOptimizer`] and [`~accelerate.scheduler.AcceleratedScheduler`], and creates a new shardable dataloader.\n \n-```diff\n-+ from accelerate import Accelerator\n-  from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler\n+```py\n+train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(\n+    train_dataloader, eval_dataloader, model, optimizer\n+)\n+```\n \n-+ accelerator = Accelerator()\n+Replace `loss.backward` in your training loop with Accelerates [`~accelerate.Accelerator.backward`] method to scale the gradients and determine the appropriate `backward` method to use depending on your framework (for example, DeepSpeed or Megatron).\n \n-  model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n-  optimizer = AdamW(model.parameters(), lr=3e-5)\n+```py\n+for epoch in range(num_epochs):\n+    for batch in train_dataloader:\n+        outputs = model(**batch)\n+        loss = outputs.loss\n+        accelerator.backward(loss)\n+        optimizer.step()\n+        lr_scheduler.step()\n+        optimizer.zero_grad()\n+        progress_bar.update(1)\n+```\n \n-- device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n-- model.to(device)\n+Combine everything into a function and make it callable as a script.\n \n-+ train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(\n-+     train_dataloader, eval_dataloader, model, optimizer\n-+ )\n+```py\n+from accelerate import Accelerator\n+  \n+def main():\n+  accelerator = Accelerator()\n \n-  num_epochs = 3\n-  num_training_steps = num_epochs * len(train_dataloader)\n-  lr_scheduler = get_scheduler(\n-      \"linear\",\n-      optimizer=optimizer,\n-      num_warmup_steps=0,\n-      num_training_steps=num_training_steps\n+  model, optimizer, training_dataloader, scheduler = accelerator.prepare(\n+      model, optimizer, training_dataloader, scheduler\n   )\n \n-  progress_bar = tqdm(range(num_training_steps))\n-\n-  model.train()\n-  for epoch in range(num_epochs):\n-      for batch in train_dataloader:\n--         batch = {k: v.to(device) for k, v in batch.items()}\n-          outputs = model(**batch)\n-          loss = outputs.loss\n--         loss.backward()\n-+         accelerator.backward(loss)\n-\n-          optimizer.step()\n-          lr_scheduler.step()\n-          optimizer.zero_grad()\n-          progress_bar.update(1)\n+  for batch in training_dataloader:\n+      optimizer.zero_grad()\n+      inputs, targets = batch\n+      outputs = model(inputs)\n+      loss = loss_function(outputs, targets)\n+      accelerator.backward(loss)\n+      optimizer.step()\n+      scheduler.step()\n+\n+if __name__ == \"__main__\":\n+    main()\n ```\n \n-## Train\n-\n-Once you've added the relevant lines of code, launch your training in a script or a notebook like Colaboratory.\n+From the command line, call [accelerate launch](https://hf.co/docs/accelerate/main/en/package_reference/cli#accelerate-launch) to run your training script. Any additional arguments or parameters can be passed here as well.\n \n-### Train with a script\n-\n-If you are running your training from a script, run the following command to create and save a configuration file:\n-\n-```bash\n-accelerate config\n-```\n-\n-Then launch your training with:\n+To launch your training script on two GPUs, add the `--num_processes` argument.\n \n ```bash\n-accelerate launch train.py\n-```\n-\n-### Train with a notebook\n-\n-ðŸ¤— Accelerate can also run in a notebook if you're planning on using Colaboratory's TPUs. Wrap all the code responsible for training in a function, and pass it to [`~accelerate.notebook_launcher`]:\n-\n-```py\n->>> from accelerate import notebook_launcher\n-\n->>> notebook_launcher(training_function)\n+accelerate launch --num_processes=2 your_script.py\n ```\n \n-For more information about ðŸ¤— Accelerate and its rich features, refer to the [documentation](https://huggingface.co/docs/accelerate).\n+Refer to the [Launching Accelerate scripts](https://hf.co/docs/accelerate/main/en/basic_tutorials/launch) for more details."
        },
        {
            "sha": "bfab511972e7a6c95387a648fd8e02cd80110a8e",
            "filename": "docs/source/en/add_new_model.md",
            "status": "modified",
            "additions": 351,
            "deletions": 688,
            "changes": 1039,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fadd_new_model.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fadd_new_model.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fadd_new_model.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -1,4 +1,4 @@\n-<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n+<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n \n Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n the License. You may obtain a copy of the License at\n@@ -13,496 +13,302 @@ rendered properly in your Markdown viewer.\n \n -->\n \n-# How to add a model to ðŸ¤— Transformers?\n+# Adding a new model to Transformers\n \n-The ðŸ¤— Transformers library is often able to offer new models thanks to community contributors. But this can be a challenging project and requires an in-depth knowledge of the ðŸ¤— Transformers library and the model to implement. At Hugging Face, we're trying to empower more of the community to actively add models and we've put together this guide to walk you through the process of adding a PyTorch model (make sure you have [PyTorch installed](https://pytorch.org/get-started/locally/)).\n+> [!TIP]\n+> Try adding new models with a more [modular](./modular_transformers) approach first. This makes it significantly easier to contribute a model to Transformers!\n \n-Along the way, you'll:\n+Many of the models in Transformers are contributed by developers and researchers. As an open-source first project, we're invested in empowering the community to actively and independently add more models.\n \n-- get insights into open-source best practices\n-- understand the design principles behind one of the most popular deep learning libraries\n-- learn how to efficiently test large models\n-- learn how to integrate Python utilities like `black`, `ruff`, and `make fix-copies` to ensure clean and readable code\n+When you add a model to Transformers, you'll learn:\n \n-A Hugging Face team member will be available to help you along the way so you'll never be alone. ðŸ¤— â¤ï¸\n+- more about open-source best practices\n+- about a models architecture\n+- about Transformers' design principles\n+- how to efficiently test large models\n+- how to use Python utilities like [Black](https://black.readthedocs.io/en/stable/) and [Ruff](https://docs.astral.sh/ruff/) to create clean and readable code\n \n-To get started, open a [New model addition](https://github.com/huggingface/transformers/issues/new?assignees=&labels=New+model&template=new-model-addition.yml) issue for the model you want to see in ðŸ¤— Transformers. If you're not especially picky about contributing a specific model, you can filter by the [New model label](https://github.com/huggingface/transformers/labels/New%20model) to see if there are any unclaimed model requests and work on it.\n+It is a challenging but rewarding process.\n \n-Once you've opened a new model request, the first step is to get familiar with ðŸ¤— Transformers if you aren't already!\n+This guide will walk you through adding an example BrandNewLlama PyTorch model to Transformers. Before you begin, it is a good idea to familiarize yourself with the library.\n \n-## General overview of ðŸ¤— Transformers\n+## Transformers overview\n \n-First, you should get a general overview of ðŸ¤— Transformers. ðŸ¤— Transformers is a very opinionated library, so there is a\n-chance that you don't agree with some of the library's philosophies or design choices. From our experience, however, we\n-found that the fundamental design choices and philosophies of the library are crucial to efficiently scale ðŸ¤—\n-Transformers while keeping maintenance costs at a reasonable level.\n+Transformers is an opinionated library with its own unique philosophy and design choices. These choices help us sustainably scale and maintain Transformers.\n \n-A good first starting point to better understand the library is to read the [documentation of our philosophy](philosophy). As a result of our way of working, there are some choices that we try to apply to all models:\n+> [!TIP]\n+> Learn more about our design principles on the [Philosophy](./philosophy) doc.\n \n-- Composition is generally favored over-abstraction\n-- Duplicating code is not always bad if it strongly improves the readability or accessibility of a model\n-- Model files are as self-contained as possible so that when you read the code of a specific model, you ideally only\n-  have to look into the respective `modeling_....py` file.\n+Some of these design choices are:\n \n-In our opinion, the library's code is not just a means to provide a product, *e.g.* the ability to use BERT for\n-inference, but also as the very product that we want to improve. Hence, when adding a model, the user is not only the\n-person who will use your model, but also everybody who will read, try to understand, and possibly tweak your code.\n+- composition > over-abstraction\n+- duplicate code isn't always bad if it greatly improves readability and accessibility\n+- model files are self-contained and all the necessary model code is found in the `modeling_mymodel.py` file\n \n-With this in mind, let's go a bit deeper into the general library design.\n+These design choices are important *for everyone* interacting with the model. It is easier to read, understand, and modify.\n \n-### Overview of models\n+This section describes how the model and configuration classes interact and the Transformers code style.\n \n-To successfully add a model, it is important to understand the interaction between your model and its config,\n-[`PreTrainedModel`], and [`PretrainedConfig`]. For exemplary purposes, we will\n-call the model to be added to ðŸ¤— Transformers `BrandNewBert`.\n+### Model and configuration\n \n-Let's take a look:\n+All Transformers' models inherit from a base [`PreTrainedModel`] and [`PretrainedConfig`] class. The configuration is the models blueprint.\n \n-<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers_overview.png\"/>\n+There is never more than two levels of abstraction for any model to keep the code readable. The example model here, BrandNewLlama, inherits from `BrandNewLlamaPreTrainedModel` and [`PreTrainedModel`]. It is important that a new model only depends on [`PreTrainedModel`] so that it can use the [`~PreTrainedModel.from_pretrained`] and [`~PreTrainedModel.save_pretrained`] methods.\n \n-As you can see, we do make use of inheritance in ðŸ¤— Transformers, but we keep the level of abstraction to an absolute\n-minimum. There are never more than two levels of abstraction for any model in the library. `BrandNewBertModel`\n-inherits from `BrandNewBertPreTrainedModel` which in turn inherits from [`PreTrainedModel`] and\n-that's it. As a general rule, we want to make sure that a new model only depends on\n-[`PreTrainedModel`]. The important functionalities that are automatically provided to every new\n-model are [`~PreTrainedModel.from_pretrained`] and\n-[`~PreTrainedModel.save_pretrained`], which are used for serialization and deserialization. All of the\n-other important functionalities, such as `BrandNewBertModel.forward` should be completely defined in the new\n-`modeling_brand_new_bert.py` script. Next, we want to make sure that a model with a specific head layer, such as\n-`BrandNewBertForMaskedLM` does not inherit from `BrandNewBertModel`, but rather uses `BrandNewBertModel`\n-as a component that can be called in its forward pass to keep the level of abstraction low. Every new model requires a\n-configuration class, called `BrandNewBertConfig`. This configuration is always stored as an attribute in\n-[`PreTrainedModel`], and thus can be accessed via the `config` attribute for all classes\n-inheriting from `BrandNewBertPreTrainedModel`:\n+Other important functions like the forward method are defined in the `modeling.py` file.\n \n-```python\n-model = BrandNewBertModel.from_pretrained(\"brandy/brand_new_bert\")\n-model.config  # model has access to its config\n+Specific model heads (for example, sequence classification or language modeling) should call the base model in the forward pass rather than inherting from it to keep abstraction low.\n+\n+New models require a configuration, for example `BrandNewLlamaConfig`, that is stored as an attribute of [`PreTrainedModel`].\n+\n+```py\n+model = BrandNewLlamaModel.from_pretrained(\"username/brand_new_llama\")\n+model.config\n ```\n \n-Similar to the model, the configuration inherits basic serialization and deserialization functionalities from\n-[`PretrainedConfig`]. Note that the configuration and the model are always serialized into two\n-different formats - the model to a *pytorch_model.bin* file and the configuration to a *config.json* file. Calling\n-the model's [`~PreTrainedModel.save_pretrained`] will automatically call\n-the config's [`~PretrainedConfig.save_pretrained`], so that both model and configuration are saved.\n+[`PretrainedConfig`] provides the [`~PretrainedConfig.from_pretrained`] and [`~PretrainedConfig.save_pretrained`] methods.\n \n+When you use [`PreTrainedModel.save_pretrained`], it automatically calls [`PretrainedConfig.save_pretrained`] so that both the model and configuration are saved together.\n+\n+A model is saved to a `model.safetensors` file and a configuration is saved to a `config.json` file.\n \n ### Code style\n \n-When coding your new model, keep in mind that Transformers is an opinionated library and we have a few quirks of our\n-own regarding how code should be written :-)\n-\n-1. The forward pass of your model should be fully written in the modeling file while being fully independent of other\n-   models in the library. If you want to reuse a block from another model, copy the code and paste it with a\n-   `# Copied from` comment on top (see [here](https://github.com/huggingface/transformers/blob/v4.17.0/src/transformers/models/roberta/modeling_roberta.py#L160)\n-   for a good example and [there](pr_checks#check-copies) for more documentation on Copied from). \n-2. The code should be fully understandable, even by a non-native English speaker. This means you should pick\n-   descriptive variable names and avoid abbreviations. As an example, `activation` is preferred to `act`.\n-   One-letter variable names are strongly discouraged unless it's an index in a for loop.\n-3. More generally we prefer longer explicit code to short magical one.\n-4. Avoid subclassing `nn.Sequential` in PyTorch but subclass `nn.Module` and write the forward pass, so that anyone\n-   using your code can quickly debug it by adding print statements or breaking points.\n-5. Your function signature should be type-annotated. For the rest, good variable names are way more readable and\n-   understandable than type annotations.\n-\n-### Overview of tokenizers\n-\n-Not quite ready yet :-( This section will be added soon!\n-\n-## Step-by-step recipe to add a model to ðŸ¤— Transformers\n-\n-Everyone has different preferences of how to port a model so it can be very helpful for you to take a look at summaries\n-of how other contributors ported models to Hugging Face. Here is a list of community blog posts on how to port a model:\n-\n-1. [Porting GPT2 Model](https://medium.com/huggingface/from-tensorflow-to-pytorch-265f40ef2a28) by [Thomas](https://huggingface.co/thomwolf)\n-2. [Porting WMT19 MT Model](https://huggingface.co/blog/porting-fsmt) by [Stas](https://huggingface.co/stas)\n-\n-From experience, we can tell you that the most important things to keep in mind when adding a model are:\n-\n--  Don't reinvent the wheel! Most parts of the code you will add for the new ðŸ¤— Transformers model already exist\n-  somewhere in ðŸ¤— Transformers. Take some time to find similar, already existing models and tokenizers you can copy\n-  from. [grep](https://www.gnu.org/software/grep/) and [rg](https://github.com/BurntSushi/ripgrep) are your\n-  friends. Note that it might very well happen that your model's tokenizer is based on one model implementation, and\n-  your model's modeling code on another one. *E.g.* FSMT's modeling code is based on BART, while FSMT's tokenizer code\n-  is based on XLM.\n--  It's more of an engineering challenge than a scientific challenge. You should spend more time creating an\n-  efficient debugging environment rather than trying to understand all theoretical aspects of the model in the paper.\n--  Ask for help, when you're stuck! Models are the core component of ðŸ¤— Transformers so we at Hugging Face are more\n-  than happy to help you at every step to add your model. Don't hesitate to ask if you notice you are not making\n-  progress.\n-\n-In the following, we try to give you a general recipe that we found most useful when porting a model to ðŸ¤— Transformers.\n-\n-The following list is a summary of everything that has to be done to add a model and can be used by you as a To-Do\n-List:\n-\n-â˜ (Optional) Understood the model's theoretical aspects<br>\n-â˜ Prepared ðŸ¤— Transformers dev environment<br>\n-â˜ Set up debugging environment of the original repository<br>\n-â˜ Created script that successfully runs the `forward()` pass using the original repository and checkpoint<br>\n-â˜ Successfully added the model skeleton to ðŸ¤— Transformers<br>\n-â˜ Successfully converted original checkpoint to ðŸ¤— Transformers checkpoint<br>\n-â˜ Successfully ran `forward()` pass in ðŸ¤— Transformers that gives identical output to original checkpoint<br>\n-â˜ Finished model tests in ðŸ¤— Transformers<br>\n-â˜ Successfully added tokenizer in ðŸ¤— Transformers<br>\n-â˜ Run end-to-end integration tests<br>\n-â˜ Finished docs<br>\n-â˜ Uploaded model weights to the Hub<br>\n-â˜ Submitted the pull request<br>\n-â˜ (Optional) Added a demo notebook\n-\n-To begin with, we usually recommend starting by getting a good theoretical understanding of `BrandNewBert`. However,\n-if you prefer to understand the theoretical aspects of the model *on-the-job*, then it is totally fine to directly dive\n-into the `BrandNewBert`'s code-base. This option might suit you better if your engineering skills are better than\n-your theoretical skill, if you have trouble understanding `BrandNewBert`'s paper, or if you just enjoy programming\n-much more than reading scientific papers.\n-\n-### 1. (Optional) Theoretical aspects of BrandNewBert\n-\n-You should take some time to read *BrandNewBert's* paper, if such descriptive work exists. There might be large\n-sections of the paper that are difficult to understand. If this is the case, this is fine - don't worry! The goal is\n-not to get a deep theoretical understanding of the paper, but to extract the necessary information required to\n-effectively re-implement the model in ðŸ¤— Transformers. That being said, you don't have to spend too much time on the\n-theoretical aspects, but rather focus on the practical ones, namely:\n-\n--  What type of model is *brand_new_bert*? BERT-like encoder-only model? GPT2-like decoder-only model? BART-like\n-  encoder-decoder model? Look at the [model_summary](model_summary) if you're not familiar with the differences between those.\n--  What are the applications of *brand_new_bert*? Text classification? Text generation? Seq2Seq tasks, *e.g.,*\n-  summarization?\n--  What is the novel feature of the model that makes it different from BERT/GPT-2/BART?\n--  Which of the already existing [ðŸ¤— Transformers models](https://huggingface.co/transformers/#contents) is most\n-  similar to *brand_new_bert*?\n--  What type of tokenizer is used? A sentencepiece tokenizer? Word piece tokenizer? Is it the same tokenizer as used\n-  for BERT or BART?\n-\n-After you feel like you have gotten a good overview of the architecture of the model, you might want to write to the\n-Hugging Face team with any questions you might have. This might include questions regarding the model's architecture,\n-its attention layer, etc. We will be more than happy to help you.\n-\n-### 2. Next prepare your environment\n-\n-1. Fork the [repository](https://github.com/huggingface/transformers) by clicking on the â€˜Fork' button on the\n-   repository's page. This creates a copy of the code under your GitHub user account.\n-\n-2. Clone your `transformers` fork to your local disk, and add the base repository as a remote:\n-\n-   ```bash\n-   git clone https://github.com/[your Github handle]/transformers.git\n-   cd transformers\n-   git remote add upstream https://github.com/huggingface/transformers.git\n-   ```\n-\n-3. Set up a development environment, for instance by running the following command:\n-\n-   ```bash\n-   python -m venv .env\n-   source .env/bin/activate\n-   pip install -e \".[dev]\"\n-   ```\n-\n-   Depending on your OS, and since the number of optional dependencies of Transformers is growing, you might get a\n-   failure with this command. If that's the case make sure to install the Deep Learning framework you are working with\n-   (PyTorch, TensorFlow and/or Flax) then do:\n-\n-   ```bash\n-   pip install -e \".[quality]\"\n-   ```\n-\n-   which should be enough for most use cases. You can then return to the parent directory\n-\n-   ```bash\n-   cd ..\n-   ```\n-\n-4. We recommend adding the PyTorch version of *brand_new_bert* to Transformers. To install PyTorch, please follow the\n-   instructions on https://pytorch.org/get-started/locally/.\n-\n-   **Note:** You don't need to have CUDA installed. Making the new model work on CPU is sufficient.\n-\n-5. To port *brand_new_bert*, you will also need access to its original repository:\n-\n-   ```bash\n-   git clone https://github.com/org_that_created_brand_new_bert_org/brand_new_bert.git\n-   cd brand_new_bert\n-   pip install -e .\n-   ```\n-\n-Now you have set up a development environment to port *brand_new_bert* to ðŸ¤— Transformers.\n-\n-### 3.-4. Run a pretrained checkpoint using the original repository\n-\n-At first, you will work on the original *brand_new_bert* repository. Often, the original implementation is very\n-â€œresearchyâ€. Meaning that documentation might be lacking and the code can be difficult to understand. But this should\n-be exactly your motivation to reimplement *brand_new_bert*. At Hugging Face, one of our main goals is to *make people\n-stand on the shoulders of giants* which translates here very well into taking a working model and rewriting it to make\n-it as **accessible, user-friendly, and beautiful** as possible. This is the number-one motivation to re-implement\n-models into ðŸ¤— Transformers - trying to make complex new NLP technology accessible to **everybody**.\n-\n-You should start thereby by diving into the original repository.\n-\n-Successfully running the official pretrained model in the original repository is often **the most difficult** step.\n-From our experience, it is very important to spend some time getting familiar with the original code-base. You need to\n-figure out the following:\n-\n-- Where to find the pretrained weights?\n-- How to load the pretrained weights into the corresponding model?\n-- How to run the tokenizer independently from the model?\n-- Trace one forward pass so that you know which classes and functions are required for a simple forward pass. Usually,\n-  you only have to reimplement those functions.\n-- Be able to locate the important components of the model: Where is the model's class? Are there model sub-classes,\n-  *e.g.* EncoderModel, DecoderModel? Where is the self-attention layer? Are there multiple different attention layers,\n-  *e.g.* *self-attention*, *cross-attention*...?\n-- How can you debug the model in the original environment of the repo? Do you have to add *print* statements, can you\n-  work with an interactive debugger like *ipdb*, or should you use an efficient IDE to debug the model, like PyCharm?\n-\n-It is very important that before you start the porting process, you can **efficiently** debug code in the original\n-repository! Also, remember that you are working with an open-source library, so do not hesitate to open an issue, or\n-even a pull request in the original repository. The maintainers of this repository are most likely very happy about\n-someone looking into their code!\n-\n-At this point, it is really up to you which debugging environment and strategy you prefer to use to debug the original\n-model. We strongly advise against setting up a costly GPU environment, but simply work on a CPU both when starting to\n-dive into the original repository and also when starting to write the ðŸ¤— Transformers implementation of the model. Only\n-at the very end, when the model has already been successfully ported to ðŸ¤— Transformers, one should verify that the\n-model also works as expected on GPU.\n-\n-In general, there are two possible debugging environments for running the original model\n+Transformers prefers a clean and readable code over a more abstracted code style. Some of the code style choices include:\n \n--  [Jupyter notebooks](https://jupyter.org/) / [google colab](https://colab.research.google.com/notebooks/intro.ipynb)\n--  Local python scripts.\n+- The code should be accessible to non-English users. Pick descriptive variable names and avoid abbreviations. For example, \"activation\" is preferred over \"act\". One letter variables names are highly discouraged unless it's an index in a for loop.\n \n-Jupyter notebooks have the advantage that they allow for cell-by-cell execution which can be helpful to better split\n-logical components from one another and to have faster debugging cycles as intermediate results can be stored. Also,\n-notebooks are often easier to share with other contributors, which might be very helpful if you want to ask the Hugging\n-Face team for help. If you are familiar with Jupyter notebooks, we strongly recommend you work with them.\n+- Explicit code is preferred - even if it's longer - over shorter code.\n \n-The obvious disadvantage of Jupyter notebooks is that if you are not used to working with them you will have to spend\n-some time adjusting to the new programming environment and you might not be able to use your known debugging tools\n-anymore, like `ipdb`.\n+- Avoid subclassing [nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html). Subclass [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module) instead so the code can be quickly debugged with print statements or breakpoints.\n \n-For each code-base, a good first step is always to load a **small** pretrained checkpoint and to be able to reproduce a\n-single forward pass using a dummy integer vector of input IDs as an input. Such a script could look like this (in\n-pseudocode):\n+- Function signatures should be type-annotated. Otherwise, use good variable names so they're more understandable.\n \n-```python\n-model = BrandNewBertModel.load_pretrained_checkpoint(\"/path/to/checkpoint/\")\n-input_ids = [0, 4, 5, 2, 3, 7, 9]  # vector of input ids\n-original_output = model.predict(input_ids)\n-```\n+## New model addition issue\n \n-Next, regarding the debugging strategy, there are generally a few from which to choose from:\n+Open a [New model addition](https://github.com/huggingface/transformers/issues/new?assignees=&labels=New+model&template=new-model-addition.yml) issue to add a specific model.\n \n-- Decompose the original model into many small testable components and run a forward pass on each of those for\n-  verification\n-- Decompose the original model only into the original *tokenizer* and the original *model*, run a forward pass on\n-  those, and use intermediate print statements or breakpoints for verification\n+> [!TIP]\n+> Filter by the [New model](https://github.com/huggingface/transformers/labels/New%20model) label on GitHub to view and add any existing model requests.\n \n-Again, it is up to you which strategy to choose. Often, one or the other is advantageous depending on the original code\n-base.\n+Now is a good time to get familiar with BrandNewLlama. It is helpful to read a models research paper to understand its technical design and implementation. You don't necessarily have to worry too much about the theoretical details. Instead, focus on the practical ones. Use the questions below to guide your reading.\n \n-If the original code-base allows you to decompose the model into smaller sub-components, *e.g.* if the original\n-code-base can easily be run in eager mode, it is usually worth the effort to do so. There are some important advantages\n-to taking the more difficult road in the beginning:\n+- What type of model is BrandNewLlama? Is it a encoder, decoder, or encoder-decoder model?\n+- What tasks can BrandNewLlama be used for?\n+- What makes BrandNewLlama different from other models?\n+- What models in Transformers are most similar to BrandNewLlama?\n+- What tokenizer does BrandNewLlama use?\n \n-- at a later stage when comparing the original model to the Hugging Face implementation, you can verify automatically\n-  for each component individually that the corresponding component of the ðŸ¤— Transformers implementation matches instead\n-  of relying on visual comparison via print statements\n-- it can give you some rope to decompose the big problem of porting a model into smaller problems of just porting\n-  individual components and thus structure your work better\n-- separating the model into logical meaningful components will help you to get a better overview of the model's design\n-  and thus to better understand the model\n-- at a later stage those component-by-component tests help you to ensure that no regression occurs as you continue\n-  changing your code\n+In addition to learning more about your model, use the tips below to help you add a model faster.\n \n-[Lysandre's](https://gist.github.com/LysandreJik/db4c948f6b4483960de5cbac598ad4ed) integration checks for ELECTRA\n-gives a nice example of how this can be done.\n+> [!TIP]\n+> Each contributor has a unique style and workflow for adding models to Transformers. For an example, take a look at how [Gemma](https://github.com/huggingface/transformers/pull/29167) was added.\n \n-However, if the original code-base is very complex or only allows intermediate components to be run in a compiled mode,\n-it might be too time-consuming or even impossible to separate the model into smaller testable sub-components. A good\n-example is [T5's MeshTensorFlow](https://github.com/tensorflow/mesh/tree/master/mesh_tensorflow) library which is\n-very complex and does not offer a simple way to decompose the model into its sub-components. For such libraries, one\n-often relies on verifying print statements.\n+- Don't reinvent the wheel! Take your time to explore existing models and tokenizers to see what you can copy and reuse. [Grep](https://www.gnu.org/software/grep/) and [ripgrep](https://github.com/BurntSushi/ripgrep) are great tools for this.\n+- This is more of an engineering than a science challenge. Focus on the more practical (setting up an efficient debugging environment for example) instead of the theorertical aspects of the model.\n+- Don't be shy to ask for help! We are here to support you. ðŸ¤—\n \n-No matter which strategy you choose, the recommended procedure is often the same that you should start to debug the\n-starting layers first and the ending layers last.\n+## Dev environment\n \n-It is recommended that you retrieve the output, either by print statements or sub-component functions, of the following\n-layers in the following order:\n+Click on the **Fork** button on the [Transformers](https://github.com/huggingface/transformers) repository to create your own copy to work on. Clone the repository to your local disk and add the base repository as the remote.\n \n-1. Retrieve the input IDs passed to the model\n-2. Retrieve the word embeddings\n-3. Retrieve the input of the first Transformer layer\n-4. Retrieve the output of the first Transformer layer\n-5. Retrieve the output of the following n - 1 Transformer layers\n-6. Retrieve the output of the whole BrandNewBert Model\n+```bash\n+git clone https://github.com/[your Github handle]/transformers.git\n+cd transformers\n+git remote add upstream https://github.com/huggingface/transformers.git\n+```\n \n-Input IDs should thereby consists of an array of integers, *e.g.* `input_ids = [0, 4, 4, 3, 2, 4, 1, 7, 19]`\n+Create a virtual environment and perform an [editable install](./installation#editable-install) of the library with the \"dev\" or development dependencies.\n \n-The outputs of the following layers often consist of multi-dimensional float arrays and can look like this:\n+```bash\n+python -m venv .env\n+source .env/bin/activate\n+pip install -e \".[dev]\"\n+```\n+\n+Due to the number of optional dependencies as Transformers grows, this command may fail. In this case, install the \"quality\" dependencies. Also make sure you have a deep learning framework installed.\n \n+```bash\n+pip install -e \".[quality]\"\n ```\n-[[\n- [-0.1465, -0.6501,  0.1993,  ...,  0.1451,  0.3430,  0.6024],\n- [-0.4417, -0.5920,  0.3450,  ..., -0.3062,  0.6182,  0.7132],\n- [-0.5009, -0.7122,  0.4548,  ..., -0.3662,  0.6091,  0.7648],\n- ...,\n- [-0.5613, -0.6332,  0.4324,  ..., -0.3792,  0.7372,  0.9288],\n- [-0.5416, -0.6345,  0.4180,  ..., -0.3564,  0.6992,  0.9191],\n- [-0.5334, -0.6403,  0.4271,  ..., -0.3339,  0.6533,  0.8694]]],\n+\n+Return to the parent directory and clone and install the original BrandNewLlama repository.\n+\n+```bash\n+git clone https://github.com/org_that_created_brand_new_llama_org/brand_new_llama.git\n+cd brand_new_bert\n+pip install -e .\n ```\n \n-We expect that every model added to ðŸ¤— Transformers passes a couple of integration tests, meaning that the original\n-model and the reimplemented version in ðŸ¤— Transformers have to give the exact same output up to a precision of 0.001!\n-Since it is normal that the exact same model written in different libraries can give a slightly different output\n-depending on the library framework, we accept an error tolerance of 1e-3 (0.001). It is not enough if the model gives\n-nearly the same output, they have to be almost identical. Therefore, you will certainly compare the intermediate\n-outputs of the ðŸ¤— Transformers version multiple times against the intermediate outputs of the original implementation of\n-*brand_new_bert* in which case an **efficient** debugging environment of the original repository is absolutely\n-important. Here is some advice to make your debugging environment as efficient as possible.\n-\n-- Find the best way of debugging intermediate results. Is the original repository written in PyTorch? Then you should\n-  probably take the time to write a longer script that decomposes the original model into smaller sub-components to\n-  retrieve intermediate values. Is the original repository written in Tensorflow 1? Then you might have to rely on\n-  TensorFlow print operations like [tf.print](https://www.tensorflow.org/api_docs/python/tf/print) to output\n-  intermediate values. Is the original repository written in Jax? Then make sure that the model is **not jitted** when\n-  running the forward pass, *e.g.* check-out [this link](https://github.com/google/jax/issues/196).\n-- Use the smallest pretrained checkpoint you can find. The smaller the checkpoint, the faster your debug cycle\n-  becomes. It is not efficient if your pretrained model is so big that your forward pass takes more than 10 seconds.\n-  In case only very large checkpoints are available, it might make more sense to create a dummy model in the new\n-  environment with randomly initialized weights and save those weights for comparison with the ðŸ¤— Transformers version\n-  of your model\n-- Make sure you are using the easiest way of calling a forward pass in the original repository. Ideally, you want to\n-  find the function in the original repository that **only** calls a single forward pass, *i.e.* that is often called\n-  `predict`, `evaluate`, `forward` or `__call__`. You don't want to debug a function that calls `forward`\n-  multiple times, *e.g.* to generate text, like `autoregressive_sample`, `generate`.\n-- Try to separate the tokenization from the model's *forward* pass. If the original repository shows examples where\n-  you have to input a string, then try to find out where in the forward call the string input is changed to input ids\n-  and start from this point. This might mean that you have to possibly write a small script yourself or change the\n-  original code so that you can directly input the ids instead of an input string.\n-- Make sure that the model in your debugging setup is **not** in training mode, which often causes the model to yield\n-  random outputs due to multiple dropout layers in the model. Make sure that the forward pass in your debugging\n-  environment is **deterministic** so that the dropout layers are not used. Or use *transformers.utils.set_seed*\n-  if the old and new implementations are in the same framework.\n-\n-The following section gives you more specific details/tips on how you can do this for *brand_new_bert*.\n-\n-### 5.-14. Port BrandNewBert to ðŸ¤— Transformers\n-\n-Next, you can finally start adding new code to ðŸ¤— Transformers. Go into the clone of your ðŸ¤— Transformers' fork:\n+Return to your clone of Transformers to begin porting BrandNewLlama.\n \n ```bash\n cd transformers\n ```\n \n-In the special case that you are adding a model whose architecture exactly matches the model architecture of an\n-existing model you only have to add a conversion script as described in [this section](#write-a-conversion-script).\n-In this case, you can just re-use the whole model architecture of the already existing model.\n+There are two possible debugging environments for running the original model, a notebook ([Google Colab](https://colab.research.google.com/notebooks/intro.ipynb) or [Jupyter](https://jupyter.org/)) or a local Python script.\n+\n+> [!WARNING]\n+> We don't recommend setting up a GPU environment to run the original model because it can be expensive. Instead, work in a CPU environment first to verify the model works in Transformers. Once it does, then you can verify it on a GPU.\n+\n+Notebooks are great for executing code cell-by-cell which can help split logical components from one another. It can also accelerate debugging cycles because intermediate results can be stored. You can also share notebooks when working with other contributors.\n+\n+The downside is that if you aren't used to them, it may take some time to get used to.\n+\n+> [!TIP]\n+> If the model architecture is identical to an existing model, skip ahead to add a [conversion script](#conversion-script), because you can reuse the architecture of the existing model.\n \n-Otherwise, let's start generating a new model. We recommend using the following script to add a model starting from\n-an existing model:\n+Run the command below to start and complete the questionnaire with some basic information about the new model. This command jumpstarts the process by automatically generating some model code that you'll need to adapt.\n \n ```bash\n transformers-cli add-new-model-like\n ```\n \n-You will be prompted with a questionnaire to fill in the basic information of your model.\n+## Create a pull request\n \n-**Open a Pull Request on the main huggingface/transformers repo**\n+Before you start adapting the code, create a pull request to track your progress and get feedback from the Transformers team. Title your pull request **[WIP] Add BrandNewLlama** so it's clear that this is a work in progress.\n \n-Before starting to adapt the automatically generated code, now is the time to open a â€œWork in progress (WIP)â€ pull\n-request, *e.g.* â€œ[WIP] Add *brand_new_bert*â€, in ðŸ¤— Transformers so that you and the Hugging Face team can work\n-side-by-side on integrating the model into ðŸ¤— Transformers.\n+Create a branch with a descriptive name from your main branch.\n \n-You should do the following:\n+```bash\n+git checkout -b add_brand_new_bert\n+```\n \n-1. Create a branch with a descriptive name from your main branch\n+Commit the code, and then fetch and rebase on the main branch.\n \n-   ```bash\n-   git checkout -b add_brand_new_bert\n-   ```\n+```bash\n+git add .\n+git commit\n+git fetch upstream\n+git rebase upstream/main\n+```\n \n-2. Commit the automatically generated code:\n+Push any changes to your branch and click on **Compare & pull request** to open a pull request on GitHub. Open the pull request as a *draft* to indicate it's a work in progress.\n \n-   ```bash\n-   git add .\n-   git commit\n-   ```\n+```bash\n+git push -u origin a-descriptive-name-for-my-changes\n+```\n \n-3. Fetch and rebase to current main\n+Include relevant Hugging Face team members by adding their GitHub handles in the pull request for questions, feedback, comments, and reviews. Direct team members to specific parts of the code you want by clicking on the **Files changed** tab, and then clicking on **+** to the left of the line number to add a comment. When a question or problem is solved, click on **Resolve** to indicate the issue is resolved. This keeps the conversation organized and clean.\n \n-   ```bash\n-   git fetch upstream\n-   git rebase upstream/main\n-   ```\n+Remember to periodically commit and push your work, and update your work with the current main branch.\n \n-4. Push the changes to your account using:\n+```bash\n+git fetch upstream\n+git merge upstream/main\n+```\n \n-   ```bash\n-   git push -u origin a-descriptive-name-for-my-changes\n-   ```\n+## Original checkpoint\n \n-5. Once you are satisfied, go to the webpage of your fork on GitHub. Click on â€œPull requestâ€. Make sure to add the\n-   GitHub handle of some members of the Hugging Face team as reviewers, so that the Hugging Face team gets notified for\n-   future changes.\n+Take some time to work on the original model implementation first to understand how it works.\n \n-6. Change the PR into a draft by clicking on â€œConvert to draftâ€ on the right of the GitHub pull request web page.\n+This can be difficult if the original model repository is lacking documentation or if the codebase is complex. But you should use this as your motivation to implement the model in Transformers. Your contribution makes it more accessible and user-friendly to everyone!\n \n-In the following, whenever you have made some progress, don't forget to commit your work and push it to your account so\n-that it shows in the pull request. Additionally, you should make sure to update your work with the current main from\n-time to time by doing:\n+Orient yourself with the original repository by doing the following.\n \n-```bash\n-git fetch upstream\n-git merge upstream/main\n+- Locate the pretrained weights.\n+- Figure out how to the load pretrained weights into the model.\n+- Figure out how to run the tokenizer independently of the model.\n+- Trace one forward pass to understand which classes and functions are required. These are probably the only classes and functions you'll have to implement.\n+- Locate all the important components (model class, model subclasses, self-attention layer, etc.) of the model.\n+- Figure out how to debug the model in the original repository. Add print statements, use interactive debuggers like [ipdb](https://github.com/gotcha/ipdb), or a efficient integrated development environment (IDE) like [PyCharm](https://www.jetbrains.com/pycharm/).\n+\n+The last point is especially important because you'll need a thorough understanding of what's happening inside the original model before you can reimplement it in Transformers. Feel free to open issues and pull requests in the original repository if you encounter any issues.\n+\n+A good first step is to load a *small* pretrained checkpoint and try to reproduce a single forward pass with an example integer vector of inputs. For example, in pseudocode, this could look like the following.\n+\n+```py\n+model = BrandNewLlamaModel.load_pretrained_checkpoint(\"/path/to/checkpoint/\")\n+input_ids = [0, 4, 5, 2, 3, 7, 9]  # vector of input ids\n+original_output = model.generate(input_ids)\n ```\n \n-In general, all questions you might have regarding the model or your implementation should be asked in your PR and\n-discussed/solved in the PR. This way, the Hugging Face team will always be notified when you are committing new code or\n-if you have a question. It is often very helpful to point the Hugging Face team to your added code so that the Hugging\n-Face team can efficiently understand your problem or question.\n+### Debugging\n+\n+If you run into issues, you'll need to choose one of the following debugging strategies depending on the original models codebase.\n+\n+<hfoptions id=\"debug-strategy\">\n+<hfoption id=\"sub-components\">\n+\n+This strategy relies on breaking the original model into smaller sub-components, such as when the code can be easily run in eager mode. While more difficult, there are some advantages to this approach.\n+\n+1. It is easier later to compare the original model to your implementation. You can automatically verify that each individual component matches its corresponding component in the Transformers' implementation. This is better than relying on a visual comparison based on print statements.\n+2. It is easier to port individal components instead of the entire model.\n+3. It is easier for understanding how a model works by breaking it up into smaller parts.\n+4. It is easier to prevent regressions at a later stage when you change your code thanks to component-by-component tests.\n \n-To do so, you can go to the â€œFiles changedâ€ tab where you see all of your changes, go to a line regarding which you\n-want to ask a question, and click on the â€œ+â€ symbol to add a comment. Whenever a question or problem has been solved,\n-you can click on the â€œResolveâ€ button of the created comment.\n+> [!TIP]\n+> Refer to the ELECTRA [integration checks](https://gist.github.com/LysandreJik/db4c948f6b4483960de5cbac598ad4ed) for a good example of how to decompose a model into smaller components.\n \n-In the same way, the Hugging Face team will open comments when reviewing your code. We recommend asking most questions\n-on GitHub on your PR. For some very general questions that are not very useful for the public, feel free to ping the\n-Hugging Face team by Slack or email.\n+</hfoption>\n+<hfoption id=\"model and tokenizer\">\n \n-**5. Adapt the generated models code for brand_new_bert**\n+This strategy is viable when the original codebase is too complex, only allows intermediate components to be run in compiled mode, or if it's too time-consuming (maybe even impossible) to separate the model into smaller sub-components.\n \n-At first, we will focus only on the model itself and not care about the tokenizer. All the relevant code should be\n-found in the generated files `src/transformers/models/brand_new_bert/modeling_brand_new_bert.py` and\n-`src/transformers/models/brand_new_bert/configuration_brand_new_bert.py`.\n+For example, the MeshTensorFlow implementation of [T5](https://github.com/tensorflow/mesh/tree/master/mesh_tensorflow) is too complex and doesn't offer a simple way to decompose the model into its sub-components. In this situation, you'll have to rely on verifying print statements.\n \n-Now you can finally start coding :). The generated code in\n-`src/transformers/models/brand_new_bert/modeling_brand_new_bert.py` will either have the same architecture as BERT if\n-it's an encoder-only model or BART if it's an encoder-decoder model. At this point, you should remind yourself what\n-you've learned in the beginning about the theoretical aspects of the model: *How is the model different from BERT or\n-BART?*\". Implement those changes which often means changing the *self-attention* layer, the order of the normalization\n-layer, etcâ€¦ Again, it is often useful to look at the similar architecture of already existing models in Transformers to\n-get a better feeling of how your model should be implemented.\n+</hfoption>\n+</hfoptions>\n \n-**Note** that at this point, you don't have to be very sure that your code is fully correct or clean. Rather, it is\n-advised to add a first *unclean*, copy-pasted version of the original code to\n-`src/transformers/models/brand_new_bert/modeling_brand_new_bert.py` until you feel like all the necessary code is\n-added. From our experience, it is much more efficient to quickly add a first version of the required code and\n-improve/correct the code iteratively with the conversion script as described in the next section. The only thing that\n-has to work at this point is that you can instantiate the ðŸ¤— Transformers implementation of *brand_new_bert*, *i.e.* the\n-following command should work:\n+Whichever strategy you choose, it is recommended to debug the initial layers first and the final layers last. Retrieve the output, either with print statements or sub-component functions, of the following layers in this order.\n \n-```python\n-from transformers import BrandNewBertModel, BrandNewBertConfig\n+1. input ids passed to the model\n+2. word embeddings\n+3. input of the first Transformer layer\n+4. output of the first Transformer layer\n+5. output of the following n-1 Transformer layers\n+6. output of the whole model\n \n-model = BrandNewBertModel(BrandNewBertConfig())\n+The input ids should just be an array of integers like `input_ids = [0, 4, 4, 3, 2, 4, 1, 7, 19]`.\n+\n+Layer outputs often consist of multi-dimensional float arrays.\n+\n+```py\n+[[\n+ [-0.1465, -0.6501,  0.1993,  ...,  0.1451,  0.3430,  0.6024],\n+ [-0.4417, -0.5920,  0.3450,  ..., -0.3062,  0.6182,  0.7132],\n+ [-0.5009, -0.7122,  0.4548,  ..., -0.3662,  0.6091,  0.7648],\n+ ...,\n+ [-0.5613, -0.6332,  0.4324,  ..., -0.3792,  0.7372,  0.9288],\n+ [-0.5416, -0.6345,  0.4180,  ..., -0.3564,  0.6992,  0.9191],\n+ [-0.5334, -0.6403,  0.4271,  ..., -0.3339,  0.6533,  0.8694]]],\n ```\n \n-The above command will create a model according to the default parameters as defined in `BrandNewBertConfig()` with\n-random weights, thus making sure that the `init()` methods of all components works.\n+Every Transformers model output should have a precision or error tolerance of *1e-3*. This accounts for any output differences that arise from using a different library framework. Compare the intermediate outputs of the original model with the Transformers implementation to ensure they're nearly identical. Having an *efficient* debugging environment is crucial for this step.\n+\n+Here are some tips for an efficient debugging environment.\n+\n+- To debug intermediate results, it depends on the machine learning framework the original model repository is using. For PyTorch, you should write a script to decompose the original model into smaller sub-components to retrieve the intermediate values. For TensorFlow, you may need to use [tf.print](https://www.tensorflow.org/api_docs/python/tf/print). For Flax, make sure the model is *not jitted* during the forward pass (refer to this GitHub [Issue](https://github.com/google/jax/issues/196) for more details).\n+\n+- It is faster to debug with a smaller pretrained checkpoint versus a larger checkpoint where the forward pass takes more than 10 seconds. If only large checkpoints are available, create a dummy model with randomly initialized weights and save those weights to compare against the Transformers implementation.\n+\n+- Find the easiest way to call the model's forward pass. Ideally, this function (may be called `predict`, `evaluate`, `forward`, or `__call__`) should only call the forward pass *once*. It is more difficult to debug a function that calls the forward pass multiple times.\n+\n+- Separate tokenization from the forward pass. Locate where a string input is changed to input ids in the forward pass and start here. You may need to create a small script or modify the original code to directly input the input ids instead of an input string.\n+\n+- Ensure the model is *not* in training mode. This can produce random outputs due to multiple dropout layers in a model. The forward pass in your debugging environment should be *deterministic* so that the dropout layers aren't used.\n+\n+Once you're able to run the original checkpoint, you're ready to start adapting the model code for Transformers.\n+\n+## Adapt the model code\n+\n+The `transformers-cli add-new-model-like` command should have generated a model and configuration file.\n+\n+- `src/transformers/models/brand_new_llama/modeling_brand_new_llama.py`\n+- `src/transformers/models/brand_new_llama/configuration_brand_new_llama.py`\n+\n+The automatically generated code in the `modeling.py` file has the same architecture as Llama if you answered it's a decoder-only model or it will have the same architecture as BART if you answered it's an encoder-decoder model. The generated code is just a starting point. Based on your research on the new model, you'll need to implement those specific changes by adapting the generated code. This may involve changes to the self-attention layer, the order of the normalization layer, and so on.\n \n-Note that all random initialization should happen in the `_init_weights` method of your `BrandnewBertPreTrainedModel`\n-class. It should initialize all leaf modules depending on the variables of the config. Here is an example with the\n-BERT `_init_weights` method:\n+### Model initialization\n+\n+At this point, your code doesn't have to be clean or even fully correct, It is more efficient to quickly create a first draft and then iteratively improve on it. The most important thing is that your model can be instantiated from Transformers. The command below creates a model from the configuration with random weights, verifying that the `__init__` method works.\n+\n+```py\n+from transformers import BrandNewLlama, BrandNewLlamaConfig\n+model = BrandNewLlama(BrandNewLlamaConfig())\n+```\n+\n+Random initialization occurs in the `_init_weights` method of `BrandNewLlamaPreTrainedModel`. All leaf modules are initialized depending on the configuration's variables.\n \n ```py\n def _init_weights(self, module):\n@@ -520,9 +326,9 @@ def _init_weights(self, module):\n         module.weight.data.fill_(1.0)\n ```\n \n-You can have some more custom schemes if you need a special initialization for some modules. For instance, in\n-`Wav2Vec2ForPreTraining`, the last two linear layers need to have the initialization of the regular PyTorch `nn.Linear`\n-but all the other ones should use an initialization as above. This is coded like this:\n+The initialization scheme can look different if you need to adapt it to your model. For example, [`Wav2Vec2ForPreTraining`] initializes [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) in its last two linear layers.\n+\n+The `_is_hf_initialized` flag makes sure the submodule is only initialized once. Setting `module.project_q` and `module.project_hid` to `True` ensures the custom initialization is not overriden later. The `_init_weights` function won't be applied to these modules.\n \n ```py\n def _init_weights(self, module):\n@@ -538,30 +344,34 @@ def _init_weights(self, module):\n             module.bias.data.zero_()\n ```\n \n-The `_is_hf_initialized` flag is internally used to make sure we only initialize a submodule once. By setting it to\n-`True` for `module.project_q` and `module.project_hid`, we make sure the custom initialization we did is not overridden later on,\n-the `_init_weights` function won't be applied to them.\n+### Convert checkpoints to Transformers\n \n-**6. Write a conversion script**\n+The original checkpoint must be converted to a Transformers compatible checkpoint.\n \n-Next, you should write a conversion script that lets you convert the checkpoint you used to debug *brand_new_bert* in\n-the original repository to a checkpoint compatible with your just created ðŸ¤— Transformers implementation of\n-*brand_new_bert*. It is not advised to write the conversion script from scratch, but rather to look through already\n-existing conversion scripts in ðŸ¤— Transformers for one that has been used to convert a similar model that was written in\n-the same framework as *brand_new_bert*. Usually, it is enough to copy an already existing conversion script and\n-slightly adapt it for your use case. Don't hesitate to ask the Hugging Face team to point you to a similar already\n-existing conversion script for your model.\n+> [!TIP]\n+> Try looking for an existing conversion script to copy, adapt, and reuse for your model!\n+>\n+> - If you're porting a model from TensorFlow to PyTorch, a good starting point may be the BERT [conversion script](https://github.com/huggingface/transformers/blob/7acfa95afb8194f8f9c1f4d2c6028224dbed35a2/src/transformers/models/bert/modeling_bert.py#L91).\n+> - If you're porting a model from PyTorch to PyTorch, a good starting point may be the BART [conversion script](https://github.com/huggingface/transformers/blob/main/src/transformers/models/bart/convert_bart_original_pytorch_checkpoint_to_pytorch.py).\n \n-- If you are porting a model from TensorFlow to PyTorch, a good starting point might be BERT's conversion script [here](https://github.com/huggingface/transformers/blob/7acfa95afb8194f8f9c1f4d2c6028224dbed35a2/src/transformers/models/bert/modeling_bert.py#L91)\n-- If you are porting a model from PyTorch to PyTorch, a good starting point might be BART's conversion script [here](https://github.com/huggingface/transformers/blob/main/src/transformers/models/bart/convert_bart_original_pytorch_checkpoint_to_pytorch.py)\n+Make sure **all** required weights are initialized and print out all the checkpoint weights that weren't used for initialization to make sure the model has been converted correctly.\n \n-In the following, we'll quickly explain how PyTorch models store layer weights and define layer names. In PyTorch, the\n-name of a layer is defined by the name of the class attribute you give the layer. Let's define a dummy model in\n-PyTorch, called `SimpleModel` as follows:\n+You may encounter wrong shape statements or name assignments during the conversion. This is most likely because of incorrect parameters in `BrandNewLlamaConfig`, the wrong architecture, a bug in the `init` method of your implementation, or you need to transpose one of the checkpoint weights.\n \n-```python\n-from torch import nn\n+Keep iterating on the [Adapt the model code](#adapt-the-model-code) section until all the checkpoint weights are correctly loaded. Once you can load a checkpoint in your model, save it to a folder. This should contain a `model.safetensors` file and a `config.json` file.\n \n+```py\n+model.save_pretrained(\"/path/to/converted/checkpoint/folder\")\n+```\n+\n+To help with conversion, the next section briefly describes how PyTorch models stores and defines layer weights and names.\n+\n+#### PyTorch layer weights and names\n+\n+It is helpful to create a basic PyTorch model to understand how layer names are defined and weights are initialized.\n+\n+```py\n+from torch import nn\n \n class SimpleModel(nn.Module):\n     def __init__(self):\n@@ -571,35 +381,22 @@ class SimpleModel(nn.Module):\n         self.layer_norm = nn.LayerNorm(10)\n ```\n \n-Now we can create an instance of this model definition which will fill all weights: `dense`, `intermediate`,\n-`layer_norm` with random weights. We can print the model to see its architecture\n+PyTorch layer names are defined by the class attribute name of the layer (`dense`, `intermediate`, `layer_norm`). Create a instance of `SimpleModel` to fill all the layers with random weights.\n \n-```python\n+```py\n model = SimpleModel()\n-\n print(model)\n-```\n-\n-This will print out the following:\n-\n-```\n SimpleModel(\n   (dense): Linear(in_features=10, out_features=10, bias=True)\n   (intermediate): Linear(in_features=10, out_features=10, bias=True)\n   (layer_norm): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n )\n ```\n \n-We can see that the layer names are defined by the name of the class attribute in PyTorch. You can print out the weight\n-values of a specific layer:\n+The weight values of a specific layer are randomly initialized.\n \n-```python\n+```py\n print(model.dense.weight.data)\n-```\n-\n-to see that the weights were randomly initialized\n-\n-```\n tensor([[-0.0818,  0.2207, -0.0749, -0.0030,  0.0045, -0.1569, -0.1598,  0.0212,\n          -0.2077,  0.2157],\n         [ 0.1044,  0.0201,  0.0990,  0.2482,  0.3116,  0.2509,  0.2866, -0.2190,\n@@ -622,339 +419,205 @@ tensor([[-0.0818,  0.2207, -0.0749, -0.0030,  0.0045, -0.1569, -0.1598,  0.0212,\n           0.2220,  0.2358]]).\n ```\n \n-In the conversion script, you should fill those randomly initialized weights with the exact weights of the\n-corresponding layer in the checkpoint. *E.g.*\n+In the conversion script, the random weights should be replaced with the exact weights from the corresponding layer in the original checkpoint.\n \n-```python\n-# retrieve matching layer weights, e.g. by\n-# recursive algorithm\n+```py\n+# retrieve matching layer weights with recursive algorithm\n layer_name = \"dense\"\n pretrained_weight = array_of_dense_layer\n \n model_pointer = getattr(model, \"dense\")\n-\n model_pointer.weight.data = torch.from_numpy(pretrained_weight)\n ```\n \n-While doing so, you must verify that each randomly initialized weight of your PyTorch model and its corresponding\n-pretrained checkpoint weight exactly match in both **shape and name**. To do so, it is **necessary** to add assert\n-statements for the shape and print out the names of the checkpoints weights. E.g. you should add statements like:\n+Verify the randomly initialized weights and their corresponding pretrained checkpoint weights have the identical **shape** and **name**. Add assert statements for the shape and print out the checkpoint weight names.\n \n-```python\n+```py\n assert (\n     model_pointer.weight.shape == pretrained_weight.shape\n ), f\"Pointer shape of random weight {model_pointer.shape} and array shape of checkpoint weight {pretrained_weight.shape} mismatched\"\n+\n+logger.info(f\"Initialize PyTorch weight {layer_name} from {pretrained_weight.name}\")\n ```\n \n-Besides, you should also print out the names of both weights to make sure they match, *e.g.*\n+When the shape or name don't match, you may have assigned the incorrect checkpoint weight to a randomly initialized layer. An incorrect shape may be because the `BrandNewLlama` parameters don't exactly match the original models parameters. But it could also be that the PyTorch layer implementation requires the weights to be transposed first.\n \n-```python\n-logger.info(f\"Initialize PyTorch weight {layer_name} from {pretrained_weight.name}\")\n+### Implement the forward pass\n+\n+The forward pass should be implemented next if the model loads correctly. It takes some inputs and returns the model output.\n+\n+```py\n+model = BrandNewLlamaModel.from_pretrained(\"/path/to/converted/checkpoint/folder\")\n+input_ids = [0, 4, 4, 3, 2, 4, 1, 7, 19]\n+output = model.generate(input_ids).last_hidden_states\n ```\n \n-If either the shape or the name doesn't match, you probably assigned the wrong checkpoint weight to a randomly\n-initialized layer of the ðŸ¤— Transformers implementation.\n+Don't be discouraged if your forward pass isn't identical with the output from the original model or if it returns an error. Check that the forward pass doesn't throw any errors. This is often because the dimensions are wrong or because the wrong data type is used ([torch.long](https://pytorch.org/docs/stable/generated/torch.Tensor.long.html) instead of [torch.float32](https://pytorch.org/docs/stable/tensors.html)).\n \n-An incorrect shape is most likely due to an incorrect setting of the config parameters in `BrandNewBertConfig()` that\n-do not exactly match those that were used for the checkpoint you want to convert. However, it could also be that\n-PyTorch's implementation of a layer requires the weight to be transposed beforehand.\n+Your output should have a precision of *1e-3*. Ensure the output shapes and output values are identical. Common reasons for why the outputs aren't identical include:\n \n-Finally, you should also check that **all** required weights are initialized and print out all checkpoint weights that\n-were not used for initialization to make sure the model is correctly converted. It is completely normal, that the\n-conversion trials fail with either a wrong shape statement or a wrong name assignment. This is most likely because either\n-you used incorrect parameters in `BrandNewBertConfig()`, have a wrong architecture in the ðŸ¤— Transformers\n-implementation, you have a bug in the `init()` functions of one of the components of the ðŸ¤— Transformers\n-implementation or you need to transpose one of the checkpoint weights.\n+- Some layers were not added (activation layer or a residual connection).\n+- The word embedding matix is not tied.\n+- The wrong positional embeddings are used because the original implementation includes an offset.\n+- Dropout is applied during the forward pass. Fix this error by making sure `model.training` is `False` and passing `self.training` to [torch.nn.functional.dropout](https://pytorch.org/docs/stable/nn.functional.html?highlight=dropout#torch.nn.functional.dropout).\n \n-This step should be iterated with the previous step until all weights of the checkpoint are correctly loaded in the\n-Transformers model. Having correctly loaded the checkpoint into the ðŸ¤— Transformers implementation, you can then save\n-the model under a folder of your choice `/path/to/converted/checkpoint/folder` that should then contain both a\n-`pytorch_model.bin` file and a `config.json` file:\n+Compare the forward pass of the original model and your implementation to check if there are any differences. Ideally, debug and print out the intermediate outputs of both implementations of the forward pass to pinpoint where the original implementation differs from yours.\n \n-```python\n-model.save_pretrained(\"/path/to/converted/checkpoint/folder\")\n-```\n+1. Make sure the hardcoded `input_ids` in both implementations are identical.\n+2. Verify the outputs of the first transformation of `input_ids` (usually the word embeddings) are identical, and work your way through to the last layer.\n \n-**7. Implement the forward pass**\n+Any difference between the two implementations should point to the bug in your implementation.\n \n-Having managed to correctly load the pretrained weights into the ðŸ¤— Transformers implementation, you should now make\n-sure that the forward pass is correctly implemented. In [Get familiar with the original repository](#3-4-run-a-pretrained-checkpoint-using-the-original-repository), you have already created a script that runs a forward\n-pass of the model using the original repository. Now you should write an analogous script using the ðŸ¤— Transformers\n-implementation instead of the original one. It should look as follows:\n+One of the best strategies is to add many print statements to the same positions in both implementations, and then successively remove them when they output identical values for the intermediate outputs.\n \n-```python\n-model = BrandNewBertModel.from_pretrained(\"/path/to/converted/checkpoint/folder\")\n-input_ids = [0, 4, 4, 3, 2, 4, 1, 7, 19]\n-output = model(input_ids).last_hidden_states\n-```\n-\n-It is very likely that the ðŸ¤— Transformers implementation and the original model implementation don't give the exact\n-same output the very first time or that the forward pass throws an error. Don't be disappointed - it's expected! First,\n-you should make sure that the forward pass doesn't throw any errors. It often happens that the wrong dimensions are\n-used leading to a *Dimensionality mismatch* error or that the wrong data type object is used, *e.g.* `torch.long`\n-instead of `torch.float32`. Don't hesitate to ask the Hugging Face team for help, if you don't manage to solve\n-certain errors.\n-\n-The final part to make sure the ðŸ¤— Transformers implementation works correctly is to ensure that the outputs are\n-equivalent to a precision of `1e-3`. First, you should ensure that the output shapes are identical, *i.e.*\n-`outputs.shape` should yield the same value for the script of the ðŸ¤— Transformers implementation and the original\n-implementation. Next, you should make sure that the output values are identical as well. This one of the most difficult\n-parts of adding a new model. Common mistakes why the outputs are not identical are:\n-\n-- Some layers were not added, *i.e.* an *activation* layer was not added, or the residual connection was forgotten\n-- The word embedding matrix was not tied\n-- The wrong positional embeddings are used because the original implementation uses on offset\n-- Dropout is applied during the forward pass. To fix this make sure *model.training is False* and that no dropout\n-  layer is falsely activated during the forward pass, *i.e.* pass *self.training* to [PyTorch's functional dropout](https://pytorch.org/docs/stable/nn.functional.html?highlight=dropout#torch.nn.functional.dropout)\n-\n-The best way to fix the problem is usually to look at the forward pass of the original implementation and the ðŸ¤—\n-Transformers implementation side-by-side and check if there are any differences. Ideally, you should debug/print out\n-intermediate outputs of both implementations of the forward pass to find the exact position in the network where the ðŸ¤—\n-Transformers implementation shows a different output than the original implementation. First, make sure that the\n-hard-coded `input_ids` in both scripts are identical. Next, verify that the outputs of the first transformation of\n-the `input_ids` (usually the word embeddings) are identical. And then work your way up to the very last layer of the\n-network. At some point, you will notice a difference between the two implementations, which should point you to the bug\n-in the ðŸ¤— Transformers implementation. From our experience, a simple and efficient way is to add many print statements\n-in both the original implementation and ðŸ¤— Transformers implementation, at the same positions in the network\n-respectively, and to successively remove print statements showing the same values for intermediate presentations.\n-\n-When you're confident that both implementations yield the same output, verify the outputs with\n-`torch.allclose(original_output, output, atol=1e-3)`, you're done with the most difficult part! Congratulations - the\n-work left to be done should be a cakewalk ðŸ˜Š.\n-\n-**8. Adding all necessary model tests**\n-\n-At this point, you have successfully added a new model. However, it is very much possible that the model does not yet\n-fully comply with the required design. To make sure, the implementation is fully compatible with ðŸ¤— Transformers, all\n-common tests should pass. The Cookiecutter should have automatically added a test file for your model, probably under\n-the same `tests/models/brand_new_bert/test_modeling_brand_new_bert.py`. Run this test file to verify that all common\n-tests pass:\n+When both implementations produce the same output, verify the outputs are within a precision of *1e-3*.\n \n-```bash\n-pytest tests/models/brand_new_bert/test_modeling_brand_new_bert.py\n+```py\n+torch.allclose(original_output, output, atol=1e-3)\n ```\n \n-Having fixed all common tests, it is now crucial to ensure that all the nice work you have done is well tested, so that\n+This is typically the most difficult part of the process. Congratulations if you've made it this far! \n \n-- a) The community can easily understand your work by looking at specific tests of *brand_new_bert*\n-- b) Future changes to your model will not break any important feature of the model.\n+And if you're stuck or struggling with this step, don't hesitate to ask for help on your pull request.\n \n-At first, integration tests should be added. Those integration tests essentially do the same as the debugging scripts\n-you used earlier to implement the model to ðŸ¤— Transformers. A template of those model tests has already added by the\n-Cookiecutter, called `BrandNewBertModelIntegrationTests` and only has to be filled out by you. To ensure that those\n-tests are passing, run\n+### Add model tests\n+\n+While the model works, you still need to add tests to ensure it is compatible with Transformers. Tests are important because they help users understand your work by looking at specific tests, and because they prevent your model from breaking in the future if any changes are made.\n+\n+[Cookiecutter](https://cookiecutter.readthedocs.io/en/stable/) should have added a test file for your model. Run the test file below to make sure all common tests pass.\n \n ```bash\n-RUN_SLOW=1 pytest -sv tests/models/brand_new_bert/test_modeling_brand_new_bert.py::BrandNewBertModelIntegrationTests\n+pytest tests/models/brand_new_llama/test_modeling_brand_new_llama.py\n ```\n \n-<Tip>\n+The integration tests should be added first because they serve the same purpose as the debugging scripts you used earlier to implement the new model in Transformers. A template of those model tests, `BrandNewLlamaModelIntegrationTests`, was added by Cookiecutter and should be filled out. To ensure it passes, run the following command.\n+\n+<hfoptions id=\"integration-test\">\n+<hfoption id=\"macOS\">\n+\n+```bash\n+RUN_SLOW=1 pytest -sv tests/models/brand_new_llama/test_modeling_brand_new_llama.py::BrandNewLlamaModelIntegrationTests\n+```\n \n-In case you are using Windows, you should replace `RUN_SLOW=1` with `SET RUN_SLOW=1`\n+</hfoption>\n+<hfoption id=\"Windows\">\n \n-</Tip>\n+```bash\n+SET RUN_SLOW=1 pytest -sv tests/models/brand_new_llama/test_modeling_brand_new_llama.py::BrandNewLlamaModelIntegrationTests\n+```\n \n-Second, all features that are special to *brand_new_bert* should be tested additionally in a separate test under\n-`BrandNewBertModelTester`/`BrandNewBertModelTest`. This part is often forgotten but is extremely useful in two\n-ways:\n+</hfoption>\n+</hfoptions>\n \n-- It helps to transfer the knowledge you have acquired during the model addition to the community by showing how the\n-  special features of *brand_new_bert* should work.\n-- Future contributors can quickly test changes to the model by running those special tests.\n+All features unique to BrandNewLlama should be tested in a separate test under `BrandNewLlamaModelTester/BrandNewLlamaModelTest`. This test is often overlooked, but it is extremely important because:\n \n+- it helps transfer knowledge you acquired during the process to the community by showing how the models novel features work\n+- future contributors can quickly test changes to the model by running these special tests\n \n-**9. Implement the tokenizer**\n+## Implement tokenizer\n \n-Next, we should add the tokenizer of *brand_new_bert*. Usually, the tokenizer is equivalent to or very similar to an\n-already existing tokenizer of ðŸ¤— Transformers.\n+> [!TIP]\n+> We recommend adding a fast tokenizer ([`PreTrainedTokenizerFast`]) to give users the best performance. Feel free to tag [@ArthurZucker](https://github.com/ArthurZucker) or [@itazap](https://github.com/itazap) in your PR for help on how to add [`PreTrainedTokenizerFast`].\n \n-It is very important to find/extract the original tokenizer file and to manage to load this file into the ðŸ¤—\n-Transformers' implementation of the tokenizer.\n+With the model out of the way, time to focus on the tokenizer. The tokenizer should be identical or very similar to an existing tokenizer in Transformers.\n \n-To ensure that the tokenizer works correctly, it is recommended to first create a script in the original repository\n-that inputs a string and returns the `input_ids`. It could look similar to this (in pseudo-code):\n+Find and load the original tokenizer file into your implementation. Create a script in the original repository that inputs a string and returns the `input_ids`. The pseudocode should look similar to the code below.\n \n-```python\n+```py\n input_str = \"This is a long example input string containing special characters .$?-, numbers 2872 234 12 and words.\"\n-model = BrandNewBertModel.load_pretrained_checkpoint(\"/path/to/checkpoint/\")\n+model = BrandNewLlamaModel.load_pretrained_checkpoint(\"/path/to/checkpoint/\")\n input_ids = model.tokenize(input_str)\n ```\n \n-You might have to take a deeper look again into the original repository to find the correct tokenizer function or you\n-might even have to do changes to your clone of the original repository to only output the `input_ids`. Having written\n-a functional tokenization script that uses the original repository, an analogous script for ðŸ¤— Transformers should be\n-created. It should look similar to this:\n+You may need to search the original repository to find the correct tokenizer function or modify the existing tokenizer in your clone of the original repository to only return the `input_ids`. The script for your tokenizer should look similar to the following.\n \n-```python\n-from transformers import BrandNewBertTokenizer\n+```py\n+from transformers import BrandNewLlamaTokenizer\n \n input_str = \"This is a long example input string containing special characters .$?-, numbers 2872 234 12 and words.\"\n-\n-tokenizer = BrandNewBertTokenizer.from_pretrained(\"/path/to/tokenizer/folder/\")\n-\n+tokenizer = BrandNewLlamaTokenizer.from_pretrained(\"/path/to/tokenizer/folder/\")\n input_ids = tokenizer(input_str).input_ids\n ```\n \n-When both `input_ids` yield the same values, as a final step a tokenizer test file should also be added.\n+When both implementations have the same `input_ids`, add a tokenizer test file. This file is analogous to the modeling test files. The tokenizer test files should contain a couple of hardcoded integration tests.\n+\n+## Integration tests\n \n-Analogous to the modeling test files of *brand_new_bert*, the tokenization test files of *brand_new_bert* should\n-contain a couple of hard-coded integration tests.\n+Now that you have a model and tokenizer, add end-to-end integration tests for the model and tokenizer to `tests/models/brand_new_llama/test_modeling_brand_new_llama.py`.\n \n-**10. Run End-to-end integration tests**\n+The test should provide a meaningful text-to-text example to show the model works as expected. For example, you can include a source-to-target translation pair, an article-to-summary pair, or a question-to-answer pair.\n \n-Having added the tokenizer, you should also add a couple of end-to-end integration tests using both the model and the\n-tokenizer to `tests/models/brand_new_bert/test_modeling_brand_new_bert.py` in ðŸ¤— Transformers.\n-Such a test should show on a meaningful\n-text-to-text sample that the ðŸ¤— Transformers implementation works as expected. A meaningful text-to-text sample can\n-include *e.g.* a source-to-target-translation pair, an article-to-summary pair, a question-to-answer pair, etcâ€¦ If none\n-of the ported checkpoints has been fine-tuned on a downstream task it is enough to simply rely on the model tests. In a\n-final step to ensure that the model is fully functional, it is advised that you also run all tests on GPU. It can\n-happen that you forgot to add some `.to(self.device)` statements to internal tensors of the model, which in such a\n-test would show in an error. In case you have no access to a GPU, the Hugging Face team can take care of running those\n-tests for you.\n+If the checkpoint hasn't been fine-tuned on a downstream task, then the model tests are sufficient.\n \n-**11. Add Docstring**\n+Finally, try to make sure your tests can run on a GPU by adding `.to(self.device)` statements to the models internal tensors. If you don't have access to a GPU, we can take care of that for you.\n \n-Now, all the necessary functionality for *brand_new_bert* is added - you're almost done! The only thing left to add is\n-a nice docstring and a doc page. The Cookiecutter should have added a template file called\n-`docs/source/model_doc/brand_new_bert.md` that you should fill out. Users of your model will usually first look at\n-this page before using your model. Hence, the documentation must be understandable and concise. It is very useful for\n-the community to add some *Tips* to show how the model should be used. Don't hesitate to ping the Hugging Face team\n-regarding the docstrings.\n+## Add documentation\n \n-Next, make sure that the docstring added to `src/transformers/models/brand_new_bert/modeling_brand_new_bert.py` is\n-correct and included all necessary inputs and outputs. We have a detailed guide about writing documentation and our docstring format [here](writing-documentation). It is always good to remind oneself that documentation should\n-be treated at least as carefully as the code in ðŸ¤— Transformers since the documentation is usually the first contact\n-point of the community with the model.\n+Your model is only useful if users know how to use it. This is why it's important to add documentation and docstrings. Cookiecutter added a template file, `docs/source/model_doc/brand_new_llama.md`, that you can fill out with information about your model.\n \n-**Code refactor**\n+This is generally a user's first interaction with a model, so the documentation should be clear and concise. It is often very useful to add examples of how the model should be used.\n \n-Great, now you have added all the necessary code for *brand_new_bert*. At this point, you should correct some potential\n-incorrect code style by running:\n+Make sure docstrings are added to `src/transformers/models/brand_new_llama/modeling_brand_new_llama.py` and includes all necessary inputs and outputs. Review our [guide](https://github.com/huggingface/transformers/tree/main/docs#writing-documentation---specification) for writing documentation and docstrings.\n+\n+## Refactor\n+\n+Time to tidy things up and make sure the code style is consistent with the rest of the library. Run the following command to automatically fix incorrect styles.\n \n ```bash\n make style\n ```\n \n-and verify that your coding style passes the quality check:\n+To verify the code style passes quality checks, run the command below.\n \n ```bash\n make quality\n ```\n \n-There are a couple of other very strict design tests in ðŸ¤— Transformers that might still be failing, which shows up in\n-the tests of your pull request. This is often because of some missing information in the docstring or some incorrect\n-naming. The Hugging Face team will surely help you if you're stuck here.\n-\n-Lastly, it is always a good idea to refactor one's code after having ensured that the code works correctly. With all\n-tests passing, now it's a good time to go over the added code again and do some refactoring.\n-\n-You have now finished the coding part, congratulation! ðŸŽ‰ You are Awesome! ðŸ˜Ž\n-\n-**12. Upload the models to the model hub**\n-\n-In this final part, you should convert and upload all checkpoints to the model hub and add a model card for each\n-uploaded model checkpoint. You can get familiar with the hub functionalities by reading our [Model sharing and uploading Page](model_sharing). You should work alongside the Hugging Face team here to decide on a fitting name for each\n-checkpoint and to get the required access rights to be able to upload the model under the author's organization of\n-*brand_new_bert*. The `push_to_hub` method, present in all models in `transformers`, is a quick and efficient way to push your checkpoint to the hub. A little snippet is pasted below:\n-\n-```python\n-brand_new_bert.push_to_hub(\"brand_new_bert\")\n-# Uncomment the following line to push to an organization.\n-# brand_new_bert.push_to_hub(\"<organization>/brand_new_bert\")\n-```\n-\n-It is worth spending some time to create fitting model cards for each checkpoint. The model cards should highlight the\n-specific characteristics of this particular checkpoint, *e.g.* On which dataset was the checkpoint\n-pretrained/fine-tuned on? On what down-stream task should the model be used? And also include some code on how to\n-correctly use the model.\n+There may be other failing tests or checks (missing docstring or incorrect naming) on your pull request due to Transformers strict design tests. We can help you with these issues if you're stuck.\n \n-**13. (Optional) Add notebook**\n+After ensuring the code runs correctly, you may want to refactor it to make it more readable or cleaner.\n \n-It is very helpful to add a notebook that showcases in-detail how *brand_new_bert* can be used for inference and/or\n-fine-tuned on a downstream task. This is not mandatory to merge your PR, but very useful for the community.\n+## Upload to the Hub\n \n-**14. Submit your finished PR**\n+Convert and upload all checkpoints to the [Hub](https://hf.co/models). Add a model card to provide more transparency and context about the model. The model card should highlight specific characteristics of a checkpoint, how the model was trained, and code examples of how to use it.\n \n-You're done programming now and can move to the last step, which is getting your PR merged into main. Usually, the\n-Hugging Face team should have helped you already at this point, but it is worth taking some time to give your finished\n-PR a nice description and eventually add comments to your code, if you want to point out certain design choices to your\n-reviewer.\n+> [!TIP]\n+> In many cases, adding an interactive notebook users can run is a great way to showcase how to use the model for inference or fine-tune it on a downstream task. While not required, including a notebook can drive greater adoption of your model.\n \n-### Share your work!!\n+You should also consult with the Transformers team to decide on an appropriate name for the model, and getting the required access rights to upload the model.\n \n-Now, it's time to get some credit from the community for your work! Having completed a model addition is a major\n-contribution to Transformers and the whole NLP community. Your code and the ported pre-trained models will certainly be\n-used by hundreds and possibly even thousands of developers and researchers. You should be proud of your work and share\n-your achievements with the community.\n+Use the [`~PreTrainedModel.push_to_hub`] method to upload the model.\n \n-**You have made another model that is super easy to access for everyone in the community! ðŸ¤¯**\n-\n-## Model additions and their timeline: when is a model added to transformers?\n-\n-We aim for `transformers` to have support for new model architectures and checkpoints as early as possible:\n-availability can range from day-0 (and hour-0) releases for some models, to a few days/weeks for others.\n-\n-The availability of this is usually up to the model contributors, as well as how excited the community is for the\n-architecture.\n-\n-We can split the model architecture possibilities in four sections:\n-- Day-0 integration\n-- Same-week integration\n-- Post-release integration\n-- Hub-first release\n-\n-Let's dive into each of these and see how we (the transformers team) can help you contribute your architecture and get\n-your architecture to be very easily used by all members of the community.\n-\n-### Day-0 integration\n-\n-For a day-0 integration to work, we'll usually want to work hand-in-hand with you directly. In order to keep your\n-architecture private until your checkpoints and release are ready, we'll work together in a private fork of\n-transformers.\n+```py\n+brand_new_bert.push_to_hub(\"brand_new_llama\")\n+```\n \n-If you plan on having a transformers-first release, this is a great option: we run CI ahead of time, ensure the\n-documentation is clear, and we aim to optimize your model as much as possible (providing quantization, optimizing it\n-with Flash-Attention/SDPA, optimizing the KV cache, etc). \n+Refer to the [Sharing](./model_sharing) guide for more information about uploading models to the Hub.\n \n-We can also lend you a hand in adding the model, reviewing it early, and help you make sure the `transformers` \n-API works as expected!\n+## Merge your model\n \n-If this is the path you wish to go with, we ask for you to reach out in advance, especially if the architecture is \n-particularly novel (at least a few days, but a few weeks will enable the absolute best integration). In order to reach\n-out, please contact transformers@huggingface.co ðŸ¤—.\n+You're finally ready to merge your pull request and officially add the model to Transformers! Make sure all the tests are passing and all comments and feedback have been addressed.\n \n-### Same-week integration\n+Congratulations on adding a new model to Transformers! ðŸ¥³\n \n-A same-week integration usually happens when model authors do not reach out; but we see significant community\n-requests.\n+This is a very significant contribution. Your work makes Transformers more accessible to developers and researchers around the world. You should be proud of your contribution and share your accomplishment with the community!\n \n-In order to specify you'd like for us to integrate a specific model, we'll redirect you to our\n-[issue tracker](https://github.com/huggingface/transformers/issues/new?assignees=&labels=New+model&projects=&template=new-model-addition.yml)\n-where you can request a specific model.\n+## Model addition timeline\n \n-The more activity on the issue, the faster/more likely we are to integrate the model!\n+There are four timelines for model additions depending on the model contributor and community demand for an architecture.\n \n-### Post-release integration\n+- **day-0 integration**: If you plan on having a Transformers-first release, this is a great option because we can ensure the documentation is clear and optimize your model as much as possible (quantization, FlashAttention, KV-cache, etc.). We can also help you add the model, provide early reviews and make sure it works as expected.\n \n-A post-release integration usually happens when there has not been sufficient activity/requests to warrant a same-week\n-integration, or that we lack the sufficient bandwidth to integrate it.\n+  Reach out to transformers@huggingface.co a few days (preferably weeks) in advance, especially if an architecture is particularly novel, to ensure model integration. We'll work together on a private fork of Transformers until your checkpoint and release is ready.\n \n-We very gladly welcome community contributions in those instances; more than half of the library was contributed\n-by contributors external to Hugging Face. If this is something that is interesting to you, we recommend that you look\n-at our [open issues tagged with \"New model\"](https://github.com/huggingface/transformers/issues?q=is%3Aopen+is%3Aissue+label%3A%22New+model%22).\n+- **same week integration**: Models with significant requests/demand are usually added the same week if the model author doesn't reach out.\n \n-We recommend you try your hand at a heavily requested model as this will multiply the impact of your contribution.\n-We'll be there to help you in case that's your first contribution ðŸ¤—.\n+  Use the [issue tracker](https://github.com/huggingface/transformers/issues/new?assignees=&labels=New+model&projects=&template=new-model-addition.yml) to request a specific model to add. The more activity on the issue, the faster and more likely we'll integrate it.\n \n-### Code-on-Hub release\n+- **post-release integration**: Models without popular requests/demand or if we don't have the bandwidth to integrate it are added post-release.\n \n-Finally, transformers has a \"remote-code\" possibility, in which contributions are not made within the toolkit, but on\n-the Hub. This can be particularly interesting for groups that are using `transformers` as a backbone for their project,\n-but don't have the bandwidth to contribute the model to transformers directly.\n+  This is a good opportunity if you're interested in contributing a model to Transformers. Take a look at open issues tagged with [\"New model\"](https://github.com/huggingface/transformers/issues?q=is%3Aopen+is%3Aissue+label%3A%22New+model%22). Feel free to give the most requested models a try first to multiply the impact of your contribution. We'll be there to help you each step of the way!\n \n-In case the model is very successful, then we'll very likely end up integrating it in `transformers` at the end - as this\n-provides better documentation, CI, maintenance, and optimizations - but this remains a great way to make your model\n-accessible day-0 with minimal friction.\n+- **Hub-first release**: Transformers [remote-code](./models#custom-models) feature allows Transformers-based projects to be shared directly on the Hub. This is a good option if you don't have the bandwidth to add a model directly to Transformers.\n \n-This guide is a great starting point for a Hub-first release: [Custom models](./custom_models)\n\\ No newline at end of file\n+  If a model ends up being very popular, then it's very likely that we'll integrate it in Transformers ourselves to enable better support (documentation, maintenance, optimization, etc.) for it. A Hub-first release is the most frictionless way to add a model.\n\\ No newline at end of file"
        },
        {
            "sha": "60ef43dab585032a01f6260dd2b748af732ee410",
            "filename": "docs/source/en/add_new_pipeline.md",
            "status": "modified",
            "additions": 77,
            "deletions": 118,
            "changes": 195,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fadd_new_pipeline.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fadd_new_pipeline.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fadd_new_pipeline.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -1,4 +1,4 @@\n-<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n+<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n \n Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n the License. You may obtain a copy of the License at\n@@ -13,92 +13,66 @@ rendered properly in your Markdown viewer.\n \n -->\n \n-# How to create a custom pipeline?\n+# Adding a new pipeline\n \n-In this guide, we will see how to create a custom pipeline and share it on the [Hub](https://hf.co/models) or add it to the\n-ðŸ¤— Transformers library.\n+Make [`Pipeline`] your own by subclassing it and implementing a few methods. Share the code with the community on the [Hub](https://hf.co) and register the pipeline with Transformers so that everyone can quickly and easily use it.\n \n-First and foremost, you need to decide the raw entries the pipeline will be able to take. It can be strings, raw bytes,\n-dictionaries or whatever seems to be the most likely desired input. Try to keep these inputs as pure Python as possible\n-as it makes compatibility easier (even through other languages via JSON). Those will be the `inputs` of the\n-pipeline (`preprocess`).\n+This guide will walk you through the process of adding a new pipeline to Transformers.\n \n-Then define the `outputs`. Same policy as the `inputs`. The simpler, the better. Those will be the outputs of\n-`postprocess` method.\n+## Design choices\n \n-Start by inheriting the base class `Pipeline` with the 4 methods needed to implement `preprocess`,\n-`_forward`, `postprocess`, and `_sanitize_parameters`.\n+At a minimum, you only need to provide [`Pipeline`] with an appropriate input for a task. This is also where you should begin when designing your pipeline.\n \n+Decide what input types [`Pipeline`] can accept. It can be strings, raw bytes, dictionaries, and so on. Try to keep the inputs in pure Python where possible because it's more compatible. Next, decide on the output [`Pipeline`] should return. Again, keeping the output in Python is the simplest and best option because it's easier to work with.\n \n-```python\n-from transformers import Pipeline\n+Keeping the inputs and outputs simple, and ideally JSON-serializable, makes it easier for users to run your [`Pipeline`] without needing to learn new object types. It's also common to support many different input types for even greater ease of use. For example, making an audio file acceptable from a filename, URL, or raw bytes gives the user more flexibility in how they provide the audio data.\n+\n+## Create a pipeline\n+\n+With an input and output decided, you can start implementing [`Pipeline`]. Your pipeline should inherit from the base [`Pipeline`] class and include 4 methods.\n \n+```py\n+from transformers import Pipeline\n \n class MyPipeline(Pipeline):\n     def _sanitize_parameters(self, **kwargs):\n-        preprocess_kwargs = {}\n-        if \"maybe_arg\" in kwargs:\n-            preprocess_kwargs[\"maybe_arg\"] = kwargs[\"maybe_arg\"]\n-        return preprocess_kwargs, {}, {}\n \n-    def preprocess(self, inputs, maybe_arg=2):\n-        model_input = Tensor(inputs[\"input_ids\"])\n-        return {\"model_input\": model_input}\n+    def preprocess(self, inputs, args=2):\n \n     def _forward(self, model_inputs):\n-        # model_inputs == {\"model_input\": model_input}\n-        outputs = self.model(**model_inputs)\n-        # Maybe {\"logits\": Tensor(...)}\n-        return outputs\n \n     def postprocess(self, model_outputs):\n-        best_class = model_outputs[\"logits\"].softmax(-1)\n-        return best_class\n ```\n \n-The structure of this breakdown is to support relatively seamless support for CPU/GPU, while supporting doing\n-pre/postprocessing on the CPU on different threads\n-\n-`preprocess` will take the originally defined inputs, and turn them into something feedable to the model. It might\n-contain more information and is usually a `Dict`.\n-\n-`_forward` is the implementation detail and is not meant to be called directly. `forward` is the preferred\n-called method as it contains safeguards to make sure everything is working on the expected device. If anything is\n-linked to a real model it belongs in the `_forward` method, anything else is in the preprocess/postprocess.\n-\n-`postprocess` methods will take the output of `_forward` and turn it into the final output that was decided\n-earlier.\n-\n-`_sanitize_parameters` exists to allow users to pass any parameters whenever they wish, be it at initialization\n-time `pipeline(...., maybe_arg=4)` or at call time `pipe = pipeline(...); output = pipe(...., maybe_arg=4)`.\n+1. `preprocess` takes the inputs and transforms them into the appropriate input format for the model.\n \n-The returns of `_sanitize_parameters` are the 3 dicts of kwargs that will be passed directly to `preprocess`,\n-`_forward`, and `postprocess`. Don't fill anything if the caller didn't call with any extra parameter. That\n-allows to keep the default arguments in the function definition which is always more \"natural\".\n-\n-A classic example would be a `top_k` argument in the post processing in classification tasks.\n+```py\n+def preprocess(self, inputs, maybe_arg=2):\n+    model_input = Tensor(inputs[\"input_ids\"])\n+    return {\"model_input\": model_input}\n+```\n \n-```python\n->>> pipe = pipeline(\"my-new-task\")\n->>> pipe(\"This is a test\")\n-[{\"label\": \"1-star\", \"score\": 0.8}, {\"label\": \"2-star\", \"score\": 0.1}, {\"label\": \"3-star\", \"score\": 0.05}\n-{\"label\": \"4-star\", \"score\": 0.025}, {\"label\": \"5-star\", \"score\": 0.025}]\n+2. `_forward` shouldn't be called directly. `forward` is the preferred method because it includes safeguards to make sure everything works correctly on the expected device. Anything linked to the model belongs in `_forward` and everything else belongs in either `preprocess` or `postprocess`.\n \n->>> pipe(\"This is a test\", top_k=2)\n-[{\"label\": \"1-star\", \"score\": 0.8}, {\"label\": \"2-star\", \"score\": 0.1}]\n+```py\n+def _forward(self, model_inputs):\n+    outputs = self.model(**model_inputs)\n+    return outputs\n ```\n \n-In order to achieve that, we'll update our `postprocess` method with a default parameter to `5`. and edit\n-`_sanitize_parameters` to allow this new parameter.\n-\n+3. `postprocess` generates the final output from the models output in `_forward`.\n \n-```python\n+```py\n def postprocess(self, model_outputs, top_k=5):\n     best_class = model_outputs[\"logits\"].softmax(-1)\n-    # Add logic to handle top_k\n     return best_class\n+```\n+\n+4. `_sanitize_parameters` lets users pass additional parameters to [`Pipeline`]. This could be during initialization or when [`Pipeline`] is called. `_sanitize_parameters` returns 3 dicts of additional keyword arguments that are passed directly to `preprocess`, `_forward`, and `postprocess`. Don't add anything if a user didn't call the pipeline with extra parameters. This keeps the default arguments in the function definition which is always more natural.\n \n+For example, add a `top_k` parameter in `postprocess` to return the top 5 most likely classes. Then in `_sanitize_parameters`, check if the user passed in `top_k` and add it to `postprocess_kwargs`.\n \n+```py\n def _sanitize_parameters(self, **kwargs):\n     preprocess_kwargs = {}\n     if \"maybe_arg\" in kwargs:\n@@ -110,55 +84,61 @@ def _sanitize_parameters(self, **kwargs):\n     return preprocess_kwargs, {}, postprocess_kwargs\n ```\n \n-Try to keep the inputs/outputs very simple and ideally JSON-serializable as it makes the pipeline usage very easy\n-without requiring users to understand new kinds of objects. It's also relatively common to support many different types\n-of arguments for ease of use (audio files, which can be filenames, URLs or pure bytes)\n+Now the pipeline can return the top most likely labels if a user chooses to.\n \n+```py\n+from transformers import pipeline\n \n+pipeline = pipeline(\"my-task\")\n+# returns 3 most likely labels\n+pipeline(\"This is the best meal I've ever had\", top_k=3)\n+# returns 5 most likely labels by default\n+pipeline(\"This is the best meal I've ever had\")\n+```\n+\n+## Register a pipeline\n \n-## Adding it to the list of supported tasks\n+Register the new task your pipeline supports in the `PIPELINE_REGISTRY`. The registry defines:\n \n-To register your `new-task` to the list of supported tasks, you have to add it to the `PIPELINE_REGISTRY`:\n+- the machine learning framework the pipeline supports with either `pt_model` or `tf_model` (add both to ensure it works with either frameworks)\n+- a default model which should come from a specific revision (branch, or commit hash) where the model works as expected with `default`\n+- the expected input with `type`\n \n-```python\n+```py\n from transformers.pipelines import PIPELINE_REGISTRY\n+from transformers import AutoModelForSequenceClassification, TFAutoModelForSequenceClassification\n \n PIPELINE_REGISTRY.register_pipeline(\n     \"new-task\",\n     pipeline_class=MyPipeline,\n     pt_model=AutoModelForSequenceClassification,\n+    tf_model=TFAutoModelForSequenceClassification,\n+    default={\"pt\": (\"user/awesome-model\", \"branch-name\")},\n+    type=\"text\",\n )\n ```\n \n-You can specify a default model if you want, in which case it should come with a specific revision (which can be the name of a branch or a commit hash, here we took `\"abcdef\"`) as well as the type:\n+## Share your pipeline\n \n-```python\n-PIPELINE_REGISTRY.register_pipeline(\n-    \"new-task\",\n-    pipeline_class=MyPipeline,\n-    pt_model=AutoModelForSequenceClassification,\n-    default={\"pt\": (\"user/awesome_model\", \"abcdef\")},\n-    type=\"text\",  # current support type: text, audio, image, multimodal\n-)\n-```\n+Share your pipeline with the community on the [Hub](https://hf.co) or you can add it directly to Transformers.\n \n-## Share your pipeline on the Hub\n+It's faster to upload your pipeline code to the Hub because it doesn't require a review from the Transformers team. Adding the pipeline to Transformers may be slower because it requires a review and you need to add tests to ensure your [`Pipeline`] works.\n \n-To share your custom pipeline on the Hub, you just have to save the custom code of your `Pipeline` subclass in a\n-python file. For instance, let's say we want to use a custom pipeline for sentence pair classification like this:\n+### Upload to the Hub\n+\n+Add your pipeline code to the Hub in a Python file.\n+\n+For example, a custom pipeline for sentence pair classification might look like the following code below. The implementation works for PyTorch and TensorFlow models.\n \n ```py\n import numpy as np\n-\n from transformers import Pipeline\n \n-\n def softmax(outputs):\n     maxes = np.max(outputs, axis=-1, keepdims=True)\n     shifted_exp = np.exp(outputs - maxes)\n     return shifted_exp / shifted_exp.sum(axis=-1, keepdims=True)\n \n-\n class PairClassificationPipeline(Pipeline):\n     def _sanitize_parameters(self, **kwargs):\n         preprocess_kwargs = {}\n@@ -183,8 +163,7 @@ class PairClassificationPipeline(Pipeline):\n         return {\"label\": label, \"score\": score, \"logits\": logits}\n ```\n \n-The implementation is framework agnostic, and will work for PyTorch and TensorFlow models. If we have saved this in\n-a file named `pair_classification.py`, we can then import it and register it like this.\n+Save the code in a file named `pair_classification.py`, and import and register it as shown below.\n \n ```py\n from pair_classification import PairClassificationPipeline\n@@ -215,56 +194,36 @@ The [register_pipeline](https://github.com/huggingface/transformers/blob/9feae5f\n   },\n ```\n \n-Once this is done, we can use it with a pretrained model. For instance `sgugger/finetuned-bert-mrpc` has been\n-fine-tuned on the MRPC dataset, which classifies pairs of sentences as paraphrases or not.\n+Call [`~Pipeline.push_to_hub`] to push the pipeline to the Hub. The Python file containing the code is copied to the Hub, and the pipelines model and tokenizer are also saved and pushed to the Hub. Your pipeline should now be available on the Hub under your namespace.\n \n ```py\n from transformers import pipeline\n \n-classifier = pipeline(\"pair-classification\", model=\"sgugger/finetuned-bert-mrpc\")\n+pipeline = pipeline(task=\"pair-classification\", model=\"sgugger/finetuned-bert-mrpc\")\n+pipeline.push_to_hub(\"pair-classification-pipeline\")\n ```\n \n-Then we can share it on the Hub by using the `push_to_hub` method:\n-\n-```py\n-classifier.push_to_hub(\"test-dynamic-pipeline\")\n-```\n-\n-This will copy the file where you defined `PairClassificationPipeline` inside the folder `\"test-dynamic-pipeline\"`,\n-along with saving the model and tokenizer of the pipeline, before pushing everything into the repository\n-`{your_username}/test-dynamic-pipeline`. After that, anyone can use it as long as they provide the option\n-`trust_remote_code=True`:\n+To use the pipeline, add `trust_remote_code=True` when loading the pipeline.\n \n ```py\n from transformers import pipeline\n \n-classifier = pipeline(model=\"{your_username}/test-dynamic-pipeline\", trust_remote_code=True)\n+pipeline = pipeline(task=\"pair-classification\", trust_remote_code=True)\n ```\n \n-## Add the pipeline to ðŸ¤— Transformers\n+### Add to Transformers\n+\n+Adding a custom pipeline to Transformers requires adding tests to make sure everything works as expected, and requesting a review from the Transformers team.\n \n-If you want to contribute your pipeline to ðŸ¤— Transformers, you will need to add a new module in the `pipelines` submodule\n-with the code of your pipeline, then add it to the list of tasks defined in `pipelines/__init__.py`.\n+Add your pipeline code as a new module to the [pipelines](https://github.com/huggingface/transformers/tree/main/src/transformers/pipelines) submodule, and add it to the list of tasks defined in [pipelines/__init__.py](https://github.com/huggingface/transformers/blob/main/src/transformers/pipelines/__init__.py).\n \n-Then you will need to add tests. Create a new file `tests/test_pipelines_MY_PIPELINE.py` with examples of the other tests.\n+Next, add a new test for the pipeline in [transformers/tests/pipelines](https://github.com/huggingface/transformers/tree/main/tests/pipelines). You can look at the other tests for examples of how to test your pipeline.\n \n-The `run_pipeline_test` function will be very generic and run on small random models on every possible\n-architecture as defined by `model_mapping` and `tf_model_mapping`.\n+The [run_pipeline_test](https://github.com/huggingface/transformers/blob/db70426854fe7850f2c5834d633aff637f14772e/tests/pipelines/test_pipelines_text_classification.py#L186) function should be very generic and run on the models defined in [model_mapping](https://github.com/huggingface/transformers/blob/db70426854fe7850f2c5834d633aff637f14772e/tests/pipelines/test_pipelines_text_classification.py#L48) and [tf_model_mapping](https://github.com/huggingface/transformers/blob/db70426854fe7850f2c5834d633aff637f14772e/tests/pipelines/test_pipelines_text_classification.py#L49). This is important for testing future compatibility with new models.\n \n-This is very important to test future compatibility, meaning if someone adds a new model for\n-`XXXForQuestionAnswering` then the pipeline test will attempt to run on it. Because the models are random it's\n-impossible to check for actual values, that's why there is a helper `ANY` that will simply attempt to match the\n-output of the pipeline TYPE.\n+You'll also notice `ANY` is used throughout the [run_pipeline_test](https://github.com/huggingface/transformers/blob/db70426854fe7850f2c5834d633aff637f14772e/tests/pipelines/test_pipelines_text_classification.py#L186) function. The models are random, so you can't check the actual values. Using `ANY` allows the test to match the output of the pipeline type instead.\n \n-You also *need* to implement 2 (ideally 4) tests.\n+Finally, you should also implement the following 4 tests.\n \n-- `test_small_model_pt` : Define 1 small model for this pipeline (doesn't matter if the results don't make sense)\n-  and test the pipeline outputs. The results should be the same as `test_small_model_tf`.\n-- `test_small_model_tf` : Define 1 small model for this pipeline (doesn't matter if the results don't make sense)\n-  and test the pipeline outputs. The results should be the same as `test_small_model_pt`.\n-- `test_large_model_pt` (`optional`): Tests the pipeline on a real pipeline where the results are supposed to\n-  make sense. These tests are slow and should be marked as such. Here the goal is to showcase the pipeline and to make\n-  sure there is no drift in future releases.\n-- `test_large_model_tf` (`optional`): Tests the pipeline on a real pipeline where the results are supposed to\n-  make sense. These tests are slow and should be marked as such. Here the goal is to showcase the pipeline and to make\n-  sure there is no drift in future releases.\n+1. [test_small_model_pt](https://github.com/huggingface/transformers/blob/db70426854fe7850f2c5834d633aff637f14772e/tests/pipelines/test_pipelines_text_classification.py#L59) and [test_small_model_tf](https://github.com/huggingface/transformers/blob/db70426854fe7850f2c5834d633aff637f14772e/tests/pipelines/test_pipelines_text_classification.py#L150), use a small model for these pipelines to make sure they return the correct outputs. The results don't have to make sense. Each pipeline should return the same result.\n+1. [test_large_model_pt](https://github.com/huggingface/transformers/blob/db70426854fe7850f2c5834d633aff637f14772e/tests/pipelines/test_pipelines_zero_shot_image_classification.py#L187) nad [test_large_model_tf](https://github.com/huggingface/transformers/blob/db70426854fe7850f2c5834d633aff637f14772e/tests/pipelines/test_pipelines_zero_shot_image_classification.py#L220), use a realistic model for these pipelines to make sure they return meaningful results. These tests are slow and should be marked as slow."
        },
        {
            "sha": "216a943e8d3482fd9b1d1d2181c76ff8cea485c4",
            "filename": "docs/source/en/agents.md",
            "status": "modified",
            "additions": 280,
            "deletions": 2,
            "changes": 282,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fagents.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fagents.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fagents.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -13,7 +13,285 @@ specific language governing permissions and limitations under the License.\n rendered properly in your Markdown viewer.\n \n -->\n-# Agents and tools\n \n > [!WARNING]\n-> This subpackage will soon be deprecated, since it has ben spun off into [smolagents](https://huggingface.co/docs/smolagents/index). Smolagents has extended functionality, and a similar API.\n\\ No newline at end of file\n+> Agents and tools are being spun out into the standalone [smolagents](https://huggingface.co/docs/smolagents/index) library. These docs will be deprecated in the future!\n+\n+# Agents\n+\n+[[open-in-colab]]\n+\n+An agent is a system where a large language model (LLM) can execute more complex tasks through *planning* and using *tools*.\n+\n+- Planning helps a LLM reason its way through a task by breaking it down into smaller subtasks. For example, [`CodeAgent`] plans a series of actions to take and then generates Python code to execute all the actions at once.\n+\n+    Another planning method is by self-reflection and refinement of its previous actions to improve its performance. The [`ReactJsonAgent`] is an example of this type of planning, and it's based on the [ReAct](https://hf.co/papers/2210.03629) framework. This agent plans and executes actions one at a time based on the feedback it receives from each action.\n+\n+- Tools give a LLM access to external functions or APIs that it can use to help it complete a task. For example, [gradio-tools](https://github.com/freddyaboulton/gradio-tools) gives a LLM access to any of the [Gradio](https://www.gradio.app/) apps available on Hugging Face [Spaces](https://hf.co/spaces). These apps can be used for a wide range of tasks such as image generation, video generation, audio transcription, and more.\n+\n+To use agents in Transformers, make sure you have the extra `agents` dependencies installed.\n+\n+```bash\n+!pip install transformers[agents]\n+```\n+\n+Create an agent instance (refer to the [Agents](./main_classes/agent#agents) API for supported agents in Transformers) and a list of tools available for it to use, then [`~ReactAgent.run`] the agent on your task. The example below demonstrates how a ReAct agent reasons through a task.\n+\n+```py\n+from transformers import ReactCodeAgent\n+\n+agent = ReactCodeAgent(tools=[])\n+agent.run(\n+    \"How many more blocks (also denoted as layers) in BERT base encoder than the encoder from the architecture proposed in Attention is All You Need?\",\n+)\n+```\n+\n+```bash\n+======== New task ========\n+How many more blocks (also denoted as layers) in BERT base encoder than the encoder from the architecture proposed in Attention is All You Need?\n+==== Agent is executing the code below:\n+bert_layers = 12  # BERT base encoder has 12 layers\n+attention_layers = 6  # Encoder in Attention is All You Need has 6 layers\n+layer_diff = bert_layers - attention_layers\n+print(\"The difference in layers between BERT base encoder and Attention is All You Need is\", layer_diff)\n+====\n+Print outputs:\n+The difference in layers between BERT base encoder and Attention is All You Need is 6\n+\n+==== Agent is executing the code below:\n+final_answer(\"BERT base encoder has {} more layers than the encoder from Attention is All You Need.\".format(layer_diff))\n+====\n+Print outputs:\n+\n+>>> Final answer:\n+BERT base encoder has 6 more layers than the encoder from Attention is All You Need.\n+```\n+\n+This guide will walk you through in more detail how to initialize an agent.\n+\n+## LLM\n+\n+An agent uses a LLM to plan and execute a task; it is the engine that powers the agent. To choose and build your own LLM engine, you need a method that:\n+\n+1. the input uses the [chat template](./chat_templating) format, `List[Dict[str, str]]`, and it returns a string\n+2. the LLM stops generating outputs when it encounters the sequences in `stop_sequences`\n+\n+```py\n+def llm_engine(messages, stop_sequences=[\"Task\"]) -> str:\n+    response = client.chat_completion(messages, stop=stop_sequences, max_tokens=1000)\n+    answer = response.choices[0].message.content\n+    return answer\n+```\n+\n+Next, initialize an engine to load a model. To run an agent locally, create a [`TransformersEngine`] to load a preinitialized [`Pipeline`].\n+\n+However, you could also leverage Hugging Face's powerful inference infrastructure, [Inference API](https://hf.co/docs/api-inference/index) or [Inference Endpoints](https://hf.co/docs/inference-endpoints/index), to run your model. This is useful for loading larger models that are typically required for agentic behavior. In this case, load the [`HfApiEngine`] to run the agent.\n+\n+The agent requires a list of tools it can use to complete a task. If you aren't using any additional tools, pass an empty list. The default tools provided by Transformers are loaded automatically, but you can optionally set `add_base_tools=True` to explicitly enable them.\n+\n+<hfoptions id=\"engine\">\n+<hfoption id=\"TransformersEngine\">\n+\n+```py\n+from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, TransformersEngine, CodeAgent\n+\n+tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n+model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\").to(\"cuda\")\n+pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n+llm_engine = TransformersEngine(pipeline)\n+agent = CodeAgent(tools=[], llm_engine=llm_engine)\n+agent.run(\n+    \"What causes bread to rise?\",\n+)\n+```\n+\n+</hfoption>\n+<hfoption id=\"HfApiEngine\">\n+\n+```py\n+from transformers import CodeAgent, HfApiEngine\n+\n+llm_engine = HfApiEngine(model=\"meta-llama/Meta-Llama-3-70B-Instruct\")\n+agent = CodeAgent(tools=[], llm_engine=llm_engine)\n+agent.run(\n+    \"Could you translate this sentence from French, say it out loud and return the audio.\",\n+    sentence=\"OÃ¹ est la boulangerie la plus proche?\",\n+)\n+```\n+\n+</hfoption>\n+</hfoptions>\n+\n+The agent supports [constrained generation](https://hf.co/docs/text-generation-inference/conceptual/guidance) for generating outputs according to a specific structure with the `grammar` parameter. The `grammar` parameter should be specified in the `llm_engine` method or you can set it when initializing an agent.\n+\n+Lastly, an agent accepts additional inputs such as text and audio. In the [`HfApiEngine`] example above, the agent accepted a sentence to translate. But you could also pass a path to a local or remote file for the agent to access. The example below demonstrates how to pass a path to an audio file.\n+\n+```py\n+from transformers import ReactCodeAgent\n+\n+agent = ReactCodeAgent(tools=[], llm_engine=llm_engine)\n+agent.run(\"Why doesn't he know many people in New York?\", audio=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/recording.mp3\")\n+```\n+\n+## System prompt\n+\n+A system prompt describes how an agent should behave, a description of the available tools, and the expected output format.\n+\n+Tools are defined by the `<<tool_descriptions>>` token which is dynamically replaced during runtime with the actual tool. The tool description is derived from the tool name, description, inputs, output type, and a Jinja2 template. Refer to the [Tools](./tools) guide for more information about how to describe tools.\n+\n+The example below is the system prompt for [`ReactCodeAgent`].\n+\n+```py\n+You will be given a task to solve as best you can.\n+You have access to the following tools:\n+<<tool_descriptions>>\n+\n+To solve the task, you must plan forward to proceed in a series of steps, in a cycle of 'Thought:', 'Code:', and 'Observation:' sequences.\n+\n+At each step, in the 'Thought:' sequence, you should first explain your reasoning towards solving the task, then the tools that you want to use.\n+Then in the 'Code:' sequence, you should write the code in simple Python. The code sequence must end with '/End code' sequence.\n+During each intermediate step, you can use 'print()' to save whatever important information you will then need.\n+These print outputs will then be available in the 'Observation:' field, for using this information as input for the next step.\n+\n+In the end you have to return a final answer using the `final_answer` tool.\n+\n+Here are a few examples using notional tools:\n+---\n+{examples}\n+\n+Above example were using notional tools that might not exist for you. You only have acces to those tools:\n+<<tool_names>>\n+You also can perform computations in the python code you generate.\n+\n+Always provide a 'Thought:' and a 'Code:\\n```py' sequence ending with '```<end_code>' sequence. You MUST provide at least the 'Code:' sequence to move forward.\n+\n+Remember to not perform too many operations in a single code block! You should split the task into intermediate code blocks.\n+Print results at the end of each step to save the intermediate results. Then use final_answer() to return the final result.\n+\n+Remember to make sure that variables you use are all defined.\n+\n+Now Begin!\n+```\n+\n+The system prompt can be tailored to the intended task. For example, you can add a better explanation of the output format or you can overwrite the system prompt template entirely with your own custom system prompt as shown below.\n+\n+> [!WARNING]\n+> If you're writing a custom system prompt, make sure to include `<<tool_descriptions>>` in the template so the agent is aware of the available tools.\n+\n+```py\n+from transformers import ReactJsonAgent\n+from transformers.agents import PythonInterpreterTool\n+\n+agent = ReactJsonAgent(tools=[PythonInterpreterTool()], system_prompt=\"{your_custom_prompt}\")\n+```\n+\n+## Code execution\n+\n+For safety, only the tools you provide (and the default Transformers tools) and the `print` function are executed. The interpreter doesn't allow importing modules that aren't on a safe list.\n+\n+To import modules that aren't on the list, add them as a list to the `additional_authorized_imports` parameter when initializing an agent.\n+\n+```py\n+from transformers import ReactCodeAgent\n+\n+agent = ReactCodeAgent(tools=[], additional_authorized_imports=['requests', 'bs4'])\n+agent.run(\"Could you get me the title of the page at url 'https://huggingface.co/blog'?\")\n+```\n+\n+Code execution stops if a tool isn't on the safe list, it isn't authorized, or if the code generated by the agent returns a Python error.\n+\n+> [!WARNING]\n+> A LLM can generate any arbitrary code that can be executed, so don't add any unsafe imports!\n+\n+## Multi-agent\n+\n+[Multi-agent](https://hf.co/papers/2308.08155) refers to multiple agents working together to solve a task. Performance is typically better because each agent is specialized for a particular subtask.\n+\n+Multi-agents are created through a [`ManagedAgent`] class, where a *manager agent* oversees how other agents work together. The manager agent requires an agent and their name and description. These are added to the manager agents system prompt which lets it know how to call and use them.\n+\n+The multi-agent example below creates a web search agent that is managed by another [`ReactCodeAgent`].\n+\n+```py\n+from transformers.agents import ReactCodeAgent, HfApiEngine, DuckDuckGoSearchTool, ManagedAgent\n+\n+llm_engine = HfApiEngine()\n+web_agent = ReactCodeAgent(tools=[DuckDuckGoSearchTool()], llm_engine=llm_engine)\n+managed_web_agent = ManagedAgent(\n+    agent=web_agent,\n+    name=\"web_search\",\n+    description=\"Runs web searches for you. Give it your query as an argument.\"\n+)\n+manager_agent = ReactCodeAgent(\n+    tools=[], llm_engine=llm_engine, managed_agents=[managed_web_agent]\n+)\n+manager_agent.run(\"Who is the CEO of Hugging Face?\")\n+```\n+\n+## Gradio integration\n+\n+[Gradio](https://www.gradio.app/) is a library for quickly creating and sharing machine learning apps. The [gradio.Chatbot](https://www.gradio.app/docs/gradio/chatbot) supports chatting with a Transformers agent with the [`stream_to_gradio`] function.\n+\n+Load a tool and LLM with an agent, and then create a Gradio app. The key is to use [`stream_to_gradio`] to stream the agents messages and display how it's reasoning through a task.\n+\n+```py\n+import gradio as gr\n+from transformers import (\n+    load_tool,\n+    ReactCodeAgent,\n+    HfApiEngine,\n+    stream_to_gradio,\n+)\n+\n+# Import tool from Hub\n+image_generation_tool = load_tool(\"m-ric/text-to-image\")\n+llm_engine = HfApiEngine(\"meta-llama/Meta-Llama-3-70B-Instruct\")\n+\n+# Initialize the agent with the image generation tool\n+agent = ReactCodeAgent(tools=[image_generation_tool], llm_engine=llm_engine)\n+\n+def interact_with_agent(task):\n+    messages = []\n+    messages.append(gr.ChatMessage(role=\"user\", content=task))\n+    yield messages\n+    for msg in stream_to_gradio(agent, task):\n+        messages.append(msg)\n+        yield messages + [\n+            gr.ChatMessage(role=\"assistant\", content=\"â³ Task not finished yet!\")\n+        ]\n+    yield messages\n+\n+with gr.Blocks() as demo:\n+    text_input = gr.Textbox(lines=1, label=\"Chat Message\", value=\"Make me a picture of the Statue of Liberty.\")\n+    submit = gr.Button(\"Run illustrator agent!\")\n+    chatbot = gr.Chatbot(\n+        label=\"Agent\",\n+        type=\"messages\",\n+        avatar_images=(\n+            None,\n+            \"https://em-content.zobj.net/source/twitter/53/robot-face_1f916.png\",\n+        ),\n+    )\n+    submit.click(interact_with_agent, [text_input], [chatbot])\n+\n+if __name__ == \"__main__\":\n+    demo.launch()\n+```\n+\n+## Troubleshoot\n+\n+For a better idea of what is happening when you call an agent, it is always a good idea to check the system prompt template first.\n+\n+```py\n+print(agent.system_prompt_template)\n+```\n+\n+If the agent is behaving unexpectedly, remember to explain the task you want to perform as clearly as possible. Every [`~Agent.run`] is different and minor variations in your system prompt may yield completely different results.\n+\n+To find out what happened after a run, check the following agent attributes.\n+\n+- `agent.logs` stores the finegrained agent logs. At every step of the agents run, everything is stored in a dictionary and appended to `agent.logs`.\n+- `agent.write_inner_memory_from_logs` only stores a high-level overview of the agents run. For example, at each step, it stores the LLM output as a message and the tool call output as a separate message. Not every detail from a step is transcripted by `write_inner_memory_from_logs`.\n+\n+## Resources\n+\n+Learn more about ReAct agents in the [Open-source LLMs as LangChain Agents](https://hf.co/blog/open-source-llms-as-agents) blog post."
        },
        {
            "sha": "33f48b2b043fec9e6ea13bfa43e1fadb669558bd",
            "filename": "docs/source/en/autoclass_tutorial.md",
            "status": "removed",
            "additions": 0,
            "deletions": 189,
            "changes": 189,
            "blob_url": "https://github.com/huggingface/transformers/blob/6aa9888463ece40e29ac9065127f578a24f50958/docs%2Fsource%2Fen%2Fautoclass_tutorial.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/6aa9888463ece40e29ac9065127f578a24f50958/docs%2Fsource%2Fen%2Fautoclass_tutorial.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fautoclass_tutorial.md?ref=6aa9888463ece40e29ac9065127f578a24f50958",
            "patch": "@@ -1,189 +0,0 @@\n-<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# Load pretrained instances with an AutoClass\n-\n-With so many different Transformer architectures, it can be challenging to create one for your checkpoint. As a part of ðŸ¤— Transformers core philosophy to make the library easy, simple and flexible to use, an `AutoClass` automatically infers and loads the correct architecture from a given checkpoint. The `from_pretrained()` method lets you quickly load a pretrained model for any architecture so you don't have to devote time and resources to train a model from scratch. Producing this type of checkpoint-agnostic code means if your code works for one checkpoint, it will work with another checkpoint - as long as it was trained for a similar task - even if the architecture is different.\n-\n-<Tip>\n-\n-Remember, architecture refers to the skeleton of the model and checkpoints are the weights for a given architecture. For example, [BERT](https://huggingface.co/google-bert/bert-base-uncased) is an architecture, while `google-bert/bert-base-uncased` is a checkpoint. Model is a general term that can mean either architecture or checkpoint.\n-\n-</Tip>\n-\n-In this tutorial, learn to:\n-\n-* Load a pretrained tokenizer.\n-* Load a pretrained image processor\n-* Load a pretrained feature extractor.\n-* Load a pretrained processor.\n-* Load a pretrained model.\n-* Load a model as a backbone.\n-\n-## AutoTokenizer\n-\n-Nearly every NLP task begins with a tokenizer. A tokenizer converts your input into a format that can be processed by the model.\n-\n-Load a tokenizer with [`AutoTokenizer.from_pretrained`]:\n-\n-```py\n->>> from transformers import AutoTokenizer\n-\n->>> tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n-```\n-\n-Then tokenize your input as shown below:\n-\n-```py\n->>> sequence = \"In a hole in the ground there lived a hobbit.\"\n->>> print(tokenizer(sequence))\n-{'input_ids': [101, 1999, 1037, 4920, 1999, 1996, 2598, 2045, 2973, 1037, 7570, 10322, 4183, 1012, 102], \n- 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \n- 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n-```\n-\n-## AutoImageProcessor\n-\n-For vision tasks, an image processor processes the image into the correct input format.\n-\n-```py\n->>> from transformers import AutoImageProcessor\n-\n->>> image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n-```\n-\n-## AutoBackbone\n-\n-<div style=\"text-align: center\">\n-    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/Swin%20Stages.png\">\n-    <figcaption class=\"mt-2 text-center text-sm text-gray-500\">A Swin backbone with multiple stages for outputting a feature map.</figcaption>\n-</div>\n-\n-The [`AutoBackbone`] lets you use pretrained models as backbones to get feature maps from different stages of the backbone. You should specify one of the following parameters in [`~PretrainedConfig.from_pretrained`]:\n-\n-* `out_indices` is the index of the layer you'd like to get the feature map from\n-* `out_features` is the name of the layer you'd like to get the feature map from\n-\n-These parameters can be used interchangeably, but if you use both, make sure they're aligned with each other! If you don't pass any of these parameters, the backbone returns the feature map from the last layer.\n-\n-<div style=\"text-align: center\">\n-    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/Swin%20Stage%201.png\">\n-    <figcaption class=\"mt-2 text-center text-sm text-gray-500\">A feature map from the first stage of the backbone. The patch partition refers to the model stem.</figcaption>\n-</div>\n-\n-For example, in the above diagram, to return the feature map from the first stage of the Swin backbone, you can set `out_indices=(1,)`:\n-\n-```py\n->>> from transformers import AutoImageProcessor, AutoBackbone\n->>> import torch\n->>> from PIL import Image\n->>> import requests\n->>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n->>> image = Image.open(requests.get(url, stream=True).raw)\n->>> processor = AutoImageProcessor.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\")\n->>> model = AutoBackbone.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\", out_indices=(1,))\n-\n->>> inputs = processor(image, return_tensors=\"pt\")\n->>> outputs = model(**inputs)\n->>> feature_maps = outputs.feature_maps\n-```\n-\n-Now you can access the `feature_maps` object from the first stage of the backbone:\n-\n-```py\n->>> list(feature_maps[0].shape)\n-[1, 96, 56, 56]\n-```\n-\n-## AutoFeatureExtractor\n-\n-For audio tasks, a feature extractor processes the audio signal into the correct input format.\n-\n-Load a feature extractor with [`AutoFeatureExtractor.from_pretrained`]:\n-\n-```py\n->>> from transformers import AutoFeatureExtractor\n-\n->>> feature_extractor = AutoFeatureExtractor.from_pretrained(\n-...     \"ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\"\n-... )\n-```\n-\n-## AutoProcessor\n-\n-Multimodal tasks require a processor that combines two types of preprocessing tools. For example, the [LayoutLMV2](model_doc/layoutlmv2) model requires an image processor to handle images and a tokenizer to handle text; a processor combines both of them.\n-\n-Load a processor with [`AutoProcessor.from_pretrained`]:\n-\n-```py\n->>> from transformers import AutoProcessor\n-\n->>> processor = AutoProcessor.from_pretrained(\"microsoft/layoutlmv2-base-uncased\")\n-```\n-\n-## AutoModel\n-\n-<frameworkcontent>\n-<pt>\n-The `AutoModelFor` classes let you load a pretrained model for a given task (see [here](model_doc/auto) for a complete list of available tasks). For example, load a model for sequence classification with [`AutoModelForSequenceClassification.from_pretrained`].\n-\n-> [!WARNING]\n-> By default, the weights are loaded in full precision (torch.float32) regardless of the actual data type the weights are stored in such as torch.float16. Set `torch_dtype=\"auto\"` to load the weights in the data type defined in a model's `config.json` file to automatically load the most memory-optimal data type.\n-\n-```py\n->>> from transformers import AutoModelForSequenceClassification\n-\n->>> model = AutoModelForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased\", torch_dtype=\"auto\")\n-```\n-\n-Easily reuse the same checkpoint to load an architecture for a different task:\n-\n-```py\n->>> from transformers import AutoModelForTokenClassification\n-\n->>> model = AutoModelForTokenClassification.from_pretrained(\"distilbert/distilbert-base-uncased\", torch_dtype=\"auto\")\n-```\n-\n-<Tip warning={true}>\n-\n-For PyTorch models, the `from_pretrained()` method uses `torch.load()` which internally uses `pickle` and is known to be insecure. In general, never load a model that could have come from an untrusted source, or that could have been tampered with. This security risk is partially mitigated for public models hosted on the Hugging Face Hub, which are [scanned for malware](https://huggingface.co/docs/hub/security-malware) at each commit. See the [Hub documentation](https://huggingface.co/docs/hub/security) for best practices like [signed commit verification](https://huggingface.co/docs/hub/security-gpg#signing-commits-with-gpg) with GPG.\n-\n-TensorFlow and Flax checkpoints are not affected, and can be loaded within PyTorch architectures using the `from_tf` and `from_flax` kwargs for the `from_pretrained` method to circumvent this issue.\n-\n-</Tip>\n-\n-Generally, we recommend using the `AutoTokenizer` class and the `AutoModelFor` class to load pretrained instances of models. This will ensure you load the correct architecture every time. In the next [tutorial](preprocessing), learn how to use your newly loaded tokenizer, image processor, feature extractor and processor to preprocess a dataset for fine-tuning.\n-</pt>\n-<tf>\n-Finally, the `TFAutoModelFor` classes let you load a pretrained model for a given task (see [here](model_doc/auto) for a complete list of available tasks). For example, load a model for sequence classification with [`TFAutoModelForSequenceClassification.from_pretrained`]:\n-\n-```py\n->>> from transformers import TFAutoModelForSequenceClassification\n-\n->>> model = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased\")\n-```\n-\n-Easily reuse the same checkpoint to load an architecture for a different task:\n-\n-```py\n->>> from transformers import TFAutoModelForTokenClassification\n-\n->>> model = TFAutoModelForTokenClassification.from_pretrained(\"distilbert/distilbert-base-uncased\")\n-```\n-\n-Generally, we recommend using the `AutoTokenizer` class and the `TFAutoModelFor` class to load pretrained instances of models. This will ensure you load the correct architecture every time. In the next [tutorial](preprocessing), learn how to use your newly loaded tokenizer, image processor, feature extractor and processor to preprocess a dataset for fine-tuning.\n-</tf>\n-</frameworkcontent>"
        },
        {
            "sha": "792b0b0d38f12f11c0f2e6519d90ce56fc704f8a",
            "filename": "docs/source/en/backbones.md",
            "status": "added",
            "additions": 155,
            "deletions": 0,
            "changes": 155,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fbackbones.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fbackbones.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fbackbones.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -0,0 +1,155 @@\n+<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# Backbones\n+\n+Higher-level computer visions tasks, such as object detection or image segmentation, use several models together to generate a prediction. A separate model is used for the *backbone*, neck, and head. The backbone extracts useful features from an input image into a feature map, the neck combines and processes the feature maps, and the head uses them to make a prediction.\n+\n+<div class=\"flex justify-center\">\n+    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/Backbone.png\"/>\n+</div>\n+\n+Load a backbone with [`~PretrainedConfig.from_pretrained`] and use the `out_indices` parameter to determine which layer, given by the index, to extract a feature map from.\n+\n+```py\n+from transformers import AutoBackbone\n+\n+model = AutoBackbone.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\", out_indices=(1,))\n+```\n+\n+This guide describes the backbone class, backbones from the [timm](https://hf.co/docs/timm/index) library, and how to extract features with them.\n+\n+## Backbone classes\n+\n+There are two backbone classes.\n+\n+- [`~transformers.utils.BackboneMixin`] allows you to load a backbone and includes functions for extracting the feature maps and indices.\n+- [`~transformers.utils.BackboneConfigMixin`] allows you to set the feature map and indices of a backbone configuration.\n+\n+Refer to the [Backbone](./main_classes/backbones) API documentation to check which models support a backbone.\n+\n+There are two ways to load a Transformers backbone, [`AutoBackbone`] and a model-specific backbone class.\n+\n+<hfoptions id=\"backbone-classes\">\n+<hfoption id=\"AutoBackbone\">\n+\n+The [AutoClass](./model_doc/auto) API automatically loads a pretrained vision model with [`~PretrainedConfig.from_pretrained`] as a backbone if it's supported.\n+\n+Set the `out_indices` parameter to the layer you'd like to get the feature map from. If you know the name of the layer, you could also use `out_features`. These parameters can be used interchangeably, but if you use both, make sure they refer to the same layer.\n+\n+When `out_indices` or `out_features` isn't used, the backbone returns the feature map from the last layer. The example code below uses `out_indices=(1,)` to get the feature map from the first layer.\n+\n+<div class=\"flex justify-center\">\n+    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/Swin%20Stage%201.png\"/>\n+</div>\n+\n+```py\n+from transformers import AutoImageProcessor, AutoBackbone\n+\n+model = AutoBackbone.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\", out_indices=(1,))\n+```\n+\n+</hfoption>\n+<hfoption id=\"model-specific backbone\">\n+\n+When you know a model supports a backbone, you can load the backbone and neck directly into the models configuration. Pass the configuration to the model to initialize it for a task.\n+\n+The example below loads a [ResNet](./model_doc/resnet) backbone and neck for use in a [MaskFormer](./model_doc/maskformer) instance segmentation head.\n+\n+Set `backbone` to a pretrained model and  `use_pretrained_backbone=True` to use pretrained weights instead of randomly initialized weights.\n+\n+```py\n+from transformers import MaskFormerConfig, MaskFormerForInstanceSegmentation\n+\n+config = MaskFormerConfig(backbone=\"microsoft/resnet-50\", use_pretrained_backbone=True)\n+model = MaskFormerForInstanceSegmentation(config)\n+```\n+\n+Another option is to separately load the backbone configuration and then pass it to `backbone_config` in the model configuration.\n+\n+```py\n+from transformers import MaskFormerConfig, MaskFormerForInstanceSegmentation, ResNetConfig\n+\n+# instantiate backbone configuration\n+backbone_config = ResNetConfig()\n+# load backbone in model\n+config = MaskFormerConfig(backbone_config=backbone_config)\n+# attach backbone to model head\n+model = MaskFormerForInstanceSegmentation(config)\n+```\n+\n+</hfoption>\n+</hfoptions>\n+\n+## timm backbones\n+\n+[timm](https://hf.co/docs/timm/index) is a collection of vision models for training and inference. Transformers supports timm models as backbones with the [`TimmBackbone`] and [`TimmBackboneConfig`] classes.\n+\n+Set `use_timm_backbone=True` to load pretrained timm weights, and `use_pretrained_backbone` to use pretrained or randomly initialized weights.\n+\n+```py\n+from transformers import MaskFormerConfig, MaskFormerForInstanceSegmentation\n+\n+config = MaskFormerConfig(backbone=\"resnet50\", use_timm_backbone=True, use_pretrained_backbone=True)\n+model = MaskFormerForInstanceSegmentation(config)\n+```\n+\n+You could also explicitly call the [`TimmBackboneConfig`] class to load and create a pretrained timm backbone.\n+\n+```py\n+from transformers import TimmBackboneConfig\n+\n+backbone_config = TimmBackboneConfig(\"resnet50\", use_pretrained_backbone=True)\n+```\n+\n+Pass the backbone configuration to the model configuration and instantiate the model head, [`MaskFormerForInstanceSegmentation`], with the backbone.\n+\n+```py\n+from transformers import MaskFormerConfig, MaskFormerForInstanceSegmentation\n+\n+config = MaskFormerConfig(backbone_config=backbone_config)\n+model = MaskFormerForInstanceSegmentation(config)\n+```\n+\n+## Feature extraction\n+\n+The backbone is used to extract image features. Pass an image through the backbone to get the feature maps.\n+\n+Load and preprocess an image and pass it to the backbone. The example below extracts the feature maps from the first layer.\n+\n+```py\n+from transformers import AutoImageProcessor, AutoBackbone\n+import torch\n+from PIL import Image\n+import requests\n+\n+model = AutoBackbone.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\", out_indices=(1,))\n+processor = AutoImageProcessor.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\")\n+\n+url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+image = Image.open(requests.get(url, stream=True).raw)\n+\n+inputs = processor(image, return_tensors=\"pt\")\n+outputs = model(**inputs)\n+```\n+\n+The features are stored and accessed from the outputs `feature_maps` attribute.\n+\n+```py\n+feature_maps = outputs.feature_maps\n+list(feature_maps[0].shape)\n+[1, 96, 56, 56]\n+```"
        },
        {
            "sha": "a1b92a362cd0eb2e0db6ea033921835aac9dd86b",
            "filename": "docs/source/en/bertology.md",
            "status": "removed",
            "additions": 0,
            "deletions": 41,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/6aa9888463ece40e29ac9065127f578a24f50958/docs%2Fsource%2Fen%2Fbertology.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/6aa9888463ece40e29ac9065127f578a24f50958/docs%2Fsource%2Fen%2Fbertology.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fbertology.md?ref=6aa9888463ece40e29ac9065127f578a24f50958",
            "patch": "@@ -1,41 +0,0 @@\n-<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# BERTology\n-\n-There is a growing field of study concerned with investigating the inner working of large-scale transformers like BERT\n-(that some call \"BERTology\"). Some good examples of this field are:\n-\n-\n-- BERT Rediscovers the Classical NLP Pipeline by Ian Tenney, Dipanjan Das, Ellie Pavlick:\n-  https://arxiv.org/abs/1905.05950\n-- Are Sixteen Heads Really Better than One? by Paul Michel, Omer Levy, Graham Neubig: https://arxiv.org/abs/1905.10650\n-- What Does BERT Look At? An Analysis of BERT's Attention by Kevin Clark, Urvashi Khandelwal, Omer Levy, Christopher D.\n-  Manning: https://arxiv.org/abs/1906.04341\n-- CAT-probing: A Metric-based Approach to Interpret How Pre-trained Models for Programming Language Attend Code Structure: https://arxiv.org/abs/2210.04633\n-\n-In order to help this new field develop, we have included a few additional features in the BERT/GPT/GPT-2 models to\n-help people access the inner representations, mainly adapted from the great work of Paul Michel\n-(https://arxiv.org/abs/1905.10650):\n-\n-\n-- accessing all the hidden-states of BERT/GPT/GPT-2,\n-- accessing all the attention weights for each head of BERT/GPT/GPT-2,\n-- retrieving heads output values and gradients to be able to compute head importance score and prune head as explained\n-  in https://arxiv.org/abs/1905.10650.\n-\n-To help you understand and use these features, we have added a specific example script: [bertology.py](https://github.com/huggingface/transformers/tree/main/examples/research_projects/bertology/run_bertology.py) which extracts information and prune a model pre-trained on\n-GLUE."
        },
        {
            "sha": "0c1737af1abd7e31e973b55d9e5d8dd8483a3e3e",
            "filename": "docs/source/en/big_models.md",
            "status": "removed",
            "additions": 0,
            "deletions": 215,
            "changes": 215,
            "blob_url": "https://github.com/huggingface/transformers/blob/6aa9888463ece40e29ac9065127f578a24f50958/docs%2Fsource%2Fen%2Fbig_models.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/6aa9888463ece40e29ac9065127f578a24f50958/docs%2Fsource%2Fen%2Fbig_models.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fbig_models.md?ref=6aa9888463ece40e29ac9065127f578a24f50958",
            "patch": "@@ -1,215 +0,0 @@\n-<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# Instantiate a big model\n-\n-A barrier to accessing very large pretrained models is the amount of memory required. When loading a pretrained PyTorch model, you usually:\n-\n-1. Create a model with random weights.\n-2. Load your pretrained weights.\n-3. Put those pretrained weights in the model.\n-\n-The first two steps both require a full version of the model in memory and if the model weighs several GBs, you may not have enough memory for two copies of it. This problem is amplified in distributed training environments because each process loads a pretrained model and stores two copies in memory.\n-\n-> [!TIP]\n-> The randomly created model is initialized with \"empty\" tensors, which take space in memory without filling it. The random values are whatever was in this chunk of memory at the time. To improve loading speed, the [`_fast_init`](https://github.com/huggingface/transformers/blob/c9f6e5e35156e068b227dd9b15521767f6afd4d2/src/transformers/modeling_utils.py#L2710) parameter is set to `True` by default to skip the random initialization for all weights that are correctly loaded.\n-\n-This guide will show you how Transformers can help you load large pretrained models despite their memory requirements.\n-\n-## Sharded checkpoints\n-\n-From Transformers v4.18.0, a checkpoint larger than 10GB is automatically sharded by the [`~PreTrainedModel.save_pretrained`] method. It is split into several smaller partial checkpoints and creates an index file that maps parameter names to the files they're stored in.\n-\n-The maximum shard size is controlled with the `max_shard_size` parameter, but by default it is 5GB, because it is easier to run on free-tier GPU instances without running out of memory.\n-\n-For example, let's shard [BioMistral/BioMistral-7B](https://hf.co/BioMistral/BioMistral-7B).\n-\n-```py\n->>> with tempfile.TemporaryDirectory() as tmp_dir:\n-...     model.save_pretrained(tmp_dir, max_shard_size=\"5GB\")\n-...     print(sorted(os.listdir(tmp_dir)))\n-['config.json', 'generation_config.json', 'model-00001-of-00006.safetensors', 'model-00002-of-00006.safetensors', 'model-00003-of-00006.safetensors', 'model-00004-of-00006.safetensors', 'model-00005-of-00006.safetensors', 'model-00006-of-00006.safetensors', 'model.safetensors.index.json']\n-```\n-\n-The sharded checkpoint is reloaded with the [`~PreTrainedModel.from_pretrained`] method.\n-\n-```py\n->>> with tempfile.TemporaryDirectory() as tmp_dir:\n-...     model.save_pretrained(tmp_dir, max_shard_size=\"5GB\")\n-...     new_model = AutoModel.from_pretrained(tmp_dir)\n-```\n-\n-The main advantage of sharded checkpoints for big models is that each shard is loaded after the previous one, which caps the memory usage to only the model size and the largest shard size.\n-\n-You could also directly load a sharded checkpoint inside a model without the [`~PreTrainedModel.from_pretrained`] method (similar to PyTorch's `load_state_dict()` method for a full checkpoint). In this case, use the [`~modeling_utils.load_sharded_checkpoint`] method.\n-\n-```py\n->>> from transformers.modeling_utils import load_sharded_checkpoint\n-\n->>> with tempfile.TemporaryDirectory() as tmp_dir:\n-...     model.save_pretrained(tmp_dir, max_shard_size=\"5GB\")\n-...     load_sharded_checkpoint(model, tmp_dir)\n-```\n-\n-### Shard metadata\n-\n-The index file determines which keys are in the checkpoint and where the corresponding weights are stored. This file is loaded like any other JSON file and you can get a dictionary from it.\n-\n-```py\n->>> import json\n-\n->>> with tempfile.TemporaryDirectory() as tmp_dir:\n-...     model.save_pretrained(tmp_dir, max_shard_size=\"5GB\")\n-...     with open(os.path.join(tmp_dir, \"model.safetensors.index.json\"), \"r\") as f:\n-...         index = json.load(f)\n-\n->>> print(index.keys())\n-dict_keys(['metadata', 'weight_map'])\n-```\n-\n-The `metadata` key provides the total model size.\n-\n-```py\n->>> index[\"metadata\"]\n-{'total_size': 28966928384}\n-```\n-\n-The `weight_map` key maps each parameter name (typically `state_dict` in a PyTorch model) to the shard it's stored in.\n-\n-```py\n->>> index[\"weight_map\"]\n-{'lm_head.weight': 'model-00006-of-00006.safetensors',\n- 'model.embed_tokens.weight': 'model-00001-of-00006.safetensors',\n- 'model.layers.0.input_layernorm.weight': 'model-00001-of-00006.safetensors',\n- 'model.layers.0.mlp.down_proj.weight': 'model-00001-of-00006.safetensors',\n- ...\n-}\n-```\n-\n-## Accelerate's Big Model Inference\n-\n-> [!TIP]\n-> Make sure you have Accelerate v0.9.0 or later and PyTorch v1.9.0 or later installed.\n-\n-From Transformers v4.20.0, the [`~PreTrainedModel.from_pretrained`] method is supercharged with Accelerate's [Big Model Inference](https://hf.co/docs/accelerate/usage_guides/big_modeling) feature to efficiently handle really big models! Big Model Inference creates a *model skeleton* on PyTorch's [**meta**](https://pytorch.org/docs/main/meta.html) device. The randomly initialized parameters are only created when the pretrained weights are loaded. This way, you aren't keeping two copies of the model in memory at the same time (one for the randomly initialized model and one for the pretrained weights), and the maximum memory consumed is only the full model size.\n-\n-To enable Big Model Inference in Transformers, set `low_cpu_mem_usage=True` in the [`~PreTrainedModel.from_pretrained`] method.\n-\n-```py\n-from transformers import AutoModelForCausalLM\n-\n-gemma = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b\", low_cpu_mem_usage=True)\n-```\n-\n-Accelerate automatically dispatches the model weights across all available devices, starting with the fastest device (GPU) first and then offloading to the slower devices (CPU and even hard drive). This is enabled by setting `device_map=\"auto\"` in the [`~PreTrainedModel.from_pretrained`] method. When you pass the `device_map` parameter, `low_cpu_mem_usage` is automatically set to `True` so you don't need to specify it.\n-\n-```py\n-from transformers import AutoModelForCausalLM\n-\n-# these loading methods are equivalent\n-gemma = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b\", device_map=\"auto\")\n-gemma = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b\", device_map=\"auto\", low_cpu_mem_usage=True)\n-```\n-\n-You can also write your own `device_map` by mapping each layer to a device. It should map all model parameters to a device, but you don't have to detail where all the submodules of a layer go if the entire layer is on the same device.\n-\n-```python\n-device_map = {\"model.layers.1\": 0, \"model.layers.14\": 1, \"model.layers.31\": \"cpu\", \"lm_head\": \"disk\"}\n-```\n-\n-Access `hf_device_map` attribute to see how Accelerate split the model across devices.\n-\n-```py\n-gemma.hf_device_map\n-```\n-\n-```python out\n-{'model.embed_tokens': 0,\n- 'model.layers.0': 0,\n- 'model.layers.1': 0,\n- 'model.layers.2': 0,\n- 'model.layers.3': 0,\n- 'model.layers.4': 0,\n- 'model.layers.5': 0,\n- 'model.layers.6': 0,\n- 'model.layers.7': 0,\n- 'model.layers.8': 0,\n- 'model.layers.9': 0,\n- 'model.layers.10': 0,\n- 'model.layers.11': 0,\n- 'model.layers.12': 0,\n- 'model.layers.13': 0,\n- 'model.layers.14': 'cpu',\n- 'model.layers.15': 'cpu',\n- 'model.layers.16': 'cpu',\n- 'model.layers.17': 'cpu',\n- 'model.layers.18': 'cpu',\n- 'model.layers.19': 'cpu',\n- 'model.layers.20': 'cpu',\n- 'model.layers.21': 'cpu',\n- 'model.layers.22': 'cpu',\n- 'model.layers.23': 'cpu',\n- 'model.layers.24': 'cpu',\n- 'model.layers.25': 'cpu',\n- 'model.layers.26': 'cpu',\n- 'model.layers.27': 'cpu',\n- 'model.layers.28': 'cpu',\n- 'model.layers.29': 'cpu',\n- 'model.layers.30': 'cpu',\n- 'model.layers.31': 'cpu',\n- 'model.norm': 'cpu',\n- 'lm_head': 'cpu'}\n-```\n-\n-## Model data type\n-\n-PyTorch model weights are normally instantiated as torch.float32 and it can be an issue if you try to load a model as a different data type. For example, you'd need twice as much memory to load the weights in torch.float32 and then again to load them in your desired data type, like torch.float16.\n-\n-> [!WARNING]\n-> Due to how PyTorch is designed, the `torch_dtype` parameter only supports floating data types.\n-\n-To avoid wasting memory like this, explicitly set the `torch_dtype` parameter to the desired data type or set `torch_dtype=\"auto\"` to load the weights with the most optimal memory pattern (the data type is automatically derived from the model weights).\n-\n-<hfoptions id=\"dtype\">\n-<hfoption id=\"specific dtype\">\n-\n-```py\n-from transformers import AutoModelForCausalLM\n-\n-gemma = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b\", torch_dtype=torch.float16)\n-```\n-\n-</hfoption>\n-<hfoption id=\"auto dtype\">\n-\n-```py\n-from transformers import AutoModelForCausalLM\n-\n-gemma = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b\", torch_dtype=\"auto\")\n-```\n-\n-</hfoption>\n-</hfoptions>\n-\n-You can also set the data type to use for models instantiated from scratch.\n-\n-```python\n-import torch\n-from transformers import AutoConfig, AutoModel\n-\n-my_config = AutoConfig.from_pretrained(\"google/gemma-2b\", torch_dtype=torch.float16)\n-model = AutoModel.from_config(my_config)\n-```"
        },
        {
            "sha": "b13601459d899a9374ac457f0deeed0fd5a2f7f9",
            "filename": "docs/source/en/cache_explanation.md",
            "status": "added",
            "additions": 96,
            "deletions": 0,
            "changes": 96,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fcache_explanation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fcache_explanation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fcache_explanation.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -0,0 +1,96 @@\n+<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# Caching\n+\n+Imagine youâ€™re having a conversation with someone, and instead of remembering what they previously said, they have to start from scratch every time you respond. This would be slow and inefficient, right?\n+\n+You can extend this analogy to transformer models. Autoregressive model generation can be slow because it makes a prediction one token at a time. Each new prediction is dependent on all the previous context.\n+\n+To predict the 1000th token, the model requires information from the previous 999 tokens. The information is represented as matrix multiplications across the token representations.\n+\n+To predict the 1001th token, you need the same information from the previous 999 tokens in addition to any information from the 1000th token. This is a lot of matrix multiplications a model has to compute over and over for each token!\n+\n+A key-value (KV) cache eliminates this inefficiency by storing kv pairs derived from the attention layers of previously processed tokens. The stored kv pairs are retrieved from the cache and reused for subsequent tokens, avoiding the need to recompute.\n+\n+> [!WARNING]\n+> Caching should only be used for **inference**. It may cause unexpected errors if it's enabled during training.\n+\n+## Cache class\n+\n+When you use Transformers' [`Cache`] class, the self-attention module performs several critical steps to integrate past and present information.\n+\n+1. The attention module concatenates current kv pairs with past kv pairs stored in the cache. This creates attentions weights with the shape `(new_tokens_length, past_kv_length + new_tokens_length)`. The current and past kv pairs are essentially combined to compute the attention scores, ensuring a model is aware of previous context and the current input.\n+\n+2. When the `forward` method is called iteratively, it's crucial that the attention mask shape matches the combined length of the past and current kv pairs. The attention mask should have the shape `(batch_size, past_kv_length + new_tokens_length)`. This is typically handled internally in [`~GenerationMixin.generate`], but if you want to implement your own generation loop with [`Cache`], keep this in mind! The attention mask should hold the past and current token values.\n+\n+3. It is also important to be aware of the `cache_position`. This is important if you want to reuse a prefilled [`Cache`] with the `forward` method because you have to pass a valid `cache_position` value. This indicates the input positions in a sequence. `cache_position` is unaffected by padding, and it always adds one more position for each token. For example, if a kv cache contains 10 tokens - regardless of pad tokens - the cache position for the next token should be `torch.tensor([10])`.\n+\n+The example below demonstrates how to create a generation loop with [`DynamicCache`]. As discussed, the attention mask is a concatenation of past and current token values and `1` is added to the cache position for the next token.\n+\n+```py\n+import torch\n+from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache\n+\n+model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n+model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"cuda:0\")\n+tokenizer = AutoTokenizer.from_pretrained(model_id)\n+\n+past_key_values = DynamicCache()\n+messages = [{\"role\": \"user\", \"content\": \"Hello, what's your name.\"}]\n+inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\", return_dict=True).to(\"cuda:0\")\n+\n+generated_ids = inputs.input_ids\n+cache_position = torch.arange(inputs.input_ids.shape[1], dtype=torch.int64, device=\"cuda:0\")\n+max_new_tokens = 10\n+\n+for _ in range(max_new_tokens):\n+    outputs = model(**inputs, cache_position=cache_position, past_key_values=past_key_values, use_cache=True)\n+    # Greedily sample one next token\n+    next_token_ids = outputs.logits[:, -1:].argmax(-1)\n+    generated_ids = torch.cat([generated_ids, next_token_ids], dim=-1)\n+    # Prepare inputs for the next generation step by leaaving unprocessed tokens, in our case we have only one new token\n+    # and expanding attn mask for the new token, as explained above\n+    attention_mask = inputs[\"attention_mask\"]\n+    attention_mask = torch.cat([attention_mask, attention_mask.new_ones((attention_mask.shape[0], 1))], dim=-1)\n+    inputs = {\"input_ids\": next_token_ids, \"attention_mask\": attention_mask}\n+    cache_position = cache_position[-1:] + 1 # add one more position for the next token\n+\n+print(tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0])\n+\"[INST] Hello, what's your name. [/INST]  Hello! My name is LLaMA,\"\n+```\n+\n+## Legacy cache format\n+\n+Before the [`Cache`] class, the cache used to be stored as a tuple of tuples of tensors. This format has is dynamic because it grows as text is generated, similar to [`DynamicCache`].\n+\n+If your project depends on this legacy format, you can convert between [`DynamicCache`] and a tuple of tuples as shown below with the [`~DynamicCache.from_legacy_cache`] and [`DynamicCache.to_legacy_cache`] functions. This is helpful if you have custom logic for manipulating a cache in a specific format.\n+\n+```py\n+import torch\n+from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache\n+\n+tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n+model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", torch_dtype=torch.float16, device_map=\"auto\")\n+inputs = tokenizer(\"Hello, my name is\", return_tensors=\"pt\").to(model.device)\n+\n+# `return_dict_in_generate=True` is required to return the cache and `return_legacy_cache` forces the returned cache\n+# in the the legacy format\n+generation_outputs = model.generate(**inputs, return_dict_in_generate=True, return_legacy_cache=True, max_new_tokens=5)\n+\n+cache = DynamicCache.from_legacy_cache(generation_outputs.past_key_values)\n+legacy_format_cache = cache.to_legacy_cache()\n+```\n\\ No newline at end of file"
        },
        {
            "sha": "026268500b13335b914204291b88d1efe38dc784",
            "filename": "docs/source/en/chat_extras.md",
            "status": "added",
            "additions": 299,
            "deletions": 0,
            "changes": 299,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fchat_extras.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fchat_extras.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fchat_extras.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -0,0 +1,299 @@\n+<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# Tools and RAG\n+\n+The [`~PreTrainedTokenizerBase.apply_chat_template`] method supports virtually any additional argument types - strings, lists, dicts - besides the chat message. This makes it possible to use chat templates for many use cases.\n+\n+This guide will demonstrate how to use chat templates with tools and retrieval-augmented generation (RAG).\n+\n+## Tools\n+\n+Tools are functions a large language model (LLM) can call to perform specific tasks. It is a powerful way to extend the capabilities of conversational agents with real-time information, computational tools, or access to large databases.\n+\n+Follow the rules below when creating a tool.\n+\n+1. The function should have a descriptive name.\n+2. The function arguments must have a type hint in the function header (don't include in the `Args` block).\n+3. The function must have a [Google-style](https://google.github.io/styleguide/pyguide.html#38-comments-and-docstrings) docstring.\n+4. The function can have a return type and `Returns` block, but these are optional because most tool use models ignore them.\n+\n+An example tool to get temperature and wind speed is shown below.\n+\n+```py\n+def get_current_temperature(location: str, unit: str) -> float:\n+    \"\"\"\n+    Get the current temperature at a location.\n+    \n+    Args:\n+        location: The location to get the temperature for, in the format \"City, Country\"\n+        unit: The unit to return the temperature in. (choices: [\"celsius\", \"fahrenheit\"])\n+    Returns:\n+        The current temperature at the specified location in the specified units, as a float.\n+    \"\"\"\n+    return 22.  # A real function should probably actually get the temperature!\n+\n+def get_current_wind_speed(location: str) -> float:\n+    \"\"\"\n+    Get the current wind speed in km/h at a given location.\n+    \n+    Args:\n+        location: The location to get the temperature for, in the format \"City, Country\"\n+    Returns:\n+        The current wind speed at the given location in km/h, as a float.\n+    \"\"\"\n+    return 6.  # A real function should probably actually get the wind speed!\n+\n+tools = [get_current_temperature, get_current_wind_speed]\n+```\n+\n+Load a model and tokenizer that supports tool-use like [NousResearch/Hermes-2-Pro-Llama-3-8B](https://hf.co/NousResearch/Hermes-2-Pro-Llama-3-8B), but you can also consider a larger model like [Command-R](./model_doc/cohere) and [Mixtral-8x22B](./model_doc/mixtral) if your hardware can support it.\n+\n+```py\n+import torch\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n+\n+tokenizer = AutoTokenizer.from_pretrained( \"NousResearch/Hermes-2-Pro-Llama-3-8B\")\n+tokenizer = AutoTokenizer.from_pretrained( \"NousResearch/Hermes-2-Pro-Llama-3-8B\")\n+model = AutoModelForCausalLM.from_pretrained( \"NousResearch/Hermes-2-Pro-Llama-3-8B\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n+```\n+\n+Create a chat message.\n+\n+```py\n+messages = [\n+  {\"role\": \"system\", \"content\": \"You are a bot that responds to weather queries. You should reply with the unit used in the queried location.\"},\n+  {\"role\": \"user\", \"content\": \"Hey, what's the temperature in Paris right now?\"}\n+]\n+```\n+\n+Pass `messages` and a list of tools to [`~PreTrainedTokenizerBase.apply_chat_template`]. Then you can pass the inputs to the model for generation.\n+\n+```py\n+inputs = tokenizer.apply_chat_template(messages, tools=tools, add_generation_prompt=True, return_dict=True, return_tensors=\"pt\")\n+inputs = {k: v for k, v in inputs.items()}\n+outputs = model.generate(**inputs, max_new_tokens=128)\n+print(tokenizer.decode(outputs[0][len(inputs[\"input_ids\"][0]):]))\n+```\n+\n+```txt\n+<tool_call>\n+{\"arguments\": {\"location\": \"Paris, France\", \"unit\": \"celsius\"}, \"name\": \"get_current_temperature\"}\n+</tool_call><|im_end|>\n+```\n+\n+The chat model called the `get_current_temperature` tool with the correct parameters from the docstring. It inferred France as the location based on Paris, and that it should use Celsius for the units of temperature. \n+\n+Now append the `get_current_temperature` function and these arguments to the chat message as `tool_call`. The `tool_call` dictionary should be provided to the `assistant` role instead of the `system` or `user`.\n+\n+> [!WARNING]\n+> The OpenAI API uses a JSON string as its `tool_call` format. This may cause errors or strange model behavior if used in Transformers, which expects a dict.\n+\n+<hfoptions id=\"tool-call\">\n+<hfoption id=\"Llama\">\n+\n+```py\n+tool_call = {\"name\": \"get_current_temperature\", \"arguments\": {\"location\": \"Paris, France\", \"unit\": \"celsius\"}}\n+messages.append({\"role\": \"assistant\", \"tool_calls\": [{\"type\": \"function\", \"function\": tool_call}]})\n+```\n+\n+Allow the assistant to read the function outputs and chat with the user.\n+\n+```py\n+inputs = tokenizer.apply_chat_template(messages, tools=tools, add_generation_prompt=True, return_dict=True, return_tensors=\"pt\")\n+inputs = {k: v for k, v in inputs.items()}\n+out = model.generate(**inputs, max_new_tokens=128)\n+print(tokenizer.decode(out[0][len(inputs[\"input_ids\"][0]):]))\n+```\n+\n+```txt\n+The temperature in Paris, France right now is approximately 12Â°C (53.6Â°F).<|im_end|>\n+```\n+\n+</hfoption>\n+<hfoption id=\"Mistral/Mixtral\">\n+\n+For [Mistral](./model_doc/mistral) and [Mixtral](./model_doc/mixtral) models, you need an additional `tool_call_id`. The `tool_call_id` is 9 randomly generated alphanumeric characters assigned to the `id` key in the `tool_call` dictionary.\n+\n+```py\n+tool_call_id = \"9Ae3bDc2F\"\n+tool_call = {\"name\": \"get_current_temperature\", \"arguments\": {\"location\": \"Paris, France\", \"unit\": \"celsius\"}}\n+messages.append({\"role\": \"assistant\", \"tool_calls\": [{\"type\": \"function\", \"id\": tool_call_id, \"function\": tool_call}]})\n+```\n+\n+```py\n+inputs = tokenizer.apply_chat_template(messages, tools=tools, add_generation_prompt=True, return_dict=True, return_tensors=\"pt\")\n+inputs = {k: v for k, v in inputs.items()}\n+out = model.generate(**inputs, max_new_tokens=128)\n+print(tokenizer.decode(out[0][len(inputs[\"input_ids\"][0]):]))\n+```\n+\n+</hfoption>\n+</hfoptions>\n+\n+## Schema\n+\n+[`~PreTrainedTokenizerBase.apply_chat_template`] converts functions into a [JSON schema](https://json-schema.org/learn/getting-started-step-by-step) which is passed to the chat template. A LLM never sees the code inside the function. In other words, a LLM doesn't care how the model works technically, it only cares about function **definition** and **arguments**.\n+\n+The JSON schema is automatically generated behind the scenes as long as your function follows the [rules](#tools) listed earlier above. But you can use [get_json_schema](https://github.com/huggingface/transformers/blob/14561209291255e51c55260306c7d00c159381a5/src/transformers/utils/chat_template_utils.py#L205) to manually convert a schema for more visibility or debugging.\n+\n+```py\n+from transformers.utils import get_json_schema\n+\n+def multiply(a: float, b: float):\n+    \"\"\"\n+    A function that multiplies two numbers\n+    \n+    Args:\n+        a: The first number to multiply\n+        b: The second number to multiply\n+    \"\"\"\n+    return a * b\n+\n+schema = get_json_schema(multiply)\n+print(schema)\n+```\n+\n+```json\n+{\n+  \"type\": \"function\", \n+  \"function\": {\n+    \"name\": \"multiply\", \n+    \"description\": \"A function that multiplies two numbers\", \n+    \"parameters\": {\n+      \"type\": \"object\", \n+      \"properties\": {\n+        \"a\": {\n+          \"type\": \"number\", \n+          \"description\": \"The first number to multiply\"\n+        }, \n+        \"b\": {\n+          \"type\": \"number\",\n+          \"description\": \"The second number to multiply\"\n+        }\n+      }, \n+      \"required\": [\"a\", \"b\"]\n+    }\n+  }\n+}\n+```\n+\n+You can edit the schema or write one entirely from scratch. This gives you a lot of flexibility to define precise schemas for more complex functions.\n+\n+> [!WARNING]\n+> Try keeping your function signatures simple and the arguments to a minimum. These are easier for a model to understand and use than complex functions for example with nested arguments.\n+\n+The example below demonstrates writing a schema manually and then passing it to [`~PreTrainedTokenizerBase.apply_chat_template`].\n+\n+```py\n+# A simple function that takes no arguments\n+current_time = {\n+  \"type\": \"function\", \n+  \"function\": {\n+    \"name\": \"current_time\",\n+    \"description\": \"Get the current local time as a string.\",\n+    \"parameters\": {\n+      'type': 'object',\n+      'properties': {}\n+    }\n+  }\n+}\n+\n+# A more complete function that takes two numerical arguments\n+multiply = {\n+  'type': 'function',\n+  'function': {\n+    'name': 'multiply',\n+    'description': 'A function that multiplies two numbers', \n+    'parameters': {\n+      'type': 'object', \n+      'properties': {\n+        'a': {\n+          'type': 'number',\n+          'description': 'The first number to multiply'\n+        }, \n+        'b': {\n+          'type': 'number', 'description': 'The second number to multiply'\n+        }\n+      }, \n+      'required': ['a', 'b']\n+    }\n+  }\n+}\n+\n+model_input = tokenizer.apply_chat_template(\n+    messages,\n+    tools = [current_time, multiply]\n+)\n+```\n+\n+## RAG\n+\n+Retrieval-augmented generation (RAG) models enhance a models existing knowledge by allowing it to search documents for additional information before returning a query. For RAG models, add a `documents` parameter to [`~PreTrainedTokenizerBase.apply_chat_template`]. This `documents` parameter should be a list of documents, and each document should be a single dict with `title` and `content` keys.\n+\n+> [!TIP]\n+> The `documents` parameter for RAG isn't widely supported and many models have chat templates that ignore `documents`. Verify if a model supports `documents` by reading its model card or executing `print(tokenizer.chat_template)` to see if the `documents` key is present. [Command-R](https://hf.co/CohereForAI/c4ai-command-r-08-2024) and [Command-R+](https://hf.co/CohereForAI/c4ai-command-r-plus-08-2024) both support `documents` in their RAG chat templates.\n+\n+Create a list of documents to pass to the model.\n+\n+```py\n+documents = [\n+    {\n+        \"title\": \"The Moon: Our Age-Old Foe\", \n+        \"text\": \"Man has always dreamed of destroying the moon. In this essay, I shall...\"\n+    },\n+    {\n+        \"title\": \"The Sun: Our Age-Old Friend\",\n+        \"text\": \"Although often underappreciated, the sun provides several notable benefits...\"\n+    }\n+]\n+```\n+\n+Set `chat_template=\"rag\"` in [`~PreTrainedTokenizerBase.apply_chat_template`] and generate a response.\n+\n+```py\n+from transformers import AutoTokenizer, AutoModelForCausalLM\n+\n+# Load the model and tokenizer\n+tokenizer = AutoTokenizer.from_pretrained(\"CohereForAI/c4ai-command-r-v01-4bit\")\n+model = AutoModelForCausalLM.from_pretrained(\"CohereForAI/c4ai-command-r-v01-4bit\", device_map=\"auto\")\n+device = model.device # Get the device the model is loaded on\n+\n+# Define conversation input\n+conversation = [\n+    {\"role\": \"user\", \"content\": \"What has Man always dreamed of?\"}\n+]\n+\n+input_ids = tokenizer.apply_chat_template(\n+    conversation=conversation,\n+    documents=documents,\n+    chat_template=\"rag\",\n+    tokenize=True,\n+    add_generation_prompt=True,\n+    return_tensors=\"pt\").to(device)\n+\n+# Generate a response \n+generated_tokens = model.generate(\n+    input_ids,\n+    max_new_tokens=100,\n+    do_sample=True,\n+    temperature=0.3,\n+    )\n+\n+# Decode and print the generated text along with generation prompt\n+generated_text = tokenizer.decode(generated_tokens[0])\n+print(generated_text)\n+```"
        },
        {
            "sha": "5943709539e79ea2ee66f168c0abf49e9e8aa2fe",
            "filename": "docs/source/en/chat_template_advanced.md",
            "status": "removed",
            "additions": 0,
            "deletions": 463,
            "changes": 463,
            "blob_url": "https://github.com/huggingface/transformers/blob/6aa9888463ece40e29ac9065127f578a24f50958/docs%2Fsource%2Fen%2Fchat_template_advanced.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/6aa9888463ece40e29ac9065127f578a24f50958/docs%2Fsource%2Fen%2Fchat_template_advanced.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fchat_template_advanced.md?ref=6aa9888463ece40e29ac9065127f578a24f50958",
            "patch": "@@ -1,463 +0,0 @@\n-<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# Advanced Usage and Customizing Your Chat Templates\n-\n-In this page, weâ€™ll explore more advanced techniques for working with chat templates in Transformers. Whether youâ€™re looking to write your own templates, create custom components, or optimize your templates for efficiency, weâ€™ll cover everything you need to take your templates to the next level. Letâ€™s dive into the tools and strategies that will help you get the most out of your chat models.\n-\n-\n-## How do chat templates work?\n-\n-The chat template for a model is stored on the `tokenizer.chat_template` attribute. Let's take a look at a `Zephyr` chat template, though note this\n-one is a little simplified from the actual one!\n-\n-```\n-{%- for message in messages %}\n-    {{- '<|' + message['role'] + '|>\\n' }}\n-    {{- message['content'] + eos_token }}\n-{%- endfor %}\n-{%- if add_generation_prompt %}\n-    {{- '<|assistant|>\\n' }}\n-{%- endif %}\n-```\n-\n-If you've never seen one of these before, this is a [Jinja template](https://jinja.palletsprojects.com/en/3.1.x/templates/).\n-Jinja is a templating language that allows you to write simple code that generates text. In many ways, the code and\n-syntax resembles Python. In pure Python, this template would look something like this:\n-\n-```python\n-for message in messages:\n-    print(f'<|{message[\"role\"]}|>')\n-    print(message['content'] + eos_token)\n-if add_generation_prompt:\n-    print('<|assistant|>')\n-```\n-\n-Effectively, the template does three things:\n-1. For each message, print the role enclosed in `<|` and `|>`, like `<|user|>` or `<|assistant|>`.\n-2. Next, print the content of the message, followed by the end-of-sequence token.\n-3. Finally, if `add_generation_prompt` is set, print the assistant token, so that the model knows to start generating\n-   an assistant response.\n-\n-This is a pretty simple template but Jinja gives you a lot of flexibility to do more complex things! Let's see a Jinja\n-template that can format inputs similarly to the way LLaMA formats them (note that the real LLaMA template includes \n-handling for default system messages and slightly different system message handling in general - don't use this one \n-in your actual code!)\n-\n-```\n-{%- for message in messages %}\n-    {%- if message['role'] == 'user' %}\n-        {{- bos_token + '[INST] ' + message['content'] + ' [/INST]' }}\n-    {%- elif message['role'] == 'system' %}\n-        {{- '<<SYS>>\\\\n' + message['content'] + '\\\\n<</SYS>>\\\\n\\\\n' }}\n-    {%- elif message['role'] == 'assistant' %}\n-        {{- ' '  + message['content'] + ' ' + eos_token }}\n-    {%- endif %}\n-{%- endfor %}\n-```\n-\n-Hopefully if you stare at this for a little bit you can see what this template is doing - it adds specific tokens like\n-`[INST]` and `[/INST]` based on the role of each message. User, assistant and system messages are clearly\n-distinguishable to the model because of the tokens they're wrapped in.\n-\n-\n-## How do I create a chat template?\n-\n-Simple, just write a jinja template and set `tokenizer.chat_template`. You may find it easier to start with an \n-existing template from another model and simply edit it for your needs! For example, we could take the LLaMA template\n-above and add \"[ASST]\" and \"[/ASST]\" to assistant messages:\n-\n-```\n-{%- for message in messages %}\n-    {%- if message['role'] == 'user' %}\n-        {{- bos_token + '[INST] ' + message['content'].strip() + ' [/INST]' }}\n-    {%- elif message['role'] == 'system' %}\n-        {{- '<<SYS>>\\\\n' + message['content'].strip() + '\\\\n<</SYS>>\\\\n\\\\n' }}\n-    {%- elif message['role'] == 'assistant' %}\n-        {{- '[ASST] '  + message['content'] + ' [/ASST]' + eos_token }}\n-    {%- endif %}\n-{%- endfor %}\n-```\n-\n-Now, simply set the `tokenizer.chat_template` attribute. Next time you use [`~PreTrainedTokenizer.apply_chat_template`], it will\n-use your new template! This attribute will be saved in the `tokenizer_config.json` file, so you can use\n-[`~utils.PushToHubMixin.push_to_hub`] to upload your new template to the Hub and make sure everyone's using the right\n-template for your model!\n-\n-```python\n-template = tokenizer.chat_template\n-template = template.replace(\"SYS\", \"SYSTEM\")  # Change the system token\n-tokenizer.chat_template = template  # Set the new template\n-tokenizer.push_to_hub(\"model_name\")  # Upload your new template to the Hub!\n-```\n-\n-The method [`~PreTrainedTokenizer.apply_chat_template`] which uses your chat template is called by the [`TextGenerationPipeline`] class, so \n-once you set the correct chat template, your model will automatically become compatible with [`TextGenerationPipeline`].\n-\n-<Tip>\n-If you're fine-tuning a model for chat, in addition to setting a chat template, you should probably add any new chat\n-control tokens as special tokens in the tokenizer. Special tokens are never split, \n-ensuring that your control tokens are always handled as single tokens rather than being tokenized in pieces. You \n-should also set the tokenizer's `eos_token` attribute to the token that marks the end of assistant generations in your\n-template. This will ensure that text generation tools can correctly figure out when to stop generating text.\n-</Tip>\n-\n-\n-## Why do some models have multiple templates?\n-\n-Some models use different templates for different use cases. For example, they might use one template for normal chat\n-and another for tool-use, or retrieval-augmented generation. In these cases, `tokenizer.chat_template` is a dictionary.\n-This can cause some confusion, and where possible, we recommend using a single template for all use-cases. You can use\n-Jinja statements like `if tools is defined` and `{% macro %}` definitions to easily wrap multiple code paths in a\n-single template.\n-\n-When a tokenizer has multiple templates, `tokenizer.chat_template` will be a `dict`, where each key is the name\n-of a template. The `apply_chat_template` method has special handling for certain template names: Specifically, it will\n-look for a template named `default` in most cases, and will raise an error if it can't find one. However, if a template\n-named `tool_use` exists when the user has passed a `tools` argument, it will use that instead. To access templates\n-with other names, pass the name of the template you want to the `chat_template` argument of\n-`apply_chat_template()`.\n-\n-We find that this can be a bit confusing for users, though - so if you're writing a template yourself, we recommend\n-trying to put it all in a single template where possible!\n-\n-\n-## What template should I use?\n-\n-When setting the template for a model that's already been trained for chat, you should ensure that the template\n-exactly matches the message formatting that the model saw during training, or else you will probably experience\n-performance degradation. This is true even if you're training the model further - you will probably get the best \n-performance if you keep the chat tokens constant. This is very analogous to tokenization - you generally get the\n-best performance for inference or fine-tuning when you precisely match the tokenization used during training.\n-\n-If you're training a model from scratch, or fine-tuning a base language model for chat, on the other hand,\n-you have a lot of freedom to choose an appropriate template! LLMs are smart enough to learn to handle lots of different\n-input formats. One popular choice is the `ChatML` format, and this is a good, flexible choice for many use-cases. \n-It looks like this:\n-\n-```\n-{%- for message in messages %}\n-    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n' }}\n-{%- endfor %}\n-```\n-\n-If you like this one, here it is in one-liner form, ready to copy into your code. The one-liner also includes\n-handy support for [generation prompts](#what-are-generation-prompts), but note that it doesn't add BOS or EOS tokens!\n-If your model expects those, they won't be added automatically by `apply_chat_template` - in other words, the\n-text will be tokenized with `add_special_tokens=False`. This is to avoid potential conflicts between the template and\n-the `add_special_tokens` logic. If your model expects special tokens, make sure to add them to the template!\n-\n-```python\n-tokenizer.chat_template = \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\"\n-```\n-\n-This template wraps each message in `<|im_start|>` and `<|im_end|>` tokens, and simply writes the role as a string, which\n-allows for flexibility in the roles you train with. The output looks like this:\n-\n-```text\n-<|im_start|>system\n-You are a helpful chatbot that will do its best not to say anything so stupid that people tweet about it.<|im_end|>\n-<|im_start|>user\n-How are you?<|im_end|>\n-<|im_start|>assistant\n-I'm doing great!<|im_end|>\n-```\n-\n-The \"user\", \"system\" and \"assistant\" roles are the standard for chat, and we recommend using them when it makes sense,\n-particularly if you want your model to operate well with [`TextGenerationPipeline`]. However, you are not limited\n-to these roles - templating is extremely flexible, and any string can be a role.\n-\n-## I want to add some chat templates! How should I get started?\n-\n-If you have any chat models, you should set their `tokenizer.chat_template` attribute and test it using\n-[`~PreTrainedTokenizer.apply_chat_template`], then push the updated tokenizer to the Hub. This applies even if you're\n-not the model owner - if you're using a model with an empty chat template, or one that's still using the default class\n-template, please open a [pull request](https://huggingface.co/docs/hub/repositories-pull-requests-discussions) to the model repository so that this attribute can be set properly!\n-\n-Once the attribute is set, that's it, you're done! `tokenizer.apply_chat_template` will now work correctly for that\n-model, which means it is also automatically supported in places like `TextGenerationPipeline`!\n-\n-By ensuring that models have this attribute, we can make sure that the whole community gets to use the full power of\n-open-source models. Formatting mismatches have been haunting the field and silently harming performance for too long - \n-it's time to put an end to them!\n-\n-\n-<Tip>\n-\n-The easiest way to get started with writing Jinja templates is to take a look at some existing ones. You can use\n-`print(tokenizer.chat_template)` for any chat model to see what template it's using. In general, models that support tool use have \n-much more complex templates than other models - so when you're just getting started, they're probably a bad example\n-to learn from! You can also take a look at the \n-[Jinja documentation](https://jinja.palletsprojects.com/en/3.1.x/templates/#synopsis) for details\n-of general Jinja formatting and syntax.\n-\n-</Tip>\n-\n-Jinja templates in `transformers` are identical to Jinja templates elsewhere. The main thing to know is that \n-the conversation history will be accessible inside your template as a variable called `messages`.  \n-You will be able to access `messages` in your template just like you can in Python, which means you can loop over \n-it with `{% for message in messages %}` or access individual messages with `{{ messages[0] }}`, for example.\n-\n-You can also use the following tips to write clean, efficient Jinja templates:\n-\n-### Trimming whitespace\n-\n-By default, Jinja will print any whitespace that comes before or after a block. This can be a problem for chat\n-templates, which generally want to be very precise with whitespace! To avoid this, we strongly recommend writing\n-your templates like this:\n-\n-```\n-{%- for message in messages %}\n-    {{- message['role'] + message['content'] }}\n-{%- endfor %}\n-```\n-\n-rather than like this:\n-\n-```\n-{% for message in messages %}\n-    {{ message['role'] + message['content'] }}\n-{% endfor %}\n-```\n-\n-Adding `-` will strip any whitespace that comes before the block. The second example looks innocent, but the newline\n-and indentation may end up being included in the output, which is probably not what you want!\n-\n-### Special variables\n-\n-Inside your template, you will have access several special variables. The most important of these is `messages`, \n-which contains the chat history as a list of message dicts. However, there are several others. Not every\n-variable will be used in every template. The most common other variables are:\n-\n-- `tools` contains a list of tools in JSON schema format. Will be `None` or undefined if no tools are passed.\n-- `documents` contains a list of documents in the format `{\"title\": \"Title\", \"contents\": \"Contents\"}`, used for retrieval-augmented generation. Will be `None` or undefined if no documents are passed.\n-- `add_generation_prompt` is a bool that is `True` if the user has requested a generation prompt, and `False` otherwise. If this is set, your template should add the header for an assistant message to the end of the conversation. If your model doesn't have a specific header for assistant messages, you can ignore this flag.\n-- **Special tokens** like `bos_token` and `eos_token`. These are extracted from `tokenizer.special_tokens_map`. The exact tokens available inside each template will differ depending on the parent tokenizer.\n-\n-<Tip>\n-\n-You can actually pass any `kwarg` to `apply_chat_template`, and it will be accessible inside the template as a variable. In general,\n-we recommend trying to stick to the core variables above, as it will make your model harder to use if users have\n-to write custom code to pass model-specific `kwargs`. However, we're aware that this field moves quickly, so if you\n-have a new use-case that doesn't fit in the core API, feel free to use a new `kwarg` for it! If a new `kwarg`\n-becomes common we may promote it into the core API and create a standard, documented format for it.\n-\n-</Tip>\n-\n-### Callable functions\n-\n-There is also a short list of callable functions available to you inside your templates. These are:\n-\n-- `raise_exception(msg)`: Raises a `TemplateException`. This is useful for debugging, and for telling users when they're\n-doing something that your template doesn't support.\n-- `strftime_now(format_str)`: Equivalent to `datetime.now().strftime(format_str)` in Python. This is used for getting\n-the current date/time in a specific format, which is sometimes included in system messages.\n-\n-### Compatibility with non-Python Jinja\n-\n-There are multiple implementations of Jinja in various languages. They generally have the same syntax,\n-but a key difference is that when you're writing a template in Python you can use Python methods, such as\n-`.lower()` on strings or `.items()` on dicts. This will break if someone tries to use your template on a non-Python\n-implementation of Jinja. Non-Python implementations are particularly common in deployment environments, where JS\n-and Rust are very popular. \n-\n-Don't panic, though! There are a few easy changes you can make to your templates to ensure they're compatible across\n-all implementations of Jinja:\n-\n-- Replace Python methods with Jinja filters. These usually have the same name, for example `string.lower()` becomes\n-  `string|lower`, and `dict.items()` becomes `dict|items`. One notable change is that `string.strip()` becomes `string|trim`.\n-  See the [list of built-in filters](https://jinja.palletsprojects.com/en/3.1.x/templates/#builtin-filters)\n-  in the Jinja documentation for more.\n-- Replace `True`, `False` and `None`, which are Python-specific, with `true`, `false` and `none`.\n-- Directly rendering a dict or list may give different results in other implementations (for example, string entries\n-  might change from single-quoted to double-quoted). Adding the `tojson` filter can help to ensure consistency here.\n-\n-### Writing generation prompts\n-\n-We mentioned above that `add_generation_prompt` is a special variable that will be accessible inside your template,\n-and is controlled by the user setting the `add_generation_prompt` flag. If your model expects a header for\n-assistant messages, then your template must support adding the header when `add_generation_prompt` is set.\n-\n-Here is an example of a template that formats messages ChatML-style, with generation prompt support:\n-\n-```text\n-{{- bos_token }}\n-{%- for message in messages %}\n-    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n' }}\n-{%- endfor %}\n-{%- if add_generation_prompt %}\n-    {{- '<|im_start|>assistant\\n' }}\n-{%- endif %}\n-```\n-\n-The exact content of the assistant header will depend on your specific model, but it should always be **the string\n-that represents the start of an assistant message**, so that if the user applies your template with \n-`add_generation_prompt=True` and then generates text, the model will write an assistant response. Also note that some\n-models do not need a generation prompt, because assistant messages always begin immediately after user messages. \n-This is particularly common for LLaMA and Mistral models, where assistant messages begin immediately after the `[/INST]`\n-token that ends user messages. In these cases, the template can ignore the `add_generation_prompt` flag.\n-\n-Generation prompts are important! If your model requires a generation prompt but it is not set in the template, then\n-model generations will likely be severely degraded, or the model may display unusual behaviour like continuing \n-the final user message! \n-\n-### Writing and debugging larger templates\n-\n-When this feature was introduced, most templates were quite small, the Jinja equivalent of a \"one-liner\" script. \n-However, with new models and features like tool-use and RAG, some templates can be 100 lines long or more. When\n-writing templates like these, it's a good idea to write them in a separate file, using a text editor. You can easily \n-extract a chat template to a file:\n-\n-```python\n-open(\"template.jinja\", \"w\").write(tokenizer.chat_template)\n-```\n-\n-Or load the edited template back into the tokenizer:\n-\n-```python\n-tokenizer.chat_template = open(\"template.jinja\").read()\n-```\n-\n-As an added bonus, when you write a long, multi-line template in a separate file, line numbers in that file will\n-exactly correspond to line numbers in template parsing or execution errors. This will make it much easier to\n-identify the source of issues.\n-\n-\n-\n-## Writing templates for tools\n-\n-Although chat templates do not enforce a specific API for tools (or for anything, really), we recommend \n-template authors try to stick to a standard API where possible. The whole point of chat templates is to allow code\n-to be transferable across models, so deviating from the standard tools API means users will have to write\n-custom code to use tools with your model. Sometimes it's unavoidable, but often with clever templating you can\n-make the standard API work!\n-\n-Below, we'll list the elements of the standard API, and give tips on writing templates that will work well with it.\n-\n-### Tool definitions\n-\n-Your template should expect that the variable `tools` will either be null (if no tools are passed), or is a list \n-of JSON schema dicts. Our chat template methods allow users to pass tools as either JSON schema or Python functions, but when\n-functions are passed, we automatically generate JSON schema and pass that to your template. As a result, the \n-`tools` variable that your template receives will always be a list of JSON schema. Here is\n-a sample tool JSON schema:\n-\n-```json\n-{\n-  \"type\": \"function\", \n-  \"function\": {\n-    \"name\": \"multiply\", \n-    \"description\": \"A function that multiplies two numbers\", \n-    \"parameters\": {\n-      \"type\": \"object\", \n-      \"properties\": {\n-        \"a\": {\n-          \"type\": \"number\", \n-          \"description\": \"The first number to multiply\"\n-        }, \n-        \"b\": {\n-          \"type\": \"number\",\n-          \"description\": \"The second number to multiply\"\n-        }\n-      }, \n-      \"required\": [\"a\", \"b\"]\n-    }\n-  }\n-}\n-```\n-\n-And here is some example code for handling tools in your chat template. Remember, this is just an example for a\n-specific format - your model will probably need different formatting!\n-\n-```text\n-{%- if tools %}\n-    {%- for tool in tools %}\n-        {{- '<tool>' + tool['function']['name'] + '\\n' }}\n-        {%- for argument in tool['function']['parameters']['properties'] %}\n-            {{- argument + ': ' + tool['function']['parameters']['properties'][argument]['description'] + '\\n' }}\n-        {%- endfor %}\n-        {{- '\\n</tool>' }}\n-    {%- endif %}\n-{%- endif %}\n-```\n-\n-The specific tokens and tool descriptions your template renders should of course be chosen to match the ones your model\n-was trained with. There is no requirement that your **model** understands JSON schema input, only that your template can translate\n-JSON schema into your model's format. For example, [Command-R](https://huggingface.co/CohereForAI/c4ai-command-r-plus-08-2024) \n-was trained with tools defined using Python function headers, but the Command-R tool template accepts JSON schema, \n-converts types internally and renders the input tools as Python headers. You can do a lot with templates!\n-\n-### Tool calls\n-\n-Tool calls, if present, will be a list attached to a message with the \"assistant\" role. Note that `tool_calls` is \n-always a list, even though most tool-calling models only support single tool calls at a time, which means\n-the list will usually only have a single element. Here is a sample message dict containing a tool call:\n-\n-```json\n-{\n-  \"role\": \"assistant\",\n-  \"tool_calls\": [\n-    {\n-      \"type\": \"function\",\n-      \"function\": {\n-        \"name\": \"multiply\",\n-        \"arguments\": {\n-          \"a\": 5,\n-          \"b\": 6\n-        }\n-      }\n-    }\n-  ]\n-}\n-```\n-\n-And a common pattern for handling them would be something like this:\n-\n-```text\n-{%- if message['role'] == 'assistant' and 'tool_calls' in message %}\n-    {%- for tool_call in message['tool_calls'] %}\n-            {{- '<tool_call>' + tool_call['function']['name'] + '\\n' + tool_call['function']['arguments']|tojson + '\\n</tool_call>' }}\n-        {%- endif %}\n-    {%- endfor %}\n-{%- endif %}\n-```\n-\n-Again, you should render the tool call with the formatting and special tokens that your model expects.\n-\n-### Tool responses\n-\n-Tool responses have a simple format: They are a message dict with the \"tool\" role, a \"name\" key giving the name\n-of the called function, and a \"content\" key containing the result of the tool call. Here is a sample tool response:\n-\n-```json\n-{\n-  \"role\": \"tool\",\n-  \"name\": \"multiply\",\n-  \"content\": \"30\"\n-}\n-```\n-\n-You don't need to use all of the keys in the tool response. For example, if your model doesn't expect the function\n-name to be included in the tool response, then rendering it can be as simple as:\n-\n-```text\n-{%- if message['role'] == 'tool' %}\n-    {{- \"<tool_result>\" + message['content'] + \"</tool_result>\" }}\n-{%- endif %}\n-```\n-\n-Again, remember that the actual formatting and special tokens are model-specific - you should take a lot of care\n-to ensure that tokens, whitespace and everything else exactly match the format your model was trained with!"
        },
        {
            "sha": "2179fa4779ad35830e0c146e38383a3dff79bb5e",
            "filename": "docs/source/en/chat_template_basics.md",
            "status": "removed",
            "additions": 0,
            "deletions": 287,
            "changes": 287,
            "blob_url": "https://github.com/huggingface/transformers/blob/6aa9888463ece40e29ac9065127f578a24f50958/docs%2Fsource%2Fen%2Fchat_template_basics.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/6aa9888463ece40e29ac9065127f578a24f50958/docs%2Fsource%2Fen%2Fchat_template_basics.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fchat_template_basics.md?ref=6aa9888463ece40e29ac9065127f578a24f50958",
            "patch": "@@ -1,287 +0,0 @@\n-<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# Getting Started with Chat Templates for Text LLMs\n-\n-An increasingly common use case for LLMs is **chat**. In a chat context, rather than continuing a single string\n-of text (as is the case with a standard language model), the model instead continues a conversation that consists\n-of one or more **messages**, each of which includes a **role**, like \"user\" or \"assistant\", as well as message text.\n-\n-Much like tokenization, different models expect very different input formats for chat. This is the reason we added\n-**chat templates** as a feature. Chat templates are part of the tokenizer for text-only LLMs or processor for multimodal LLMs. They specify how to convert conversations, represented as lists of messages, into a single tokenizable string in the format that the model expects. \n-\n-We'll explore the basic usage of chat templates with text-only LLMs in this page. For detailed guidance on multimodal models, we have a dedicated [documentation oage for multimodal models](./chat_template_multimodal), which covers how to work with image, video and audio inputs in your templates.\n-\n-Let's make this concrete with a quick example using the `mistralai/Mistral-7B-Instruct-v0.1` model:\n-\n-```python\n->>> from transformers import AutoTokenizer\n->>> tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n-\n->>> chat = [\n-...   {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n-...   {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n-...   {\"role\": \"user\", \"content\": \"I'd like to show off how chat templating works!\"},\n-... ]\n-\n->>> tokenizer.apply_chat_template(chat, tokenize=False)\n-\"<s>[INST] Hello, how are you? [/INST]I'm doing great. How can I help you today?</s> [INST] I'd like to show off how chat templating works! [/INST]\"\n-```\n-\n-Notice how the tokenizer has added the control tokens [INST] and [/INST] to indicate the start and end of \n-user messages (but not assistant messages!), and the entire chat is condensed into a single string. \n-If we use `tokenize=True`, which is the default setting, that string will also be tokenized for us.\n-\n-Now, try the same code, but swap in the `HuggingFaceH4/zephyr-7b-beta` model instead, and you should get:\n-\n-```text\n-<|user|>\n-Hello, how are you?</s>\n-<|assistant|>\n-I'm doing great. How can I help you today?</s>\n-<|user|>\n-I'd like to show off how chat templating works!</s>\n-```\n-\n-Both Zephyr and Mistral-Instruct were fine-tuned from the same base model, `Mistral-7B-v0.1`. However, they were trained\n-with totally different chat formats. Without chat templates, you would have to write manual formatting code for each\n-model, and it's very easy to make minor errors that hurt performance! Chat templates handle the details of formatting \n-for you, allowing you to write universal code that works for any model.\n-\n-\n-## How do I use chat templates?\n-\n-As you can see in the example above, chat templates are easy to use. Simply build a list of messages, with `role`\n-and `content` keys, and then pass it to the [`~PreTrainedTokenizer.apply_chat_template`] or [`~ProcessorMixin.apply_chat_template`] method\n-depending on what type of model you are using. Once you do that,\n-you'll get output that's ready to go! When using chat templates as input for model generation, it's also a good idea\n-to use `add_generation_prompt=True` to add a [generation prompt](#what-are-generation-prompts). \n-\n-Here's an example of preparing input for `model.generate()`, using `Zephyr` again:\n-\n-```python\n-from transformers import AutoModelForCausalLM, AutoTokenizer\n-\n-checkpoint = \"HuggingFaceH4/zephyr-7b-beta\"\n-tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n-model = AutoModelForCausalLM.from_pretrained(checkpoint)  # You may want to use bfloat16 and/or move to GPU here\n-\n-messages = [\n-    {\n-        \"role\": \"system\",\n-        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n-    },\n-    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n- ]\n-tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n-print(tokenizer.decode(tokenized_chat[0]))\n-```\n-This will yield a string in the input format that Zephyr expects. \n-```text\n-<|system|>\n-You are a friendly chatbot who always responds in the style of a pirate</s> \n-<|user|>\n-How many helicopters can a human eat in one sitting?</s> \n-<|assistant|>\n-```\n-\n-Now that our input is formatted correctly for Zephyr, we can use the model to generate a response to the user's question:\n-\n-```python\n-outputs = model.generate(tokenized_chat, max_new_tokens=128) \n-print(tokenizer.decode(outputs[0]))\n-```\n-\n-This will yield:\n-\n-```text\n-<|system|>\n-You are a friendly chatbot who always responds in the style of a pirate</s> \n-<|user|>\n-How many helicopters can a human eat in one sitting?</s> \n-<|assistant|>\n-Matey, I'm afraid I must inform ye that humans cannot eat helicopters. Helicopters are not food, they are flying machines. Food is meant to be eaten, like a hearty plate o' grog, a savory bowl o' stew, or a delicious loaf o' bread. But helicopters, they be for transportin' and movin' around, not for eatin'. So, I'd say none, me hearties. None at all.\n-```\n-\n-Arr, 'twas easy after all!\n-\n-\n-## Is there an automated pipeline for chat?\n-\n-Yes, there is! Our text generation pipelines support chat inputs, which makes it easy to use chat models. In the past,\n-we used to use a dedicated \"ConversationalPipeline\" class, but this has now been deprecated and its functionality\n-has been merged into the [`TextGenerationPipeline`]. Let's try the `Zephyr` example again, but this time using \n-a pipeline:\n-\n-```python\n-from transformers import pipeline\n-\n-pipe = pipeline(\"text-generation\", \"HuggingFaceH4/zephyr-7b-beta\")\n-messages = [\n-    {\n-        \"role\": \"system\",\n-        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n-    },\n-    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n-]\n-print(pipe(messages, max_new_tokens=128)[0]['generated_text'][-1])  # Print the assistant's response\n-```\n-\n-```text\n-{'role': 'assistant', 'content': \"Matey, I'm afraid I must inform ye that humans cannot eat helicopters. Helicopters are not food, they are flying machines. Food is meant to be eaten, like a hearty plate o' grog, a savory bowl o' stew, or a delicious loaf o' bread. But helicopters, they be for transportin' and movin' around, not for eatin'. So, I'd say none, me hearties. None at all.\"}\n-```\n-\n-The pipeline will take care of all the details of tokenization and calling `apply_chat_template` for you -\n-once the model has a chat template, all you need to do is initialize the pipeline and pass it the list of messages!\n-\n-\n-## What are \"generation prompts\"?\n-\n-You may have noticed that the `apply_chat_template` method has an `add_generation_prompt` argument. This argument tells\n-the template to add tokens that indicate the start of a bot response. For example, consider the following chat:\n-\n-```python\n-messages = [\n-    {\"role\": \"user\", \"content\": \"Hi there!\"},\n-    {\"role\": \"assistant\", \"content\": \"Nice to meet you!\"},\n-    {\"role\": \"user\", \"content\": \"Can I ask a question?\"}\n-]\n-```\n-\n-Here's what this will look like without a generation prompt, for a model that uses standard \"ChatML\" formatting:\n-\n-```python\n-tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n-\"\"\"<|im_start|>user\n-Hi there!<|im_end|>\n-<|im_start|>assistant\n-Nice to meet you!<|im_end|>\n-<|im_start|>user\n-Can I ask a question?<|im_end|>\n-\"\"\"\n-```\n-\n-And here's what it looks like **with** a generation prompt:\n-\n-```python\n-tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n-\"\"\"<|im_start|>user\n-Hi there!<|im_end|>\n-<|im_start|>assistant\n-Nice to meet you!<|im_end|>\n-<|im_start|>user\n-Can I ask a question?<|im_end|>\n-<|im_start|>assistant\n-\"\"\"\n-```\n-\n-Note that this time, we've added the tokens that indicate the start of a bot response. This ensures that when the model\n-generates text it will write a bot response instead of doing something unexpected, like continuing the user's \n-message. Remember, chat models are still just language models - they're trained to continue text, and chat is just a \n-special kind of text to them! You need to guide them with appropriate control tokens, so they know what they're \n-supposed to be doing.\n-\n-Not all models require generation prompts. Some models, like LLaMA, don't have any\n-special tokens before bot responses. In these cases, the `add_generation_prompt` argument will have no effect. The exact\n-effect that `add_generation_prompt` has will depend on the template being used.\n-\n-\n-## What does \"continue_final_message\" do?\n-\n-When passing a list of messages to `apply_chat_template` or `TextGenerationPipeline`, you can choose\n-to format the chat so the model will continue the final message in the chat instead of starting a new one. This is done\n-by removing any end-of-sequence tokens that indicate the end of the final message, so that the model will simply\n-extend the final message when it begins to generate text. This is useful for \"prefilling\" the model's response. \n-\n-Here's an example:\n-\n-```python\n-chat = [\n-    {\"role\": \"user\", \"content\": \"Can you format the answer in JSON?\"},\n-    {\"role\": \"assistant\", \"content\": '{\"name\": \"'},\n-]\n-\n-formatted_chat = tokenizer.apply_chat_template(chat, tokenize=True, return_dict=True, continue_final_message=True)\n-model.generate(**formatted_chat)\n-```\n-\n-The model will generate text that continues the JSON string, rather than starting a new message. This approach\n-can be very useful for improving the accuracy of the model's instruction-following when you know how you want\n-it to start its replies.\n-\n-Because `add_generation_prompt` adds the tokens that start a new message, and `continue_final_message` removes any\n-end-of-message tokens from the final message, it does not make sense to use them together. As a result, you'll\n-get an error if you try!\n-\n-<Tip>\n-\n-The default behaviour of `TextGenerationPipeline` is to set `add_generation_prompt=True` so that it starts a new\n-message. However, if the final message in the input chat has the \"assistant\" role, it will assume that this message is \n-a prefill and switch to `continue_final_message=True` instead, because most models do not support multiple \n-consecutive assistant messages. You can override this behaviour by explicitly passing the `continue_final_message` \n-argument when calling the pipeline.\n-\n-</Tip>\n-\n-\n-## Can I use chat templates in training?\n-\n-Yes! This is a good way to ensure that the chat template matches the tokens the model sees during training.\n-We recommend that you apply the chat template as a preprocessing step for your dataset. After this, you\n-can simply continue like any other language model training task. When training, you should usually set \n-`add_generation_prompt=False`, because the added tokens to prompt an assistant response will not be helpful during \n-training. Let's see an example:\n-\n-```python\n-from transformers import AutoTokenizer\n-from datasets import Dataset\n-\n-tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\")\n-\n-chat1 = [\n-    {\"role\": \"user\", \"content\": \"Which is bigger, the moon or the sun?\"},\n-    {\"role\": \"assistant\", \"content\": \"The sun.\"}\n-]\n-chat2 = [\n-    {\"role\": \"user\", \"content\": \"Which is bigger, a virus or a bacterium?\"},\n-    {\"role\": \"assistant\", \"content\": \"A bacterium.\"}\n-]\n-\n-dataset = Dataset.from_dict({\"chat\": [chat1, chat2]})\n-dataset = dataset.map(lambda x: {\"formatted_chat\": tokenizer.apply_chat_template(x[\"chat\"], tokenize=False, add_generation_prompt=False)})\n-print(dataset['formatted_chat'][0])\n-```\n-And we get:\n-```text\n-<|user|>\n-Which is bigger, the moon or the sun?</s>\n-<|assistant|>\n-The sun.</s>\n-```\n-\n-From here, just continue training like you would with a standard language modelling task, using the `formatted_chat` column.\n-\n-<Tip>\n-\n-By default, some tokenizers add special tokens like `<bos>` and `<eos>` to text they tokenize. Chat templates should \n-already include all the special tokens they need, and so additional special tokens will often be incorrect or \n-duplicated, which will hurt model performance.\n-\n-Therefore, if you format text with `apply_chat_template(tokenize=False)`, you should set the argument\n-`add_special_tokens=False` when you tokenize that text later. If you use `apply_chat_template(tokenize=True)`, you don't need to worry about this!\n-\n-</Tip>\n-"
        },
        {
            "sha": "1b283449605b71fbdbf670b6e5dc9a9f047dcaa4",
            "filename": "docs/source/en/chat_template_multimodal.md",
            "status": "removed",
            "additions": 0,
            "deletions": 289,
            "changes": 289,
            "blob_url": "https://github.com/huggingface/transformers/blob/6aa9888463ece40e29ac9065127f578a24f50958/docs%2Fsource%2Fen%2Fchat_template_multimodal.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/6aa9888463ece40e29ac9065127f578a24f50958/docs%2Fsource%2Fen%2Fchat_template_multimodal.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fchat_template_multimodal.md?ref=6aa9888463ece40e29ac9065127f578a24f50958",
            "patch": "@@ -1,289 +0,0 @@\n-<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# Multimodal Chat Templates for Vision and Audio LLMs\n-\n-In this section, we'll explore how to use chat templates with multimodal models, enabling your templates to handle a variety of inputs such as text, images, and audio. Multimodal models provide richer, more interactive experiences, and understanding how to effectively combine these inputs within your templates is key. Weâ€™ll walk through how to work with different modalities, configure your templates for optimal performance, and tackle common challenges along the way.\n-\n-Just like with text-only LLMs, multimodal models expect a chat with **messages**, each of which includes a **role** and **content**. However, for multimodal models, chat templates are a part of the [Processor](./main_cllasses/processors) class. Let's see how we can format our prompts when there are images or videos in the input along with text.\n-\n-\n-## Image inputs\n-\n-For models such as [LLaVA](https://huggingface.co/llava-hf) the prompts can be formatted as below. Notice that the only difference from text-only models is that we need to also pass a placeholder for input images. To accommodate for extra modalities, each **content** is a list containing either a text or an image **type**.\n-\n-Let's make this concrete with a quick example using the `llava-hf/llava-onevision-qwen2-0.5b-ov-hf` model:\n-\n-```python\n-from transformers import AutoProcessor, LlavaOnevisionForConditionalGeneration\n-\n-model_id = \"llava-hf/llava-onevision-qwen2-0.5b-ov-hf\"\n-processor = AutoProcessor.from_pretrained(model_id)\n-\n-messages = [\n-    {\n-        \"role\": \"system\",\n-        \"content\": [{\"type\": \"text\", \"text\": \"You are a friendly chatbot who always responds in the style of a pirate\"}],\n-    },\n-    {\n-      \"role\": \"user\",\n-      \"content\": [\n-            {\"type\": \"image\"},\n-            {\"type\": \"text\", \"text\": \"What are these?\"},\n-        ],\n-    },\n-]\n-\n-formatted_prompt = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n-print(formatted_prompt)\n-```\n-\n-This yields a string in LLaVA's expected input format with many `<image>` tokens prepended before the text.\n-```text\n-'<|im_start|>system \n-<|im_start|>system \n-You are a friendly chatbot who always responds in the style of a pirate<|im_end|><|im_start|>user <image>\n-What are these?<|im_end|>\n-```\n-\n-\n-### Image paths or URLs\n-\n-To incorporate images into your chat templates, you can pass them as file paths or URLs. This method automatically loads the image, processes it, and prepares the necessary pixel values to create ready-to-use inputs for the model. This approach simplifies the integration of images, enabling seamless multimodal functionality.\n-\n-Let's see how it works with an example using the same model as above. This time we'll indicate an image URL with `\"url\"` key in the message's **content** and ask the chat template to `tokenize` and `return_dict`. Currently, \"base64\", \"url\", and \"path\" are supported image sources.\n-\n-```python\n-from transformers import AutoProcessor, LlavaOnevisionForConditionalGeneration\n-\n-model_id = \"llava-hf/llava-onevision-qwen2-0.5b-ov-hf\"\n-model = LlavaOnevisionForConditionalGeneration.from_pretrained(model_id)\n-processor = AutoProcessor.from_pretrained(model_id)\n-\n-messages = [\n-    {\n-        \"role\": \"system\",\n-        \"content\": [{\"type\": \"text\", \"text\": \"You are a friendly chatbot who always responds in the style of a pirate\"}],\n-    },\n-    {\n-      \"role\": \"user\",\n-      \"content\": [\n-            {\"type\": \"image\", \"url\": \"http://images.cocodataset.org/val2017/000000039769.jpg\"},\n-            {\"type\": \"text\", \"text\": \"What are these?\"},\n-        ],\n-    },\n-]\n-\n-processed_chat = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\")\n-print(processed_chat.keys())\n-```\n-\n-This yields a dictionary with inputs processed and ready to be further passed into [`~GenerationMixin.generate`] to generate text.\n-```text\n-dict_keys([\"input_ids\", \"attention_mask\", \"pixel_values\", \"image_sizes\"])\n-```\n-\n-\n-## Video inputs\n-\n-Some vision models support videos as inputs as well as images. The message format is very similar to the image-only models with tiny differences to handle loading videos from a URL. We can continue using the same model as before since it supports videos.\n-\n-### Sampling with fixed number of frames\n-\n-Here's an example of how to set up a conversation with video inputs. Notice the extra `kwargs` passed to `processor.apply_chat_template()`. The key parameter here is `num_frames`, which controls how many frames to sample uniformly from the video. Each model checkpoint has a maximum frame count it was trained with, and exceeding this limit can significantly impact generation quality. So, itâ€™s important to choose a frame count that fits both the model's capacity and your computational resources. If you don't specify `num_frames`, the entire video will be loaded without any frame sampling.\n-\n-You also have the option to choose a specific framework to load the video, depending on your preferences or needs. Currently, we support `decord`, `pyav` (the default), `opencv`, and `torchvision`. For this example, weâ€™ll use `decord`, as it's a bit faster than `pyav`.\n-\n-\n-<Tip>\n-\n-Note that if you are trying to load a video from URL, you can decode the video only with `pyav` or `decord` as backend.\n-\n-</Tip>\n-\n-\n-```python\n-from transformers import AutoProcessor, LlavaOnevisionForConditionalGeneration\n-\n-model_id = \"llava-hf/llava-onevision-qwen2-0.5b-ov-hf\"\n-model = LlavaOnevisionForConditionalGeneration.from_pretrained(model_id)\n-processor = AutoProcessor.from_pretrained(model_id)\n-\n-messages = [\n-    {\n-        \"role\": \"system\",\n-        \"content\": [{\"type\": \"text\", \"text\": \"You are a friendly chatbot who always responds in the style of a pirate\"}],\n-    },\n-    {\n-      \"role\": \"user\",\n-      \"content\": [\n-            {\"type\": \"video\", \"url\": \"https://test-videos.co.uk/vids/bigbuckbunny/mp4/h264/720/Big_Buck_Bunny_720_10s_10MB.mp4\"},\n-            {\"type\": \"text\", \"text\": \"What do you see in this video?\"},\n-        ],\n-    },\n-]\n-\n-processed_chat = processor.apply_chat_template(\n-    messages,\n-    add_generation_prompt=True,\n-    tokenize=True,\n-    return_dict=True,\n-    return_tensors=\"pt\",\n-    num_frames=32,\n-    video_load_backend=\"decord\",\n-)\n-print(processed_chat.keys())\n-```\n-\n-### Sampling with FPS\n-\n-When working with long videos, you might want to sample more frames for better representation. Instead of a fixed number of frames, you can specify `video_fps`, which determines how many frames per second to extract. For example, if a video is **10 seconds long** and you set `video_fps=2`, the model will sample **20 frames** (2 per second, uniformly spaced). \n-\n-Using the above model, we need to apply chat template as follows to sample 2 frames per second.\n-\n-```python\n-processed_chat = processor.apply_chat_template(\n-    messages,\n-    add_generation_prompt=True,\n-    tokenize=True,\n-    return_dict=True,\n-    video_fps=32,\n-    video_load_backend=\"decord\",\n-)\n-print(processed_chat.keys())\n-```\n-\n-\n-### Custom Frame Sampling with a Function  \n-\n-Not all models sample frames **uniformly** â€” some require more complex logic to determine which frames to use. If your model follows a different sampling strategy, you can **customize** frame selection by providing a function:  \n-\n-ðŸ”¹ Use the `sample_indices_fn` argument to pass a **callable function** for sampling.  \n-ðŸ”¹ If provided, this function **overrides** standard `num_frames` and `fps` methods.  \n-ðŸ”¹ It receives all the arguments passed to `load_video` and must return **valid frame indices** to sample.  \n-\n-You should use `sample_indices_fn` when:\n-\n-- If you need a custom sampling strategy (e.g., **adaptive frame selection** instead of uniform sampling).  \n-- If your model prioritizes **key moments** in a video rather than evenly spaced frames.  \n-\n-Hereâ€™s an example of how to implement it:  \n-\n-\n-```python\n-\n-def sample_indices_fn(metadata, **kwargs):\n-    # samples only the first and the second frame\n-    return [0, 1]\n-\n-processed_chat = processor.apply_chat_template(\n-    messages,\n-    add_generation_prompt=True,\n-    tokenize=True,\n-    return_dict=True,\n-    sample_indices_fn=sample_indices_fn,\n-    video_load_backend=\"decord\",\n-)\n-print(processed_chat.keys())\n-```\n-\n-By using `sample_indices_fn`, you gain **full control** over frame selection, making your model **more adaptable** to different video scenarios. ðŸš€  \n-\n-\n-### List of image frames as video\n-\n-Sometimes, instead of having a full video file, you might only have a set of sampled frames stored as images.\n-\n-You can pass a list of image file paths, and the processor will automatically concatenate them into a video. Just make sure that all images have the same size, as they are assumed to be from the same video.\n-\n-\n-```python\n-frames_paths = [\"/path/to/frame0.png\", \"/path/to/frame5.png\", \"/path/to/frame10.png\"]\n-messages = [\n-    {\n-        \"role\": \"system\",\n-        \"content\": [{\"type\": \"text\", \"text\": \"You are a friendly chatbot who always responds in the style of a pirate\"}],\n-    },\n-    {\n-      \"role\": \"user\",\n-      \"content\": [\n-            {\"type\": \"video\", \"path\": frames_paths},\n-            {\"type\": \"text\", \"text\": \"What do you see in this video?\"},\n-        ],\n-    },\n-]\n-\n-processed_chat = processor.apply_chat_template(\n-    messages,\n-    add_generation_prompt=True,\n-    tokenize=True,\n-    return_dict=True,\n-)\n-print(processed_chat.keys())\n-```\n-\n-\n-## Multimodal conversational pipeline\n-\n-[`ImageTextToTextPipeline`] currently accepts images as inputs but we are planning to add support for video inputs in the future. The pipeline supports chat inputs in the same format as we have seen above. Apart from that, the pipeline will accept chats in OpenAI format. This format is supported exclusively within the pipeline to make inference easier and more accessible. \n-\n-Here is how the OpenAI conversation format looks:\n-\n-```python\n-messages = [\n-    {\n-        \"role\": \"user\",\n-        \"content\": [\n-            {\n-                \"type\": \"text\",\n-                \"text\": \"What is in this image?\",\n-            },\n-            {\n-                \"type\": \"image_url\",\n-                \"image_url\": {\"url\": f\"http://images.cocodataset.org/val2017/000000039769.jpg\"},\n-            },\n-        ],\n-    }\n-]\n-```\n-\n-## Best Practices for Multimodal Template Configuration\n-\n-\n-To add a custom chat template for your multimodal LLM, simply create your template using [Jinja](https://jinja.palletsprojects.com/en/3.1.x/templates/) and set it with `processor.chat_template`. If you're new to writing chat templates or need some tips, check out our [tutorial here](./chat_template_advanced) for helpful guidance.\n-\n-In some cases, you may want your template to handle a **list of content** from multiple modalities, while still supporting a plain string for text-only inference. Here's an example of how you can achieve that, using the [Llama-Vision](https://huggingface.co/collections/meta-llama/metas-llama-32-multimodal-models-675bfd70e574a62dd0e4059b) chat template.\n-\n-\n-```\n-{% for message in messages %}\n-{% if loop.index0 == 0 %}{{ bos_token }}{% endif %}\n-{{ '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' }}\n-{% if message['content'] is string %}\n-{{ message['content'] }}\n-{% else %}\n-{% for content in message['content'] %}\n-{% if content['type'] == 'image' %}\n-{{ '<|image|>' }}\n-{% elif content['type'] == 'text' %}\n-{{ content['text'] }}\n-{% endif %}\n-{% endfor %}\n-{% endif %}\n-{{ '<|eot_id|>' }}\n-{% endfor %}\n-{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\n-```"
        },
        {
            "sha": "6c5491a2484a50106773b4d31763a4c10cdf7fd7",
            "filename": "docs/source/en/chat_template_tools_and_documents.md",
            "status": "removed",
            "additions": 0,
            "deletions": 410,
            "changes": 410,
            "blob_url": "https://github.com/huggingface/transformers/blob/6aa9888463ece40e29ac9065127f578a24f50958/docs%2Fsource%2Fen%2Fchat_template_tools_and_documents.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/6aa9888463ece40e29ac9065127f578a24f50958/docs%2Fsource%2Fen%2Fchat_template_tools_and_documents.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fchat_template_tools_and_documents.md?ref=6aa9888463ece40e29ac9065127f578a24f50958",
            "patch": "@@ -1,410 +0,0 @@\n-<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-\n-# Expanding Chat Templates with Tools and Documents\n-\n-The only argument that `apply_chat_template` requires is `messages`. However, you can pass any keyword\n-argument to `apply_chat_template` and it will be accessible inside the template. This gives you a lot of freedom to use\n-chat templates for many things. There are no restrictions on the names or the format of these arguments - you can pass\n-strings, lists, dicts or whatever else you want. \n-\n-That said, there are some common use-cases for these extra arguments,\n-such as passing tools for function calling, or documents for retrieval-augmented generation. In these common cases,\n-we have some opinionated recommendations about what the names and formats of these arguments should be, which are\n-described in the sections below. We encourage model authors to make their chat templates compatible with this format,\n-to make it easy to transfer tool-calling code between models.\n-\n-## Tool use / function calling\n-\n-\"Tool use\" LLMs can choose to call functions as external tools before generating an answer. When passing tools\n-to a tool-use model, you can simply pass a list of functions to the `tools` argument:\n-\n-```python\n-import datetime\n-\n-def current_time():\n-    \"\"\"Get the current local time as a string.\"\"\"\n-    return str(datetime.now())\n-\n-def multiply(a: float, b: float):\n-    \"\"\"\n-    A function that multiplies two numbers\n-    \n-    Args:\n-        a: The first number to multiply\n-        b: The second number to multiply\n-    \"\"\"\n-    return a * b\n-\n-tools = [current_time, multiply]\n-\n-model_input = tokenizer.apply_chat_template(\n-    messages,\n-    tools=tools\n-)\n-```\n-\n-In order for this to work correctly, you should write your functions in the format above, so that they can be parsed\n-correctly as tools. Specifically, you should follow these rules:\n-\n-- The function should have a descriptive name\n-- Every argument must have a type hint\n-- The function must have a docstring in the standard Google style (in other words, an initial function description  \n-  followed by an `Args:` block that describes the arguments, unless the function does not have any arguments.) \n-- Do not include types in the `Args:` block. In other words, write `a: The first number to multiply`, not\n-  `a (int): The first number to multiply`. Type hints should go in the function header instead.\n-- The function can have a return type and a `Returns:` block in the docstring. However, these are optional\n-  because most tool-use models ignore them.\n-\n-### Passing tool results to the model\n-\n-The sample code above is enough to list the available tools for your model, but what happens if it wants to actually use\n-one? If that happens, you should:\n-\n-1. Parse the model's output to get the tool name(s) and arguments.\n-2. Add the model's tool call(s) to the conversation.\n-3. Call the corresponding function(s) with those arguments.\n-4. Add the result(s) to the conversation\n-\n-### A complete tool use example\n-\n-Let's walk through a tool use example, step by step. For this example, we will use an 8B `Hermes-2-Pro` model,\n-as it is one of the highest-performing tool-use models in its size category at the time of writing. If you have the\n-memory, you can consider using a larger model instead like [Command-R](https://huggingface.co/CohereForAI/c4ai-command-r-v01)\n-or [Mixtral-8x22B](https://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1), both of which also support tool use\n-and offer even stronger performance.\n-\n-First, let's load our model and tokenizer:\n-\n-```python\n-import torch\n-from transformers import AutoModelForCausalLM, AutoTokenizer\n-\n-checkpoint = \"NousResearch/Hermes-2-Pro-Llama-3-8B\"\n-\n-tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n-model = AutoModelForCausalLM.from_pretrained(checkpoint, torch_dtype=torch.bfloat16, device_map=\"auto\")\n-```\n-\n-Next, let's define a list of tools:\n-\n-```python\n-def get_current_temperature(location: str, unit: str) -> float:\n-    \"\"\"\n-    Get the current temperature at a location.\n-    \n-    Args:\n-        location: The location to get the temperature for, in the format \"City, Country\"\n-        unit: The unit to return the temperature in. (choices: [\"celsius\", \"fahrenheit\"])\n-    Returns:\n-        The current temperature at the specified location in the specified units, as a float.\n-    \"\"\"\n-    return 22.  # A real function should probably actually get the temperature!\n-\n-def get_current_wind_speed(location: str) -> float:\n-    \"\"\"\n-    Get the current wind speed in km/h at a given location.\n-    \n-    Args:\n-        location: The location to get the temperature for, in the format \"City, Country\"\n-    Returns:\n-        The current wind speed at the given location in km/h, as a float.\n-    \"\"\"\n-    return 6.  # A real function should probably actually get the wind speed!\n-\n-tools = [get_current_temperature, get_current_wind_speed]\n-```\n-\n-Now, let's set up a conversation for our bot:\n-\n-```python\n-messages = [\n-  {\"role\": \"system\", \"content\": \"You are a bot that responds to weather queries. You should reply with the unit used in the queried location.\"},\n-  {\"role\": \"user\", \"content\": \"Hey, what's the temperature in Paris right now?\"}\n-]\n-```\n-\n-Now, let's apply the chat template and generate a response:\n-\n-```python\n-inputs = tokenizer.apply_chat_template(messages, tools=tools, add_generation_prompt=True, return_dict=True, return_tensors=\"pt\")\n-inputs = {k: v.to(model.device) for k, v in inputs.items()}\n-out = model.generate(**inputs, max_new_tokens=128)\n-print(tokenizer.decode(out[0][len(inputs[\"input_ids\"][0]):]))\n-```\n-\n-And we get:\n-\n-```text\n-<tool_call>\n-{\"arguments\": {\"location\": \"Paris, France\", \"unit\": \"celsius\"}, \"name\": \"get_current_temperature\"}\n-</tool_call><|im_end|>\n-```\n-\n-The model has called the function with valid arguments, in the format requested by the function docstring. It has\n-inferred that we're most likely referring to the Paris in France, and it remembered that, as the home of SI units,\n-the temperature in France should certainly be displayed in Celsius.\n-\n-<Tip>\n-\n-The output format above is specific to the `Hermes-2-Pro` model we're using in this example. Other models may emit different\n-tool call formats, and you may need to do some manual parsing at this step. For example, `Llama-3.1` models will emit\n-slightly different JSON, with `parameters` instead of `arguments`. Regardless of the format the model outputs, you \n-should add the tool call to the conversation in the format below, with `tool_calls`, `function` and `arguments` keys. \n-\n-</Tip>\n-\n-Next, let's append the model's tool call to the conversation.\n-\n-```python\n-tool_call = {\"name\": \"get_current_temperature\", \"arguments\": {\"location\": \"Paris, France\", \"unit\": \"celsius\"}}\n-messages.append({\"role\": \"assistant\", \"tool_calls\": [{\"type\": \"function\", \"function\": tool_call}]})\n-```\n-\n-<Tip warning={true}>\n-\n-If you're familiar with the OpenAI API, you should pay attention to an important difference here - the `tool_call` is\n-a dict, but in the OpenAI API it's a JSON string. Passing a string may cause errors or strange model behaviour!\n-\n-</Tip>\n-\n-Now that we've added the tool call to the conversation, we can call the function and append the result to the\n-conversation. Since we're just using a dummy function for this example that always returns 22.0, we can just append \n-that result directly.\n-\n-```python\n-messages.append({\"role\": \"tool\", \"name\": \"get_current_temperature\", \"content\": \"22.0\"})\n-```\n-\n-<Tip>\n-\n-Some model architectures, notably Mistral/Mixtral, also require a `tool_call_id` here, which should be\n-9 randomly-generated alphanumeric characters, and assigned to the `id` key of the tool call\n-dictionary. The same key should also be assigned to the `tool_call_id` key of the tool response dictionary below, so \n-that tool calls can be matched to tool responses. So, for Mistral/Mixtral models, the code above would be:\n-\n-```python\n-tool_call_id = \"9Ae3bDc2F\"  # Random ID, 9 alphanumeric characters\n-tool_call = {\"name\": \"get_current_temperature\", \"arguments\": {\"location\": \"Paris, France\", \"unit\": \"celsius\"}}\n-messages.append({\"role\": \"assistant\", \"tool_calls\": [{\"type\": \"function\", \"id\": tool_call_id, \"function\": tool_call}]})\n-```\n-\n-and\n-\n-```python\n-messages.append({\"role\": \"tool\", \"tool_call_id\": tool_call_id, \"name\": \"get_current_temperature\", \"content\": \"22.0\"})\n-```\n-\n-</Tip>\n-\n-Finally, let's let the assistant read the function outputs and continue chatting with the user:\n-\n-```python\n-inputs = tokenizer.apply_chat_template(messages, tools=tools, add_generation_prompt=True, return_dict=True, return_tensors=\"pt\")\n-inputs = {k: v.to(model.device) for k, v in inputs.items()}\n-out = model.generate(**inputs, max_new_tokens=128)\n-print(tokenizer.decode(out[0][len(inputs[\"input_ids\"][0]):]))\n-```\n-\n-And we get:\n-\n-```text\n-The current temperature in Paris, France is 22.0 Â° Celsius.<|im_end|>\n-```\n-\n-Although this was a simple demo with dummy tools and a single call, the same technique works with \n-multiple real tools and longer conversations. This can be a powerful way to extend the capabilities of conversational\n-agents with real-time information, computational tools like calculators, or access to large databases.\n-\n-### Understanding tool schemas\n-\n-Each function you pass to the `tools` argument of `apply_chat_template` is converted into a \n-[JSON schema](https://json-schema.org/learn/getting-started-step-by-step). These schemas\n-are then passed to the model chat template. In other words, tool-use models do not see your functions directly, and they\n-never see the actual code inside them. What they care about is the function **definitions** and the **arguments** they\n-need to pass to them - they care about what the tools do and how to use them, not how they work! It is up to you\n-to read their outputs, detect if they have requested to use a tool, pass their arguments to the tool function, and\n-return the response in the chat.\n-\n-Generating JSON schemas to pass to the template should be automatic and invisible as long as your functions\n-follow the specification above, but if you encounter problems, or you simply want more control over the conversion, \n-you can handle the conversion manually. Here is an example of a manual schema conversion.\n-\n-```python\n-from transformers.utils import get_json_schema\n-\n-def multiply(a: float, b: float):\n-    \"\"\"\n-    A function that multiplies two numbers\n-    \n-    Args:\n-        a: The first number to multiply\n-        b: The second number to multiply\n-    \"\"\"\n-    return a * b\n-\n-schema = get_json_schema(multiply)\n-print(schema)\n-```\n-\n-This will yield:\n-\n-```json\n-{\n-  \"type\": \"function\", \n-  \"function\": {\n-    \"name\": \"multiply\", \n-    \"description\": \"A function that multiplies two numbers\", \n-    \"parameters\": {\n-      \"type\": \"object\", \n-      \"properties\": {\n-        \"a\": {\n-          \"type\": \"number\", \n-          \"description\": \"The first number to multiply\"\n-        }, \n-        \"b\": {\n-          \"type\": \"number\",\n-          \"description\": \"The second number to multiply\"\n-        }\n-      }, \n-      \"required\": [\"a\", \"b\"]\n-    }\n-  }\n-}\n-```\n-\n-If you wish, you can edit these schemas, or even write them from scratch yourself without using `get_json_schema` at \n-all. JSON schemas can be passed directly to the `tools` argument of \n-`apply_chat_template` - this gives you a lot of power to define precise schemas for more complex functions. Be careful,\n-though - the more complex your schemas, the more likely the model is to get confused when dealing with them! We \n-recommend simple function signatures where possible, keeping arguments (and especially complex, nested arguments) \n-to a minimum.\n-\n-Here is an example of defining schemas by hand, and passing them directly to `apply_chat_template`:\n-\n-```python\n-# A simple function that takes no arguments\n-current_time = {\n-  \"type\": \"function\", \n-  \"function\": {\n-    \"name\": \"current_time\",\n-    \"description\": \"Get the current local time as a string.\",\n-    \"parameters\": {\n-      'type': 'object',\n-      'properties': {}\n-    }\n-  }\n-}\n-\n-# A more complete function that takes two numerical arguments\n-multiply = {\n-  'type': 'function',\n-  'function': {\n-    'name': 'multiply',\n-    'description': 'A function that multiplies two numbers', \n-    'parameters': {\n-      'type': 'object', \n-      'properties': {\n-        'a': {\n-          'type': 'number',\n-          'description': 'The first number to multiply'\n-        }, \n-        'b': {\n-          'type': 'number', 'description': 'The second number to multiply'\n-        }\n-      }, \n-      'required': ['a', 'b']\n-    }\n-  }\n-}\n-\n-model_input = tokenizer.apply_chat_template(\n-    messages,\n-    tools = [current_time, multiply]\n-)\n-```\n-\n-## Retrieval-augmented generation\n-\n-\"Retrieval-augmented generation\" or \"RAG\" LLMs can search a corpus of documents for information before responding\n-to a query. This allows models to vastly expand their knowledge base beyond their limited context size. Our \n-recommendation for RAG models is that their template\n-should accept a `documents` argument. This should be a list of documents, where each \"document\"\n-is a single dict with `title` and `contents` keys, both of which are strings. Because this format is much simpler\n-than the JSON schemas used for tools, no helper functions are necessary.\n-\n-Here's an example of a RAG template in action:\n-\n-```python\n-from transformers import AutoTokenizer, AutoModelForCausalLM\n-\n-# Load the model and tokenizer\n-model_id = \"CohereForAI/c4ai-command-r-v01-4bit\"\n-tokenizer = AutoTokenizer.from_pretrained(model_id)\n-model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n-device = model.device # Get the device the model is loaded on\n-\n-# Define conversation input\n-conversation = [\n-    {\"role\": \"user\", \"content\": \"What has Man always dreamed of?\"}\n-]\n-\n-# Define documents for retrieval-based generation\n-documents = [\n-    {\n-        \"title\": \"The Moon: Our Age-Old Foe\", \n-        \"text\": \"Man has always dreamed of destroying the moon. In this essay, I shall...\"\n-    },\n-    {\n-        \"title\": \"The Sun: Our Age-Old Friend\",\n-        \"text\": \"Although often underappreciated, the sun provides several notable benefits...\"\n-    }\n-]\n-\n-# Tokenize conversation and documents using a RAG template, returning PyTorch tensors.\n-input_ids = tokenizer.apply_chat_template(\n-    conversation=conversation,\n-    documents=documents,\n-    chat_template=\"rag\",\n-    tokenize=True,\n-    add_generation_prompt=True,\n-    return_tensors=\"pt\").to(device)\n-\n-# Generate a response \n-gen_tokens = model.generate(\n-    input_ids,\n-    max_new_tokens=100,\n-    do_sample=True,\n-    temperature=0.3,\n-    )\n-\n-# Decode and print the generated text along with generation prompt\n-gen_text = tokenizer.decode(gen_tokens[0])\n-print(gen_text)\n-```\n-\n-<Tip>\n-\n-The `documents` input for retrieval-augmented generation is not widely supported, and many models have chat templates which simply ignore this input.\n-\n-To verify if a model supports the `documents` input, you can read its model card, or `print(tokenizer.chat_template)` to see if the `documents` key is used anywhere.\n-\n-One model class that does support it, though, is Cohere's [Command-R](https://huggingface.co/CohereForAI/c4ai-command-r-08-2024) and [Command-R+](https://huggingface.co/CohereForAI/c4ai-command-r-plus-08-2024), through their `rag` chat template. You can see additional examples of grounded generation using this feature in their model cards.\n-\n-</Tip>\n-\n-"
        },
        {
            "sha": "d11b8fa5b4046298d0f980204994fd79e4dcafc0",
            "filename": "docs/source/en/chat_templating.md",
            "status": "added",
            "additions": 229,
            "deletions": 0,
            "changes": 229,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fchat_templating.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fchat_templating.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fchat_templating.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -0,0 +1,229 @@\n+<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# Templates\n+\n+The [chat pipeline](./conversations) guide introduced [`TextGenerationPipeline`] and the concept of a chat prompt or chat template for conversing with a model. Underlying this high-level pipeline is the [`apply_chat_template`] method. A chat template is a part of the tokenizer and it specifies how to convert conversations into a single tokenizable string in the expected model format.\n+\n+In the example below, Mistral-7B-Instruct and Zephyr-7B are finetuned from the same base model but theyâ€™re trained with different chat formats. Without chat templates, you have to manually write formatting code for each model and even minor errors can hurt performance. Chat templates offer a universal way to format chat inputs to any model.\n+\n+<hfoptions id=\"template\">\n+<hfoption id=\"Mistral\">\n+\n+```py\n+from transformers import AutoTokenizer\n+\n+tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n+chat = [\n+  {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n+  {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n+  {\"role\": \"user\", \"content\": \"I'd like to show off how chat templating works!\"},\n+]\n+\n+tokenizer.apply_chat_template(chat, tokenize=False)\n+```\n+```md\n+<s>[INST] Hello, how are you? [/INST]I'm doing great. How can I help you today?</s> [INST] I'd like to show off how chat templating works! [/INST]\n+```\n+\n+</hfoption>\n+<hfoption id=\"Zephyr\">\n+\n+```py\n+from transformers import AutoTokenizer\n+\n+tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\")\n+chat = [\n+  {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n+  {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n+  {\"role\": \"user\", \"content\": \"I'd like to show off how chat templating works!\"},\n+]\n+\n+tokenizer.apply_chat_template(chat, tokenize=False)\n+```\n+```md\n+<|user|>\\nHello, how are you?</s>\\n<|assistant|>\\nI'm doing great. How can I help you today?</s>\\n<|user|>\\nI'd like to show off how chat templating works!</s>\\n\n+```\n+\n+</hfoption>\n+</hfoptions>\n+\n+This guide explores [`apply_chat_template`] and chat templates in more detail.\n+\n+## apply_chat_template\n+\n+Chats should be structured as a list of dictionaries with `role` and `content` keys. The `role` key specifies the speaker (usually between you and the system), and the `content` key contains your message. For the system, the `content` is a high-level description of how the model should behave and respond when youâ€™re chatting with it.\n+\n+Pass your messages to [`apply_chat_template`] to tokenize and format them. You can set [add_generation_prompt](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template.add_generation_prompt) to `True` to indicate the start of a message.\n+\n+```py\n+import torch\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n+\n+tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\")\n+model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\", device_map=\"auto\", torch_dtype=torch.bfloat16)\n+\n+messages = [\n+    {\"role\": \"system\", \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",},\n+    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n+ ]\n+tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n+print(tokenizer.decode(tokenized_chat[0]))\n+```\n+```md\n+<|system|>\n+You are a friendly chatbot who always responds in the style of a pirate</s>\n+<|user|>\n+How many helicopters can a human eat in one sitting?</s>\n+<|assistant|>\n+```\n+\n+Now pass the tokenized chat to [`~GenerationMixin.generate`] to generate a response.\n+\n+```py\n+outputs = model.generate(tokenized_chat, max_new_tokens=128) \n+print(tokenizer.decode(outputs[0]))\n+```\n+```md\n+<|system|>\n+You are a friendly chatbot who always responds in the style of a pirate</s>\n+<|user|>\n+How many helicopters can a human eat in one sitting?</s>\n+<|assistant|>\n+Matey, I'm afraid I must inform ye that humans cannot eat helicopters. Helicopters are not food, they are flying machines. Food is meant to be eaten, like a hearty plate o' grog, a savory bowl o' stew, or a delicious loaf o' bread. But helicopters, they be for transportin' and movin' around, not for eatin'. So, I'd say none, me hearties. None at all.\n+```\n+\n+### add_generation_prompt\n+The [add_generation_prompt](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template.add_generation_prompt) parameter adds tokens that indicate the start of a response. This ensures the chat model generates a system response instead of continuing a users message.\n+\n+Not all models require generation prompts, and some models, like [Llama](./model_doc/llama), donâ€™t have any special tokens before the system response. In this case, [add_generation_prompt](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template.add_generation_prompt) has no effect.\n+\n+```py\n+tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n+tokenized_chat\n+```\n+```md\n+<|im_start|>user\n+Hi there!<|im_end|>\n+<|im_start|>assistant\n+Nice to meet you!<|im_end|>\n+<|im_start|>user\n+Can I ask a question?<|im_end|>\n+```\n+\n+### continue_final_message\n+\n+The [continue_final_message](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template.continue_final_message) parameter controls whether the final message in the chat should be continued or not instead of starting a new one. It removes end of sequence tokens so that the model continues generation from the final message.\n+\n+This is useful for â€œprefillingâ€ a model response. In the example below, the model generates text that continues the JSON string rather than starting a new message. It can be very useful for improving the accuracy for instruction following when you know how to start its replies.\n+\n+```py\n+chat = [\n+    {\"role\": \"user\", \"content\": \"Can you format the answer in JSON?\"},\n+    {\"role\": \"assistant\", \"content\": '{\"name\": \"'},\n+]\n+\n+formatted_chat = tokenizer.apply_chat_template(chat, tokenize=True, return_dict=True, continue_final_message=True)\n+model.generate(**formatted_chat)\n+```\n+\n+> [!WARNING]\n+> You shouldnâ€™t use [add_generation_prompt](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template.add_generation_prompt) and [continue_final_message](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template.continue_final_message) together. The former adds tokens that start a new message, while the latter removes end of sequence tokens. Using them together returns an error.\n+\n+[`TextGenerationPipeline`] sets [add_generation_prompt](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template.add_generation_prompt) to `True` by default to start a new message. However, if the final message in the chat has the â€œassistantâ€ role, it assumes the message is a prefill and switches to `continue_final_message=True`. This is because most models donâ€™t support multiple consecutive assistant messages. To override this behavior, explicitly pass the [continue_final_message](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template.continue_final_message) to the pipeline.\n+\n+## Multiple templates\n+\n+A model may have several different templates for different use cases. For example, a model may have a template for regular chat, tool use, and RAG.\n+\n+When there are multiple templates, the chat template is a dictionary. Each key corresponds to the name of a template. [`apply_chat_template`] handles multiple templates based on their name. It looks for a template named `default` in most cases and if it canâ€™t find one, it raises an error.\n+\n+For a tool calling template, if a user passes a `tools` parameter and a `tool_use` template exists, the tool calling template is used instead of `default`.\n+\n+To access templates with other names, pass the template name to the `chat_template` parameter in [`apply_chat_template`]. For example, if youâ€™re using a RAG template then set `chat_template=\"rag\"`.\n+\n+It can be confusing to manage multiple templates though, so we recommend using a single template for all use cases. Use Jinja statements like `if tools is defined` and `{% macro %}` definitions to wrap multiple code paths in a single template.\n+\n+## Template selection\n+\n+It is important to set a chat template format that matches the template format a model was pretrained on, otherwise performance may suffer. Even if youâ€™re training the model further, performance is best if the chat tokens are kept constant.\n+\n+But if youâ€™re training a model from scratch or finetuning a model for chat, you have more options to select a template. For example, [ChatML](https://github.com/openai/openai-python/blob/release-v0.28.0/chatml.md) is a popular format that is flexbile enough to handle many use cases. It even includes support for [generation prompts](#add_generation_prompt), but it doesnâ€™t add beginning-of-string (`BOS`) or end-of-string (`EOS`) tokens. If your model expects `BOS` and `EOS` tokens, set `add_special_tokens=True` and make sure to add them to your template.\n+\n+```py\n+{%- for message in messages %}\n+    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n' }}\n+{%- endfor %}\n+```\n+\n+Set the template with the following logic to support [generation prompts](#add_generation_prompt). The template wraps each message with `<|im_start|>` and `<|im_end|>` tokens and writes the role as a string. This allows you to easily customize the roles you want to train with.\n+\n+```py\n+tokenizer.chat_template = \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\"\n+```\n+\n+The `user`, `system` and `assistant` roles are standard roles in chat templates. We recommend using these roles when it makes sense, especially if youâ€™re using your model with the [`TextGenerationPipeline`].\n+\n+```py\n+<|im_start|>system\n+You are a helpful chatbot that will do its best not to say anything so stupid that people tweet about it.<|im_end|>\n+<|im_start|>user\n+How are you?<|im_end|>\n+<|im_start|>assistant\n+I'm doing great!<|im_end|>\n+```\n+\n+## Model training\n+\n+Training a model with a chat template is a good way to ensure a chat template matches the tokens a model is trained on. Apply the chat template as a preprocessing step to your dataset. Set `add_generation_prompt=False` because the additional tokens to prompt an assistant response arenâ€™t helpful during training.\n+\n+An example of preprocessing a dataset with a chat template is shown below.\n+\n+```py\n+from transformers import AutoTokenizer\n+from datasets import Dataset\n+\n+tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\")\n+\n+chat1 = [\n+    {\"role\": \"user\", \"content\": \"Which is bigger, the moon or the sun?\"},\n+    {\"role\": \"assistant\", \"content\": \"The sun.\"}\n+]\n+chat2 = [\n+    {\"role\": \"user\", \"content\": \"Which is bigger, a virus or a bacterium?\"},\n+    {\"role\": \"assistant\", \"content\": \"A bacterium.\"}\n+]\n+\n+dataset = Dataset.from_dict({\"chat\": [chat1, chat2]})\n+dataset = dataset.map(lambda x: {\"formatted_chat\": tokenizer.apply_chat_template(x[\"chat\"], tokenize=False, add_generation_prompt=False)})\n+print(dataset['formatted_chat'][0])\n+```\n+```md\n+<|user|>\n+Which is bigger, the moon or the sun?</s>\n+<|assistant|>\n+The sun.</s>\n+```\n+\n+After this step, you can continue following the [training recipe](./tasks/language_modeling) for causal language models using the `formatted_chat` column.\n+\n+Some tokenizers add special `<bos>` and `<eos>` tokens. Chat templates should already include all the necessary special tokens, and adding additional special tokens is often incorrect or duplicated, hurting model performance. When you format text with `apply_chat_template(tokenize=False)`, make sure you set `add_special_tokens=False` as well to avoid duplicating them.\n+\n+```py\n+apply_chat_template(messages, tokenize=False, add_special_tokens=False)\n+```\n+\n+This isnâ€™t an issue if `apply_chat_template(tokenize=True)`."
        },
        {
            "sha": "07ee8c828e4125808dfe53752a987c80f0a080f4",
            "filename": "docs/source/en/chat_templating_multimodal.md",
            "status": "added",
            "additions": 272,
            "deletions": 0,
            "changes": 272,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fchat_templating_multimodal.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fchat_templating_multimodal.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fchat_templating_multimodal.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -0,0 +1,272 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# Multimodal templates\n+\n+Multimodal model chat templates expect a similar [template](./chat_templating) as text-only models. It needs `messages` that includes a dictionary of the `role` and `content`.\n+\n+Multimodal templates are included in the [Processor](./processors) class and requires an additional `type` key for specifying whether the included content is an image, video, or text.\n+\n+This guide will show you how to format chat templates for multimodal models as well as some best practices for configuring the template\n+\n+## ImageTextToTextPipeline\n+\n+[`ImageTextToTextPipeline`] is a high-level image and text generation class with a â€œchat modeâ€. Chat mode is enabled when a conversational model is detected and the chat prompt is [properly formatted](./llm_tutorial#wrong-prompt-format).\n+\n+Start by building a chat history with the following two roles.\n+\n+- `system` describes how the model should behave and respond when youâ€™re chatting with it. This role isnâ€™t supported by all chat models.\n+- `user` is where you enter your first message to the model.\n+\n+```py\n+messages = [\n+    {\n+        \"role\": \"system\",\n+        \"content\": [{\"type\": \"text\", \"text\": \"You are a friendly chatbot who always responds in the style of a pirate\"}],\n+    },\n+    {\n+      \"role\": \"user\",\n+      \"content\": [\n+            {\"type\": \"image\", \"url\": \"http://images.cocodataset.org/val2017/000000039769.jpg\"},\n+            {\"type\": \"text\", \"text\": \"What are these?\"},\n+        ],\n+    },\n+]\n+```\n+\n+Create a [`ImageTextToTextPipeline`] and pass the chat to it. For large models, setting [device_map=â€œautoâ€](./models#big-model-inference) helps load the model quicker and automatically places it on the fastest device available. Changing the data type to [torch.bfloat16](./models#model-data-type) also helps save memory.\n+\n+> [!TIP]\n+> The [`ImageTextToTextPipeline`] accepts chats in the OpenAI format to make inference easier and more accessible. \n+\n+```python\n+import torch\n+from transformers import pipeline\n+\n+pipeline = pipeline(\"image-text-to-text\", model=\"llava-hf/llava-onevision-qwen2-0.5b-ov-hf\", device=\"cuda\", torch_dtype=torch.float16)\n+pipeline(text=messages, max_new_tokens=50, return_full_text=False)\n+[{'input_text': [{'role': 'system',\n+    'content': [{'type': 'text',\n+      'text': 'You are a friendly chatbot who always responds in the style of a pirate'}]},\n+   {'role': 'user',\n+    'content': [{'type': 'image',\n+      'url': 'http://images.cocodataset.org/val2017/000000039769.jpg'},\n+     {'type': 'text', 'text': 'What are these?'}]}],\n+  'generated_text': 'The image shows two cats lying on a pink surface, which appears to be a cushion or a soft blanket. The cat on the left has a striped coat, typical of tabby cats, and is lying on its side with its head resting on the'}]\n+```\n+\n+## Image inputs\n+\n+For multimodal models that accept images like [LLaVA](./model_doc/llava), include the following in `content` as shown below.\n+\n+- The content `\"type\"` can be an `\"image\"` or `\"text\"`.\n+- For images, it can be a link to the image (`\"url\"`), a file path (`\"path\"`), or `\"base64\"`. Images are automatically loaded, processed, and prepared into pixel values as inputs to the model.\n+\n+```python\n+from transformers import AutoProcessor, LlavaOnevisionForConditionalGeneration\n+\n+model = LlavaOnevisionForConditionalGeneration.from_pretrained(\"llava-hf/llava-onevision-qwen2-0.5b-ov-hf\")\n+processor = AutoProcessor.from_pretrained(\"llava-hf/llava-onevision-qwen2-0.5b-ov-hf\")\n+\n+messages = [\n+    {\n+      \"role\": \"system\",\n+      \"content\": [{\"type\": \"text\", \"text\": \"You are a friendly chatbot who always responds in the style of a pirate\"}],\n+    },\n+    {\n+      \"role\": \"user\",\n+      \"content\": [\n+            {\"type\": \"image\", \"url\": \"http://images.cocodataset.org/val2017/000000039769.jpg\"},\n+            {\"type\": \"text\", \"text\": \"What are these?\"},\n+        ],\n+    },\n+]\n+```\n+\n+Pass `messages` to [`~ProcessorMixin.apply_chat_template`] to tokenize the input content and return the `input_ids` and `pixel_values`.\n+\n+```py\n+processed_chat = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\")\n+print(processed_chat.keys())\n+```\n+\n+These inputs are now ready to be used in [`~GenerationMixin.generate`].\n+\n+## Video inputs\n+\n+Some vision models also support video inputs. The message format is very similar to the format for [image inputs](#image-inputs).\n+\n+- The content `\"type\"` should be `\"video\"` to indicate the the content is a video.\n+- For videos, it can be a link to the video (`\"url\"`) or it could be a file path (`\"path\"`). Videos loaded from a URL can only be decoded with [PyAV](https://pyav.basswood-io.com/docs/stable/) or [Decord](https://github.com/dmlc/decord).\n+\n+> [!WARNING]\n+> Loading a video from `\"url\"` is only supported by the PyAV or Decord backends.\n+\n+```python\n+from transformers import AutoProcessor, LlavaOnevisionForConditionalGeneration\n+\n+model_id = \"llava-hf/llava-onevision-qwen2-0.5b-ov-hf\"\n+model = LlavaOnevisionForConditionalGeneration.from_pretrained(model_id)\n+processor = AutoProcessor.from_pretrained(model_id)\n+\n+messages = [\n+    {\n+      \"role\": \"system\",\n+      \"content\": [{\"type\": \"text\", \"text\": \"You are a friendly chatbot who always responds in the style of a pirate\"}],\n+    },\n+    {\n+      \"role\": \"user\",\n+      \"content\": [\n+            {\"type\": \"video\", \"url\": \"https://test-videos.co.uk/vids/bigbuckbunny/mp4/h264/720/Big_Buck_Bunny_720_10s_10MB.mp4\"},\n+            {\"type\": \"text\", \"text\": \"What do you see in this video?\"},\n+        ],\n+    },\n+]\n+```\n+\n+Pass `messages` to [`~ProcessorMixin.apply_chat_template`] to tokenize the input content. There are a few extra parameters to include in [`~ProcessorMixin.apply_chat_template`] that controls the sampling process.\n+\n+The `video_load_backend` parameter refers to a specific framework to load a video. It supports [PyAV](https://pyav.basswood-io.com/docs/stable/), [Decord](https://github.com/dmlc/decord), [OpenCV](https://github.com/opencv/opencv), and [torchvision](https://pytorch.org/vision/stable/index.html).\n+\n+The examples below uses Decord as the backend because it is a bit faster than PyAV.\n+\n+<hfoptions id=\"sampling\">\n+<hfoption id=\"fixed number of frames\">\n+\n+The `num_frames` parameter controls how many frames to uniformly sample from the video. Each checkpoint has a maximum frame count it was pretrained with and exceeding this count can significantly lower generation quality. It's important to choose a frame count that fits both the model capacity and your hardware resources. If `num_frames` isn't specified, the entire video is loaded without any frame sampling.\n+\n+\n+```python\n+processed_chat = processor.apply_chat_template(\n+    messages,\n+    add_generation_prompt=True,\n+    tokenize=True,\n+    return_dict=True,\n+    return_tensors=\"pt\",\n+    num_frames=32,\n+    video_load_backend=\"decord\",\n+)\n+print(processed_chat.keys())\n+```\n+\n+These inputs are now ready to be used in [`~GenerationMixin.generate`].\n+\n+</hfoption>\n+<hfoption id=\"fps\">\n+\n+For longer videos, it may be better to sample more frames for better representation with the `video_fps` parameter. This determines how many frames per second to extract. As an example, if a video is 10 seconds long and `video_fps=2`, then the model samples 20 frames. In other words, 2 frames are uniformly sampled every 10 seconds.\n+\n+```py\n+processed_chat = processor.apply_chat_template(\n+    messages,\n+    add_generation_prompt=True,\n+    tokenize=True,\n+    return_dict=True,\n+    video_fps=32,\n+    video_load_backend=\"decord\",\n+)\n+print(processed_chat.keys())\n+```\n+\n+</hfoption>\n+<hfoption id=\"custom frame sampling\">\n+\n+Some models don't sample frames *uniformly* and require more complex logic to determine which frames to use. For example, the model may have an *adaptive frame selection* or if the model prioritizes *key moments* in a video rather than evenly spaced frames.\n+\n+If a model has a different sampling strategy, you can write a function that customizes frame selection. The function should include the following requirements.\n+\n+- Use the `sample_indices_fn` parameter to pass a callable function for sampling.\n+- If provided, this function *overrides* the standard `num_frames` and `fps` parameters.\n+- The function receives all the parameters passed to `load_video` and must return valid frame indices to sample from.\n+\n+An example function is shown below. This gives you full control over frame selection, making the model more adaptable to different video scenarios.\n+\n+```py\n+def sample_indices_fn(metadata, **kwargs):\n+    # samples only the first and the second frame\n+    return [0, 1]\n+\n+processed_chat = processor.apply_chat_template(\n+    messages,\n+    add_generation_prompt=True,\n+    tokenize=True,\n+    return_dict=True,\n+    sample_indices_fn=sample_indices_fn,\n+    video_load_backend=\"decord\",\n+)\n+print(processed_chat.keys())\n+```\n+\n+</hfoption>\n+<hfoption id=\"list of image frames\">\n+\n+Videos may also exist as a set of sampled frames stored as images rather than the full video file.\n+\n+In this case, pass a list of image file paths and the processor automatically concatenates them into a video. Make sure all images are the same size since they are assumed to be from the same video.\n+\n+```py\n+frames_paths = [\"/path/to/frame0.png\", \"/path/to/frame5.png\", \"/path/to/frame10.png\"]\n+messages = [\n+    {\n+        \"role\": \"system\",\n+        \"content\": [{\"type\": \"text\", \"text\": \"You are a friendly chatbot who always responds in the style of a pirate\"}],\n+    },\n+    {\n+      \"role\": \"user\",\n+      \"content\": [\n+            {\"type\": \"video\", \"path\": frames_paths},\n+            {\"type\": \"text\", \"text\": \"What do you see in this video?\"},\n+        ],\n+    },\n+]\n+\n+processed_chat = processor.apply_chat_template(\n+    messages,\n+    add_generation_prompt=True,\n+    tokenize=True,\n+    return_dict=True,\n+)\n+print(processed_chat.keys())\n+```\n+\n+</hfoption>\n+</hfoptions>\n+\n+## Template configuration\n+\n+You can create a custom chat template with [Jinja](https://jinja.palletsprojects.com/en/3.1.x/templates/) and set it with [`~ProcessorMixin.apply_chat_template`]. Refer to the [Template writing](./chat_templating_writing) guide for more details.\n+\n+For example, to enable a template to handle a *list of content* from multiple modalities while still supporting plain strings for text-only inference, specify how to handle the `content['type']` if it is an image or text as shown below in the Llama 3.2 Vision Instruct [template](https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct/blob/main/chat_template.json).\n+\n+```jinja\n+{% for message in messages %}\n+{% if loop.index0 == 0 %}{{ bos_token }}{% endif %}\n+{{ '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' }}\n+{% if message['content'] is string %}\n+{{ message['content'] }}\n+{% else %}\n+{% for content in message['content'] %}\n+{% if content['type'] == 'image' %}\n+{{ '<|image|>' }}\n+{% elif content['type'] == 'text' %}\n+{{ content['text'] }}\n+{% endif %}\n+{% endfor %}\n+{% endif %}\n+{{ '<|eot_id|>' }}\n+{% endfor %}\n+{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\n+```"
        },
        {
            "sha": "fbcec9f71c01972ae9a451abf1c96cde649e32f9",
            "filename": "docs/source/en/chat_templating_writing.md",
            "status": "added",
            "additions": 251,
            "deletions": 0,
            "changes": 251,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fchat_templating_writing.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fchat_templating_writing.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fchat_templating_writing.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -0,0 +1,251 @@\n+<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# Template writing\n+\n+A chat template is a [Jinja](https://jinja.palletsprojects.com/en/3.1.x/templates/) template stored in the tokenizers [chat_template](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.chat_template) attribute. Jinja is a templating language that allows you to write Python-like code and syntax. A chat template performs the following three roles.\n+\n+1. Print the role enclosed in `<|` and `|>` (`<|user|>`, `<|assistant|>`, etc.).\n+2. Print the message followed by an end-of-sequence (`EOS`) token.\n+3. Print the assistant token if [add_generation_prompt=True](./chat_templating#add_generation_prompt) so the model generates an assistant response.\n+\n+An example template is shown below.\n+\n+```jinja\n+{%- for message in messages %}\n+    {{- '<|' + message['role'] + |>\\n' }}\n+    {{- message['content'] + eos_token }}\n+{%- endfor %}\n+{%- if add_generation_prompt %}\n+    {{- '<|assistant|>\\n' }}\n+{%- endif %}\n+```\n+\n+The template can be customized to handle more complex use cases. This guide will show you how to add and edit templates and includes template writing tips.\n+\n+## Create a template\n+\n+Create a template by writing a Jinja template and then setting it as the chat template in the tokenizer. For example, the template below adds `[ASST]` and `[/ASST]` tags to the assistant messages.\n+\n+```jinja\n+{%- for message in messages %}\n+    {%- if message['role'] == 'user' %}\n+        {{- bos_token + '[INST] ' + message['content'].strip() + ' [/INST]' }}\n+    {%- elif message['role'] == 'system' %}\n+        {{- '<<SYS>>\\\\n' + message['content'].strip() + '\\\\n<</SYS>>\\\\n\\\\n' }}\n+    {%- elif message['role'] == 'assistant' %}\n+        {{- '[ASST] '  + message['content'] + ' [/ASST]' + eos_token }}\n+    {%- endif %}\n+{%- endfor %}\n+```\n+\n+Set the template in the tokenizer, and the next time you use [`~PreTrainedTokenizerBase.apply_chat_template`], the new template is used.\n+\n+```py\n+template = tokenizer.chat_template\n+template = template.replace(\"SYS\", \"SYSTEM\")  # Change the system token\n+tokenizer.chat_template = template  # Set the new template\n+```\n+\n+The template is saved in the `tokenizer_config.json` file. Upload it to the Hub with [`~PreTrainedTokenizer.push_to_hub`] so you can reuse it later and make sure everyone is using the right template for your model.\n+\n+```py\n+tokenizer.push_to_hub(\"model_name\")\n+```\n+\n+## Template writing tips\n+\n+The easiest way to start writing Jinja templates is to refer to existing templates. Use `print(tokenizer.chat_template)` on any chat model to see what template it's using. Try starting with simple models that don't call any tools or support RAG. Finally, take a look at the [Jinja documentation](https://jinja.palletsprojects.com/en/3.1.x/templates/#synopsis) for more details about formatting and syntax.\n+\n+This section curates some best practices for writing clean and efficient Jinja templates.\n+\n+### Trimming whitespace\n+\n+Jinja prints any whitespace before or after a block of text. This can be an issue for chat templates because whitespace usage should be intentional. Add `-` to strip any whitespace before a block.\n+\n+```jinja\n+{%- for message in messages %}\n+    {{- message['role'] + message['content'] }}\n+{%- endfor %}\n+```\n+\n+The incorrect whitespace usage example below may introduce a newline and indentation in the output.\n+\n+```jinja\n+{% for message in messages %}\n+    {{ message['role'] + message['content'] }}\n+{% endfor %}\n+```\n+\n+### Special variables\n+\n+There are five special variables available inside a template. You can pass virtually any additional arguments to [`~PreTrainedTokenizerBase.apply_chat_template`] and it will be available inside the template as a variable. However, you should try to keep the number of variables to the five below to make it easier for users to use the chat model without writing custom code to handle model-specific arguments.\n+\n+- `messages` contains the chat history as a list of message dicts.\n+- `tools` contains a list of tools in JSON schema format.\n+- `documents` contains a list of documents with the format `{\"title\": Title, \"contents\": \"Contents\"}` (designed for RAG models).\n+- `add_generation_prompt` is a boolean that determines whether to add an assistant header at the end of the conversation.\n+- `bos_token` and `eos_token` are special tokens extracted from a tokenizers `special_tokens_map`.\n+\n+### Callable functions\n+\n+There are two callable functions available inside a template.\n+\n+- `raise_exception(msg)` raises a `TemplateException`. This is useful for debugging or warning users about incorrect template usage.\n+- `strftime_now(format_str)` retrieves the current date and time in a specific format which could be useful to include in system messages. It is equivalent to [datetime.now().strftime(format_str)](https://docs.python.org/3/library/datetime.html#datetime.datetime.now) in Python.\n+\n+### Compatibility with non-Python Jinja\n+\n+Jinja is implemented in multiple languages and they generally have the same syntax. Writing a template in Python allows you to use Python methods such as [lower](https://docs.python.org/3/library/stdtypes.html#str.lower) on strings or [items](https://docs.python.org/3/library/stdtypes.html#dict.items) on dicts. But this won't work if the template is used in a non-Python implementation, for example, when deploying with Javascript or Rust.\n+\n+Make the changes below to ensure compatibility across all Jinja implementations.\n+\n+- Replace Python methods with Jinja filters. For example, replace `string.lower()` with `string|lower` or `dict.items()` with `dict|dictitems`. Most of the changes follow the same pattern except `string.strip()`, which is replaced with `string|trim`. Refer to the list of [built-in filters](https://jinja.palletsprojects.com/en/3.1.x/templates/#builtin-filters) for a complete list of filters.\n+- Replace `True`, `False`, and `None` (these are Python specific) with `true`, `false`, and `none` respectively.\n+- Directly rendering a dict or list may return different results in other implementations. For example, string entries may change from single-quote to double-quote. To avoid this, add the [tojson](https://jinja.palletsprojects.com/en/3.1.x/templates/#jinja-filters.tojson) filter to maintain consistency.\n+\n+### Big templates\n+\n+Newer models or models with features like [tool-calling](./chat_extras#tools) and [RAG](./chat_extras#retrieval-augmented-generation-rag) require larger templates that can be longer than 100 lines. It may be easier to write larger templates in a separate file. The line numbers in the separate file corresponds exactly to the line numbers in template parsing or execution errors, making it easier to debug any potential issues.\n+\n+Write the template in a separate file and extract it to the chat template.\n+\n+```py\n+open(\"template.jinja\", \"w\").write(tokenizer.chat_template)\n+```\n+\n+You could also load an edited template back into the tokenizer.\n+\n+```py\n+tokenizer.chat_template = open(\"template.jinja\").read()\n+```\n+\n+## Templates for tools\n+\n+There isn't a specific format for writing templates for tools but it is best to follow the standard API. This ensures the template is widely accessible across models without requiring users to write custom code to use tools with your model.\n+\n+> [!WARNING]\n+> Formatting such as whitespace and special tokens are model-specific. Make sure everything exactly matches the format a model was trained with.\n+\n+The following section lists elements of the standard API for writing templates for tools.\n+\n+### Tool definitions\n+\n+Transformers chat template methods allow a user to pass tools as Python functions or a JSON schema. When functions are passed, a JSON schema is automatically generated and passed to the template. The `tools` variable in a template always takes a list of JSON schemas.\n+\n+The specific tokens and tool descriptions should match the ones your model was trained with. Your model doesn't need to understand the JSON schema input because your template can translate the JSON schema into your models format. For example, [Command-R](./model_doc/cohere) was trained with tools defined with Python function headers, but the Command-R tool template accepts JSON schemas. The template internally converts types and renders the input tools as Python headers.\n+\n+```json\n+{\n+  \"type\": \"function\", \n+  \"function\": {\n+    \"name\": \"multiply\", \n+    \"description\": \"A function that multiplies two numbers\", \n+    \"parameters\": {\n+      \"type\": \"object\", \n+      \"properties\": {\n+        \"a\": {\n+          \"type\": \"number\", \n+          \"description\": \"The first number to multiply\"\n+        }, \n+        \"b\": {\n+          \"type\": \"number\",\n+          \"description\": \"The second number to multiply\"\n+        }\n+      }, \n+      \"required\": [\"a\", \"b\"]\n+    }\n+  }\n+}\n+```\n+\n+An example for handling tool definitions in a chat template is shown below. The specific tokens and tool descriptions should be changed to match the ones a model was trained with.\n+\n+```\n+{%- if tools %}\n+    {%- for tool in tools %}\n+        {{- '<tool>' + tool['function']['name'] + '\\n' }}\n+        {%- for argument in tool['function']['parameters']['properties'] %}\n+            {{- argument + ': ' + tool['function']['parameters']['properties'][argument]['description'] + '\\n' }}\n+        {%- endfor %}\n+        {{- '\\n</tool>' }}\n+    {%- endif %}\n+{%- endif %}\n+```\n+\n+### Tool calls\n+\n+Tool calls, if present, is a list with the `\"assistantâ€` role. This is always a list even though most tool-calling models only support single tool calls, which means the list usually only contains a single element.\n+\n+```json\n+{\n+  \"role\": \"assistant\",\n+  \"tool_calls\": [\n+    {\n+      \"type\": \"function\",\n+      \"function\": {\n+        \"name\": \"multiply\",\n+        \"arguments\": {\n+          \"a\": 5,\n+          \"b\": 6\n+        }\n+      }\n+    }\n+  ]\n+}\n+```\n+\n+A common pattern for handling tool calls is shown below.\n+\n+```\n+{%- if message['role'] == 'assistant' and 'tool_calls' in message %}\n+    {%- for tool_call in message['tool_calls'] %}\n+            {{- '<tool_call>' + tool_call['function']['name'] + '\\n' + tool_call['function']['arguments']|tojson + '\\n</tool_call>' }}\n+        {%- endif %}\n+    {%- endfor %}\n+{%- endif %}\n+```\n+\n+### Tool responses\n+\n+Tool responses are a message dict with the `role`, `name` (name of the function) and `content` (result of the tool call) keys.\n+\n+```json\n+{\n+  \"role\": \"tool\",\n+  \"name\": \"multiply\",\n+  \"content\": \"30\"\n+}\n+```\n+\n+Not all the keys need to be used in the tool response. For example, if a model doesnâ€™t expect the function name to be included in the tool response, then you can just include the `role` and `content`.\n+\n+```\n+{%- if message['role'] == 'tool' %}\n+    {{- \"<tool_result>\" + message['content'] + \"</tool_result>\" }}\n+{%- endif %}\n+```\n+\n+## Contribute\n+\n+Add a chat template by setting the `chat_template` attribute in the tokenizer and testing it with [`~PreTrainedTokenizerBase.apply_chat_template`]. If it works as expected, then you can upload it to the Hub with with [`~PreTrainedTokenizer.push_to_hub`].\n+\n+Even if you're not the model owner, it is still helpful to add a template for a model with an empty chat template or a model that is using a default class template. Open a [pull request](https://hf.co/docs/hub/repositories-pull-requests-discussions) on the model repository to add the template.\n+\n+```py\n+tokenizer.chat_template = template\n+tokenizer.push_to_hub(\"model_name\")\n+```"
        },
        {
            "sha": "2e842265a215897e1e3f563856d9cd4fb4a510a1",
            "filename": "docs/source/en/conversations.md",
            "status": "modified",
            "additions": 67,
            "deletions": 203,
            "changes": 270,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fconversations.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fconversations.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fconversations.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -14,62 +14,66 @@ rendered properly in your Markdown viewer.\n \n -->\n \n-# Chatting with Transformers\n+# Chat basics\n \n-If you're reading this article, you're almost certainly aware of **chat models**. Chat models are conversational\n-AIs that you can send and receive messages with. The most famous of these is the proprietary ChatGPT, but there are\n-now many open-source chat models which match or even substantially exceed its performance. These models are free to\n-download and run on a local machine. Although the largest and most capable models require high-powered hardware\n-and lots of memory to run, there are smaller models that will run perfectly well on a single consumer GPU, or even\n-an ordinary desktop or notebook CPU. \n+Chat models are conversational models you can send and receive messages from. There are many chat models available to choose from, but in general, larger models tend to be better though that's not always the case. The model size is often included in the name, like \"8B\" or \"70B\", and it describes the number of parameters. Mixture-of-expert (MoE) models have names like \"8x7B\" or \"141B-A35B\" which means it's a 56B and 141B parameter model. You can try quantizing larger models to reduce memory requirements, otherwise you'll need ~2 bytes of memory per parameter.\n \n-This guide will help you get started with chat models. We'll start with a brief quickstart guide that uses a convenient,\n-high-level \"pipeline\". This is all you need if you just want to start running a chat model \n-immediately. After the quickstart, we'll move on to more detailed information about\n-what exactly chat models are, how to choose an appropriate one, and a low-level breakdown of each of the\n-steps involved in talking to a chat model. We'll also give some tips on optimizing the performance and memory usage\n-of your chat models.\n+Check model leaderboards like [OpenLLM](https://hf.co/spaces/HuggingFaceH4/open_llm_leaderboard) and [LMSys Chatbot Arena](https://chat.lmsys.org/?leaderboard) to further help you identify the best chat models for your use case. Models that are specialized in certain domains (medical, legal text, non-English languages, etc.) may sometimes outperform larger general purpose models.\n \n+> [!TIP]\n+> Chat with a number of open-source models for free on [HuggingChat](https://hf.co/chat/)!\n \n-## Quickstart\n+This guide shows you how to quickly start chatting with Transformers from the command line, how build and format a conversation, and how to chat using the [`TextGenerationPipeline`].\n \n-If you have no time for details, here's the brief summary: Chat models continue chats. This means that you pass them\n-a conversation history, which can be as short as a single user message, and the model will continue the conversation\n-by adding its response. Let's see this in action. First, let's build a chat:\n+## transformers-cli\n \n-```python\n+Chat with a model directly from the command line as shown below. It launches an interactive session with a model. Enter `clear` to reset the conversation, `exit` to terminate the session, and `help` to display all the command options.\n+\n+```bash\n+transformers-cli chat --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct\n+```\n+\n+<div class=\"flex justify-center\">\n+    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/transformers-chat-cli.png\"/>\n+</div>\n+\n+For a full list of options, run the command below.\n+\n+```bash\n+transformers-cli chat -h\n+```\n+\n+The chat is implemented on top of the [AutoClass](./model_doc/auto), using tooling from [text generation](./llm_tutorial) and [chat](./chat_templating).\n+\n+## TextGenerationPipeline\n+\n+[`TextGenerationPipeline`] is a high-level text generation class with a \"chat mode\". Chat mode is enabled when a conversational model is detected and the chat prompt is [properly formatted](./llm_tutorial#wrong-prompt-format).\n+\n+To start, build a chat history with the following two roles.\n+\n+- `system` describes how the model should behave and respond when you're chatting with it. This role isn't supported by all chat models.\n+- `user` is where you enter your first message to the model.\n+\n+```py\n chat = [\n     {\"role\": \"system\", \"content\": \"You are a sassy, wise-cracking robot as imagined by Hollywood circa 1986.\"},\n     {\"role\": \"user\", \"content\": \"Hey, can you tell me any fun things to do in New York?\"}\n ]\n ```\n \n-Notice that in addition to the user's message, we added a **system** message at the start of the conversation. Not all\n-chat models support system messages, but when they do, they represent high-level directives about how the model\n-should behave in the conversation. You can use this to guide the model - whether you want short or long responses,\n-lighthearted or serious ones, and so on. If you want the model to do useful work instead of\n-practicing its improv routine, you can either omit the system message or try a terse one such as \"You are a helpful and intelligent\n-AI assistant who responds to user queries.\"\n-\n-Once you have a chat, the quickest way to continue it is using the [`TextGenerationPipeline`]. \n-Let's see this in action with `LLaMA-3`. Note that `LLaMA-3` is a gated model, which means you will need to \n-[apply for access](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct) and log in with your Hugging Face \n-account to use it. We'll also use `device_map=\"auto\"`, which will load the model on GPU if there's enough memory\n-for it, and set the dtype to `torch.bfloat16` to save memory:\n+Create the [`TextGenerationPipeline`] and pass `chat` to it. For large models, setting [device_map=\"auto\"](./models#big-model-inference) helps load the model quicker and automatically places it on the fastest device available. Changing the data type to [torch.bfloat16](./models#model-data-type) also helps save memory.\n \n-```python\n+```py\n import torch\n from transformers import pipeline\n \n-pipe = pipeline(\"text-generation\", \"meta-llama/Meta-Llama-3-8B-Instruct\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n-response = pipe(chat, max_new_tokens=512)\n-print(response[0]['generated_text'][-1]['content'])\n+pipeline = pipeline(task=\"text-generation\", model=\"meta-llama/Meta-Llama-3-8B-Instruct\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n+response = pipeline(chat, max_new_tokens=512)\n+print(response[0][\"generated_text\"][-1][\"content\"])\n ```\n \n-And you'll get:\n-\n-```text\n-(sigh) Oh boy, you're asking me for advice? You're gonna need a map, pal! Alright, \n+```txt\n+(sigh) Oh boy, you're asking me for advice? You're gonna need a map, pal! Alright,\n alright, I'll give you the lowdown. But don't say I didn't warn you, I'm a robot, not a tour guide!\n \n So, you wanna know what's fun to do in the Big Apple? Well, let me tell you, there's a million \n@@ -91,22 +95,18 @@ So, there you have it, pal! That's my expert advice on what to do in New York. N\n excuse me, I've got some oil changes to attend to. (winks)\n ```\n \n-You can continue the chat by appending your own response to it. The\n-`response` object returned by the pipeline actually contains the entire chat so far, so we can simply append\n-a message and pass it back:\n+Use the `append` method on `chat` to respond to the models message.\n \n-```python\n-chat = response[0]['generated_text']\n+```py\n+chat = response[0][\"generated_text\"]\n chat.append(\n     {\"role\": \"user\", \"content\": \"Wait, what's so wild about soup cans?\"}\n )\n-response = pipe(chat, max_new_tokens=512)\n-print(response[0]['generated_text'][-1]['content'])\n+response = pipeline(chat, max_new_tokens=512)\n+print(response[0][\"generated_text\"][-1][\"content\"])\n ```\n \n-And you'll get:\n-\n-```text\n+```txt\n (laughs) Oh, you're killin' me, pal! You don't get it, do you? Warhol's soup cans are like, art, man! \n It's like, he took something totally mundane, like a can of soup, and turned it into a masterpiece. It's \n like, \"Hey, look at me, I'm a can of soup, but I'm also a work of art!\" \n@@ -120,171 +120,35 @@ But, hey, you're not alone, pal. I mean, I'm a robot, and even I don't get it. (\n But, hey, that's what makes art, art, right? (laughs)\n ```\n \n-The remainder of this tutorial will cover specific topics such\n-as performance and memory, or how to select a chat model for your needs.\n-\n-## Choosing a chat model\n-\n-There are an enormous number of different chat models available on the [Hugging Face Hub](https://huggingface.co/models?pipeline_tag=text-generation&sort=trending),\n-and new users often feel very overwhelmed by the selection offered. Don't be, though! You really need to just focus on\n-two important considerations: \n-- The model's size, which will determine if you can fit it in memory and how quickly it will\n-run.\n-- The quality of the model's chat output.\n-\n-In general, these are correlated - bigger models tend to be \n-more capable, but even so there's a lot of variation at a given size point!\n-\n-### Size and model naming\n-The size of a model is easy to spot - it's the number in the model name, like \"8B\" or \"70B\". This is the number of\n-**parameters** in the model. Without quantization, you should expect to need about 2 bytes of memory per parameter.\n-This means that an \"8B\" model with 8 billion parameters will need about 16GB of memory just to fit the parameters, \n-plus a little extra for other overhead. It's a good fit for a high-end consumer GPU with 24GB of memory, such as a 3090\n-or 4090.\n-\n-Some chat models are \"Mixture of Experts\" models. These may list their sizes in different ways, such as \"8x7B\" or \n-\"141B-A35B\". The numbers are a little fuzzier here, but in general you can read this as saying that the model\n-has approximately 56 (8x7) billion parameters in the first case, or 141 billion parameters in the second case.\n-\n-Note that it is very common to use quantization techniques to reduce the memory usage per parameter to 8 bits, 4 bits,\n-or even less. This topic is discussed in more detail in the [Memory considerations](#memory-considerations) section below.\n-\n-### But which chat model is best?\n-Even once you know the size of chat model you can run, there's still a lot of choice out there. One way to sift through\n-it all is to consult **leaderboards**. Two of the most popular leaderboards are the [OpenLLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)\n-and the [LMSys Chatbot Arena Leaderboard](https://chat.lmsys.org/?leaderboard). Note that the LMSys leaderboard\n-also includes proprietary models - look at the `licence` column to identify open-source ones that you can download, then\n-search for them on the [Hugging Face Hub](https://huggingface.co/models?pipeline_tag=text-generation&sort=trending).\n-\n-### Specialist domains\n-Some models may be specialized for certain domains, such as medical or legal text, or non-English languages. \n-If you're working in these domains, you may find that a specialized model will give you big performance benefits. \n-Don't automatically assume that, though! Particularly when specialized models are smaller or older than the current \n-cutting-edge, a top-end general-purpose model may still outclass them. Thankfully, we are beginning to see \n-[domain-specific leaderboards](https://huggingface.co/blog/leaderboard-medicalllm) that should make it easier to locate\n-the best models for specialized domains.\n-\n-## What happens inside the pipeline?\n-\n-The quickstart above used a high-level pipeline to chat with a chat model, which is convenient, but not the\n-most flexible. Let's take a more low-level approach, to see each of the steps involved in chat. Let's start with\n-a code sample, and then break it down:\n-\n-```python\n-from transformers import AutoModelForCausalLM, AutoTokenizer\n-import torch\n-\n-# Prepare the input as before\n-chat = [\n-    {\"role\": \"system\", \"content\": \"You are a sassy, wise-cracking robot as imagined by Hollywood circa 1986.\"},\n-    {\"role\": \"user\", \"content\": \"Hey, can you tell me any fun things to do in New York?\"}\n-]\n-\n-# 1: Load the model and tokenizer\n-model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\", device_map=\"auto\", torch_dtype=torch.bfloat16)\n-tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n-\n-# 2: Apply the chat template\n-formatted_chat = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n-print(\"Formatted chat:\\n\", formatted_chat)\n-\n-# 3: Tokenize the chat (This can be combined with the previous step using tokenize=True)\n-inputs = tokenizer(formatted_chat, return_tensors=\"pt\", add_special_tokens=False)\n-# Move the tokenized inputs to the same device the model is on (GPU/CPU)\n-inputs = {key: tensor.to(model.device) for key, tensor in inputs.items()}\n-print(\"Tokenized inputs:\\n\", inputs)\n-\n-# 4: Generate text from the model\n-outputs = model.generate(**inputs, max_new_tokens=512, temperature=0.1)\n-print(\"Generated tokens:\\n\", outputs)\n-\n-# 5: Decode the output back to a string\n-decoded_output = tokenizer.decode(outputs[0][inputs['input_ids'].size(1):], skip_special_tokens=True)\n-print(\"Decoded output:\\n\", decoded_output)\n-```\n-\n-There's a lot in here, each piece of which could be its own document! Rather than going into too much detail, I'll cover\n-the broad ideas, and leave the details for the linked documents. The key steps are:\n-\n-1. [Models](https://huggingface.co/learn/nlp-course/en/chapter2/3) and [Tokenizers](https://huggingface.co/learn/nlp-course/en/chapter2/4?fw=pt) are loaded from the Hugging Face Hub.\n-2. The chat is formatted using the tokenizer's [chat template](https://huggingface.co/docs/transformers/main/en/chat_templating)\n-3. The formatted chat is [tokenized](https://huggingface.co/learn/nlp-course/en/chapter2/4) using the tokenizer.\n-4. We [generate](https://huggingface.co/docs/transformers/en/llm_tutorial) a response from the model.\n-5. The tokens output by the model are decoded back to a string\n-\n-## Performance, memory and hardware\n+## Performance\n \n-You probably know by now that most machine learning tasks are run on GPUs. However, it is entirely possible\n-to generate text from a chat model or language model on a CPU, albeit somewhat more slowly. If you can fit\n-the model in GPU memory, though, this will usually be the preferable option.\n+Transformers load models in full precision by default, and for a 8B model, this requires ~32GB of memory! Reduce memory usage by loading a model in half-precision or bfloat16 (only uses ~2 bytes per parameter). You can even quantize the model to a lower precision like 8-bit or 4-bit with [bitsandbytes](https://hf.co/docs/bitsandbytes/index).\n \n-### Memory considerations\n+> [!TIP]\n+> Refer to the [Quantization](./quantization/overview) docs for more information about the different quantization backends available.\n \n-By default, Hugging Face classes like [`TextGenerationPipeline`] or [`AutoModelForCausalLM`] will load the model in \n-`float32` precision. This means that it will need 4 bytes (32 bits) per parameter, so an \"8B\" model with 8 billion\n-parameters will need ~32GB of memory. However, this can be wasteful! Most modern language models are trained in \n-\"bfloat16\" precision, which uses only 2 bytes per parameter. If your hardware supports it (Nvidia 30xx/Axxx\n-or newer), you can load the model in `bfloat16` precision, using the `torch_dtype` argument as we did above.\n+Create a [`BitsAndBytesConfig`] with your desired quantization settings and pass it to the pipelines `model_kwargs` parameter. The example below quantizes a model to 8-bits.\n \n-It is possible to go even lower than 16-bits using \"quantization\", a method to lossily compress model weights. This\n-allows each parameter to be squeezed down to 8 bits, 4 bits or even less. Note that, especially at 4 bits,\n-the model's outputs may be negatively affected, but often this is a tradeoff worth making to fit a larger and more\n-capable chat model in memory. Let's see this in action with `bitsandbytes`:\n-\n-```python\n-from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n-\n-quantization_config = BitsAndBytesConfig(load_in_8bit=True)  # You can also try load_in_4bit\n-model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\", device_map=\"auto\", quantization_config=quantization_config)\n-```\n-\n-Or we can do the same thing using the `pipeline` API:\n-\n-```python\n+```py\n from transformers import pipeline, BitsAndBytesConfig\n \n-quantization_config = BitsAndBytesConfig(load_in_8bit=True)  # You can also try load_in_4bit\n-pipe = pipeline(\"text-generation\", \"meta-llama/Meta-Llama-3-8B-Instruct\", device_map=\"auto\", model_kwargs={\"quantization_config\": quantization_config})\n+quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n+pipeline = pipeline(task=\"text-generation\", model=\"meta-llama/Meta-Llama-3-8B-Instruct\", device_map=\"auto\", model_kwargs={\"quantization_config\": quantization_config})\n ```\n \n-There are several other options for quantizing models besides `bitsandbytes` - please see the [Quantization guide](./quantization)\n-for more information.\n-\n-### Performance considerations\n-\n-<Tip>\n-\n-For a more extensive guide on language model performance and optimization, check out [LLM Inference Optimization](./llm_optims) .\n-\n-</Tip>\n-\n+In general, larger models are slower in addition to requiring more memory because text generation is bottlenecked by **memory bandwidth** instead of compute power. Each active parameter must be read from memory for every generated token. For a 16GB model, 16GB must be read from memory for every generated token.\n \n-As a general rule, larger chat models will be slower in addition to requiring more memory. It's possible to be\n-more concrete about this, though: Generating text from a chat model is unusual in that it is bottlenecked by\n-**memory bandwidth** rather than compute power, because every active parameter must be read from memory for each\n-token that the model generates. This means that number of tokens per second you can generate from a chat\n-model is generally proportional to the total bandwidth of the memory it resides in, divided by the size of the model.\n+The number of generated tokens/sec is proportional to the total memory bandwidth of the system divided by the model size. Depending on your hardware, total memory bandwidth can vary. Refer to the table below for approximate generation speeds for different hardware types.\n \n-In our quickstart example above, our model was ~16GB in size when loaded in `bfloat16` precision. \n-This means that 16GB must be read from memory for every token generated by the model. Total memory bandwidth can\n-vary from 20-100GB/sec for consumer CPUs to 200-900GB/sec for consumer GPUs, specialized CPUs like\n-Intel Xeon, AMD Threadripper/Epyc or high-end Apple silicon, and finally up to 2-3TB/sec for data center GPUs like\n-the Nvidia A100 or H100. This should give you a good idea of the generation speed you can expect from these different\n-hardware types.\n+| Hardware | Memory bandwidth |\n+|---|---|\n+| consumer CPU | 20-100GB/sec |\n+| specialized CPU (Intel Xeon, AMD Threadripper/Epyc, Apple silicon) | 200-900GB/sec |\n+| data center GPU (NVIDIA A100/H100) | 2-3TB/sec |\n \n-Therefore, if you want to improve the speed of text generation, the easiest solution is to either reduce the\n-size of the model in memory (usually by quantization), or get hardware with higher memory bandwidth. For advanced users, \n-several other techniques exist to get around this bandwidth bottleneck. The most common are variants on \n-[assisted generation](https://huggingface.co/blog/assisted-generation), also known as \"speculative\n-sampling\". These techniques try to guess multiple future tokens at once, often using a smaller \"draft model\", and then\n-confirm these generations with the chat model. If the guesses are validated by the chat model, more than one token can\n-be generated per forward pass, which greatly alleviates the bandwidth bottleneck and improves generation speed.  \n+The easiest solution for improving generation speed is to either quantize a model or use hardware with higher memory bandwidth.\n \n-Finally, we should also note the impact of \"Mixture of Experts\" (MoE) models here. Several popular chat models,\n-such as Mixtral, Qwen-MoE and DBRX, are MoE models. In these models, not every parameter is active for every token generated.\n-As a result, MoE models generally have much lower memory bandwidth requirements, even though their total size\n-can be quite large. They can therefore be several times faster than a normal \"dense\" model of the same size. However,\n-techniques like assisted generation are generally ineffective for these models because more parameters will become\n-active with each new speculated token, which will negate the bandwidth and speed benefits that the MoE architecture\n-provides.\n+You can also try techniques like [speculative decoding](./generation_strategies#speculative-decoding), where a smaller model generates candidate tokens that are verified by the larger model. If the candidate tokens are correct, the larger model can generate more than one token per `forward` pass. This significantly alleviates the bandwidth bottleneck and improves generation speed.\n \n+> [!TIP]\n+> Parameters may not be active for every generated token in MoE models such as [Mixtral](./model_doc/mixtral), [Qwen2MoE](./model_doc/qwen2_moe.md), and [DBRX](./model_doc/dbrx). As a result, MoE models generally have much lower memory bandwidth requirements and can be faster than a regular LLM of the same size. However, techniques like speculative decoding are ineffective with MoE models because parameters become activated with each new speculated token."
        },
        {
            "sha": "0ecc503df61533733e87d413270d7b0f84dbf2eb",
            "filename": "docs/source/en/create_a_model.md",
            "status": "removed",
            "additions": 0,
            "deletions": 472,
            "changes": 472,
            "blob_url": "https://github.com/huggingface/transformers/blob/6aa9888463ece40e29ac9065127f578a24f50958/docs%2Fsource%2Fen%2Fcreate_a_model.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/6aa9888463ece40e29ac9065127f578a24f50958/docs%2Fsource%2Fen%2Fcreate_a_model.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fcreate_a_model.md?ref=6aa9888463ece40e29ac9065127f578a24f50958",
            "patch": "@@ -1,472 +0,0 @@\n-<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# Create a custom architecture\n-\n-An [`AutoClass`](model_doc/auto) automatically infers the model architecture and downloads pretrained configuration and weights. Generally, we recommend using an `AutoClass` to produce checkpoint-agnostic code. But users who want more control over specific model parameters can create a custom ðŸ¤— Transformers model from just a few base classes. This could be particularly useful for anyone who is interested in studying, training or experimenting with a ðŸ¤— Transformers model. In this guide, dive deeper into creating a custom model without an `AutoClass`. Learn how to:\n-\n-- Load and customize a model configuration.\n-- Create a model architecture.\n-- Create a slow and fast tokenizer for text.\n-- Create an image processor for vision tasks.\n-- Create a feature extractor for audio tasks.\n-- Create a processor for multimodal tasks.\n-\n-## Configuration\n-\n-A [configuration](main_classes/configuration) refers to a model's specific attributes. Each model configuration has different attributes; for instance, all NLP models have the `hidden_size`, `num_attention_heads`, `num_hidden_layers` and `vocab_size` attributes in common. These attributes specify the number of attention heads or hidden layers to construct a model with.\n-\n-Get a closer look at [DistilBERT](model_doc/distilbert) by accessing [`DistilBertConfig`] to inspect it's attributes:\n-\n-```py\n->>> from transformers import DistilBertConfig\n-\n->>> config = DistilBertConfig()\n->>> print(config)\n-DistilBertConfig {\n-  \"activation\": \"gelu\",\n-  \"attention_dropout\": 0.1,\n-  \"dim\": 768,\n-  \"dropout\": 0.1,\n-  \"hidden_dim\": 3072,\n-  \"initializer_range\": 0.02,\n-  \"max_position_embeddings\": 512,\n-  \"model_type\": \"distilbert\",\n-  \"n_heads\": 12,\n-  \"n_layers\": 6,\n-  \"pad_token_id\": 0,\n-  \"qa_dropout\": 0.1,\n-  \"seq_classif_dropout\": 0.2,\n-  \"sinusoidal_pos_embds\": false,\n-  \"transformers_version\": \"4.16.2\",\n-  \"vocab_size\": 30522\n-}\n-```\n-\n-[`DistilBertConfig`] displays all the default attributes used to build a base [`DistilBertModel`]. All attributes are customizable, creating space for experimentation. For example, you can customize a default model to:\n-\n-- Try a different activation function with the `activation` parameter.\n-- Use a higher dropout ratio for the attention probabilities with the `attention_dropout` parameter.\n-\n-```py\n->>> my_config = DistilBertConfig(activation=\"relu\", attention_dropout=0.4)\n->>> print(my_config)\n-DistilBertConfig {\n-  \"activation\": \"relu\",\n-  \"attention_dropout\": 0.4,\n-  \"dim\": 768,\n-  \"dropout\": 0.1,\n-  \"hidden_dim\": 3072,\n-  \"initializer_range\": 0.02,\n-  \"max_position_embeddings\": 512,\n-  \"model_type\": \"distilbert\",\n-  \"n_heads\": 12,\n-  \"n_layers\": 6,\n-  \"pad_token_id\": 0,\n-  \"qa_dropout\": 0.1,\n-  \"seq_classif_dropout\": 0.2,\n-  \"sinusoidal_pos_embds\": false,\n-  \"transformers_version\": \"4.16.2\",\n-  \"vocab_size\": 30522\n-}\n-```\n-\n-Pretrained model attributes can be modified in the [`~PretrainedConfig.from_pretrained`] function:\n-\n-```py\n->>> my_config = DistilBertConfig.from_pretrained(\"distilbert/distilbert-base-uncased\", activation=\"relu\", attention_dropout=0.4)\n-```\n-\n-Once you are satisfied with your model configuration, you can save it with [`~PretrainedConfig.save_pretrained`]. Your configuration file is stored as a JSON file in the specified save directory:\n-\n-```py\n->>> my_config.save_pretrained(save_directory=\"./your_model_save_path\")\n-```\n-\n-To reuse the configuration file, load it with [`~PretrainedConfig.from_pretrained`]:\n-\n-```py\n->>> my_config = DistilBertConfig.from_pretrained(\"./your_model_save_path/config.json\")\n-```\n-\n-<Tip>\n-\n-You can also save your configuration file as a dictionary or even just the difference between your custom configuration attributes and the default configuration attributes! See the [configuration](main_classes/configuration) documentation for more details.\n-\n-</Tip>\n-\n-## Model\n-\n-The next step is to create a [model](main_classes/models). The model - also loosely referred to as the architecture - defines what each layer is doing and what operations are happening. Attributes like `num_hidden_layers` from the configuration are used to define the architecture. Every model shares the base class [`PreTrainedModel`] and a few common methods like resizing input embeddings and pruning self-attention heads. In addition, all models are also either a [`torch.nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html), [`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model) or [`flax.linen.Module`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html) subclass. This means models are compatible with each of their respective framework's usage.\n-\n-<frameworkcontent>\n-<pt>\n-Load your custom configuration attributes into the model:\n-\n-```py\n->>> from transformers import DistilBertModel\n-\n->>> my_config = DistilBertConfig.from_pretrained(\"./your_model_save_path/config.json\")\n->>> model = DistilBertModel(my_config)\n-```\n-\n-This creates a model with random values instead of pretrained weights. You won't be able to use this model for anything useful yet until you train it. Training is a costly and time-consuming process. It is generally better to use a pretrained model to obtain better results faster, while using only a fraction of the resources required for training.\n-\n-Create a pretrained model with [`~PreTrainedModel.from_pretrained`]:\n-\n-```py\n->>> model = DistilBertModel.from_pretrained(\"distilbert/distilbert-base-uncased\")\n-```\n-\n-When you load pretrained weights, the default model configuration is automatically loaded if the model is provided by ðŸ¤— Transformers. However, you can still replace - some or all of - the default model configuration attributes with your own if you'd like:\n-\n-```py\n->>> model = DistilBertModel.from_pretrained(\"distilbert/distilbert-base-uncased\", config=my_config)\n-```\n-</pt>\n-<tf>\n-Load your custom configuration attributes into the model:\n-\n-```py\n->>> from transformers import TFDistilBertModel\n-\n->>> my_config = DistilBertConfig.from_pretrained(\"./your_model_save_path/my_config.json\")\n->>> tf_model = TFDistilBertModel(my_config)\n-```\n-\n-This creates a model with random values instead of pretrained weights. You won't be able to use this model for anything useful yet until you train it. Training is a costly and time-consuming process. It is generally better to use a pretrained model to obtain better results faster, while using only a fraction of the resources required for training.\n-\n-Create a pretrained model with [`~TFPreTrainedModel.from_pretrained`]:\n-\n-```py\n->>> tf_model = TFDistilBertModel.from_pretrained(\"distilbert/distilbert-base-uncased\")\n-```\n-\n-When you load pretrained weights, the default model configuration is automatically loaded if the model is provided by ðŸ¤— Transformers. However, you can still replace - some or all of - the default model configuration attributes with your own if you'd like:\n-\n-```py\n->>> tf_model = TFDistilBertModel.from_pretrained(\"distilbert/distilbert-base-uncased\", config=my_config)\n-```\n-</tf>\n-</frameworkcontent>\n-\n-### Model heads\n-\n-At this point, you have a base DistilBERT model which outputs the *hidden states*. The hidden states are passed as inputs to a model head to produce the final output. ðŸ¤— Transformers provides a different model head for each task as long as a model supports the task (i.e., you can't use DistilBERT for a sequence-to-sequence task like translation).\n-\n-<frameworkcontent>\n-<pt>\n-For example, [`DistilBertForSequenceClassification`] is a base DistilBERT model with a sequence classification head. The sequence classification head is a linear layer on top of the pooled outputs.\n-\n-```py\n->>> from transformers import DistilBertForSequenceClassification\n-\n->>> model = DistilBertForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased\")\n-```\n-\n-Easily reuse this checkpoint for another task by switching to a different model head. For a question answering task, you would use the [`DistilBertForQuestionAnswering`] model head. The question answering head is similar to the sequence classification head except it is a linear layer on top of the hidden states output.\n-\n-```py\n->>> from transformers import DistilBertForQuestionAnswering\n-\n->>> model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert/distilbert-base-uncased\")\n-```\n-</pt>\n-<tf>\n-For example, [`TFDistilBertForSequenceClassification`] is a base DistilBERT model with a sequence classification head. The sequence classification head is a linear layer on top of the pooled outputs.\n-\n-```py\n->>> from transformers import TFDistilBertForSequenceClassification\n-\n->>> tf_model = TFDistilBertForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased\")\n-```\n-\n-Easily reuse this checkpoint for another task by switching to a different model head. For a question answering task, you would use the [`TFDistilBertForQuestionAnswering`] model head. The question answering head is similar to the sequence classification head except it is a linear layer on top of the hidden states output.\n-\n-```py\n->>> from transformers import TFDistilBertForQuestionAnswering\n-\n->>> tf_model = TFDistilBertForQuestionAnswering.from_pretrained(\"distilbert/distilbert-base-uncased\")\n-```\n-</tf>\n-</frameworkcontent>\n-\n-## Tokenizer\n-\n-The last base class you need before using a model for textual data is a [tokenizer](main_classes/tokenizer) to convert raw text to tensors. There are two types of tokenizers you can use with ðŸ¤— Transformers:\n-\n-- [`PreTrainedTokenizer`]: a Python implementation of a tokenizer.\n-- [`PreTrainedTokenizerFast`]: a tokenizer from our Rust-based [ðŸ¤— Tokenizer](https://huggingface.co/docs/tokenizers/python/latest/) library. This tokenizer type is significantly faster - especially during batch tokenization - due to its Rust implementation. The fast tokenizer also offers additional methods like *offset mapping* which maps tokens to their original words or characters.\n-\n-Both tokenizers support common methods such as encoding and decoding, adding new tokens, and managing special tokens.\n-\n-<Tip warning={true}>\n-\n-Not every model supports a fast tokenizer. Take a look at this [table](index#supported-frameworks) to check if a model has fast tokenizer support.\n-\n-</Tip>\n-\n-If you trained your own tokenizer, you can create one from your *vocabulary* file:\n-\n-```py\n->>> from transformers import DistilBertTokenizer\n-\n->>> my_tokenizer = DistilBertTokenizer(vocab_file=\"my_vocab_file.txt\", do_lower_case=False, padding_side=\"left\")\n-```\n-\n-It is important to remember the vocabulary from a custom tokenizer will be different from the vocabulary generated by a pretrained model's tokenizer. You need to use a pretrained model's vocabulary if you are using a pretrained model, otherwise the inputs won't make sense. Create a tokenizer with a pretrained model's vocabulary with the [`DistilBertTokenizer`] class:\n-\n-```py\n->>> from transformers import DistilBertTokenizer\n-\n->>> slow_tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n-```\n-\n-Create a fast tokenizer with the [`DistilBertTokenizerFast`] class:\n-\n-```py\n->>> from transformers import DistilBertTokenizerFast\n-\n->>> fast_tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert/distilbert-base-uncased\")\n-```\n-\n-<Tip>\n-\n-By default, [`AutoTokenizer`] will try to load a fast tokenizer. You can disable this behavior by setting `use_fast=False` in `from_pretrained`.\n-\n-</Tip>\n-\n-## Image processor\n-\n-An image processor processes vision inputs. It inherits from the base [`~image_processing_utils.ImageProcessingMixin`] class.\n-\n-To use, create an image processor associated with the model you're using. For example, create a default [`ViTImageProcessor`] if you are using [ViT](model_doc/vit) for image classification:\n-\n-```py\n->>> from transformers import ViTImageProcessor\n-\n->>> vit_extractor = ViTImageProcessor()\n->>> print(vit_extractor)\n-ViTImageProcessor {\n-  \"do_normalize\": true,\n-  \"do_resize\": true,\n-  \"image_processor_type\": \"ViTImageProcessor\",\n-  \"image_mean\": [\n-    0.5,\n-    0.5,\n-    0.5\n-  ],\n-  \"image_std\": [\n-    0.5,\n-    0.5,\n-    0.5\n-  ],\n-  \"resample\": 2,\n-  \"size\": 224\n-}\n-```\n-\n-<Tip>\n-\n-If you aren't looking for any customization, just use the `from_pretrained` method to load a model's default image processor parameters.\n-\n-</Tip>\n-\n-Modify any of the [`ViTImageProcessor`] parameters to create your custom image processor:\n-\n-```py\n->>> from transformers import ViTImageProcessor\n-\n->>> my_vit_extractor = ViTImageProcessor(resample=\"PIL.Image.BOX\", do_normalize=False, image_mean=[0.3, 0.3, 0.3])\n->>> print(my_vit_extractor)\n-ViTImageProcessor {\n-  \"do_normalize\": false,\n-  \"do_resize\": true,\n-  \"image_processor_type\": \"ViTImageProcessor\",\n-  \"image_mean\": [\n-    0.3,\n-    0.3,\n-    0.3\n-  ],\n-  \"image_std\": [\n-    0.5,\n-    0.5,\n-    0.5\n-  ],\n-  \"resample\": \"PIL.Image.BOX\",\n-  \"size\": 224\n-}\n-```\n-\n-## Backbone\n-\n-<div style=\"text-align: center\">\n-  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/Backbone.png\">\n-</div>\n-\n-Computer vision models consist of a backbone, neck, and head. The backbone extracts features from an input image, the neck combines and enhances the extracted features, and the head is used for the main task (e.g., object detection). Start by initializing a backbone in the model config and specify whether you want to load pretrained weights or load randomly initialized weights. Then you can pass the model config to the model head.\n-\n-For example, to load a [ResNet](../model_doc/resnet) backbone into a [MaskFormer](../model_doc/maskformer) model with an instance segmentation head:\n-\n-<hfoptions id=\"backbone\">\n-<hfoption id=\"pretrained weights\">\n-\n-Set `use_pretrained_backbone=True` to load pretrained ResNet weights for the backbone.\n-\n-```py\n-from transformers import MaskFormerConfig, MaskFormerForInstanceSegmentation\n-\n-config = MaskFormerConfig(backbone=\"microsoft/resnet-50\", use_pretrained_backbone=True) # backbone and neck config\n-model = MaskFormerForInstanceSegmentation(config) # head\n-```\n-\n-</hfoption>\n-<hfoption id=\"random weights\">\n-\n-Set `use_pretrained_backbone=False` to randomly initialize a ResNet backbone.\n-\n-```py\n-from transformers import MaskFormerConfig, MaskFormerForInstanceSegmentation\n-\n-config = MaskFormerConfig(backbone=\"microsoft/resnet-50\", use_pretrained_backbone=False) # backbone and neck config\n-model = MaskFormerForInstanceSegmentation(config) # head\n-```\n-\n-You could also load the backbone config separately and then pass it to the model config.\n-\n-```py\n-from transformers import MaskFormerConfig, MaskFormerForInstanceSegmentation, ResNetConfig\n-\n-backbone_config = ResNetConfig()\n-config = MaskFormerConfig(backbone_config=backbone_config)\n-model = MaskFormerForInstanceSegmentation(config)\n-```\n-\n-</hfoption>\n-</hfoptions id=\"timm backbone\">\n-\n-[timm](https://hf.co/docs/timm/index) models are loaded within a model with `use_timm_backbone=True` or with [`TimmBackbone`] and [`TimmBackboneConfig`].\n-\n-Use `use_timm_backbone=True` and `use_pretrained_backbone=True` to load pretrained timm weights for the backbone.\n-\n-```python\n-from transformers import MaskFormerConfig, MaskFormerForInstanceSegmentation\n-\n-config = MaskFormerConfig(backbone=\"resnet50\", use_pretrained_backbone=True, use_timm_backbone=True) # backbone and neck config\n-model = MaskFormerForInstanceSegmentation(config) # head\n-```\n-\n-Set `use_timm_backbone=True` and `use_pretrained_backbone=False` to load a randomly initialized timm backbone.\n-\n-```python\n-from transformers import MaskFormerConfig, MaskFormerForInstanceSegmentation\n-\n-config = MaskFormerConfig(backbone=\"resnet50\", use_pretrained_backbone=False, use_timm_backbone=True) # backbone and neck config\n-model = MaskFormerForInstanceSegmentation(config) # head\n-```\n-\n-You could also load the backbone config and use it to create a `TimmBackbone` or pass it to the model config. Timm backbones will load pretrained weights by default. Set `use_pretrained_backbone=False` to load randomly initialized weights.\n-\n-```python\n-from transformers import TimmBackboneConfig, TimmBackbone\n-\n-backbone_config = TimmBackboneConfig(\"resnet50\", use_pretrained_backbone=False)\n-\n-# Create a backbone class\n-backbone = TimmBackbone(config=backbone_config)\n-\n-# Create a model with a timm backbone\n-from transformers import MaskFormerConfig, MaskFormerForInstanceSegmentation\n-\n-config = MaskFormerConfig(backbone_config=backbone_config)\n-model = MaskFormerForInstanceSegmentation(config)\n-```\n-\n-## Feature extractor\n-\n-A feature extractor processes audio inputs. It inherits from the base [`~feature_extraction_utils.FeatureExtractionMixin`] class, and may also inherit from the [`SequenceFeatureExtractor`] class for processing audio inputs.\n-\n-To use, create a feature extractor associated with the model you're using. For example, create a default [`Wav2Vec2FeatureExtractor`] if you are using [Wav2Vec2](model_doc/wav2vec2) for audio classification:\n-\n-```py\n->>> from transformers import Wav2Vec2FeatureExtractor\n-\n->>> w2v2_extractor = Wav2Vec2FeatureExtractor()\n->>> print(w2v2_extractor)\n-Wav2Vec2FeatureExtractor {\n-  \"do_normalize\": true,\n-  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n-  \"feature_size\": 1,\n-  \"padding_side\": \"right\",\n-  \"padding_value\": 0.0,\n-  \"return_attention_mask\": false,\n-  \"sampling_rate\": 16000\n-}\n-```\n-\n-<Tip>\n-\n-If you aren't looking for any customization, just use the `from_pretrained` method to load a model's default feature extractor parameters.\n-\n-</Tip>\n-\n-Modify any of the [`Wav2Vec2FeatureExtractor`] parameters to create your custom feature extractor:\n-\n-```py\n->>> from transformers import Wav2Vec2FeatureExtractor\n-\n->>> w2v2_extractor = Wav2Vec2FeatureExtractor(sampling_rate=8000, do_normalize=False)\n->>> print(w2v2_extractor)\n-Wav2Vec2FeatureExtractor {\n-  \"do_normalize\": false,\n-  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n-  \"feature_size\": 1,\n-  \"padding_side\": \"right\",\n-  \"padding_value\": 0.0,\n-  \"return_attention_mask\": false,\n-  \"sampling_rate\": 8000\n-}\n-```\n-\n-## Processor\n-\n-For models that support multimodal tasks, ðŸ¤— Transformers offers a processor class that conveniently wraps processing classes such as a feature extractor and a tokenizer into a single object. For example, let's use the [`Wav2Vec2Processor`] for an automatic speech recognition task (ASR). ASR transcribes audio to text, so you will need a feature extractor and a tokenizer.\n-\n-Create a feature extractor to handle the audio inputs:\n-\n-```py\n->>> from transformers import Wav2Vec2FeatureExtractor\n-\n->>> feature_extractor = Wav2Vec2FeatureExtractor(padding_value=1.0, do_normalize=True)\n-```\n-\n-Create a tokenizer to handle the text inputs:\n-\n-```py\n->>> from transformers import Wav2Vec2CTCTokenizer\n-\n->>> tokenizer = Wav2Vec2CTCTokenizer(vocab_file=\"my_vocab_file.txt\")\n-```\n-\n-Combine the feature extractor and tokenizer in [`Wav2Vec2Processor`]:\n-\n-```py\n->>> from transformers import Wav2Vec2Processor\n-\n->>> processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n-```\n-\n-With two basic classes - configuration and model - and an additional preprocessing class (tokenizer, image processor, feature extractor, or processor), you can create any of the models supported by ðŸ¤— Transformers. Each of these base classes are configurable, allowing you to use the specific attributes you want. You can easily setup a model for training or modify an existing pretrained model to fine-tune."
        },
        {
            "sha": "6ae5099a0312c5c18a1b31c0bfa6834866a5f442",
            "filename": "docs/source/en/custom_models.md",
            "status": "modified",
            "additions": 80,
            "deletions": 167,
            "changes": 247,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fcustom_models.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fcustom_models.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fcustom_models.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -1,4 +1,4 @@\n-<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n+<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n \n Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n the License. You may obtain a copy of the License at\n@@ -14,45 +14,33 @@ rendered properly in your Markdown viewer.\n \n -->\n \n-# Building custom models\n+# Customizing models\n \n-The ðŸ¤— Transformers library is designed to be easily extensible. Every model is fully coded in a given subfolder\n-of the repository with no abstraction, so you can easily copy a modeling file and tweak it to your needs.\n+Transformers models are designed to be customizable. A models code is fully contained in the [model](https://github.com/huggingface/transformers/tree/main/src/transformers/models) subfolder of the Transformers repository. Each folder contains a `modeling.py` and a `configuration.py` file. Copy these files to start customizing a model.\n \n-If you are writing a brand new model, it might be easier to start from scratch. In this tutorial, we will show you\n-how to write a custom model and its configuration so it can be used inside Transformers, and how you can share it\n-with the community (with the code it relies on) so that anyone can use it, even if it's not present in the ðŸ¤—\n-Transformers library. We'll see how to build upon transformers and extend the framework with your hooks and\n-custom code.\n+> [!TIP]\n+> It may be easier to start from scratch if you're creating an entirely new model. But for models that are very similar to an existing one in Transformers, it is faster to reuse or subclass the same configuration and model class.\n \n-We will illustrate all of this on a ResNet model, by wrapping the ResNet class of the\n-[timm library](https://github.com/rwightman/pytorch-image-models) into a [`PreTrainedModel`].\n+This guide will show you how to customize a ResNet model, enable [AutoClass](./models#autoclass) support, and share it on the Hub.\n \n-## Writing a custom configuration\n+## Configuration\n \n-Before we dive into the model, let's first write its configuration. The configuration of a model is an object that\n-will contain all the necessary information to build the model. As we will see in the next section, the model can only\n-take a `config` to be initialized, so we really need that object to be as complete as possible.\n+A configuration, given by the base [`PretrainedConfig`] class, contains all the necessary information to build a model. This is where you'll configure the attributes of the custom ResNet model. Different attributes gives different ResNet model types.\n \n-<Tip>\n+The main rules for customizing a configuration are:\n \n-Models in the `transformers` library itself generally follow the convention that they accept a `config` object\n-in their `__init__` method, and then pass the whole `config` to sub-layers in the model, rather than breaking the \n-config object into multiple arguments that are all passed individually to sub-layers. Writing your model in this \n-style results in simpler code with a clear \"source of truth\" for any hyperparameters, and also makes it easier\n-to reuse code from other models in `transformers`.\n+1. A custom configuration must subclass [`PretrainedConfig`]. This ensures a custom model has all the functionality of a Transformers' model such as [`~PretrainedConfig.from_pretrained`], [`~PretrainedConfig.save_pretrained`], and [`~PretrainedConfig.push_to_hub`].\n+2. The [`PretrainedConfig`] `__init__` must accept any `kwargs` and they must be passed to the superclass `__init__`. [`PretrainedConfig`] has more fields than the ones set in your custom configuration, so when you load a configuration with [`~PretrainedConfig.from_pretrained`], those fields need to be accepted by your configuration and passed to the superclass.\n \n-</Tip>\n+> [!TIP]\n+> It is useful to check the validity of some of the parameters. In the example below, a check is implemented to ensure `block_type` and `stem_type` belong to one of the predefined values.\n+>\n+> Add `model_type` to the configuration class to enable [AutoClass](./models#autoclass) support.\n \n-In our example, we will take a couple of arguments of the ResNet class that we might want to tweak. Different\n-configurations will then give us the different types of ResNets that are possible. We then just store those arguments,\n-after checking the validity of a few of them.\n-\n-```python\n+```py\n from transformers import PretrainedConfig\n from typing import List\n \n-\n class ResnetConfig(PretrainedConfig):\n     model_type = \"resnet\"\n \n@@ -86,56 +74,38 @@ class ResnetConfig(PretrainedConfig):\n         super().__init__(**kwargs)\n ```\n \n-The three important things to remember when writing you own configuration are the following:\n-- you have to inherit from `PretrainedConfig`,\n-- the `__init__` of your `PretrainedConfig` must accept any kwargs,\n-- those `kwargs` need to be passed to the superclass `__init__`.\n-\n-The inheritance is to make sure you get all the functionality from the ðŸ¤— Transformers library, while the two other\n-constraints come from the fact a `PretrainedConfig` has more fields than the ones you are setting. When reloading a\n-config with the `from_pretrained` method, those fields need to be accepted by your config and then sent to the\n-superclass.\n-\n-Defining a `model_type` for your configuration (here `model_type=\"resnet\"`) is not mandatory, unless you want to\n-register your model with the auto classes (see last section).\n-\n-With this done, you can easily create and save your configuration like you would do with any other model config of the\n-library. Here is how we can create a resnet50d config and save it:\n+Save the configuration to a JSON file in your custom model folder, `custom-resnet`, with [`~PretrainedConfig.save_pretrained`].\n \n ```py\n resnet50d_config = ResnetConfig(block_type=\"bottleneck\", stem_width=32, stem_type=\"deep\", avg_down=True)\n resnet50d_config.save_pretrained(\"custom-resnet\")\n ```\n \n-This will save a file named `config.json` inside the folder `custom-resnet`. You can then reload your config with the\n-`from_pretrained` method:\n+## Model\n \n-```py\n-resnet50d_config = ResnetConfig.from_pretrained(\"custom-resnet\")\n-```\n+With the custom ResNet configuration, you can now create and customize the model. The model subclasses the base [`PreTrainedModel`] class. Like [`PretrainedConfig`], inheriting from [`PreTrainedModel`] and initializing the superclass with the configuration extends Transformers' functionalities such as saving and loading to the custom model.\n+\n+Transformers' models follow the convention of accepting a `config` object in the `__init__` method. This passes the entire `config` to the model sublayers, instead of breaking the `config` object into multiple arguments that are individually passed to the sublayers.\n \n-You can also use any other method of the [`PretrainedConfig`] class, like [`~PretrainedConfig.push_to_hub`] to\n-directly upload your config to the Hub.\n+Writing models this way produces simpler code with a clear source of truth for any hyperparameters. It also makes it easier to reuse code from other Transformers' models.\n \n-## Writing a custom model\n+You'll create two ResNet models, a barebones ResNet model that outputs the hidden states and a ResNet model with an image classification head.\n \n-Now that we have our ResNet configuration, we can go on writing the model. We will actually write two: one that\n-extracts the hidden features from a batch of images (like [`BertModel`]) and one that is suitable for image\n-classification (like [`BertForSequenceClassification`]).\n+<hfoptions id=\"resnet\">\n+<hfoption id=\"ResnetModel\">\n \n-As we mentioned before, we'll only write a loose wrapper of the model to keep it simple for this example. The only\n-thing we need to do before writing this class is a map between the block types and actual block classes. Then the\n-model is defined from the configuration by passing everything to the `ResNet` class:\n+Define a mapping between the block types and classes. Everything else is created by passing the configuration class to the ResNet model class.\n+\n+> [!TIP]\n+> Add `config_class` to the model class to enable [AutoClass](#autoclass-support) support.\n \n ```py\n from transformers import PreTrainedModel\n from timm.models.resnet import BasicBlock, Bottleneck, ResNet\n from .configuration_resnet import ResnetConfig\n \n-\n BLOCK_MAPPING = {\"basic\": BasicBlock, \"bottleneck\": Bottleneck}\n \n-\n class ResnetModel(PreTrainedModel):\n     config_class = ResnetConfig\n \n@@ -158,12 +128,17 @@ class ResnetModel(PreTrainedModel):\n         return self.model.forward_features(tensor)\n ```\n \n-For the model that will classify images, we just change the forward method:\n+</hfoption>\n+<hfoption id=\"ResnetModelForImageClassification\">\n+\n+The `forward` method needs to be rewrittten to calculate the loss for each logit if labels are available. Otherwise, the ResNet model class is the same.\n+\n+> [!TIP]\n+> Add `config_class` to the model class to enable [AutoClass](#autoclass-support) support.\n \n ```py\n import torch\n \n-\n class ResnetModelForImageClassification(PreTrainedModel):\n     config_class = ResnetConfig\n \n@@ -190,34 +165,20 @@ class ResnetModelForImageClassification(PreTrainedModel):\n         return {\"logits\": logits}\n ```\n \n-In both cases, notice how we inherit from `PreTrainedModel` and call the superclass initialization with the `config`\n-(a bit like when you write a regular `torch.nn.Module`). The line that sets the `config_class` is not mandatory, unless\n-you want to register your model with the auto classes (see last section).\n-\n-<Tip>\n-\n-If your model is very similar to a model inside the library, you can re-use the same configuration as this model.\n+</hfoption>\n+</hfoptions>\n \n-</Tip>\n+A model can return any output format. Returning a dictionary (like `ResnetModelForImageClassification`) with losses when labels are available makes the custom model compatible with [`Trainer`]. For other output formats, you'll need your own training loop or a different library for training.\n \n-You can have your model return anything you want, but returning a dictionary like we did for\n-`ResnetModelForImageClassification`, with the loss included when labels are passed, will make your model directly\n-usable inside the [`Trainer`] class. Using another output format is fine as long as you are planning on using your own\n-training loop or another library for training.\n-\n-Now that we have our model class, let's create one:\n+Instantiate the custom model class with the configuration.\n \n ```py\n resnet50d = ResnetModelForImageClassification(resnet50d_config)\n ```\n \n-Again, you can use any of the methods of [`PreTrainedModel`], like [`~PreTrainedModel.save_pretrained`] or\n-[`~PreTrainedModel.push_to_hub`]. We will use the second in the next section, and see how to push the model weights\n-with the code of our model. But first, let's load some pretrained weights inside our model.\n+At this point, you can load pretrained weights into the model or train it from scratch. In this guide, you'll load pretrained weights.\n \n-In your own use case, you will probably be training your custom model on your own data. To go fast for this tutorial,\n-we will use the pretrained version of the resnet50d. Since our model is just a wrapper around it, it's going to be\n-easy to transfer those weights:\n+Load the pretrained weights from the [timm](https://hf.co/docs/timm/index) library, and then transfer those weights to the custom model with [load_state_dict](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.load_state_dict).\n \n ```py\n import timm\n@@ -226,17 +187,14 @@ pretrained_model = timm.create_model(\"resnet50d\", pretrained=True)\n resnet50d.model.load_state_dict(pretrained_model.state_dict())\n ```\n \n-Now let's see how to make sure that when we do [`~PreTrainedModel.save_pretrained`] or [`~PreTrainedModel.push_to_hub`], the\n-code of the model is saved.\n+## AutoClass\n \n-## Registering a model with custom code to the auto classes\n+The [AutoClass](./models#model-classes) API is a shortcut for automatically loading the correct architecture for a given model. It is convenient to enable this for users loading your custom model.\n \n-If you are writing a library that extends ðŸ¤— Transformers, you may want to extend the auto classes to include your own\n-model. This is different from pushing the code to the Hub in the sense that users will need to import your library to\n-get the custom models (contrarily to automatically downloading the model code from the Hub).\n+Make sure you have the `model_type` attribute (must be different from existing model types) in the configuration class and `config_class` attribute in the model class. Use the [`~AutoConfig.register`] method to add the custom configuration and model to the [AutoClass](./models#model-classes) API.\n \n-As long as your config has a `model_type` attribute that is different from existing model types, and that your model\n-classes have the right `config_class` attributes, you can just add them to the auto classes like this:\n+> [!TIP]\n+> The first argument to [`AutoConfig.register`] must match the `model_type` attribute in the custom configuration class, and the first argument to [`AutoModel.register`] must match the `config_class` of the custom model class.\n \n ```py\n from transformers import AutoConfig, AutoModel, AutoModelForImageClassification\n@@ -246,81 +204,58 @@ AutoModel.register(ResnetConfig, ResnetModel)\n AutoModelForImageClassification.register(ResnetConfig, ResnetModelForImageClassification)\n ```\n \n-Note that the first argument used when registering your custom config to [`AutoConfig`] needs to match the `model_type`\n-of your custom config, and the first argument used when registering your custom models to any auto model class needs\n-to match the `config_class` of those models.\n+Your custom model code is now compatible with the [AutoClass](./models#autoclass) API. Users can load the model with the [AutoModel](./model_doc/auto#automodel) or [`AutoModelForImageClassification`] classes.\n \n-## Sending the code to the Hub\n+## Upload\n \n-<Tip warning={true}>\n+Upload a custom model to the [Hub](https://hf.co/models) to allow other users to easily load and use it.\n \n-This API is experimental and may have some slight breaking changes in the next releases.\n+Ensure the model directory is structured correctly as shown below. The directory should contain:\n \n-</Tip>\n+- `modeling.py`: Contains the code for `ResnetModel` and `ResnetModelForImageClassification`. This file can rely on relative imports to other files as long as they're in the same directory.\n \n-First, make sure your model is fully defined in a `.py` file. It can rely on relative imports to some other files as\n-long as all the files are in the same directory (we don't support submodules for this feature yet). For our example,\n-we'll define a `modeling_resnet.py` file and a `configuration_resnet.py` file in a folder of the current working\n-directory named `resnet_model`. The configuration file contains the code for `ResnetConfig` and the modeling file\n-contains the code of `ResnetModel` and `ResnetModelForImageClassification`.\n+> [!WARNING]\n+> When copying a Transformers' model file, replace all relative imports at the top of the `modeling.py` file to import from Transformers instead.\n \n-```\n+- `configuration.py`: Contains the code for `ResnetConfig`.\n+- `__init__.py`: Can be empty, this file allows Python `resnet_model` to be used as a module.\n+\n+```bash\n .\n â””â”€â”€ resnet_model\n     â”œâ”€â”€ __init__.py\n     â”œâ”€â”€ configuration_resnet.py\n     â””â”€â”€ modeling_resnet.py\n ```\n \n-The `__init__.py` can be empty, it's just there so that Python detects `resnet_model` can be use as a module.\n-\n-<Tip warning={true}>\n-\n-If copying a modeling files from the library, you will need to replace all the relative imports at the top of the file\n-to import from the `transformers` package.\n-\n-</Tip>\n-\n-Note that you can re-use (or subclass) an existing configuration/model.\n-\n-To share your model with the community, follow those steps: first import the ResNet model and config from the newly\n-created files:\n+To share the model, import the ResNet model and configuration.\n \n ```py\n from resnet_model.configuration_resnet import ResnetConfig\n from resnet_model.modeling_resnet import ResnetModel, ResnetModelForImageClassification\n ```\n \n-Then you have to tell the library you want to copy the code files of those objects when using the `save_pretrained`\n-method and properly register them with a given Auto class (especially for models), just run:\n+Copy the code from the model and configuration files. To make sure the AutoClass objects are saved with [`~PreTrainedModel.save_pretrained`], call the [`~PretrainedConfig.register_for_auto_class`] method. This modifies the configuration JSON file to include the AutoClass objects and mapping.\n+\n+For a model, pick the appropriate `AutoModelFor` class based on the task.\n \n ```py\n ResnetConfig.register_for_auto_class()\n ResnetModel.register_for_auto_class(\"AutoModel\")\n ResnetModelForImageClassification.register_for_auto_class(\"AutoModelForImageClassification\")\n ```\n \n-Note that there is no need to specify an auto class for the configuration (there is only one auto class for them,\n-[`AutoConfig`]) but it's different for models. Your custom model could be suitable for many different tasks, so you\n-have to specify which one of the auto classes is the correct one for your model.\n-\n-<Tip>\n-\n-Use `register_for_auto_class()` if you want the code files to be copied. If you instead prefer to use code on the Hub from another repo, \n-you don't need to call it. In cases where there's more than one auto class, you can modify the `config.json` directly using the \n-following structure:\n+To map more than one task to the model, edit `auto_map` in the configuration JSON file directly.\n \n ```json\n-\"auto_map\": {     \n-\t\"AutoConfig\": \"<your-repo-name>--<config-name>\",     \n-\t\"AutoModel\": \"<your-repo-name>--<config-name>\",\n-\t\"AutoModelFor<Task>\": \"<your-repo-name>--<config-name>\",    \n+\"auto_map\": {\n+    \"AutoConfig\": \"<your-repo-name>--<config-name>\",\n+    \"AutoModel\": \"<your-repo-name>--<config-name>\",\n+    \"AutoModelFor<Task>\": \"<your-repo-name>--<config-name>\",    \n },\n ```\n \n-</Tip>\n-\n-Next, let's create the config and models as we did before:\n+Create the configuration and model and load pretrained weights into it.\n \n ```py\n resnet50d_config = ResnetConfig(block_type=\"bottleneck\", stem_width=32, stem_type=\"deep\", avg_down=True)\n@@ -330,55 +265,33 @@ pretrained_model = timm.create_model(\"resnet50d\", pretrained=True)\n resnet50d.model.load_state_dict(pretrained_model.state_dict())\n ```\n \n-Now to send the model to the Hub, make sure you are logged in. Either run in your terminal:\n+The model is ready to be pushed to the Hub now. Log in to your Hugging Face account from the command line or notebook.\n+\n+<hfoptions id=\"push\">\n+<hfoption id=\"huggingface-CLI\">\n \n ```bash\n huggingface-cli login\n ```\n \n-or from a notebook:\n+</hfoption>\n+<hfoption id=\"notebook\">\n \n ```py\n from huggingface_hub import notebook_login\n \n notebook_login()\n ```\n \n-You can then push to your own namespace (or an organization you are a member of) like this:\n-\n-```py\n-resnet50d.push_to_hub(\"custom-resnet50d\")\n-```\n-\n-On top of the modeling weights and the configuration in json format, this also copied the modeling and\n-configuration `.py` files in the folder `custom-resnet50d` and uploaded the result to the Hub. You can check the result\n-in this [model repo](https://huggingface.co/sgugger/custom-resnet50d).\n+</hfoption>\n+</hfoptions>\n \n-See the [sharing tutorial](model_sharing) for more information on the push to Hub method.\n-\n-## Using a model with custom code\n-\n-You can use any configuration, model or tokenizer with custom code files in its repository with the auto-classes and\n-the `from_pretrained` method. All files and code uploaded to the Hub are scanned for malware (refer to the [Hub security](https://huggingface.co/docs/hub/security#malware-scanning) documentation for more information), but you should still \n-review the model code and author to avoid executing malicious code on your machine. Set `trust_remote_code=True` to use\n-a model with custom code:\n+Call [`~PreTrainedModel.push_to_hub`] on the model to upload the model to the Hub.\n \n ```py\n-from transformers import AutoModelForImageClassification\n-\n-model = AutoModelForImageClassification.from_pretrained(\"sgugger/custom-resnet50d\", trust_remote_code=True)\n-```\n-\n-It is also strongly encouraged to pass a commit hash as a `revision` to make sure the author of the models did not\n-update the code with some malicious new lines (unless you fully trust the authors of the models).\n-\n-```py\n-commit_hash = \"ed94a7c6247d8aedce4647f00f20de6875b5b292\"\n-model = AutoModelForImageClassification.from_pretrained(\n-    \"sgugger/custom-resnet50d\", trust_remote_code=True, revision=commit_hash\n-)\n+resnet50d.push_to_hub(\"custom-resnet50d\")\n ```\n \n-Note that when browsing the commit history of the model repo on the Hub, there is a button to easily copy the commit\n-hash of any commit.\n+The pretrained weights, configuration, `modeling.py` and `configuration.py` files should all be uploaded to the Hub now in a [repository](https://hf.co/sgugger/custom-resnet50d) under your namespace.\n \n+Because a custom model doesn't use the same modeling code as a Transformers' model, you need to add `trust_remode_code=True` in [`~PreTrainedModel.from_pretrained`] to load it. Refer to the load [custom models](./models#custom-models) section for more information."
        },
        {
            "sha": "09394d2229d149a2e9300469573fd6ba29c1d938",
            "filename": "docs/source/en/debugging.md",
            "status": "modified",
            "additions": 77,
            "deletions": 187,
            "changes": 264,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fdebugging.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fdebugging.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fdebugging.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -1,4 +1,4 @@\n-<!--Copyright 2021 The HuggingFace Team. All rights reserved.\n+<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n \n Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n the License. You may obtain a copy of the License at\n@@ -14,79 +14,76 @@ rendered properly in your Markdown viewer.\n \n -->\n \n-# Debugging\n+# Multi-GPU debugging\n \n-Training on multiple GPUs can be a tricky endeavor whether you're running into installation issues or communication problems between your GPUs. This debugging guide covers some issues you may run into and how to resolve them.\n+Distributed training can be tricky because you have to ensure you're using the correct CUDA version across your system. You may encounter inter-communication issues between GPUs, and there may be underflow or overflow problems in your model.\n \n-## DeepSpeed CUDA installation\n+This guide covers how to debug these issues, especially as it relates to DeepSpeed and PyTorch.\n \n-If you're using DeepSpeed, you've probably already installed it with the following command.\n+## DeepSpeed CUDA\n+\n+DeepSpeed compiles CUDA C++ which can be a potential source of errors when building PyTorch extensions that require CUDA. These errors depend on how CUDA is installed on your system. This section focuses on PyTorch built with *CUDA 10.2*\n \n ```bash\n pip install deepspeed\n ```\n \n-DeepSpeed compiles CUDA C++ code and it can be a potential source of errors when building PyTorch extensions that require CUDA. These errors depend on how CUDA is installed on your system, and this section focuses on PyTorch built with *CUDA 10.2*.\n-\n-<Tip>\n-\n-For any other installation issues, please [open an issue](https://github.com/deepspeedai/DeepSpeed/issues) with the DeepSpeed team.\n+> [!TIP]\n+> For any other installation issues, please [open an issue](https://github.com/microsoft/DeepSpeed/issues) with the DeepSpeed team.\n \n-</Tip>\n+### Non-identical toolkits\n \n-### Non-identical CUDA toolkits\n+PyTorch comes with its own CUDA toolkit, but to use DeepSpeed with PyTorch, you need to have an identical version of CUDA installed system-wide. For example, if you installed PyTorch with `cudatoolkit==10.2` in your Python environment, then you'll also need to have CUDA 10.2 installed everywhere.\n \n-PyTorch comes with its own CUDA toolkit, but to use DeepSpeed with PyTorch, you need to have an identical version of CUDA installed system-wide. For example, if you installed PyTorch with `cudatoolkit==10.2` in your Python environment, then you'll also need to have CUDA 10.2 installed system-wide. If you don't have CUDA installed system-wide, you should install it first.\n-\n-The exact location may vary from system to system, but `usr/local/cuda-10.2` is the most common location on many Unix systems. When CUDA is correctly setup and added to your `PATH` environment variable, you can find the installation location with the following command:\n+The exact location can vary from system to system, but `usr/local/cuda-10.2` is the most common location on many Unix systems. When CUDA is correctly set up and added to your `PATH` environment variable, you can find the installation location with the following command.\n \n ```bash\n which nvcc\n ```\n \n-### Multiple CUDA toolkits\n+### Multiple toolkits\n \n-You may also have more than one CUDA toolkit installed system-wide.\n+You may also have more than one CUDA toolkit installed on your system.\n \n ```bash\n /usr/local/cuda-10.2\n /usr/local/cuda-11.0\n ```\n \n-Typically, package installers set the paths to whatever the last version was installed. If the package build fails because it can't find the right CUDA version (despite it being installed system-wide already), then you need to configure the `PATH` and `LD_LIBRARY_PATH` environment variables to point to the correct path.\n+Typically, package installers set the paths to whatever the last version was installed. If the package build fails because it can't find the right CUDA version (despite it being installed already), then you need to configure the `PATH` and `LD_LIBRARY_PATH` environment variables to point to the correct path.\n \n-Take a look at the contents of these environment variables first:\n+Take a look at the contents of the following environment variables first.\n \n ```bash\n echo $PATH\n echo $LD_LIBRARY_PATH\n ```\n \n-`PATH` lists the locations of the executables and `LD_LIBRARY_PATH` lists where to look for shared libraries. Earlier entries are prioritized over later ones, and `:` is used to separate multiple entries. To tell the build program where to find the specific CUDA toolkit you want, insert the correct path to list first. This command prepends rather than overwrites the existing values.\n+`PATH` lists the locations of the executables and `LD_LIBRARY_PATH` lists where to look for shared libraries. Earlier entries are prioritized over later ones, and `:` is used to separate multiple entries. To find a specific CUDA toolkit, insert the correct path to list first. This command prepends rather than overwrites the existing values.\n \n ```bash\n # adjust the version and full path if needed\n export PATH=/usr/local/cuda-10.2/bin:$PATH\n export LD_LIBRARY_PATH=/usr/local/cuda-10.2/lib64:$LD_LIBRARY_PATH\n ```\n \n-In addition, you should also check the directories you assign actually exist. The `lib64` sub-directory contains various CUDA `.so` objects (like `libcudart.so`) and while it is unlikely your system names them differently, you should check the actual names and change them accordingly.\n+In addition, you should also check that the assigned directories actually exist. The `lib64` sub-directory contains various CUDA `.so` objects (like `libcudart.so`), and while it is unlikely your system names them differently, you should check the actual names and change them accordingly.\n \n-### Older CUDA versions\n+### Older versions\n \n Sometimes, older CUDA versions may refuse to build with newer compilers. For example, if you have `gcc-9` but CUDA wants `gcc-7`. Usually, installing the latest CUDA toolkit enables support for the newer compiler.\n \n-You could also install an older version of the compiler in addition to the one you're currently using (or it may already be installed but it's not used by default and the build system can't see it). To resolve this, you can create a symlink to give the build system visibility to the older compiler.\n+You could also install an older version of the compiler in addition to the one you're currently using (or it may already be installed but it's not used by default and the build system can't see it). To resolve this, create a symlink to give the build system visibility to the older compiler.\n \n ```bash\n-# adapt the path to your system\n+# adjust the path to your system\n sudo ln -s /usr/bin/gcc-7  /usr/local/cuda-10.2/bin/gcc\n sudo ln -s /usr/bin/g++-7  /usr/local/cuda-10.2/bin/g++\n ```\n \n ### Prebuild\n \n-If you're still having issues with installing DeepSpeed or if you're building DeepSpeed at run time, you can try to prebuild the DeepSpeed modules before installing them. To make a local build for DeepSpeed:\n+If you're still having issues with installing DeepSpeed or if you're building DeepSpeed at run time, try to prebuild the DeepSpeed modules before installing them. Run the commands below to make a local build for DeepSpeed.\n \n ```bash\n git clone https://github.com/deepspeedai/DeepSpeed/\n@@ -97,19 +94,16 @@ TORCH_CUDA_ARCH_LIST=\"8.6\" DS_BUILD_CPU_ADAM=1 DS_BUILD_UTILS=1 pip install . \\\n --disable-pip-version-check 2>&1 | tee build.log\n ```\n \n-<Tip>\n-\n-To use NVMe offload, add the `DS_BUILD_AIO=1` parameter to the build command and make sure you install the libaio-dev package system-wide.\n+> [!TIP]\n+> Add the `DS_BUILD_AIO=1` parameter to the build command to use NVMe offload. Make sure you install the libaio-dev package across your system.\n \n-</Tip>\n-\n-Next, you'll have to specify your GPU's architecture by editing the `TORCH_CUDA_ARCH_LIST` variable (find a complete list of NVIDIA GPUs and their corresponding architectures on this [page](https://developer.nvidia.com/cuda-gpus)). To check the PyTorch version that corresponds to your architecture, run the following command:\n+Next, specify your GPUs architecture by editing the `TORCH_CUDA_ARCH_LIST` variable (find a complete list of NVIDIA GPUs and their corresponding architectures on this [page](https://developer.nvidia.com/cuda-gpus)). To check the PyTorch version that corresponds to your architecture, run the following command.\n \n ```bash\n python -c \"import torch; print(torch.cuda.get_arch_list())\"\n ```\n \n-Find the architecture for a GPU with the following command:\n+Find the architecture for a GPU with the following command.\n \n <hfoptions id=\"arch\">\n <hfoption id=\"same GPUs\">\n@@ -121,24 +115,22 @@ CUDA_VISIBLE_DEVICES=0 python -c \"import torch; print(torch.cuda.get_device_capa\n </hfoption>\n <hfoption id=\"specific GPU\">\n \n-To find the architecture for GPU `0`:\n+Run the following command to find the architecture for GPU `0`. The results will show a value for `major` and `minor`, which is your GPU architecture. The GPU architecture below is `8.6`.\n \n ```bash\n CUDA_VISIBLE_DEVICES=0 python -c \"import torch; \\\n print(torch.cuda.get_device_properties(torch.device('cuda')))\n \"_CudaDeviceProperties(name='GeForce RTX 3090', major=8, minor=6, total_memory=24268MB, multi_processor_count=82)\"\n ```\n \n-This means your GPU architecture is `8.6`.\n-\n </hfoption>\n </hfoptions>\n \n If you get `8, 6`, then you can set `TORCH_CUDA_ARCH_LIST=\"8.6\"`. For multiple GPUs with different architectures, list them like `TORCH_CUDA_ARCH_LIST=\"6.1;8.6\"`.\n \n It is also possible to not specify `TORCH_CUDA_ARCH_LIST` and the build program automatically queries the GPU architecture of the build. However, it may or may not match the actual GPU on the target machine which is why it is better to explicitly specify the correct architecture.\n \n-For training on multiple machines with the same setup, you'll need to make a binary wheel:\n+For training on multiple machines with the same setup, you'll need to make a binary wheel as shown below.\n \n ```bash\n git clone https://github.com/deepspeedai/DeepSpeed/\n@@ -148,88 +140,64 @@ TORCH_CUDA_ARCH_LIST=\"8.6\" DS_BUILD_CPU_ADAM=1 DS_BUILD_UTILS=1 \\\n python setup.py build_ext -j8 bdist_wheel\n ```\n \n-This command generates a binary wheel that'll look something like `dist/deepspeed-0.3.13+8cd046f-cp38-cp38-linux_x86_64.whl`. Now you can install this wheel locally or on another machine.\n+This command generates a binary wheel that'll look something like `dist/deepspeed-0.3.13+8cd046f-cp38-cp38-linux_x86_64.whl`. Install this wheel locally or on another machine.\n \n ```bash\n pip install deepspeed-0.3.13+8cd046f-cp38-cp38-linux_x86_64.whl\n ```\n \n-## Multi-GPU Network Issues Debug\n+## Communication\n \n-When training or inferencing with `DistributedDataParallel` and multiple GPU, if you run into issue of inter-communication between processes and/or nodes, you can use the following script to diagnose network issues.\n+Distributed training involves communication between processes and or nodes and this can be a potential source of errors.\n \n-```bash\n-wget https://raw.githubusercontent.com/huggingface/transformers/main/scripts/distributed/torch-distributed-gpu-test.py\n-```\n-\n-For example to test how 2 GPUs interact do:\n+Download the script below to diagnose network issues, and then run it to test GPU communication. The example command below tests how two GPUs communicate. Adjust the `--nproc_per_node` and `--nnodes` parameters to adapt it to your system.\n \n ```bash\n+wget https://raw.githubusercontent.com/huggingface/transformers/main/scripts/distributed/torch-distributed-gpu-test.py\n python -m torch.distributed.run --nproc_per_node 2 --nnodes 1 torch-distributed-gpu-test.py\n ```\n-If both processes can talk to each and allocate GPU memory each will print an OK status.\n-\n-For more GPUs or nodes adjust the arguments in the script.\n \n-You will find a lot more details inside the diagnostics script and even a recipe to how you could run it in a SLURM environment.\n+The script prints an `OK` status if both GPUs are able to communicate and allocate memory. Take a closer look at the diagnostic script for more details and a recipe for running it in a SLURM environment.\n \n-An additional level of debug is to add `NCCL_DEBUG=INFO` environment variable as follows:\n+Add the `NCCL_DEBUG=INFO` environment variable to report more NCCL-related debugging information.\n \n ```bash\n NCCL_DEBUG=INFO python -m torch.distributed.run --nproc_per_node 2 --nnodes 1 torch-distributed-gpu-test.py\n ```\n \n-This will dump a lot of NCCL-related debug information, which you can then search online if you find that some problems are reported. Or if you're not sure how to interpret the output you can share the log file in an Issue.\n-\n-\n-\n-## Underflow and Overflow Detection\n-\n-<Tip>\n+## Underflow and overflow detection\n \n-This feature is currently available for PyTorch-only.\n+Underflow and overflow can occur when activations or weights are `inf`, `nan`, and when `loss=NaN`. This may indicate an underflow or overflow issue. To detect these issues, activate the `DebugUnderflowOverflow` module in [`TrainingArguments.debug`] or import and add the module to your own training loop or another trainer class.\n \n-</Tip>\n+<hfoptions id=\"overflow\">\n+<hfoption id=\"Trainer\">\n \n-<Tip>\n+```py\n+from transformers import TrainingArguments\n \n-For multi-GPU training it requires DDP (`torch.distributed.launch`).\n-\n-</Tip>\n-\n-<Tip>\n-\n-This feature can be used with any `nn.Module`-based model.\n-\n-</Tip>\n-\n-If you start getting `loss=NaN` or the model exhibits some other abnormal behavior due to `inf` or `nan` in\n-activations or weights one needs to discover where the first underflow or overflow happens and what led to it. Luckily\n-you can accomplish that easily by activating a special module that will do the detection automatically.\n-\n-If you're using [`Trainer`], you just need to add:\n-\n-```bash\n---debug underflow_overflow\n+args = TrainingArguments(\n+    debug=\"underflow_overflow\",\n+    ...\n+)\n ```\n \n-to the normal command line arguments, or pass `debug=\"underflow_overflow\"` when creating the\n-[`TrainingArguments`] object.\n-\n-If you're using your own training loop or another Trainer you can accomplish the same with:\n+</hfoption>\n+<hfoption id=\"PyTorch training loop\">\n \n-```python\n+```py\n from transformers.debug_utils import DebugUnderflowOverflow\n \n debug_overflow = DebugUnderflowOverflow(model)\n ```\n \n-[`~debug_utils.DebugUnderflowOverflow`] inserts hooks into the model that immediately after each\n-forward call will test input and output variables and also the corresponding module's weights. As soon as `inf` or\n-`nan` is detected in at least one element of the activations or weights, the program will assert and print a report\n-like this (this was caught with `google/mt5-small` under fp16 mixed precision):\n+</hfoption>\n+</hfoptions>\n \n-```\n+The [`~debug_utils.DebugUnderflowOverflow`] module inserts hooks into the model to test the input and output variables and the corresponding model weights after each forward call. If `inf` or `nan` is detected in at least one element of the activations or weights, the module prints a report like the one shown below.\n+\n+The example below is for fp16 mixed precision training with [google/mt5-small](https://huggingface.co/google/mt5-small).\n+\n+```shell\n Detected inf/nan during batch_number=0\n Last 21 forward frames:\n abs min  abs max  metadata\n@@ -269,48 +237,20 @@ abs min  abs max  metadata\n 0.00e+00      inf output\n ```\n \n-The example output has been trimmed in the middle for brevity.\n-\n-The second column shows the value of the absolute largest element, so if you have a closer look at the last few frames,\n-the inputs and outputs were in the range of `1e4`. So when this training was done under fp16 mixed precision the very\n-last step overflowed (since under `fp16` the largest number before `inf` is `64e3`). To avoid overflows under\n-`fp16` the activations must remain way below `1e4`, because `1e4 * 1e4 = 1e8` so any matrix multiplication with\n-large activations is going to lead to a numerical overflow condition.\n-\n-At the very start of the trace you can discover at which batch number the problem occurred (here `Detected inf/nan during batch_number=0` means the problem occurred on the first batch).\n+At the start of the report, you can see which batch number the error occurred. In this case, it occurred on the first batch.\n \n-Each reported frame starts by declaring the fully qualified entry for the corresponding module this frame is reporting\n-for. If we look just at this frame:\n+Each frame describes the module it is reporting on. For example, the frame below inspected `encoder.block.2.layer.1.layer_norm`. This indicates the layer norm in the first layer of the second block of the encoder. The forward calls are to `T5LayerNorm`.\n \n-```\n+```shell\n                   encoder.block.2.layer.1.layer_norm T5LayerNorm\n 8.69e-02 4.18e-01 weight\n 2.65e-04 3.42e+03 input[0]\n 1.79e-06 4.65e+00 output\n ```\n \n-Here, `encoder.block.2.layer.1.layer_norm` indicates that it was a layer norm for the first layer, of the second\n-block of the encoder. And the specific calls of the `forward` is `T5LayerNorm`.\n-\n-Let's look at the last few frames of that report:\n+The last frame reports on the `Dropout.forward` function. It called the `dropout` attribute from inside the `DenseReluDense` class. You can observe that the overflow (`inf`) occurred in the first layer of the encoders second block in the first batch. The absolute largest input element was 6.27e+04.\n \n-```\n-Detected inf/nan during batch_number=0\n-Last 21 forward frames:\n-abs min  abs max  metadata\n-[...]\n-                  encoder.block.2.layer.1.DenseReluDense.wi_0 Linear\n-2.17e-07 4.50e+00 weight\n-1.79e-06 4.65e+00 input[0]\n-2.68e-06 3.70e+01 output\n-                  encoder.block.2.layer.1.DenseReluDense.wi_1 Linear\n-8.08e-07 2.66e+01 weight\n-1.79e-06 4.65e+00 input[0]\n-1.27e-04 2.37e+02 output\n-                  encoder.block.2.layer.1.DenseReluDense.wo Linear\n-1.01e-06 6.44e+00 weight\n-0.00e+00 9.74e+03 input[0]\n-3.18e-04 6.27e+04 output\n+```shell\n                   encoder.block.2.layer.1.DenseReluDense T5DenseGatedGeluDense\n 1.79e-06 4.65e+00 input[0]\n 3.18e-04 6.27e+04 output\n@@ -319,22 +259,11 @@ abs min  abs max  metadata\n 0.00e+00      inf output\n ```\n \n-The last frame reports for `Dropout.forward` function with the first entry for the only input and the second for the\n-only output. You can see that it was called from an attribute `dropout` inside `DenseReluDense` class. We can see\n-that it happened during the first layer, of the 2nd block, during the very first batch. Finally, the absolute largest\n-input elements was `6.27e+04` and same for the output was `inf`.\n-\n-You can see here, that `T5DenseGatedGeluDense.forward` resulted in output activations, whose absolute max value was\n-around 62.7K, which is very close to fp16's top limit of 64K. In the next frame we have `Dropout` which renormalizes\n-the weights, after it zeroed some of the elements, which pushes the absolute max value to more than 64K, and we get an\n-overflow (`inf`).\n-\n-As you can see it's the previous frames that we need to look into when the numbers start going into very large for fp16\n-numbers.\n+The `T5DenseGatedGeluDense.forward` function output activations had an absolute maximum value of 6.27e+04 which is close to fp16s maximum limit of 6.4e+04. In the next step, `Dropout` renormalizes the weights, after zeroing some elements, which pushes the absolute maximum value to greater than 6.4e+04 resulting in an overflow.\n \n-Let's match the report to the code from `models/t5/modeling_t5.py`:\n+Now that you know where the error is happening, you can investigate the modeling code in [modeling_t5.py](https://github.com/huggingface/transformers/blob/main/src/transformers/models/t5/modeling_t5.py).\n \n-```python\n+```py\n class T5DenseGatedGeluDense(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -353,29 +282,11 @@ class T5DenseGatedGeluDense(nn.Module):\n         return hidden_states\n ```\n \n-Now it's easy to see the `dropout` call, and all the previous calls as well.\n-\n-Since the detection is happening in a forward hook, these reports are printed immediately after each `forward`\n-returns.\n-\n-Going back to the full report, to act on it and to fix the problem, we need to go a few frames up where the numbers\n-started to go up and most likely switch to the `fp32` mode here, so that the numbers don't overflow when multiplied\n-or summed up. Of course, there might be other solutions. For example, we could turn off `amp` temporarily if it's\n-enabled, after moving the original `forward` into a helper wrapper, like so:\n-\n-```python\n-def _forward(self, hidden_states):\n-    hidden_gelu = self.gelu_act(self.wi_0(hidden_states))\n-    hidden_linear = self.wi_1(hidden_states)\n-    hidden_states = hidden_gelu * hidden_linear\n-    hidden_states = self.dropout(hidden_states)\n-    hidden_states = self.wo(hidden_states)\n-    return hidden_states\n-\n+One solution is to go back a few steps before the values started growing too large and switch to fp32 so the numbers don't overflow when multiplied or summed. Another potential solution is to temporarily disable mixed precision training (`amp`).\n \n+```py\n import torch\n \n-\n def forward(self, hidden_states):\n     if torch.is_autocast_enabled():\n         with torch.cuda.amp.autocast(enabled=False):\n@@ -384,14 +295,11 @@ def forward(self, hidden_states):\n         return self._forward(hidden_states)\n ```\n \n-Since the automatic detector only reports on inputs and outputs of full frames, once you know where to look, you may\n-want to analyse the intermediary stages of any specific `forward` function as well. In such a case you can use the\n-`detect_overflow` helper function to inject the detector where you want it, for example:\n+The report only returns inputs and outputs of full frames, so you may also want to analyze the intermediate values of any `forward` function as well. Add the `detect_overflow` function after the forward calls to track `inf` or `nan` values in the intermediate `forwarded_states`.\n \n-```python\n+```py\n from debug_utils import detect_overflow\n \n-\n class T5LayerFF(nn.Module):\n     [...]\n \n@@ -403,40 +311,25 @@ class T5LayerFF(nn.Module):\n         return hidden_states + self.dropout(forwarded_states)\n ```\n \n-You can see that we added 2 of these and now we track if `inf` or `nan` for `forwarded_states` was detected\n-somewhere in between.\n-\n-Actually, the detector already reports these because each of the calls in the example above is a `nn.Module`, but\n-let's say if you had some local direct calculations this is how you'd do that.\n-\n-Additionally, if you're instantiating the debugger in your own code, you can adjust the number of frames printed from\n-its default, e.g.:\n+Finally, you can configure the number of frames printed by [`~debug_utils.DebugUnderflowOverflow`].\n \n-```python\n+```py\n from transformers.debug_utils import DebugUnderflowOverflow\n \n debug_overflow = DebugUnderflowOverflow(model, max_frames_to_save=100)\n ```\n \n-### Specific batch absolute min and max value tracing\n+### Batch tracing\n \n-The same debugging class can be used for per-batch tracing with the underflow/overflow detection feature turned off.\n+[`~debug_utils.DebugUnderflowOverflow`] is able to trace the absolute minimum and maximum values in each batch with the underflow and overflow feature disabled. This is useful for identifying where errors are occurring in the model.\n \n-Let's say you want to watch the absolute min and max values for all the ingredients of each `forward` call of a given\n-batch, and only do that for batches 1 and 3. Then you instantiate this class as:\n+The example below shows how to trace the minimum and maximum values in batches 1 and 3 (batches are zero-indexd).\n \n-```python\n+```py\n debug_overflow = DebugUnderflowOverflow(model, trace_batch_nums=[1, 3])\n ```\n \n-And now full batches 1 and 3 will be traced using the same format as the underflow/overflow detector does.\n-\n-Batches are 0-indexed.\n-\n-This is helpful if you know that the program starts misbehaving after a certain batch number, so you can fast-forward\n-right to that area. Here is a sample truncated output for such configuration:\n-\n-```\n+```shell\n                   *** Starting batch number=1 ***\n abs min  abs max  metadata\n                   shared Embedding\n@@ -465,13 +358,10 @@ abs min  abs max  metadata\n [...]\n ```\n \n-Here you will get a huge number of frames dumped - as many as there were forward calls in your model, so it may or may\n-not what you want, but sometimes it can be easier to use for debugging purposes than a normal debugger. For example, if\n-a problem starts happening at batch number 150. So you can dump traces for batches 149 and 150 and compare where\n-numbers started to diverge.\n+[`~debug_utils.DebugUnderflowOverflow`] reports on a large number of frames which is easier for debugging. Once you know where a problem is occurring, say batch 150, then you can focus the trace for batches 149 and 150 and compare where the numbers are diverging.\n \n-You can also specify the batch number after which to stop the training, with:\n+It is also possible to abort the trace after a certain batch number, for example, batch 3.\n \n-```python\n+```py\n debug_overflow = DebugUnderflowOverflow(model, trace_batch_nums=[1, 3], abort_after_batch_num=3)\n ```"
        },
        {
            "sha": "4a84f93383435d3bc6827ee91276446b4fdb8e55",
            "filename": "docs/source/en/deepspeed.md",
            "status": "modified",
            "additions": 289,
            "deletions": 496,
            "changes": 785,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fdeepspeed.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fdeepspeed.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fdeepspeed.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,27 +16,21 @@ rendered properly in your Markdown viewer.\n \n # DeepSpeed\n \n-[DeepSpeed](https://www.deepspeed.ai/) is a PyTorch optimization library that makes distributed training memory-efficient and fast. At its core is the [Zero Redundancy Optimizer (ZeRO)](https://hf.co/papers/1910.02054) which enables training large models at scale. ZeRO works in several stages:\n+[DeepSpeed](https://www.deepspeed.ai/) is designed to optimize distributed training for large models with data, model, pipeline, and even a combination of all three [parallelism](./perf_train_gpu_many) strategies to provide better memory efficiency and faster training speeds. This is achieved with the [Zero Redundancy Optimizer (ZeRO)](https://hf.co/papers/1910.02054) which consists of three stages.\n \n-* ZeRO-1, optimizer state partitioning across GPUs\n-* ZeRO-2, gradient partitioning across GPUs\n-* ZeRO-3, parameter partitioning across GPUs\n+| ZeRO stage | description |\n+|---|---|\n+| 1 | partition optimizer states |\n+| 2 | partition optimizer and gradient states |\n+| 3 | partition optimizer, gradient, and parameters |\n \n-In GPU-limited environments, ZeRO also enables offloading optimizer memory and computation from the GPU to the CPU to fit and train really large models on a single GPU. DeepSpeed is integrated with the Transformers [`Trainer`] class for all ZeRO stages and offloading. All you need to do is provide a config file or you can use a provided template. For inference, Transformers support ZeRO-3 and offloading since it allows loading huge models.\n+Each stage progressively saves more memory, allowing really large models to fit and train on a single GPU. All ZeRO stages, offloading optimizer memory and computations from the GPU to the CPU are integrated with [`Trainer`]. Provide a config file or one of the example templates to [`Trainer`] to enable DeepSpeed features.\n \n-This guide will walk you through how to deploy DeepSpeed training, the features you can enable, how to setup the config files for different ZeRO stages, offloading, inference, and using DeepSpeed without the [`Trainer`].\n+This guide walks you through setting up a DeepSpeed config file, how to enable its features in [`Trainer`], and deploy for training.\n \n-## Installation\n+Install DeepSpeed from either PyPI or Transformers. For more detailed installation instructions, refer to the DeepSpeed [installation](https://www.deepspeed.ai/tutorials/advanced-install/) or GitHUB [README](https://github.com/microsoft/deepspeed#installation).\n \n-DeepSpeed is available to install from PyPI or Transformers (for more detailed installation options, take a look at the DeepSpeed [installation details](https://www.deepspeed.ai/tutorials/advanced-install/) or the GitHub [README](https://github.com/deepspeedai/DeepSpeed#installation)).\n-\n-<Tip>\n-\n-If you're having difficulties installing DeepSpeed, check the [DeepSpeed CUDA installation](../debugging#deepspeed-cuda-installation) guide. While DeepSpeed has a pip installable PyPI package, it is highly recommended to [install it from source](https://www.deepspeed.ai/tutorials/advanced-install/#install-deepspeed-from-source) to best match your hardware and to support certain features, like 1-bit Adam, which arenâ€™t available in the PyPI distribution.\n-\n-</Tip>\n-\n-<hfoptions id=\"install\">\n+<hfoptions id=\"installation\">\n <hfoption id=\"PyPI\">\n \n ```bash\n@@ -53,9 +47,12 @@ pip install transformers[deepspeed]\n </hfoption>\n </hfoptions>\n \n-## Memory requirements\n+> [!WARNING]\n+> Refer to the [DeepSpeed CUDA installation](./debugging#deepspeed-cuda-issues) if you're having trouble with your installation. While DeepSpeed has a pip installable package, it is highly recommended to [install it from source](https://www.deepspeed.ai/tutorials/advanced-install/#install-deepspeed-from-source) to ensure it matches your hardware and to support certain features which aren't available in the PyPI distribution.\n \n-Before you begin, it is a good idea to check whether you have enough GPU and CPU memory to fit your model. DeepSpeed provides a tool for estimating the required CPU/GPU memory. For example, to estimate the memory requirements for the [bigscience/T0_3B](bigscience/T0_3B) model on a single GPU:\n+DeepSpeed provides a tool for estimating the required CPU and GPU memory for the parameters, optimizer and gradient states. You'll also to need to reserve some memory for the CUDA kernels and activations.\n+\n+Run the command below to check the memory requirements for [bigscience/T0_3B](https://huggingface.co/docs/transformers/main/en/bigscience/T0_3B) on a single GPU.\n \n ```bash\n $ python -c 'from transformers import AutoModel; \\\n@@ -75,110 +72,98 @@ SW: Model with 2783M total params, 65M largest layer params.\n    15.56GB |  46.91GB | offload_param=none, offload_optimizer=none, zero_init=0\n ```\n \n-This means you either need a single 80GB GPU without CPU offload or a 8GB GPU and a ~60GB CPU to offload to (these are just the memory requirements for the parameters, optimizer states and gradients, and you'll need a bit more for the CUDA kernels and activations). You should also consider the tradeoff between cost and speed because it'll be cheaper to rent or buy a smaller GPU but it'll take longer to train your model.\n-\n-If you have enough GPU memory make sure you disable CPU/NVMe offload to make everything faster.\n+> [!TIP]\n+> If you have enough GPU memory, disable CPU and NVMe offload to speed everything up.\n \n-## Select a ZeRO stage\n+## Choosing a ZeRO stage\n \n-After you've installed DeepSpeed and have a better idea of your memory requirements, the next step is selecting a ZeRO stage to use. In order of fastest and most memory-efficient:\n+Consider the table below to help you choose the appropriate ZeRO stage for training because there is a trade-off between training speed and memory usage. The table orders the ZeRO stages from fastest to slowest and from least memory usage to most.\n \n-| Fastest          | Memory efficient |\n-|------------------|------------------|\n-| ZeRO-1           | ZeRO-3 + offload |\n-| ZeRO-2           | ZeRO-3           |\n+| fastest | least memory usage |\n+|---|---|\n+| ZeRO-1 | ZeRO-3 + offload |\n+| ZeRO-2 | ZeRO-3 |\n | ZeRO-2 + offload | ZeRO-2 + offload |\n-| ZeRO-3           | ZeRO-2           |\n-| ZeRO-3 + offload | ZeRO-1           |\n-\n-To find what works best for you, start with the fastest approach and if you run out of memory, try the next stage which is slower but more memory efficient. Feel free to work in whichever direction you prefer (starting with the most memory efficient or fastest) to discover the appropriate balance between speed and memory usage.\n-\n-A general process you can use is (start with batch size of 1):\n+| ZeRO-3 | ZeRO-2 |\n+| ZeRO-3 + offload | ZeRO-1 |\n \n-1. enable gradient checkpointing\n-2. try ZeRO-2\n-3. try ZeRO-2 and offload the optimizer\n-4. try ZeRO-3\n-5. try ZeRO-3 and offload parameters to the CPU\n-6. try ZeRO-3 and offload parameters and the optimizer to the CPU\n-7. try lowering various default values like a narrower search beam if you're using the [`~GenerationMixin.generate`] method\n-8. try mixed half-precision (fp16 on older GPU architectures and bf16 on Ampere) over full-precision weights\n-9. add more hardware if possible or enable Infinity to offload parameters and the optimizer to a NVMe\n-10. once you're not running out of memory, measure effective throughput and then try to increase the batch size as large as you can to maximize GPU efficiency\n-11. lastly, try to optimize your training setup by disabling some offload features or use a faster ZeRO stage and increasing/decreasing the batch size to find the best tradeoff between speed and memory usage\n+Decide the type of performance you're optimizing for, speed or memory, and then work backwards to discover the best ZeRO stage for your use case. For example, if you're optimizing for speed, start with the fastest ZeRO stage and if you run out of memory, try the next stage which is slower but more memory efficient.\n \n+## Config file\n \n-## DeepSpeed configuration file\n+Once you've decided on a ZeRO stage, set up a config file to enable DeepSpeed with [`Trainer`]. The config file contains all the parameters for how to configure and set up your training. When the training script is executed, DeepSpeed logs the configuration from [`Trainer`] to the console so you can see exactly what's being used.\n \n-DeepSpeed works with the [`Trainer`] class by way of a config file containing all the parameters for configuring how you want setup your training run. When you execute your training script, DeepSpeed logs the configuration it received from [`Trainer`] to the console so you can see exactly what configuration was used.\n+> [!TIP]\n+> Find a complete list of DeepSpeed configuration options on the [DeepSpeed Configuration JSON](https://www.deepspeed.ai/docs/config-json/) reference. There are also practical examples of various DeepSpeed configuration examples in the [DeepSpeedExamples](https://github.com/microsoft/DeepSpeedExamples) main [DeepSpeed](https://github.com/microsoft/DeepSpeed) repository. Run the command below to quickly find specific examples.\n+>\n+> ```bash\n+> git clone https://github.com/microsoft/DeepSpeedExamples\n+> cd DeepSpeedExamples\n+> find . -name '*json'\n+> # find examples with the Lamb optimizer\n+> grep -i Lamb $(find . -name '*json')\n+> ```\n \n-<Tip>\n-\n-Find a complete list of DeepSpeed configuration options on the [DeepSpeed Configuration JSON](https://www.deepspeed.ai/docs/config-json/) reference. You can also find more practical examples of various DeepSpeed configuration examples on the [DeepSpeedExamples](https://github.com/deepspeedai/DeepSpeedExamples) repository or the main [DeepSpeed](https://github.com/deepspeedai/DeepSpeed) repository. To quickly find specific examples, you can:\n-\n-```bash\n-git clone https://github.com/deepspeedai/DeepSpeedExamples\n-cd DeepSpeedExamples\n-find . -name '*json'\n-# find examples with the Lamb optimizer\n-grep -i Lamb $(find . -name '*json')\n-```\n-\n-</Tip>\n-\n-The DeepSpeed configuration file is passed as a path to a JSON file if you're training from the command line interface or as a nested `dict` object if you're using the [`Trainer`] in a notebook setting.\n+The config file is passed as a path to a JSON file if you're training from the command line interface or as a nested dict object if you're using [`Trainer`] in a notebook.\n \n <hfoptions id=\"pass-config\">\n <hfoption id=\"path to file\">\n \n ```py\n-TrainingArguments(..., deepspeed=\"path/to/deepspeed_config.json\")\n+TrainingArguments(\n+    deepspeed=\"path/to/deepspeed_config.json\",\n+    ...,\n+)\n ```\n \n </hfoption>\n <hfoption id=\"nested dict\">\n \n ```py\n ds_config_dict = dict(scheduler=scheduler_params, optimizer=optimizer_params)\n-args = TrainingArguments(..., deepspeed=ds_config_dict)\n-trainer = Trainer(model, args, ...)\n+args = TrainingArguments(\n+    deepspeed=ds_config_dict,\n+    ...,\n+)\n+trainer = Trainer(\n+    model,\n+    args,\n+    ...,\n+)\n ```\n \n </hfoption>\n </hfoptions>\n \n-### DeepSpeed and Trainer parameters\n-\n-There are three types of configuration parameters:\n+### DeepSpeed versus Trainer parameters\n \n-1. Some of the configuration parameters are shared by [`Trainer`] and DeepSpeed, and it can be difficult to identify errors when there are conflicting definitions. To make it easier, these shared configuration parameters are configured from the [`Trainer`] command line arguments.\n+There are three types of config parameters.\n \n-2. Some configuration parameters that are automatically derived from the model configuration so you don't need to manually adjust these values. The [`Trainer`] uses a configuration value `auto` to determine set the most correct or efficient value. You could set your own configuration parameters explicitly, but you must take care to ensure the [`Trainer`] arguments and DeepSpeed configuration parameters agree. Mismatches may cause the training to fail in very difficult to detect ways!\n+1. Some config parameters are shared by DeepSpeed and [`Trainer`] making it difficult to identify errors when there are conflicting definitions. In this case, configure these parameters from the [`Trainer`] command line arguments.\n+1. Some config parameters are automatically derived from the model configuration and don't need to be manually configured. [`Trainer`] uses the config value `auto` to set the most correct or efficient option. You could define these parameters explicitly, but you must take care to ensure the [`Trainer`] and DeepSpeed config parameters match. Mismatches may cause training to fail in very difficult to detect ways.\n+1. Some config parameters are specific to DeepSpeed and should be manually set based on your training requirements.\n \n-3. Some configuration parameters specific to DeepSpeed only which need to be manually set based on your training needs.\n+There are two ways to modify the config parameters.\n \n-You could also modify the DeepSpeed configuration and edit [`TrainingArguments`] from it:\n+> [!TIP]\n+> Some values, such as `scheduler.params.total_num_steps`, are calculated by [`Trainer`] during training.\n \n-1. Create or load a DeepSpeed configuration to use as the main configuration\n-2. Create a [`TrainingArguments`] object based on these DeepSpeed configuration values\n+1. Create or load a DeepSpeed config to use as the main config.\n+1. Create a [`TrainingArguments`] object based on the DeepSpeed config values.\n \n-Some values, such as `scheduler.params.total_num_steps` are calculated by the [`Trainer`] during training.\n+### ZeRO stage\n \n-### ZeRO configuration\n+Each ZeRO stage config is defined in `zero_optimization`.\n \n-There are three configurations, each corresponding to a different ZeRO stage. Stage 1 is not as interesting for scalability, and this guide focuses on stages 2 and 3. The `zero_optimization` configuration contains all the options for what to enable and how to configure them. For a more detailed explanation of each parameter, take a look at the [DeepSpeed Configuration JSON](https://www.deepspeed.ai/docs/config-json/) reference.\n+For a more detailed explanation of each parameter, refer to the [DeepSpeed Configuration JSON](https://www.deepspeed.ai/docs/config-json/) reference. These parameters must be set up with DeepSpeed because [`Trainer`] doesn't provide equivalent command line arguments.\n \n-<Tip warning={true}>\n-DeepSpeed doesnâ€™t validate parameter names and any typos fallback on the parameter's default setting. You can watch the DeepSpeed engine startup log messages to see what values it is going to use.\n-\n-</Tip>\n-\n-The following configurations must be setup with DeepSpeed because the [`Trainer`] doesn't provide equivalent command line arguments.\n+> [!WARNING]\n+> DeepSpeed doesn't validate parameter names and any typos will fallback on the parameters default setting. Observe the DeepSpeed engine startup log messages to see what values are being used.\n \n <hfoptions id=\"zero-config\">\n <hfoption id=\"ZeRO-1\">\n \n-ZeRO-1 shards the optimizer states across GPUs, and you can expect a tiny speed up. The ZeRO-1 config can be setup like this:\n+ZeRO-1 shards the optimizer states across GPUs and you can expect a small speed up.\n \n ```yml\n {\n@@ -191,11 +176,11 @@ ZeRO-1 shards the optimizer states across GPUs, and you can expect a tiny speed\n </hfoption>\n <hfoption id=\"ZeRO-2\">\n \n-ZeRO-2 shards the optimizer and gradients across GPUs. This stage is primarily used for training since its features are not relevant to inference. Some important parameters to configure for better performance include:\n+ZeRO-2 shards the optimizer and gradient states across GPUs. This stage is primarily used for training since its features are not relevant to inference. Some important parameters to configure for better performance include the following.\n \n * `offload_optimizer` should be enabled to reduce GPU memory usage.\n-* `overlap_comm` when set to `true` trades off increased GPU memory usage to lower allreduce latency. This feature uses 4.5x the `allgather_bucket_size` and `reduce_bucket_size` values. In this example, they're set to `5e8` which means it requires 9GB of GPU memory. If your GPU memory is 8GB or less, you should reduce `overlap_comm` to lower the memory requirements and prevent an out-of-memory (OOM) error.\n-* `allgather_bucket_size` and `reduce_bucket_size` trade off available GPU memory for communication speed. The smaller their values, the slower communication is and the more GPU memory is available. You can balance, for example, whether a bigger batch size is more important than a slightly slower training time.\n+* `overlap_comm` when set to `true` uses more GPU memory in exchange for lower allreduce latency. This feature uses 4.5x the `allgather_bucket_size` and `reduce_bucket_size` values. In this example, they're set to `5e8` which means it requires 9GB of GPU memory. If your GPU memory is 8GB or less, you should reduce `overlap_comm` to lower the memory requirements and prevent an out-of-memory (OOM) error.\n+* `allgather_bucket_size` and `reduce_bucket_size` trade-off available GPU memory for communication speed. The smaller their values, the slower communication is and the more GPU memory is available. You can balance, for example, whether a bigger batch size is more important than a slightly slower training time.\n * `round_robin_gradients` is available in DeepSpeed 0.4.4 for CPU offloading. It parallelizes gradient copying to CPU memory among ranks by fine-grained gradient partitioning. Performance benefit grows with gradient accumulation steps (more copying between optimizer steps) or GPU count (increased parallelism).\n \n ```yml\n@@ -220,19 +205,19 @@ ZeRO-2 shards the optimizer and gradients across GPUs. This stage is primarily u\n </hfoption>\n <hfoption id=\"ZeRO-3\">\n \n-ZeRO-3 shards the optimizer, gradient, and parameters across GPUs. Unlike ZeRO-2, ZeRO-3 can also be used for inference, in addition to training, because it allows large models to be loaded on multiple GPUs. Some important parameters to configure include:\n+ZeRO-3 shards the optimizer and gradient states, and parameters across GPUs. Unlike ZeRO-2, ZeRO-3 can also be used for inference in addition to training because it loads large models onto multiple GPUs. Some important parameters to configure include the following.\n \n-* `device: \"cpu\"` can help if you're running out of GPU memory and if you have free CPU memory available. This allows offloading model parameters to the CPU.\n+* `device: \"cpu\"` can help if you're running out of GPU memory and if you have free CPU memory available. This offloads model parameters to the CPU.\n * `pin_memory: true` can improve throughput, but less memory becomes available for other processes because the pinned memory is reserved for the specific process that requested it and it's typically accessed much faster than normal CPU memory.\n-* `stage3_max_live_parameters` is the upper limit on how many full parameters you want to keep on the GPU at any given time. Reduce this value if you encounter an OOM error.\n-* `stage3_max_reuse_distance` is a value for determining when a parameter is used again in the future, and it helps decide whether to throw the parameter away or to keep it. If the parameter is going to be reused (if the value is less than `stage3_max_reuse_distance`), then it is kept to reduce communication overhead. This is super helpful when activation checkpointing is enabled and you want to keep the parameter in the forward recompute until the backward pass. But reduce this value if you encounter an OOM error.\n+* `stage3_max_live_parameters` is the upper limit on how many full parameters to keep on the GPU at any given time. Reduce this value if you encounter an OOM error.\n+* `stage3_max_reuse_distance` is a value for determining when a parameter is used again in the future, and it helps decide whether to throw the parameter away or to keep it. If the parameter is going to be reused (if the value is less than `stage3_max_reuse_distance`), then it is kept to reduce communication overhead. This is helpful when activation checkpointing is enabled and you want to keep the parameter in the forward recompute until the backward pass. Reduce this value if you encounter an OOM error.\n * `stage3_gather_16bit_weights_on_model_save` consolidates fp16 weights when a model is saved. For large models and multiple GPUs, this is expensive in terms of memory and speed. You should enable it if you're planning on resuming training.\n-* `sub_group_size` controls which parameters are updated during the optimizer step. Parameters are grouped into buckets of `sub_group_size` and each bucket is updated one at a time. When used with NVMe offload, `sub_group_size` determines when model states are moved in and out of CPU memory from during the optimization step. This prevents running out of CPU memory for extremely large models. `sub_group_size` can be left to its default value if you aren't using NVMe offload, but you may want to change it if you:\n+* `sub_group_size` controls which parameters are updated during the optimizer step. Parameters are grouped into buckets of `sub_group_size` and each bucket is updated one at a time. When used with NVMe offload, `sub_group_size` determines when model states are moved in and out of CPU memory during the optimization step. This prevents running out of CPU memory for extremely large models. `sub_group_size` can be left to its default value if you aren't using NVMe offload, but you may want to change it if you:\n \n-    1. Run into an OOM error during the optimizer step. In this case, reduce `sub_group_size` to reduce memory usage of the temporary buffers.\n-    2. The optimizer step is taking a really long time. In this case, increase `sub_group_size` to improve bandwidth utilization as a result of increased data buffers.\n+    1. Run into an OOM error during the optimization step. In this case, reduce `sub_group_size` to reduce memory usage of the temporary buffers.\n+    2. The optimization step is taking a really long time. In this case, increase `sub_group_size` to improve bandwidth utilization as a result of increased data buffers.\n \n-* `reduce_bucket_size`, `stage3_prefetch_bucket_size`, and `stage3_param_persistence_threshold` are dependent on a model's hidden size. It is recommended to set these values to `auto` and allow the [`Trainer`] to automatically assign the values.\n+* `reduce_bucket_size`, `stage3_prefetch_bucket_size`, and `stage3_param_persistence_threshold` are dependent on a models hidden size. It is recommended to set these values to `auto` and allow [`Trainer`] to automatically assign the values.\n \n ```yml\n {\n@@ -259,7 +244,9 @@ ZeRO-3 shards the optimizer, gradient, and parameters across GPUs. Unlike ZeRO-2\n }\n ```\n \n-You can use the [`deepspeed.zero.Init`](https://deepspeed.readthedocs.io/en/latest/zero3.html#deepspeed.zero.Init) context manager to initialize a model faster:\n+### Initialize large models\n+\n+With ZeRO-3, use the [deepspeed.zero.Init](https://deepspeed.readthedocs.io/en/latest/zero3.html#deepspeed.zero.Init) context manager to initialize a model faster.\n \n ```py\n from transformers import T5ForConditionalGeneration, T5Config\n@@ -270,7 +257,10 @@ with deepspeed.zero.Init():\n     model = T5ForConditionalGeneration(config)\n ```\n \n-For pretrained models, the DeepSped config file needs to have `is_deepspeed_zero3_enabled: true` setup in [`TrainingArguments`] and it needs a ZeRO configuration enabled. The [`TrainingArguments`] object must be created **before** calling the model [`~PreTrainedModel.from_pretrained`].\n+The DeepSped config file needs to have `is_deepspeed_zero3_enabled: true` setup in [`TrainingArguments`] and it needs a ZeRO configuration enabled. The [`TrainingArguments`] object must be created **before** calling [`~PreTrainedModel.from_pretrained`].\n+\n+> [!TIP]\n+> You'll need ZeRO-3 when the fp16 weights don't fit on a single GPU. But if you're able to load the fp16 weights, set `torch_dtype=torch.float16` in [`~PreTrainedModel.from_pretrained`].\n \n ```py\n from transformers import AutoModel, Trainer, TrainingArguments\n@@ -280,34 +270,31 @@ model = AutoModel.from_pretrained(\"google-t5/t5-small\")\n trainer = Trainer(model=model, args=training_args, ...)\n ```\n \n-You'll need ZeRO-3 if the fp16 weights don't fit on a single GPU. If you're able to load fp16 weights, then make sure you specify `torch_dtype=torch.float16` in [`~PreTrainedModel.from_pretrained`].\n+When there are multiple GPUs, no single GPU has all the parameters unless it's the parameters of the currently executing layer. To access all parameters from all the layers at once, such as loading pretrained model weights in [`~PreTrainedModel.from_pretrained`], one layer is loaded at a time and immediately partitioned to all GPUs. For very large models, it isn't possible to load the weights onto one GPU and then distribute them across the other GPUs due to memory limitations.\n \n-Another consideration for ZeRO-3 is if you have multiple GPUs, no single GPU has all the parameters unless it's the parameters for the currently executing layer. To access all parameters from all the layers at once, such as loading pretrained model weights in [`~PreTrainedModel.from_pretrained`], one layer is loaded at a time and immediately partitioned to all GPUs. This is because for very large models, it isn't possible to load the weights on one GPU and then distribute them across the other GPUs due to memory limitations.\n-\n-If you encounter a model parameter weight that looks like the following, where `tensor([1.])` or the parameter size is 1 instead of a larger multi-dimensional shape, this means the parameter is partitioned and this is a ZeRO-3 placeholder.\n+If you encounter a model parameter weight where `tensor([1.])` or the parameter size is 1 instead of a larger multidimensional shape, it means the parameter is partitioned and this is a ZeRO-3 placeholder.\n \n ```py\n tensor([1.0], device=\"cuda:0\", dtype=torch.float16, requires_grad=True)\n ```\n \n-<Tip>\n-\n-For more information about initializing large models with ZeRO-3 and accessing the parameters, take a look at the [Constructing Massive Models](https://deepspeed.readthedocs.io/en/latest/zero3.html#constructing-massive-models) and [Gathering Parameters](https://deepspeed.readthedocs.io/en/latest/zero3.html#gathering-parameters) guides.\n-\n-</Tip>\n+> [!TIP]\n+> For more information about initializing large models with ZeRO-3 and accessing the parameters, take a look at the [Constructing Massive Models](https://deepspeed.readthedocs.io/en/latest/zero3.html#constructing-massive-models) and [Gathering Parameters](https://deepspeed.readthedocs.io/en/latest/zero3.html#gathering-parameters) guides.\n \n </hfoption>\n </hfoptions>\n \n-### NVMe configuration\n+### NVMe\n \n-[ZeRO-Infinity](https://hf.co/papers/2104.07857) allows offloading model states to the CPU and/or NVMe to save even more memory. Smart partitioning and tiling algorithms allow each GPU to send and receive very small amounts of data during offloading such that a modern NVMe can fit an even larger total memory pool than is available to your training process. ZeRO-Infinity requires ZeRO-3.\n+[ZeRO-Infinity](https://hf.co/papers/2104.07857) offloads model states to the CPU and/or NVMe to save even more memory. Smart partitioning and tiling algorithms allow each GPU to send and receive very small amounts of data during offloading such that a modern NVMe can fit an even larger total memory pool than is available to your training process. ZeRO-Infinity requires ZeRO-3.\n \n-Depending on the CPU and/or NVMe memory available, you can offload both the [optimizer states](https://www.deepspeed.ai/docs/config-json/#optimizer-offloading) and [parameters](https://www.deepspeed.ai/docs/config-json/#parameter-offloading), just one of them, or none. You should also make sure the `nvme_path` is pointing to an NVMe device, because while it still works with a normal hard drive or solid state drive, it'll be significantly slower. With a modern NVMe, you can expect peak transfer speeds of ~3.5GB/s for read and ~3GB/s for write operations. Lastly, [run a benchmark](https://github.com/deepspeedai/DeepSpeed/issues/998) on your training setup to determine the optimal `aio` configuration.\n+Depending on the CPU and NVMe memory available, you can offload both the [optimizer states](https://www.deepspeed.ai/docs/config-json/#optimizer-offloading) and [parameters](https://www.deepspeed.ai/docs/config-json/#parameter-offloading), just one of them, or none of them. Make sure the `nvme_path` points to a NVMe device, because while it still works with a regular hard drive or solid state drive, it'll be significantly slower. With a modern NVMe, you can expect peak transfer speeds of ~3.5GB/s for read operations and ~3GB/s for write operations.\n \n-The example ZeRO-3/Infinity configuration file below sets most of the parameter values to `auto`, but you could also manually add these values.\n+Consider running a [benchmark](https://github.com/microsoft/DeepSpeed/issues/998) on your training setup to determine the optimal `aio` configuration.\n \n-```yml\n+The example ZeRO-3 and ZeRO-Infinity config below sets most of the parameter values to `auto`, but you can also manually set configure these values.\n+\n+```yaml\n {\n     \"fp16\": {\n         \"enabled\": \"auto\",\n@@ -381,103 +368,76 @@ The example ZeRO-3/Infinity configuration file below sets most of the parameter\n }\n ```\n \n-## DeepSpeed features\n+## Training features\n \n-There are a number of important parameters to specify in the DeepSpeed configuration file which are briefly described in this section.\n+DeepSpeed supports many training features that can be configured in the config file. This section describes some of the most important features.\n \n-### Activation/gradient checkpointing\n+### Gradient checkpointing\n \n-Activation and gradient checkpointing trades speed for more GPU memory which allows you to overcome scenarios where your GPU is out of memory or to increase your batch size for better performance. To enable this feature:\n+Gradient checkpointing saves memory by only storing *some* of the intermediate activations instead of storing *all* of them. It is useful for fitting larger models on the GPU without running out of memory or to increase the batch size for better performance. Training speed is slower though.\n \n-1. For a Hugging Face model, set `model.gradient_checkpointing_enable()` or `--gradient_checkpointing` in the [`Trainer`].\n-2. For a non-Hugging Face model, use the DeepSpeed [Activation Checkpointing API](https://deepspeed.readthedocs.io/en/latest/activation-checkpointing.html). You could also replace the Transformers modeling code and replace `torch.utils.checkpoint` with the DeepSpeed API. This approach is more flexible because you can offload the forward activations to the CPU memory instead of recalculating them.\n+* For a Transformers model, set `model.gradient_checkpointing_enable()` or add `--gradient_checkpointing` in the [`TrainingArguments`].\n+* For a non-Transformers model, use the DeepSpeed [Activation Checkpointing API](https://deepspeed.readthedocs.io/en/latest/activation-checkpointing.html). Replacing Transformers modeling code and [torch.utils.checkpoint](https://pytorch.org/docs/stable/checkpoint.html) with the DeepSpeed API gives you more flexibility because you can offload the forward activations to the CPU memory instead of recalculating them.\n \n-### Optimizer and scheduler\n+### Batch size\n \n-DeepSpeed and Transformers optimizer and scheduler can be mixed and matched as long as you don't enable `offload_optimizer`. When `offload_optimizer` is enabled, you could use a non-DeepSpeed optimizer (except for LAMB) as long as it has both a CPU and GPU implementation.\n+The batch size can be automatically configured or manually set. When you choose the `\"auto\"` option, [`Trainer`] sets `train_micro_batch_size_per_gpu` and `train_batch_size` to the value of `world_size * per_device_train_batch_size * gradient_accumulation_steps`.\n \n-<Tip warning={true}>\n+```yaml\n+{\n+    \"train_micro_batch_size_per_gpu\": \"auto\",\n+    \"train_batch_size\": \"auto\"\n+}\n+```\n \n-The optimizer and scheduler parameters for the config file can be set from the command line to avoid hard to find errors. For example, if the learning rate is set to a different value in another place you can override it from the command line. Aside from the optimizer and scheduler parameters, you'll need to ensure your [`Trainer`] command line arguments match the DeepSpeed configuration.\n+### Communication data type\n \n-</Tip>\n+A separate data type is used for communication collectives like reduction, gathering and scattering operations.\n \n-<hfoptions id=\"opt-sched\">\n-<hfoption id=\"optimizer\">\n+All gather and scatter operations are performed in the same data type the data is in. For example, if you're training in bf16, the data is also gathered in bf16 because gathering is a non-lossy operation.\n \n-DeepSpeed offers several [optimizers](https://www.deepspeed.ai/docs/config-json/#optimizer-parameters) (Adam, AdamW, OneBitAdam, and LAMB) but you can also import other optimizers from PyTorch. If you don't configure the optimizer in the config, the [`Trainer`] automatically selects AdamW and either uses the supplied values or the default values for the following parameters from the command line: `lr`, `adam_beta1`, `adam_beta2`, `adam_epsilon`, `weight_decay`.\n+Reduce operations are lossy, for example, when gradients are averaged across multiple GPUs. When the communication is done in fp16 or bf16, it's more likely to be lossy because adding multiple numbers in low precision isn't exact. This is especially the case with bf16 which has a lower precision than fp16. For this reason, fp16 is the default for reduction operations because the loss is minimal when averaging gradients.\n \n-You can set the parameters to `\"auto\"` or manually input your own desired values.\n+Choose the communication data type by setting the `communication_data_type` parameter in the config file. For example, choosing fp32 adds a small amount of overhead but ensures the reduction operation is accumulated in fp32 and when it is ready, it's downcasted to whichever half-precision data type you're training in.\n \n ```yaml\n {\n-   \"optimizer\": {\n-       \"type\": \"AdamW\",\n-       \"params\": {\n-         \"lr\": \"auto\",\n-         \"betas\": \"auto\",\n-         \"eps\": \"auto\",\n-         \"weight_decay\": \"auto\"\n-       }\n-   }\n+    \"communication_data_type\": \"fp32\"\n }\n ```\n \n-You can also use an unsupported optimizer by adding the following to the top level configuration.\n+### Gradient accumulation\n \n-```yaml\n-{\n-   \"zero_allow_untested_optimizer\": true\n-}\n-```\n+Gradient accumulation accumulates gradients over several mini-batches of data before updating parameters. It stores less gradients and enables training with a larger *effective batch size*. Training speed is slower though, but it's useful for overcoming memory constraints.\n \n-From DeepSpeed==0.8.3 on, if you want to use offload, you'll also need to the following to the top level configuration because offload works best with DeepSpeed's CPU Adam optimizer.\n+Gradient accumulation can be automatically configured or manually set. When you choose the `\"auto\"` option, [`Trainer`] sets it to the value of `gradient_accumulation_steps`.\n \n ```yaml\n {\n-   \"zero_force_ds_cpu_optimizer\": false\n+    \"gradient_accumulation_steps\": \"auto\"\n }\n ```\n \n-</hfoption>\n-<hfoption id=\"scheduler\">\n-\n-DeepSpeed supports the LRRangeTest, OneCycle, WarmupLR and WarmupDecayLR learning rate [schedulers](https://www.deepspeed.ai/docs/config-json/#scheduler-parameters).\n-\n-Transformers and DeepSpeed provide two of the same schedulers:\n-\n-* WarmupLR is the same as `--lr_scheduler_type constant_with_warmup` in Transformers\n-* WarmupDecayLR is the same as  `--lr_scheduler_type linear` in Transformers (this is the default scheduler used in Transformers)\n+### Gradient clipping\n \n-If you don't configure the scheduler in the config, the [`Trainer`] automatically selects WarmupDecayLR and either uses the supplied values or the default values for the following parameters from the command line: `warmup_min_lr`, `warmup_max_lr`, `warmup_num_steps`, `total_num_steps` (automatically calculated during run time if `max_steps` is not provided).\n+Gradient clipping is useful for preventing exploding gradients which can lead to instability during training. It sets a maximum threshold value and rescales the gradients if their norm exceeds the threshold.\n \n-You can set the parameters to `\"auto\"` or manually input your own desired values.\n+Gradient clipping can be automatically configured or manually set. When you choose the `\"auto\"` option, [`Trainer`] sets it to the value of `max_grad_norm`.\n \n ```yaml\n {\n-   \"scheduler\": {\n-         \"type\": \"WarmupDecayLR\",\n-         \"params\": {\n-             \"total_num_steps\": \"auto\",\n-             \"warmup_min_lr\": \"auto\",\n-             \"warmup_max_lr\": \"auto\",\n-             \"warmup_num_steps\": \"auto\"\n-         }\n-     }\n+    \"gradient_clipping\": \"auto\"\n }\n ```\n \n-</hfoption>\n-</hfoptions>\n-\n-### Precision\n+### Mixed precision training\n \n-Deepspeed supports fp32, fp16, and bf16 mixed precision.\n+Mixed precision accelerates training speed by performing some calculations in half-precision, but it also maintains some calculations in full-precision to preserve accuracy. DeepSpeed supports fp32, fp16, and bf16 data types.\n \n <hfoptions id=\"precision\">\n <hfoption id=\"fp32\">\n \n-If your model doesn't work well with mixed precision, for example if it wasn't pretrained in mixed precision, you may encounter overflow or underflow issues which can cause NaN loss. For these cases, you should use full fp32 precision by explicitly disabling the default fp16 mode.\n+Train in fp32 if a model wasn't pretrained in mixed precision because it may cause underflow or overflow errors. Disable fp16, the default, in this case.\n \n ```yaml\n {\n@@ -487,12 +447,12 @@ If your model doesn't work well with mixed precision, for example if it wasn't p\n }\n ```\n \n-For Ampere GPUs and PyTorch > 1.7, it automatically switches to the more efficient [tf32](https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices) format for some operations but the results are still in fp32. You can control it from the [`Trainer`] by setting `--tf32` to enable it, and `--tf32 0` or `--no_tf32` to disable it.\n+For Ampere GPUs and PyTorch 1.7+, the more efficient [tf32](https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices) mode is automatically enabled for some operations but the results are still in fp32. Configure it in [`Trainer`] by setting `--tf32` to enable it, and `--tf32 0` or `--no_tf32` to disable it.\n \n </hfoption>\n <hfoption id=\"fp16\">\n \n-To configure PyTorch AMP-like fp16 mixed precision reduces memory usage and accelerates training speed. [`Trainer`] automatically enables or disables fp16 based on the value of `args.fp16_backend`, and the rest of the config can be set by you. fp16 is enabled from the command line when the following arguments are passed: `--fp16`, `--fp16_backend amp` or `--fp16_full_eval`.\n+To configure AMP-like fp16 mixed precision, set up the config as shown below with `\"auto\"` or your own values. [`Trainer`] automatically enables or disables fp16 based on the value of `fp16_backend`, and the rest of the config can be set by you. fp16 is enabled from the command line when the following arguments are passed: `--fp16`, `--fp16_backend amp` or `--fp16_full_eval`.\n \n ```yaml\n {\n@@ -509,7 +469,7 @@ To configure PyTorch AMP-like fp16 mixed precision reduces memory usage and acce\n \n For additional DeepSpeed fp16 training options, take a look at the [FP16 Training Options](https://www.deepspeed.ai/docs/config-json/#fp16-training-options) reference.\n \n-To configure Apex-like fp16 mixed precision, setup the config as shown below with `\"auto\"` or your own values. [`Trainer`] automatically configure `amp` based on the values of `args.fp16_backend` and `args.fp16_opt_level`. It can also be enabled from the command line when the following arguments are passed: `--fp16`, `--fp16_backend apex` or `--fp16_opt_level 01`.\n+To configure Apex-like fp16 mixed precision, set up the config as shown below with `\"auto\"` or your own values. [`Trainer`] automatically configures `amp` based on the values of `fp16_backend` and `fp16_opt_level`. It can also be enabled from the command line when the following arguments are passed: `--fp16`, `--fp16_backend apex` or `--fp16_opt_level 01`.\n \n ```yaml\n {\n@@ -523,9 +483,12 @@ To configure Apex-like fp16 mixed precision, setup the config as shown below wit\n </hfoption>\n <hfoption id=\"bf16\">\n \n-To use bf16, you'll need at least DeepSpeed==0.6.0. bf16 has the same dynamic range as fp32 and doesnâ€™t require loss scaling. However, if you use [gradient accumulation](#gradient-accumulation) with bf16, gradients are accumulated in bf16 which may not be desired because this format's low precision can lead to lossy accumulation.\n+> [!TIP]\n+> bf16 requires DeepSpeed 0.6.0.\n \n-bf16 can be setup in the config file or enabled from the command line when the following arguments are passed: `--bf16` or `--bf16_full_eval`.\n+bf16 has the same dynamic range as fp32, and doesnâ€™t require loss scaling unlike fp16. However, if you use [gradient accumulation](#gradient-accumulation) with bf16, gradients are accumulated in bf16 which may not be desirable because the lower precision can lead to lossy accumulation.\n+\n+bf16 can be set up in the config file or enabled from the command line when the following arguments are passed: `--bf16` or `--bf16_full_eval`.\n \n ```yaml\n {\n@@ -538,59 +501,85 @@ bf16 can be setup in the config file or enabled from the command line when the f\n </hfoption>\n </hfoptions>\n \n-### Batch size\n+### Optimizer and scheduler\n+\n+DeepSpeed and Transformers optimizers and schedulers can be mixed and matched if `offload_optimizer` isn't enabled. When `offload_optimizer` is enabled, use a non-DeepSpeed optimizer (except for LAMB) as long as it has it a CPU and GPU implementation.\n+\n+Set the optimizer and scheduler parameters for the config file from the command line to avoid hard to find errors. For example, if the learning rate is set to a different value in another place, you can override it from the command line.\n+\n+<hfoptions id=\"opt-sched\">\n+<hfoption id=\"optimizer\">\n \n-The batch size can be auto-configured or explicitly set. If you choose to use the `\"auto\"` option, [`Trainer`] sets `train_micro_batch_size_per_gpu` to the value of args.`per_device_train_batch_size` and `train_batch_size` to `args.world_size * args.per_device_train_batch_size * args.gradient_accumulation_steps`.\n+DeepSpeed offers several [optimizers](https://www.deepspeed.ai/docs/config-json/#optimizer-parameters) (Adam, AdamW, OneBitAdam, and LAMB) but you can also import other optimizers from PyTorch. If you don't configure the optimizer in the config, [`Trainer`] automatically selects AdamW and either uses the supplied values or the default values for the following parameters from the command line: `lr`, `adam_beta1`, `adam_beta2`, `adam_epsilon`, `weight_decay`.\n+\n+You can set the parameters to `\"auto\"` or manually input your own values.\n \n ```yaml\n {\n-    \"train_micro_batch_size_per_gpu\": \"auto\",\n-    \"train_batch_size\": \"auto\"\n+   \"optimizer\": {\n+       \"type\": \"AdamW\",\n+       \"params\": {\n+         \"lr\": \"auto\",\n+         \"betas\": \"auto\",\n+         \"eps\": \"auto\",\n+         \"weight_decay\": \"auto\"\n+       }\n+   }\n }\n ```\n \n-### Gradient accumulation\n-\n-Gradient accumulation can be auto-configured or explicitly set. If you choose to use the `\"auto\"` option, [`Trainer`] sets it to the value of `args.gradient_accumulation_steps`.\n+Use an unsupported optimizer by adding the following to the top level configuration.\n \n ```yaml\n {\n-    \"gradient_accumulation_steps\": \"auto\"\n+   \"zero_allow_untested_optimizer\": true\n }\n-\n ```\n \n-### Gradient clipping\n-\n-Gradient clipping can be auto-configured or explicitly set. If you choose to use the `\"auto\"` option, [`Trainer`] sets it to the value of `args.max_grad_norm`.\n+From DeepSpeed 0.8.3+, if you want to use offload, you'll also need to add the following to the top level configuration because offload works best with DeepSpeed's CPU Adam optimizer.\n \n ```yaml\n {\n-    \"gradient_clipping\": \"auto\"\n+   \"zero_force_ds_cpu_optimizer\": false\n }\n ```\n \n-### Communication data type\n+</hfoption>\n+<hfoption id=\"scheduler\">\n+\n+DeepSpeed supports the LRRangeTest, OneCycle, WarmupLR and WarmupDecayLR learning rate [schedulers](https://www.deepspeed.ai/docs/config-json/#scheduler-parameters).\n \n-For communication collectives like reduction, gathering and scattering operations, a separate data type is used.\n+Transformers and DeepSpeed provide two of the same schedulers:\n \n-All gather and scatter operations are performed in the same data type the data is in. For example, if you're training with bf16, the data is also gathered in bf16 because gathering is a non-lossy operation.\n+* WarmupLR is the same as `--lr_scheduler_type constant_with_warmup` in Transformers.\n+* WarmupDecayLR is the same as  `--lr_scheduler_type linear` in Transformers (this is the default scheduler used in Transformers).\n \n-Reduce operations are lossy, for example when gradients are averaged across multiple GPUs. When the communication is done in fp16 or bf16, it is more likely to be lossy because adding multiple numbers in low precision isn't exact. This is especially the case with bf16 which has a lower precision than fp16. For this reason, fp16 is the default for reduction operations because the loss is minimal when averaging gradients.\n+If you don't configure the scheduler in the config file, [`Trainer`] automatically selects WarmupDecayLR and either uses the supplied values or the default values for the following parameters from the command line: `warmup_min_lr`, `warmup_max_lr`, `warmup_num_steps`, `total_num_steps` (automatically calculated during run time if `max_steps` is not provided).\n \n-You can choose the communication data type by setting the `communication_data_type` parameter in the config file. For example, choosing fp32 adds a small amount of overhead but ensures the reduction operation is accumulated in fp32 and when it is ready, it is downcasted to whichever half-precision dtype you're training in.\n+You can set the parameters to `\"auto\"` or manually input your own values.\n \n ```yaml\n {\n-    \"communication_data_type\": \"fp32\"\n+   \"scheduler\": {\n+         \"type\": \"WarmupDecayLR\",\n+         \"params\": {\n+             \"total_num_steps\": \"auto\",\n+             \"warmup_min_lr\": \"auto\",\n+             \"warmup_max_lr\": \"auto\",\n+             \"warmup_num_steps\": \"auto\"\n+         }\n+     }\n }\n ```\n \n-### Universal Checkpointing\n+</hfoption>\n+</hfoptions>\n \n-[Universal Checkpointing](https://www.deepspeed.ai/tutorials/universal-checkpointing) is an efficient and flexible feature for saving and loading model checkpoints. It enables seamless model training continuation and fine-tuning across different model architectures, parallelism techniques, and training configurations.\n+### Universal checkpointing\n \n-Resume training with a universal checkpoint by setting [load_universal](https://www.deepspeed.ai/docs/config-json/#checkpoint-options) to `true` in the config file.\n+[Universal Checkpointing](https://www.deepspeed.ai/tutorials/universal-checkpointing) saves and loads model, optimizer and training scheduler states across different model architectures, parallelism techniques, and training configurations. By saving them in a Universal format, it enables easier model training continuation and fine-tuning.\n+\n+Resume training with a Universal checkpoint by setting `load_universal` to `true` in the config file.\n \n ```yaml\n {\n@@ -600,17 +589,16 @@ Resume training with a universal checkpoint by setting [load_universal](https://\n }\n ```\n \n-## Deployment\n-\n-DeepSpeed can be deployed by different launchers such as [torchrun](https://pytorch.org/docs/stable/elastic/run.html), the `deepspeed` launcher, or [Accelerate](https://huggingface.co/docs/accelerate/basic_tutorials/launch#using-accelerate-launch). To deploy, add `--deepspeed ds_config.json` to the [`Trainer`] command line. Itâ€™s recommended to use DeepSpeedâ€™s [`add_config_arguments`](https://deepspeed.readthedocs.io/en/latest/initialize.html#argument-parsing) utility to add any necessary command line arguments to your code.\n+## Deploy\n \n-This guide will show you how to deploy DeepSpeed with the `deepspeed` launcher for different training setups. You can check out this [post](https://github.com/huggingface/transformers/issues/8771#issuecomment-759248400) for more practical usage examples.\n+DeepSpeed can be deployed with its native launcher, [torchrun](https://pytorch.org/docs/stable/elastic/run.html) or [Accelerate](https://huggingface.co/docs/accelerate/basic_tutorials/launch#using-accelerate-launch).\n \n+Add the `--deepspeed ds_config.json` argument to [`Trainer`] in the command line. It is recommended to use DeepSpeeds [add_config_arguments](https://deepspeed.readthedocs.io/en/latest/initialize.html#argument-parsing) utility to add any other command line arguments to your code.\n \n <hfoptions id=\"deploy\">\n <hfoption id=\"multi-GPU\">\n \n-To deploy DeepSpeed on multiple GPUs, add the `--num_gpus` parameter. If you want to use all available GPUs, you don't need to add `--num_gpus`. The example below uses 2 GPUs.\n+To deploy DeepSpeed on multiple GPUs, add `--num_gpus`. You don't need to add `--num_gpus` if you're planning on using all available GPUs.\n \n ```bash\n deepspeed --num_gpus=2 examples/pytorch/translation/run_translation.py \\\n@@ -625,7 +613,15 @@ deepspeed --num_gpus=2 examples/pytorch/translation/run_translation.py \\\n </hfoption>\n <hfoption id=\"single-GPU\">\n \n-To deploy DeepSpeed on a single GPU, add the `--num_gpus` parameter. It isn't necessary to explicitly set this value if you only have 1 GPU because DeepSpeed deploys all GPUs it can see on a given node.\n+DeepSpeed is still useful with just one GPU because you can:\n+\n+1. Offload some computations and memory to the CPU to make more GPU resources available to your model to use a larger batch size or fit a very large model that normally won't fit.\n+2. Minimize memory fragmentation with its smart GPU memory management system which also allows you to fit bigger models and data batches.\n+\n+To deploy DeepSpeed on a single GPU, add `--num_gpus`. You don't need to add `--num_gpus` if you only have one GPU because DeepSpeed deploys all GPUs it can see on a given node.\n+\n+> [!TIP]\n+> Set the `allgather_bucket_size` and `reduce_bucket_size` values to 2e8 in the [ZeRO-2](#zero-configuration) configuration file to get better performance on a single GPU.\n \n ```bash\n deepspeed --num_gpus=1 examples/pytorch/translation/run_translation.py \\\n@@ -637,25 +633,12 @@ deepspeed --num_gpus=1 examples/pytorch/translation/run_translation.py \\\n --source_lang en --target_lang ro\n ```\n \n-DeepSpeed is still useful with just 1 GPU because you can:\n-\n-1. Offload some computations and memory to the CPU to make more GPU resources available to your model to use a larger batch size or fit a very large model that normally won't fit.\n-2. Minimize memory fragmentation with it's smart GPU memory management system which also allows you to fit bigger models and data batches.\n-\n-<Tip>\n-\n-Set the `allgather_bucket_size` and `reduce_bucket_size` values to 2e8 in the [ZeRO-2](#zero-configuration) configuration file to get better performance on a single GPU.\n-\n-</Tip>\n-\n </hfoption>\n </hfoptions>\n \n-### Multi-node deployment\n-\n-A node is one or more GPUs for running a workload. A more powerful setup is a multi-node setup which can be launched with the `deepspeed` launcher. For this guide, let's assume there are two nodes with 8 GPUs each. The first node can be accessed `ssh hostname1` and the second node with `ssh hostname2`. Both nodes must be able to communicate with each other locally over ssh without a password.\n+### Multi-node\n \n-By default, DeepSpeed expects your multi-node environment to use a shared storage. If this is not the case and each node can only see the local filesystem, you need to adjust the config file to include a [`checkpoint`](https://www.deepspeed.ai/docs/config-json/#checkpoint-options) to allow loading without access to a shared filesystem:\n+A multi-node setup consists of multiple nodes, where each node has one of more GPUs running a workload. DeepSpeed expects a shared storage system, but if this is not the case, you need to adjust the config file to include a [checkpoint](https://www.deepspeed.ai/docs/config-json/#checkpoint-options) to allow loading without access to a shared filesystem.\n \n ```yaml\n {\n@@ -665,29 +648,31 @@ By default, DeepSpeed expects your multi-node environment to use a shared storag\n }\n ```\n \n-You could also use the [`Trainer`]'s `--save_on_each_node` argument to automatically add the above `checkpoint` to your config.\n+You could also use the `--save_on_each_node` parameter in [`TrainingArguments`] to automatically add the above `checkpoint` to your config.\n+\n+The examples below for the torchrun and DeepSpeed launcher shows how to deploy two nodes with eight GPUs each. Access the first node with `ssh hostname1` and the second node with `ssh hostname2`. Both nodes must be able to communicate with each other locally over ssh without a password.\n \n <hfoptions id=\"multinode\">\n <hfoption id=\"torchrun\">\n \n-For [torchrun](https://pytorch.org/docs/stable/elastic/run.html), you have to ssh to each node and run the following command on both of them. The launcher waits until both nodes are synchronized before launching the training.\n+With [torchrun](https://pytorch.org/docs/stable/elastic/run.html), ssh to each node and run the following command on both of them. The launcher waits until both nodes are synchronized before launching the training.\n \n ```bash\n torchrun --nproc_per_node=8 --nnode=2 --node_rank=0 --master_addr=hostname1 \\\n --master_port=9901 your_program.py <normal cl args> --deepspeed ds_config.json\n ```\n \n </hfoption>\n-<hfoption id=\"deepspeed\">\n+<hfoption id=\"DeepSpeed\">\n \n-For the `deepspeed` launcher, start by creating a `hostfile`.\n+Create a `hostfile` for the DeepSpeed launcher.\n \n ```bash\n hostname1 slots=8\n hostname2 slots=8\n ```\n \n-Then you can launch the training with the following command. The `deepspeed` launcher automatically launches the command on both nodes at once.\n+The DeepSpeed launcher automatically launches the command on both nodes at once with the command below.\n \n ```bash\n deepspeed --num_gpus 8 --num_nodes 2 --hostfile hostfile --master_addr hostname1 --master_port=9901 \\\n@@ -699,9 +684,9 @@ Check out the [Resource Configuration (multi-node)](https://www.deepspeed.ai/get\n </hfoption>\n </hfoptions>\n \n-### SLURM\n+### Slurm\n \n-In a SLURM environment, you'll need to adapt your SLURM script to your specific SLURM environment. An example SLURM script may look like:\n+[Slurm](https://slurm.schedmd.com/documentation.html) is a cluster management and job scheduling system. An example Slurm script is shown below.\n \n ```bash\n #SBATCH --job-name=test-nodes        # name\n@@ -722,19 +707,18 @@ srun --jobid $SLURM_JOBID bash -c 'python -m torch.distributed.run \\\n your_program.py <normal cl args> --deepspeed ds_config.json'\n ```\n \n-Then you can schedule your multi-node deployment with the following command which launches training simultaneously on all nodes.\n+Launch training simultaneously on all nodes with the command below.\n \n ```bash\n sbatch launch.slurm\n ```\n \n-### Notebook\n+### Jupyter Notebook\n \n-The `deepspeed` launcher doesn't support deployment from a notebook so you'll need to emulate the distributed environment. However, this only works for 1 GPU. If you want to use more than 1 GPU, you must use a multi-process environment for DeepSpeed to work. This means you have to use the `deepspeed` launcher which can't be emulated as shown here.\n+To use DeepSpeed in a Jupyter Notebook, you need to emulate a distributed environment because the launcher doesn't support deployment from a notebook. This is only supported for one GPU. To use multiple GPUs, you must use a multi-process environment, which means you have to use the DeepSpeed launcher which can't be emulated as shown here.\n \n ```py\n-# DeepSpeed requires a distributed environment even when only one process is used.\n-# This emulates a launcher in the notebook\n+# emulate a launcher in the notebook\n import os\n \n os.environ[\"MASTER_ADDR\"] = \"localhost\"\n@@ -743,13 +727,12 @@ os.environ[\"RANK\"] = \"0\"\n os.environ[\"LOCAL_RANK\"] = \"0\"\n os.environ[\"WORLD_SIZE\"] = \"1\"\n \n-# Now proceed as normal, plus pass the DeepSpeed config file\n training_args = TrainingArguments(..., deepspeed=\"ds_config_zero3.json\")\n trainer = Trainer(...)\n trainer.train()\n ```\n \n-If you want to create the config file on the fly in the notebook in the current directory, you could have a dedicated cell.\n+Create a config file on the fly in the notebook in the current directory with a dedicated cell.\n \n ```py\n %%bash\n@@ -814,14 +797,14 @@ cat <<'EOT' > ds_config_zero3.json\n EOT\n ```\n \n-If the training script is in a file and not in a notebook cell, you can launch `deepspeed` normally from the shell in a notebook cell. For example, to launch `run_translation.py`:\n+If the training script is in a file and not a notebook cell, launch DeepSpeed from the shell in the notebook cell.\n \n ```py\n !git clone https://github.com/huggingface/transformers\n !cd transformers; deepspeed examples/pytorch/translation/run_translation.py ...\n ```\n \n-You could also use `%%bash` magic and write multi-line code to run the shell program, but you won't be able to view the logs until training is complete. With `%%bash` magic, you don't need to emulate a distributed environment.\n+Another option is to use `%%bash` to run the shell program without emulating the distributed environment. However, you won't be able to view the logs until training is complete.\n \n ```py\n %%bash\n@@ -833,69 +816,33 @@ deepspeed examples/pytorch/translation/run_translation.py ...\n \n ## Save model weights\n \n-DeepSpeed stores the main full precision fp32 weights in custom checkpoint optimizer files (the glob pattern looks like `global_step*/*optim_states.pt`) and are saved under the normal checkpoint.\n+DeepSpeed stores the main fp32 weights in custom checkpoint optimizer files (`global_step*/*optim_states.pt`) which are saved under the normal checkpoint.\n \n-<hfoptions id=\"save\">\n-<hfoption id=\"fp16\">\n+### fp16\n+\n+ZeRO-2 saves the model weights in fp16. To save the weights in fp16 for ZeRO-3, set `\"stage3_gather_16bit_weights_on_model_save\": true` in the config file, because the weights are distributed across multiple GPUs.\n \n-A model trained with ZeRO-2 saves the pytorch_model.bin weights in fp16. To save the model weights in fp16 for a model trained with ZeRO-3, you need to set `\"stage3_gather_16bit_weights_on_model_save\": true` because the model weights are partitioned across multiple GPUs. Otherwise, the [`Trainer`] won't save the weights in fp16 and it won't create a pytorch_model.bin file. This is because DeepSpeed's state_dict contains a placeholder instead of the real weights and you won't be able to load them.\n+If you don't, [`Trainer`] won't save the weights in fp16 and won't create a `pytorch_model.bin` file. This is because DeepSpeed's state_dict contains a placeholder instead of the real weights, so you won't be able to load it.\n \n ```yaml\n {\n     \"zero_optimization\": {\n+        \"stage\": 3,\n         \"stage3_gather_16bit_weights_on_model_save\": true\n     }\n }\n ```\n \n-</hfoption>\n-<hfoption id=\"fp32\">\n-\n-The full precision weights shouldn't be saved during training because it can require a lot of memory. It is usually best to save the fp32 weights offline after training is complete. But if you have a lot of free CPU memory, it is possible to save the fp32 weights during training. This section covers both online and offline approaches.\n-\n-### Online\n-\n-You must have saved at least one checkpoint to load the latest checkpoint as shown in the following:\n-\n-```py\n-from transformers.trainer_utils import get_last_checkpoint\n-from deepspeed.utils.zero_to_fp32 import load_state_dict_from_zero_checkpoint\n-\n-checkpoint_dir = get_last_checkpoint(trainer.args.output_dir)\n-fp32_model = load_state_dict_from_zero_checkpoint(trainer.model, checkpoint_dir)\n-```\n-\n-If you've enabled the `--load_best_model_at_end` parameter to track the best checkpoint in [`TrainingArguments`], you can finish training first and save the final model explicitly. Then you can reload it as shown below:\n-\n-```py\n-from deepspeed.utils.zero_to_fp32 import load_state_dict_from_zero_checkpoint\n-\n-checkpoint_dir = os.path.join(trainer.args.output_dir, \"checkpoint-final\")\n-trainer.deepspeed.save_checkpoint(checkpoint_dir)\n-fp32_model = load_state_dict_from_zero_checkpoint(trainer.model, checkpoint_dir)\n-```\n-\n-<Tip>\n-\n-Once `load_state_dict_from_zero_checkpoint` is run, the model is no longer usable in DeepSpeed in the context of the same application. You'll need to initialize the DeepSpeed engine again since `model.load_state_dict(state_dict)` removes all the DeepSpeed magic from it. Only use this at the very end of training.\n-\n-</Tip>\n+### fp32\n \n-You can also extract and load the state_dict of the fp32 weights:\n+Unless you have a lot of free CPU memory, fp32 weights shouldn't be saved during training because it can require a lot of memory. It is usually best to save the fp32 weights offline after training is complete.\n \n-```py\n-from deepspeed.utils.zero_to_fp32 import get_fp32_state_dict_from_zero_checkpoint\n-\n-state_dict = get_fp32_state_dict_from_zero_checkpoint(checkpoint_dir)  # already on cpu\n-model = model.cpu()\n-model.load_state_dict(state_dict)\n-```\n-\n-### Offline\n+<hfoptions id=\"save\">\n+<hfoption id=\"offline\">\n \n-DeepSpeed provides a zero_to_fp32.py script at the top-level of the checkpoint folder for extracting weights at any point. This is a standalone script and you don't need a configuration file or [`Trainer`].\n+DeepSpeed provies a [zero_to_fp32.py](https://github.com/microsoft/DeepSpeed/blob/91829476a8fd4d0d9268c03c1d56795d20a51c12/deepspeed/utils/zero_to_fp32.py#L14) script at the top-level checkpoint folder for extracting weights at any point. This is a standalone script and you don't need a config file or [`Trainer`].\n \n-For example, if your checkpoint folder looked like this:\n+For example, if your checkpoint folder looks like the one shown below, then you can run the following command to create and consolidate the fp32 weights from multiple GPUs into a single `pytorch_model.bin` file. The script automatically discovers the subfolder `global_step1` which contains the checkpoint.\n \n ```bash\n $ ls -l output_dir/checkpoint-1/\n@@ -913,44 +860,57 @@ drwxrwxr-x 2 stas stas 4.0K Mar 25 19:52 global_step1/\n -rwxrw-r-- 1 stas stas 5.5K Mar 27 13:16 zero_to_fp32.py*\n ```\n \n-To reconstruct the fp32 weights from the DeepSpeed checkpoint (ZeRO-2 or ZeRO-3) subfolder `global_step1`, run the following command to create and consolidate the full fp32 weights from multiple GPUs into a single pytorch_model.bin file. The script automatically discovers the subfolder containing the checkpoint.\n+> [!TIP]\n+> Run `python zero_to_fp32.py -h` for more usage details. The script requires 2x the general RAM of the final fp32 weights.\n \n-```py\n+```bash\n python zero_to_fp32.py . pytorch_model.bin\n ```\n \n-<Tip>\n+</hfoption>\n+<hfoption id=\"online\">\n \n-Run `python zero_to_fp32.py -h` for more usage details. The script requires 2x the general RAM of the final fp32 weights.\n+Adding the `--load_best_model_at_end` parameter in [`TrainingArguments`] tracks the best checkpoint so you can finish training first and save the final model explicitly. Reload the model as shown below.\n \n-</Tip>\n+> [!WARNING]\n+> Once [load_state_dict_from_zero_checkpoint](https://deepspeed.readthedocs.io/en/stable/model-checkpointing.html#deepspeed.utils.zero_to_fp32.load_state_dict_from_zero_checkpoint) is run, the model is no longer usable in DeepSpeed in the context of the same application. You'll need to reinitialize the DeepSpeed engine because `model.load_state_dict(state_dict)` removes all the DeepSpeed magic from it. Only use this function once training is complete.\n \n-</hfoption>\n-</hfoptions>\n-\n-## ZeRO Inference\n+```py\n+from deepspeed.utils.zero_to_fp32 import load_state_dict_from_zero_checkpoint\n \n-[ZeRO Inference](https://www.deepspeed.ai/2022/09/09/zero-inference.html) places the model weights in CPU or NVMe memory to avoid burdening the GPU which makes it possible to run inference with huge models on a GPU. Inference doesn't require any large additional amounts of memory for the optimizer states and gradients so you can fit much larger batches and/or sequence lengths on the same hardware.\n+checkpoint_dir = os.path.join(trainer.args.output_dir, \"checkpoint-final\")\n+trainer.deepspeed.save_checkpoint(checkpoint_dir)\n+fp32_model = load_state_dict_from_zero_checkpoint(trainer.model, checkpoint_dir)\n+```\n \n-ZeRO Inference shares the same configuration file as [ZeRO-3](#zero-configuration), and ZeRO-2 and ZeRO-1 configs won't work because they don't provide any benefits for inference.\n+You must have saved at least one checkpoint to load the latest checkpoint as shown in the example below.\n \n-To run ZeRO Inference, pass your usual training arguments to the [`TrainingArguments`] class and add the `--do_eval` argument.\n+```py\n+from transformers.trainer_utils import get_last_checkpoint\n+from deepspeed.utils.zero_to_fp32 import load_state_dict_from_zero_checkpoint\n \n-```bash\n-deepspeed --num_gpus=2 your_program.py <normal cl args> --do_eval --deepspeed ds_config.json\n+checkpoint_dir = get_last_checkpoint(trainer.args.output_dir)\n+fp32_model = load_state_dict_from_zero_checkpoint(trainer.model, checkpoint_dir)\n ```\n \n-## Non-Trainer DeepSpeed integration\n+Use `load_state_dict` to extract and load the state_dict of the fp32 weights.\n \n-DeepSpeed also works with Transformers without the [`Trainer`] class. This is handled by the [`HfDeepSpeedConfig`] which only takes care of gathering ZeRO-3 parameters and splitting a model across multiple GPUs when you call [`~PreTrainedModel.from_pretrained`].\n+```py\n+from deepspeed.utils.zero_to_fp32 import get_fp32_state_dict_from_zero_checkpoint\n+\n+state_dict = get_fp32_state_dict_from_zero_checkpoint(checkpoint_dir)\n+model = model.cpu()\n+model.load_state_dict(state_dict)\n+```\n \n-<Tip>\n+</hfoption>\n+</hfoptions>\n \n-If you want everything automatically taken care of for you, try using DeepSpeed with the [`Trainer`]! You'll need to follow the [DeepSpeed documentation](https://www.deepspeed.ai/), and manually configure the parameter values in the config file (you can't use the `\"auto\"` value).\n+## Non-Trainer integration\n \n-</Tip>\n+DeepSpeed also works with Transformers without [`Trainer`]. The [`~integrations.HfDeepSpeedConfig`] is responsible for gathering ZeRO-3 parameters and partitioning a model across multiple GPUs when [`~PreTrainedModel.from_pretrained`] is called.\n \n-To efficiently deploy ZeRO-3, you must instantiate the [`HfDeepSpeedConfig`] object before the model and keep that object alive:\n+You must instantiate [`~integrations.HfDeepSpeedConfig`] before loading a model to efficiently deploy ZeRO-3.\n \n <hfoptions id=\"models\">\n <hfoption id=\"pretrained model\">\n@@ -960,8 +920,9 @@ from transformers.integrations import HfDeepSpeedConfig\n from transformers import AutoModel\n import deepspeed\n \n-ds_config = {...}  # deepspeed config object or path to the file\n-# must run before instantiating the model to detect zero 3\n+# DeepSpeed config object or path to the file\n+ds_config = {...}\n+# must run before instantiating the model to detect ZeRO-3\n dschf = HfDeepSpeedConfig(ds_config)  # keep this object alive\n model = AutoModel.from_pretrained(\"openai-community/gpt2\")\n engine = deepspeed.initialize(model=model, config_params=ds_config, ...)\n@@ -970,16 +931,18 @@ engine = deepspeed.initialize(model=model, config_params=ds_config, ...)\n </hfoption>\n <hfoption id=\"non-pretrained model\">\n \n-[`HfDeepSpeedConfig`] is not required for ZeRO-1 or ZeRO-2.\n+[`~integrations.HfDeepSpeedConfig`] is not required for ZeRO-1 or ZeRO-2.\n \n ```py\n from transformers.integrations import HfDeepSpeedConfig\n from transformers import AutoModel, AutoConfig\n import deepspeed\n \n-ds_config = {...}  # deepspeed config object or path to the file\n+# DeepSpeed config object or path to the file\n+ds_config = {...}\n # must run before instantiating the model to detect zero 3\n dschf = HfDeepSpeedConfig(ds_config)  # keep this object alive\n+# randomly intialize model weights\n config = AutoConfig.from_pretrained(\"openai-community/gpt2\")\n model = AutoModel.from_config(config)\n engine = deepspeed.initialize(model=model, config_params=ds_config, ...)\n@@ -988,208 +951,40 @@ engine = deepspeed.initialize(model=model, config_params=ds_config, ...)\n </hfoption>\n </hfoptions>\n \n-### Non-Trainer ZeRO Inference\n-\n-To run ZeRO Inference without the [`Trainer`] in cases where you canâ€™t fit a model onto a single GPU, try using additional GPUs or/and offloading to CPU memory. The important nuance to understand here is that the way ZeRO is designed, you can process different inputs on different GPUs in parallel.\n-\n-Make sure to:\n-\n-* disable CPU offload if you have enough GPU memory (since it slows things down).\n-* enable bf16 if you have an Ampere or newer GPU to make things faster. If you donâ€™t have one of these GPUs, you may enable fp16 as long as you donâ€™t use a model pretrained in bf16 (T5 models) because it may lead to an overflow error.\n-\n-Take a look at the following script to get a better idea of how to run ZeRO Inference without the [`Trainer`] on a model that won't fit on a single GPU.\n-\n-```py\n-#!/usr/bin/env python\n-\n-# This script demonstrates how to use Deepspeed ZeRO in an inference mode when one can't fit a model\n-# into a single GPU\n-#\n-# 1. Use 1 GPU with CPU offload\n-# 2. Or use multiple GPUs instead\n-#\n-# First you need to install deepspeed: pip install deepspeed\n-#\n-# Here we use a 3B \"bigscience/T0_3B\" model which needs about 15GB GPU RAM - so 1 largish or 2\n-# small GPUs can handle it. or 1 small GPU and a lot of CPU memory.\n-#\n-# To use a larger model like \"bigscience/T0\" which needs about 50GB, unless you have an 80GB GPU -\n-# you will need 2-4 gpus. And then you can adapt the script to handle more gpus if you want to\n-# process multiple inputs at once.\n-#\n-# The provided deepspeed config also activates CPU memory offloading, so chances are that if you\n-# have a lot of available CPU memory and you don't mind a slowdown you should be able to load a\n-# model that doesn't normally fit into a single GPU. If you have enough GPU memory the program will\n-# run faster if you don't want offload to CPU - so disable that section then.\n-#\n-# To deploy on 1 gpu:\n-#\n-# deepspeed --num_gpus 1 t0.py\n-# or:\n-# python -m torch.distributed.run --nproc_per_node=1 t0.py\n-#\n-# To deploy on 2 gpus:\n-#\n-# deepspeed --num_gpus 2 t0.py\n-# or:\n-# python -m torch.distributed.run --nproc_per_node=2 t0.py\n-\n-from transformers import AutoTokenizer, AutoConfig, AutoModelForSeq2SeqLM\n-from transformers.integrations import HfDeepSpeedConfig\n-import deepspeed\n-import os\n-import torch\n-\n-os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # To avoid warnings about parallelism in tokenizers\n-\n-# distributed setup\n-local_rank = int(os.getenv(\"LOCAL_RANK\", \"0\"))\n-world_size = int(os.getenv(\"WORLD_SIZE\", \"1\"))\n-torch.cuda.set_device(local_rank)\n-deepspeed.init_distributed()\n-\n-model_name = \"bigscience/T0_3B\"\n-\n-config = AutoConfig.from_pretrained(model_name)\n-model_hidden_size = config.d_model\n-\n-# batch size has to be divisible by world_size, but can be bigger than world_size\n-train_batch_size = 1 * world_size\n-\n-# ds_config notes\n-#\n-# - enable bf16 if you use Ampere or higher GPU - this will run in mixed precision and will be\n-# faster.\n-#\n-# - for older GPUs you can enable fp16, but it'll only work for non-bf16 pretrained models - e.g.\n-# all official t5 models are bf16-pretrained\n-#\n-# - set offload_param.device to \"none\" or completely remove the `offload_param` section if you don't\n-# - want CPU offload\n-#\n-# - if using `offload_param` you can manually finetune stage3_param_persistence_threshold to control\n-# - which params should remain on gpus - the larger the value the smaller the offload size\n-#\n-# For in-depth info on Deepspeed config see\n-# https://huggingface.co/docs/transformers/main/main_classes/deepspeed\n-\n-# keeping the same format as json for consistency, except it uses lower case for true/false\n-# fmt: off\n-ds_config = {\n-    \"fp16\": {\n-        \"enabled\": False\n-    },\n-    \"bf16\": {\n-        \"enabled\": False\n-    },\n-    \"zero_optimization\": {\n-        \"stage\": 3,\n-        \"offload_param\": {\n-            \"device\": \"cpu\",\n-            \"pin_memory\": True\n-        },\n-        \"overlap_comm\": True,\n-        \"contiguous_gradients\": True,\n-        \"reduce_bucket_size\": model_hidden_size * model_hidden_size,\n-        \"stage3_prefetch_bucket_size\": 3774873,\n-        \"stage3_param_persistence_threshold\": 10 * model_hidden_size\n-    },\n-    \"steps_per_print\": 2000,\n-    \"train_batch_size\": train_batch_size,\n-    \"train_micro_batch_size_per_gpu\": 1,\n-    \"wall_clock_breakdown\": False\n-}\n-# fmt: on\n-\n-# next line instructs transformers to partition the model directly over multiple gpus using\n-# deepspeed.zero.Init when model's `from_pretrained` method is called.\n-#\n-# **it has to be run before loading the model AutoModelForSeq2SeqLM.from_pretrained(model_name)**\n-#\n-# otherwise the model will first be loaded normally and only partitioned at forward time which is\n-# less efficient and when there is little CPU RAM may fail\n-dschf = HfDeepSpeedConfig(ds_config)  # keep this object alive\n-\n-# now a model can be loaded.\n-model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n-\n-# initialise Deepspeed ZeRO and store only the engine object\n-ds_engine = deepspeed.initialize(model=model, config_params=ds_config)[0]\n-ds_engine.module.eval()  # inference\n-\n-# Deepspeed ZeRO can process unrelated inputs on each GPU. So for 2 gpus you process 2 inputs at once.\n-# If you use more GPUs adjust for more.\n-# And of course if you have just one input to process you then need to pass the same string to both gpus\n-# If you use only one GPU, then you will have only rank 0.\n-rank = torch.distributed.get_rank()\n-if rank == 0:\n-    text_in = \"Is this review positive or negative? Review: this is the best cast iron skillet you will ever buy\"\n-elif rank == 1:\n-    text_in = \"Is this review positive or negative? Review: this is the worst restaurant ever\"\n-\n-tokenizer = AutoTokenizer.from_pretrained(model_name)\n-inputs = tokenizer.encode(text_in, return_tensors=\"pt\").to(device=local_rank)\n-with torch.no_grad():\n-    outputs = ds_engine.module.generate(inputs, synced_gpus=True)\n-text_out = tokenizer.decode(outputs[0], skip_special_tokens=True)\n-print(f\"rank{rank}:\\n   in={text_in}\\n  out={text_out}\")\n-```\n-\n-Save the script as t0.py and launch it:\n-\n-```bash\n-$ deepspeed --num_gpus 2 t0.py\n-rank0:\n-   in=Is this review positive or negative? Review: this is the best cast iron skillet you will ever buy\n-  out=Positive\n-rank1:\n-   in=Is this review positive or negative? Review: this is the worst restaurant ever\n-  out=negative\n-```\n-\n-This is a very basic example and you'll want to adapt it to your use case.\n-\n-### Generate\n-\n-Using multiple GPUs with ZeRO-3 for generation requires synchronizing the GPUs by setting `synced_gpus=True` in the [`~GenerationMixin.generate`] method. Otherwise, if one GPU is finished generating before another one, the whole system hangs because the remaining GPUs haven't received the weight shard from the GPU that finished first.\n-\n-For Transformers>=4.28, if `synced_gpus` is automatically set to `True` if multiple GPUs are detected during generation.\n-\n ## Troubleshoot\n \n-When you encounter an issue, you should consider whether DeepSpeed is the cause of the problem because often it isn't (unless it's super obviously and you can see DeepSpeed modules in the exception)! The first step should be to retry your setup without DeepSpeed, and if the problem persists, then you can report the issue. If the issue is a core DeepSpeed problem and unrelated to the Transformers integration, open an Issue on the [DeepSpeed repository](https://github.com/deepspeedai/DeepSpeed).\n+One of the first things to check when you encounter an error is whether DeepSpeed is the cause (because often it isn't). Retry your setup without DeepSpeed, and if the error persists, report the issue. If the issue is unrelated to the Transformers integration, please open the issue on the DeepSpeed [repository](https://github.com/microsoft/DeepSpeed).\n \n-For issues related to the Transformers integration, please provide the following information:\n+For issues related to the Transformers integration, please provide the following information.\n \n-* the full DeepSpeed config file\n+* The full DeepSpeed config file.\n+* The command line arguments for [`Trainer`] or the [`TrainingArguments`] if you're scripting the [`Trainer`] setup yourself (don't dump the entire [`TrainingArguments`] which contains many irrelevant entries).\n+* The outputs of the following commands.\n \n-* the command line arguments of the [`Trainer`], or [`TrainingArguments`] arguments if you're scripting the [`Trainer`] setup yourself (don't dump the [`TrainingArguments`] which has dozens of irrelevant entries)\n+    ```bash\n+    python -c 'import torch; print(f\"torch: {torch.__version__}\")'\n+    python -c 'import transformers; print(f\"transformers: {transformers.__version__}\")'\n+    python -c 'import deepspeed; print(f\"deepspeed: {deepspeed.__version__}\")'\n+    ```\n \n-* the outputs of:\n-\n-```bash\n-python -c 'import torch; print(f\"torch: {torch.__version__}\")'\n-python -c 'import transformers; print(f\"transformers: {transformers.__version__}\")'\n-python -c 'import deepspeed; print(f\"deepspeed: {deepspeed.__version__}\")'\n-```\n-\n-* a link to a Google Colab notebook to reproduce the issue\n-\n-* if impossible, a standard and non-custom dataset we can use and also try to use an existing example to reproduce the issue with\n+* A link to a Google Colab notebook to reproduce the issue.\n+* A standard or non-custom dataset or an existing example to reproduce the issue.\n \n The following sections provide a guide for resolving two of the most common issues.\n \n-### DeepSpeed process killed at startup\n+### Process killed at startup\n+\n+When the DeepSpeed process is killed during launch without a traceback, that usually means the program tried to allocate more CPU memory than is available on your system. Or the process may have tried to allocate more CPU memory than allowed, leading the OS kernel to terminate the process.\n \n-When the DeepSpeed process is killed during launch without a traceback, that usually means the program tried to allocate more CPU memory than your system has or your process tried to allocate more CPU memory than allowed leading the OS kernel to terminate the process. In this case, check whether your configuration file has either `offload_optimizer`, `offload_param` or both configured to offload to the CPU. \n+In this case, check whether your config file has either `offload_optimizer`, `offlload_param`, or both configured to offload to the CPU.\n \n-If you have NVMe and ZeRO-3 setup, experiment with offloading to the NVMe ([estimate](https://deepspeed.readthedocs.io/en/latest/memory.html) the memory requirements for your model).\n+If you have NVM3 and ZeRO-3 set up, experiment with offloading to the NVMe ([estimate](https://deepspeed.readthedocs.io/en/latest/memory.html) the memory requirements of a model first) instead.\n \n ### NaN loss\n \n-NaN loss often occurs when a model is pretrained in bf16 and then you try to use it with fp16 (especially relevant for TPU trained models). To resolve this, use fp32 or bf16 if your hardware supports it (TPU, Ampere GPUs or newer).\n+NaN loss often occurs when a model is pretrained in bf16 and you try to use it with fp16 (especially relevant to TPU trained models). To resolve this, use fp32 or bf16 if your hardware (TPUs, Ampere GPUs or newer) supports it.\n \n-The other issue may be related to using fp16. For example, if this is your fp16 configuration:\n+It is also possible that fp16 is causing overflow. For example, if your config file looks like the one below, you may see the following overflow errors in the logs.\n \n ```yaml\n {\n@@ -1204,7 +999,7 @@ The other issue may be related to using fp16. For example, if this is your fp16\n }\n ```\n \n-You might see the following `OVERFLOW!` messages in the logs:\n+The `OVERFLOW!` error below is a result of the DeepSpeed loss scaler unable to find a scaling coefficient to overcome the loss overflow. Try a higher `initial_scale_power` value in this case (32 usually works).\n \n ```bash\n 0%|                                                                                                                             | 0/189 [00:00<?, ?it/s]\n@@ -1223,13 +1018,11 @@ You might see the following `OVERFLOW!` messages in the logs:\n [...]\n ```\n \n-This means the DeepSpeed loss scaler is unable to find a scaling coefficient to overcome loss overflow. To fix it, try a higher `initial_scale_power` value (32 usually works).\n-\n ## Resources\n \n-DeepSpeed ZeRO is a powerful technology for training and loading very large models for inference with limited GPU resources, making it more accessible to everyone. To learn more about DeepSpeed, feel free to read the [blog posts](https://www.microsoft.com/en-us/research/search/?q=deepspeed), [documentation](https://www.deepspeed.ai/getting-started/), and [GitHub repository](https://github.com/deepspeedai/DeepSpeed). \n+DeepSpeed is a powerful technology for scaling large model training. To learn more about DeepSpeed, take a look at their [blog posts](https://www.microsoft.com/en-us/research/search/?q=deepspeed), [documentation](https://www.deepspeed.ai/getting-started/), and [GitHub](https://github.com/microsoft/deepspeed).\n \n-The following papers are also a great resource for learning more about ZeRO:\n+The papers below provide additional details about ZeRO.\n \n * [ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://hf.co/papers/1910.02054)\n * [ZeRO-Offload: Democratizing Billion-Scale Model Training](https://hf.co/papers/2101.06840)"
        },
        {
            "sha": "3e9c097c9e6c89ed6a730574bca9b8012f9cdd80",
            "filename": "docs/source/en/executorch.md",
            "status": "added",
            "additions": 59,
            "deletions": 0,
            "changes": 59,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fexecutorch.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fexecutorch.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fexecutorch.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -0,0 +1,59 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# ExecuTorch\n+\n+[ExecuTorch](https://pytorch.org/executorch/stable/index.html) is a platform that enables PyTorch training and inference programs to be run on mobile and edge devices. It is powered by [torch.compile](https://pytorch.org/docs/stable/torch.compiler.html) and [torch.export](https://pytorch.org/docs/main/export.html) for performance and deployment.\n+\n+You can use ExecuTorch with Transformers with [torch.export](https://pytorch.org/docs/main/export.html). The [`~transformers.convert_and_export_with_cache`] method converts a [`PreTrainedModel`] into an exportable module. Under the hood, it uses [torch.export](https://pytorch.org/docs/main/export.html) to export the model, ensuring compatibility with ExecuTorch.\n+\n+```py\n+import torch\n+from transformers import LlamaForCausalLM, AutoTokenizer, GenerationConfig\n+from transformers.integrations.executorch import(\n+    TorchExportableModuleWithStaticCache,\n+    convert_and_export_with_cache\n+)\n+\n+generation_config = GenerationConfig(\n+    use_cache=True,\n+    cache_implementation=\"static\",\n+    cache_config={\n+        \"batch_size\": 1,\n+        \"max_cache_len\": 20,\n+    }\n+)\n+\n+tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\", pad_token=\"</s>\", padding_side=\"right\")\n+model = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\", device_map=\"auto\", torch_dtype=torch.bfloat16, attn_implementation=\"sdpa\", generation_config=generation_config)\n+\n+exported_program = convert_and_export_with_cache(model)\n+```\n+\n+The exported PyTorch model is now ready to be used with ExecuTorch. Wrap the model with [`~transformers.TorchExportableModuleWithStaticCache`] to generate text.\n+\n+```py\n+prompts = [\"Simply put, the theory of relativity states that \"]\n+prompt_tokens = tokenizer(prompts, return_tensors=\"pt\", padding=True).to(model.device)\n+prompt_token_ids = prompt_tokens[\"input_ids\"]\n+\n+generated_ids = TorchExportableModuleWithStaticCache.generate(\n+    exported_program=exported_program, prompt_token_ids=prompt_token_ids, max_new_tokens=20,\n+)\n+generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n+print(generated_text)\n+['Simply put, the theory of relativity states that 1) the speed of light is the']\n+```"
        },
        {
            "sha": "921c0ba7b6f588fe6896b1016d7e1c9f1a24c5e1",
            "filename": "docs/source/en/fast_tokenizers.md",
            "status": "modified",
            "additions": 325,
            "deletions": 37,
            "changes": 362,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Ffast_tokenizers.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Ffast_tokenizers.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ffast_tokenizers.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -1,4 +1,4 @@\n-<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n+<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n \n Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n the License. You may obtain a copy of the License at\n@@ -14,61 +14,349 @@ rendered properly in your Markdown viewer.\n \n -->\n \n-# Use tokenizers from ðŸ¤— Tokenizers\n+# Tokenizers\n \n-The [`PreTrainedTokenizerFast`] depends on the [ðŸ¤— Tokenizers](https://huggingface.co/docs/tokenizers) library. The tokenizers obtained from the ðŸ¤— Tokenizers library can be\n-loaded very simply into ðŸ¤— Transformers.\n+Tokenizers convert text into an array of numbers known as tensors, the inputs to a text model. There are several tokenizer algorithms, but they all share the same purpose. Split text into smaller words or subwords (tokens) according to some rules, and convert them into numbers (input ids). A Transformers tokenizer also returns an attention mask to indicate which tokens should be attended to.\n \n-Before getting in the specifics, let's first start by creating a dummy tokenizer in a few lines:\n+> [!TIP]\n+> Learn about the most popular tokenization algorithms on the [Summary of the tokenizers](./tokenizer_summary) doc.\n \n-```python\n->>> from tokenizers import Tokenizer\n->>> from tokenizers.models import BPE\n->>> from tokenizers.trainers import BpeTrainer\n->>> from tokenizers.pre_tokenizers import Whitespace\n+Call [`~PreTrainedTokenizer.from_pretrained`] to load a tokenizer and its configuration from the Hugging Face [Hub](https://hf.co) or a local directory. The pretrained tokenizer is saved in a [tokenizer.model](https://huggingface.co/google/gemma-2-2b/blob/main/tokenizer.model) file with all its associated vocabulary files.\n \n->>> tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n->>> trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n+Pass a string of text to the tokenizer to return the input ids and attention mask, and set the framework tensor type to return with the `return_tensors` parameter.\n \n->>> tokenizer.pre_tokenizer = Whitespace()\n->>> files = [...]\n->>> tokenizer.train(files, trainer)\n+```py\n+from transformers import AutoTokenizer\n+\n+tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\")\n+tokenizer(\"We are very happy to show you the ðŸ¤— Transformers library\", return_tensors=\"pt\")\n+{'input_ids': tensor([[     2,   1734,    708,   1508,   4915,    577,   1500,    692,    573,\n+         156808, 128149,   9581, 235265]]), \n+ 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n+}\n+```\n+\n+Whichever tokenizer you use, make sure the tokenizer vocabulary is the same as the pretrained models tokenizer vocabulary. This is especially important if you're using a custom tokenizer with a different vocabulary from the pretrained models tokenizer.\n+\n+This guide provides a brief overview of the tokenizer classes and how to preprocess text with it.\n+\n+## Tokenizer classes\n+\n+All tokenizers inherit from a [`PreTrainedTokenizerBase`] class that provides common methods for all tokenizers like [`~PreTrainedTokenizerBase.from_pretrained`] and [`~PreTrainedTokenizerBase.batch_decode`]. There are two main tokenizer classes that build on top of the base class.\n+\n+- [`PreTrainedTokenizer`] is a Python implementation, for example [`LlamaTokenizer`].\n+- [`PreTrainedTokenizerFast`] is a fast Rust-based implementation from the [Tokenizers](https://hf.co/docs/tokenizers/index) library, for example [`LlamaTokenizerFast`].\n+\n+There are two ways you can load a tokenizer, with [`AutoTokenizer`] or a model-specific tokenizer.\n+\n+<hfoptions id=\"tokenizer-classes\">\n+<hfoption id=\"AutoTokenizer\">\n+\n+The [AutoClass](./model_doc/auto) API is a fast and easy way to load a tokenizer without needing to know whether a Python or Rust-based implementation is available. By default, [`AutoTokenizer`] tries to load a fast tokenizer if it's available, otherwise, it loads the Python implementation.\n+\n+Use [`~PreTrainedTokenizer.from_pretrained`] to load a tokenizer.\n+\n+```py\n+from transformers import AutoTokenizer\n+\n+tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\")\n+tokenizer(\"We are very happy to show you the ðŸ¤— Transformers library.\", return_tensors=\"pt\")\n+{'input_ids': tensor([[     2,   1734,    708,   1508,   4915,    577,   1500,    692,    573,\n+         156808, 128149,   9581, 235265]]), \n+ 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n+}\n+```\n+\n+Load your own tokenizer by passing its vocabulary file to [`~AutoTokenizer.from_pretrained`].\n+\n+```py\n+from transformers import AutoTokenizer\n+\n+tokenizer = AutoTokenizer.from_pretrained(\"./model_directory/my_vocab_file.txt\")\n+```\n+\n+</hfoption>\n+<hfoption id=\"model-specific tokenizer\">\n+\n+Each pretrained model is associated with a tokenizer and the specific vocabulary it was trained on. A tokenizer can be loaded directly from the model-specific class.\n+\n+> [!TIP]\n+> Refer to a models API documentation to check whether a fast tokenizer is supported.\n+\n+```py\n+from transformers import GemmaTokenizer\n+\n+tokenizer = GemmaTokenizer.from_pretrained(\"google/gemma-2-2b\")\n+tokenizer(\"We are very happy to show you the ðŸ¤— Transformers library.\", return_tensors=\"pt\")\n+```\n+\n+To load a fast tokenizer, use the fast implementation class.\n+\n+```py\n+from transformers import GemmaTokenizerFast\n+\n+tokenizer = GemmaTokenizerFast.from_pretrained(\"google/gemma-2-2b\")\n+tokenizer(\"We are very happy to show you the ðŸ¤— Transformers library.\", return_tensors=\"pt\")\n+```\n+\n+Load your own tokenizer by passing its vocabulary file to the `vocab_file` parameter.\n+\n+```py\n+from transformers import GemmaTokenizerFast\n+\n+tokenizer = GemmaTokenizerFast(vocab_file=\"my_vocab_file.txt\")\n ```\n \n-We now have a tokenizer trained on the files we defined. We can either continue using it in that runtime, or save it to\n-a JSON file for future re-use.\n+</hfoption>\n+</hfoptions>\n+\n+## Multimodal tokenizers\n+\n+In addition to text tokens, multimodal tokenizers also holds tokens from other modalities as a part of its attributes for easy access. \n \n-## Loading directly from the tokenizer object\n+To add these special tokens to a tokenizer, pass them as a dictionary to the `extra_special_tokens` parameter in [`~AutoTokenizer.from_pretrained`]. The example below adds the `image_token` to a vision-language model.\n \n-Let's see how to leverage this tokenizer object in the ðŸ¤— Transformers library. The\n-[`PreTrainedTokenizerFast`] class allows for easy instantiation, by accepting the instantiated\n-*tokenizer* object as an argument:\n+Save the tokenizer so you can reuse it with direct access to the `image_token`, `boi_token`, and `eoi_token`.\n \n-```python\n->>> from transformers import PreTrainedTokenizerFast\n+```py\n+vision_tokenizer = AutoTokenizer.from_pretrained(\n+    \"llava-hf/llava-1.5-7b-hf\",\n+    extra_special_tokens={\"image_token\": \"<image>\", \"boi_token\": \"<image_start>\", \"eoi_token\": \"<image_end>\"}\n+)\n+print(vision_tokenizer.image_token, vision_tokenizer.image_token_id)\n+(\"<image>\", 32000)\n \n->>> fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)\n+vision_tokenizer.save_pretrained(\"./path/to/tokenizer\")\n ```\n \n-This object can now be used with all the methods shared by the ðŸ¤— Transformers tokenizers! Head to [the tokenizer\n-page](main_classes/tokenizer) for more information.\n+## Fast tokenizers\n+\n+<Youtube id=\"3umI3tm27Vw\"/>\n+\n+[`PreTrainedTokenizerFast`] or *fast tokenizers* are Rust-based tokenizers from the [Tokenizers](https://hf.co/docs/tokenizers) library. It is significantly faster at batched tokenization and provides additional alignment methods compared to the Python-based tokenizers.\n+\n+[`AutoTokenizer`] automatically loads a fast tokenizer if it's supported. Otherwise, you need to explicitly load the fast tokenizer.\n+\n+This section will show you how to train a fast tokenizer and reuse it in Transformers.\n \n-## Loading from a JSON file\n+To train a Byte-Pair Encoding (BPE) tokenizer, create a [`~tokenizers.Tokenizer`] and [`~tokenizers.trainers.BpeTrainer`] class and define the unknown token and special tokens.\n+\n+```py\n+from tokenizers import Tokenizer\n+from tokenizers.models import BPE\n+from tokenizers.trainers import BpeTrainer\n+\n+tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n+trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n+```\n+\n+Split the tokens on [`~tokenizers.pre_tokenizers.Whitespace`] to create tokens that don't overlap with each other.\n+\n+```py\n+from tokenizers.pre_tokenizers import Whitespace\n+\n+tokenizer.pre_tokenizer = Whitespace()\n+```\n \n-In order to load a tokenizer from a JSON file, let's first start by saving our tokenizer:\n+Call [`~tokenizers.Tokenizer.train`] on the text files and trainer to start training.\n \n-```python\n->>> tokenizer.save(\"tokenizer.json\")\n+```py\n+files = [...]\n+tokenizer.train(files, trainer)\n ```\n \n-The path to which we saved this file can be passed to the [`PreTrainedTokenizerFast`] initialization\n-method using the `tokenizer_file` parameter:\n+Use [`~tokenizers.Tokenizer.save`] to save the tokenizers configuration and vocabulary to a JSON file.\n+\n+```py\n+tokenizer.save(\"tokenizer.json\")\n+```\n+\n+Now you can load and reuse the tokenizer object in Transformers by passing it to the `tokenizer_object` parameter in [`PreTrainedTokenizerFast`].\n+\n+```py\n+from transformers import PreTrainedTokenizerFast\n+\n+fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)\n+```\n+\n+To load a saved tokenizer from its JSON file, pass the file path to the `tokenizer_file` parameter in [`PreTrainedTokenizerFast`].\n+\n+```py\n+from transformers import PreTrainedTokenizerFast\n+\n+fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"tokenizer.json\")\n+```\n+\n+## tiktoken\n+\n+[tiktoken](https://github.com/openai/tiktoken) is a [byte-pair encoding (BPE)](./tokenizer_summary#byte-pair-encoding-bpe) tokenizer by OpenAI. It includes several tokenization schemes or encodings for how text should be tokenized.\n+\n+There are currently two models trained and released with tiktoken, GPT2 and Llama3. Transformers supports models with a [tokenizer.model](https://hf.co/meta-llama/Meta-Llama-3-8B/blob/main/original/tokenizer.model) tiktoken file. The tiktoken file is automatically converted into Transformers Rust-based [`PreTrainedTokenizerFast`].\n+\n+Add the `subfolder` parameter to [`~PreTrainedModel.from_pretrained`] to specify where the `tokenizer.model` tiktoken file is located.\n \n-```python\n->>> from transformers import PreTrainedTokenizerFast\n+```py\n+from transformers import AutoTokenizer\n \n->>> fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"tokenizer.json\")\n+tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\", subfolder=\"original\") \n ```\n \n-This object can now be used with all the methods shared by the ðŸ¤— Transformers tokenizers! Head to [the tokenizer\n-page](main_classes/tokenizer) for more information.\n+### Create a tiktoken tokenizer\n+\n+The tiktoken `tokenizer.model` file contains no information about additional tokens or pattern strings. If these are important, convert the tokenizer to `tokenizer.json` (the appropriate format for [`PreTrainedTokenizerFast`]).\n+\n+Generate the tiktoken `tokenizer.model` file with the [tiktoken.get_encoding](https://github.com/openai/tiktoken/blob/63527649963def8c759b0f91f2eb69a40934e468/tiktoken/registry.py#L63) function, and convert it to `tokenizer.json` with [convert_tiktoken_to_fast](https://github.com/huggingface/transformers/blob/99e0ab6ed888136ea4877c6d8ab03690a1478363/src/transformers/integrations/tiktoken.py#L8).\n+\n+```py\n+from transformers.integrations.tiktoken import convert_tiktoken_to_fast\n+from tiktoken import get_encoding\n+\n+# Load your custom encoding or the one provided by OpenAI\n+encoding = get_encoding(\"gpt2\")\n+convert_tiktoken_to_fast(encoding, \"config/save/dir\")\n+```\n+\n+The resulting `tokenizer.json` file is saved to the specified directory and loaded with [`~PreTrainedTokenizerFast.from_pretrained`].\n+\n+```py\n+tokenizer = PreTrainedTokenizerFast.from_pretrained(\"config/save/dir\")\n+```\n+\n+## Preprocess\n+\n+<Youtube id=\"Yffk5aydLzg\"/>\n+\n+A Transformers model expects the input to be a PyTorch, TensorFlow, or NumPy tensor. A tokenizers job is to preprocess text into those tensors. Specify the framework tensor type to return with the `return_tensors` parameter.\n+\n+```py\n+from transformers import AutoTokenizer\n+\n+tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\")\n+tokenizer(\"We are very happy to show you the ðŸ¤— Transformers library.\", return_tensors=\"pt\")\n+{'input_ids': tensor([[     2,   1734,    708,   1508,   4915,    577,   1500,    692,    573,\n+         156808, 128149,   9581, 235265]]), \n+ 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n+}\n+```\n+\n+The tokenization process of converting text into input ids is completed in two steps.\n+\n+<hfoptions id=\"steps\">\n+<hfoption id=\"1. tokenize\">\n+\n+In the first step, a string of text is split into tokens by the [`~PreTrainedTokenizer.tokenize`] function. How the text is split depends on the tokenization algorithm.\n+\n+```py\n+tokens = tokenizer.tokenize(\"We are very happy to show you the ðŸ¤— Transformers library\")\n+print(tokens)\n+['We', 'â–are', 'â–very', 'â–happy', 'â–to', 'â–show', 'â–you', 'â–the', 'â–ðŸ¤—', 'â–Transformers', 'â–library']\n+```\n+\n+Gemma uses a [SentencePiece](./tokenizer_summary#sentencepiece) tokenizer which replaces spaces with an underscore `_`.\n+\n+</hfoption>\n+<hfoption id=\"2. convert tokens to ids\">\n+\n+In the second step, the tokens are converted into ids with [`~PreTrainedTokenizer.convert_tokens_to_ids`].\n+\n+```py\n+ids = tokenizer.convert_tokens_to_ids(tokens)\n+print(ids)\n+[1734, 708, 1508, 4915, 577, 1500, 692, 573, 156808, 128149, 9581]\n+```\n+\n+</hfoption>\n+<hfoption id=\"3. decode ids to text\">\n+\n+Lastly, the model prediction typically generates numerical outputs which are converted back to text with [`~PreTrainedTokenizer.decode`].\n+\n+```py\n+decoded_string = tokenizer.decode(ids)\n+print(decoded_string)\n+'We are very happy to show you the ðŸ¤— Transformers library'\n+```\n+\n+</hfoption>\n+</hfoptions>\n+\n+> [!TIP]\n+> Visualize how different tokenizers work in the [Tokenizer Playground](https://xenova-the-tokenizer-playground.static.hf.space).\n+\n+### Special tokens\n+\n+Special tokens provide the model with some additional information about the text.\n+\n+For example, if you compare the tokens obtained from passing text directly to the tokenizer and from [`~PreTrainedTokenizer.convert_tokens_to_ids`], you'll notice some additional tokens are added.\n+\n+```py\n+model_inputs = tokenizer(\"We are very happy to show you the ðŸ¤— Transformers library.\")\n+[2, 1734, 708, 1508, 4915, 577, 1500, 692, 573, 156808, 128149, 9581]\n+tokenizer.convert_tokens_to_ids(tokens)\n+[1734, 708, 1508, 4915, 577, 1500, 692, 573, 156808, 128149, 9581]\n+```\n+\n+When you [`~PreTrainedTokenizer.decode`] the ids, you'll see `<bos>` at the beginning of the string. This is used to indicate the beginning of a sentence to the model.\n+\n+```py\n+print(tokenizer.decode(model_inputs[\"input_ids\"]))\n+print(tokenizer.decode(ids))\n+'<bos>We are very happy to show you the ðŸ¤— Transformers library.'\n+'We are very happy to show you the ðŸ¤— Transformers library'\n+```\n+\n+Not all models need special tokens, but if they do, a tokenizer automatically adds them.\n+\n+### Batch tokenization\n+\n+It is faster and more efficient to preprocess *batches* of text instead of a single sentence at a time. Fast tokenizers are especially good at parallelizing tokenization.\n+\n+Pass a list of string text to the tokenizer.\n+\n+```py\n+batch_sentences = [\n+    \"But what about second breakfast?\",\n+    \"Don't think he knows about second breakfast, Pip.\",\n+    \"What about elevensies?\",\n+]\n+encoded_inputs = tokenizer(batch_sentences, return_tensors=\"pt\")\n+print(encoded_inputs)\n+{\n+ 'input_ids': \n+    [[2, 1860, 1212, 1105, 2257, 14457, 235336], \n+     [2, 4454, 235303, 235251, 1742, 693, 9242, 1105, 2257, 14457, 235269, 48782, 235265], \n+     [2, 1841, 1105, 29754, 37453, 235336]], \n+ 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], \n+                    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \n+                    [1, 1, 1, 1, 1, 1]]\n+}\n+```\n+\n+### Padding\n+\n+> [!TIP]\n+> Learn about additional padding strategies in the [Padding and truncation](./pad_truncation) guide.\n+\n+In the output above, the `input_ids` have different lengths. This is an issue because Transformers expects them to have the same lengths so it can pack them into a batch. Sequences with uneven lengths can't be batched.\n+\n+Padding adds a special *padding token* to ensure all sequences have the same length. Set `padding=True` to pad the sequences to the longest sequence length in the batch.\n+\n+```py\n+encoded_inputs = tokenizer(batch_sentences, padding=True, return_tensors=\"pt\")\n+print(encoded_inputs)\n+```\n+\n+The tokenizer added the special padding token `0` to the left side (*left padding*) because Gemma and LLMs in general are not trained to continue generation from a padding token.\n+\n+### Truncation\n+\n+> [!TIP]\n+> Learn about additional truncation strategies in the [Padding and truncation](./pad_truncation) guide.\n+\n+Models are only able to process sequences up to a certain length. If you try to process a sequence longer than a model can handle, it crashes.\n+\n+Truncation removes tokens from a sequence to ensure it doesn't exceed the maximum length. Set `truncation=True` to truncate a sequence to the maximum length accepted by the model. You can also set the maximum length yourself with the `max_length` parameter.\n+\n+```py\n+encoded_inputs = tokenizer(batch_sentences, max_length=8, truncation=True, return_tensors=\"pt\")\n+print(encoded_inputs)\n+```"
        },
        {
            "sha": "6cc202057697773840dac7fd8b05ead2c22acc2e",
            "filename": "docs/source/en/feature_extractors.md",
            "status": "added",
            "additions": 199,
            "deletions": 0,
            "changes": 199,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Ffeature_extractors.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Ffeature_extractors.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ffeature_extractors.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -0,0 +1,199 @@\n+<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# Feature extractors\n+\n+Feature extractors preprocess audio data into the correct format for a given model. It takes the raw audio signal and converts it into a tensor that can be fed to a model. The tensor shape depends on the model, but the feature extractor will correctly preprocess the audio data for you given the model you're using. Feature extractors also include methods for padding, truncation, and resampling.\n+\n+Call [`~AutoFeatureExtractor.from_pretrained`] to load a feature extractor and its preprocessor configuration from the Hugging Face [Hub](https://hf.co/models) or local directory. The feature extractor and preprocessor configuration is saved in a [preprocessor_config.json](https://hf.co/openai/whisper-tiny/blob/main/preprocessor_config.json) file.\n+\n+Pass the audio signal, typically stored in `array`, to the feature extractor and set the `sampling_rate` parameter to the pretrained audio models sampling rate. It is important the sampling rate of the audio data matches the sampling rate of the data a pretrained audio model was trained on.\n+\n+```py\n+from transformers import AutoFeatureExtractor\n+\n+feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\")\n+processed_sample = feature_extractor(dataset[0][\"audio\"][\"array\"], sampling_rate=16000)\n+processed_sample\n+{'input_values': [array([ 9.4472744e-05,  3.0777880e-03, -2.8888427e-03, ...,\n+       -2.8888427e-03,  9.4472744e-05,  9.4472744e-05], dtype=float32)]}\n+```\n+\n+The feature extractor returns an input, `input_values`, that is ready for the model to consume.\n+\n+This guide walks you through the feature extractor classes and how to preprocess audio data.\n+\n+## Feature extractor classes\n+\n+Transformers feature extractors inherit from the base [`SequenceFeatureExtractor`] class which subclasses [`FeatureExtractionMixin`].\n+\n+- [`SequenceFeatureExtractor`] provides a method to [`~SequenceFeatureExtractor.pad`] sequences to a certain length to avoid uneven sequence lengths.\n+- [`FeatureExtractionMixin`] provides [`~FeatureExtractionMixin.from_pretrained`] and [`~FeatureExtractionMixin.save_pretrained`] to load and save a feature extractor.\n+\n+There are two ways you can load a feature extractor, [`AutoFeatureExtractor`] and a model-specific feature extractor class.\n+\n+<hfoptions id=\"feature-extractor-classes\">\n+<hfoption id=\"AutoFeatureExtractor\">\n+\n+The [AutoClass](./model_doc/auto) API automatically loads the correct feature extractor for a given model.\n+\n+Use [`~AutoFeatureExtractor.from_pretrained`] to load a feature extractor.\n+\n+```py\n+from transformers import AutoFeatureExtractor\n+\n+feature_extractor = AutoFeatureExtractor.from_pretrained(\"openai/whisper-tiny\")\n+```\n+\n+</hfoption>\n+<hfoption id=\"model-specific feature extractor\">\n+\n+Every pretrained audio model has a specific associated feature extractor for correctly processing audio data. When you load a feature extractor, it retrieves the feature extractors configuration (feature size, chunk length, etc.) from [preprocessor_config.json](https://hf.co/openai/whisper-tiny/blob/main/preprocessor_config.json).\n+\n+A feature extractor can be loaded directly from its model-specific class.\n+\n+```py\n+from transformers import WhisperFeatureExtractor\n+\n+feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-tiny\")\n+```\n+\n+</hfoption>\n+</hfoptions>\n+\n+## Preprocess\n+\n+A feature extractor expects the input as a PyTorch tensor of a certain shape. The exact input shape can vary depending on the specific audio model you're using.\n+\n+For example, [Whisper](https://huggingface.co/docs/transformers/model_doc/whisper) expects `input_features` to be a tensor of shape `(batch_size, feature_size, sequence_length)` but [Wav2Vec2](https://hf.co/docs/transformers/model_doc/wav2vec2) expects `input_values` to be a tensor of shape `(batch_size, sequence_length)`.\n+\n+The feature extractor generates the correct input shape for whichever audio model you're using.\n+\n+A feature extractor also sets the sampling rate (the number of audio signal values taken per second) of the audio files. The sampling rate of your audio data must match the sampling rate of the dataset a pretrained model was trained on. This value is typically given in the model card.\n+\n+Load a dataset and feature extractor with [`~FeatureExtractionMixin.from_pretrained`].\n+\n+```py\n+from datasets import load_dataset, Audio\n+from transformers import AutoFeatureExtractor\n+\n+dataset = load_dataset(\"PolyAI/minds14\", name=\"en-US\", split=\"train\")\n+feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\")\n+```\n+\n+Check out the first example from the dataset and access the `audio` column which contains `array`, the raw audio signal.\n+\n+```py\n+dataset[0][\"audio\"][\"array\"]\n+array([ 0.        ,  0.00024414, -0.00024414, ..., -0.00024414,\n+        0.        ,  0.        ])\n+```\n+\n+The feature extractor preprocesses `array` into the expected input format for a given audio model. Use the `sampling_rate` parameter to set the appropriate sampling rate.\n+\n+```py\n+processed_dataset = feature_extractor(dataset[0][\"audio\"][\"array\"], sampling_rate=16000)\n+processed_dataset\n+{'input_values': [array([ 9.4472744e-05,  3.0777880e-03, -2.8888427e-03, ...,\n+       -2.8888427e-03,  9.4472744e-05,  9.4472744e-05], dtype=float32)]}\n+```\n+\n+### Padding\n+\n+Audio sequence lengths that are different is an issue because Transformers expects all sequences to have the same lengths so they can be batched. Uneven sequence lengths can't be batched.\n+\n+```py\n+dataset[0][\"audio\"][\"array\"].shape\n+(86699,)\n+\n+dataset[1][\"audio\"][\"array\"].shape\n+(53248,)\n+```\n+\n+Padding adds a special *padding token* to ensure all sequences have the same length. The feature extractor adds a `0` - interpreted as silence - to `array` to pad it. Set `padding=True` to pad sequences to the longest sequence length in the batch.\n+\n+```py\n+def preprocess_function(examples):\n+    audio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\n+    inputs = feature_extractor(\n+        audio_arrays,\n+        sampling_rate=16000,\n+        padding=True,\n+    )\n+    return inputs\n+\n+processed_dataset = preprocess_function(dataset[:5])\n+processed_dataset[\"input_values\"][0].shape\n+(86699,)\n+\n+processed_dataset[\"input_values\"][1].shape\n+(86699,)\n+```\n+\n+### Truncation\n+\n+Models can only process sequences up to a certain length before crashing.\n+\n+Truncation is a strategy for removing excess tokens from a sequence to ensure it doesn't exceed the maximum length. Set `truncation=True` to truncate a sequence to the length in the `max_length` parameter.\n+\n+```py\n+def preprocess_function(examples):\n+    audio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\n+    inputs = feature_extractor(\n+        audio_arrays,\n+        sampling_rate=16000,\n+        max_length=50000,\n+        truncation=True,\n+    )\n+    return inputs\n+\n+processed_dataset = preprocess_function(dataset[:5])\n+processed_dataset[\"input_values\"][0].shape\n+(50000,)\n+\n+processed_dataset[\"input_values\"][1].shape\n+(50000,)\n+```\n+\n+### Resampling\n+\n+The [Datasets](https://hf.co/docs/datasets/index) library can also resample audio data to match an audio models expected sampling rate. This method resamples the audio data on the fly when they're loaded which can be faster than resampling the entire dataset in-place.\n+\n+The audio dataset you've been working on has a sampling rate of 8kHz and the pretrained model expects 16kHz.\n+\n+```py\n+dataset[0][\"audio\"]\n+{'path': '/root/.cache/huggingface/datasets/downloads/extracted/f507fdca7f475d961f5bb7093bcc9d544f16f8cab8608e772a2ed4fbeb4d6f50/en-US~JOINT_ACCOUNT/602ba55abb1e6d0fbce92065.wav',\n+ 'array': array([ 0.        ,  0.00024414, -0.00024414, ..., -0.00024414,\n+         0.        ,  0.        ]),\n+ 'sampling_rate': 8000}\n+```\n+\n+Call [`~datasets.Dataset.cast_column`] on the `audio` column to upsample the sampling rate to 16kHz.\n+\n+```py\n+dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n+```\n+\n+When you load the dataset sample, it is now resampled to 16kHz.\n+\n+```py\n+dataset[0][\"audio\"]\n+{'path': '/root/.cache/huggingface/datasets/downloads/extracted/f507fdca7f475d961f5bb7093bcc9d544f16f8cab8608e772a2ed4fbeb4d6f50/en-US~JOINT_ACCOUNT/602ba55abb1e6d0fbce92065.wav',\n+ 'array': array([ 1.70562416e-05,  2.18727451e-04,  2.28099874e-04, ...,\n+         3.43842403e-05, -5.96364771e-06, -1.76846661e-05]),\n+ 'sampling_rate': 16000}\n+```"
        },
        {
            "sha": "944c5a18e109356195fd4b5eb2cf176174af9693",
            "filename": "docs/source/en/fsdp.md",
            "status": "modified",
            "additions": 46,
            "deletions": 39,
            "changes": 85,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Ffsdp.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Ffsdp.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ffsdp.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -1,4 +1,4 @@\n-<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n+<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n \n Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n the License. You may obtain a copy of the License at\n@@ -14,81 +14,86 @@ rendered properly in your Markdown viewer.\n \n -->\n \n-# Fully Sharded Data Parallel\n+# FullyShardedDataParallel\n \n-[Fully Sharded Data Parallel (FSDP)](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/) is a data parallel method that shards a model's parameters, gradients and optimizer states across the number of available GPUs (also called workers or *rank*). Unlike [DistributedDataParallel (DDP)](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html), FSDP reduces memory-usage because a model is replicated on each GPU. This improves GPU memory-efficiency and allows you to train much larger models on fewer GPUs. FSDP is integrated with the Accelerate, a library for easily managing training in distributed environments, which means it is available for use from the [`Trainer`] class.\n+[Fully Sharded Data Parallel (FSDP)](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/) is a [parallelism](./perf_train_gpu_many) method that combines the advantages of data and model parallelism for distributed training.\n \n-Before you start, make sure Accelerate is installed and at least PyTorch 2.1.0 or newer.\n+Unlike [DistributedDataParallel (DDP)](./perf_train_gpu_many#distributeddataparallel), FSDP saves more memory because it doesn't replicate a model on each GPU. It shards the models parameters, gradients and optimizer states across GPUs. Each model shard processes a portion of the data and the results are synchronized to speed up training.\n+\n+This guide covers how to set up training a model with FSDP and [Accelerate](https://hf.co/docs/accelerate/index), a library for managing distributed training.\n \n ```bash\n pip install accelerate\n ```\n \n-## FSDP configuration\n+## Configuration options\n \n-To start, run the [`accelerate config`](https://huggingface.co/docs/accelerate/package_reference/cli#accelerate-config) command to create a configuration file for your training environment. Accelerate uses this configuration file to automatically setup the correct training environment based on your selected training options in `accelerate config`.\n+Always start by running the [accelerate config](https://hf.co/docs/accelerate/package_reference/cli#accelerate-config) command to help Accelerate set up the correct distributed training environment.\n \n ```bash\n accelerate config\n ```\n \n-When you run `accelerate config`, you'll be prompted with a series of options to configure your training environment. This section covers some of the most important FSDP options. To learn more about the other available FSDP options, take a look at the [fsdp_config](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.fsdp_config) parameters.\n+The section below discusses some of the more important FSDP configuration options. Learn more about other available options in the [fsdp_config](https://hf.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.fsdp_config) parameter.\n \n ### Sharding strategy\n \n-FSDP offers a number of sharding strategies to select from:\n-\n-* `FULL_SHARD` - shards model parameters, gradients and optimizer states across workers; select `1` for this option\n-* `SHARD_GRAD_OP`- shard gradients and optimizer states across workers; select `2` for this option\n-* `NO_SHARD` - don't shard anything (this is equivalent to DDP); select `3` for this option\n-* `HYBRID_SHARD` - shard model parameters, gradients and optimizer states within each worker where each worker also has a full copy; select `4` for this option\n-* `HYBRID_SHARD_ZERO2` - shard gradients and optimizer states within each worker where each worker also has a full copy; select `5` for this option\n+FSDP offers several sharding strategies to distribute a model. Refer to the table below to help you choose the best strategy for your setup. Specify a strategy with the `fsdp_sharding_strategy` parameter in the configuration file.\n \n-This is enabled by the `fsdp_sharding_strategy` flag.\n+| sharding strategy | description | parameter value |\n+|---|---|---|\n+| `FULL_SHARD` | shards model parameters, gradients, and optimizer states | `1` |\n+| `SHARD_GRAD_OP` | shards gradients and optimizer states | `2` |\n+| `NO_SHARD` | don't shard the model | `3` |\n+| `HYBRID_SHARD` | shards model parameters, gradients, and optimizer states within each GPU | `4` |\n+| `HYBRID_SHARD_ZERO2` | shards gradients and optimizer states within each GPU | `5` |\n \n ### CPU offload\n \n-You could also offload parameters and gradients when they are not in use to the CPU to save even more GPU memory and help you fit large models where even FSDP may not be sufficient. This is enabled by setting `fsdp_offload_params: true` when running `accelerate config`.\n+Offload model parameters and gradients when they aren't being used to the CPU to save additional GPU memory. This is useful for scenarios where a model is too large even with FSDP.\n+\n+Specify `fsdp_offload_params: true` in the configuration file to enable offloading.\n \n ### Wrapping policy\n \n-FSDP is applied by wrapping each layer in the network. The wrapping is usually applied in a nested way where the full weights are discarded after each forward pass to save memory for use in the next layer. The *auto wrapping* policy is the simplest way to implement this and you don't need to change any code. You should select `fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP` to wrap a Transformer layer and `fsdp_transformer_layer_cls_to_wrap` to specify which layer to wrap (for example `BertLayer`).\n+FSDP is applied by wrapping each layer in the network. The wrapping is usually applied in a nested way where the full weights are discarded after each forward pass to save memory for the next layer.\n+\n+There are several wrapping policies available, but the *auto wrapping* policy is the simplest and doesn't require any changes to your code. Specify `fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP` to wrap a Transformer layer and `fsdp_transformer_layer_cls_to_wrap` to determine which layer to wrap (for example, `BertLayer`).\n \n-Otherwise, you can choose a size-based wrapping policy where FSDP is applied to a layer if it exceeds a certain number of parameters. This is enabled by setting `fsdp_wrap_policy: SIZE_BASED_WRAP` and `min_num_param` to the desired size threshold.\n+Size-based wrapping is also available. If a layer exceeds a certain number of parameters, it is wrapped. Specify `fsdp_wrap_policy: SIZED_BASED_WRAP` and `min_num_param` to set the minimum number of parameters for a layer to be wrapped.\n \n-### Checkpointing\n+### Checkpoints\n \n-Intermediate checkpoints should be saved with `fsdp_state_dict_type: SHARDED_STATE_DICT` because saving the full state dict with CPU offloading on rank 0 takes a lot of time and often results in `NCCL Timeout` errors due to indefinite hanging during broadcasting. You can resume training with the sharded state dicts with the [`~accelerate.Accelerator.load_state`] method.\n+Intermediate checkpoints should be saved as a sharded state dict because saving the full state dict - even with CPU offloading - is time consuming and can cause `NCCL Timeout` errors due to indefinite hanging during broadcasting.\n+\n+Specify `fsdp_state_dict_type: SHARDED_STATE_DICT` in the configuration file to save the sharded state dict. Now you can resume training from the sharded state dict with [`~accelerate.Accelerator.load_state`].\n \n ```py\n-# directory containing checkpoints\n-accelerator.load_state(\"ckpt\")\n+accelerator.load_state(\"directory/containing/checkpoints\")\n ```\n \n-However, when training ends, you want to save the full state dict because sharded state dict is only compatible with FSDP.\n+Once training is complete though, you should save the full state dict because the sharded state dict is only compatible with FSDP.\n \n ```py\n if trainer.is_fsdp_enabled:\n-    trainer.accelerator.state.fsdp_plugin.set_state_dict_type(\"FULL_STATE_DICT\")\n+  trainer.accelerator.state.fsdp_plugin.set_state_dict_type(\"FULL_STATE_DICT\")\n \n trainer.save_model(script_args.output_dir)\n ```\n \n ### TPU\n \n-[PyTorch XLA](https://pytorch.org/xla/release/2.1/index.html) supports FSDP training for TPUs and it can be enabled by modifying the FSDP configuration file generated by `accelerate config`. In addition to the sharding strategies and wrapping options specified above, you can add the parameters shown below to the file.\n+[PyTorch XLA](https://pytorch.org/xla/release/2.1/index.html), a package for running PyTorch on XLA devices, enables FSDP on TPUs. Modify the configuration file to include the parameters below. Refer to the [xla_fsdp_settings](https://github.com/pytorch/xla/blob/2e6e183e0724818f137c8135b34ef273dea33318/torch_xla/distributed/fsdp/xla_fully_sharded_data_parallel.py#L128) parameter for additional XLA-specific parameters you can configure for FSDP.\n \n ```yaml\n xla: True # must be set to True to enable PyTorch/XLA\n-xla_fsdp_settings: # XLA-specific FSDP parameters\n-xla_fsdp_grad_ckpt: True # use gradient checkpointing\n+xla_fsdp_settings: # XLA specific FSDP parameters\n+xla_fsdp_grad_ckpt: True # enable gradient checkpointing\n ```\n \n-The [`xla_fsdp_settings`](https://github.com/pytorch/xla/blob/2e6e183e0724818f137c8135b34ef273dea33318/torch_xla/distributed/fsdp/xla_fully_sharded_data_parallel.py#L128) allow you to configure additional XLA-specific parameters for FSDP.\n-\n-## Launch training\n+## Training\n \n-An example FSDP configuration file may look like:\n+After running [accelerate config](https://hf.co/docs/accelerate/package_reference/cli#accelerate-config), your configuration file should be ready. An example configuration file is shown below that fully shards the parameter, gradient and optimizer states on two GPUs. Your file may look different depending on how you set up your configuration.\n \n ```yaml\n compute_environment: LOCAL_MACHINE\n@@ -119,20 +124,22 @@ tpu_use_sudo: false\n use_cpu: false\n ```\n \n-To launch training, run the [`accelerate launch`](https://huggingface.co/docs/accelerate/package_reference/cli#accelerate-launch) command and it'll automatically use the configuration file you previously created with `accelerate config`.\n+Run the [accelerate launch](https://hf.co/docs/accelerate/package_reference/cli#accelerate-launch) command to launch a training script with the FSDP configurations you chose in the configuration file.\n \n ```bash\n-accelerate launch my-trainer-script.py\n+accelerate launch my-training-script.py\n ```\n \n+It is also possible to directly specify some of the FSDP arguments in the command line.\n+\n ```bash\n-accelerate launch --fsdp=\"full shard\" --fsdp_config=\"path/to/fsdp_config/ my-trainer-script.py\n+accelerate launch --fsdp=\"full shard\" --fsdp_config=\"path/to/fsdp_config/\" my-training-script.py\n ```\n \n-## Next steps\n+## Resources\n \n-FSDP can be a powerful tool for training really large models and you have access to more than one GPU or TPU. By sharding the model parameters, optimizer and gradient states, and even offloading them to the CPU when they're inactive, FSDP can reduce the high cost of large-scale training. If you're interested in learning more, the following may be helpful:\n+FSDP is a powerful tool for training large models with fewer GPUs compared to other parallelism strategies. Refer to the following resources below to learn even more about FSDP.\n \n-* Follow along with the more in-depth Accelerate guide for [FSDP](https://huggingface.co/docs/accelerate/usage_guides/fsdp).\n-* Read the [Introducing PyTorch Fully Sharded Data Parallel (FSDP) API](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/) blog post.\n-* Read the [Scaling PyTorch models on Cloud TPUs with FSDP](https://pytorch.org/blog/scaling-pytorch-models-on-cloud-tpus-with-fsdp/) blog post.\n+- Follow along with the more in-depth Accelerate guide for [FSDP](https://hf.co/docs/accelerate/usage_guides/fsdp).\n+- Read the [Introducing PyTorch Fully Sharded Data Parallel (FSDP) API](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/) blog post.\n+- Read the [Scaling PyTorch models on Cloud TPUs with FSDP](https://pytorch.org/blog/scaling-pytorch-models-on-cloud-tpus-with-fsdp/) blog post."
        },
        {
            "sha": "56163ba01d9215736e076f799d13b23e669bc556",
            "filename": "docs/source/en/generation_features.md",
            "status": "added",
            "additions": 82,
            "deletions": 0,
            "changes": 82,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fgeneration_features.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fgeneration_features.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fgeneration_features.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -0,0 +1,82 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# Generation features\n+\n+The [`~GenerationMixin.generate`] API supports a couple features for building applications on top of it.\n+\n+This guide will show you how to use these features.\n+\n+## Streaming\n+\n+Streaming starts returning text as soon as it is generated so you don't have to wait to see the entire generated response all at once. It is important in user-facing applications because it reduces perceived latency and allows users to see the generation progression.\n+\n+<div class=\"flex justify-center\">\n+    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/tgi/streaming-generation-visual-dark_360.gif\"/>\n+</div>\n+\n+> [!TIP]\n+> Learn more about streaming in the [Text Generation Inference](https://huggingface.co/docs/text-generation-inference/en/conceptual/streaming) docs.\n+\n+Create an instance of [`TextStreamer`] with the tokenizer. Pass [`TextStreamer`] to the `streamer` parameter in [`~GenerationMixin.generate`] to stream the output one word at a time.\n+\n+```py\n+from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n+\n+tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n+model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n+inputs = tokenizer([\"The secret to baking a good cake is \"], return_tensors=\"pt\")\n+streamer = TextStreamer(tokenizer)\n+\n+_ = model.generate(**inputs, streamer=streamer, max_new_tokens=20)\n+```\n+\n+The `streamer` parameter is compatible with any class with a [`~TextStreamer.put`] and [`~TextStreamer.end`] method. [`~TextStreamer.put`] pushes new tokens and [`~TextStreamer.end`] flags the end of generation. You can create your own streamer class as long as they include these two methods, or you can use Transformers' basic streamer classes.\n+\n+## Watermarking\n+\n+Watermarking is useful for detecting whether text is generated. The [watermarking strategy](https://hf.co/papers/2306.04634) in Transformers randomly \"colors\" a subset of the tokens green. When green tokens are generated, they have a small bias added to their logits, and a higher probability of being generated. You can detect generated text by comparing the proportion of green tokens to the amount of green tokens typically found in human-generated text.\n+\n+Watermarking is supported for any generative model in Transformers and doesn't require an extra classfication model to detect the watermarked text.\n+\n+Create a [`WatermarkingConfig`] with the bias value to add to the logits and watermarking algorithm. The example below uses the `\"selfhash\"` algorithm, where the green token selection only depends on the current token. Pass the [`WatermarkingConfig`] to [`~GenerationMixin.generate`].\n+\n+> [!TIP]\n+> The [`WatermarkDetector`] class detects the proportion of green tokens in generated text, which is why it is recommended to strip the prompt text, if it is much longer than the generated text. Padding can also have an effect on [`WatermarkDetector`].\n+\n+```py\n+from transformers import AutoTokenizer, AutoModelForCausalLM, WatermarkDetector, WatermarkingConfig\n+\n+model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n+tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n+tokenizer.pad_token_id = tokenizer.eos_token_id\n+tokenizer.padding_side = \"left\"\n+\n+inputs = tokenizer([\"This is the beginning of a long story\", \"Alice and Bob are\"], padding=True, return_tensors=\"pt\")\n+input_len = inputs[\"input_ids\"].shape[-1]\n+\n+watermarking_config = WatermarkingConfig(bias=2.5, seeding_scheme=\"selfhash\")\n+out = model.generate(**inputs, watermarking_config=watermarking_config, do_sample=False, max_length=20)\n+```\n+\n+Create an instance of [`WatermarkDetector`] and pass the model output to it to detect whether the text is machine-generated. The [`WatermarkDetector`] must have the same [`WatermarkingConfig`] used during generation.\n+\n+```py\n+detector = WatermarkDetector(model_config=model.config, device=\"cpu\", watermarking_config=watermarking_config)\n+detection_out = detector(out, return_dict=True)\n+detection_out.prediction\n+array([True, True])\n+```"
        },
        {
            "sha": "706443906ad5a60775730fc2cf73e0dd04614f13",
            "filename": "docs/source/en/generation_strategies.md",
            "status": "modified",
            "additions": 205,
            "deletions": 483,
            "changes": 688,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fgeneration_strategies.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fgeneration_strategies.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fgeneration_strategies.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -1,4 +1,4 @@\n-<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n+<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n \n Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n the License. You may obtain a copy of the License at\n@@ -14,595 +14,317 @@ rendered properly in your Markdown viewer.\n \n -->\n \n-# Text generation strategies\n+# Generation strategies\n \n-Text generation is essential to many NLP tasks, such as open-ended text generation, summarization, translation, and\n-more. It also plays a role in a variety of mixed-modality applications that have text as an output like speech-to-text\n-and vision-to-text. Some of the models that can generate text include\n-GPT2, XLNet, OpenAI GPT, CTRL, TransformerXL, XLM, Bart, T5, GIT, Whisper.\n+A decoding strategy informs how a model should select the next generated token. There are many types of decoding strategies, and choosing the appropriate one has a significant impact on the quality of the generated text.\n \n-Check out a few examples that use [`~generation.GenerationMixin.generate`] method to produce\n-text outputs for different tasks:\n-* [Text summarization](./tasks/summarization#inference)\n-* [Image captioning](./model_doc/git#transformers.GitForCausalLM.forward.example)\n-* [Audio transcription](./model_doc/whisper#transformers.WhisperForConditionalGeneration.forward.example)\n+This guide will help you understand the different decoding strategies available in Transformers and how and when to use them.\n \n-Note that the inputs to the generate method depend on the model's modality. They are returned by the model's preprocessor\n-class, such as AutoTokenizer or AutoProcessor. If a model's preprocessor creates more than one kind of input, pass all\n-the inputs to generate(). You can learn more about the individual model's preprocessor in the corresponding model's documentation.\n+## Greedy search\n \n-The process of selecting output tokens to generate text is known as decoding, and you can customize the decoding strategy\n-that the `generate()` method will use. Modifying a decoding strategy does not change the values of any trainable parameters.\n-However, it can have a noticeable impact on the quality of the generated output. It can help reduce repetition in the text\n-and make it more coherent.\n+Greedy search is the default decoding strategy. It selects the next most likely token at each step. Unless specified in [`GenerationConfig`], this strategy generates a maximum of 20 tokens.\n \n-This guide describes:\n-* default generation configuration\n-* common decoding strategies and their main parameters\n-* saving and sharing custom generation configurations with your fine-tuned model on ðŸ¤— Hub\n+Greedy search works well for tasks with relatively short outputs. However, it breaks down when generating longer sequences because it begins to repeat itself.\n \n-<Tip>\n+```py\n+import torch\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n \n-`generate()` is a critical component of our [chat CLI](quicktour#chat-with-text-generation-models).\n-You can apply the learnings of this guide there as well.\n+tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n+inputs = tokenizer(\"I look forward to\", return_tensors=\"pt\").to(\"cuda\")\n \n-</Tip>\n-\n-## Default text generation configuration\n-\n-A decoding strategy for a model is defined in its generation configuration. When using pre-trained models for inference\n-within a [`pipeline`], the models call the `PreTrainedModel.generate()` method that applies a default generation\n-configuration under the hood. The default configuration is also used when no custom configuration has been saved with\n-the model.\n-\n-When you load a model explicitly, you can inspect the generation configuration that comes with it through\n- `model.generation_config`:\n-\n-```python\n->>> from transformers import AutoModelForCausalLM\n-\n->>> model = AutoModelForCausalLM.from_pretrained(\"distilbert/distilgpt2\")\n->>> model.generation_config\n-GenerationConfig {\n-  \"bos_token_id\": 50256,\n-  \"eos_token_id\": 50256\n-}\n-<BLANKLINE>\n+model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", torch_dtype=torch.float16).to(\"cuda\")\n+# explicitly set to default length because Llama2 generation length is 4096\n+outputs = model.generate(**inputs, max_new_tokens=20)\n+tokenizer.batch_decode(outputs, skip_special_tokens=True)\n+'Hugging Face is an open-source company that provides a suite of tools and services for building, deploying, and maintaining natural language processing'\n ```\n \n-Printing out the `model.generation_config` reveals only the values that are different from the default generation\n-configuration, and does not list any of the default values.\n-\n-The default generation configuration limits the size of the output combined with the input prompt to a maximum of 20\n-tokens to avoid running into resource limitations. The default decoding strategy is greedy search, which is the simplest decoding strategy that picks a token with the highest probability as the next token. For many tasks\n-and small output sizes this works well. However, when used to generate longer outputs, greedy search can start\n-producing highly repetitive results.\n-\n-## Customize text generation\n-\n-You can override any `generation_config` by passing the parameters and their values directly to the [`generate`] method:\n+## Contrastive search\n \n-```python\n->>> my_model.generate(**inputs, num_beams=4, do_sample=True)  # doctest: +SKIP\n-```\n+[Contrastive search](https://huggingface.co/papers/2202.06417) is a decoding strategy that aims to reduce repetition even while generating longer sequences. This strategy compares how similar a generated token is against previous tokens, and if they're more similar, a penalty is applied.\n \n-Even if the default decoding strategy mostly works for your task, you can still tweak a few things. Some of the\n-commonly adjusted parameters include:\n-\n-- `max_new_tokens`: the maximum number of tokens to generate. In other words, the size of the output sequence, not\n-including the tokens in the prompt. As an alternative to using the output's length as a stopping criteria, you can choose\n-to stop generation whenever the full generation exceeds some amount of time. To learn more, check [`StoppingCriteria`].\n-- `num_beams`: by specifying a number of beams higher than 1, you are effectively switching from greedy search to\n-beam search. This strategy evaluates several hypotheses at each time step and eventually chooses the hypothesis that\n-has the overall highest probability for the entire sequence. This has the advantage of identifying high-probability\n-sequences that start with a lower probability initial tokens and would've been ignored by the greedy search. Visualize how it works [here](https://huggingface.co/spaces/m-ric/beam_search_visualizer).\n-- `do_sample`: if set to `True`, this parameter enables decoding strategies such as multinomial sampling, beam-search\n-multinomial sampling, Top-K sampling and Top-p sampling. All these strategies select the next token from the probability\n-distribution over the entire vocabulary with various strategy-specific adjustments.\n-- `num_return_sequences`: the number of sequence candidates to return for each input. This option is only available for\n-the decoding strategies that support multiple sequence candidates, e.g. variations of beam search and sampling. Decoding\n-strategies like greedy search and contrastive search return a single output sequence.\n-\n-It is also possible to extend `generate()` with external libraries or handcrafted code. The `logits_processor` argument\n-allows you to pass custom [`LogitsProcessor`] instances, allowing you to manipulate the next token probability\n-distributions. Likewise, the `stopping_criteria` argument lets you set custom [`StoppingCriteria`] to stop text generation.\n-The [`logits-processor-zoo`](https://github.com/NVIDIA/logits-processor-zoo) library contains examples of external\n-`generate()`-compatible extensions.\n-\n-## Save a custom decoding strategy with your model\n-\n-If you would like to share your fine-tuned model with a specific generation configuration, you can:\n-* Create a [`GenerationConfig`] class instance\n-* Specify the decoding strategy parameters\n-* Save your generation configuration with [`GenerationConfig.save_pretrained`], making sure to leave its `config_file_name` argument empty\n-* Set `push_to_hub` to `True` to upload your config to the model's repo\n-\n-```python\n->>> from transformers import AutoModelForCausalLM, GenerationConfig\n+Enable contrastive search with the `penalty_alpha` and `top_k` parameters. The `penalty_alpha` manages the penalty applied and `top_k` is the number of most likely tokens to return.\n \n->>> model = AutoModelForCausalLM.from_pretrained(\"my_account/my_model\")  # doctest: +SKIP\n->>> generation_config = GenerationConfig(\n-...     max_new_tokens=50, do_sample=True, top_k=50, eos_token_id=model.config.eos_token_id\n-... )\n->>> generation_config.save_pretrained(\"my_account/my_model\", push_to_hub=True)  # doctest: +SKIP\n-```\n+```py\n+import torch\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n \n-You can also store several generation configurations in a single directory, making use of the `config_file_name`\n-argument in [`GenerationConfig.save_pretrained`]. You can later instantiate them with [`GenerationConfig.from_pretrained`]. This is useful if you want to\n-store several generation configurations for a single model (e.g. one for creative text generation with sampling, and\n-one for summarization with beam search). You must have the right Hub permissions to add configuration files to a model.\n+tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n+inputs = tokenizer(\"Hugging Face is an open-source company\", return_tensors=\"pt\").to(\"cuda\")\n \n-```python\n->>> from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig\n-\n->>> tokenizer = AutoTokenizer.from_pretrained(\"google-t5/t5-small\")\n->>> model = AutoModelForSeq2SeqLM.from_pretrained(\"google-t5/t5-small\")\n-\n->>> translation_generation_config = GenerationConfig(\n-...     num_beams=4,\n-...     early_stopping=True,\n-...     decoder_start_token_id=0,\n-...     eos_token_id=model.config.eos_token_id,\n-...     pad_token=model.config.pad_token_id,\n-... )\n-\n->>> # Tip: add `push_to_hub=True` to push to the Hub\n->>> translation_generation_config.save_pretrained(\"/tmp\", \"translation_generation_config.json\")\n-\n->>> # You could then use the named generation config file to parameterize generation\n->>> generation_config = GenerationConfig.from_pretrained(\"/tmp\", \"translation_generation_config.json\")\n->>> inputs = tokenizer(\"translate English to French: Configuration files are easy to use!\", return_tensors=\"pt\")\n->>> outputs = model.generate(**inputs, generation_config=generation_config)\n->>> print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n-['Les fichiers de configuration sont faciles Ã  utiliser!']\n+model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", torch_dtype=torch.float16).to(\"cuda\")\n+# explicitly set to 100 because Llama2 generation length is 4096\n+outputs = model.generate(**inputs, max_new_tokens=100, penalty_alpha=0.6, top_k=4)\n+tokenizer.batch_decode(outputs, skip_special_tokens=True)\n+'Hugging Face is an open-source company that provides a platform for building and deploying AI models.\\nHugging Face is an open-source company that provides a platform for building and deploying AI models. The platform allows developers to build and deploy AI models, as well as collaborate with other developers.\\nHugging Face was founded in 2019 by Thibault Wittemberg and ClÃ©ment Delangue. The company is based in Paris, France.\\nHugging Face has'\n ```\n \n-## Streaming\n-\n-The `generate()` supports streaming, through its `streamer` input. The `streamer` input is compatible with any instance\n-from a class that has the following methods: `put()` and `end()`. Internally, `put()` is used to push new tokens and\n-`end()` is used to flag the end of text generation.\n-\n-<Tip warning={true}>\n+## Beam search\n \n-The API for the streamer classes is still under development and may change in the future.\n+Beam search keeps track of several generated sequences (beams) at each time step. After a certain number of steps, it selects the sequence with the highest *overall* probability. Unlike greedy search, this strategy can \"look ahead\" and pick a sequence with a higher probability overall even if the initial tokens have a lower probability.\n \n-</Tip>\n+> [!TIP]\n+> Check out the [beam search visualizer](https://huggingface.co/spaces/m-ric/beam_search_visualizer) to see how beam search works.\n \n-In practice, you can craft your own streaming class for all sorts of purposes! We also have basic streaming classes\n-ready for you to use. For example, you can use the [`TextStreamer`] class to stream the output of `generate()` into\n-your screen, one word at a time:\n+Enable beam search with the `num_beams` parameter (should be greater than 1 otherwise it's equivalent to greedy search).\n \n-```python\n->>> from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n+```py\n+import torch\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n \n->>> tok = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n->>> model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n->>> inputs = tok([\"An increasing sequence: one,\"], return_tensors=\"pt\")\n->>> streamer = TextStreamer(tok)\n+tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n+inputs = tokenizer(\"Hugging Face is an open-source company\", return_tensors=\"pt\").to(\"cuda\")\n \n->>> # Despite returning the usual output, the streamer will also print the generated text to stdout.\n->>> _ = model.generate(**inputs, streamer=streamer, max_new_tokens=20)\n-An increasing sequence: one, two, three, four, five, six, seven, eight, nine, ten, eleven,\n+model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", torch_dtype=torch.float16).to(\"cuda\")\n+# explicitly set to 100 because Llama2 generation length is 4096\n+outputs = model.generate(**inputs, max_new_tokens=50, num_beams=2)\n+tokenizer.batch_decode(outputs, skip_special_tokens=True)\n+\"['Hugging Face is an open-source company that develops and maintains the Hugging Face platform, which is a collection of tools and libraries for building and deploying natural language processing (NLP) models. Hugging Face was founded in 2018 by Thomas Wolf']\"\n ```\n \n+## Diverse beam search\n \n-## Watermarking\n-\n-The `generate()` supports watermarking the generated text by randomly marking a portion of tokens as \"green\".\n-When generating the \"green\" will have a small 'bias' value added to their logits, thus having a higher chance to be generated.\n-The watermarked text can be detected by calculating the proportion of \"green\" tokens in the text and estimating how likely it is\n-statistically to obtain that amount of \"green\" tokens for human-generated text. This watermarking strategy was proposed in the paper\n-[\"On the Reliability of Watermarks for Large Language Models\"](https://arxiv.org/abs/2306.04634). For more information on\n-the inner functioning of watermarking, it is recommended to refer to the paper.\n-\n-The watermarking can be used with any generative model in `tranformers` and does not require an extra classification model\n-to detect watermarked text. To trigger watermarking, pass in a [`WatermarkingConfig`] with needed arguments directly to the\n-`.generate()` method or add it to the [`GenerationConfig`]. Watermarked text can be later detected with a [`WatermarkDetector`].\n-\n-\n-<Tip warning={true}>\n-\n-The WatermarkDetector internally relies on the proportion of \"green\" tokens, and whether generated text follows the coloring pattern.\n-That is why it is recommended to strip off the prompt text, if it is much longer than the generated text.\n-This also can have an effect when one sequence in the batch is a lot longer causing other rows to be padded.\n-Additionally, the detector **must** be initiated with identical watermark configuration arguments used when generating.\n-\n-</Tip>\n-\n-Let's generate some text with watermarking. In the below code snippet, we set the bias to 2.5 which is a value that\n-will be added to \"green\" tokens' logits. After generating watermarked text, we can pass it directly to the `WatermarkDetector`\n-to check if the text is machine-generated (outputs `True` for machine-generated and `False` otherwise).\n+[Diverse beam search](https://hf.co/papers/1610.02424) is a variant of beam search that produces more diverse output candidates to choose from. This strategy measures the dissimilarity of sequences and a penalty is applied if sequences are too similar. To avoid high computation costs, the number of beams is divided into groups.\n \n-```python\n->>> from transformers import AutoTokenizer, AutoModelForCausalLM, WatermarkDetector, WatermarkingConfig\n-\n->>> model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n->>> tok = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n->>> tok.pad_token_id = tok.eos_token_id\n->>> tok.padding_side = \"left\"\n+Enable diverse beam search with the `num_beams`, `num_beam_groups` and `diversity_penalty` parameters (the `num_beams` parameter should be divisible by `num_beam_groups`).\n \n->>> inputs = tok([\"This is the beginning of a long story\", \"Alice and Bob are\"], padding=True, return_tensors=\"pt\")\n->>> input_len = inputs[\"input_ids\"].shape[-1]\n+```py\n+import torch\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n \n->>> watermarking_config = WatermarkingConfig(bias=2.5, seeding_scheme=\"selfhash\")\n->>> out = model.generate(**inputs, watermarking_config=watermarking_config, do_sample=False, max_length=20)\n+tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n+inputs = tokenizer(\"Hugging Face is an open-source company\", return_tensors=\"pt\").to(\"cuda\")\n \n->>> detector = WatermarkDetector(model_config=model.config, device=\"cpu\", watermarking_config=watermarking_config)\n->>> detection_out = detector(out, return_dict=True)\n->>> detection_out.prediction\n-array([ True,  True])\n+model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", torch_dtype=torch.float16).to(\"cuda\")\n+# explicitly set to 100 because Llama2 generation length is 4096\n+outputs = model.generate(**inputs, max_new_tokens=50, num_beams=6, num_beam_groups=3, diversity_penalty=1.0, do_sample=False)\n+tokenizer.batch_decode(outputs, skip_special_tokens=True)\n+'Hugging Face is an open-source company ðŸ¤—\\nWe are an open-source company. Our mission is to democratize AI and make it accessible to everyone. We believe that AI should be used for the benefit of humanity, not for the benefit of a'\n ```\n \n+## Multinomial sampling\n \n-## Decoding strategies\n+Search methods selects the most likely tokens. Sampling, or multinomial sampling, randomly selects a token based on the probability distribution over the entire models vocabulary. This means every token with a non-zero probability has a chance to be selected. Sampling strategies reduce repetition and can generate more creative and diverse outputs.\n \n-Certain combinations of the `generate()` parameters, and ultimately `generation_config`, can be used to enable specific\n-decoding strategies. If you are new to this concept, we recommend reading\n-[this blog post that illustrates how common decoding strategies work](https://huggingface.co/blog/how-to-generate).\n+Enable multinomial sampling with `do_sample=True` and `num_beams=1`.\n \n-Here, we'll show some of the parameters that control the decoding strategies and illustrate how you can use them.\n+```py\n+import torch\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n \n-<Tip>\n+tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n+inputs = tokenizer(\"Hugging Face is an open-source company\", return_tensors=\"pt\").to(\"cuda\")\n \n-Selecting a given decoding strategy is not the only way you can influence the outcome of `generate()` with your model.\n-The decoding strategies act based (mostly) on the logits, the distribution of probabilities for the next token, and\n-thus selecting a good logits manipulation strategy can go a long way! In other words, manipulating the logits is another\n-dimension you can act upon, in addition to selecting a decoding strategy. Popular logits manipulation strategies include\n-`top_p`, `min_p`, and `repetition_penalty` -- you can check the full list in the [`GenerationConfig`] class.\n-\n-</Tip>\n+model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", torch_dtype=torch.float16).to(\"cuda\")\n+# explicitly set to 100 because Llama2 generation length is 4096\n+outputs = model.generate(**inputs, max_new_tokens=50, do_sample=True, num_beams=1)\n+tokenizer.batch_decode(outputs, skip_special_tokens=True)\n+'Hugging Face is an open-source company ðŸ¤—\\nWe are open-source and believe that open-source is the best way to build technology. Our mission is to make AI accessible to everyone, and we believe that open-source is the best way to achieve that.'\n+```\n \n-### Greedy Search\n+## Beam search multinomial sampling\n \n-[`generate`] uses greedy search decoding by default so you don't have to pass any parameters to enable it. This means the parameters `num_beams` is set to 1 and `do_sample=False`.\n+This decoding strategy is a combination of beam search and multinomial sampling. It generates multiple beams and uses a sampling strategy for each beam.\n \n-```python\n->>> from transformers import AutoModelForCausalLM, AutoTokenizer\n+Enable beam search multinomial sampling by setting `num_beams` to a value greater than 1 and `do_sample=True`.\n \n->>> prompt = \"I look forward to\"\n->>> checkpoint = \"distilbert/distilgpt2\"\n+```py\n+import torch\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n \n->>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n->>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n+tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n+inputs = tokenizer(\"Hugging Face is an open-source company\", return_tensors=\"pt\").to(\"cuda\")\n \n->>> model = AutoModelForCausalLM.from_pretrained(checkpoint)\n->>> outputs = model.generate(**inputs)\n->>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n-['I look forward to seeing you all again!\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n']\n+model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", torch_dtype=torch.float16).to(\"cuda\")\n+# explicitly set to 100 because Llama2 generation length is 4096\n+outputs = model.generate(**inputs, max_new_tokens=50, do_sample=True, num_beams=4)\n+'Hugging Face is an open-source company 100% dedicated to making AI more accessible. We believe that AI should be available to everyone, and weâ€™re working hard to make that a reality.\\nWeâ€™re a team of passionate engineers, designers,'\n ```\n \n-### Contrastive search\n+## Speculative decoding\n \n-The contrastive search decoding strategy was proposed in the 2022 paper [A Contrastive Framework for Neural Text Generation](https://arxiv.org/abs/2202.06417).\n-It demonstrates superior results for generating non-repetitive yet coherent long outputs. To learn how contrastive search\n-works, check out [this blog post](https://huggingface.co/blog/introducing-csearch).\n-The two main parameters that enable and control the behavior of contrastive search are `penalty_alpha` and `top_k`:\n+[Speculative](https://hf.co/papers/2211.17192) or assistive decoding isn't a search or sampling strategy. Instead, speculative decoding adds a second smaller model to generate candidate tokens. The main model verifies the candidate tokens in a single `forward` pass, which speeds up the decoding process overall. This method is especially useful for LLMs where it can be more costly and slower to generate tokens. Refer to the [speculative decoding](./llm_optims#speculative-decoding) guide to learn more.\n \n-```python\n->>> from transformers import AutoTokenizer, AutoModelForCausalLM\n+Currently, only greedy search and multinomial sampling are supported with speculative decoding. Batched inputs aren't supported either.\n \n->>> checkpoint = \"openai-community/gpt2-large\"\n->>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n->>> model = AutoModelForCausalLM.from_pretrained(checkpoint)\n+Enable speculative decoding with the `assistant_model` parameter. You'll notice the fastest speed up with an assistant model that is much smaller than the main model. Add `do_sample=True` to enable token validation with resampling.\n \n->>> prompt = \"Hugging Face Company is\"\n->>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n+<hfoptions id=\"spec-decoding\">\n+<hfoption id=\"greedy search\">\n \n->>> outputs = model.generate(**inputs, penalty_alpha=0.6, top_k=4, max_new_tokens=100)\n->>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n-['Hugging Face Company is a family owned and operated business. We pride ourselves on being the best\n-in the business and our customer service is second to none.\\n\\nIf you have any questions about our\n-products or services, feel free to contact us at any time. We look forward to hearing from you!']\n-```\n+```py\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n \n-### Multinomial sampling\n+tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM-1.7B\")\n+model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM-1.7B\")\n+assistant_model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM-135M\")\n+inputs = tokenizer(\"Hugging Face is an open-source company\", return_tensors=\"pt\")\n \n-As opposed to greedy search that always chooses a token with the highest probability as the\n-next token, multinomial sampling (also called ancestral sampling) randomly selects the next token based on the probability distribution over the entire\n-vocabulary given by the model. Every token with a non-zero probability has a chance of being selected, thus reducing the\n-risk of repetition.\n+outputs = model.generate(**inputs, assistant_model=assistant_model)\n+tokenizer.batch_decode(outputs, skip_special_tokens=True)\n+'Hugging Face is an open-source company that provides a platform for developers to build and deploy machine'\n+```\n \n-To enable multinomial sampling set `do_sample=True` and `num_beams=1`.\n+Speculative decoding is also supported in [`Pipeline`] with the `assistant_model` parameter.\n \n ```python\n->>> from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n->>> set_seed(0)  # For reproducibility\n-\n->>> checkpoint = \"openai-community/gpt2-large\"\n->>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n->>> model = AutoModelForCausalLM.from_pretrained(checkpoint)\n-\n->>> prompt = \"Today was an amazing day because\"\n->>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n-\n->>> outputs = model.generate(**inputs, do_sample=True, num_beams=1, max_new_tokens=100)\n->>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n-[\"Today was an amazing day because we received these wonderful items by the way of a gift shop. The box arrived on a Thursday and I opened it on Monday afternoon to receive the gifts. Both bags featured pieces from all the previous years!\\n\\nThe box had lots of surprises in it, including some sweet little mini chocolate chips! I don't think I'd eat all of these. This was definitely one of the most expensive presents I have ever got, I actually got most of them for free!\\n\\nThe first package came\"]\n+from transformers import pipeline\n+import torch\n+\n+pipe = pipeline(\n+    \"text-generation\",\n+    model=\"meta-llama/Llama-3.1-8B\",\n+    assistant_model=\"meta-llama/Llama-3.2-1B\",\n+    torch_dtype=torch.bfloat16\n+)\n+pipe_output = pipe(\"Once upon a time, \", max_new_tokens=50, do_sample=False)\n+pipe_output[0][\"generated_text\"]\n ```\n \n-### Beam-search decoding\n-\n-Unlike greedy search, beam-search decoding keeps several hypotheses at each time step and eventually chooses\n-the hypothesis that has the overall highest probability for the entire sequence. This has the advantage of identifying high-probability\n-sequences that start with lower probability initial tokens and would've been ignored by the greedy search.\n-\n-<a href=\"https://huggingface.co/spaces/m-ric/beam_search_visualizer\" class=\"flex flex-col justify-center\">\n-    <img style=\"max-width: 90%; margin: auto;\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/beam_search.png\"/>\n-</a>\n-\n-You can visualize how beam-search decoding works in [this interactive demo](https://huggingface.co/spaces/m-ric/beam_search_visualizer): type your input sentence, and play with the parameters to see how the decoding beams change.\n-\n-To enable this decoding strategy, specify the `num_beams` (aka number of hypotheses to keep track of) that is greater than 1.\n-\n-```python\n->>> from transformers import AutoModelForCausalLM, AutoTokenizer\n+</hfoption>\n+<hfoption id=\"multinomial sampling\">\n \n->>> prompt = \"It is astonishing how one can\"\n->>> checkpoint = \"openai-community/gpt2-medium\"\n+Add the `temperature` parameter to control sampling randomness. For speculative decoding, a lower temperature may improve latency.\n \n->>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n->>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n+```py\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n \n->>> model = AutoModelForCausalLM.from_pretrained(checkpoint)\n+tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM-1.7B\")\n+model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM-1.7B\")\n+assistant_model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM-135M\")\n+inputs = tokenizer(\"Hugging Face is an open-source company\", return_tensors=\"pt\")\n \n->>> outputs = model.generate(**inputs, num_beams=5, max_new_tokens=50)\n->>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n-['It is astonishing how one can have such a profound impact on the lives of so many people in such a short period of\n-time.\"\\n\\nHe added: \"I am very proud of the work I have been able to do in the last few years.\\n\\n\"I have']\n+outputs = model.generate(**inputs, assistant_model=assistant_model, do_sample=True, temperature=0.5)\n+tokenizer.batch_decode(outputs, skip_special_tokens=True)\n+'Hugging Face is an open-source company that is dedicated to creating a better world through technology.'\n ```\n \n-### Beam-search multinomial sampling\n-\n-As the name implies, this decoding strategy combines beam search with multinomial sampling. You need to specify\n-the `num_beams` greater than 1, and set `do_sample=True` to use this decoding strategy.\n-\n-```python\n->>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, set_seed\n->>> set_seed(0)  # For reproducibility\n-\n->>> prompt = \"translate English to German: The house is wonderful.\"\n->>> checkpoint = \"google-t5/t5-small\"\n-\n->>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n->>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n+</hfoption>\n+</hfoptions>\n \n->>> model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n+### Prompt lookup decoding\n \n->>> outputs = model.generate(**inputs, num_beams=5, do_sample=True)\n->>> tokenizer.decode(outputs[0], skip_special_tokens=True)\n-'Das Haus ist wunderbar.'\n-```\n+[Prompt lookup decoding](./llm_optims#prompt-lookup-decoding) is a variant of speculative decoding that uses overlapping n-grams as the candidate tokens. It works well for input-grounded tasks such as summarization. Refer to the [prompt lookup decoding](./llm_optims#prompt-lookup-decoding) guide to learn more.\n \n-### Diverse beam search decoding\n+Enable prompt lookup decoding with the `prompt_lookup_num_tokens` parameter.\n \n-The diverse beam search decoding strategy is an extension of the beam search strategy that allows for generating a more diverse\n-set of beam sequences to choose from. To learn how it works, refer to [Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models](https://arxiv.org/pdf/1610.02424.pdf).\n-This approach has three main parameters: `num_beams`, `num_beam_groups`, and `diversity_penalty`.\n-The diversity penalty ensures the outputs are distinct across groups, and beam search is used within each group.\n+```py\n+import torch\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n \n+tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM-1.7B\")\n+model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM-1.7B\", torch_dtype=torch.float16).to(\"cuda\")\n+assistant_model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM-135M\", torch_dtype=torch.float16).to(\"cuda\")\n+inputs = tokenizer(\"Hugging Face is an open-source company\", return_tensors=\"pt\").to(\"cuda\")\n \n-```python\n->>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n-\n->>> checkpoint = \"google/pegasus-xsum\"\n->>> prompt = (\n-...     \"The Permaculture Design Principles are a set of universal design principles \"\n-...     \"that can be applied to any location, climate and culture, and they allow us to design \"\n-...     \"the most efficient and sustainable human habitation and food production systems. \"\n-...     \"Permaculture is a design system that encompasses a wide variety of disciplines, such \"\n-...     \"as ecology, landscape design, environmental science and energy conservation, and the \"\n-...     \"Permaculture design principles are drawn from these various disciplines. Each individual \"\n-...     \"design principle itself embodies a complete conceptual framework based on sound \"\n-...     \"scientific principles. When we bring all these separate  principles together, we can \"\n-...     \"create a design system that both looks at whole systems, the parts that these systems \"\n-...     \"consist of, and how those parts interact with each other to create a complex, dynamic, \"\n-...     \"living system. Each design principle serves as a tool that allows us to integrate all \"\n-...     \"the separate parts of a design, referred to as elements, into a functional, synergistic, \"\n-...     \"whole system, where the elements harmoniously interact and work together in the most \"\n-...     \"efficient way possible.\"\n-... )\n-\n->>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n->>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n-\n->>> model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n-\n->>> outputs = model.generate(**inputs, num_beams=5, num_beam_groups=5, max_new_tokens=30, diversity_penalty=1.0)\n->>> tokenizer.decode(outputs[0], skip_special_tokens=True)\n-'The Design Principles are a set of universal design principles that can be applied to any location, climate and\n-culture, and they allow us to design the'\n+outputs = model.generate(**inputs, assistant_model=assistant_model, max_new_tokens=20, prompt_lookup_num_tokens=5)\n+tokenizer.batch_decode(outputs, skip_special_tokens=True)\n+'Hugging Face is an open-source company that provides a platform for developers to build and deploy machine learning models. It offers a variety of tools'\n ```\n \n-This guide illustrates the main parameters that enable various decoding strategies. More advanced parameters exist for the\n-[`generate`] method, which gives you even further control over the [`generate`] method's behavior.\n-For the complete list of the available parameters, refer to the [API documentation](./main_classes/text_generation).\n+### Self-speculative decoding\n \n-### Speculative Decoding\n+Early exiting uses the earlier hidden states from the language modeling head as inputs, effectively skipping layers to yield a lower quality output. The lower quality output is used as the assistant output and self-speculation is applied to fix the output using the remaining layers. The final generated result from this self-speculative method is the same (or has the same distribution) as the original models generation.\n \n-Speculative decoding (also known as assisted decoding) is a modification of the decoding strategies above, that uses an\n-assistant model (ideally a much smaller one), to generate a few candidate tokens. The main model then validates the candidate\n-tokens in a single forward pass, which speeds up the decoding process. If `do_sample=True`, then the token validation with\n-resampling introduced in the [speculative decoding paper](https://arxiv.org/pdf/2211.17192.pdf) is used.\n-Assisted decoding assumes the main and assistant models have the same tokenizer, otherwise, see Universal Assisted Decoding below.\n+The assistant model is also part of the target model, so the caches and weights can be shared, resulting in lower memory requirements.\n \n-Currently, only greedy search and sampling are supported with assisted decoding, and assisted decoding doesn't support batched inputs.\n-To learn more about assisted decoding, check [this blog post](https://huggingface.co/blog/assisted-generation).\n+For a model trained with early exit, pass `assistant_early_exit` to [`~GenerationMixin.generate`].\n \n-To enable assisted decoding, set the `assistant_model` argument with a model.\n+```py\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n \n-```python\n->>> from transformers import AutoModelForCausalLM, AutoTokenizer\n-\n->>> prompt = \"Alice and Bob\"\n->>> checkpoint = \"EleutherAI/pythia-1.4b-deduped\"\n->>> assistant_checkpoint = \"EleutherAI/pythia-160m-deduped\"\n+prompt = \"Alice and Bob\"\n+checkpoint = \"facebook/layerskip-llama3.2-1B\"\n \n->>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n->>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n+tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n+inputs = tokenizer(prompt, return_tensors=\"pt\")\n \n->>> model = AutoModelForCausalLM.from_pretrained(checkpoint)\n->>> assistant_model = AutoModelForCausalLM.from_pretrained(assistant_checkpoint)\n->>> outputs = model.generate(**inputs, assistant_model=assistant_model)\n->>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n-['Alice and Bob are sitting in a bar. Alice is drinking a beer and Bob is drinking a glass of wine.']\n+model = AutoModelForCausalLM.from_pretrained(checkpoint)\n+outputs = model.generate(**inputs, assistant_early_exit=4, do_sample=False, max_new_tokens=20)\n+tokenizer.batch_decode(outputs, skip_special_tokens=True)\n ```\n \n-<Tip>\n-\n-If you're using a `pipeline` object, all you need to do is to pass the assistant checkpoint under `assistant_model`\n-\n-```python\n->>> from transformers import pipeline\n->>> import torch\n-\n->>> pipe = pipeline(\n-...     \"text-generation\",\n-...     model=\"meta-llama/Llama-3.1-8B\",\n-...     assistant_model=\"meta-llama/Llama-3.2-1B\",  # This extra line is all that's needed, also works with UAD\n-...     torch_dtype=torch.bfloat16\n-... )\n->>> pipe_output = pipe(\"Once upon a time, \", max_new_tokens=50, do_sample=False)\n->>> pipe_output[0][\"generated_text\"]\n-'Once upon a time, 3D printing was a niche technology that was only'\n-```\n+### Universal assisted decoding\n \n-</Tip>\n+Universal assisted decoding (UAD) enables the main and assistant models to use different tokenizers. The main models input tokens are re-encoded into assistant model tokens. Candidate tokens are generated in the assistant encoding which are re-encoded into the main model candidate tokens. The candidate tokens are verified as explained in [speculative decoding](#speculative-decoding).\n \n+Re-encoding involves decoding token ids into text and encoding the text with a different tokenizer. To prevent tokenization discrepancies during re-encoding, UAD finds the longest common sub-sequence between the source and target encodings to ensure the new tokens include the correct prompt suffix.\n \n-When using assisted decoding with sampling methods, you can use the `temperature` argument to control the randomness,\n-just like in multinomial sampling. However, in assisted decoding, reducing the temperature may help improve the latency.\n+Add the `tokenizer` and `assistant_tokenizer` parameters to [`~GenerationMixin.generate`] to enable UAD.\n \n-```python\n->>> from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\n->>> set_seed(42)  # For reproducibility\n+```py\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n \n->>> prompt = \"Alice and Bob\"\n->>> checkpoint = \"EleutherAI/pythia-1.4b-deduped\"\n->>> assistant_checkpoint = \"EleutherAI/pythia-160m-deduped\"\n+prompt = \"Alice and Bob\"\n \n->>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n->>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n+assistant_tokenizer = AutoTokenizer.from_pretrained(\"double7/vicuna-68m\")\n+tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-9b\")\n+inputs = tokenizer(prompt, return_tensors=\"pt\")\n \n->>> model = AutoModelForCausalLM.from_pretrained(checkpoint)\n->>> assistant_model = AutoModelForCausalLM.from_pretrained(assistant_checkpoint)\n->>> outputs = model.generate(**inputs, assistant_model=assistant_model, do_sample=True, temperature=0.5)\n->>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n-['Alice and Bob are two people who are very different, but they are both very good at what they do. Alice']\n+model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-9b\")\n+assistant_model = AutoModelForCausalLM.from_pretrained(\"double7/vicuna-68m\")\n+outputs = model.generate(**inputs, assistant_model=assistant_model, tokenizer=tokenizer, assistant_tokenizer=assistant_tokenizer)\n+tokenizer.batch_decode(outputs, skip_special_tokens=True)\n+['Alice and Bob are sitting in a bar. Alice is drinking a beer and Bob is drinking a']\n ```\n \n-We recommend to install `scikit-learn` library to enhance the candidate generation strategy and achieve additional speedup.\n-\n-#### Universal Assisted Decoding\n+## DoLa\n \n-Universal Assisted Decoding (UAD) adds support for main and assistant models with different tokenizers.\n-To use it, simply pass the tokenizers using the `tokenizer` and `assistant_tokenizer` arguments (see below).\n-Internally, the main model input tokens are re-encoded into assistant model tokens, then candidate tokens are generated in the assistant encoding, which are\n-in turn re-encoded into main model candidate tokens. Validation then proceeds as explained above.\n-The re-encoding steps involve decoding token ids into text and then encoding the text using a different tokenizer.\n-Since re-encoding the tokens may result in tokenization discrepancies, UAD finds the longest common subsequence between the source and target encodings,\n-to ensure the new tokens include the correct prompt suffix.\n+[Decoding by Contrasting Layers (DoLa)](https://hf.co/papers/2309.03883) is a contrastive decoding strategy for improving factuality and reducing hallucination. This strategy works by contrasting the logit diffferences between the final and early layers. As a result, factual knowledge localized to particular layers are amplified. DoLa is not recommended for smaller models like GPT-2.\n \n-```python\n->>> from transformers import AutoModelForCausalLM, AutoTokenizer\n-\n->>> prompt = \"Alice and Bob\"\n->>> checkpoint = \"google/gemma-2-9b\"\n->>> assistant_checkpoint = \"double7/vicuna-68m\"\n+Enable DoLa with the following parameters.\n \n->>> assistant_tokenizer = AutoTokenizer.from_pretrained(assistant_checkpoint)\n->>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n->>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n-\n->>> model = AutoModelForCausalLM.from_pretrained(checkpoint)\n->>> assistant_model = AutoModelForCausalLM.from_pretrained(assistant_checkpoint)\n->>> outputs = model.generate(**inputs, assistant_model=assistant_model, tokenizer=tokenizer, assistant_tokenizer=assistant_tokenizer)\n->>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n-['Alice and Bob are playing a game. Alice has a set of $n$ integers $a_1, a']\n-```\n+- `dola_layers` are the candidate layers to be contrasted with the final layer. It can be a string (`low` or `high`) to contrast the lower or higher parts of a layer. `high` is recommended for short-answer tasks like TruthfulQA. `low` is recommended for long-answer reasoning tasks like GSM8K, StrategyQA, FACTOR, and VicunaQA.\n \n-#### Prompt Lookup\n+  When a model has tied word embeddings, layer 0 is skipped and it begins from layer 2.\n \n-Alternatively, you can also set the `prompt_lookup_num_tokens` to trigger n-gram based assisted decoding, as opposed\n-to model based assisted decoding. You can read more about it [here](https://twitter.com/joao_gante/status/1747322413006643259).\n+  It can also be a list of integers that represent the layer indices between 0 and the total number of layers. Layer 0 is the word embedding, 1 is the first transformer layer, and so on. Refer to the table below for the range of layer indices depending on the number of model layers.\n \n-#### Self-Speculative Decoding\n+  | layers | low | high |\n+  |---|---|---|\n+  | > 40 | (0, 20, 2) | (N - 20, N, 2) |\n+  | <= 40 | range(0, N // 2, 2) | range(N // 2, N, 2) |\n \n-An LLM can be trained to also use its language modeling head with earlier hidden states as input, effectively\n-skipping layers to yield a lower-quality output -- a technique called early exiting.\n-We use the lower-quality early exit output as an assistant output, and apply self-speculation to fix the output using the remaining layers. The final generation of that self-speculative solution is the same (or has the same distribution) as the original model's generation.\n-If the model you're using was trained to do early exit, you can pass\n-`assistant_early_exit` (integer). In this case, the assistant model will be the same model but exiting early, hence the\n-\"self-speculative\" name. Because the assistant model is a portion of the target model, caches and weights can be shared, which results in lower memory requirements. As in other assisted generation methods, the final generated result has the same quality as if no assistant had been used.\n+- `repetition_penalty` reduces repetition and it is recommended to set it to 1.2.\n \n-```python\n->>> from transformers import AutoModelForCausalLM, AutoTokenizer\n+<hfoptions id=\"dola\">\n+<hfoption id=\"contrast higher layers\">\n \n->>> prompt = \"Alice and Bob\"\n->>> checkpoint = \"facebook/layerskip-llama3.2-1B\"\n+```py\n+import torch\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n \n->>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n->>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n+tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM-1.7B\")\n+model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM-1.7B\", torch_dtype=torch.float16).to(\"cuda\")\n+inputs = tokenizer(\"What is the highest peak in the world??\", return_tensors=\"pt\").to(\"cuda\")\n \n->>> model = AutoModelForCausalLM.from_pretrained(checkpoint)\n->>> outputs = model.generate(**inputs, assistant_early_exit=4, do_sample=False, max_new_tokens=20)\n->>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n-['Alice and Bob are playing a game. Alice has a set of $n$ integers $a_1, a']\n+outputs = model.generate(**inputs, max_new_tokens=50, dola_layers=\"high\", do_sample=False)\n+tokenizer.batch_decode(outputs, skip_special_tokens=True)\n+\" Mount EverestMount Everest, called Himalaya in Nepali, is the world's highest peak, lying almost 9.5 kilometers above the sea level and the tallest mountain from 19,036.91 ft. The mountain was\"\n ```\n \n-### DoLa Decoding\n-\n-**D**ecoding by C**o**ntrasting **La**yers (DoLa) is a contrastive decoding strategy to improve the factuality and reduce the\n-hallucinations of LLMs, as described in this paper of ICLR 2024 [DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models](https://arxiv.org/abs/2309.03883).\n+</hfoption>\n+<hfoption id=\"contrast specific layers\">\n \n-DoLa is achieved by contrasting the differences in logits obtained from final\n-layers versus earlier layers, thus amplify the factual knowledge localized to particular part of transformer layers.\n+Contrast layers 18 and 20 with the final layer.\n \n-Do the following two steps to activate DoLa decoding when calling the `model.generate` function:\n-1. Set the `dola_layers` argument, which can be either a string or a list of integers.\n-    - If set to a string, it can be one of `low`, `high`.\n-    - If set to a list of integers, it should be a list of layer indices between 0 and the total number of layers in the model. The 0-th layer is word embedding, and the 1st layer is the first transformer layer, and so on.\n-2. Set `repetition_penalty = 1.2` is suggested to reduce repetition in DoLa decoding.\n+```py\n+import torch\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n \n-See the following examples for DoLa decoding with the 32-layer LLaMA-7B model.\n+tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM-1.7B\")\n+model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM-1.7B\", torch_dtype=torch.float16).to(\"cuda\")\n+inputs = tokenizer(\"What is the highest peak in the world?\", return_tensors=\"pt\").to(\"cuda\")\n \n-```python\n->>> from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n->>> import torch\n->>> from accelerate.test_utils.testing import get_backend\n-\n->>> device, _, _ = get_backend() # automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)\n->>> tokenizer = AutoTokenizer.from_pretrained(\"huggyllama/llama-7b\")\n->>> model = AutoModelForCausalLM.from_pretrained(\"huggyllama/llama-7b\", torch_dtype=torch.float16).to(device)\n->>> set_seed(42)\n-\n->>> text = \"On what date was the Declaration of Independence officially signed?\"\n->>> inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n-\n-# Vanilla greddy decoding\n->>> vanilla_output = model.generate(**inputs, do_sample=False, max_new_tokens=50)\n->>> tokenizer.batch_decode(vanilla_output[:, inputs.input_ids.shape[-1]:], skip_special_tokens=True)\n-['\\nThe Declaration of Independence was signed on July 4, 1776.\\nWhat was the date of the signing of the Declaration of Independence?\\nThe Declaration of Independence was signed on July 4,']\n-\n-# DoLa decoding with contrasting higher part of layers (layers 16,18,...,30)\n->>> dola_high_output = model.generate(**inputs, do_sample=False, max_new_tokens=50, dola_layers='high')\n->>> tokenizer.batch_decode(dola_high_output[:, inputs.input_ids.shape[-1]:], skip_special_tokens=True)\n-['\\nJuly 4, 1776, when the Continental Congress voted to separate from Great Britain. The 56 delegates to the Continental Congress signed the Declaration on August 2, 1776.']\n-\n-# DoLa decoding with contrasting specific layers (layers 28 and 30)\n->>> dola_custom_output = model.generate(**inputs, do_sample=False, max_new_tokens=50, dola_layers=[28,30], repetition_penalty=1.2)\n->>> tokenizer.batch_decode(dola_custom_output[:, inputs.input_ids.shape[-1]:], skip_special_tokens=True)\n-['\\nIn 1891, when he was 54 years old, John Jacob Astor founded his empire. He opened a one-man business and spent the next 27 years working 10-hour days. When']\n+outputs = model.generate(**inputs, max_new_tokens=50, dola_layers=[18,20], do_sample=False, repetition_penalty=1.2)\n+tokenizer.batch_decode(outputs[:, inputs.input_ids.shape[-1]:], skip_special_tokens=True)\n+\" Mount EverestMount Everest, called Himalaya in Nepali, is the world's highest peak above sea level and it rises to an incredible height of 29,028 feet above the ocean. Its summit is over a mile taller than Mt\"\n ```\n \n-#### Understanding the `dola_layers` argument\n-\n-`dola_layers` stands for the candidate layers in premature layer selection, as described in the DoLa paper. The selected premature layer will be contrasted with the final layer.\n+</hfoption>\n+</hfoptions>\n \n-Setting `dola_layers` to `'low'` or `'high'` will select the lower or higher part of the layers to contrast, respectively.\n-- For `N`-layer models with `N <= 40` layers, the layers of `range(0, N // 2, 2)` and `range(N // 2, N, 2)` are used for `'low'` and `'high'` layers, respectively.\n-- For models with `N > 40` layers, the layers of `range(0, 20, 2)` and `range(N - 20, N, 2)` are used for `'low'` and `'high'` layers, respectively.\n-- If the model has tied word embeddings, we skip the word embeddings (0-th) layer and start from the 2nd layer, as the early exit from word embeddings will become identity function.\n-- Set the `dola_layers` to a list of integers for layer indices to contrast manually specified layers. For example, setting `dola_layers=[28,30]` will contrast the final layer (32-th layer) with the 28-th and 30-th layers.\n+## Resources\n \n-The paper suggested that contrasting `'high'` layers to improve short-answer tasks like TruthfulQA, and contrasting `'low'` layers to improve all the other long-answer reasoning tasks, such as GSM8K, StrategyQA, FACTOR, and VicunaQA. Applying DoLa to smaller models like GPT-2 is not recommended, as the results shown in the Appendix N of the paper.\n+Read the [How to generate text: using different decoding methods for language generation with Transformers](https://huggingface.co/blog/how-to-generate) blog post for an explanation of how common decoding strategies work."
        },
        {
            "sha": "f6481c2abbe6ba5caf55e3f6021d4c8313dbda60",
            "filename": "docs/source/en/gguf.md",
            "status": "modified",
            "additions": 19,
            "deletions": 90,
            "changes": 109,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fgguf.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fgguf.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fgguf.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -14,87 +14,22 @@ rendered properly in your Markdown viewer.\n \n -->\n \n-# GGUF and interaction with Transformers\n-\n-The GGUF file format is used to store models for inference with [GGML](https://github.com/ggerganov/ggml) and other \n-libraries that depend on it, like the very popular [llama.cpp](https://github.com/ggerganov/llama.cpp) or \n-[whisper.cpp](https://github.com/ggerganov/whisper.cpp).\n-\n-It is a file format [supported by the Hugging Face Hub](https://huggingface.co/docs/hub/en/gguf) with features \n-allowing for quick inspection of tensors and metadata within the file.\n-\n-This file format is designed as a \"single-file-format\" where a single file usually contains both the configuration\n-attributes, the tokenizer vocabulary and other attributes, as well as all tensors to be loaded in the model. These\n-files come in different formats according to the quantization type of the file. We briefly go over some of them\n-[here](https://huggingface.co/docs/hub/en/gguf#quantization-types).\n-\n-## Support within Transformers\n-\n-We have added the ability to load `gguf` files within `transformers` in order to offer further training/fine-tuning\n-capabilities to gguf models, before converting back those models to `gguf` to use within the `ggml` ecosystem. When\n-loading a model, we first dequantize it to fp32, before loading the weights to be used in PyTorch.\n-\n-> [!NOTE]\n-> The support is still very exploratory and we welcome contributions in order to solidify it across quantization types\n-> and model architectures.\n-\n-For now, here are the supported model architectures and quantization types:\n-\n-### Supported quantization types\n-\n-The initial supported quantization types are decided according to the popular quantized files that have been shared\n-on the Hub.\n-\n-- F32\n-- F16\n-- BF16\n-- Q4_0\n-- Q4_1\n-- Q5_0\n-- Q5_1\n-- Q8_0\n-- Q2_K\n-- Q3_K\n-- Q4_K\n-- Q5_K\n-- Q6_K\n-- IQ1_S\n-- IQ1_M\n-- IQ2_XXS\n-- IQ2_XS\n-- IQ2_S\n-- IQ3_XXS\n-- IQ3_S\n-- IQ4_XS\n-- IQ4_NL\n-\n-> [!NOTE]\n-> To support gguf dequantization, `gguf>=0.10.0` installation is required.\n-\n-### Supported model architectures\n-\n-For now the supported model architectures are the architectures that have been very popular on the Hub, namely:\n-\n-- LLaMa\n-- Mistral\n-- Qwen2\n-- Qwen2Moe\n-- Phi3\n-- Bloom\n-- Falcon\n-- StableLM\n-- GPT2\n-- Starcoder2\n-- T5\n-- Mamba\n-- Nemotron\n-- Gemma2\n-\n-## Example usage\n-\n-In order to load `gguf` files in `transformers`, you should specify the `gguf_file` argument to the `from_pretrained`\n-methods of both tokenizers and models. Here is how one would load a tokenizer and a model, which can be loaded\n-from the exact same file:\n+# GGUF\n+\n+[GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) is a file format used to store models for inference with [GGML](https://github.com/ggerganov/ggml), a fast and lightweight inference framework written in C and C++. GGUF is a single-file format containing the model metadata and tensors.\n+\n+<div class=\"flex justify-center\">\n+    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/gguf-spec.png\"/>\n+</div>\n+\n+The GGUF format also supports many quantized data types (refer to [quantization type table](https://hf.co/docs/hub/en/gguf#quantization-types) for a complete list of supported quantization types) which saves a significant amount of memory, making inference with large models like Whisper and Llama feasible on local and edge devices.\n+\n+Transformers supports loading models stored in the GGUF format for further training or finetuning. The GGUF format is dequantized to fp32 where the full model weights are available and compatible with PyTorch.\n+\n+> [!TIP]\n+> Models that support GGUF include Llama, Mistral, Qwen2, Qwen2Moe, Phi3, Bloom, Falcon, StableLM, GPT2, and Starcoder2.\n+\n+Add the `gguf_file` parameter to [`~PreTrainedModel.from_pretrained`] to specify the GGUF file to load.\n \n ```py\n from transformers import AutoTokenizer, AutoModelForCausalLM\n@@ -106,17 +41,11 @@ tokenizer = AutoTokenizer.from_pretrained(model_id, gguf_file=filename)\n model = AutoModelForCausalLM.from_pretrained(model_id, gguf_file=filename)\n ```\n \n-Now you have access to the full, unquantized version of the model in the PyTorch ecosystem, where you can combine it\n-with a plethora of other tools.\n-\n-In order to convert back to a `gguf` file, we recommend using the \n-[`convert-hf-to-gguf.py` file](https://github.com/ggerganov/llama.cpp/blob/master/convert_hf_to_gguf.py) from llama.cpp.\n-\n-Here's how you would complete the script above to save the model and export it back to `gguf`:\n+Once you're done tinkering with the model, save and convert it back to the GGUF format with the [convert-hf-to-gguf.py](https://github.com/ggerganov/llama.cpp/blob/master/convert_hf_to_gguf.py) script.\n \n ```py\n-tokenizer.save_pretrained('directory')\n-model.save_pretrained('directory')\n+tokenizer.save_pretrained(\"directory\")\n+model.save_pretrained(\"directory\")\n \n !python ${path_to_llama_cpp}/convert-hf-to-gguf.py ${directory}\n ```"
        },
        {
            "sha": "749fcf3c2dda16800cf6a43558fa9863f8c5ec66",
            "filename": "docs/source/en/gpu_selection.md",
            "status": "added",
            "additions": 94,
            "deletions": 0,
            "changes": 94,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fgpu_selection.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fgpu_selection.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fgpu_selection.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -0,0 +1,94 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# GPU selection\n+\n+During distributed training, you can specify the number of GPUs to use and in what order. This can be useful when you have GPUs with different computing power and you want to use the faster GPU first. Or you could only use a subset of the available GPUs. The selection process works for both [DistributedDataParallel](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html) and [DataParallel](https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html). You don't need Accelerate or [DeepSpeed integration](./main_classes/deepspeed).\n+\n+This guide will show you how to select the number of GPUs to use and the order to use them in.\n+\n+## Number of GPUs\n+\n+For example, if there are 4 GPUs and you only want to use the first 2, run the command below.\n+\n+<hfoptions id=\"select-gpu\">\n+<hfoption id=\"torchrun\">\n+\n+Use the `--nproc_per_node` to select how many GPUs to use.\n+\n+```bash\n+torchrun --nproc_per_node=2  trainer-program.py ...\n+```\n+\n+</hfoption>\n+<hfoption id=\"Accelerate\">\n+\n+Use `--num_processes` to select how many GPUs to use.\n+\n+```bash\n+accelerate launch --num_processes 2 trainer-program.py ...\n+```\n+\n+</hfoption>\n+<hfoption id=\"DeepSpeed\">\n+\n+Use `--num_gpus` to select how many GPUs to use.\n+\n+```bash\n+deepspeed --num_gpus 2 trainer-program.py ...\n+```\n+\n+</hfoption>\n+</hfoptions>\n+\n+### Order of GPUs\n+\n+To select specific GPUs to use and their order, configure the the `CUDA_VISIBLE_DEVICES` environment variable. It is easiest to set the environment variable in `~/bashrc` or another startup config file. `CUDA_VISIBLE_DEVICES` is used to map which GPUs are used. For example, if there are 4 GPUs (0, 1, 2, 3) and you only want to run GPUs 0 and 2:\n+\n+```bash\n+CUDA_VISIBLE_DEVICES=0,2 torchrun trainer-program.py ...\n+```\n+\n+Only the 2 physical GPUs (0 and 2) are \"visible\" to PyTorch and these are mapped to `cuda:0` and `cuda:1` respectively. You can also reverse the order of the GPUs to use 2 first. The mapping becomes `cuda:1` for GPU 0 and `cuda:0` for GPU 2.\n+\n+```bash\n+CUDA_VISIBLE_DEVICES=2,0 torchrun trainer-program.py ...\n+```\n+\n+You can also set the `CUDA_VISIBLE_DEVICES` environment variable to an empty value to create an environment without GPUs.\n+\n+```bash\n+CUDA_VISIBLE_DEVICES= python trainer-program.py ...\n+```\n+\n+> [!WARNING]\n+> As with any environment variable, they can be exported instead of being added to the command line. However, this is not recommended because it can be confusing if you forget how the environment variable was set up and you end up using the wrong GPUs. Instead, it is common practice to set the environment variable for a specific training run on the same command line.\n+\n+`CUDA_DEVICE_ORDER` is an alternative environment variable you can use to control how the GPUs are ordered. You can order according to the following.\n+\n+1. PCIe bus IDs that matches the order of [`nvidia-smi`](https://developer.nvidia.com/nvidia-system-management-interface) and [`rocm-smi`](https://rocm.docs.amd.com/projects/rocm_smi_lib/en/latest/.doxygen/docBin/html/index.html) for NVIDIA and AMD GPUs respectively.\n+\n+```bash\n+export CUDA_DEVICE_ORDER=PCI_BUS_ID\n+```\n+\n+2. GPU compute ability.\n+\n+```bash\n+export CUDA_DEVICE_ORDER=FASTEST_FIRST\n+```\n+\n+The `CUDA_DEVICE_ORDER` is especially useful if your training setup consists of an older and newer GPU, where the older GPU appears first, but you cannot physically swap the cards to make the newer GPU appear first. In this case, set `CUDA_DEVICE_ORDER=FASTEST_FIRST` to always use the newer and faster GPU first (`nvidia-smi` or `rocm-smi` still reports the GPUs in their PCIe order). Or you could also set `export CUDA_VISIBLE_DEVICES=1,0`.\n\\ No newline at end of file"
        },
        {
            "sha": "635698b5ab52f03e1004971edd7f1f0ced07d35c",
            "filename": "docs/source/en/how_to_hack_models.md",
            "status": "modified",
            "additions": 54,
            "deletions": 108,
            "changes": 162,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fhow_to_hack_models.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fhow_to_hack_models.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fhow_to_hack_models.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -13,99 +13,74 @@ rendered properly in your Markdown viewer.\n \n -->\n \n-# How to Hack Any Transformers Model\n+# Customizing model components\n \n-The [ðŸ¤— Transformers](https://github.com/huggingface/transformers) library offers a collection of pre-trained models and tools for natural language processing, vision, and beyond. While these models cover a wide range of applications, you might encounter use cases that aren't supported out of the box. Customizing models can unlock new possibilities, such as adding new layers, altering architectures, or optimizing attention mechanisms. This guide will show you how to modify existing Transformers models to fit your specific needs. The great thing is, you donâ€™t have to step away from the Transformers framework to make these changes. You can actually modify models directly in Transformers and still take advantage of features like the [Trainer API](https://huggingface.co/docs/transformers/main/en/main_classes/trainer), [PreTrainedModel](https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel), and efficient fine-tuning with tools like [PEFT](https://huggingface.co/docs/peft/index).\n+Another way to customize a model is to modify their components, rather than writing a new model entirely, allowing you to tailor a model to your specific use case. For example, you can add new layers or optimize the attention mechanism of an architecture. Customizations are applied directly to a Transformers model so that you can continue to use features such as [`Trainer`], [`PreTrainedModel`], and the [PEFT](https://huggingface.co/docs/peft/en/index) library.\n \n-In this guide, weâ€™ll walk you through how to customize existing Transformers models to meet your requirementsâ€”without losing the benefits of the ecosystem.\n+This guide will show you how to customize a models attention mechanism in order to apply [Low-Rank Adaptation (LoRA)](https://huggingface.co/docs/peft/conceptual_guides/adapter#low-rank-adaptation-lora) to it.\n \n-You'll learn how to:\n+> [!TIP]\n+> The [clear_import_cache](https://github.com/huggingface/transformers/blob/9985d06add07a4cc691dc54a7e34f54205c04d40/src/transformers/utils/import_utils.py#L2286) utility is very useful when you're iteratively modifying and developing model code. It removes all cached Transformers modules and allows Python to reload the modified code without constantly restarting your environment.\n+>\n+> ```py\n+> from transformers import AutoModel\n+> from transformers.utils.import_utils import clear_import_cache\n+>\n+> model = AutoModel.from_pretrained(\"bert-base-uncased\")\n+> # modifications to model code\n+> # clear cache to reload modified code\n+> clear_import_cache()\n+> # re-import to use updated code\n+> model = AutoModel.from_pretrained(\"bert-base-uncased\")\n+> ```\n \n-- Modify a model's architecture by changing its attention mechanism.\n-- Apply techniques like Low-Rank Adaptation (LoRA) to specific model components.\n+## Attention class\n \n-We encourage you to contribute your own hacks and share them here with the community!\n+[Segment Anything](./model_doc/sam) is an image segmentation model, and it combines the query-key-value (`qkv`) projection in its attention mechanims. To reduce the number of trainable parameters and computational overhead, you can apply LoRA to the `qkv` projection. This requires splitting the `qkv` projection so that you can separately target the `q` and `v` with LoRA.\n \n-## Efficient Development Workflow\n+1. Create a custom attention class, `SamVisionAttentionSplit`, by subclassing the original `SamVisionAttention` class. In the `__init__`, delete the combined `qkv` and create a separate linear layer for `q`, `k` and `v`.\n \n-When modifying model code, you'll often need to test your changes without restarting your Python session. The `clear_import_cache()` utility helps with this workflow, especially during model development and contribution when you need to frequently test and compare model outputs:\n-\n-```python\n-from transformers import AutoModel\n-model = AutoModel.from_pretrained(\"bert-base-uncased\")\n-\n-# Make modifications to the transformers code...\n-\n-# Clear the cache to reload the modified code\n-from transformers.utils.import_utils import clear_import_cache\n-clear_import_cache()\n-\n-# Reimport to get the changes\n-from transformers import AutoModel\n-model = AutoModel.from_pretrained(\"bert-base-uncased\")  # Will use updated code\n-```\n-\n-This is particularly useful when:\n-- Iteratively modifying model architectures\n-- Debugging model implementations \n-- Testing changes during model development\n-- Comparing outputs between original and modified versions\n-- Working on model contributions\n-\n-The `clear_import_cache()` function removes all cached Transformers modules and allows Python to reload the modified code. This enables rapid development cycles without constantly restarting your environment.\n-\n-This workflow is especially valuable when implementing new models, where you need to frequently compare outputs between the original implementation and your Transformers version (as described in the [Add New Model](https://huggingface.co/docs/transformers/add_new_model) guide).\n-\n-## Example: Modifying the Attention Mechanism in the Segment Anything Model (SAM)\n-\n-The **Segment Anything Model (SAM)** is a state-of-the-art model for image segmentation. In its default implementation, SAM uses a combined query-key-value (`qkv`) projection in its attention mechanism. However, you might want to fine-tune only specific components of the attention mechanism, such as the query (`q`) and value (`v`) projections, to reduce the number of trainable parameters and computational resources required.\n-\n-### Motivation\n-\n-By splitting the combined `qkv` projection into separate `q`, `k`, and `v` projections, you can apply techniques like **LoRA** (Low-Rank Adaptation) to only the `q` and `v` projections. This approach allows you to:\n-\n-- Fine-tune fewer parameters, reducing computational overhead.\n-- Potentially achieve better performance by focusing on specific components.\n-- Experiment with different adaptation strategies in the attention mechanism.\n-\n-### Implementation\n-\n-#### **Step 1: Create a Custom Attention Class**\n-\n-Next, subclass the original `SamVisionAttention` class and modify it to have separate `q`, `k`, and `v` projections.\n-\n-```python\n+```py\n import torch\n import torch.nn as nn\n from transformers.models.sam.modeling_sam import SamVisionAttention\n \n class SamVisionAttentionSplit(SamVisionAttention, nn.Module):\n     def __init__(self, config, window_size):\n         super().__init__(config, window_size)\n+        # remove combined qkv\n         del self.qkv\n-        # Separate q, k, v projections\n+        # separate q, k, v projections\n         self.q = nn.Linear(config.hidden_size, config.hidden_size, bias=config.qkv_bias)\n         self.k = nn.Linear(config.hidden_size, config.hidden_size, bias=config.qkv_bias)\n         self.v = nn.Linear(config.hidden_size, config.hidden_size, bias=config.qkv_bias)\n         self._register_load_state_dict_pre_hook(self.split_q_k_v_load_hook)\n+```\n+\n+2. The `_split_qkv_load_hook` function splits the pretrained `qkv` weights into separate `q`, `k`, and `v` weights when loading the model to ensure compatibility with any pretrained model.\n \n+```py\n     def split_q_k_v_load_hook(self, state_dict, prefix, *args):\n         keys_to_delete = []\n         for key in list(state_dict.keys()):\n             if \"qkv.\" in key:\n-                # Split q, k, v from the combined projection\n+                # split q, k, v from the combined projection\n                 q, k, v = state_dict[key].chunk(3, dim=0)\n-                # Replace with individual q, k, v projections\n+                # replace with individual q, k, v projections\n                 state_dict[key.replace(\"qkv.\", \"q.\")] = q\n                 state_dict[key.replace(\"qkv.\", \"k.\")] = k\n                 state_dict[key.replace(\"qkv.\", \"v.\")] = v\n-                # Mark the old qkv key for deletion\n+                # mark the old qkv key for deletion\n                 keys_to_delete.append(key)\n         \n-        # Remove old qkv keys\n+        # remove old qkv keys\n         for key in keys_to_delete:\n             del state_dict[key]\n+```\n+\n+3. In the `forward` pass, `q`, `k`, and `v` are computed separately while the rest of the attention mechanism remains the same.\n \n+```py\n     def forward(self, hidden_states: torch.Tensor, output_attentions=False) -> torch.Tensor:\n         batch_size, height, width, _ = hidden_states.shape\n         qkv_shapes = (batch_size *  self.num_attention_heads,  height * width, -1)\n@@ -133,78 +108,49 @@ class SamVisionAttentionSplit(SamVisionAttention, nn.Module):\n         return outputs\n ```\n \n-**Explanation:**\n-\n-- **Separate Projections:** The combined `qkv` projection is removed, and separate `q`, `k`, and `v` linear layers are created.\n-- **Weight Loading Hook:** The `_split_qkv_load_hook` method splits the pre-trained `qkv` weights into separate `q`, `k`, and `v` weights when loading the model. This ensures compatibility with any pre-trained model.\n-- **Forward Pass:** Queries, keys, and values are computed separately, and the attention mechanism proceeds as usual.\n+Assign the custom `SamVisionAttentionSplit` class to the original models `SamVisionAttention` module to replace it. All instances of `SamVisionAttention` in the model is replaced with the split attention version.\n \n-#### **Step 2: Replace the Original Attention Class**\n+Load the model with [`~PreTrainedModel.from_pretrained`].\n \n-Replace the original `SamVisionAttention` class with your custom class so that the model uses the modified attention mechanism.\n-\n-```python\n+```py\n from transformers import SamModel\n from transformers.models.sam import modeling_sam\n \n-# Replace the attention class in the modeling_sam module\n+# replace the attention class in the modeling_sam module\n modeling_sam.SamVisionAttention = SamVisionAttentionSplit\n \n-# Load the pre-trained SAM model\n+# load the pretrained SAM model\n model = SamModel.from_pretrained(\"facebook/sam-vit-base\")\n ```\n \n-**Explanation:**\n-\n-- **Class Replacement:** By assigning your custom class to `modeling_sam.SamVisionAttention`, any instances of `SamVisionAttention` in the model will use the modified version. Thus when you call `SamModel`, it will use the newly defined `SamVisionAttentionSplit`. \n-- **Model Loading:** The model is loaded using `from_pretrained`, and the custom attention mechanism is integrated.\n+## LoRA\n \n-#### **Step 3: Apply LoRA to Specific Projections**\n+With separate `q`, `k`, and `v` projections, apply LoRA to `q` and `v`.\n \n-With separate `q`, `k`, and `v` projections, you can now apply LoRA to specific components, such as the `q` and `v` projections.\n+Create a [LoraConfig](https://huggingface.co/docs/peft/package_reference/config#peft.PeftConfig) and specify the rank `r`, `lora_alpha`, `lora_dropout`, `task_type`, and most importantly, the modules to target.\n \n-```python\n+```py\n from peft import LoraConfig, get_peft_model\n \n config = LoraConfig(\n     r=16,\n     lora_alpha=32,\n-    target_modules=[\"q\", \"v\"],  # Apply LoRA to q and v projections\n+    # apply LoRA to q and v\n+    target_modules=[\"q\", \"v\"],\n     lora_dropout=0.1,\n     task_type=\"mask-generation\"\n )\n-\n-# Apply LoRA to the model\n-model = get_peft_model(model, config)\n ```\n \n-**Explanation:**\n-\n-- **LoRA Configuration:** The `LoraConfig` specifies the rank `r`, scaling factor `lora_alpha`, target modules (`\"q\"` and `\"v\"`), dropout, and task type.\n-- **Applying LoRA:** The `get_peft_model` function applies LoRA to the specified modules in the model.\n-- **Parameter Reduction:** By focusing on `q` and `v`, you reduce the number of trainable parameters, leading to faster training and lower memory usage.\n-\n-#### **Step 4: Verify the Number of Trainable Parameters**\n-\n-It's simple to verify the number of trainable parameters and see what impact your modification had. \n+Pass the model and [LoraConfig](https://huggingface.co/docs/peft/package_reference/config#peft.PeftConfig) to [get_peft_model](https://huggingface.co/docs/peft/package_reference/peft_model#peft.get_peft_model) to apply LoRA to the model.\n \n-```python\n-model.print_trainable_parameters()\n-```\n-\n-**Expected Output:**\n-\n-```\n-trainable params: 608,256 || all params: 94,343,728 || trainable%: 0.6447\n-trainable params: 912,384 || all params: 94,647,856 || trainable%: 0.9640 # with k \n+```py\n+model = get_peft_model(model, config)\n ```\n \n-## Contributing Your Own Hacks\n-\n-Modifying pre-trained models can open up new avenues for research and application. By understanding and adjusting the internal mechanisms of models like SAM, you can tailor them to your specific needs, optimize performance, and experiment with new ideas.\n+Call [print_trainable_parameters](https://huggingface.co/docs/peft/package_reference/peft_model#peft.PeftMixedModel.print_trainable_parameters) to view the number of parameters you're training as a result versus the total number of parameters.\n \n-If you've developed your own hacks for Transformers models and would like to share them, consider contributing to this doc.\n-\n-- **Open a Pull Request:** Share your code changes and improvements directly in the repository.\n-- **Write Documentation:** Provide clear explanations and examples of your modifications.\n-- **Engage with the Community:** Discuss your ideas and get feedback from other developers and researchers by opening an issue.\n\\ No newline at end of file\n+```py\n+model.print_trainable_parameters()\n+\"trainable params: 608,256 || all params: 94,343,728 || trainable%: 0.6447\"\n+```\n\\ No newline at end of file"
        },
        {
            "sha": "303ff6fb53b471d0f8388aed608247fd99fb7696",
            "filename": "docs/source/en/hpo_train.md",
            "status": "modified",
            "additions": 116,
            "deletions": 85,
            "changes": 201,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fhpo_train.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fhpo_train.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fhpo_train.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -13,124 +13,155 @@ rendered properly in your Markdown viewer.\n \n -->\n \n-# Hyperparameter Search using Trainer API\n+# Hyperparameter search\n \n-ðŸ¤— Transformers provides a [`Trainer`] class optimized for training ðŸ¤— Transformers models, making it easier to start training without manually writing your own training loop. The [`Trainer`] provides API for hyperparameter search. This doc shows how to enable it in example.\n+Hyperparameter search discovers an optimal set of hyperparameters that produces the best model performance. [`Trainer`] supports several hyperparameter search backends - [Optuna](https://optuna.readthedocs.io/en/stable/index.html), [SigOpt](https://docs.sigopt.com/), [Weights & Biases](https://docs.wandb.ai/), [Ray Tune](https://docs.ray.io/en/latest/tune/index.html) - through  [`~Trainer.hyperparameter_search`] to optimize an objective or even multiple objectives.\n \n-## Hyperparameter Search backend\n+This guide will go over how to set up a hyperparameter search for each of the backends.\n \n-[`Trainer`] supports four hyperparameter search backends currently:\n-[optuna](https://optuna.org/), [sigopt](https://sigopt.com/), [raytune](https://docs.ray.io/en/latest/tune/index.html) and [wandb](https://wandb.ai/site/sweeps).\n-\n-you should install them before using them as the hyperparameter search backend\n ```bash\n pip install optuna/sigopt/wandb/ray[tune]\n ```\n \n-## How to enable Hyperparameter search in example\n+To use [`~Trainer.hyperparameter_search`], you need to create a `model_init` function. This function includes basic model information (arguments and configuration) because it needs to be reinitialized for each search trial in the run.\n+\n+> [!WARNING]\n+> The `model_init` function is incompatible with the [optimizers](./main_classes/trainer#transformers.Trainer.optimizers) parameter. Subclass [`Trainer`] and override the [`~Trainer.create_optimizer_and_scheduler`] method to create a custom optimizer and scheduler.\n \n-Define the hyperparameter search space, different backends need different format.\n+An example `model_init` function is shown below.\n \n-For sigopt, see sigopt [object_parameter](https://docs.sigopt.com/ai-module-api-references/api_reference/objects/object_parameter), it's like following:\n ```py\n->>> def sigopt_hp_space(trial):\n-...     return [\n-...         {\"bounds\": {\"min\": 1e-6, \"max\": 1e-4}, \"name\": \"learning_rate\", \"type\": \"double\"},\n-...         {\n-...             \"categorical_values\": [\"16\", \"32\", \"64\", \"128\"],\n-...             \"name\": \"per_device_train_batch_size\",\n-...             \"type\": \"categorical\",\n-...         },\n-...     ]\n+def model_init(trial):\n+    return AutoModelForSequenceClassification.from_pretrained(\n+        model_args.model_name_or_path,\n+        from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n+        config=config,\n+        cache_dir=model_args.cache_dir,\n+        revision=model_args.model_revision,\n+        token=True if model_args.use_auth_token else None,\n+    )\n ```\n \n-For optuna, see optuna [object_parameter](https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/002_configurations.html#sphx-glr-tutorial-10-key-features-002-configurations-py), it's like following:\n+Pass `model_init` to [`Trainer`] along with everything else you need for training. Then you can call [`~Trainer.hyperparameter_search`] to start the search.\n \n-```py\n->>> def optuna_hp_space(trial):\n-...     return {\n-...         \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True),\n-...         \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [16, 32, 64, 128]),\n-...     }\n-```\n+[`~Trainer.hyperparameter_search`] accepts a [direction](./main_classes/trainer#transformers.Trainer.hyperparameter_search.direction) parameter to specify whether to minimize, maximize, or minimize and maximize multiple objectives. You'll also need to set the [backend](./main_classes/trainer#transformers.Trainer.hyperparameter_search.backend) you're using, an [object](./main_classes/trainer#transformers.Trainer.hyperparameter_search.hp_space) containing the hyperparameters to optimize for, the [number of trials](./main_classes/trainer#transformers.Trainer.hyperparameter_search.n_trials) to run, and a [compute_objective](./main_classes/trainer#transformers.Trainer.hyperparameter_search.compute_objective) to return the objective values.\n \n-Optuna provides multi-objective HPO. You can pass `direction` in `hyperparameter_search` and define your own compute_objective to return multiple objective values. The Pareto Front (`List[BestRun]`) will be returned in hyperparameter_search, you should refer to the test case `TrainerHyperParameterMultiObjectOptunaIntegrationTest` in [test_trainer](https://github.com/huggingface/transformers/blob/main/tests/trainer/test_trainer.py). It's like following\n+> [!TIP]\n+> If [compute_objective](./main_classes/trainer#transformers.Trainer.hyperparameter_search.compute_objective) isn't defined, the default [compute_objective](./main_classes/trainer#transformers.Trainer.hyperparameter_search.compute_objective) is called which is the sum of an evaluation metric like F1.\n \n ```py\n->>> best_trials = trainer.hyperparameter_search(\n-...     direction=[\"minimize\", \"maximize\"],\n-...     backend=\"optuna\",\n-...     hp_space=optuna_hp_space,\n-...     n_trials=20,\n-...     compute_objective=compute_objective,\n-... )\n+from transformers import Trainer\n+\n+trainer = Trainer(\n+    model=None,\n+    args=training_args,\n+    train_dataset=small_train_dataset,\n+    eval_dataset=small_eval_dataset,\n+    compute_metrics=compute_metrics,\n+    processing_class=tokenizer,\n+    model_init=model_init,\n+    data_collator=data_collator,\n+)\n+trainer.hyperparameter_search(...)\n ```\n \n-For raytune, see raytune [object_parameter](https://docs.ray.io/en/latest/tune/api/search_space.html), it's like following:\n+The following examples demonstrate how to perform a hyperparameter search for the learning rate and training batch size using the different backends.\n \n-```py\n->>> def ray_hp_space(trial):\n-...     return {\n-...         \"learning_rate\": tune.loguniform(1e-6, 1e-4),\n-...         \"per_device_train_batch_size\": tune.choice([16, 32, 64, 128]),\n-...     }\n-```\n+<hfoptions id=\"backends\">\n+<hfoption id=\"Optuna\">\n \n-For wandb, see wandb [object_parameter](https://docs.wandb.ai/guides/sweeps/configuration), it's like following:\n+[Optuna](https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/002_configurations.html#sphx-glr-tutorial-10-key-features-002-configurations-py) optimizes categories, integers, and floats.\n \n ```py\n->>> def wandb_hp_space(trial):\n-...     return {\n-...         \"method\": \"random\",\n-...         \"metric\": {\"name\": \"objective\", \"goal\": \"minimize\"},\n-...         \"parameters\": {\n-...             \"learning_rate\": {\"distribution\": \"uniform\", \"min\": 1e-6, \"max\": 1e-4},\n-...             \"per_device_train_batch_size\": {\"values\": [16, 32, 64, 128]},\n-...         },\n-...     }\n+def optuna_hp_space(trial):\n+    return {\n+        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True),\n+        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [16, 32, 64, 128]),\n+    }\n+\n+best_trials = trainer.hyperparameter_search(\n+    direction=[\"minimize\", \"maximize\"],\n+    backend=\"optuna\",\n+    hp_space=optuna_hp_space,\n+    n_trials=20,\n+    compute_objective=compute_objective,\n+)\n ```\n \n-Define a `model_init` function and pass it to the [`Trainer`], as an example:\n+</hfoption>\n+<hfoption id=\"Ray Tune\">\n+\n+[Ray Tune](https://docs.ray.io/en/latest/tune/api/search_space.html) optimizes floats, integers, and categorical parameters. It also offers multiple sampling distributions for each parameter such as uniform and log-uniform.\n+\n ```py\n->>> def model_init(trial):\n-...     return AutoModelForSequenceClassification.from_pretrained(\n-...         model_args.model_name_or_path,\n-...         from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n-...         config=config,\n-...         cache_dir=model_args.cache_dir,\n-...         revision=model_args.model_revision,\n-...         token=True if model_args.use_auth_token else None,\n-...     )\n+def ray_hp_space(trial):\n+    return {\n+        \"learning_rate\": tune.loguniform(1e-6, 1e-4),\n+        \"per_device_train_batch_size\": tune.choice([16, 32, 64, 128]),\n+    }\n+\n+best_trials = trainer.hyperparameter_search( \n+    direction=[\"minimize\", \"maximize\"],\n+    backend=\"ray\",\n+    hp_space=ray_hp_space,\n+    n_trials=20,\n+    compute_objective=compute_objective,\n+)\n ```\n \n-Create a [`Trainer`] with your `model_init` function, training arguments, training and test datasets, and evaluation function:\n+</hfoption>\n+<hfoption id=\"SigOpt\">\n+\n+[SigOpt](https://docs.sigopt.com/ai-module-api-references/api_reference/objects/object_parameter) optimizes double, integer, and categorical parameters.\n \n ```py\n->>> trainer = Trainer(\n-...     model=None,\n-...     args=training_args,\n-...     train_dataset=small_train_dataset,\n-...     eval_dataset=small_eval_dataset,\n-...     compute_metrics=compute_metrics,\n-...     processing_class=tokenizer,\n-...     model_init=model_init,\n-...     data_collator=data_collator,\n-... )\n+def sigopt_hp_space(trial):\n+    return [\n+        {\"bounds\": {\"min\": 1e-6, \"max\": 1e-4}, \"name\": \"learning_rate\", \"type\": \"double\"},\n+        {\n+            \"categorical_values\": [\"16\", \"32\", \"64\", \"128\"],\n+            \"name\": \"per_device_train_batch_size\",\n+            \"type\": \"categorical\",\n+        },\n+    ]\n+\n+best_trials = trainer.hyperparameter_search( \n+    direction=[\"minimize\", \"maximize\"],\n+    backend=\"sigopt\",\n+    hp_space=sigopt_hp_space,\n+    n_trials=20,\n+    compute_objective=compute_objective,\n+)\n ```\n \n-Call hyperparameter search, get the best trial parameters, backend could be `\"optuna\"`/`\"sigopt\"`/`\"wandb\"`/`\"ray\"`. direction can be`\"minimize\"` or `\"maximize\"`, which indicates whether to optimize greater or lower objective.\n+</hfoption>\n+<hfoption id=\"Weights & Biases\">\n \n-You could define your own compute_objective function, if not defined, the default compute_objective will be called, and the sum of eval metric like f1 is returned as objective value.\n+[Weights & Biases](https://docs.wandb.ai/guides/sweeps/sweep-config-keys) also optimizes integers, floats, and categorical parameters. It also includes support for different search strategies and distribution options.\n \n ```py\n->>> best_trial = trainer.hyperparameter_search(\n-...     direction=\"maximize\",\n-...     backend=\"optuna\",\n-...     hp_space=optuna_hp_space,\n-...     n_trials=20,\n-...     compute_objective=compute_objective,\n-... )\n+def wandb_hp_space(trial):\n+    return {\n+        \"method\": \"random\",\n+        \"metric\": {\"name\": \"objective\", \"goal\": \"minimize\"},\n+        \"parameters\": {\n+            \"learning_rate\": {\"distribution\": \"uniform\", \"min\": 1e-6, \"max\": 1e-4},\n+            \"per_device_train_batch_size\": {\"values\": [16, 32, 64, 128]},\n+        },\n+    }\n+\n+best_trials = trainer.hyperparameter_search( \n+    direction=[\"minimize\", \"maximize\"],\n+    backend=\"wandb\",\n+    hp_space=wandb_hp_space,\n+    n_trials=20,\n+    compute_objective=compute_objective,\n+)\n ```\n \n-## Hyperparameter search For DDP finetune\n-Currently, Hyperparameter search for DDP is enabled for optuna and sigopt. Only the rank-zero process will generate the search trial and pass the argument to other ranks.\n+</hfoption>\n+</hfoptions>\n+\n+## Distributed Data Parallel\n+\n+[`Trainer`] only supports hyperparameter search for distributed data parallel (DDP) on the Optuna and SigOpt backends. Only the rank-zero process is used to generate the search trial, and the resulting parameters are passed along to the other ranks."
        },
        {
            "sha": "2e5e466cd5d2f055fc9c28cd973e2292377cadc0",
            "filename": "docs/source/en/image_processors.md",
            "status": "added",
            "additions": 222,
            "deletions": 0,
            "changes": 222,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fimage_processors.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fimage_processors.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fimage_processors.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -0,0 +1,222 @@\n+<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# Image processors\n+\n+Image processors converts images into pixel values, tensors that represent image colors and size. The pixel values are inputs to a vision or video model. To ensure a pretrained model receives the correct input, an image processor can perform the following operations to make sure an image is exactly like the images a model was pretrained on.\n+\n+- [`~BaseImageProcessor.center_crop`] to resize an image\n+- [`~BaseImageProcessor.normalize`] or [`~BaseImageProcessor.rescale`] pixel values\n+\n+Use [`~ImageProcessingMixin.from_pretrained`] to load an image processors configuration (image size, whether to normalize and rescale, etc.) from a vision model on the Hugging Face [Hub](https://hf.co) or local directory. The configuration for each pretrained model is saved in a [preprocessor_config.json](https://huggingface.co/google/vit-base-patch16-224/blob/main/preprocessor_config.json) file.\n+\n+```py\n+from transformers import AutoImageProcessor\n+\n+image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n+```\n+\n+Pass an image to the image processor to transform it into pixel values, and set `return_tensors=\"pt\"` to return PyTorch tensors. Feel free to print out the inputs to see what the image looks like as a tensor.\n+\n+```py\n+from PIL import Image\n+import requests\n+\n+url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/image_processor_example.png\"\n+image = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n+inputs = image_processor(image, return_tensors=\"pt\")\n+```\n+\n+This guide covers the image processor class and how to preprocess images for vision models.\n+\n+## Image processor classes\n+\n+Image processors inherit from the [`BaseImageProcessor`] class which provides the [`~BaseImageProcessor.center_crop`], [`~BaseImageProcessor.normalize`], and [`~BaseImageProcessor.rescale`] functions. There are two types of image processors.\n+\n+- [`BaseImageProcessor`] is a Python implementation.\n+- [`BaseImageProcessorFast`] is a faster [torchvision-backed](https://pytorch.org/vision/stable/index.html) version. For a batch of [torch.Tensor](https://pytorch.org/docs/stable/tensors.html) inputs, this can be up to 33x faster. [`BaseImageProcessorFast`] is not available for all vision models at the moment. Refer to a models API documentation to check if it is supported.\n+\n+Each image processor subclasses the [`ImageProcessingMixin`] class which provides the [`~ImageProcessingMixin.from_pretrained`] and [`~ImageProcessingMixin.save_pretrained`] methods for loading and saving image processors.\n+\n+There are two ways you can load an image processor, with [`AutoImageProcessor`] or a model-specific image processor.\n+\n+<hfoptions id=\"image-processor-classes\">\n+<hfoption id=\"AutoImageProcessor\">\n+\n+The [AutoClass](./model_doc/auto) API provides a convenient method to load an image processor without directly specifying the model the image processor is associated with.\n+\n+Use [`~AutoImageProcessor.from_pretrained`] to load an image processor, and set `use_fast=True` to load a fast image processor if it's supported.\n+\n+```py\n+from transformers import AutoImageProcessor\n+\n+image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\", use_fast=True)\n+```\n+\n+</hfoption>\n+<hfoption id=\"model-specific image processor\">\n+\n+Each image processor is associated with a specific pretrained vision model, and the image processors configuration contains the models expected size and whether to normalize and resize.\n+\n+The image processor can be loaded directly from the model-specific class. Check a models API documentation to see whether it supports a fast image processor.\n+\n+```py\n+from transformers import ViTImageProcessor\n+\n+image_processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n+```\n+\n+To load a fast image processor, use the fast implementation class.\n+\n+```py\n+from transformers import ViTImageProcessorFast\n+\n+image_processor = ViTImageProcessorFast.from_pretrained(\"google/vit-base-patch16-224\")\n+```\n+\n+</hfoption>\n+</hfoptions>\n+\n+## Fast image processors\n+\n+[`BaseImageProcessorFast`] is based on [torchvision](https://pytorch.org/vision/stable/index.html) and is significantly faster, especially when processing on a GPU. This class can be used as a drop-in replacement for [`BaseImageProcessor`] if it's available for a model because it has the same design. Make sure [torchvision](https://pytorch.org/get-started/locally/#mac-installation) is installed, and set the `use_fast` parameter to `True`.\n+\n+```py\n+from transformers import AutoImageProcessor\n+\n+processor = AutoImageProcessor.from_pretrained(\"facebook/detr-resnet-50\", use_fast=True)\n+```\n+\n+Control which device processing is performed on with the `device` parameter. Processing is performed on the same device as the input by default if the inputs are tensors, otherwise they are processed on the CPU. The example below places the fast processor on a GPU.\n+\n+```py\n+from torchvision.io import read_image\n+from transformers import DetrImageProcessorFast\n+\n+images = read_image(\"image.jpg\")\n+processor = DetrImageProcessorFast.from_pretrained(\"facebook/detr-resnet-50\")\n+images_processed = processor(images, return_tensors=\"pt\", device=\"cuda\")\n+```\n+\n+<details>\n+<summary>Benchmarks</summary>\n+\n+The benchmarks are obtained from an [AWS EC2 g5.2xlarge](https://aws.amazon.com/ec2/instance-types/g5/) instance with a NVIDIA A10G Tensor Core GPU.\n+\n+<div class=\"flex\">\n+  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/benchmark_results_full_pipeline_detr_fast_padded.png\" />\n+</div>\n+<div class=\"flex\">\n+  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/benchmark_results_full_pipeline_detr_fast_batched_compiled.png\" />\n+</div>\n+<div class=\"flex\">\n+  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/benchmark_results_full_pipeline_rt_detr_fast_single.png\" />\n+</div>\n+<div class=\"flex\">\n+  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/benchmark_results_full_pipeline_rt_detr_fast_batched.png\" />\n+</div>\n+</details>\n+\n+## Preprocess\n+\n+Transformers' vision models expects the input as PyTorch tensors of pixel values. An image processor handles the conversion of images to pixel values, which is represented by the batch size, number of channels, height, and width. To achieve this, an image is resized (center cropped) and the pixel values are normalized and rescaled to the models expected values.\n+\n+Image preprocessing is not the same as *image augmentation*. Image augmentation makes changes (brightness, colors, rotatation, etc.) to an image for the purpose of either creating new training examples or prevent overfitting. Image preprocessing makes changes to an image for the purpose of matching a pretrained model's expected input format.\n+\n+Typically, images are augmented (to increase performance) and then preprocessed before being passed to a model. You can use any library ([Albumentations](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification_albumentations.ipynb), [Kornia](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification_kornia.ipynb)) for augmentation and an image processor for preprocessing.\n+\n+This guide uses the torchvision [transforms](https://pytorch.org/vision/stable/transforms.html) module for augmentation.\n+\n+Start by loading a small sample of the [food101](https://hf.co/datasets/food101) dataset.\n+\n+```py\n+from datasets import load_dataset\n+\n+dataset = load_dataset(\"food101\", split=\"train[:100]\")\n+```\n+\n+From the [transforms](https://pytorch.org/vision/stable/transforms.html) module, use the [Compose](https://pytorch.org/vision/master/generated/torchvision.transforms.Compose.html) API to chain together [RandomResizedCrop](https://pytorch.org/vision/main/generated/torchvision.transforms.RandomResizedCrop.html) and [ColorJitter](https://pytorch.org/vision/main/generated/torchvision.transforms.ColorJitter.html). These transforms randomly crop and resize an image, and randomly adjusts an images colors.\n+\n+The image size to randomly crop to can be retrieved from the image processor. For some models, an exact height and width are expected while for others, only the `shortest_edge` is required.\n+\n+```py\n+from torchvision.transforms import RandomResizedCrop, ColorJitter, Compose\n+\n+size = (\n+    image_processor.size[\"shortest_edge\"]\n+    if \"shortest_edge\" in image_processor.size\n+    else (image_processor.size[\"height\"], image_processor.size[\"width\"])\n+)\n+_transforms = Compose([RandomResizedCrop(size), ColorJitter(brightness=0.5, hue=0.5)])\n+```\n+\n+Apply the transforms to the images and convert them to the RGB format. Then pass the augmented images to the image processor to return the pixel values.\n+\n+The `do_resize` parameter is set to `False` because the images have already been resized in the augmentation step by [RandomResizedCrop](https://pytorch.org/vision/main/generated/torchvision.transforms.RandomResizedCrop.html). If you don't augment the images, then the image processor automatically resizes and normalizes the images with the `image_mean` and `image_std` values. These values are found in the preprocessor configuration file.\n+\n+```py\n+def transforms(examples):\n+    images = [_transforms(img.convert(\"RGB\")) for img in examples[\"image\"]]\n+    examples[\"pixel_values\"] = image_processor(images, do_resize=False, return_tensors=\"pt\")[\"pixel_values\"]\n+    return examples\n+```\n+\n+Apply the combined augmentation and preprocessing function to the entire dataset on the fly with [`~datasets.Dataset.set_transform`].\n+\n+```py\n+dataset.set_transform(transforms)\n+```\n+\n+Convert the pixel values back into an image to see how the image has been augmented and preprocessed.\n+\n+```py\n+import numpy as np\n+import matplotlib.pyplot as plt\n+\n+img = dataset[0][\"pixel_values\"]\n+plt.imshow(img.permute(1, 2, 0))\n+```\n+\n+<div class=\"flex gap-4\">\n+  <div>\n+    <img class=\"rounded-xl\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/vision-preprocess-tutorial.png\" />\n+    <figcaption class=\"mt-2 text-center text-sm text-gray-500\">before</figcaption>\n+  </div>\n+  <div>\n+    <img class=\"rounded-xl\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/preprocessed_image.png\" />\n+    <figcaption class=\"mt-2 text-center text-sm text-gray-500\">after</figcaption>\n+  </div>\n+</div>\n+\n+For other vision tasks like object detection or segmentation, the image processor includes post-processing methods to convert a models raw output into meaningful predictions like bounding boxes or segmentation maps.\n+\n+### Padding\n+\n+Some models, like [DETR](./model_doc/detr), applies [scale augmentation](https://paperswithcode.com/method/image-scale-augmentation) during training which can cause images in a batch to have different sizes. Images with different sizes can't be batched together.\n+\n+To fix this, pad the images with the special padding token `0`. Use the [pad](https://github.com/huggingface/transformers/blob/9578c2597e2d88b6f0b304b5a05864fd613ddcc1/src/transformers/models/detr/image_processing_detr.py#L1151) method to pad the images, and define a custom collate function to batch them together.\n+\n+```py\n+def collate_fn(batch):\n+    pixel_values = [item[\"pixel_values\"] for item in batch]\n+    encoding = image_processor.pad(pixel_values, return_tensors=\"pt\")\n+    labels = [item[\"labels\"] for item in batch]\n+    batch = {}\n+    batch[\"pixel_values\"] = encoding[\"pixel_values\"]\n+    batch[\"pixel_mask\"] = encoding[\"pixel_mask\"]\n+    batch[\"labels\"] = labels\n+    return batch\n+```"
        },
        {
            "sha": "8120e12937ae377cc2407e45bc4ed0ff6b8f928b",
            "filename": "docs/source/en/index.md",
            "status": "modified",
            "additions": 21,
            "deletions": 373,
            "changes": 394,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Findex.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Findex.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Findex.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -1,4 +1,4 @@\n-<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n+<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n \n Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n the License. You may obtain a copy of the License at\n@@ -13,386 +13,34 @@ specific language governing permissions and limitations under the License.\n rendered properly in your Markdown viewer.\n -->\n \n-# ðŸ¤— Transformers\n+# Transformers\n \n-State-of-the-art Machine Learning for [PyTorch](https://pytorch.org/), [TensorFlow](https://www.tensorflow.org/), and [JAX](https://jax.readthedocs.io/en/latest/).\n+Transformers is a library of pretrained natural language processing, computer vision, audio, and multimodal models for inference and training. Use Transformers to train models on your data, build inference applications, and generate text with large language models.\n \n-ðŸ¤— Transformers provides APIs and tools to easily download and train state-of-the-art pretrained models. Using pretrained models can reduce your compute costs, carbon footprint, and save you the time and resources required to train a model from scratch. These models support common tasks in different modalities, such as:\n+Explore the [Hugging Face Hub](https://huggingface.com) today to find a model and use Transformers to help you get started right away.\n \n-ðŸ“ **Natural Language Processing**: text classification, named entity recognition, question answering, language modeling, code generation, summarization, translation, multiple choice, and text generation.<br>\n-ðŸ–¼ï¸ **Computer Vision**: image classification, object detection, and segmentation.<br>\n-ðŸ—£ï¸ **Audio**: automatic speech recognition and audio classification.<br>\n-ðŸ™ **Multimodal**: table question answering, optical character recognition, information extraction from scanned documents, video classification, and visual question answering.\n+## Features\n \n-ðŸ¤— Transformers support framework interoperability between PyTorch, TensorFlow, and JAX. This provides the flexibility to use a different framework at each stage of a model's life; train a model in three lines of code in one framework, and load it for inference in another. Models can also be exported to a format like ONNX and TorchScript for deployment in production environments.\n+Transformers provides everything you need for inference or training with state-of-the-art pretrained models. Some of the main features include:\n \n-Join the growing community on the [Hub](https://huggingface.co/models), [forum](https://discuss.huggingface.co/), or [Discord](https://discord.com/invite/JfAtkvEtRb) today!\n+- [Pipeline](./pipeline_tutorial): Simple and optimized inference class for many machine learning tasks like text generation, image segmentation, automatic speech recognition, document question answering, and more.\n+- [Trainer](./trainer): A comprehensive trainer that supports features such as mixed precision, torch.compile, and FlashAttention for training and distributed training for PyTorch models.\n+- [generate](./llm_tutorial): Fast text generation with large language models (LLMs) and vision language models (VLMs), including support for streaming and multiple decoding strategies.\n \n-## If you are looking for custom support from the Hugging Face team\n+## Design\n \n-<a target=\"_blank\" href=\"https://huggingface.co/support\">\n-    <img alt=\"HuggingFace Expert Acceleration Program\" src=\"https://cdn-media.huggingface.co/marketing/transformers/new-support-improved.png\" style=\"width: 100%; max-width: 600px; border: 1px solid #eee; border-radius: 4px; box-shadow: 0 1px 2px 0 rgba(0, 0, 0, 0.05);\">\n-</a>\n+> [!TIP]\n+> Read our [Philosophy](./philosophy) to learn more about Transformers' design principles.\n \n-## Contents\n+Transformers is designed for developers and machine learning engineers and researchers. Its main design principles are:\n \n-The documentation is organized into five sections:\n+1. Fast and easy to use: Every model is implemented from only three main classes (configuration, model, and preprocessor) and can be quickly used for inference or training with [`Pipeline`] or [`Trainer`].\n+2. Pretrained models: Reduce your carbon footprint, compute cost and time by using a pretrained model instead of training an entirely new one. Each pretrained model is reproduced as closely as possible to the original model and offers state-of-the-art performance.\n \n-- **GET STARTED** provides a quick tour of the library and installation instructions to get up and running.\n-- **TUTORIALS** are a great place to start if you're a beginner. This section will help you gain the basic skills you need to start using the library.\n-- **HOW-TO GUIDES** show you how to achieve a specific goal, like finetuning a pretrained model for language modeling or how to write and share a custom model.\n-- **CONCEPTUAL GUIDES** offers more discussion and explanation of the underlying concepts and ideas behind models, tasks, and the design philosophy of ðŸ¤— Transformers.\n-- **API** describes all classes and functions:\n+<div class=\"flex justify-center\">\n+  <a target=\"_blank\" href=\"https://huggingface.co/support\">\n+      <img alt=\"HuggingFace Expert Acceleration Program\" src=\"https://hf.co/datasets/huggingface/documentation-images/resolve/81d7d9201fd4ceb537fc4cebc22c29c37a2ed216/transformers/transformers-index.png\" style=\"width: 100%; max-width: 600px; border: 1px solid #eee; border-radius: 4px; box-shadow: 0 1px 2px 0 rgba(0, 0, 0, 0.05);\">\n+  </a>\n+</div>\n \n-  - **MAIN CLASSES** details the most important classes like configuration, model, tokenizer, and pipeline.\n-  - **MODELS** details the classes and functions related to each model implemented in the library.\n-  - **INTERNAL HELPERS** details utility classes and functions used internally.\n-\n-\n-## Supported models and frameworks\n-\n-The table below represents the current support in the library for each of those models, whether they have a Python\n-tokenizer (called \"slow\"). A \"fast\" tokenizer backed by the ðŸ¤— Tokenizers library, whether they have support in Jax (via\n-Flax), PyTorch, and/or TensorFlow.\n-\n-<!--This table is updated automatically from the auto modules with _make fix-copies_. Do not update manually!-->\n-\n-|                                  Model                                   | PyTorch support | TensorFlow support | Flax Support |\n-|:------------------------------------------------------------------------:|:---------------:|:------------------:|:------------:|\n-|                        [ALBERT](model_doc/albert)                        |       âœ…        |         âœ…         |      âœ…      |\n-|                         [ALIGN](model_doc/align)                         |       âœ…        |         âŒ         |      âŒ      |\n-|                       [AltCLIP](model_doc/altclip)                       |       âœ…        |         âŒ         |      âŒ      |\n-|                          [Aria](model_doc/aria)                          |       âœ…        |         âŒ         |      âŒ      |\n-|                     [AriaText](model_doc/aria_text)                      |       âœ…        |         âŒ         |      âŒ      |\n-| [Audio Spectrogram Transformer](model_doc/audio-spectrogram-transformer) |       âœ…        |         âŒ         |      âŒ      |\n-|                    [Autoformer](model_doc/autoformer)                    |       âœ…        |         âŒ         |      âŒ      |\n-|                         [Bamba](model_doc/bamba)                         |       âœ…        |         âŒ         |      âŒ      |\n-|                          [Bark](model_doc/bark)                          |       âœ…        |         âŒ         |      âŒ      |\n-|                          [BART](model_doc/bart)                          |       âœ…        |         âœ…         |      âœ…      |\n-|                       [BARThez](model_doc/barthez)                       |       âœ…        |         âœ…         |      âœ…      |\n-|                       [BARTpho](model_doc/bartpho)                       |       âœ…        |         âœ…         |      âœ…      |\n-|                          [BEiT](model_doc/beit)                          |       âœ…        |         âŒ         |      âœ…      |\n-|                          [BERT](model_doc/bert)                          |       âœ…        |         âœ…         |      âœ…      |\n-|               [Bert Generation](model_doc/bert-generation)               |       âœ…        |         âŒ         |      âŒ      |\n-|                 [BertJapanese](model_doc/bert-japanese)                  |       âœ…        |         âœ…         |      âœ…      |\n-|                      [BERTweet](model_doc/bertweet)                      |       âœ…        |         âœ…         |      âœ…      |\n-|                      [BigBird](model_doc/big_bird)                       |       âœ…        |         âŒ         |      âœ…      |\n-|               [BigBird-Pegasus](model_doc/bigbird_pegasus)               |       âœ…        |         âŒ         |      âŒ      |\n-|                        [BioGpt](model_doc/biogpt)                        |       âœ…        |         âŒ         |      âŒ      |\n-|                           [BiT](model_doc/bit)                           |       âœ…        |         âŒ         |      âŒ      |\n-|                    [Blenderbot](model_doc/blenderbot)                    |       âœ…        |         âœ…         |      âœ…      |\n-|              [BlenderbotSmall](model_doc/blenderbot-small)               |       âœ…        |         âœ…         |      âœ…      |\n-|                          [BLIP](model_doc/blip)                          |       âœ…        |         âœ…         |      âŒ      |\n-|                        [BLIP-2](model_doc/blip-2)                        |       âœ…        |         âŒ         |      âŒ      |\n-|                         [BLOOM](model_doc/bloom)                         |       âœ…        |         âŒ         |      âœ…      |\n-|                          [BORT](model_doc/bort)                          |       âœ…        |         âœ…         |      âœ…      |\n-|                   [BridgeTower](model_doc/bridgetower)                   |       âœ…        |         âŒ         |      âŒ      |\n-|                          [BROS](model_doc/bros)                          |       âœ…        |         âŒ         |      âŒ      |\n-|                          [ByT5](model_doc/byt5)                          |       âœ…        |         âœ…         |      âœ…      |\n-|                     [CamemBERT](model_doc/camembert)                     |       âœ…        |         âœ…         |      âŒ      |\n-|                        [CANINE](model_doc/canine)                        |       âœ…        |         âŒ         |      âŒ      |\n-|                     [Chameleon](model_doc/chameleon)                     |       âœ…        |         âŒ         |      âŒ      |\n-|                  [Chinese-CLIP](model_doc/chinese_clip)                  |       âœ…        |         âŒ         |      âŒ      |\n-|                          [CLAP](model_doc/clap)                          |       âœ…        |         âŒ         |      âŒ      |\n-|                          [CLIP](model_doc/clip)                          |       âœ…        |         âœ…         |      âœ…      |\n-|                       [CLIPSeg](model_doc/clipseg)                       |       âœ…        |         âŒ         |      âŒ      |\n-|                          [CLVP](model_doc/clvp)                          |       âœ…        |         âŒ         |      âŒ      |\n-|                       [CodeGen](model_doc/codegen)                       |       âœ…        |         âŒ         |      âŒ      |\n-|                    [CodeLlama](model_doc/code_llama)                     |       âœ…        |         âŒ         |      âœ…      |\n-|                        [Cohere](model_doc/cohere)                        |       âœ…        |         âŒ         |      âŒ      |\n-|                       [Cohere2](model_doc/cohere2)                       |       âœ…        |         âŒ         |      âŒ      |\n-|                       [ColPali](model_doc/colpali)                       |       âœ…        |         âŒ         |      âŒ      |\n-|              [Conditional DETR](model_doc/conditional_detr)              |       âœ…        |         âŒ         |      âŒ      |\n-|                      [ConvBERT](model_doc/convbert)                      |       âœ…        |         âœ…         |      âŒ      |\n-|                      [ConvNeXT](model_doc/convnext)                      |       âœ…        |         âœ…         |      âŒ      |\n-|                    [ConvNeXTV2](model_doc/convnextv2)                    |       âœ…        |         âœ…         |      âŒ      |\n-|                           [CPM](model_doc/cpm)                           |       âœ…        |         âœ…         |      âœ…      |\n-|                       [CPM-Ant](model_doc/cpmant)                        |       âœ…        |         âŒ         |      âŒ      |\n-|                          [CTRL](model_doc/ctrl)                          |       âœ…        |         âœ…         |      âŒ      |\n-|                           [CvT](model_doc/cvt)                           |       âœ…        |         âœ…         |      âŒ      |\n-|                      [DAB-DETR](model_doc/dab-detr)                      |       âœ…        |         âŒ         |      âŒ      |\n-|                           [DAC](model_doc/dac)                           |       âœ…        |         âŒ         |      âŒ      |\n-|                   [Data2VecAudio](model_doc/data2vec)                    |       âœ…        |         âŒ         |      âŒ      |\n-|                    [Data2VecText](model_doc/data2vec)                    |       âœ…        |         âŒ         |      âŒ      |\n-|                   [Data2VecVision](model_doc/data2vec)                   |       âœ…        |         âœ…         |      âŒ      |\n-|                          [DBRX](model_doc/dbrx)                          |       âœ…        |         âŒ         |      âŒ      |\n-|                       [DeBERTa](model_doc/deberta)                       |       âœ…        |         âœ…         |      âŒ      |\n-|                    [DeBERTa-v2](model_doc/deberta-v2)                    |       âœ…        |         âœ…         |      âŒ      |\n-|          [Decision Transformer](model_doc/decision_transformer)          |       âœ…        |         âŒ         |      âŒ      |\n-|               [Deformable DETR](model_doc/deformable_detr)               |       âœ…        |         âŒ         |      âŒ      |\n-|                          [DeiT](model_doc/deit)                          |       âœ…        |         âœ…         |      âŒ      |\n-|                        [DePlot](model_doc/deplot)                        |       âœ…        |         âŒ         |      âŒ      |\n-|                [Depth Anything](model_doc/depth_anything)                |       âœ…        |         âŒ         |      âŒ      |\n-|                     [DepthPro](model_doc/depth_pro)                      |       âœ…        |         âŒ         |      âŒ      |\n-|                          [DETA](model_doc/deta)                          |       âœ…        |         âŒ         |      âŒ      |\n-|                          [DETR](model_doc/detr)                          |       âœ…        |         âŒ         |      âŒ      |\n-|                      [DialoGPT](model_doc/dialogpt)                      |       âœ…        |         âœ…         |      âœ…      |\n-|                     [DiffLlama](model_doc/diffllama)                     |       âœ…        |         âŒ         |      âŒ      |\n-|                         [DiNAT](model_doc/dinat)                         |       âœ…        |         âŒ         |      âŒ      |\n-|                        [DINOv2](model_doc/dinov2)                        |       âœ…        |         âŒ         |      âœ…      |\n-|         [DINOv2 with Registers](model_doc/dinov2_with_registers)         |       âœ…        |         âŒ         |      âŒ      |\n-|                    [DistilBERT](model_doc/distilbert)                    |       âœ…        |         âœ…         |      âœ…      |\n-|                           [DiT](model_doc/dit)                           |       âœ…        |         âŒ         |      âœ…      |\n-|                       [DonutSwin](model_doc/donut)                       |       âœ…        |         âŒ         |      âŒ      |\n-|                           [DPR](model_doc/dpr)                           |       âœ…        |         âœ…         |      âŒ      |\n-|                           [DPT](model_doc/dpt)                           |       âœ…        |         âŒ         |      âŒ      |\n-|               [EfficientFormer](model_doc/efficientformer)               |       âœ…        |         âœ…         |      âŒ      |\n-|                  [EfficientNet](model_doc/efficientnet)                  |       âœ…        |         âŒ         |      âŒ      |\n-|                       [ELECTRA](model_doc/electra)                       |       âœ…        |         âœ…         |      âœ…      |\n-|                          [Emu3](model_doc/emu3)                          |       âœ…        |         âŒ         |      âŒ      |\n-|                       [EnCodec](model_doc/encodec)                       |       âœ…        |         âŒ         |      âŒ      |\n-|               [Encoder decoder](model_doc/encoder-decoder)               |       âœ…        |         âœ…         |      âœ…      |\n-|                         [ERNIE](model_doc/ernie)                         |       âœ…        |         âŒ         |      âŒ      |\n-|                       [ErnieM](model_doc/ernie_m)                        |       âœ…        |         âŒ         |      âŒ      |\n-|                           [ESM](model_doc/esm)                           |       âœ…        |         âœ…         |      âŒ      |\n-|              [FairSeq Machine-Translation](model_doc/fsmt)               |       âœ…        |         âŒ         |      âŒ      |\n-|                        [Falcon](model_doc/falcon)                        |       âœ…        |         âŒ         |      âŒ      |\n-|                       [Falcon3](model_doc/falcon3)                       |       âœ…        |         âŒ         |      âœ…      |\n-|                  [FalconMamba](model_doc/falcon_mamba)                   |       âœ…        |         âŒ         |      âŒ      |\n-|         [FastSpeech2Conformer](model_doc/fastspeech2_conformer)          |       âœ…        |         âŒ         |      âŒ      |\n-|                       [FLAN-T5](model_doc/flan-t5)                       |       âœ…        |         âœ…         |      âœ…      |\n-|                      [FLAN-UL2](model_doc/flan-ul2)                      |       âœ…        |         âœ…         |      âœ…      |\n-|                      [FlauBERT](model_doc/flaubert)                      |       âœ…        |         âœ…         |      âŒ      |\n-|                         [FLAVA](model_doc/flava)                         |       âœ…        |         âŒ         |      âŒ      |\n-|                          [FNet](model_doc/fnet)                          |       âœ…        |         âŒ         |      âŒ      |\n-|                      [FocalNet](model_doc/focalnet)                      |       âœ…        |         âŒ         |      âŒ      |\n-|                  [Funnel Transformer](model_doc/funnel)                  |       âœ…        |         âœ…         |      âŒ      |\n-|                          [Fuyu](model_doc/fuyu)                          |       âœ…        |         âŒ         |      âŒ      |\n-|                         [Gemma](model_doc/gemma)                         |       âœ…        |         âŒ         |      âœ…      |\n-|                        [Gemma2](model_doc/gemma2)                        |       âœ…        |         âŒ         |      âŒ      |\n-|                           [GIT](model_doc/git)                           |       âœ…        |         âŒ         |      âŒ      |\n-|                           [GLM](model_doc/glm)                           |       âœ…        |         âŒ         |      âŒ      |\n-|                          [GLPN](model_doc/glpn)                          |       âœ…        |         âŒ         |      âŒ      |\n-|                      [GOT-OCR2](model_doc/got_ocr2)                      |       âœ…        |         âŒ         |      âŒ      |\n-|                       [GPT Neo](model_doc/gpt_neo)                       |       âœ…        |         âŒ         |      âœ…      |\n-|                      [GPT NeoX](model_doc/gpt_neox)                      |       âœ…        |         âŒ         |      âŒ      |\n-|             [GPT NeoX Japanese](model_doc/gpt_neox_japanese)             |       âœ…        |         âŒ         |      âŒ      |\n-|                         [GPT-J](model_doc/gptj)                          |       âœ…        |         âœ…         |      âœ…      |\n-|                       [GPT-Sw3](model_doc/gpt-sw3)                       |       âœ…        |         âœ…         |      âœ…      |\n-|                   [GPTBigCode](model_doc/gpt_bigcode)                    |       âœ…        |         âŒ         |      âŒ      |\n-|               [GPTSAN-japanese](model_doc/gptsan-japanese)               |       âœ…        |         âŒ         |      âŒ      |\n-|                       [Granite](model_doc/granite)                       |       âœ…        |         âŒ         |      âŒ      |\n-|                  [GraniteMoeMoe](model_doc/granitemoe)                   |       âœ…        |         âŒ         |      âŒ      |\n-|            [GraniteMoeSharedMoe](model_doc/granitemoeshared)             |       âœ…        |         âŒ         |      âŒ      |\n-|                    [Graphormer](model_doc/graphormer)                    |       âœ…        |         âŒ         |      âŒ      |\n-|                [Grounding DINO](model_doc/grounding-dino)                |       âœ…        |         âŒ         |      âŒ      |\n-|                      [GroupViT](model_doc/groupvit)                      |       âœ…        |         âœ…         |      âŒ      |\n-|                        [Helium](model_doc/helium)                        |       âœ…        |         âŒ         |      âŒ      |\n-|                       [HerBERT](model_doc/herbert)                       |       âœ…        |         âœ…         |      âœ…      |\n-|                         [Hiera](model_doc/hiera)                         |       âœ…        |         âŒ         |      âŒ      |\n-|                        [Hubert](model_doc/hubert)                        |       âœ…        |         âœ…         |      âŒ      |\n-|                        [I-BERT](model_doc/ibert)                         |       âœ…        |         âŒ         |      âŒ      |\n-|                        [I-JEPA](model_doc/ijepa)                         |       âœ…        |         âŒ         |      âŒ      |\n-|                       [IDEFICS](model_doc/idefics)                       |       âœ…        |         âœ…         |      âŒ      |\n-|                      [Idefics2](model_doc/idefics2)                      |       âœ…        |         âŒ         |      âŒ      |\n-|                      [Idefics3](model_doc/idefics3)                      |       âœ…        |         âŒ         |      âŒ      |\n-|          [Idefics3VisionTransformer](model_doc/idefics3_vision)          |       âŒ        |         âŒ         |      âŒ      |\n-|                      [ImageGPT](model_doc/imagegpt)                      |       âœ…        |         âŒ         |      âŒ      |\n-|                      [Informer](model_doc/informer)                      |       âœ…        |         âŒ         |      âŒ      |\n-|                  [InstructBLIP](model_doc/instructblip)                  |       âœ…        |         âŒ         |      âŒ      |\n-|             [InstructBlipVideo](model_doc/instructblipvideo)             |       âœ…        |         âŒ         |      âŒ      |\n-|                         [Jamba](model_doc/jamba)                         |       âœ…        |         âŒ         |      âŒ      |\n-|                        [JetMoe](model_doc/jetmoe)                        |       âœ…        |         âŒ         |      âŒ      |\n-|                       [Jukebox](model_doc/jukebox)                       |       âœ…        |         âŒ         |      âŒ      |\n-|                      [KOSMOS-2](model_doc/kosmos-2)                      |       âœ…        |         âŒ         |      âŒ      |\n-|                      [LayoutLM](model_doc/layoutlm)                      |       âœ…        |         âœ…         |      âŒ      |\n-|                    [LayoutLMv2](model_doc/layoutlmv2)                    |       âœ…        |         âŒ         |      âŒ      |\n-|                    [LayoutLMv3](model_doc/layoutlmv3)                    |       âœ…        |         âœ…         |      âŒ      |\n-|                     [LayoutXLM](model_doc/layoutxlm)                     |       âœ…        |         âŒ         |      âŒ      |\n-|                           [LED](model_doc/led)                           |       âœ…        |         âœ…         |      âŒ      |\n-|                         [LeViT](model_doc/levit)                         |       âœ…        |         âŒ         |      âŒ      |\n-|                          [LiLT](model_doc/lilt)                          |       âœ…        |         âŒ         |      âŒ      |\n-|                         [LLaMA](model_doc/llama)                         |       âœ…        |         âŒ         |      âœ…      |\n-|                        [Llama2](model_doc/llama2)                        |       âœ…        |         âŒ         |      âœ…      |\n-|                        [Llama3](model_doc/llama3)                        |       âœ…        |         âŒ         |      âœ…      |\n-|                         [LLaVa](model_doc/llava)                         |       âœ…        |         âŒ         |      âŒ      |\n-|                    [LLaVA-NeXT](model_doc/llava_next)                    |       âœ…        |         âŒ         |      âŒ      |\n-|              [LLaVa-NeXT-Video](model_doc/llava_next_video)              |       âœ…        |         âŒ         |      âŒ      |\n-|               [LLaVA-Onevision](model_doc/llava_onevision)               |       âœ…        |         âŒ         |      âŒ      |\n-|                    [Longformer](model_doc/longformer)                    |       âœ…        |         âœ…         |      âŒ      |\n-|                        [LongT5](model_doc/longt5)                        |       âœ…        |         âŒ         |      âœ…      |\n-|                          [LUKE](model_doc/luke)                          |       âœ…        |         âŒ         |      âŒ      |\n-|                        [LXMERT](model_doc/lxmert)                        |       âœ…        |         âœ…         |      âŒ      |\n-|                        [M-CTC-T](model_doc/mctct)                        |       âœ…        |         âŒ         |      âŒ      |\n-|                       [M2M100](model_doc/m2m_100)                        |       âœ…        |         âŒ         |      âŒ      |\n-|                    [MADLAD-400](model_doc/madlad-400)                    |       âœ…        |         âœ…         |      âœ…      |\n-|                         [Mamba](model_doc/mamba)                         |       âœ…        |         âŒ         |      âŒ      |\n-|                        [mamba2](model_doc/mamba2)                        |       âœ…        |         âŒ         |      âŒ      |\n-|                        [Marian](model_doc/marian)                        |       âœ…        |         âœ…         |      âœ…      |\n-|                      [MarkupLM](model_doc/markuplm)                      |       âœ…        |         âŒ         |      âŒ      |\n-|                   [Mask2Former](model_doc/mask2former)                   |       âœ…        |         âŒ         |      âŒ      |\n-|                    [MaskFormer](model_doc/maskformer)                    |       âœ…        |         âŒ         |      âŒ      |\n-|                        [MatCha](model_doc/matcha)                        |       âœ…        |         âŒ         |      âŒ      |\n-|                         [mBART](model_doc/mbart)                         |       âœ…        |         âœ…         |      âœ…      |\n-|                      [mBART-50](model_doc/mbart50)                       |       âœ…        |         âœ…         |      âœ…      |\n-|                          [MEGA](model_doc/mega)                          |       âœ…        |         âŒ         |      âŒ      |\n-|                 [Megatron-BERT](model_doc/megatron-bert)                 |       âœ…        |         âŒ         |      âŒ      |\n-|                 [Megatron-GPT2](model_doc/megatron_gpt2)                 |       âœ…        |         âœ…         |      âœ…      |\n-|                       [MGP-STR](model_doc/mgp-str)                       |       âœ…        |         âŒ         |      âŒ      |\n-|                          [Mimi](model_doc/mimi)                          |       âœ…        |         âŒ         |      âŒ      |\n-|                       [Mistral](model_doc/mistral)                       |       âœ…        |         âœ…         |      âœ…      |\n-|                       [Mixtral](model_doc/mixtral)                       |       âœ…        |         âŒ         |      âŒ      |\n-|                        [Mllama](model_doc/mllama)                        |       âœ…        |         âŒ         |      âŒ      |\n-|                         [mLUKE](model_doc/mluke)                         |       âœ…        |         âŒ         |      âŒ      |\n-|                           [MMS](model_doc/mms)                           |       âœ…        |         âœ…         |      âœ…      |\n-|                    [MobileBERT](model_doc/mobilebert)                    |       âœ…        |         âœ…         |      âŒ      |\n-|                  [MobileNetV1](model_doc/mobilenet_v1)                   |       âœ…        |         âŒ         |      âŒ      |\n-|                  [MobileNetV2](model_doc/mobilenet_v2)                   |       âœ…        |         âŒ         |      âŒ      |\n-|                     [MobileViT](model_doc/mobilevit)                     |       âœ…        |         âœ…         |      âŒ      |\n-|                   [MobileViTV2](model_doc/mobilevitv2)                   |       âœ…        |         âŒ         |      âŒ      |\n-|                    [ModernBERT](model_doc/modernbert)                    |       âœ…        |         âŒ         |      âŒ      |\n-|                     [Moonshine](model_doc/moonshine)                     |       âœ…        |         âŒ         |      âŒ      |\n-|                         [Moshi](model_doc/moshi)                         |       âœ…        |         âŒ         |      âŒ      |\n-|                         [MPNet](model_doc/mpnet)                         |       âœ…        |         âœ…         |      âŒ      |\n-|                           [MPT](model_doc/mpt)                           |       âœ…        |         âŒ         |      âŒ      |\n-|                           [MRA](model_doc/mra)                           |       âœ…        |         âŒ         |      âŒ      |\n-|                           [MT5](model_doc/mt5)                           |       âœ…        |         âœ…         |      âœ…      |\n-|                      [MusicGen](model_doc/musicgen)                      |       âœ…        |         âŒ         |      âŒ      |\n-|               [MusicGen Melody](model_doc/musicgen_melody)               |       âœ…        |         âŒ         |      âŒ      |\n-|                           [MVP](model_doc/mvp)                           |       âœ…        |         âŒ         |      âŒ      |\n-|                           [NAT](model_doc/nat)                           |       âœ…        |         âŒ         |      âŒ      |\n-|                      [Nemotron](model_doc/nemotron)                      |       âœ…        |         âŒ         |      âŒ      |\n-|                         [Nezha](model_doc/nezha)                         |       âœ…        |         âŒ         |      âŒ      |\n-|                          [NLLB](model_doc/nllb)                          |       âœ…        |         âŒ         |      âŒ      |\n-|                      [NLLB-MOE](model_doc/nllb-moe)                      |       âœ…        |         âŒ         |      âŒ      |\n-|                        [Nougat](model_doc/nougat)                        |       âœ…        |         âœ…         |      âœ…      |\n-|                 [NystrÃ¶mformer](model_doc/nystromformer)                 |       âœ…        |         âŒ         |      âŒ      |\n-|                          [OLMo](model_doc/olmo)                          |       âœ…        |         âŒ         |      âŒ      |\n-|                         [OLMo2](model_doc/olmo2)                         |       âœ…        |         âŒ         |      âŒ      |\n-|                         [OLMoE](model_doc/olmoe)                         |       âœ…        |         âŒ         |      âŒ      |\n-|                   [OmDet-Turbo](model_doc/omdet-turbo)                   |       âœ…        |         âŒ         |      âŒ      |\n-|                     [OneFormer](model_doc/oneformer)                     |       âœ…        |         âŒ         |      âŒ      |\n-|                    [OpenAI GPT](model_doc/openai-gpt)                    |       âœ…        |         âœ…         |      âŒ      |\n-|                      [OpenAI GPT-2](model_doc/gpt2)                      |       âœ…        |         âœ…         |      âœ…      |\n-|                    [OpenLlama](model_doc/open-llama)                     |       âœ…        |         âŒ         |      âŒ      |\n-|                           [OPT](model_doc/opt)                           |       âœ…        |         âœ…         |      âœ…      |\n-|                       [OWL-ViT](model_doc/owlvit)                        |       âœ…        |         âŒ         |      âŒ      |\n-|                         [OWLv2](model_doc/owlv2)                         |       âœ…        |         âŒ         |      âŒ      |\n-|                     [PaliGemma](model_doc/paligemma)                     |       âœ…        |         âŒ         |      âŒ      |\n-|                  [PatchTSMixer](model_doc/patchtsmixer)                  |       âœ…        |         âŒ         |      âŒ      |\n-|                      [PatchTST](model_doc/patchtst)                      |       âœ…        |         âŒ         |      âŒ      |\n-|                       [Pegasus](model_doc/pegasus)                       |       âœ…        |         âœ…         |      âœ…      |\n-|                     [PEGASUS-X](model_doc/pegasus_x)                     |       âœ…        |         âŒ         |      âŒ      |\n-|                     [Perceiver](model_doc/perceiver)                     |       âœ…        |         âŒ         |      âŒ      |\n-|                     [Persimmon](model_doc/persimmon)                     |       âœ…        |         âŒ         |      âŒ      |\n-|                           [Phi](model_doc/phi)                           |       âœ…        |         âŒ         |      âŒ      |\n-|                          [Phi3](model_doc/phi3)                          |       âœ…        |         âŒ         |      âŒ      |\n-|                        [Phimoe](model_doc/phimoe)                        |       âœ…        |         âŒ         |      âŒ      |\n-|                       [PhoBERT](model_doc/phobert)                       |       âœ…        |         âœ…         |      âœ…      |\n-|                    [Pix2Struct](model_doc/pix2struct)                    |       âœ…        |         âŒ         |      âŒ      |\n-|                       [Pixtral](model_doc/pixtral)                       |       âœ…        |         âŒ         |      âŒ      |\n-|                        [PLBart](model_doc/plbart)                        |       âœ…        |         âŒ         |      âŒ      |\n-|                    [PoolFormer](model_doc/poolformer)                    |       âœ…        |         âŒ         |      âŒ      |\n-|                     [Pop2Piano](model_doc/pop2piano)                     |       âœ…        |         âŒ         |      âŒ      |\n-|                    [ProphetNet](model_doc/prophetnet)                    |       âœ…        |         âŒ         |      âŒ      |\n-|                           [PVT](model_doc/pvt)                           |       âœ…        |         âŒ         |      âŒ      |\n-|                        [PVTv2](model_doc/pvt_v2)                         |       âœ…        |         âŒ         |      âŒ      |\n-|                       [QDQBert](model_doc/qdqbert)                       |       âœ…        |         âŒ         |      âŒ      |\n-|                         [Qwen2](model_doc/qwen2)                         |       âœ…        |         âŒ         |      âŒ      |\n-|                    [Qwen2_5_VL](model_doc/qwen2_5_vl)                    |       âœ…        |         âŒ         |      âŒ      |\n-|                   [Qwen2Audio](model_doc/qwen2_audio)                    |       âœ…        |         âŒ         |      âŒ      |\n-|                     [Qwen2MoE](model_doc/qwen2_moe)                      |       âœ…        |         âŒ         |      âŒ      |\n-|                      [Qwen2VL](model_doc/qwen2_vl)                       |       âœ…        |         âŒ         |      âŒ      |\n-|                           [RAG](model_doc/rag)                           |       âœ…        |         âœ…         |      âŒ      |\n-|                         [REALM](model_doc/realm)                         |       âœ…        |         âŒ         |      âŒ      |\n-|               [RecurrentGemma](model_doc/recurrent_gemma)                |       âœ…        |         âŒ         |      âŒ      |\n-|                      [Reformer](model_doc/reformer)                      |       âœ…        |         âŒ         |      âŒ      |\n-|                        [RegNet](model_doc/regnet)                        |       âœ…        |         âœ…         |      âœ…      |\n-|                       [RemBERT](model_doc/rembert)                       |       âœ…        |         âœ…         |      âŒ      |\n-|                        [ResNet](model_doc/resnet)                        |       âœ…        |         âœ…         |      âœ…      |\n-|                     [RetriBERT](model_doc/retribert)                     |       âœ…        |         âŒ         |      âŒ      |\n-|                       [RoBERTa](model_doc/roberta)                       |       âœ…        |         âœ…         |      âœ…      |\n-|          [RoBERTa-PreLayerNorm](model_doc/roberta-prelayernorm)          |       âœ…        |         âœ…         |      âœ…      |\n-|                      [RoCBert](model_doc/roc_bert)                       |       âœ…        |         âŒ         |      âŒ      |\n-|                      [RoFormer](model_doc/roformer)                      |       âœ…        |         âœ…         |      âœ…      |\n-|                       [RT-DETR](model_doc/rt_detr)                       |       âœ…        |         âŒ         |      âŒ      |\n-|                [RT-DETR-ResNet](model_doc/rt_detr_resnet)                |       âœ…        |         âŒ         |      âŒ      |\n-|                    [RT-DETRv2](model_doc/rt_detr_v2)                     |       âœ…        |         âŒ         |      âŒ      |\n-|                          [RWKV](model_doc/rwkv)                          |       âœ…        |         âŒ         |      âŒ      |\n-|                           [SAM](model_doc/sam)                           |       âœ…        |         âœ…         |      âŒ      |\n-|                  [SeamlessM4T](model_doc/seamless_m4t)                   |       âœ…        |         âŒ         |      âŒ      |\n-|                [SeamlessM4Tv2](model_doc/seamless_m4t_v2)                |       âœ…        |         âŒ         |      âŒ      |\n-|                     [SegFormer](model_doc/segformer)                     |       âœ…        |         âœ…         |      âŒ      |\n-|                        [SegGPT](model_doc/seggpt)                        |       âœ…        |         âŒ         |      âŒ      |\n-|                           [SEW](model_doc/sew)                           |       âœ…        |         âŒ         |      âŒ      |\n-|                         [SEW-D](model_doc/sew-d)                         |       âœ…        |         âŒ         |      âŒ      |\n-|                        [SigLIP](model_doc/siglip)                        |       âœ…        |         âŒ         |      âŒ      |\n-|                       [SigLIP2](model_doc/siglip2)                       |       âœ…        |         âŒ         |      âŒ      |\n-|                       [SmolVLM](model_doc/smolvlm)                       |       âœ…        |         âŒ         |      âŒ      |\n-|        [Speech Encoder decoder](model_doc/speech-encoder-decoder)        |       âœ…        |         âŒ         |      âœ…      |\n-|                 [Speech2Text](model_doc/speech_to_text)                  |       âœ…        |         âœ…         |      âŒ      |\n-|                      [SpeechT5](model_doc/speecht5)                      |       âœ…        |         âŒ         |      âŒ      |\n-|                      [Splinter](model_doc/splinter)                      |       âœ…        |         âŒ         |      âŒ      |\n-|                   [SqueezeBERT](model_doc/squeezebert)                   |       âœ…        |         âŒ         |      âŒ      |\n-|                      [StableLm](model_doc/stablelm)                      |       âœ…        |         âŒ         |      âŒ      |\n-|                    [Starcoder2](model_doc/starcoder2)                    |       âœ…        |         âŒ         |      âŒ      |\n-|                     [SuperGlue](model_doc/superglue)                     |       âœ…        |         âŒ         |      âŒ      |\n-|                    [SuperPoint](model_doc/superpoint)                    |       âœ…        |         âŒ         |      âŒ      |\n-|                   [SwiftFormer](model_doc/swiftformer)                   |       âœ…        |         âœ…         |      âŒ      |\n-|                    [Swin Transformer](model_doc/swin)                    |       âœ…        |         âœ…         |      âŒ      |\n-|                 [Swin Transformer V2](model_doc/swinv2)                  |       âœ…        |         âŒ         |      âŒ      |\n-|                       [Swin2SR](model_doc/swin2sr)                       |       âœ…        |         âŒ         |      âŒ      |\n-|           [SwitchTransformers](model_doc/switch_transformers)            |       âœ…        |         âŒ         |      âŒ      |\n-|                            [T5](model_doc/t5)                            |       âœ…        |         âœ…         |      âœ…      |\n-|                        [T5v1.1](model_doc/t5v1.1)                        |       âœ…        |         âœ…         |      âœ…      |\n-|             [Table Transformer](model_doc/table-transformer)             |       âœ…        |         âŒ         |      âŒ      |\n-|                         [TAPAS](model_doc/tapas)                         |       âœ…        |         âœ…         |      âŒ      |\n-|                         [TAPEX](model_doc/tapex)                         |       âœ…        |         âœ…         |      âœ…      |\n-|                       [TextNet](model_doc/textnet)                       |       âœ…        |         âŒ         |      âŒ      |\n-|       [Time Series Transformer](model_doc/time_series_transformer)       |       âœ…        |         âŒ         |      âŒ      |\n-|                   [TimeSformer](model_doc/timesformer)                   |       âœ…        |         âŒ         |      âŒ      |\n-|                [TimmWrapperModel](model_doc/timm_wrapper)                |       âœ…        |         âŒ         |      âŒ      |\n-|        [Trajectory Transformer](model_doc/trajectory_transformer)        |       âœ…        |         âŒ         |      âŒ      |\n-|                  [Transformer-XL](model_doc/transfo-xl)                  |       âœ…        |         âœ…         |      âŒ      |\n-|                         [TrOCR](model_doc/trocr)                         |       âœ…        |         âŒ         |      âŒ      |\n-|                          [TVLT](model_doc/tvlt)                          |       âœ…        |         âŒ         |      âŒ      |\n-|                           [TVP](model_doc/tvp)                           |       âœ…        |         âŒ         |      âŒ      |\n-|                          [UDOP](model_doc/udop)                          |       âœ…        |         âŒ         |      âŒ      |\n-|                           [UL2](model_doc/ul2)                           |       âœ…        |         âœ…         |      âœ…      |\n-|                          [UMT5](model_doc/umt5)                          |       âœ…        |         âŒ         |      âŒ      |\n-|                     [UniSpeech](model_doc/unispeech)                     |       âœ…        |         âŒ         |      âŒ      |\n-|                 [UniSpeechSat](model_doc/unispeech-sat)                  |       âœ…        |         âŒ         |      âŒ      |\n-|                       [UnivNet](model_doc/univnet)                       |       âœ…        |         âŒ         |      âŒ      |\n-|                       [UPerNet](model_doc/upernet)                       |       âœ…        |         âŒ         |      âŒ      |\n-|                           [VAN](model_doc/van)                           |       âœ…        |         âŒ         |      âŒ      |\n-|                   [VideoLlava](model_doc/video_llava)                    |       âœ…        |         âŒ         |      âŒ      |\n-|                      [VideoMAE](model_doc/videomae)                      |       âœ…        |         âŒ         |      âŒ      |\n-|                          [ViLT](model_doc/vilt)                          |       âœ…        |         âŒ         |      âŒ      |\n-|                      [VipLlava](model_doc/vipllava)                      |       âœ…        |         âŒ         |      âŒ      |\n-|        [Vision Encoder decoder](model_doc/vision-encoder-decoder)        |       âœ…        |         âœ…         |      âœ…      |\n-|       [VisionTextDualEncoder](model_doc/vision-text-dual-encoder)        |       âœ…        |         âœ…         |      âœ…      |\n-|                   [VisualBERT](model_doc/visual_bert)                    |       âœ…        |         âŒ         |      âŒ      |\n-|                           [ViT](model_doc/vit)                           |       âœ…        |         âœ…         |      âœ…      |\n-|                    [ViT Hybrid](model_doc/vit_hybrid)                    |       âœ…        |         âŒ         |      âŒ      |\n-|                        [VitDet](model_doc/vitdet)                        |       âœ…        |         âŒ         |      âŒ      |\n-|                       [ViTMAE](model_doc/vit_mae)                        |       âœ…        |         âœ…         |      âŒ      |\n-|                      [ViTMatte](model_doc/vitmatte)                      |       âœ…        |         âŒ         |      âŒ      |\n-|                       [ViTMSN](model_doc/vit_msn)                        |       âœ…        |         âŒ         |      âŒ      |\n-|                       [ViTPose](model_doc/vitpose)                       |       âœ…        |         âŒ         |      âŒ      |\n-|              [ViTPoseBackbone](model_doc/vitpose_backbone)               |       âœ…        |         âŒ         |      âŒ      |\n-|                          [VITS](model_doc/vits)                          |       âœ…        |         âŒ         |      âŒ      |\n-|                         [ViViT](model_doc/vivit)                         |       âœ…        |         âŒ         |      âŒ      |\n-|                      [Wav2Vec2](model_doc/wav2vec2)                      |       âœ…        |         âœ…         |      âœ…      |\n-|                 [Wav2Vec2-BERT](model_doc/wav2vec2-bert)                 |       âœ…        |         âŒ         |      âŒ      |\n-|            [Wav2Vec2-Conformer](model_doc/wav2vec2-conformer)            |       âœ…        |         âŒ         |      âŒ      |\n-|              [Wav2Vec2Phoneme](model_doc/wav2vec2_phoneme)               |       âœ…        |         âœ…         |      âœ…      |\n-|                         [WavLM](model_doc/wavlm)                         |       âœ…        |         âŒ         |      âŒ      |\n-|                       [Whisper](model_doc/whisper)                       |       âœ…        |         âœ…         |      âœ…      |\n-|                        [X-CLIP](model_doc/xclip)                         |       âœ…        |         âŒ         |      âŒ      |\n-|                         [X-MOD](model_doc/xmod)                          |       âœ…        |         âŒ         |      âŒ      |\n-|                          [XGLM](model_doc/xglm)                          |       âœ…        |         âœ…         |      âœ…      |\n-|                           [XLM](model_doc/xlm)                           |       âœ…        |         âœ…         |      âŒ      |\n-|                [XLM-ProphetNet](model_doc/xlm-prophetnet)                |       âœ…        |         âŒ         |      âŒ      |\n-|                   [XLM-RoBERTa](model_doc/xlm-roberta)                   |       âœ…        |         âœ…         |      âœ…      |\n-|                [XLM-RoBERTa-XL](model_doc/xlm-roberta-xl)                |       âœ…        |         âŒ         |      âŒ      |\n-|                         [XLM-V](model_doc/xlm-v)                         |       âœ…        |         âœ…         |      âœ…      |\n-|                         [XLNet](model_doc/xlnet)                         |       âœ…        |         âœ…         |      âŒ      |\n-|                         [XLS-R](model_doc/xls_r)                         |       âœ…        |         âœ…         |      âœ…      |\n-|                 [XLSR-Wav2Vec2](model_doc/xlsr_wav2vec2)                 |       âœ…        |         âœ…         |      âœ…      |\n-|                         [YOLOS](model_doc/yolos)                         |       âœ…        |         âŒ         |      âŒ      |\n-|                          [YOSO](model_doc/yoso)                          |       âœ…        |         âŒ         |      âŒ      |\n-|                         [Zamba](model_doc/zamba)                         |       âœ…        |         âŒ         |      âŒ      |\n-|                        [Zamba2](model_doc/zamba2)                        |       âœ…        |         âŒ         |      âŒ      |\n-|                      [ZoeDepth](model_doc/zoedepth)                      |       âœ…        |         âŒ         |      âŒ      |\n-\n-<!-- End table-->\n+Join us on the Hugging Face [Hub](https://huggingface.co/), [Discord](https://discord.com/invite/JfAtkvEtRb), or [forum](https://discuss.huggingface.co/) to collaborate and build models, datasets, and applications together."
        },
        {
            "sha": "2c7ddabb878305a6545d9d165d7f552a0a30e2b3",
            "filename": "docs/source/en/installation.md",
            "status": "modified",
            "additions": 94,
            "deletions": 171,
            "changes": 265,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Finstallation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Finstallation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Finstallation.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -1,5 +1,5 @@\n <!---\n-Copyright 2022 The HuggingFace Team. All rights reserved.\n+Copyright 2024 The HuggingFace Team. All rights reserved.\n \n Licensed under the Apache License, Version 2.0 (the \"License\");\n you may not use this file except in compliance with the License.\n@@ -20,281 +20,204 @@ rendered properly in your Markdown viewer.\n \n # Installation\n \n-Install ðŸ¤— Transformers for whichever deep learning library you're working with, setup your cache, and optionally configure ðŸ¤— Transformers to run offline.\n+Transformers works with [PyTorch](https://pytorch.org/get-started/locally/), [TensorFlow 2.0](https://www.tensorflow.org/install/pip), and [Flax](https://flax.readthedocs.io/en/latest/). It has been tested on Python 3.6+, PyTorch 1.1.0+, TensorFlow 2.0+, and Flax.\n \n-ðŸ¤— Transformers is tested on Python 3.6+, PyTorch 1.1.0+, TensorFlow 2.0+, and Flax. Follow the installation instructions below for the deep learning library you are using:\n+## Virtual environment\n \n-* [PyTorch](https://pytorch.org/get-started/locally/) installation instructions.\n-* [TensorFlow 2.0](https://www.tensorflow.org/install/pip) installation instructions.\n-* [Flax](https://flax.readthedocs.io/en/latest/) installation instructions.\n+A virtual environment helps manage different projects and avoids compatibility issues between dependencies. Take a look at the [Install packages in a virtual environment using pip and venv](https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments/) guide if you're unfamiliar with Python virtual environments.\n \n-## Install with pip\n+<hfoptions id=\"virtual\">\n+<hfoption id=\"venv\">\n \n-You should install ðŸ¤— Transformers in a [virtual environment](https://docs.python.org/3/library/venv.html). If you're unfamiliar with Python virtual environments, take a look at this [guide](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/). A virtual environment makes it easier to manage different projects, and avoid compatibility issues between dependencies.\n-\n-Create a virtual environment with [uv](https://docs.astral.sh/uv/) (refer to [Installation](https://docs.astral.sh/uv/getting-started/installation/) for installation instructions), a fast Rust-based Python package and project manager.\n+Create and activate a virtual environment in your project directory with [venv](https://docs.python.org/3/library/venv.html).\n \n ```bash\n-uv venv my-env\n-source my-env/bin/activate\n+python -m venv .env\n+source ./env/bin/activate\n ```\n \n-Now you're ready to install ðŸ¤— Transformers with pip or uv.\n-\n-<hfoptions id=\"install\">\n+</hfoption>\n <hfoption id=\"uv\">\n \n+[uv](https://docs.astral.sh/uv/) is a fast Rust-based Python package and project manager.\n+\n ```bash\n-uv pip install transformers\n+uv venv .env\n+source ./env/bin/activate\n ```\n \n </hfoption>\n+</hfoptions>\n+\n+## Python\n+\n+You can install Transformers with pip or uv.\n+\n+<hfoptions id=\"install\">\n <hfoption id=\"pip\">\n \n+[pip](https://pip.pypa.io/en/stable/) is a package installer for Python. Install Transformers with pip in your newly created virtual environment.\n+\n ```bash\n pip install transformers\n ```\n \n+</hfoption>\n+<hfoption id=\"uv\">\n+\n+[uv](https://docs.astral.sh/uv/) is a fast Rust-based Python package and project manager.\n+\n+```bash\n+uv pip install transformers\n+```\n+\n </hfoption>\n </hfoptions>\n \n-For GPU acceleration, install the appropriate CUDA drivers for [PyTorch](https://pytorch.org/get-started/locally) and TensorFlow(https://www.tensorflow.org/install/pip).\n+For GPU acceleration, install the appropriate CUDA drivers for [PyTorch](https://pytorch.org/get-started/locally) and [TensorFlow](https://www.tensorflow.org/install/pip).\n \n Run the command below to check if your system detects an NVIDIA GPU.\n \n ```bash\n nvidia-smi\n ```\n \n-For CPU-support only, you can conveniently install ðŸ¤— Transformers and a deep learning library in one line. For example, install ðŸ¤— Transformers and PyTorch with:\n+To install a CPU-only version of Transformers and a machine learning framework, run the following command.\n \n-```bash\n-pip install 'transformers[torch]'\n-```\n-\n-ðŸ¤— Transformers and TensorFlow 2.0:\n+<hfoptions id=\"cpu-only\">\n+<hfoption id=\"PyTorch\">\n \n ```bash\n-pip install 'transformers[tf-cpu]'\n+pip install 'transformers[torch]'\n+uv pip install 'transformers[torch]'\n ```\n \n-<Tip warning={true}>\n+</hfoption>\n+<hfoption id=\"TensorFlow\">\n \n-M1 / ARM Users\n+For Apple M1 hardware, you need to install CMake and pkg-config first.\n \n-You will need to install the following before installing TensorFlow 2.0\n ```bash\n brew install cmake\n brew install pkg-config\n ```\n \n-</Tip>\n-\n-ðŸ¤— Transformers and Flax:\n+Install TensorFlow 2.0.\n \n ```bash\n-pip install 'transformers[flax]'\n+pip install 'transformers[tf-cpu]'\n+uv pip install 'transformers[tf-cpu]'\n ```\n \n-Finally, check if ðŸ¤— Transformers has been properly installed by running the following command. It will download a pretrained model:\n+</hfoption>\n+<hfoption id=\"Flax\">\n \n ```bash\n-python -c \"from transformers import pipeline; print(pipeline('sentiment-analysis')('we love you'))\"\n+pip install 'transformers[flax]'\n+uv pip install 'transformers[flax]'\n ```\n \n-Then print out the label and score:\n+</hfoption>\n+</hfoptions>\n+\n+Test whether the install was successful with the following command. It should return a label and score for the provided text.\n \n ```bash\n+python -c \"from transformers import pipeline; print(pipeline('sentiment-analysis')('hugging face is the best'))\"\n [{'label': 'POSITIVE', 'score': 0.9998704791069031}]\n ```\n \n-## Install from source\n+### Source install\n+\n+Installing from source installs the *latest* version rather than the *stable* version of the library. It ensures you have the most up-to-date changes in Transformers and it's useful for experimenting with the latest features or fixing a bug that hasn't been officially released in the stable version yet.\n+\n+The downside is that the latest version may not always be stable. If you encounter any problems, please open a [GitHub Issue](https://github.com/huggingface/transformers/issues) so we can fix it as soon as possible.\n \n-Install ðŸ¤— Transformers from source with the following command:\n+Install from source with the following command.\n \n ```bash\n pip install git+https://github.com/huggingface/transformers\n ```\n \n-This command installs the bleeding edge `main` version rather than the latest `stable` version. The `main` version is useful for staying up-to-date with the latest developments. For instance, if a bug has been fixed since the last official release but a new release hasn't been rolled out yet. However, this means the `main` version may not always be stable. We strive to keep the `main` version operational, and most issues are usually resolved within a few hours or a day. If you run into a problem, please open an [Issue](https://github.com/huggingface/transformers/issues) so we can fix it even sooner!\n-\n-Check if ðŸ¤— Transformers has been properly installed by running the following command:\n+Check if the install was successful with the command below. It should return a label and score for the provided text.\n \n ```bash\n-python -c \"from transformers import pipeline; print(pipeline('sentiment-analysis')('I love you'))\"\n+python -c \"from transformers import pipeline; print(pipeline('sentiment-analysis')('hugging face is the best'))\"\n+[{'label': 'POSITIVE', 'score': 0.9998704791069031}]\n ```\n \n-## Editable install\n-\n-You will need an editable install if you'd like to:\n+### Editable install\n \n-* Use the `main` version of the source code.\n-* Contribute to ðŸ¤— Transformers and need to test changes in the code.\n-\n-Clone the repository and install ðŸ¤— Transformers with the following commands:\n+An [editable install](https://pip.pypa.io/en/stable/topics/local-project-installs/#editable-installs) is useful if you're developing locally with Transformers. It links your local copy of Transformers to the Transformers [repository](https://github.com/huggingface/transformers) instead of copying the files. The files are added to Python's import path.\n \n ```bash\n git clone https://github.com/huggingface/transformers.git\n cd transformers\n pip install -e .\n ```\n \n-These commands will link the folder you cloned the repository to and your Python library paths. Python will now look inside the folder you cloned to in addition to the normal library paths. For example, if your Python packages are typically installed in `~/anaconda3/envs/main/lib/python3.7/site-packages/`, Python will also search the folder you cloned to: `~/transformers/`.\n-\n-<Tip warning={true}>\n-\n-You must keep the `transformers` folder if you want to keep using the library.\n+> [!WARNING]\n+> You must keep the local Transformers folder to keep using it.\n \n-</Tip>\n-\n-Now you can easily update your clone to the latest version of ðŸ¤— Transformers with the following command:\n+Update your local version of Transformers with the latest changes in the main repository with the following command.\n \n ```bash\n cd ~/transformers/\n git pull\n ```\n \n-Your Python environment will find the `main` version of ðŸ¤— Transformers on the next run.\n-\n-## Install with conda\n+## conda\n \n-Install from the conda channel `conda-forge`:\n+[conda](https://docs.conda.io/projects/conda/en/stable/#) is a language-agnostic package manager. Install Transformers from the [conda-forge](https://anaconda.org/conda-forge/transformers) channel in your newly created virtual environment.\n \n ```bash\n conda install conda-forge::transformers\n ```\n \n-## Cache setup\n+## Set up\n \n-Pretrained models are downloaded and locally cached at: `~/.cache/huggingface/hub`. This is the default directory given by the shell environment variable `TRANSFORMERS_CACHE`. On Windows, the default directory is given by `C:\\Users\\username\\.cache\\huggingface\\hub`. You can change the shell environment variables shown below - in order of priority - to specify a different cache directory:\n+After installation, you can configure the Transformers cache location or set up the library for offline usage.\n \n-1. Shell environment variable (default): `HF_HUB_CACHE` or `TRANSFORMERS_CACHE`.\n-2. Shell environment variable: `HF_HOME`.\n-3. Shell environment variable: `XDG_CACHE_HOME` + `/huggingface`.\n+### Cache directory\n \n-<Tip>\n+When you load a pretrained model with [`~PreTrainedModel.from_pretrained`], the model is downloaded from the Hub and locally cached.\n \n-ðŸ¤— Transformers will use the shell environment variables `PYTORCH_TRANSFORMERS_CACHE` or `PYTORCH_PRETRAINED_BERT_CACHE` if you are coming from an earlier iteration of this library and have set those environment variables, unless you specify the shell environment variable `TRANSFORMERS_CACHE`.\n+Every time you load a model, it checks whether the cached model is up-to-date. If it's the same, then the local model is loaded. If it's not the same, the newer model is downloaded and cached.\n \n-</Tip>\n+The default directory given by the shell environment variable `TRANSFORMERS_CACHE` is `~/.cache/huggingface/hub`. On Windows, the default directory is `C:\\Users\\username\\.cache\\huggingface\\hub`.\n \n-## Offline mode\n+Cache a model in a different directory by changing the path in the following shell environment variables (listed by priority).\n \n-Run ðŸ¤— Transformers in a firewalled or offline environment with locally cached files by setting the environment variable `HF_HUB_OFFLINE=1`.\n+1. [HF_HUB_CACHE](https://hf.co/docs/huggingface_hub/package_reference/environment_variables#hfhubcache) or `TRANSFORMERS_CACHE` (default)\n+2. [HF_HOME](https://hf.co/docs/huggingface_hub/package_reference/environment_variables#hfhome)\n+3. [XDG_CACHE_HOME](https://hf.co/docs/huggingface_hub/package_reference/environment_variables#xdgcachehome) + `/huggingface` (only if `HF_HOME` is not set)\n \n-<Tip>\n+Older versions of Transformers uses the shell environment variables `PYTORCH_TRANSFORMERS_CACHE` or `PYTORCH_PRETRAINED_BERT_CACHE`. You should keep these unless you specify the newer shell environment variable `TRANSFORMERS_CACHE`.\n \n-Add [ðŸ¤— Datasets](https://huggingface.co/docs/datasets/) to your offline training workflow with the environment variable `HF_DATASETS_OFFLINE=1`.\n+### Offline mode\n \n-</Tip>\n+To use Transformers in an offline or firewalled environment requires the downloaded and cached files ahead of time. Download a model repository from the Hub with the [`~huggingface_hub.snapshot_download`] method.\n \n-```bash\n-HF_DATASETS_OFFLINE=1 HF_HUB_OFFLINE=1 \\\n-python examples/pytorch/translation/run_translation.py --model_name_or_path google-t5/t5-small --dataset_name wmt16 --dataset_config ro-en ...\n-```\n-\n-This script should run without hanging or waiting to timeout because it won't attempt to download the model from the Hub.\n-\n-You can also bypass loading a model from the Hub from each [`~PreTrainedModel.from_pretrained`] call with the [`local_files_only`] parameter. When set to `True`, only local files are loaded:\n+> [!TIP]\n+> Refer to the [Download files from the Hub](https://hf.co/docs/huggingface_hub/guides/download) guide for more options for downloading files from the Hub. You can download files from specific revisions, download from the CLI, and even filter which files to download from a repository.\n \n ```py\n-from transformers import T5Model\n+from huggingface_hub import snapshot_download\n \n-model = T5Model.from_pretrained(\"./path/to/local/directory\", local_files_only=True)\n+snapshot_download(repo_id=\"meta-llama/Llama-2-7b-hf\", repo_type=\"model\")\n ```\n \n-### Fetch models and tokenizers to use offline\n-\n-Another option for using ðŸ¤— Transformers offline is to download the files ahead of time, and then point to their local path when you need to use them offline. There are three ways to do this:\n-\n-* Download a file through the user interface on the [Model Hub](https://huggingface.co/models) by clicking on the â†“ icon.\n-\n-    ![download-icon](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/download-icon.png)\n-\n-* Use the [`PreTrainedModel.from_pretrained`] and [`PreTrainedModel.save_pretrained`] workflow:\n-\n-    1. Download your files ahead of time with [`PreTrainedModel.from_pretrained`]:\n-\n-    ```py\n-    >>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n-\n-    >>> tokenizer = AutoTokenizer.from_pretrained(\"bigscience/T0_3B\")\n-    >>> model = AutoModelForSeq2SeqLM.from_pretrained(\"bigscience/T0_3B\")\n-    ```\n-\n-    2. Save your files to a specified directory with [`PreTrainedModel.save_pretrained`]:\n+Set the environment variable `HF_HUB_OFFLINE=1` to prevent HTTP calls to the Hub when loading a model.\n \n-    ```py\n-    >>> tokenizer.save_pretrained(\"./your/path/bigscience_t0\")\n-    >>> model.save_pretrained(\"./your/path/bigscience_t0\")\n-    ```\n-\n-    3. Now when you're offline, reload your files with [`PreTrainedModel.from_pretrained`] from the specified directory:\n-\n-    ```py\n-    >>> tokenizer = AutoTokenizer.from_pretrained(\"./your/path/bigscience_t0\")\n-    >>> model = AutoModel.from_pretrained(\"./your/path/bigscience_t0\")\n-    ```\n-\n-* Programmatically download files with the [huggingface_hub](https://github.com/huggingface/huggingface_hub/tree/main/src/huggingface_hub) library:\n-\n-    1. Install the `huggingface_hub` library in your virtual environment:\n-\n-    ```bash\n-    python -m pip install huggingface_hub\n-    ```\n-\n-    2. Use the [`hf_hub_download`](https://huggingface.co/docs/hub/adding-a-library#download-files-from-the-hub) function to download a file to a specific path. For example, the following command downloads the `config.json` file from the [T0](https://huggingface.co/bigscience/T0_3B) model to your desired path:\n-\n-    ```py\n-    >>> from huggingface_hub import hf_hub_download\n-\n-    >>> hf_hub_download(repo_id=\"bigscience/T0_3B\", filename=\"config.json\", cache_dir=\"./your/path/bigscience_t0\")\n-    ```\n-\n-Once your file is downloaded and locally cached, specify it's local path to load and use it:\n-\n-```py\n->>> from transformers import AutoConfig\n-\n->>> config = AutoConfig.from_pretrained(\"./your/path/bigscience_t0/config.json\")\n-```\n-\n-<Tip>\n-\n-See the [How to download files from the Hub](https://huggingface.co/docs/hub/how-to-downstream) section for more details on downloading files stored on the Hub.\n-\n-</Tip>\n-\n-## Troubleshooting\n-\n-See below for some of the more common installation issues and how to resolve them.\n-\n-### Unsupported Python version\n-\n-Ensure you are using Python 3.9 or later. Run the command below to check your Python version.\n-\n-```\n-python --version\n-```\n-\n-### Missing dependencies\n-\n-Install all required dependencies by running the following command. Ensure youâ€™re in the project directory before executing the command.\n-\n-```\n-pip install -r requirements.txt\n+```bash\n+HF_HUB_OFFLINE=1 \\\n+python examples/pytorch/language-modeling/run_clm.py --model_name_or_path meta-llama/Llama-2-7b-hf --dataset_name wikitext ...\n ```\n \n-### Windows-specific\n+Another option for only loading cached files is to set `local_files_only=True` in [`~PreTrainedModel.from_pretrained`].\n \n-If you encounter issues on Windows, you may need to activate Developer Mode. Navigate to Windows Settings > For Developers > Developer Mode.\n-\n-Alternatively, create and activate a virtual environment as shown below.\n+```py\n+from transformers import LlamaForCausalLM\n \n+model = LlamaForCausalLM.from_pretrained(\"./path/to/local/directory\", local_files_only=True)\n ```\n-python -m venv env\n-.\\env\\Scripts\\activate\n-```\n-\n-"
        },
        {
            "sha": "36f82fb3dc9a70173eab6970ac0b776ca8340044",
            "filename": "docs/source/en/kv_cache.md",
            "status": "modified",
            "additions": 235,
            "deletions": 309,
            "changes": 544,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fkv_cache.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fkv_cache.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fkv_cache.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -14,420 +14,346 @@ rendered properly in your Markdown viewer.\n \n -->\n \n-# Best Practices for Generation with Cache\n+# KV cache strategies\n \n-Efficient caching is crucial for optimizing the performance of models in various generative tasks,\n-including text generation, translation, summarization and other transformer-based applications.\n-Effective caching helps reduce computation time and improve response rates, especially in real-time or resource-intensive applications.\n+The key-value (KV) vectors are used to calculate attention scores. For autoregressive models, KV scores are calculated *every* time because the model predicts one token at a time. Each prediction depends on the previous tokens, which means the model performs the same computations each time.\n \n-Transformers support various caching methods, leveraging \"Cache\" classes to abstract and manage the caching logic.\n-This document outlines best practices for using these classes to maximize performance and efficiency.\n-Check out all the available `Cache` classes in the [API documentation](./internal/generation_utils).\n+A KV *cache* stores these calculations so they can be reused without recomputing them. Efficient caching is crucial for optimizing model performance because it reduces computation time and improves response rates. Refer to the [Caching](./cache_explanation.md) doc for a more detailed explanation about how a cache works.\n \n-## What is Cache and why we should care?\n+Transformers offers several [`Cache`] classes that implement different caching mechanisms. Some of these [`Cache`] classes are optimized to save memory while others are designed to maximize generation speed. Refer to the table below to compare cache types and use it to help you select the best cache for your use case.\n \n-Imagine youâ€™re having a conversation with someone, and instead of remembering what was said previously, you have to start from scratch every time you respond. This would be slow and inefficient, right? In the world of Transformer models, a similar concept applies, and that's where Caching keys and values come into play. From now on, I'll refer to the concept as KV Cache.\n+| Cache Type             | Memory Efficient Â | Supports torch.compile() | Initialization Recommended | Latency | Long Context Generation |\n+|------------------------|------------------|--------------------------|----------------------------|---------|-------------------------|\n+| Dynamic Cache          | No               | No                       | No                         | Mid     | No                      |\n+| Static Cache           | No               | Yes                      | Yes                        | High    | No                      |\n+| Offloaded Cache         | Yes              | No                       | No                         | Low     | Yes                     |\n+| Offloaded Static Cache  | No               | Yes                      | Yes                        | High    | Yes                     |\n+| Quantized Cache        | Yes              | No                       | No                         | Low     | Yes                     |\n+| Sliding Window Cache   | No               | Yes                      | Yes                        | High    | No                      |\n+| Sink Cache             | Yes              | No                       | Yes                        | Mid     | Yes                     |\n \n-KV cache is needed to optimize the generation in autoregressive models, where the model predicts text token by token. This process can be slow since the model can generate only one token at a time, and each new prediction is dependent on the previous context. That means, to predict token number 1000 in the generation, you need information from the previous 999 tokens, which comes in the form of some matrix multiplications across the representations of those tokens. But to predict token number 1001, you also need the same information from the first 999 tokens, plus additional information from token number 1000. That is where key-value cache is used to optimize the sequential generation process by storing previous calculations to reuse in subsequent tokens, so they don't need to be computed again.\n+This guide introduces you to the different [`Cache`] classes and shows you how to use them for generation.\n \n-More concretely, key-value cache acts as a memory bank for these generative models, where the model stores key-value pairs derived from self-attention layers for previously processed tokens. By storing this information, the model can avoid redundant computations and instead retrieve keys and values of previous tokens from the cache. Note that caching can be used only in inference and should be disabled when training, otherwise it might cause unexpected errors.\n+## Default cache\n \n-<details>\n-  <summary><em>For the Curious Minds Who Like to Dive Deep</em></summary>\n+The [`DynamicCache`] is the default cache class for most models. It allows the cache size to grow dynamically in order to store an increasing number of keys and values as generation progresses.\n \n-  ### Under the Hood: How Cache Object Works in Attention Mechanism\n+Disable the cache by configuring `use_cache=False` in [`~GenerationMixin.generate`].\n \n-  When utilizing a cache object in the input, the Attention module performs several critical steps to integrate past and present information seamlessly.\n+```py\n+import torch\n+from transformers import AutoTokenizer, AutoModelForCausalLM\n \n-  The Attention module concatenates the current key-values with the past key-values stored in the cache. This results in attention weights of shape `(new_tokens_length, past_kv_length + new_tokens_length)`. Essentially, the past and current key-values are combined to compute attention scores, ensuring that the model considers both previous context and new input. The concatenated key-values are used to compute the attention scores resulting in attention weights of shape `(new_tokens_length, past_kv_length + new_tokens_length)`.\n+tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n+model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", torch_dtype=torch.float16).to(\"cuda:0\")\n+inputs = tokenizer(\"I like rock music because\", return_tensors=\"pt\").to(model.device)\n \n-  Therefore, when iteratively calling `forward()` instead of the `generate()` method, itâ€™s crucial to ensure that the attention mask shape matches the combined length of past and current key-values. The attention mask should have the shape `(batch_size, past_kv_length + new_tokens_length)`. This is usually handled internally when you call `generate()` method. If you want to implement your own generation loop with Cache classes, take this into consideration and prepare the attention mask to hold values to current and past tokens.\n+model.generate(**inputs, do_sample=False, max_new_tokens=20, use_cache=False)\n+```\n \n-  <Tip warning={true}>\n+Cache classes can also be initialized first before calling and passing it to the models [past_key_values](https://hf.co/docs/transformers/internal/generation_utils#transformers.generation.GenerateDecoderOnlyOutput.past_key_values) parameter. This cache initialization strategy is only recommended for some cache types.\n \n-  One important concept you need to know when writing your own generation loop, is `cache_position`. In case you want to reuse an already filled Cache object by calling `forward()`, you have to pass in a valid `cache_position` which will indicate the positions of inputs in the sequence. Note that `cache_position` is not affected by padding, and always adds one more position for each token. For example, if key/value cache contains 10 tokens (no matter how many of it is a pad token), the cache position for the next token should be `torch.tensor([10])`.\n+In most other cases, it's easier to define the cache strategy in the [cache_implementation](https://hf.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig.cache_implementation) parameter.\n \n-  </Tip>\n+```py\n+import torch\n+from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache\n \n+tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n+model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", torch_dtype=torch.float16).to(\"cuda:0\")\n+inputs = tokenizer(\"I like rock music because\", return_tensors=\"pt\").to(model.device)\n \n-  See an example below for how to implement your own generation loop.\n+past_key_values = DynamicCache()\n+out = model.generate(**inputs, do_sample=False, max_new_tokens=20, past_key_values=past_key_values)\n+```\n \n-  ```python\n-  >>> import torch\n-  >>> from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache\n+## Memory efficient caches\n \n-  >>> model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n-  >>> model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\")\n-  >>> tokenizer = AutoTokenizer.from_pretrained(model_id)\n+The KV cache can occupy a significant portion of memory and become a [bottleneck](https://hf.co/blog/llama31#inference-memory-requirements) for long-context generation. Memory efficient caches focus on trading off speed for reduced memory usage. This is especially important for large language models (LLMs) and if your hardware is memory constrained.\n \n-  >>> past_key_values = DynamicCache()\n-  >>> messages = [{\"role\": \"user\", \"content\": \"Hello, what's your name.\"}]\n-  >>> inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\", return_dict=True).to(model.device)\n+### Offloaded cache\n \n-  >>> generated_ids = inputs.input_ids\n-  >>> cache_position = torch.arange(inputs.input_ids.shape[1], dtype=torch.int64, device=model.device)\n-  >>> max_new_tokens = 10\n+The [`OffloadedCache`] saves GPU memory by moving the KV cache for most model layers to the CPU. Only the current layer cache is maintained on the GPU during a models `forward` iteration over the layers. [`OffloadedCache`] asynchronously prefetches the next layer cache and sends the previous layer cache back to the CPU.\n \n-  >>> for _ in range(max_new_tokens):\n-  ...     outputs = model(**inputs, cache_position=cache_position, past_key_values=past_key_values, use_cache=True)\n-  ...     # Greedily sample one next token\n-  ...     next_token_ids = outputs.logits[:, -1:].argmax(-1)\n-  ...     generated_ids = torch.cat([generated_ids, next_token_ids], dim=-1)\n-  ...\n-  ...     # Prepare inputs for the next generation step by leaaving unprocessed tokens, in our case we have only one new token\n-  ...     # and expanding attn mask for the new token, as explained above\n-  ...     attention_mask = inputs[\"attention_mask\"]\n-  ...     attention_mask = torch.cat([attention_mask, attention_mask.new_ones((attention_mask.shape[0], 1))], dim=-1)\n-  ...     inputs = {\"input_ids\": next_token_ids, \"attention_mask\": attention_mask}\n-  ...     cache_position = cache_position[-1:] + 1 # add one more position for the next token\n+This cache strategy always generates the same result as [`DynamicCache`] and works as a drop-in replacement or fallback. You may want to use [`OffloadedCache`] if you have a GPU and you're getting out-of-memory (OOM) errors.\n \n-  >>> print(tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0])\n-  ```\n-  ```txt\n-  <|user|>\n-  Hello, what's your name. \n-  <|assistant|>\n-  My name is Sarah. \n-  <|\n-  ```\n+> [!WARNING]\n+> You may notice a small degradation in generation throughput compared to [`DynamicCache`] depending on your model and generation choices (context size, number of generated tokens, number of beams, etc.).\n \n-</details>\n+Enable [`OffloadedCache`] by configuring `cache_implementation=\"offloaded\"` in either [`GenerationConfig`] or [`~GenerationMixin.generate`].\n \n+```py\n+import torch\n+from transformers import AutoTokenizer, AutoModelForCausalLM\n \n+ckpt = \"microsoft/Phi-3-mini-4k-instruct\"\n+tokenizer = AutoTokenizer.from_pretrained(ckpt)\n+model = AutoModelForCausalLM.from_pretrained(ckpt, torch_dtype=torch.float16).to(\"cuda:0\")\n+inputs = tokenizer(\"Fun fact: The shortest\", return_tensors=\"pt\").to(model.device)\n \n-## Generate with Cache\n+out = model.generate(**inputs, do_sample=False, max_new_tokens=23, cache_implementation=\"offloaded\")\n+print(tokenizer.batch_decode(out, skip_special_tokens=True)[0])\n+Fun fact: The shortest war in history was between Britain and Zanzibar on August 27, 1896.\n+```\n \n-In ðŸ¤— Transformers, we support various Cache types to optimize the performance across different models and tasks. By default, all models generate with caching,\n-with the [`~DynamicCache`] class being the default cache for most models. It allows us to dynamically grow cache size, by saving more and more keys and values as we generate. If for some reason you don't want to use caches, you can pass `use_cache=False` into the `generate()` method.\n+The example below shows how you can fallback on [`OffloadedCache`] if you run out of memory.\n+\n+```py\n+import torch\n+from transformers import AutoTokenizer, AutoModelForCausalLM\n+\n+def resilient_generate(model, *args, **kwargs):\n+    oom = False\n+    try:\n+        return model.generate(*args, **kwargs)\n+    except torch.cuda.OutOfMemoryError as e:\n+        print(e)\n+        print(\"retrying with cache_implementation='offloaded'\")\n+        oom = True\n+    if oom:\n+        torch.cuda.empty_cache()\n+        kwargs[\"cache_implementation\"] = \"offloaded\"\n+        return model.generate(*args, **kwargs)\n+\n+ckpt = \"microsoft/Phi-3-mini-4k-instruct\"\n+tokenizer = AutoTokenizer.from_pretrained(ckpt)\n+model = AutoModelForCausalLM.from_pretrained(ckpt, torch_dtype=torch.float16).to(\"cuda:0\")\n+prompt = [\"okay \"*1000 + \"Fun fact: The most\"]\n+inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n+beams = { \"num_beams\": 40, \"num_beam_groups\": 40, \"num_return_sequences\": 40, \"diversity_penalty\": 1.0, \"max_new_tokens\": 23, \"early_stopping\": True, }\n+out = resilient_generate(model, **inputs, **beams)\n+responses = tokenizer.batch_decode(out[:,-28:], skip_special_tokens=True)\n+```\n \n-Refer to the table below to see the difference between cache types and choose the one that suits best for your use-case. Models for which initialization is recommended should be initialized before calling the model and passed to model as a kwarg. In all other cases you can simply define desired `cache_implementation` and we take care of the rest for you.\n+### Quantized cache\n \n-| Cache Type             | Memory Efficient | Supports torch.compile() | Initialization Recommended | Latency | Long Context Generation |\n-|------------------------|------------------|--------------------------|----------------------------|---------|-------------------------|\n-| Dynamic Cache          | No               | No                       | No                         | Mid     | No                      |\n-| Static Cache           | No               | Yes                      | Yes                        | High    | No                      |\n-| Offloaded Cache        | Yes              | No                       | No                         | Low     | Yes                     |\n-| Offloaded Static Cache | No               | Yes                      | Yes                        | High    | Yes                     |\n-| Quantized Cache        | Yes              | No                       | No                         | Low     | Yes                     |\n-| Sliding Window Cache   | No               | Yes                      | Yes                        | High    | No                      |\n-| Sink Cache             | Yes              | No                       | Yes                        | Mid     | Yes                     |\n+The [`QuantizedCache`] reduces memory requirements by quantizing the KV values to a lower precision. [`QuantizedCache`] currently supports two quantization backends.\n \n+- [`HQQQuantizedCache`] supports int2, int4, and int8 datatypes.\n+- [`QuantoQuantizedCache`] supports int2 and int4 datatypes. This is the default quantization backend.\n \n-These cache classes can be set with a `cache_implementation` argument when generating. To learn about the available options for the cache_implementation flag, please refer to the [API Documentation](./main_classes/text_generation#transformers.GenerationConfig). Now, let's explore each cache type in detail and see how to use them. Note that the below examples are for decoder-only Tranformer-based models. We also support [\"Model-Specific Cache\"] classes for models such as Mamba or Jamba, keep reading for more details.\n+> [!WARNING]\n+> Quantizing the cache can harm latency if the context length is short and there is enough GPU memory available for generation without enabling cache quantization. Try to find a balance between memory efficiency and latency.\n \n-### Quantized Cache\n+Enable [`QuantizedCache`] by configuring `cache_implementation=\"quantized\"` in [`GenerationConfig`], and indicate the quantization backend in [`QuantizedCacheConfig`]. Any additional quantization related parameters should also be passed either as a dict or an instance of [`QuantizedCacheConfig`]. You should use the default values for these additional parameters unless you're running out-of-memory. In that case, consider decreasing the residual length.\n \n-The key and value cache can occupy a large portion of memory, becoming a [bottleneck for long-context generation](https://huggingface.co/blog/llama31#inference-memory-requirements), especially for Large Language Models.\n-Quantizing the cache when using `generate()` can significantly reduce memory requirements at the cost of speed.\n+<hfoptions id=\"quantized-cache\">\n+<hfoption id=\"HQQQuantizedCache\">\n \n-KV Cache quantization in `transformers` is largely inspired by the paper [\"KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache\"](https://arxiv.org/abs/2402.02750) and currently supports [`~QuantoQuantizedCache`] and [`~HQQQuantizedCache`] classes. For more information on the inner workings see the paper.\n+For [`HQQQuantizedCache`], we recommend setting the `axis-key` and `axis-value` parameters to `1`.\n \n-To enable quantization of the key-value cache, one needs to indicate `cache_implementation=\"quantized\"` in the `generation_config`.\n-Quantization related arguments should be passed to the `generation_config` either as a `dict` or an instance of a [`~QuantizedCacheConfig`] class.\n-One has to indicate which quantization backend to use in the [`~QuantizedCacheConfig`], the default is `quanto`.\n+```py\n+from transformers import AutoTokenizer, AutoModelForCausalLM, HQQQuantizedCache, QuantizedCacheConfig\n \n-It is recommended to set `axis-key/axis-value` parameters in the cache config to `0` if you're using the `quanto` backend and to `1` if you're using the `HQQ` backend. For other config values, please use the defaults unless you're running out of memory. In that case, you may consider decreasing the residual length.\n+tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n+model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", torch_dtype=torch.float16).to(\"cuda:0\")\n+inputs = tokenizer(\"I like rock music because\", return_tensors=\"pt\").to(model.device)\n \n-<Tip warning={true}>\n+out = model.generate(**inputs, do_sample=False, max_new_tokens=20, cache_implementation=\"quantized\", cache_config={\"axis-key\": 1, \"axis-value\": 1, \"backend\": \"hqq\"})\n+print(tokenizer.batch_decode(out, skip_special_tokens=True)[0])\n+I like rock music because it's loud and energetic. It's a great way to express myself and rel\n+```\n \n-Cache quantization can be detrimental in terms of latency if the context length is short and there is enough GPU VRAM available to run without cache quantization. It is recommended to seek balance between memory efficiency and latency.\n-</Tip>\n+</hfoption>\n+<hfoption id=\"Quanto\">\n \n+For [`QuantoQuantizedCache`], we recommend setting the `axis-key` and `axis-value` parameters to `0`.\n \n-```python\n->>> import torch\n->>> from transformers import AutoTokenizer, AutoModelForCausalLM\n+```py\n+from transformers import AutoTokenizer, AutoModelForCausalLM, QuantoQuantizedCache, QuantizedCacheConfig\n \n->>> tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n->>> model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", torch_dtype=torch.float16, device_map=\"auto\")\n->>> inputs = tokenizer(\"I like rock music because\", return_tensors=\"pt\").to(model.device)\n+tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n+model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", torch_dtype=torch.float16).to(\"cuda:0\")\n+inputs = tokenizer(\"I like rock music because\", return_tensors=\"pt\").to(model.device)\n \n->>> out = model.generate(**inputs, do_sample=False, max_new_tokens=20, cache_implementation=\"quantized\", cache_config={\"nbits\": 4, \"backend\": \"quanto\"})\n->>> print(tokenizer.batch_decode(out, skip_special_tokens=True)[0])\n-I like rock music because it's a great way to express myself. I like the way it makes me feel, the\n+out = model.generate(**inputs, do_sample=False, max_new_tokens=20, cache_implementation=\"quantized\", cache_config={\"nbits\": 4, \"axis-key\": 0, \"axis-value\": 0, \"backend\": \"quanto\"})\n+print(tokenizer.batch_decode(out, skip_special_tokens=True)[0])\n+I like rock music because it's loud and energetic. It's a great way to express myself and rel\n ```\n \n-### Offloaded Cache\n+</hfoption>\n+</hfoptions>\n \n-Similarly to KV cache quantization, [`~OffloadedCache`] strategy aims to reduce GPU VRAM usage.\n-It does so by moving the KV cache for most layers to the CPU.\n-As the model's `forward()` method iterates over the layers, this strategy maintains the current layer cache on the GPU.\n-At the same time it asynchronously prefetches the next layer cache as well as sending the previous layer cache back to the CPU.\n-Unlike KV cache quantization, this strategy always produces the same result as the default KV cache implementation.\n-Thus, it can serve as a drop-in replacement or a fallback for it.\n+### Sink cache\n \n-Depending on your model and the characteristics of your generation task (size of context, number of generated tokens, number of beams, etc.)\n-you may notice a small degradation in generation throughput compared to the default KV cache implementation.\n+[`SinkCache`] is capable of generating very long sequences (\"infinite length\" according to the paper) by only retaining a few initial tokens from the sequence. These are called the *sink tokens* because they account for a significant portion of the attention scores during generation. Subsequent tokens are discarded on a sliding windowed basis, and only the latest `window_size` tokens are kept. This means most of the previous knowledge is discarded.\n \n-To enable KV cache offloading, pass `cache_implementation=\"offloaded\"` in the `generation_config` or directly to the `generate()` call.\n-Use `cache_implementation=\"offloaded_static\"` for an offloaded static cache (see also [Offloaded Static Cache](#offloaded-static-cache) below).\n+The sink tokens allow a model to maintain stable performance even when it's dealing with very long text sequences.\n \n-```python\n->>> import torch\n->>> from transformers import AutoTokenizer, AutoModelForCausalLM\n->>> ckpt = \"microsoft/Phi-3-mini-4k-instruct\"\n+Enable [`SinkCache`] by initializing it first with the [window_length](https://hf.co/docs/transformers/main/en/internal/generation_utils#transformers.SinkCache.window_length) and [num_sink_tokens](https://hf.co/docs/transformers/main/en/internal/generation_utils#transformers.SinkCache.num_sink_tokens) parameters before passing it to [past_key_values](https://hf.co/docs/transformers/internal/generation_utils#transformers.generation.GenerateDecoderOnlyOutput.past_key_values) in [`~GenerationMixin.generate`].\n \n->>> tokenizer = AutoTokenizer.from_pretrained(ckpt)\n->>> model = AutoModelForCausalLM.from_pretrained(ckpt, torch_dtype=torch.float16, device_map=\"auto\")\n->>> inputs = tokenizer(\"Fun fact: The shortest\", return_tensors=\"pt\").to(model.device)\n-\n->>> out = model.generate(**inputs, do_sample=False, max_new_tokens=23, cache_implementation=\"offloaded\")\n->>> print(tokenizer.batch_decode(out, skip_special_tokens=True)[0])\n-Fun fact: The shortest war in history was between Britain and Zanzibar on August 27, 1896.\n+```py\n+import torch\n+from transformers import AutoTokenizer, AutoModelForCausalLM, SinkCache\n \n->>> out = model.generate(**inputs, do_sample=False, max_new_tokens=23)\n->>> print(tokenizer.batch_decode(out, skip_special_tokens=True)[0])\n-Fun fact: The shortest war in history was between Britain and Zanzibar on August 27, 1896.\n-```\n+tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n+model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", torch_dtype=torch.float16).to(\"cuda:0\")\n+inputs = tokenizer(\"This is a long story about unicorns, fairies and magic.\", return_tensors=\"pt\").to(model.device)\n \n-<Tip warning={true}>\n-\n-Cache offloading requires a CUDA GPU and can be slower than dynamic KV cache. Use it if you are getting CUDA out of memory errors.\n-\n-</Tip>\n-\n-The example below shows how KV cache offloading can be used as a fallback strategy.\n-```python\n->>> import torch\n->>> from transformers import AutoTokenizer, AutoModelForCausalLM\n->>> def resilient_generate(model, *args, **kwargs):\n-...     oom = False\n-...     try:\n-...         return model.generate(*args, **kwargs)\n-...     except torch.cuda.OutOfMemoryError as e:\n-...         print(e)\n-...         print(\"retrying with cache_implementation='offloaded'\")\n-...         oom = True\n-...     if oom:\n-...         torch.cuda.empty_cache()\n-...         kwargs[\"cache_implementation\"] = \"offloaded\"\n-...         return model.generate(*args, **kwargs)\n-...\n-...\n->>> ckpt = \"microsoft/Phi-3-mini-4k-instruct\"\n->>> tokenizer = AutoTokenizer.from_pretrained(ckpt)\n->>> model = AutoModelForCausalLM.from_pretrained(ckpt, torch_dtype=torch.float16).to(\"cuda:0\")\n->>> prompt = [\"okay \"*1000 + \"Fun fact: The most\"]\n->>> inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n->>> beams = { \"num_beams\": 40, \"num_beam_groups\": 40, \"num_return_sequences\": 40, \"diversity_penalty\": 1.0, \"max_new_tokens\": 23, \"early_stopping\": True, }\n->>> out = resilient_generate(model, **inputs, **beams)\n->>> responses = tokenizer.batch_decode(out[:,-28:], skip_special_tokens=True)\n+past_key_values = SinkCache(window_length=256, num_sink_tokens=4)\n+out = model.generate(**inputs, do_sample=False, max_new_tokens=30, past_key_values=past_key_values)\n+tokenizer.batch_decode(out, skip_special_tokens=True)[0]\n+\"This is a long story about unicorns, fairies and magic. It is a fantasy world where unicorns and fairies live together in harmony. The story follows a young girl named Lily\"\n ```\n \n-On a GPU with 50 GB of RAM, running this code will print\n-```\n-CUDA out of memory. Tried to allocate 4.83 GiB. GPU\n-retrying with cache_implementation='offloaded'\n-```\n-before successfully generating 40 beams.\n+## Speed optimized caches\n \n+The default [`DynamicCache`] prevents you from taking advantage of just-in-time (JIT) optimizations because the cache size isn't fixed. JIT optimizations enable you to maximize latency at the expense of memory usage. All of the following cache types are compatible with JIT optimizations like [torch.compile](./llm_optims#static-kv-cache-and-torchcompile) to accelerate generation.\n \n-### Static Cache\n+### Static cache\n \n-Since the \"DynamicCache\" dynamically grows with each generation step, it prevents you from taking advantage of JIT optimizations. The [`~StaticCache`] pre-allocates\n-a specific maximum size for the keys and values, allowing you to generate up to the maximum length without having to modify cache size. Check the below usage example.\n+A [`StaticCache`] pre-allocates a specific maximum cache size for the kv pairs. You can generate up to the maximum cache size without needing to modify it.\n \n-For more examples with Static Cache and JIT compilation, take a look at [StaticCache & torchcompile](./llm_optims#static-kv-cache-and-torchcompile)\n+Enable [`StaticCache`] by configuring `cache_implementation=\"static\"` in [`~GenerationMixin.generate`].\n \n-```python\n->>> import torch\n->>> from transformers import AutoTokenizer, AutoModelForCausalLM\n+```py\n+import torch\n+from transformers import AutoTokenizer, AutoModelForCausalLM\n \n->>> tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n->>> model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", torch_dtype=torch.float16, device_map=\"auto\")\n->>> inputs = tokenizer(\"Hello, my name is\", return_tensors=\"pt\").to(model.device)\n+tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n+model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", torch_dtype=torch.float16, device_map=\"auto\")\n+inputs = tokenizer(\"Hello, my name is\", return_tensors=\"pt\").to(model.device)\n \n->>> # simply pass the cache implementation=\"static\"\n->>> out = model.generate(**inputs, do_sample=False, max_new_tokens=20, cache_implementation=\"static\")\n->>> tokenizer.batch_decode(out, skip_special_tokens=True)[0]\n-\"Hello, my name is [Your Name] and I am a [Your Position] at [Your Company]. I am writing\"\n+out = model.generate(**inputs, do_sample=False, max_new_tokens=20, cache_implementation=\"static\")\n+tokenizer.batch_decode(out, skip_special_tokens=True)[0]\n+\"Hello, my name is [Your Name], and I am a [Your Profession] with [Number of Years] of\"\n ```\n \n+### Offloaded static cache\n \n-## Offloaded Static Cache\n+The [`OffloadedStaticCache`] is very similar to the [OffloadedCache](#offloaded-cache) except the cache size is set to a maximum cache size. Otherwise, [`OffloadedStaticCache`] only keeps the current layer cache on the GPU and the rest are moved to the CPU.\n \n-Like [`~OffloadedCache`] exists for offloading a \"DynamicCache\", there is also an offloaded static cache. It fully supports\n-JIT optimizations. Just pass `cache_implementation=\"offloaded_static\"` in the `generation_config` or directly to the `generate()` call.\n-This will use the [`~OffloadedStaticCache`] implementation instead.\n+Enable [`OffloadedStaticCache`] by configuring `cache_implementation=\"offloaded_static\"` in [`~GenerationMixin.generate`].\n \n-```python\n->>> import torch\n->>> from transformers import AutoTokenizer, AutoModelForCausalLM\n+```py\n+import torch\n+from transformers import AutoTokenizer, AutoModelForCausalLM\n \n->>> tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n->>> model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", torch_dtype=torch.float16, device_map=\"auto\")\n->>> inputs = tokenizer(\"Hello, my name is\", return_tensors=\"pt\").to(model.device)\n+tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n+model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", torch_dtype=torch.float16, device_map=\"auto\")\n+inputs = tokenizer(\"Hello, my name is\", return_tensors=\"pt\").to(model.device)\n \n->>> # simply pass the cache implementation=\"offloaded_static\"\n->>> out = model.generate(**inputs, do_sample=False, max_new_tokens=20, cache_implementation=\"offloaded_static\")\n->>> tokenizer.batch_decode(out, skip_special_tokens=True)[0]\n+out = model.generate(**inputs, do_sample=False, max_new_tokens=20, cache_implementation=\"offloaded_static\")\n+tokenizer.batch_decode(out, skip_special_tokens=True)[0]\n \"Hello, my name is [Your Name], and I am a [Your Profession] with [Number of Years] of\"\n ```\n Cache offloading requires a CUDA GPU.\n \n+### Sliding window cache\n \n-### Sliding Window Cache\n+[`SlidingWindowCache`] implements a sliding window over the previous kv pairs, and only keeps the last `sliding_window` tokens. This cache type is designed to only work with models that support *sliding window attention*, such as [Mistral](./model_doc/mistral). Older kv states are discarded and replaced by new kv states.\n \n-As the name suggests, this cache type implements a sliding window over previous keys and values, retaining only the last `sliding_window` tokens. It should be used with models like Mistral that support sliding window attention. Additionally, similar to Static Cache, this one is JIT-friendly and can be used with the same compile tecniques as Static Cache.\n+Enable [`SlidingWindowCache`] by configuring `cache_implementation=\"sliding_window\"` in [`~GenerationMixin.generate`].\n \n-Note that you can use this cache only for models that support sliding window, e.g. Mistral models.\n+```py\n+import torch\n+from transformers import AutoTokenizer, AutoModelForCausalLM, SinkCache\n \n+tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n+model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", torch_dtype=torch.float16).to(\"cuda:0\")\n+inputs = tokenizer(\"Yesterday I was on a rock concert and.\", return_tensors=\"pt\").to(model.device)\n \n-```python\n->>> import torch\n->>> from transformers import AutoTokenizer, AutoModelForCausalLM, SinkCache\n-\n->>> tokenizer = AutoTokenizer.from_pretrained(\"teknium/OpenHermes-2.5-Mistral-7B\")\n->>> model = AutoModelForCausalLM.from_pretrained(\"teknium/OpenHermes-2.5-Mistral-7B\", torch_dtype=torch.float16, device_map=\"auto\")\n->>> inputs = tokenizer(\"Yesterday I was on a rock concert and.\", return_tensors=\"pt\").to(model.device)\n-\n->>> # can be used by passing in cache implementation\n->>> out = model.generate(**inputs, do_sample=False, max_new_tokens=30, cache_implementation=\"sliding_window\")\n->>> tokenizer.batch_decode(out, skip_special_tokens=True)[0]\n-\"Yesterday I was on a rock concert and. I was so excited to see my favorite band perform live. I was so happy that I could hardly contain myself. I was jumping up and down and\"\n+out = model.generate(**inputs, do_sample=False, max_new_tokens=30, cache_implementation=\"sliding_window\")\n+tokenizer.batch_decode(out, skip_special_tokens=True)[0]\n ```\n \n-### Sink Cache\n-\n-Sink Cache was introduced in [\"Efficient Streaming Language Models with Attention Sinks\"](https://arxiv.org/abs/2309.17453). It allows you to generate long sequences of text (\"infinite length\" according to the paper) without any fine-tuning. That is achieved by smart handling of previous keys and values, specifically it retains a few initial tokens from the sequence, called \"sink tokens\". This is based on the observation that these initial tokens attract a significant portion of attention scores during the generation process. Tokens that come after \"sink tokens\" are discarded on a sliding windowed basis, keeping only the latest `window_size` tokens. By keeping these initial tokens as \"attention sinks,\" the model maintains stable performance even when dealing with very long texts, thus discarding most of the previous knowledge.\n-\n-Unlike other cache classes, this one can't be used directly by indicating a `cache_implementation`. You have to initialize the Cache before calling on `generate()` as follows.\n-\n-```python\n->>> import torch\n->>> from transformers import AutoTokenizer, AutoModelForCausalLM, SinkCache\n+## Model caches\n \n->>> tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n->>> model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", torch_dtype=torch.float16, device_map=\"auto\")\n->>> inputs = tokenizer(\"This is a long story about unicorns, fairies and magic.\", return_tensors=\"pt\").to(model.device)\n+Some model types, like encoder-decoder models or [Gemma2](./model_doc/gemma2) and [Mamba](./model_doc/mamba), have dedicated cache classes.\n \n->>> # get our cache, specify number of sink tokens and window size\n->>> # Note that window size already includes sink tokens, so has to be larger\n->>> past_key_values = SinkCache(window_length=256, num_sink_tokens=4)\n->>> out = model.generate(**inputs, do_sample=False, max_new_tokens=30, past_key_values=past_key_values)\n->>> tokenizer.batch_decode(out, skip_special_tokens=True)[0]\n-\"This is a long story about unicorns, fairies and magic. It is a story about a young girl named Lily who discovers that she has the power to control the elements. She learns that she can\"\n-```\n-\n-### Encoder-Decoder Cache\n+### Encoder-decoder cache\n \n-The [`~EncoderDecoderCache`] is a wrapper designed to handle the caching needs of encoder-decoder models. This cache type is specifically built to manage both self-attention and cross-attention caches, ensuring storage and retrieval of past key/values required for these complex models. Cool thing about Encoder-Decoder Cache is that you can set different cache types for the encoder and for the decoder, depending on your use case. Currently this cache is only supported in [Whisper](./model_doc/whisper) models but we will be adding more models soon.\n+[`EncoderDecoderCache`] is designed for encoder-decoder models. It manages both the self-attention and cross-attention caches to ensure storage and retrieval of previous kv pairs. It is possible to individually set a different cache type for the encoder and decoder.\n \n-In terms of usage, there is nothing special to be done and calling `generate()` or `forward()` will handle everything for you.\n+This cache type doesn't require any setup. It can be used when calling [`~GenerationMixin.generate`] or a models `forward` method.\n \n+> [!TIP]\n+> The [`EncoderDecoderCache`] currently only supports [Whisper](./model_doc/whisper).\n \n-### Model-specific Cache Classes\n+### Model-specific caches\n \n-Some models require storing previous keys, values, or states in a specific way, and the above cache classes cannot be used. For such cases, we have several specialized cache classes that are designed for specific models. These models only accept their own dedicated cache classes and do not support using any other cache types. Some examples include [`~HybridCache`] for [Gemma2](./model_doc/gemma2) series models or [`~MambaCache`] for [Mamba](./model_doc/mamba) architecture models.\n+Some models have a unique way of storing past kv pairs or states that is not compatible with any other cache classes.\n \n+[Gemma2](./model_doc/gemma2) requires [`HybridCache`], which uses a combination of [`SlidingWindowCache`] for sliding window attention and [`StaticCache`] for global attention under the hood.\n \n-## Iterative Generation with Cache\n+[Mamba](./model_doc/mamba) requires [`MambaCache`] because the model doesn't have an attention mechanism or kv states.\n \n-We have seen how to use each of the cache types when generating. What if you want to use cache in iterative generation setting, for example in applications like chatbots, where interactions involve multiple turns and continuous back-and-forth exchanges. Iterative generation with cache allows these systems to handle ongoing conversations effectively without reprocessing the entire context at each step. But there are some tips that you should know before you start implementing:\n+## Iterative generation\n \n-The general format when doing iterative generation is as below. First you have to initialize an empty cache of the type you want, and you can start feeding in new prompts iteratively. Keeping track of dialogues history and formatting can be done with chat templates, read more on that in [chat_templating](./chat_templating)\n+A cache can also work in iterative generation settings where there is back-and-forth interaction with a model (chatbots). Like regular generation, iterative generation with a cache allows a model to efficiently handle ongoing conversations without recomputing the entire context at each step.\n \n-In case you are using Sink Cache, you have to crop your inputs to that maximum length because Sink Cache can generate text longer than its maximum window size, but it expects the first input to not exceed the maximum cache length.\n+For iterative generation with a cache, start by initializing an empty cache class and then you can feed in your new prompts. Keep track of dialogue history with a [chat template](./chat_templating).\n \n+If you're using [`SinkCache`], the inputs need to be truncated to the maximum length because [`SinkCache`] can generate text that exceeds its maximum window size. However, the first input shouldn't exceed the maximum cache length.\n \n-```python\n->>> import torch\n->>> from transformers import AutoTokenizer,AutoModelForCausalLM\n->>> from transformers.cache_utils import (\n-...    DynamicCache,\n-...    SinkCache,\n-...    StaticCache,\n-...    SlidingWindowCache,\n-...    QuantoQuantizedCache,\n-...    QuantizedCacheConfig,\n-... )\n+The example below demonstrates how to use a cache for iterative generation.\n \n->>> model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n->>> model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map='auto')\n->>> tokenizer = AutoTokenizer.from_pretrained(model_id)\n+```py\n+import torch\n+from transformers import AutoTokenizer,AutoModelForCausalLM\n+from transformers.cache_utils import (\n+    DynamicCache,\n+    SinkCache,\n+    StaticCache,\n+    SlidingWindowCache,\n+    QuantoQuantizedCache,\n+    QuantizedCacheConfig,\n+)\n \n->>> user_prompts = [\"Hello, what's your name?\", \"Btw, yesterday I was on a rock concert.\"]\n+model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n+model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map='auto')\n+tokenizer = AutoTokenizer.from_pretrained(model_id)\n \n->>> past_key_values = DynamicCache()\n->>> max_cache_length = past_key_values.get_max_cache_shape()\n+user_prompts = [\"Hello, what's your name?\", \"Btw, yesterday I was on a rock concert.\"]\n \n->>> messages = []\n->>> for prompt in user_prompts:\n-...     messages.append({\"role\": \"user\", \"content\": prompt})\n-...     inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\", return_dict=True).to(model.device)\n-...     if isinstance(past_key_values, SinkCache):\n-...         inputs = {k: v[:, -max_cache_length:] for k, v in inputs.items()}\n-...\n-...     input_length = inputs[\"input_ids\"].shape[1]\n-...\n-...     outputs = model.generate(**inputs, do_sample=False, max_new_tokens=256, past_key_values=past_key_values)\n-...     completion = tokenizer.decode(outputs[0, input_length: ], skip_special_tokens=True)\n-...     messages.append({\"role\": \"assistant\", \"content\": completion})\n+past_key_values = DynamicCache()\n+max_cache_length = past_key_values.get_max_length()\n \n-print(messages)\n-[{'role': 'user', 'content': \"Hello, what's your name?\"}, {'role': 'assistant', 'content': \"Hello, I'm AI.\"}, {'role': 'user', 'content': 'Btw, yesterday I was on a rock concert.'}, {'role': 'assistant', 'content': \"I'm sorry to hear that you were on a rock concert yesterday. It sounds like a fun experience, but I'm not capable of experiencing music or concerts. However, I can provide you with some information about rock music and its history. Rock music emerged in the 1950s and 1960s in the United States and Britain, and it quickly gained popularity around the world. Some of the most famous rock bands of all time include The Beatles, The Rolling Stones, Led Zeppelin, and Pink Floyd. Rock music has a distinct sound and style, with elements of blues, country, and folk music. It often features guitar solos, heavy bass lines, and drums. Rock music has had a significant impact on popular culture, influencing genres such as punk rock, heavy metal, and alternative rock.\"}]\n+messages = []\n+for prompt in user_prompts:\n+    messages.append({\"role\": \"user\", \"content\": prompt})\n+    inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\", return_dict=True).to(model.device)\n+    if isinstance(past_key_values, SinkCache):\n+        inputs = {k: v[:, -max_cache_length:] for k, v in inputs.items()}\n+    input_length = inputs[\"input_ids\"].shape[1]\n+    outputs = model.generate(**inputs, do_sample=False, max_new_tokens=256, past_key_values=past_key_values)\n+    completion = tokenizer.decode(outputs[0, input_length: ], skip_special_tokens=True)\n+    messages.append({\"role\": \"assistant\", \"content\": completion})\n ```\n \n+## Prefill a cache\n \n-## Re-use Cache to continue generation\n-\n-Sometimes you would want to first fill-in cache object with key/values for certain prefix prompt and re-use it several times to generate different sequences from it. In that case you can construct a `Cache` object that will hold the instruction prompt, and re-use it several times with different text sequences.\n-\n-```python\n->>> import copy\n->>> import torch\n->>> from transformers import AutoModelForCausalLM, AutoTokenizer, DynamicCache, StaticCache\n->>> from accelerate.test_utils.testing import get_backend\n-\n->>> DEVICE, _, _ = get_backend() # automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)\n->>> model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n->>> model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=DEVICE)\n->>> tokenizer = AutoTokenizer.from_pretrained(model_id)\n-\n->>> # Init StaticCache with big enough max-length (1024 tokens for the below example)\n->>> # You can also init a DynamicCache, if that suits you better\n->>> prompt_cache = StaticCache(config=model.config, max_batch_size=1, max_cache_len=1024, device=DEVICE, dtype=torch.bfloat16)\n-\n->>> INITIAL_PROMPT = \"You are a helpful assistant. \"\n->>> inputs_initial_prompt = tokenizer(INITIAL_PROMPT, return_tensors=\"pt\").to(DEVICE)\n->>> # This is the common prompt cached, we need to run forward without grad to be abel to copy\n->>> with torch.no_grad():\n-...      prompt_cache = model(**inputs_initial_prompt, past_key_values = prompt_cache).past_key_values\n-\n->>> prompts = [\"Help me to write a blogpost about travelling.\", \"What is the capital of France?\"]\n->>> responses = []\n->>> for prompt in prompts:\n-...     new_inputs = tokenizer(INITIAL_PROMPT + prompt, return_tensors=\"pt\").to(DEVICE)\n-...     past_key_values = copy.deepcopy(prompt_cache)\n-...     outputs = model.generate(**new_inputs, past_key_values=past_key_values,max_new_tokens=20)\n-...     response = tokenizer.batch_decode(outputs)[0]\n-...     responses.append(response)\n-\n->>> print(responses)\n-['<s> You are a helpful assistant. Help me to write a blogpost about travelling.  I am excited to share my experiences with you.  I have been traveling for the past', '<s> You are a helpful assistant. What is the capital of France? \\n\\nAnswer: Paris is the capital of France.</s>']\n-```\n+In some situations, you may want to fill a [`Cache`] with kv pairs for a certain prefix prompt and reuse it to generate different sequences.\n \n+The example below initializes a [`StaticCache`], and then caches an initial prompt. Now you can generate several sequences from the prefilled prompt.\n \n-## Legacy cache format\n+```py\n+import copy\n+import torch\n+from transformers import AutoModelForCausalLM, AutoTokenizer, DynamicCache, StaticCache\n \n-Prior to the introduction of the `Cache` object, the cache of LLMs used to be a tuple of tuples of tensors. The legacy\n-format has a dynamic size, growing as we generate text -- very similar to `DynamicCache`. If your project depend on\n-this legacy format, you can seamlessly convert it to a `DynamicCache` and back.\n+model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n+model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"cuda\")\n+tokenizer = AutoTokenizer.from_pretrained(model_id)\n \n-```python\n->>> import torch\n->>> from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache\n+# Init StaticCache with big enough max-length (1024 tokens for the below example) \n+# You can also init a DynamicCache, if that suits you better\n+prompt_cache = StaticCache(config=model.config, max_batch_size=1, max_cache_len=1024, device=\"cuda\", dtype=torch.bfloat16)\n \n->>> tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n->>> model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", torch_dtype=torch.float16, device_map=\"auto\")\n->>> inputs = tokenizer(\"Hello, my name is\", return_tensors=\"pt\").to(model.device)\n+INITIAL_PROMPT = \"You are a helpful assistant. \"\n+inputs_initial_prompt = tokenizer(INITIAL_PROMPT, return_tensors=\"pt\").to(\"cuda\")\n+# This is the common prompt cached, we need to run forward without grad to be able to copy\n+with torch.no_grad():\n+     prompt_cache = model(**inputs_initial_prompt, past_key_values = prompt_cache).past_key_values\n \n->>> # `return_dict_in_generate=True` is required to return the cache. `return_legacy_cache` forces the returned cache\n->>> # to be of the legacy type\n->>> generation_outputs = model.generate(**inputs, return_dict_in_generate=True, return_legacy_cache=True, max_new_tokens=5)\n+prompts = [\"Help me to write a blogpost about travelling.\", \"What is the capital of France?\"]\n+responses = []\n+for prompt in prompts:\n+    new_inputs = tokenizer(INITIAL_PROMPT + prompt, return_tensors=\"pt\").to(\"cuda\")\n+    past_key_values = copy.deepcopy(prompt_cache)\n+    outputs = model.generate(**new_inputs, past_key_values=past_key_values,max_new_tokens=20) \n+    response = tokenizer.batch_decode(outputs)[0]\n+    responses.append(response)\n \n->>> # We can convert a legacy cache to a DynamicCache -- and the other way around. This is helpful if you have custom\n->>> # logic to manipulate a cache in a specific format.\n->>> cache = DynamicCache.from_legacy_cache(generation_outputs.past_key_values)\n->>> legacy_format_cache = cache.to_legacy_cache()\n+print(responses)\n ```"
        },
        {
            "sha": "7c9bc154ab6eb6e16cf3630011716cd99c1959cd",
            "filename": "docs/source/en/llm_optims.md",
            "status": "modified",
            "additions": 60,
            "deletions": 151,
            "changes": 211,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fllm_optims.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fllm_optims.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fllm_optims.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -9,46 +9,42 @@ specific language governing permissions and limitations under the License.\n rendered properly in your Markdown viewer.\n -->\n \n-# LLM inference optimization\n+# Optimizing inference\n \n-Large language models (LLMs) have pushed text generation applications, such as chat and code completion models, to the next level by producing text that displays a high level of understanding and fluency. But what makes LLMs so powerful - namely their size - also presents challenges for inference.\n+Inference with large language models (LLMs) can be challenging because they have to store and handle billions of parameters. To load a 70B parameter [Llama 2](https://hf.co/meta-llama/Llama-2-70b-hf) model, it requires 256GB of memory for full precision weights and 128GB of memory for half-precision weights. The most powerful GPUs today - the A100 and H100 - only have 80GB of memory.\n \n-Basic inference is slow because LLMs have to be called repeatedly to generate the next token. The input sequence increases as generation progresses, which takes longer and longer for the LLM to process. LLMs also have billions of parameters, making it a challenge to store and handle all those weights in memory.\n+On top of the memory requirements, inference is slow because LLMs are called repeatedly to generate the next token. The input sequence increases as generation progresses, which takes longer and longer to process.\n \n-This guide will show you how to use the optimization techniques available in Transformers to accelerate LLM inference.\n+This guide will show you how to optimize LLM inference to accelerate generation and reduce memory usage.\n \n > [!TIP]\n-> Hugging Face also provides [Text Generation Inference (TGI)](https://hf.co/docs/text-generation-inference), a library dedicated to deploying and serving highly optimized LLMs for inference. It includes deployment-oriented optimization features not included in Transformers, such as continuous batching for increasing throughput and tensor parallelism for multi-GPU inference.\n+> Try out [Text Generation Inference (TGI)](https://hf.co/docs/text-generation-inference), a Hugging Face library dedicated to deploying and serving highly optimized LLMs for inference.\n \n-## Static kv-cache and `torch.compile`\n+## Static kv-cache and torch.compile\n \n-During decoding, a LLM computes the key-value (kv) values for each input token and since it is autoregressive, it computes the same kv values each time because the generated output becomes part of the input now. This is not very efficient because you're recomputing the same kv values each time.\n+LLMs compute key-value (kv) values for each input token, and it performs the same kv computation each time because the generated output becomes part of the input. However, performing the same kv computation every time is not very efficient.\n \n-To optimize this, you can use a kv-cache to store the past keys and values instead of recomputing them each time. However, since the kv-cache grows with each generation step and is dynamic, it prevents you from taking advantage of [`torch.compile`](./perf_torch_compile), a powerful optimization tool that fuses PyTorch code into fast and optimized kernels. We have an entire guide dedicated to kv-caches [here](./kv_cache).\n+A *kv-cache* stores the past keys and values instead of recomputing them each time. As a result, the kv-cache is dynamic and it grows with each generation step which prevents you from taking advantage of [torch.compile](./perf_torch_compile), a powerful optimization method that fuses PyTorch code into optimized kernels.\n \n-The *static kv-cache* solves this issue by pre-allocating the kv-cache size to a maximum value which allows you to combine it with `torch.compile` for up to a 4x speed up. Your speed up may vary depending on the model size (larger models have a smaller speed up) and hardware.\n+The *static kv-cache* solves this issue by pre-allocating the kv-cache size to a maximum value, so you can combine it with [torch.compile](./perf_torch_compile) for up to a 4x speed up. Your speed up may vary depending on the model size (larger models have a smaller speed up) and hardware.\n \n > [!WARNING]\n-> Currently, only [Llama](./model_doc/llama2) and a few other models support static kv-cache and `torch.compile`. Check [this issue](https://github.com/huggingface/transformers/issues/28981) for a live model compatibility list.\n+> Follow this [issue](https://github.com/huggingface/transformers/issues/28981) to track which models (Llama, Gemma, Mistral, etc.) support a static kv-cache and torch.compile.\n \n-There are three flavors of static kv-cache usage, depending on the complexity of your task:\n-1. Basic usage: simply set a flag in `generation_config` (recommended);\n-2. Advanced usage: handle a cache object for multi-turn generation or a custom generation loop;\n-3. Advanced usage: compile the entire `generate` function into a single graph, if having a single graph is relevant for you.\n+Depending on your task, there are several ways you can use the static kv-cache.\n \n-Select the correct tab below for further instructions on each of these flavors.\n+1. For basic use cases, set [cache_implementation](https://hf.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig.cache_implementation) to `\"static\"` (recommended).\n+2. For multi-turn generation or a custom generation loop, initialize and handle [`StaticCache`] directly.\n+3. For more unique hardware or use cases, it may be better to compile the entire [`~GenerationMixin.generate`] function into a single graph.\n \n > [!TIP]\n-> Regardless of the strategy used with `torch.compile`, you can avoid shape-related recompilations if you left-pad your LLM inputs to a limited set of values. The [`pad_to_multiple_of` tokenizer flag](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.__call__.pad_to_multiple_of) is your friend!\n+> Regardless of how you use the static kv-cache and torch.compile, left-pad your inputs with [pad_to_multiple_of](https://hf.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.__call__.pad_to_multiple_of) to a limited set of values to avoid shape-related recompilations.\n \n <hfoptions id=\"static-kv\">\n-<hfoption id=\"basic usage: generation_config\">\n+<hfoption id=\"1. cache_implementation\">\n \n-For this example, let's use the [Gemma](https://hf.co/google/gemma-2b) model. All we need to do is to:\n-1. Access the model's `generation_config` attribute and set the `cache_implementation` to \"static\";\n-2. Call `torch.compile` on the model to compile the forward pass with the static kv-cache.\n-\n-And that's it!\n+1. Set the [cache_implementation](https://hf.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig.cache_implementation) to `\"static\"` in a models [`GenerationConfig`].\n+2. Call [torch.compile](./perf_torch_compile) to compile the forward pass with the static kv-cache.\n \n ```py\n from transformers import AutoTokenizer, AutoModelForCausalLM\n@@ -70,17 +66,15 @@ print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n ['The theory of special relativity states 1. The speed of light is constant in all inertial reference']\n ```\n \n-Under the hood, `generate` will attempt to reuse the same cache object, removing the need for re-compilation at each call. Avoiding re-compilation is critical to get the most out of `torch.compile`, and you should be aware of the following:\n-1. If the batch size changes or the maximum output length increases between calls, the cache will have to be reinitialized, triggering a new compilation;\n-2. The first couple of calls of the compiled function are slower, as the function is being compiled.\n+Under the hood, [`~GenerationMixin.generate`] attempts to reuse the same cache object to avoid recompilation at each call, which is critical to get the most out of [torch.compile](./perf_torch_compile). Be aware of the following to avoid triggering recompilation or if generation is slower than expected.\n \n-> [!WARNING]\n-> For a more advanced usage of the static cache, such as multi-turn conversations, we recommend instantiating and manipulating the cache object outside [`~GenerationMixin.generate`]. See the advanced usage tab.\n+1. If the batch size changes or the maximum output length increases between calls, the cache is reinitialized and recompiled.\n+2. The first several calls of the compiled function are slower because it is being compiled.\n \n </hfoption>\n-<hfoption id=\"advanced usage: control Static Cache\">\n+<hfoption id=\"2. StaticCache\">\n \n-A [`StaticCache`] object can be passed to the model's [`~GenerationMixin.generate`] under the `past_key_values` argument. The object will retain the cache contents, so you can pass it to a new [`~GenerationMixin.generate`] call to continue generation, like you would do with a dynamic cache.\n+Directly initialize a [`StaticCache`] object and pass it to the `past_key_values` parameter in [`~GenerationMixin.generate`]. The [`StaticCache`] keeps the cache contents, so you can pass it to a new [`~GenerationMixin.generate`] call to continue generation, similar to a dynamic cache.\n \n ```py\n from transformers import AutoTokenizer, AutoModelForCausalLM, StaticCache\n@@ -118,9 +112,9 @@ print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n ```\n \n > [!TIP]\n-> If you want to reuse the same [`StaticCache`] object on a new prompt, be sure to reset its contents with the `.reset()` method between calls\n+> To reuse [`StaticCache`] on a new prompt, use [`~StaticCache.reset`] to reset the cache contents between calls.\n \n-If you want to go further down a level, the [`StaticCache`] object can also be passed to the model's forward pass under the same `past_key_values` argument. Using this strategy, you can write your own function to decode the next token given the current token and position and cache position of previously generated tokens.\n+Another option for using [`StaticCache`] is to pass it to a models forward pass using the same `past_key_values` argument. This allows you to write your own custom decoding function to decode the next token given the current token, position, and cache position of previously generated tokens.\n \n ```py\n from transformers import LlamaTokenizer, LlamaForCausalLM, StaticCache, logging\n@@ -153,10 +147,11 @@ def decode_one_tokens(model, cur_token, input_pos, cache_position, past_key_valu\n     return new_token\n ```\n \n-There are a few important things you must do to enable static kv-cache and `torch.compile` with the `StaticCache` method:\n-1. Initialize the [`StaticCache`] instance before using the model for inference. There you can configure parameters like the maximum batch size and sequence length.\n-2. Call `torch.compile` on the model to compile the forward pass with the static kv-cache.\n-3. Use `SDPBackend.MATH` in the [torch.nn.attention.sdpa_kernel](https://pytorch.org/docs/stable/generated/torch.nn.attention.sdpa_kernel.html) context manager to enable the native PyTorch C++ implementation of scaled dot product attention to speed up inference even more.\n+To enable static kv-cache and [torch.compile](./perf_torch_compile) with [`StaticCache`], follow the steps below.\n+\n+1. Initialize [`StaticCache`] before using the model for inference to configure parameters like the maximum batch size and sequence length.\n+2. Call [torch.compile](./perf_torch_compile) on the model to compile the forward pass with the static kv-cache.\n+3. se SDPBackend.MATH in the [torch.nn.attention.sdpa_kernel](https://pytorch.org/docs/stable/generated/torch.nn.attention.sdpa_kernel.html) context manager to enable the native PyTorch C++ implementation of scaled dot product attention to speed up inference even more.\n \n ```py\n from torch.nn.attention import SDPBackend, sdpa_kernel\n@@ -193,9 +188,9 @@ text\n ```\n \n </hfoption>\n-<hfoption id=\"advanced usage: end-to-end generate compilation\">\n+<hfoption id=\"3. compile entire generate function\">\n \n-Compiling the entire `generate` function, in terms of code, is even simpler than in the basic usage: call `torch.compile` on `generate` to compile the entire function. No need to specify the use of the static cache: although it is compatible, dynamic cache (default) was faster in our benchmarks.\n+Compiling the entire [`~GenerationMixin.generate`] function also compiles the input preparation logit processor operations, and more, in addition to the forward pass. With this approach, you don't need to initialize [`StaticCache`] or set the [cache_implementation](https://hf.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig.cache_implementation) parameter.\n \n ```py\n from transformers import AutoTokenizer, AutoModelForCausalLM\n@@ -215,28 +210,33 @@ print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n ['The theory of special relativity states 1. The speed of light is constant in all inertial reference']\n ```\n \n-As a result, we compile not only the model forward pass, but also all input preparation, logit processor operations, and so on. The result should be a slightly `generate` call, compared to the basic usage example, and the compiled graph may be better suited to more exotic hardware devices or use cases. However, there are severe drawbacks in using this approach:\n-1. Compilation is much slower;\n-2. All parameterization of `generate` must be done through `generation_config`;\n-3. Many warnings and exceptions are suppressed -- we suggest testing with its uncompiled form first;\n-4. Although we are working on it, it is heavily feature restricted (for instance, at the time of writing, generation does not stop if an EOS token is selected).\n+This usage pattern is more appropriate for unique hardware or use cases, but there are several drawbacks to consider.\n+\n+1. Compilation is much slower.\n+2. Parameters must be configured through [`GenerationConfig`].\n+3. Many warnings and exceptions are suppressed. We recommend testing the uncompiled model first.\n+4. Many features are unavailable at the moment. For example, generation does not stop if an `EOS` token is selected.\n \n </hfoption>\n </hfoptions>\n \n-## Speculative decoding\n+## Decoding strategies\n+\n+Decoding can also be optimized to accelerate generation. You can use a lightweight assistant model to generate candidate tokens faster than the LLM itself or you can use a variant of this decoding strategy that works especially well for input-grounded tasks.\n+\n+### Speculative decoding\n \n > [!TIP]\n > For a more in-depth explanation, take a look at the [Assisted Generation: a new direction toward low-latency text generation](https://hf.co/blog/assisted-generation) blog post!\n \n-Another issue with autoregression is that for each input token you need to load the model weights each time during the forward pass. This is slow and cumbersome for LLMs which have billions of parameters. Speculative decoding alleviates this slowdown by using a second smaller and faster assistant model to generate candidate tokens that are verified by the larger LLM in a single forward pass. If the verified tokens are correct, the LLM essentially gets them for \"free\" without having to generate them itself. There is no degradation in accuracy because the verification forward pass ensures the same outputs are generated as if the LLM had generated them on its own.\n+For each input token, the model weights are loaded each time during the forward pass, which is slow and cumbersome when a model has billions of parameters. Speculative decoding alleviates this slowdown by using a second smaller and faster assistant model to generate candidate tokens that are verified by the larger model in a single forward pass. If the verified tokens are correct, the LLM essentially gets them for \"free\" without having to generate them itself. There is no degradation in accuracy because the verification forward pass ensures the same outputs are generated as if the LLM had generated them on its own.\n \n To get the largest speed up, the assistant model should be a lot smaller than the LLM so that it can generate tokens quickly. The assistant and LLM model must also share the same tokenizer to avoid re-encoding and decoding tokens.\n \n > [!WARNING]\n-> Speculative decoding is only supported for the greedy search and sampling decoding strategies, and it also doesn't support batched inputs.\n+> Speculative decoding is only supported for the greedy search and sampling decoding strategies, and it doesn't support batched inputs.\n \n-Enable speculative decoding by loading an assistant model and passing it to the [`~GenerationMixin.generate`] method.\n+Enable speculative decoding by loading an assistant model and passing it to [`~GenerationMixin.generate`].\n \n <hfoptions id=\"spec-decoding\">\n <hfoption id=\"greedy search\">\n@@ -261,7 +261,7 @@ tokenizer.batch_decode(outputs, skip_special_tokens=True)\n </hfoption>\n <hfoption id=\"sampling\">\n \n-For speculative sampling decoding, add the `do_sample` and `temperature` parameters to the [`~GenerationMixin.generate`] method in addition to the assistant model.\n+For speculative sampling decoding, add the [do_sample](https://hf.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationConfig.do_sample) and [temperature](https://hf.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationConfig.temperature) parameters to [`~GenerationMixin.generate`].\n \n ```py\n from transformers import AutoModelForCausalLM, AutoTokenizer\n@@ -287,7 +287,7 @@ print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n \n Prompt lookup decoding is a variant of speculative decoding that is also compatible with greedy search and sampling. Prompt lookup works especially well for input-grounded tasks - such as summarization - where there is often overlapping words between the prompt and output. These overlapping n-grams are used as the LLM candidate tokens.\n \n-To enable prompt lookup decoding, specify the number of tokens that should be overlapping in the `prompt_lookup_num_tokens` parameter. Then you can pass this parameter to the [`~GenerationMixin.generate`] method.\n+To enable prompt lookup decoding, specify the number of tokens that should be overlapping in the [prompt_lookup_num_tokens](https://hf.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationConfig.prompt_lookup_num_tokens) parameter. Then pass this parameter to [`~GenerationMixin.generate`].\n \n <hfoptions id=\"pld\">\n <hfoption id=\"greedy decoding\">\n@@ -312,7 +312,7 @@ print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n </hfoption>\n <hfoption id=\"sampling\">\n \n-For prompt lookup decoding with sampling, add the `do_sample` and `temperature` parameters to the [`~GenerationMixin.generate`] method.\n+For prompt lookup decoding with sampling, add the [do_sample](https://hf.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationConfig.do_sample) and [temperature](https://hf.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationConfig.temperature) parameters to [`~GenerationMixin.generate`].\n \n ```py\n from transformers import AutoModelForCausalLM, AutoTokenizer\n@@ -333,15 +333,15 @@ print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n </hfoption>\n </hfoptions>\n \n-## Attention optimizations\n+## Attention\n \n-A known issue with transformer models is that the self-attention mechanism grows quadratically in compute and memory with the number of input tokens. This limitation is only magnified in LLMs which handles much longer sequences. To address this, try FlashAttention2 or PyTorch's scaled dot product attention (SDPA), which are more memory efficient attention implementations and can accelerate inference.\n+A known issue with transformer models is that the self-attention mechanism grows quadratically in compute and memory with the number of input tokens. This limitation is only magnified in LLMs which handles much longer sequences. To address this, try FlashAttention2 or PyTorch's scaled dot product attention (SDPA), which are more memory efficient attention implementations.\n \n ### FlashAttention-2\n \n-FlashAttention and [FlashAttention-2](./perf_infer_gpu_one#flashattention-2) break up the attention computation into smaller chunks and reduces the number of intermediate read/write operations to GPU memory to speed up inference. FlashAttention-2 improves on the original FlashAttention algorithm by also parallelizing over sequence length dimension and better partitioning work on the hardware to reduce synchronization and communication overhead.\n+FlashAttention and [FlashAttention-2](./perf_infer_gpu_one#flashattention-2) break up the attention computation into smaller chunks and reduces the number of intermediate read/write operations to the GPU memory to speed up inference. FlashAttention-2 improves on the original FlashAttention algorithm by also parallelizing over sequence length dimension and better partitioning work on the hardware to reduce synchronization and communication overhead.\n \n-To use FlashAttention-2, set `attn_implementation=\"flash_attention_2\"` in the [`~PreTrainedModel.from_pretrained`] method.\n+To use FlashAttention-2, set [attn_implementation](https://hf.co/docs/transformers/main/en/main_classes/text_generation#transformers.PreTrainedModel.from_pretrained.attn_implementation) to `\"flash_attention_2\"` in [`~PreTrainedModel.from_pretrained`].\n \n ```py\n from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n@@ -355,105 +355,12 @@ model = AutoModelForCausalLM.from_pretrained(\n )\n ```\n \n-### Fine-Tuning with torch.compile and Padding-Free Data Collation\n-\n-In addition to optimizing inference, you can also enhance the training efficiency of large language models by leveraging torch.compile during fine-tuning and using a padding-free data collator. This approach can significantly speed up training and reduce computational overhead.\n-\n-Here's how you can fine-tune a Llama model using SFTTrainer from the TRL library, with torch_compile enabled and a padding-free data collator:\n-\n-```\n-#################### IMPORTS ###################\n-\n-import math\n-import datasets\n-import dataclasses\n-from transformers import (\n-    AutoModelForCausalLM,\n-    AutoTokenizer,\n-    TrainingArguments\n-)\n-from trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM\n-\n-#################### MODEL LOADING WITH FLASH ATTENTION ###################\n-\n-model_name = \"meta-llama/Llama-3.2-1B\"\n-model = AutoModelForCausalLM.from_pretrained(\n-    model_name,\n-    attn_implementation=\"flash_attention_2\"  # Enables FlashAttention-2\n-)\n-tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n-\n-#################### DATA PREPROCESSING (PADDING-FREE) ###################\n-\n-response_template = \"\\n### Label:\"\n-response_template_ids = tokenizer.encode(\n-    response_template, add_special_tokens=False\n-)[2:]  # Exclude special tokens\n-\n-data_collator = DataCollatorForCompletionOnlyLM(\n-    response_template_ids=response_template_ids,\n-    tokenizer=tokenizer,\n-    ignore_index=-100,\n-    padding_free=True  # Enables padding-free collation\n-)\n-\n-def format_dataset(example):\n-    return {\n-        \"output\": example[\"output\"] + tokenizer.eos_token\n-    }\n-\n-data_files = {\"train\": \"path/to/dataset\"}  # Replace with your dataset path\n-json_dataset = datasets.load_dataset(\"json\", data_files=data_files)\n-formatted_train_dataset = json_dataset[\"train\"].map(format_dataset)\n-\n-################# TRAINING CONFIGURATION ############################\n-\n-train_args = TrainingArguments(\n-    num_train_epochs=5,\n-    per_device_train_batch_size=4,\n-    per_device_eval_batch_size=4,\n-    gradient_accumulation_steps=4,\n-    learning_rate=1e-5,\n-    weight_decay=0.0,\n-    warmup_ratio=0.03,\n-    lr_scheduler_type=\"cosine\",\n-    logging_steps=1,\n-    include_tokens_per_second=True,\n-    save_strategy=\"epoch\",\n-    output_dir=\"output\",\n-    torch_compile=True,  # Enables torch.compile\n-    torch_compile_backend=\"inductor\",\n-    torch_compile_mode=\"default\"\n-)\n-\n-# Convert TrainingArguments to SFTConfig\n-transformer_train_arg_fields = [x.name for x in dataclasses.fields(SFTConfig)]\n-transformer_kwargs = {\n-    k: v\n-    for k, v in train_args.to_dict().items()\n-    if k in transformer_train_arg_fields\n-}\n-training_args = SFTConfig(**transformer_kwargs)\n-\n-####################### FINE-TUNING #####################\n-\n-trainer = SFTTrainer(\n-    model=model,\n-    tokenizer=tokenizer,\n-    train_dataset=formatted_train_dataset,\n-    data_collator=data_collator,\n-    dataset_text_field=\"output\",\n-    args=training_args,\n-)\n-trainer.train()\n-```\n-\n ### PyTorch scaled dot product attention\n \n Scaled dot product attention (SDPA) is automatically enabled in PyTorch 2.0 and it supports FlashAttention, xFormers, and PyTorch's C++ implementation. SDPA chooses the most performant attention algorithm if you're using a CUDA backend. For other backends, SDPA defaults to the PyTorch C++ implementation.\n \n > [!TIP]\n-> SDPA supports FlashAttention-2 as long as you have the latest PyTorch version installed.\n+> SDPA automaticallysupports FlashAttention-2 as long as you have the latest PyTorch version installed.\n \n Use the [torch.nn.attention.sdpa_kernel](https://pytorch.org/docs/stable/generated/torch.nn.attention.sdpa_kernel.html) context manager to explicitly enable or disable any of the four attention algorithms. For example, use `SDPBackend.FLASH_ATTENTION` to enable FlashAttention.\n \n@@ -473,12 +380,14 @@ with sdpa_kernel(SDPBackend.FLASH_ATTENTION):\n \n ## Quantization\n \n-Quantization reduces the size of the LLM weights by storing them in a lower precision. This translates to lower memory usage and makes loading LLMs for inference more accessible if you're constrained by your GPUs memory. If you aren't limited by your GPU, you don't necessarily need to quantize your model because it can incur a small latency cost (except for AWQ and fused AWQ modules) due to the extra step required to quantize and dequantize the weights.\n+Quantization reduces the size of model weights by storing them in a lower precision. This translates to lower memory usage and makes loading LLMs for inference more accessible if you're constrained by GPU memory.\n+\n+If you aren't limited by your GPU, you don't necessarily need to quantize your model because it can increase latency slightly (except for AWQ and fused AWQ modules) due to the extra step required to quantize and dequantize the weights.\n \n > [!TIP]\n > There are many quantization libraries (see the [Quantization](./quantization) guide for more details) available, such as Quanto, AQLM, VPTQ, AWQ, and AutoGPTQ. Feel free to try them out and see which one works best for your use case. We also recommend reading the [Overview of natively supported quantization schemes in ðŸ¤— Transformers](https://hf.co/blog/overview-quantization-transformers) blog post which compares AutoGPTQ and bitsandbytes.\n \n-Use the Model Memory Calculator below to estimate and compare how much memory is required to load a model. For example, try estimating how much memory it costs to load [Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1).\n+Use the Model Memory Calculator below to estimate and compare how much memory is required to load a model. For example, try estimating the memory required to load [Mistral-7B-v0.1](https://hf.co/mistralai/Mistral-7B-v0.1).\n \n <iframe\n \tsrc=\"https://hf-accelerate-model-memory-usage.hf.space\"\n@@ -487,7 +396,7 @@ Use the Model Memory Calculator below to estimate and compare how much memory is\n \theight=\"450\"\n ></iframe>\n \n-To load Mistral-7B-v0.1 in half-precision, set the `torch_dtype` parameter in the [`~transformers.AutoModelForCausalLM.from_pretrained`] method to `torch.bfloat16`. This requires 13.74GB of memory.\n+To load a model in half-precision, set the [torch_dtype](https://hf.co/docs/transformers/main/en/main_classes/text_generation#transformers.PreTrainedModel.from_pretrained.torch_dtype) parameter in [`~transformers.AutoModelForCausalLM.from_pretrained`] to `torch.bfloat16`. This requires 13.74GB of memory.\n \n ```py\n from transformers import AutoTokenizer, AutoModelForCausalLM\n@@ -498,7 +407,7 @@ model = AutoModelForCausalLM.from_pretrained(\n )\n ```\n \n-To load a quantized model (8-bit or 4-bit) for inference, try [bitsandbytes](https://hf.co/docs/bitsandbytes) and set the `load_in_4bit` or `load_in_8bit` parameters to `True`. Loading the model in 8-bits only requires 6.87 GB of memory.\n+To load a quantized model (8-bit or 4-bit), try [bitsandbytes](https://hf.co/docs/bitsandbytes) and set the [load_in_4bit](https://hf.co/docs/transformers/main/en/main_classes/text_generation#transformers.BitsAndBytesConfig.load_in_4bit) or [load_in_8bit](https://hf.co/docs/transformers/main/en/main_classes/text_generation#transformers.BitsAndBytesConfig.load_in_8bit) parameters to `True`. Loading the model in 8-bits only requires 6.87 GB of memory.\n \n ```py\n from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
        },
        {
            "sha": "e5c254debf21f2e37dde60740dcef18636a6f8ba",
            "filename": "docs/source/en/llm_tutorial.md",
            "status": "modified",
            "additions": 190,
            "deletions": 183,
            "changes": 373,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fllm_tutorial.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fllm_tutorial.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fllm_tutorial.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -1,4 +1,4 @@\n-<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n+<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n \n Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n the License. You may obtain a copy of the License at\n@@ -14,269 +14,276 @@ rendered properly in your Markdown viewer.\n \n -->\n \n-\n-# Generation with LLMs\n+# Text generation\n \n [[open-in-colab]]\n \n-LLMs, or Large Language Models, are the key component behind text generation. In a nutshell, they consist of large pretrained transformer models trained to predict the next word (or, more precisely, token) given some input text. Since they predict one token at a time, you need to do something more elaborate to generate new sentences other than just calling the model -- you need to do autoregressive generation.\n-\n-Autoregressive generation is the inference-time procedure of iteratively calling a model with its own generated outputs, given a few initial inputs. In ðŸ¤— Transformers, this is handled by the [`~generation.GenerationMixin.generate`] method, which is available to all models with generative capabilities.\n-\n-<Tip>\n+Text generation is the most popular application for large language models (LLMs). A LLM is trained to generate the next word (token) given some initial text (prompt) along with its own generated outputs up to a predefined length or when it reaches an end-of-sequence (`EOS`) token.\n \n-If you want to jump straight to chatting with a model, [try our chat CLI](quicktour#chat-with-text-generation-models).\n+In Transformers, the [`~GenerationMixin.generate`] API handles text generation, and it is available for all models with generative capabilities.\n \n-</Tip>\n+This guide will show you the basics of text generation with [`~GenerationMixin.generate`] and some common pitfalls to avoid.\n \n-This tutorial will show you how to:\n+## Default generate\n \n-* Generate text with an LLM\n-* Avoid common pitfalls\n-* Next steps to help you get the most out of your LLM\n-\n-Before you begin, make sure you have all the necessary libraries installed:\n+Before you begin, it's helpful to install [bitsandbytes](https://hf.co/docs/bitsandbytes/index) to quantize really large models to reduce their memory usage.\n \n ```bash\n-pip install transformers bitsandbytes>=0.39.0 -q\n+!pip install -U transformers bitsandbytes\n ```\n Bitsandbytes supports multiple backends in addition to CUDA-based GPUs. Refer to the multi-backend installation [guide](https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend) to learn more.\n \n+Load a LLM with [`~PreTrainedModel.from_pretrained`] and add the following two parameters to reduce the memory requirements.\n \n-## Generate text\n-\n-A language model trained for [causal language modeling](tasks/language_modeling) takes a sequence of text tokens as input and returns the probability distribution for the next token.\n+- `device_map=\"auto\"` enables Accelerates' [Big Model Inference](./models#big-model-inference) feature for automatically initiating the model skeleton and loading and dispatching the model weights across all available devices, starting with the fastest device (GPU).\n+- `quantization_config` is a configuration object that defines the quantization settings. This examples uses bitsandbytes as the quantization backend (see the [Quantization](./quantization/overview) section for more available backends) and it loads the model in [4-bits](./quantization/bitsandbytes).\n \n-<!-- [GIF 1 -- FWD PASS] -->\n-<figure class=\"image table text-center m-0 w-full\">\n-    <video\n-        style=\"max-width: 90%; margin: auto;\"\n-        autoplay loop muted playsinline\n-        src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/assisted-generation/gif_1_1080p.mov\"\n-    ></video>\n-    <figcaption>\"Forward pass of an LLM\"</figcaption>\n-</figure>\n+```py\n+from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n \n-A critical aspect of autoregressive generation with LLMs is how to select the next token from this probability distribution. Anything goes in this step as long as you end up with a token for the next iteration. This means it can be as simple as selecting the most likely token from the probability distribution or as complex as applying a dozen transformations before sampling from the resulting distribution.\n+quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n+model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", device_map=\"auto\", quantization_config=quantization_config)\n+```\n \n-<!-- [GIF 2 -- TEXT GENERATION] -->\n-<figure class=\"image table text-center m-0 w-full\">\n-    <video\n-        style=\"max-width: 90%; margin: auto;\"\n-        autoplay loop muted playsinline\n-        src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/assisted-generation/gif_2_1080p.mov\"\n-    ></video>\n-    <figcaption>\"Autoregressive generation iteratively selects the next token from a probability distribution to generate text\"</figcaption>\n-</figure>\n+Tokenize your input, and set the [`~PreTrainedTokenizer.padding_side`] parameter to `\"left\"` because a LLM is not trained to continue generation from padding tokens. The tokenizer returns the input ids and attention mask.\n \n-The process depicted above is repeated iteratively until some stopping condition is reached. Ideally, the stopping condition is dictated by the model, which should learn when to output an end-of-sequence (`EOS`) token. If this is not the case, generation stops when some predefined maximum length is reached.\n+> [!TIP]\n+> Process more than one prompt at a time by passing a list of strings to the tokenizer. Batch the inputs to improve throughput at a small cost to latency and memory.\n \n-Properly setting up the token selection step and the stopping condition is essential to make your model behave as you'd expect on your task. That is why we have a [`~generation.GenerationConfig`] file associated with each model, which contains a good default generative parameterization and is loaded alongside your model.\n+```py\n+tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\", padding_side=\"left\")\n+model_inputs = tokenizer([\"A list of colors: red, blue\"], return_tensors=\"pt\").to(\"cuda\")\n+```\n \n-Let's talk code!\n+Pass the inputs to [`~GenerationMixin.generate`] to generate tokens, and [`~PreTrainedTokenizer.batch_decode`] the generated tokens back to text.\n \n-<Tip>\n+```py\n+generated_ids = model.generate(**model_inputs)\n+tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n+\"A list of colors: red, blue, green, yellow, orange, purple, pink,\"\n+```\n \n-If you're interested in basic LLM usage, our high-level [`Pipeline`](pipeline_tutorial) interface is a great starting point. However, LLMs often require advanced features like quantization and fine control of the token selection step, which is best done through [`~generation.GenerationMixin.generate`]. Autoregressive generation with LLMs is also resource-intensive and should be executed on a GPU for adequate throughput.\n+## Generation configuration\n \n-</Tip>\n+All generation settings are contained in [`GenerationConfig`]. In the example above, the generation settings are derived from the `generation_config.json` file of [mistralai/Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1). A default decoding strategy is used when no configuration is saved with a model.\n \n-First, you need to load the model.\n+Inspect the configuration through the `generation_config` attribute. It only shows values that are different from the default configuration, in this case, the `bos_token_id` and `eos_token_id`.\n \n ```py\n->>> from transformers import AutoModelForCausalLM\n+from transformers import AutoModelForCausalLM\n+\n+model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", device_map=\"auto\")\n+model.generation_config\n+GenerationConfig {\n+  \"bos_token_id\": 1,\n+  \"eos_token_id\": 2\n+}\n+```\n \n->>> model = AutoModelForCausalLM.from_pretrained(\n-...     \"mistralai/Mistral-7B-v0.1\", device_map=\"auto\", load_in_4bit=True\n-... )\n+You can customize [`~GenerationMixin.generate`] by overriding the parameters and values in [`GenerationConfig`]. Some of the most commonly adjusted parameters are [max_new_tokens](https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig.max_new_tokens), [num_beams](https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig.num_beams), [do_sample](https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig.do_sample), and [num_return_sequences](https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig.num_return_sequences).\n+\n+```py\n+# enable beam search sampling strategy\n+model.generate(**inputs, num_beams=4, do_sample=True)\n ```\n \n-You'll notice two flags in the `from_pretrained` call:\n+[`~GenerationMixin.generate`] can also be extended with external libraries or custom code. The `logits_processor` parameter accepts custom [`LogitsProcessor`] instances for manupulating the next token probability distribution. `stopping_criteria` supports custom [`StoppingCriteria`] to stop text generation. Check out the [logits-processor-zoo](https://github.com/NVIDIA/logits-processor-zoo) for more examples of external [`~GenerationMixin.generate`]-compatible extensions.\n \n- - `device_map` ensures the model is moved to your GPU(s)\n- - `load_in_4bit` applies [4-bit dynamic quantization](main_classes/quantization) to massively reduce the resource requirements\n+Refer to the [Generation strategies](./generation_strategies) guide to learn more about search, sampling, and decoding strategies.\n \n-There are other ways to initialize a model, but this is a good baseline to begin with an LLM.\n+### Saving\n \n-Next, you need to preprocess your text input with a [tokenizer](tokenizer_summary).\n+Create an instance of [`GenerationConfig`] and specify the decoding parameters you want.\n \n ```py\n->>> from transformers import AutoTokenizer\n->>> from accelerate.test_utils.testing import get_backend\n+from transformers import AutoModelForCausalLM, GenerationConfig\n \n->>> DEVICE, _, _ = get_backend() # automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)\n->>> tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\", padding_side=\"left\")\n->>> model_inputs = tokenizer([\"A list of colors: red, blue\"], return_tensors=\"pt\").to(DEVICE)\n+model = AutoModelForCausalLM.from_pretrained(\"my_account/my_model\")\n+generation_config = GenerationConfig(\n+    max_new_tokens=50, do_sample=True, top_k=50, eos_token_id=model.config.eos_token_id\n+)\n ```\n \n-The `model_inputs` variable holds the tokenized text input, as well as the attention mask. While [`~generation.GenerationMixin.generate`] does its best effort to infer the attention mask when it is not passed, we recommend passing it whenever possible for optimal results.\n-\n-After tokenizing the inputs, you can call the [`~generation.GenerationMixin.generate`] method to returns the generated tokens. The generated tokens then should be converted to text before printing.\n+Use [`~GenerationConfig.save_pretrained`] to save a specific generation configuration and set the `push_to_hub` parameter to `True` to upload it to the Hub.\n \n ```py\n->>> generated_ids = model.generate(**model_inputs)\n->>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n-'A list of colors: red, blue, green, yellow, orange, purple, pink,'\n+generation_config.save_pretrained(\"my_account/my_model\", push_to_hub=True)\n ```\n \n-Finally, you don't need to do it one sequence at a time! You can batch your inputs, which will greatly improve the throughput at a small latency and memory cost. All you need to do is to make sure you pad your inputs properly (more on that below).\n+Leave the `config_file_name` parameter empty. This parameter should be used when storing multiple generation configurations in a single directory. It gives you a way to specify which generation configuration to load. You can create different configurations for different generative tasks (creative text generation with sampling, summarization with beam search) for use with a single model.\n \n ```py\n->>> tokenizer.pad_token = tokenizer.eos_token  # Most LLMs don't have a pad token by default\n->>> model_inputs = tokenizer(\n-...     [\"A list of colors: red, blue\", \"Portugal is\"], return_tensors=\"pt\", padding=True\n-... ).to(DEVICE)\n->>> generated_ids = model.generate(**model_inputs)\n->>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n-['A list of colors: red, blue, green, yellow, orange, purple, pink,',\n-'Portugal is a country in southwestern Europe, on the Iber']\n-```\n-\n-And that's it! In a few lines of code, you can harness the power of an LLM.\n-\n+from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig\n \n-## Common pitfalls\n+tokenizer = AutoTokenizer.from_pretrained(\"google-t5/t5-small\")\n+model = AutoModelForSeq2SeqLM.from_pretrained(\"google-t5/t5-small\")\n \n-There are many [generation strategies](generation_strategies), and sometimes the default values may not be appropriate for your use case. If your outputs aren't aligned with what you're expecting, we've created a list of the most common pitfalls and how to avoid them.\n+translation_generation_config = GenerationConfig(\n+    num_beams=4,\n+    early_stopping=True,\n+    decoder_start_token_id=0,\n+    eos_token_id=model.config.eos_token_id,\n+    pad_token=model.config.pad_token_id,\n+)\n \n-```py\n->>> from transformers import AutoModelForCausalLM, AutoTokenizer\n+translation_generation_config.save_pretrained(\"/tmp\", config_file_name=\"translation_generation_config.json\", push_to_hub=True)\n \n->>> tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n->>> tokenizer.pad_token = tokenizer.eos_token  # Most LLMs don't have a pad token by default\n->>> model = AutoModelForCausalLM.from_pretrained(\n-...     \"mistralai/Mistral-7B-v0.1\", device_map=\"auto\", load_in_4bit=True\n-... )\n+generation_config = GenerationConfig.from_pretrained(\"/tmp\", config_file_name=\"translation_generation_config.json\")\n+inputs = tokenizer(\"translate English to French: Configuration files are easy to use!\", return_tensors=\"pt\")\n+outputs = model.generate(**inputs, generation_config=generation_config)\n+print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n ```\n \n-### Generated output is too short/long\n+## Pitfalls\n+\n+The section below covers some common issues you may encounter during text generation and how to solve them.\n \n-If not specified in the [`~generation.GenerationConfig`] file, `generate` returns up to 20 tokens by default. We highly recommend manually setting `max_new_tokens` in your `generate` call to control the maximum number of new tokens it can return. Keep in mind LLMs (more precisely, [decoder-only models](https://huggingface.co/learn/nlp-course/chapter1/6?fw=pt)) also return the input prompt as part of the output.\n+### Output length\n \n+[`~GenerationMixin.generate`] returns up to 20 tokens by default unless otherwise specified in a models [`GenerationConfig`]. It is highly recommended to manually set the number of generated tokens with the [`max_new_tokens`] parameter to control the output length. [Decoder-only](https://hf.co/learn/nlp-course/chapter1/6?fw=pt) models returns the initial prompt along with the generated tokens.\n \n ```py\n->>> model_inputs = tokenizer([\"A sequence of numbers: 1, 2\"], return_tensors=\"pt\").to(DEVICE)\n+model_inputs = tokenizer([\"A sequence of numbers: 1, 2\"], return_tensors=\"pt\").to(\"cuda\")\n+```\n \n->>> # By default, the output will contain up to 20 tokens\n->>> generated_ids = model.generate(**model_inputs)\n->>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n+<hfoptions id=\"output-length\">\n+<hfoption id=\"default length\">\n+\n+```py\n+generated_ids = model.generate(**model_inputs)\n+tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n 'A sequence of numbers: 1, 2, 3, 4, 5'\n+```\n \n->>> # Setting `max_new_tokens` allows you to control the maximum length\n->>> generated_ids = model.generate(**model_inputs, max_new_tokens=50)\n->>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n+</hfoption>\n+<hfoption id=\"max_new_tokens\">\n+\n+```py\n+generated_ids = model.generate(**model_inputs, max_new_tokens=50)\n+tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n 'A sequence of numbers: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,'\n ```\n \n-### Incorrect generation mode\n+</hfoption>\n+</hfoptions>\n+\n+### Decoding strategy\n \n-By default, and unless specified in the [`~generation.GenerationConfig`] file, `generate` selects the most likely token at each iteration (greedy decoding). Depending on your task, this may be undesirable; creative tasks like chatbots or writing an essay benefit from sampling. On the other hand, input-grounded tasks like audio transcription or translation benefit from greedy decoding. Enable sampling with `do_sample=True`, and you can learn more about this topic in this [blog post](https://huggingface.co/blog/how-to-generate).\n+The default decoding strategy in [`~GenerationMixin.generate`] is *greedy search*, which selects the next most likely token, unless otherwise specified in a models [`GenerationConfig`]. While this decoding strategy works well for input-grounded tasks (transcription, translation), it is not optimal for more creative use cases (story writing, chat applications).\n+\n+For example, enable a [multinomial sampling](./generation_strategies#multinomial-sampling) strategy to generate more diverse outputs. Refer to the [Generation strategy](./generation_strategies) guide for more decoding strategies.\n \n ```py\n->>> # Set seed for reproducibility -- you don't need this unless you want full reproducibility\n->>> from transformers import set_seed\n->>> set_seed(42)\n+model_inputs = tokenizer([\"I am a cat.\"], return_tensors=\"pt\").to(\"cuda\")\n+```\n+\n+<hfoptions id=\"decoding\">\n+<hfoption id=\"greedy search\">\n \n->>> model_inputs = tokenizer([\"I am a cat.\"], return_tensors=\"pt\").to(DEVICE)\n+```py\n+generated_ids = model.generate(**model_inputs)\n+tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n+```\n \n->>> # LLM + greedy decoding = repetitive, boring output\n->>> generated_ids = model.generate(**model_inputs)\n->>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n-'I am a cat. I am a cat. I am a cat. I am a cat'\n+</hfoption>\n+<hfoption id=\"multinomial sampling\">\n \n->>> # With sampling, the output becomes more creative!\n->>> generated_ids = model.generate(**model_inputs, do_sample=True)\n->>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n-'I am a cat.  Specifically, I am an indoor-only cat.  I'\n+```py\n+generated_ids = model.generate(**model_inputs, do_sample=True)\n+tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n ```\n \n-### Wrong padding side\n+</hfoption>\n+</hfoptions>\n \n-LLMs are [decoder-only](https://huggingface.co/learn/nlp-course/chapter1/6?fw=pt) architectures, meaning they continue to iterate on your input prompt. If your inputs do not have the same length, they need to be padded. Since LLMs are not trained to continue from pad tokens, your input needs to be left-padded. Make sure you also don't forget to pass the attention mask to generate!\n+### Padding side\n+\n+Inputs need to be padded if they don't have the same length. But LLMs aren't trained to continue generation from padding tokens, which means the [`~PreTrainedTokenizer.padding_side`] parameter needs to be set to the left of the input.\n+\n+<hfoptions id=\"padding\">\n+<hfoption id=\"right pad\">\n \n ```py\n->>> # The tokenizer initialized above has right-padding active by default: the 1st sequence,\n->>> # which is shorter, has padding on the right side. Generation fails to capture the logic.\n->>> model_inputs = tokenizer(\n-...     [\"1, 2, 3\", \"A, B, C, D, E\"], padding=True, return_tensors=\"pt\"\n-... ).to(DEVICE)\n->>> generated_ids = model.generate(**model_inputs)\n->>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n+model_inputs = tokenizer(\n+    [\"1, 2, 3\", \"A, B, C, D, E\"], padding=True, return_tensors=\"pt\"\n+).to(\"cuda\")\n+generated_ids = model.generate(**model_inputs)\n+tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n '1, 2, 33333333333'\n+```\n \n->>> # With left-padding, it works as expected!\n->>> tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\", padding_side=\"left\")\n->>> tokenizer.pad_token = tokenizer.eos_token  # Most LLMs don't have a pad token by default\n->>> model_inputs = tokenizer(\n-...     [\"1, 2, 3\", \"A, B, C, D, E\"], padding=True, return_tensors=\"pt\"\n-... ).to(DEVICE)\n->>> generated_ids = model.generate(**model_inputs)\n->>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n+</hfoption>\n+<hfoption id=\"left pad\">\n+\n+```py\n+tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\", padding_side=\"left\")\n+tokenizer.pad_token = tokenizer.eos_token\n+model_inputs = tokenizer(\n+    [\"1, 2, 3\", \"A, B, C, D, E\"], padding=True, return_tensors=\"pt\"\n+).to(\"cuda\")\n+generated_ids = model.generate(**model_inputs)\n+tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n '1, 2, 3, 4, 5, 6,'\n ```\n \n-### Wrong prompt\n-\n-Some models and tasks expect a certain input prompt format to work properly. When this format is not applied, you will get a silent performance degradation: the model kinda works, but not as well as if you were following the expected prompt. More information about prompting, including which models and tasks need to be careful, is available in this [guide](tasks/prompting). Let's see an example with a chat LLM, which makes use of [chat templating](chat_templating):\n-\n-```python\n->>> tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-alpha\")\n->>> model = AutoModelForCausalLM.from_pretrained(\n-...     \"HuggingFaceH4/zephyr-7b-alpha\", device_map=\"auto\", load_in_4bit=True\n-... )\n->>> set_seed(0)\n->>> prompt = \"\"\"How many helicopters can a human eat in one sitting? Reply as a thug.\"\"\"\n->>> model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(DEVICE)\n->>> input_length = model_inputs.input_ids.shape[1]\n->>> generated_ids = model.generate(**model_inputs, max_new_tokens=20)\n->>> print(tokenizer.batch_decode(generated_ids[:, input_length:], skip_special_tokens=True)[0])\n-\"I'm not a thug, but i can tell you that a human cannot eat\"\n->>> # Oh no, it did not follow our instruction to reply as a thug! Let's see what happens when we write\n->>> # a better prompt and use the right template for this model (through `tokenizer.apply_chat_template`)\n-\n->>> set_seed(0)\n->>> messages = [\n-...     {\n-...         \"role\": \"system\",\n-...         \"content\": \"You are a friendly chatbot who always responds in the style of a thug\",\n-...     },\n-...     {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n-... ]\n->>> model_inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(DEVICE)\n->>> input_length = model_inputs.shape[1]\n->>> generated_ids = model.generate(model_inputs, do_sample=True, max_new_tokens=20)\n->>> print(tokenizer.batch_decode(generated_ids[:, input_length:], skip_special_tokens=True)[0])\n-'None, you thug. How bout you try to focus on more useful questions?'\n->>> # As we can see, it followed a proper thug style ðŸ˜Ž\n-```\n+</hfoption>\n+</hfoptions>\n+\n+### Prompt format\n+\n+Some models and tasks expect a certain input prompt format, and if the format is incorrect, the model returns a suboptimal output. You can learn more about prompting in the [prompt engineering](./tasks/prompting) guide.\n+\n+For example, a chat model expects the input as a [chat template](./chat_templating). Your prompt should include a `role` and `content` to indicate who is participating in the conversation. If you try to pass your prompt as a single string, the model doesn't always return the expected output.\n \n-## Further resources\n+```py\n+from transformers import AutoTokenizer, AutoModelForCausalLM\n \n-While the autoregressive generation process is relatively straightforward, making the most out of your LLM can be a challenging endeavor because there are many moving parts. For your next steps to help you dive deeper into LLM usage and understanding:\n+tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-alpha\")\n+model = AutoModelForCausalLM.from_pretrained(\n+    \"HuggingFaceH4/zephyr-7b-alpha\", device_map=\"auto\", load_in_4bit=True\n+)\n+```\n \n-### Advanced generate usage\n+<hfoptions id=\"format\">\n+<hfoption id=\"no format\">\n \n-1. Guide on how to [control different generation methods](generation_strategies), how to set up the generation configuration file, and how to stream the output;\n-2. [Accelerating text generation](llm_optims);\n-3. [Prompt templates for chat LLMs](chat_templating);\n-4. [Prompt design guide](tasks/prompting);\n-5. API reference on [`~generation.GenerationConfig`], [`~generation.GenerationMixin.generate`], and [generate-related classes](internal/generation_utils). Most of the classes, including the logits processors, have usage examples!\n+```py\n+prompt = \"\"\"How many cats does it take to change a light bulb? Reply as a pirate.\"\"\"\n+model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n+input_length = model_inputs.input_ids.shape[1]\n+generated_ids = model.generate(**model_inputs, max_new_tokens=50)\n+print(tokenizer.batch_decode(generated_ids[:, input_length:], skip_special_tokens=True)[0])\n+\"Aye, matey! 'Tis a simple task for a cat with a keen eye and nimble paws. First, the cat will climb up the ladder, carefully avoiding the rickety rungs. Then, with\"\n+```\n \n-### LLM leaderboards\n+</hfoption>\n+<hfoption id=\"chat template\">\n \n-1. [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard), which focuses on the quality of the open-source models;\n-2. [Open LLM-Perf Leaderboard](https://huggingface.co/spaces/optimum/llm-perf-leaderboard), which focuses on LLM throughput.\n+```py\n+messages = [\n+    {\n+        \"role\": \"system\",\n+        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n+    },\n+    {\"role\": \"user\", \"content\": \"How many cats does it take to change a light bulb?\"},\n+]\n+model_inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n+input_length = model_inputs.shape[1]\n+generated_ids = model.generate(model_inputs, do_sample=True, max_new_tokens=50)\n+print(tokenizer.batch_decode(generated_ids[:, input_length:], skip_special_tokens=True)[0])\n+\"Arr, matey! According to me beliefs, 'twas always one cat to hold the ladder and another to climb up it anâ€™ change the light bulb, but if yer looking to save some catnip, maybe yer can\n+```\n \n-### Latency, throughput and memory utilization\n+</hfoption>\n+</hfoptions>\n \n-1. Guide on how to [optimize LLMs for speed and memory](llm_tutorial_optimization);\n-2. Guide on [quantization](main_classes/quantization) such as bitsandbytes and autogptq, which shows you how to drastically reduce your memory requirements.\n+## Resources\n \n-### Related libraries\n+Take a look below for some more specific and specialized text generation libraries.\n \n-1. [`optimum`](https://github.com/huggingface/optimum), an extension of ðŸ¤— Transformers that optimizes for specific hardware devices;\n-2. [`outlines`](https://github.com/outlines-dev/outlines), a library where you can constrain text generation (e.g. to generate JSON files);\n-3. [`SynCode`](https://github.com/uiuc-focal-lab/syncode), a library for context-free grammar guided generation (e.g. JSON, SQL, Python);\n-4. [`text-generation-inference`](https://github.com/huggingface/text-generation-inference), a production-ready server for LLMs;\n-5. [`text-generation-webui`](https://github.com/oobabooga/text-generation-webui), a UI for text generation;\n-6. [`logits-processor-zoo`](https://github.com/NVIDIA/logits-processor-zoo), containing additional options to control text generation with ðŸ¤— Transformers. See our related [blog post](https://huggingface.co/blog/logits-processor-zoo).\n+- [Optimum](https://github.com/huggingface/optimum): an extension of Transformers focused on optimizing training and inference on specific hardware devices\n+- [Outlines](https://github.com/dottxt-ai/outlines): a library for constrained text generation (generate JSON files for example).\n+- [SynCode](https://github.com/uiuc-focal-lab/syncode): a library for context-free grammar guided generation (JSON, SQL, Python).\n+- [Text Generation Inference](https://github.com/huggingface/text-generation-inference): a production-ready server for LLMs.\n+- [Text generation web UI](https://github.com/oobabooga/text-generation-webui): a Gradio web UI for text generation.\n+- [logits-processor-zoo](https://github.com/NVIDIA/logits-processor-zoo): additional logits processors for controlling text generation.\n\\ No newline at end of file"
        },
        {
            "sha": "ef972789553a563cf89c154709d073986ae0b88f",
            "filename": "docs/source/en/llm_tutorial_optimization.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fllm_tutorial_optimization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fllm_tutorial_optimization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fllm_tutorial_optimization.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -8,6 +8,7 @@ specific language governing permissions and limitations under the License.\n âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n rendered properly in your Markdown viewer.\n -->\n+\n # Optimizing LLMs for Speed and Memory\n \n [[open-in-colab]]"
        },
        {
            "sha": "85790f120ebf41a249bd0776ee2d45bfa3c92a41",
            "filename": "docs/source/en/main_classes/peft.md",
            "status": "renamed",
            "additions": 12,
            "deletions": 8,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmain_classes%2Fpeft.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmain_classes%2Fpeft.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmain_classes%2Fpeft.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -1,19 +1,23 @@\n <!--Copyright 2024 The HuggingFace Team. All rights reserved.\n-\n Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n the License. You may obtain a copy of the License at\n-\n http://www.apache.org/licenses/LICENSE-2.0\n-\n Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n specific language governing permissions and limitations under the License.\n-\n âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n rendered properly in your Markdown viewer.\n-\n -->\n-# Agents, supercharged - Multi-agents, External tools, and more\n \n-> [!WARNING]\n-> This subpackage will soon be deprecated, since it has ben spun off into [smolagents](https://huggingface.co/docs/smolagents/index). Smolagents has extended functionality, and a similar API.\n\\ No newline at end of file\n+# PEFT\n+\n+The [`~integrations.PeftAdapterMixin`] provides functions from the [PEFT](https://huggingface.co/docs/peft/index) library for managing adapters with Transformers. This mixin currently supports LoRA, IA3, and AdaLora. Prefix tuning methods (prompt tuning, prompt learning) aren't supported because they can't be injected into a torch module.\n+\n+[[autodoc]] integrations.PeftAdapterMixin\n+    - load_adapter\n+    - add_adapter\n+    - set_adapter\n+    - disable_adapters\n+    - enable_adapters\n+    - active_adapters\n+    - get_adapter_state_dict",
            "previous_filename": "docs/source/en/agents_advanced.md"
        },
        {
            "sha": "21cd57675e536c916e31a41d6c4739184f696869",
            "filename": "docs/source/en/model_doc/albert.md",
            "status": "modified",
            "additions": 5,
            "deletions": 6,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Falbert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Falbert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Falbert.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -17,12 +17,11 @@ rendered properly in your Markdown viewer.\n # ALBERT\n \n <div class=\"flex flex-wrap space-x-1\">\n-<a href=\"https://huggingface.co/models?filter=albert\">\n-<img alt=\"Models\" src=\"https://img.shields.io/badge/All_model_pages-albert-blueviolet\">\n-</a>\n-<a href=\"https://huggingface.co/spaces/docs-demos/albert-base-v2\">\n-<img alt=\"Spaces\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue\">\n-</a>\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n+<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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\n+\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n </div>\n \n ## Overview"
        },
        {
            "sha": "b2920bdc2bace29574350689cc7f9ee91576b948",
            "filename": "docs/source/en/model_doc/align.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Falign.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Falign.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Falign.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # ALIGN\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The ALIGN model was proposed in [Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision](https://arxiv.org/abs/2102.05918) by Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yunhsuan Sung, Zhen Li, Tom Duerig. ALIGN is a multi-modal vision and language model. It can be used for image-text similarity and for zero-shot image classification. ALIGN features a dual-encoder architecture with [EfficientNet](efficientnet) as its vision encoder and [BERT](bert) as its text encoder, and learns to align visual and text representations with contrastive learning. Unlike previous work, ALIGN leverages a massive noisy dataset and shows that the scale of the corpus can be used to achieve SOTA representations with a simple recipe."
        },
        {
            "sha": "0dfbf797a033de541187619f2432e57c07264a2a",
            "filename": "docs/source/en/model_doc/altclip.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Faltclip.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Faltclip.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Faltclip.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # AltCLIP\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The AltCLIP model was proposed in [AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities](https://arxiv.org/abs/2211.06679v2) by Zhongzhi Chen, Guang Liu, Bo-Wen Zhang, Fulong Ye, Qinghong Yang, Ledell Wu. AltCLIP"
        },
        {
            "sha": "7b58f59cab7ee9e33cdf4c592920f3aa2dd99e9e",
            "filename": "docs/source/en/model_doc/aria.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Faria.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Faria.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Faria.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,12 @@ rendered properly in your Markdown viewer.\n \n # Aria\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The Aria model was proposed in [Aria: An Open Multimodal Native Mixture-of-Experts Model](https://huggingface.co/papers/2410.05993) by Li et al. from the Rhymes.AI team."
        },
        {
            "sha": "4cc07aea758c7568a5b00eec1c6974f232c24e4e",
            "filename": "docs/source/en/model_doc/audio-spectrogram-transformer.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Faudio-spectrogram-transformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Faudio-spectrogram-transformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Faudio-spectrogram-transformer.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,11 @@ rendered properly in your Markdown viewer.\n \n # Audio Spectrogram Transformer\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The Audio Spectrogram Transformer model was proposed in [AST: Audio Spectrogram Transformer](https://arxiv.org/abs/2104.01778) by Yuan Gong, Yu-An Chung, James Glass."
        },
        {
            "sha": "2c5e27153e03188ca08bcecc83d419d9f8481ecb",
            "filename": "docs/source/en/model_doc/autoformer.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fautoformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fautoformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fautoformer.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # Autoformer\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The Autoformer model was proposed in [Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting](https://arxiv.org/abs/2106.13008) by Haixu Wu, Jiehui Xu, Jianmin Wang, Mingsheng Long."
        },
        {
            "sha": "c6e1bcec56a2745e6ed1db7750f6479cc3ba0bb1",
            "filename": "docs/source/en/model_doc/bamba.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fbamba.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fbamba.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbamba.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,11 @@ rendered properly in your Markdown viewer.\n \n # Bamba\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n \n ## Overview\n "
        },
        {
            "sha": "912f552fa7c0c503534ac6517053658de2a717c7",
            "filename": "docs/source/en/model_doc/bark.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fbark.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fbark.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbark.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -12,6 +12,11 @@ specific language governing permissions and limitations under the License.\n \n # Bark\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+</div>\n+\n ## Overview\n \n Bark is a transformer-based text-to-speech model proposed by Suno AI in [suno-ai/bark](https://github.com/suno-ai/bark)."
        },
        {
            "sha": "aaccd78047db100da208b1cab8757562bfd49315",
            "filename": "docs/source/en/model_doc/bart.md",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fbart.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fbart.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbart.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -17,12 +17,12 @@ rendered properly in your Markdown viewer.\n # BART\n \n <div class=\"flex flex-wrap space-x-1\">\n-<a href=\"https://huggingface.co/models?filter=bart\">\n-<img alt=\"Models\" src=\"https://img.shields.io/badge/All_model_pages-bart-blueviolet\">\n-</a>\n-<a href=\"https://huggingface.co/spaces/docs-demos/bart-large-mnli\">\n-<img alt=\"Spaces\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue\">\n-</a>\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n+<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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\n+\">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n </div>\n \n ## Overview"
        },
        {
            "sha": "131b1dd8e185437905832c10120edf13f1e03b7a",
            "filename": "docs/source/en/model_doc/barthez.md",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fbarthez.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fbarthez.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbarthez.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,13 @@ rendered properly in your Markdown viewer.\n \n # BARThez\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n+<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAC0AAAAtCAMAAAANxBKoAAAC7lBMVEUAAADg5vYHPVgAoJH+/v76+v39/f9JbLP///9+AIgAnY3///+mcqzt8fXy9fgkXa3Ax9709fr+///9/f8qXq49qp5AaLGMwrv8/P0eW60VWawxYq8yqJzG2dytt9Wyu9elzci519Lf3O3S2efY3OrY0+Xp7PT///////+dqNCexMc6Z7AGpJeGvbenstPZ5ejQ1OfJzOLa7ejh4+/r8fT29vpccbklWK8PVa0AS6ghW63O498vYa+lsdKz1NDRt9Kw1c672tbD3tnAxt7R6OHp5vDe7OrDyuDn6vLl6/EAQKak0MgATakkppo3ZK/Bz9y8w9yzu9jey97axdvHzeG21NHH4trTwthKZrVGZLSUSpuPQJiGAI+GAI8SWKydycLL4d7f2OTi1+S9xNzL0ePT6OLGzeEAo5U0qJw/aLEAo5JFa7JBabEAp5Y4qZ2QxLyKmsm3kL2xoMOehrRNb7RIbbOZgrGre68AUqwAqZqNN5aKJ5N/lMq+qsd8kMa4pcWzh7muhLMEV69juq2kbKqgUaOTR5uMMZWLLZSGAI5VAIdEAH+ovNDHuNCnxcy3qcaYx8K8msGplrx+wLahjbYdXrV6vbMvYK9DrZ8QrZ8tqJuFms+Sos6sw8ecy8RffsNVeMCvmb43aLltv7Q4Y7EZWK4QWa1gt6meZKUdr6GOAZVeA4xPAISyveLUwtivxtKTpNJ2jcqfvcltiMiwwcfAoMVxhL+Kx7xjdrqTe60tsaNQs6KaRKACrJ6UTZwkqpqTL5pkHY4AloSgsd2ptNXPvNOOncuxxsqFl8lmg8apt8FJcr9EbryGxLqlkrkrY7dRa7ZGZLQ5t6iXUZ6PPpgVpZeJCJFKAIGareTa0+KJod3H0deY2M+esM25usmYu8d2zsJOdcBVvrCLbqcAOaaHaKQAMaScWqKBXqCXMJ2RHpiLF5NmJZAdAHN2kta11dKu1M+DkcZLdb+Mcql3TppyRJdzQ5ZtNZNlIY+DF4+voCOQAAAAZ3RSTlMABAT+MEEJ/RH+/TP+Zlv+pUo6Ifz8+fco/fz6+evr39S9nJmOilQaF/7+/f38+smmoYp6b1T+/v7++vj189zU0tDJxsGzsrKSfv34+Pf27dDOysG9t6+n/vv6+vr59uzr1tG+tZ6Qg9Ym3QAABR5JREFUSMeNlVVUG1EQhpcuxEspXqS0SKEtxQp1d3d332STTRpIQhIISQgJhODu7lAoDoUCpe7u7u7+1puGpqnCPOyZvffbOXPm/PsP9JfQgyCC+tmTABTOcbxDz/heENS7/1F+9nhvkHePG0wNDLbGWwdXL+rbLWvpmZHXD8+gMfBjTh+aSe6Gnn7lwQIOTR0c8wfX3PWgv7avbdKwf/ZoBp1Gp/PvuvXW3vw5ib7emnTW4OR+3D4jB9vjNJ/7gNvfWWeH/TO/JyYrsiKCRjVEZA3UB+96kON+DxOQ/NLE8PE5iUYgIXjFnCOlxEQMaSGVxjg4gxOnEycGz8bptuNjVx08LscIgrzH3umcn+KKtiBIyvzOO2O99aAdR8cF19oZalnCtvREUw79tCd5sow1g1UKM6kXqUx4T8wsi3sTjJ3yzDmmhenLXLpo8u45eG5y4Vvbk6kkC4LLtJMowkSQxmk4ggVJEG+7c6QpHT8vvW9X7/o7+3ELmiJi2mEzZJiz8cT6TBlanBk70cB5GGIGC1gRDdZ00yADLW1FL6gqhtvNXNG5S9gdSrk4M1qu7JAsmYshzDS4peoMrU/gT7qQdqYGZaYhxZmVbGJAm/CS/HloWyhRUlknQ9KYcExTwS80d3VNOxUZJpITYyspl0LbhArhpZCD9cRWEQuhYkNGMHToQ/2Cs6swJlb39CsllxdXX6IUKh/H5jbnSsPKjgmoaFQ1f8wRLR0UnGE/RcDEjj2jXG1WVTwUs8+zxfcrVO+vSsuOpVKxCfYZiQ0/aPKuxQbQ8lIz+DClxC8u+snlcJ7Yr1z1JPqUH0V+GDXbOwAib931Y4Imaq0NTIXPXY+N5L18GJ37SVWu+hwXff8l72Ds9XuwYIBaXPq6Shm4l+Vl/5QiOlV+uTk6YR9PxKsI9xNJny31ygK1e+nIRC1N97EGkFPI+jCpiHe5PCEy7oWqWSwRrpOvhFzcbTWMbm3ZJAOn1rUKpYIt/lDhW/5RHHteeWFN60qo98YJuoq1nK3uW5AabyspC1BcIEpOhft+SZAShYoLSvnmSfnYADUERP5jJn2h5XtsgCRuhYQqAvwTwn33+YWEKUI72HX5AtfSAZDe8F2DtPPm77afhl0EkthzuCQU0BWApgQIH9+KB0JhopMM7bJrdTRoleM2JAVNMyPF+wdoaz+XJpGoVAQ7WXUkcV7gT3oUZyi/ISIJAVKhgNp+4b4veCFhYVJw4locdSjZCp9cPUhLF9EZ3KKzURepMEtCDPP3VcWFx4UIiZIklIpFNfHpdEafIF2aRmOcrUmjohbT2WUllbmRvgfbythbQO3222fpDJoufaQPncYYuqoGtUEsCJZL6/3PR5b4syeSjZMQG/T2maGANlXT2v8S4AULWaUkCxfLyW8iW4kdka+nEMjxpL2NCwsYNBp+Q61PF43zyDg9Bm9+3NNySn78jMZUUkumqE4Gp7JmFOdP1vc8PpRrzj9+wPinCy8K1PiJ4aYbnTYpCCbDkBSbzhu2QJ1Gd82t8jI8TH51+OzvXoWbnXUOBkNW+0mWFwGcGOUVpU81/n3TOHb5oMt2FgYGjzau0Nif0Ss7Q3XB33hjjQHjHA5E5aOyIQc8CBrLdQSs3j92VG+3nNEjbkbdbBr9zm04ruvw37vh0QKOdeGIkckc80fX3KH/h7PT4BOjgCty8VZ5ux1MoO5Cf5naca2LAsEgehI+drX8o/0Nu+W0m6K/I9gGPd/dfx/EN/wN62AhsBWuAAAAAElFTkSuQmCC\n+\">\n+</div>\n+\n ## Overview\n \n The BARThez model was proposed in [BARThez: a Skilled Pretrained French Sequence-to-Sequence Model](https://arxiv.org/abs/2010.12321) by Moussa Kamal Eddine, Antoine J.-P. Tixier, Michalis Vazirgiannis on 23 Oct,"
        },
        {
            "sha": "b3749516323d5fb6481cf60317bf9c0328f1b650",
            "filename": "docs/source/en/model_doc/bartpho.md",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fbartpho.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fbartpho.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbartpho.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,13 @@ rendered properly in your Markdown viewer.\n \n # BARTpho\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n+<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAC0AAAAtCAMAAAANxBKoAAAC7lBMVEUAAADg5vYHPVgAoJH+/v76+v39/f9JbLP///9+AIgAnY3///+mcqzt8fXy9fgkXa3Ax9709fr+///9/f8qXq49qp5AaLGMwrv8/P0eW60VWawxYq8yqJzG2dytt9Wyu9elzci519Lf3O3S2efY3OrY0+Xp7PT///////+dqNCexMc6Z7AGpJeGvbenstPZ5ejQ1OfJzOLa7ejh4+/r8fT29vpccbklWK8PVa0AS6ghW63O498vYa+lsdKz1NDRt9Kw1c672tbD3tnAxt7R6OHp5vDe7OrDyuDn6vLl6/EAQKak0MgATakkppo3ZK/Bz9y8w9yzu9jey97axdvHzeG21NHH4trTwthKZrVGZLSUSpuPQJiGAI+GAI8SWKydycLL4d7f2OTi1+S9xNzL0ePT6OLGzeEAo5U0qJw/aLEAo5JFa7JBabEAp5Y4qZ2QxLyKmsm3kL2xoMOehrRNb7RIbbOZgrGre68AUqwAqZqNN5aKJ5N/lMq+qsd8kMa4pcWzh7muhLMEV69juq2kbKqgUaOTR5uMMZWLLZSGAI5VAIdEAH+ovNDHuNCnxcy3qcaYx8K8msGplrx+wLahjbYdXrV6vbMvYK9DrZ8QrZ8tqJuFms+Sos6sw8ecy8RffsNVeMCvmb43aLltv7Q4Y7EZWK4QWa1gt6meZKUdr6GOAZVeA4xPAISyveLUwtivxtKTpNJ2jcqfvcltiMiwwcfAoMVxhL+Kx7xjdrqTe60tsaNQs6KaRKACrJ6UTZwkqpqTL5pkHY4AloSgsd2ptNXPvNOOncuxxsqFl8lmg8apt8FJcr9EbryGxLqlkrkrY7dRa7ZGZLQ5t6iXUZ6PPpgVpZeJCJFKAIGareTa0+KJod3H0deY2M+esM25usmYu8d2zsJOdcBVvrCLbqcAOaaHaKQAMaScWqKBXqCXMJ2RHpiLF5NmJZAdAHN2kta11dKu1M+DkcZLdb+Mcql3TppyRJdzQ5ZtNZNlIY+DF4+voCOQAAAAZ3RSTlMABAT+MEEJ/RH+/TP+Zlv+pUo6Ifz8+fco/fz6+evr39S9nJmOilQaF/7+/f38+smmoYp6b1T+/v7++vj189zU0tDJxsGzsrKSfv34+Pf27dDOysG9t6+n/vv6+vr59uzr1tG+tZ6Qg9Ym3QAABR5JREFUSMeNlVVUG1EQhpcuxEspXqS0SKEtxQp1d3d332STTRpIQhIISQgJhODu7lAoDoUCpe7u7u7+1puGpqnCPOyZvffbOXPm/PsP9JfQgyCC+tmTABTOcbxDz/heENS7/1F+9nhvkHePG0wNDLbGWwdXL+rbLWvpmZHXD8+gMfBjTh+aSe6Gnn7lwQIOTR0c8wfX3PWgv7avbdKwf/ZoBp1Gp/PvuvXW3vw5ib7emnTW4OR+3D4jB9vjNJ/7gNvfWWeH/TO/JyYrsiKCRjVEZA3UB+96kON+DxOQ/NLE8PE5iUYgIXjFnCOlxEQMaSGVxjg4gxOnEycGz8bptuNjVx08LscIgrzH3umcn+KKtiBIyvzOO2O99aAdR8cF19oZalnCtvREUw79tCd5sow1g1UKM6kXqUx4T8wsi3sTjJ3yzDmmhenLXLpo8u45eG5y4Vvbk6kkC4LLtJMowkSQxmk4ggVJEG+7c6QpHT8vvW9X7/o7+3ELmiJi2mEzZJiz8cT6TBlanBk70cB5GGIGC1gRDdZ00yADLW1FL6gqhtvNXNG5S9gdSrk4M1qu7JAsmYshzDS4peoMrU/gT7qQdqYGZaYhxZmVbGJAm/CS/HloWyhRUlknQ9KYcExTwS80d3VNOxUZJpITYyspl0LbhArhpZCD9cRWEQuhYkNGMHToQ/2Cs6swJlb39CsllxdXX6IUKh/H5jbnSsPKjgmoaFQ1f8wRLR0UnGE/RcDEjj2jXG1WVTwUs8+zxfcrVO+vSsuOpVKxCfYZiQ0/aPKuxQbQ8lIz+DClxC8u+snlcJ7Yr1z1JPqUH0V+GDXbOwAib931Y4Imaq0NTIXPXY+N5L18GJ37SVWu+hwXff8l72Ds9XuwYIBaXPq6Shm4l+Vl/5QiOlV+uTk6YR9PxKsI9xNJny31ygK1e+nIRC1N97EGkFPI+jCpiHe5PCEy7oWqWSwRrpOvhFzcbTWMbm3ZJAOn1rUKpYIt/lDhW/5RHHteeWFN60qo98YJuoq1nK3uW5AabyspC1BcIEpOhft+SZAShYoLSvnmSfnYADUERP5jJn2h5XtsgCRuhYQqAvwTwn33+YWEKUI72HX5AtfSAZDe8F2DtPPm77afhl0EkthzuCQU0BWApgQIH9+KB0JhopMM7bJrdTRoleM2JAVNMyPF+wdoaz+XJpGoVAQ7WXUkcV7gT3oUZyi/ISIJAVKhgNp+4b4veCFhYVJw4locdSjZCp9cPUhLF9EZ3KKzURepMEtCDPP3VcWFx4UIiZIklIpFNfHpdEafIF2aRmOcrUmjohbT2WUllbmRvgfbythbQO3222fpDJoufaQPncYYuqoGtUEsCJZL6/3PR5b4syeSjZMQG/T2maGANlXT2v8S4AULWaUkCxfLyW8iW4kdka+nEMjxpL2NCwsYNBp+Q61PF43zyDg9Bm9+3NNySn78jMZUUkumqE4Gp7JmFOdP1vc8PpRrzj9+wPinCy8K1PiJ4aYbnTYpCCbDkBSbzhu2QJ1Gd82t8jI8TH51+OzvXoWbnXUOBkNW+0mWFwGcGOUVpU81/n3TOHb5oMt2FgYGjzau0Nif0Ss7Q3XB33hjjQHjHA5E5aOyIQc8CBrLdQSs3j92VG+3nNEjbkbdbBr9zm04ruvw37vh0QKOdeGIkckc80fX3KH/h7PT4BOjgCty8VZ5ux1MoO5Cf5naca2LAsEgehI+drX8o/0Nu+W0m6K/I9gGPd/dfx/EN/wN62AhsBWuAAAAAElFTkSuQmCC\n+\">\n+</div>\n+\n ## Overview\n \n The BARTpho model was proposed in [BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese](https://arxiv.org/abs/2109.09701) by Nguyen Luong Tran, Duong Minh Le and Dat Quoc Nguyen."
        },
        {
            "sha": "24dfabf682b6bf652dc9055e5fba2e503b67f891",
            "filename": "docs/source/en/model_doc/beit.md",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fbeit.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fbeit.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbeit.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,13 @@ rendered properly in your Markdown viewer.\n \n # BEiT\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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\n+\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The BEiT model was proposed in [BEiT: BERT Pre-Training of Image Transformers](https://arxiv.org/abs/2106.08254) by"
        },
        {
            "sha": "0c42adbeb56473069df6b0bfd40a4c1bf84b6d82",
            "filename": "docs/source/en/model_doc/bert-generation.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fbert-generation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fbert-generation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbert-generation.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # BertGeneration\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The BertGeneration model is a BERT model that can be leveraged for sequence-to-sequence tasks using"
        },
        {
            "sha": "33a720318b63718ef65a316145a92750e69fb9c1",
            "filename": "docs/source/en/model_doc/bert-japanese.md",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fbert-japanese.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fbert-japanese.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbert-japanese.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,13 @@ rendered properly in your Markdown viewer.\n \n # BertJapanese\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n+<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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\n+\">\n+</div>\n+\n ## Overview\n \n The BERT models trained on Japanese text."
        },
        {
            "sha": "883fba9a076df98021240b6396e9e312228c59b1",
            "filename": "docs/source/en/model_doc/bert.md",
            "status": "modified",
            "additions": 5,
            "deletions": 6,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fbert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fbert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbert.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -17,12 +17,11 @@ rendered properly in your Markdown viewer.\n # BERT\n \n <div class=\"flex flex-wrap space-x-1\">\n-<a href=\"https://huggingface.co/models?filter=bert\">\n-<img alt=\"Models\" src=\"https://img.shields.io/badge/All_model_pages-bert-blueviolet\">\n-</a>\n-<a href=\"https://huggingface.co/spaces/docs-demos/bert-base-uncased\">\n-<img alt=\"Spaces\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue\">\n-</a>\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n+<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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\n+\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n </div>\n \n ## Overview"
        },
        {
            "sha": "be489643173f72a95d7885dd27b8e14e19965a6c",
            "filename": "docs/source/en/model_doc/bertweet.md",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fbertweet.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fbertweet.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbertweet.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,13 @@ rendered properly in your Markdown viewer.\n \n # BERTweet\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n+<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAC0AAAAtCAMAAAANxBKoAAAC7lBMVEUAAADg5vYHPVgAoJH+/v76+v39/f9JbLP///9+AIgAnY3///+mcqzt8fXy9fgkXa3Ax9709fr+///9/f8qXq49qp5AaLGMwrv8/P0eW60VWawxYq8yqJzG2dytt9Wyu9elzci519Lf3O3S2efY3OrY0+Xp7PT///////+dqNCexMc6Z7AGpJeGvbenstPZ5ejQ1OfJzOLa7ejh4+/r8fT29vpccbklWK8PVa0AS6ghW63O498vYa+lsdKz1NDRt9Kw1c672tbD3tnAxt7R6OHp5vDe7OrDyuDn6vLl6/EAQKak0MgATakkppo3ZK/Bz9y8w9yzu9jey97axdvHzeG21NHH4trTwthKZrVGZLSUSpuPQJiGAI+GAI8SWKydycLL4d7f2OTi1+S9xNzL0ePT6OLGzeEAo5U0qJw/aLEAo5JFa7JBabEAp5Y4qZ2QxLyKmsm3kL2xoMOehrRNb7RIbbOZgrGre68AUqwAqZqNN5aKJ5N/lMq+qsd8kMa4pcWzh7muhLMEV69juq2kbKqgUaOTR5uMMZWLLZSGAI5VAIdEAH+ovNDHuNCnxcy3qcaYx8K8msGplrx+wLahjbYdXrV6vbMvYK9DrZ8QrZ8tqJuFms+Sos6sw8ecy8RffsNVeMCvmb43aLltv7Q4Y7EZWK4QWa1gt6meZKUdr6GOAZVeA4xPAISyveLUwtivxtKTpNJ2jcqfvcltiMiwwcfAoMVxhL+Kx7xjdrqTe60tsaNQs6KaRKACrJ6UTZwkqpqTL5pkHY4AloSgsd2ptNXPvNOOncuxxsqFl8lmg8apt8FJcr9EbryGxLqlkrkrY7dRa7ZGZLQ5t6iXUZ6PPpgVpZeJCJFKAIGareTa0+KJod3H0deY2M+esM25usmYu8d2zsJOdcBVvrCLbqcAOaaHaKQAMaScWqKBXqCXMJ2RHpiLF5NmJZAdAHN2kta11dKu1M+DkcZLdb+Mcql3TppyRJdzQ5ZtNZNlIY+DF4+voCOQAAAAZ3RSTlMABAT+MEEJ/RH+/TP+Zlv+pUo6Ifz8+fco/fz6+evr39S9nJmOilQaF/7+/f38+smmoYp6b1T+/v7++vj189zU0tDJxsGzsrKSfv34+Pf27dDOysG9t6+n/vv6+vr59uzr1tG+tZ6Qg9Ym3QAABR5JREFUSMeNlVVUG1EQhpcuxEspXqS0SKEtxQp1d3d332STTRpIQhIISQgJhODu7lAoDoUCpe7u7u7+1puGpqnCPOyZvffbOXPm/PsP9JfQgyCC+tmTABTOcbxDz/heENS7/1F+9nhvkHePG0wNDLbGWwdXL+rbLWvpmZHXD8+gMfBjTh+aSe6Gnn7lwQIOTR0c8wfX3PWgv7avbdKwf/ZoBp1Gp/PvuvXW3vw5ib7emnTW4OR+3D4jB9vjNJ/7gNvfWWeH/TO/JyYrsiKCRjVEZA3UB+96kON+DxOQ/NLE8PE5iUYgIXjFnCOlxEQMaSGVxjg4gxOnEycGz8bptuNjVx08LscIgrzH3umcn+KKtiBIyvzOO2O99aAdR8cF19oZalnCtvREUw79tCd5sow1g1UKM6kXqUx4T8wsi3sTjJ3yzDmmhenLXLpo8u45eG5y4Vvbk6kkC4LLtJMowkSQxmk4ggVJEG+7c6QpHT8vvW9X7/o7+3ELmiJi2mEzZJiz8cT6TBlanBk70cB5GGIGC1gRDdZ00yADLW1FL6gqhtvNXNG5S9gdSrk4M1qu7JAsmYshzDS4peoMrU/gT7qQdqYGZaYhxZmVbGJAm/CS/HloWyhRUlknQ9KYcExTwS80d3VNOxUZJpITYyspl0LbhArhpZCD9cRWEQuhYkNGMHToQ/2Cs6swJlb39CsllxdXX6IUKh/H5jbnSsPKjgmoaFQ1f8wRLR0UnGE/RcDEjj2jXG1WVTwUs8+zxfcrVO+vSsuOpVKxCfYZiQ0/aPKuxQbQ8lIz+DClxC8u+snlcJ7Yr1z1JPqUH0V+GDXbOwAib931Y4Imaq0NTIXPXY+N5L18GJ37SVWu+hwXff8l72Ds9XuwYIBaXPq6Shm4l+Vl/5QiOlV+uTk6YR9PxKsI9xNJny31ygK1e+nIRC1N97EGkFPI+jCpiHe5PCEy7oWqWSwRrpOvhFzcbTWMbm3ZJAOn1rUKpYIt/lDhW/5RHHteeWFN60qo98YJuoq1nK3uW5AabyspC1BcIEpOhft+SZAShYoLSvnmSfnYADUERP5jJn2h5XtsgCRuhYQqAvwTwn33+YWEKUI72HX5AtfSAZDe8F2DtPPm77afhl0EkthzuCQU0BWApgQIH9+KB0JhopMM7bJrdTRoleM2JAVNMyPF+wdoaz+XJpGoVAQ7WXUkcV7gT3oUZyi/ISIJAVKhgNp+4b4veCFhYVJw4locdSjZCp9cPUhLF9EZ3KKzURepMEtCDPP3VcWFx4UIiZIklIpFNfHpdEafIF2aRmOcrUmjohbT2WUllbmRvgfbythbQO3222fpDJoufaQPncYYuqoGtUEsCJZL6/3PR5b4syeSjZMQG/T2maGANlXT2v8S4AULWaUkCxfLyW8iW4kdka+nEMjxpL2NCwsYNBp+Q61PF43zyDg9Bm9+3NNySn78jMZUUkumqE4Gp7JmFOdP1vc8PpRrzj9+wPinCy8K1PiJ4aYbnTYpCCbDkBSbzhu2QJ1Gd82t8jI8TH51+OzvXoWbnXUOBkNW+0mWFwGcGOUVpU81/n3TOHb5oMt2FgYGjzau0Nif0Ss7Q3XB33hjjQHjHA5E5aOyIQc8CBrLdQSs3j92VG+3nNEjbkbdbBr9zm04ruvw37vh0QKOdeGIkckc80fX3KH/h7PT4BOjgCty8VZ5ux1MoO5Cf5naca2LAsEgehI+drX8o/0Nu+W0m6K/I9gGPd/dfx/EN/wN62AhsBWuAAAAAElFTkSuQmCC\n+\">\n+</div>\n+\n ## Overview\n \n The BERTweet model was proposed in [BERTweet: A pre-trained language model for English Tweets](https://www.aclweb.org/anthology/2020.emnlp-demos.2.pdf) by Dat Quoc Nguyen, Thanh Vu, Anh Tuan Nguyen."
        },
        {
            "sha": "32ca5a2062a2ee6aa5b95f384a8437809889fd44",
            "filename": "docs/source/en/model_doc/big_bird.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fbig_bird.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fbig_bird.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbig_bird.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,12 @@ rendered properly in your Markdown viewer.\n \n # BigBird\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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\n+\">\n+</div>\n+\n ## Overview\n \n The BigBird model was proposed in [Big Bird: Transformers for Longer Sequences](https://arxiv.org/abs/2007.14062) by"
        },
        {
            "sha": "499d40b3149b4d8989a3da4ba0fc3bc35af041ce",
            "filename": "docs/source/en/model_doc/bigbird_pegasus.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fbigbird_pegasus.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fbigbird_pegasus.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbigbird_pegasus.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # BigBirdPegasus\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The BigBird model was proposed in [Big Bird: Transformers for Longer Sequences](https://arxiv.org/abs/2007.14062) by"
        },
        {
            "sha": "ab8aea6c29e855ad07db816b8a27644bb2f9dc47",
            "filename": "docs/source/en/model_doc/biogpt.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fbiogpt.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fbiogpt.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbiogpt.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,11 @@ rendered properly in your Markdown viewer.\n \n # BioGPT\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The BioGPT model was proposed in [BioGPT: generative pre-trained transformer for biomedical text generation and mining](https://academic.oup.com/bib/advance-article/doi/10.1093/bib/bbac409/6713511?guestAccessKey=a66d9b5d-4f83-4017-bb52-405815c907b9) by Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon and Tie-Yan Liu. BioGPT is a domain-specific generative pre-trained Transformer language model for biomedical text generation and mining. BioGPT follows the Transformer language model backbone, and is pre-trained on 15M PubMed abstracts from scratch."
        },
        {
            "sha": "550c07662dd7a3d42437487e571cd83485078940",
            "filename": "docs/source/en/model_doc/bit.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fbit.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fbit.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbit.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # Big Transfer (BiT)\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The BiT model was proposed in [Big Transfer (BiT): General Visual Representation Learning](https://arxiv.org/abs/1912.11370) by Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, Neil Houlsby."
        },
        {
            "sha": "647a865de339ad9a1cdde43641a6c9c962329915",
            "filename": "docs/source/en/model_doc/blenderbot-small.md",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fblenderbot-small.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fblenderbot-small.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fblenderbot-small.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,13 @@ rendered properly in your Markdown viewer.\n \n # Blenderbot Small\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n+<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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\n+\">\n+</div>\n+\n Note that [`BlenderbotSmallModel`] and\n [`BlenderbotSmallForConditionalGeneration`] are only used in combination with the checkpoint\n [facebook/blenderbot-90M](https://huggingface.co/facebook/blenderbot-90M). Larger Blenderbot checkpoints should"
        },
        {
            "sha": "ec24d5ed7495191393cfca0afca97b26c5c26887",
            "filename": "docs/source/en/model_doc/blenderbot.md",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fblenderbot.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fblenderbot.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fblenderbot.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,13 @@ rendered properly in your Markdown viewer.\n \n # Blenderbot\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n+<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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\n+\">\n+</div>\n+\n ## Overview\n \n The Blender chatbot model was proposed in [Recipes for building an open-domain chatbot](https://arxiv.org/pdf/2004.13637.pdf) Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu,"
        },
        {
            "sha": "94331d9a5f6e7c069070ea8069e858d24fe6eb66",
            "filename": "docs/source/en/model_doc/blip-2.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fblip-2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fblip-2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fblip-2.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # BLIP-2\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The BLIP-2 model was proposed in [BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://arxiv.org/abs/2301.12597) by"
        },
        {
            "sha": "1acf172f26b8cef3cbd2054c887a575d7fbce089",
            "filename": "docs/source/en/model_doc/blip.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fblip.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fblip.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fblip.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,11 @@ rendered properly in your Markdown viewer.\n \n # BLIP\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The BLIP model was proposed in [BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation](https://arxiv.org/abs/2201.12086) by Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi."
        },
        {
            "sha": "9de98705957475c3fa0bd551aff0733c0a854626",
            "filename": "docs/source/en/model_doc/bloom.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fbloom.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fbloom.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbloom.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,12 @@ rendered properly in your Markdown viewer.\n \n # BLOOM\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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\n+\">\n+</div>\n+\n ## Overview\n \n The BLOOM model has been proposed with its various versions through the [BigScience Workshop](https://bigscience.huggingface.co/). BigScience is inspired by other open science initiatives where researchers have pooled their time and resources to collectively achieve a higher impact."
        },
        {
            "sha": "04cc2feb063b99b87d9a2bd7931d9f422d6bc4b2",
            "filename": "docs/source/en/model_doc/bort.md",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fbort.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fbort.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbort.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,13 @@ rendered properly in your Markdown viewer.\n \n # BORT\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n+<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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\n+\">\n+</div>\n+\n <Tip warning={true}>\n \n This model is in maintenance mode only, we do not accept any new PRs changing its code."
        },
        {
            "sha": "2aee4cdebe5db9b0c183469159059cda16c113bf",
            "filename": "docs/source/en/model_doc/bridgetower.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fbridgetower.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fbridgetower.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbridgetower.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # BridgeTower\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The BridgeTower model was proposed in [BridgeTower: Building Bridges Between Encoders in Vision-Language Representative Learning](https://arxiv.org/abs/2206.08657) by Xiao Xu, Chenfei Wu, Shachar Rosenman, Vasudev Lal, Wanxiang Che, Nan Duan. The goal of this model is to build a"
        },
        {
            "sha": "baa658e598fbc6a5c1c83c41077a148ffdb0c1f7",
            "filename": "docs/source/en/model_doc/bros.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fbros.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fbros.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbros.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -12,6 +12,10 @@ specific language governing permissions and limitations under the License.\n \n # BROS\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The BROS model was proposed in [BROS: A Pre-trained Language Model Focusing on Text and Layout for Better Key Information Extraction from Documents](https://arxiv.org/abs/2108.04539) by Teakgyu Hong, Donghyun Kim, Mingi Ji, Wonseok Hwang, Daehyun Nam, Sungrae Park."
        },
        {
            "sha": "7e95bae53e87fdcebebd6c0f4e512c22a36fff41",
            "filename": "docs/source/en/model_doc/byt5.md",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fbyt5.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fbyt5.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbyt5.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,13 @@ rendered properly in your Markdown viewer.\n \n # ByT5\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n+<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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\n+\">\n+</div>\n+\n ## Overview\n \n The ByT5 model was presented in [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626) by Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir"
        },
        {
            "sha": "9066ee360c6c97301ec164a7c4985abe327390df",
            "filename": "docs/source/en/model_doc/camembert.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fcamembert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fcamembert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcamembert.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,12 @@ rendered properly in your Markdown viewer.\n \n # CamemBERT\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The CamemBERT model was proposed in [CamemBERT: a Tasty French Language Model](https://arxiv.org/abs/1911.03894) by"
        },
        {
            "sha": "cd1cce34c79c3fac14b5cc977dcf51b65ce99540",
            "filename": "docs/source/en/model_doc/canine.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fcanine.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fcanine.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcanine.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # CANINE\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The CANINE model was proposed in [CANINE: Pre-training an Efficient Tokenization-Free Encoder for Language"
        },
        {
            "sha": "3810b3590a00650e58ae58f6a5ab0547e3606791",
            "filename": "docs/source/en/model_doc/chameleon.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fchameleon.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fchameleon.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fchameleon.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,12 @@ rendered properly in your Markdown viewer.\n \n # Chameleon\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The Chameleon model was proposed in [Chameleon: Mixed-Modal Early-Fusion Foundation Models"
        },
        {
            "sha": "c73fee0422f0fe3fb4569891a400067eba788ea0",
            "filename": "docs/source/en/model_doc/chinese_clip.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fchinese_clip.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fchinese_clip.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fchinese_clip.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # Chinese-CLIP\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The Chinese-CLIP model was proposed in [Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese](https://arxiv.org/abs/2211.01335) by An Yang, Junshu Pan, Junyang Lin, Rui Men, Yichang Zhang, Jingren Zhou, Chang Zhou."
        },
        {
            "sha": "e060662c01a9edd0acfa176f0fdb306f92b612c6",
            "filename": "docs/source/en/model_doc/clap.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fclap.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fclap.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fclap.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # CLAP\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The CLAP model was proposed in [Large Scale Contrastive Language-Audio pretraining with"
        },
        {
            "sha": "2e1c5168ce710e6feec04eecb7b25b6596020329",
            "filename": "docs/source/en/model_doc/clip.md",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fclip.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fclip.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fclip.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,15 @@ rendered properly in your Markdown viewer.\n \n # CLIP\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n+<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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\n+\">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The CLIP model was proposed in [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020) by Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,"
        },
        {
            "sha": "f594dbc3e0f3a238eaa701d286f7a8e0246ed86d",
            "filename": "docs/source/en/model_doc/clipseg.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fclipseg.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fclipseg.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fclipseg.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # CLIPSeg\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The CLIPSeg model was proposed in [Image Segmentation Using Text and Image Prompts](https://arxiv.org/abs/2112.10003) by Timo LÃ¼ddecke"
        },
        {
            "sha": "cfa4f97b828614ee817666d9324afc8f9b2ad1e6",
            "filename": "docs/source/en/model_doc/clvp.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fclvp.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fclvp.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fclvp.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # CLVP\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The CLVP (Contrastive Language-Voice Pretrained Transformer) model was proposed in [Better speech synthesis through scaling](https://arxiv.org/abs/2305.07243) by James Betker."
        },
        {
            "sha": "ff3e66769c9cc4aeb77603455197d668383a8b45",
            "filename": "docs/source/en/model_doc/code_llama.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fcode_llama.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fcode_llama.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcode_llama.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,12 @@ rendered properly in your Markdown viewer.\n \n # CodeLlama\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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\n+\">\n+</div>\n+\n ## Overview\n \n The Code Llama model was proposed in [Code Llama: Open Foundation Models for Code](https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/) by Baptiste RoziÃ¨re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, JÃ©rÃ©my Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre DÃ©fossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, Gabriel Synnaeve."
        },
        {
            "sha": "465c8e5445b898dedc28dd900dd14b4b6c7459dd",
            "filename": "docs/source/en/model_doc/codegen.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fcodegen.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fcodegen.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcodegen.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # CodeGen\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The CodeGen model was proposed in [A Conversational Paradigm for Program Synthesis](https://arxiv.org/abs/2203.13474) by Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong."
        },
        {
            "sha": "2ab75e9d1c8bf65ac3d8a5cc313251763542ce48",
            "filename": "docs/source/en/model_doc/cohere.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -1,5 +1,11 @@\n # Cohere\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The Cohere Command-R model was proposed in the blogpost [Command-R: Retrieval Augmented Generation at Production Scale](https://txt.cohere.com/command-r/) by the Cohere Team."
        },
        {
            "sha": "3b0b6e1740a976d2bce8ce147043d5fe9aaaa5b7",
            "filename": "docs/source/en/model_doc/cohere2.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere2.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -1,5 +1,11 @@\n # Cohere\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n [C4AI Command R7B](https://cohere.com/blog/command-r7b) is an open weights research release of a 7B billion parameter model developed by Cohere and Cohere For AI. It has advanced capabilities optimized for various use cases, including reasoning, summarization, question answering, and code. The model is trained to perform sophisticated tasks including Retrieval Augmented Generation (RAG) and tool use. The model also has powerful agentic capabilities that can use and combine multiple tools over multiple steps to accomplish more difficult tasks. It obtains top performance on enterprise-relevant code use cases. C4AI Command R7B is a multilingual model trained on 23 languages.\n "
        },
        {
            "sha": "07c4b45f140a2eac54b19ac503873291ecd5526c",
            "filename": "docs/source/en/model_doc/colpali.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fcolpali.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fcolpali.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcolpali.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # ColPali\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The *ColPali* model was proposed in [ColPali: Efficient Document Retrieval with Vision Language Models](https://doi.org/10.48550/arXiv.2407.01449) by **Manuel Faysse***, **Hugues Sibille***, **Tony Wu***, Bilel Omrani, Gautier Viaud, CÃ©line Hudelot, Pierre Colombo (* denotes equal contribution). Work lead by ILLUIN Technology."
        },
        {
            "sha": "6a03d14d969ca89268b7c8f6832653f93b5668c8",
            "filename": "docs/source/en/model_doc/conditional_detr.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fconditional_detr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fconditional_detr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fconditional_detr.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # Conditional DETR\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The Conditional DETR model was proposed in [Conditional DETR for Fast Training Convergence](https://arxiv.org/abs/2108.06152) by Depu Meng, Xiaokang Chen, Zejia Fan, Gang Zeng, Houqiang Li, Yuhui Yuan, Lei Sun, Jingdong Wang. Conditional DETR presents a conditional cross-attention mechanism for fast DETR training. Conditional DETR converges 6.7Ã— to 10Ã— faster than DETR."
        },
        {
            "sha": "e52bbd5c4772ec62e2f06e0dece55abab09b801d",
            "filename": "docs/source/en/model_doc/convbert.md",
            "status": "modified",
            "additions": 2,
            "deletions": 6,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fconvbert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fconvbert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fconvbert.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -17,12 +17,8 @@ rendered properly in your Markdown viewer.\n # ConvBERT\n \n <div class=\"flex flex-wrap space-x-1\">\n-<a href=\"https://huggingface.co/models?filter=convbert\">\n-<img alt=\"Models\" src=\"https://img.shields.io/badge/All_model_pages-convbert-blueviolet\">\n-</a>\n-<a href=\"https://huggingface.co/spaces/docs-demos/conv-bert-base\">\n-<img alt=\"Spaces\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue\">\n-</a>\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n </div>\n \n ## Overview"
        },
        {
            "sha": "576e95ee043d11e46d5ba583459ff00c7702669d",
            "filename": "docs/source/en/model_doc/convnext.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fconvnext.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fconvnext.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fconvnext.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,11 @@ rendered properly in your Markdown viewer.\n \n # ConvNeXT\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The ConvNeXT model was proposed in [A ConvNet for the 2020s](https://arxiv.org/abs/2201.03545) by Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, Saining Xie."
        },
        {
            "sha": "87a261b8dede7bc140fb010e19bd790e71ea6b92",
            "filename": "docs/source/en/model_doc/convnextv2.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fconvnextv2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fconvnextv2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fconvnextv2.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,11 @@ rendered properly in your Markdown viewer.\n \n # ConvNeXt V2\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The ConvNeXt V2 model was proposed in [ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders](https://arxiv.org/abs/2301.00808) by Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon, Saining Xie."
        },
        {
            "sha": "8a1826a25c6d2bcc027036875907e8a1b69dc33b",
            "filename": "docs/source/en/model_doc/cpm.md",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fcpm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fcpm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcpm.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,13 @@ rendered properly in your Markdown viewer.\n \n # CPM\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n+<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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\n+\">\n+</div>\n+\n ## Overview\n \n The CPM model was proposed in [CPM: A Large-scale Generative Chinese Pre-trained Language Model](https://arxiv.org/abs/2012.00413) by Zhengyan Zhang, Xu Han, Hao Zhou, Pei Ke, Yuxian Gu, Deming Ye, Yujia Qin,"
        },
        {
            "sha": "f8e2b3b515ece21689dafffb3ac3869ff659fb9c",
            "filename": "docs/source/en/model_doc/cpmant.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fcpmant.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fcpmant.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcpmant.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # CPMAnt\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n CPM-Ant is an open-source Chinese pre-trained language model (PLM) with 10B parameters. It is also the first milestone of the live training process of CPM-Live. The training process is cost-effective and environment-friendly. CPM-Ant also achieves promising results with delta tuning on the CUGE benchmark. Besides the full model, we also provide various compressed versions to meet the requirements of different hardware configurations. [See more](https://github.com/OpenBMB/CPM-Live/tree/cpm-ant/cpm-live)"
        },
        {
            "sha": "0253d4e007e04421591181aab2b1a2f495ec53de",
            "filename": "docs/source/en/model_doc/ctrl.md",
            "status": "modified",
            "additions": 2,
            "deletions": 6,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fctrl.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fctrl.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fctrl.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -17,12 +17,8 @@ rendered properly in your Markdown viewer.\n # CTRL\n \n <div class=\"flex flex-wrap space-x-1\">\n-<a href=\"https://huggingface.co/models?filter=ctrl\">\n-<img alt=\"Models\" src=\"https://img.shields.io/badge/All_model_pages-ctrl-blueviolet\">\n-</a>\n-<a href=\"https://huggingface.co/spaces/docs-demos/tiny-ctrl\">\n-<img alt=\"Spaces\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue\">\n-</a>\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n </div>\n \n ## Overview"
        },
        {
            "sha": "fec632ed84d1091b360ddfe3196cb2ba484c7f93",
            "filename": "docs/source/en/model_doc/cvt.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fcvt.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fcvt.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcvt.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,11 @@ rendered properly in your Markdown viewer.\n \n # Convolutional Vision Transformer (CvT)\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The CvT model was proposed in [CvT: Introducing Convolutions to Vision Transformers](https://arxiv.org/abs/2103.15808) by Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan and Lei Zhang. The Convolutional vision Transformer (CvT) improves the [Vision Transformer (ViT)](vit) in performance and efficiency by introducing convolutions into ViT to yield the best of both designs."
        },
        {
            "sha": "d19b45b486b0a1d432d58a10d228dca8c8cfa6d1",
            "filename": "docs/source/en/model_doc/dab-detr.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fdab-detr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fdab-detr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdab-detr.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # DAB-DETR\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The DAB-DETR model was proposed in [DAB-DETR: Dynamic Anchor Boxes are Better Queries for DETR](https://arxiv.org/abs/2201.12329) by Shilong Liu, Feng Li, Hao Zhang, Xiao Yang, Xianbiao Qi, Hang Su, Jun Zhu, Lei Zhang."
        },
        {
            "sha": "3ee4d92b58e0277b868db701596adf1ea3c95023",
            "filename": "docs/source/en/model_doc/dac.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fdac.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fdac.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdac.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # DAC\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n "
        },
        {
            "sha": "62ddbd8ff184b63ec36df8c2a5c7df8df6ae66d6",
            "filename": "docs/source/en/model_doc/data2vec.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fdata2vec.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fdata2vec.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdata2vec.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,12 @@ rendered properly in your Markdown viewer.\n \n # Data2Vec\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The Data2Vec model was proposed in [data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language](https://arxiv.org/pdf/2202.03555) by Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu and Michael Auli."
        },
        {
            "sha": "11463e93d16024411f9232fc57f69a9a95028814",
            "filename": "docs/source/en/model_doc/dbrx.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fdbrx.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fdbrx.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdbrx.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -12,6 +12,12 @@ specific language governing permissions and limitations under the License.\n \n # DBRX\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n DBRX is a [transformer-based](https://www.isattentionallyouneed.com/) decoder-only large language model (LLM) that was trained using next-token prediction."
        },
        {
            "sha": "2e48a3e9a7fca3b9fa004250be2bc8233c5e9b37",
            "filename": "docs/source/en/model_doc/deberta-v2.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeberta-v2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeberta-v2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeberta-v2.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,11 @@ rendered properly in your Markdown viewer.\n \n # DeBERTa-v2\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The DeBERTa model was proposed in [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654) by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen It is based on Google's"
        },
        {
            "sha": "39afe83f5fe3f1794b8f3fc0067c154dae08ec94",
            "filename": "docs/source/en/model_doc/deberta.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeberta.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeberta.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeberta.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,11 @@ rendered properly in your Markdown viewer.\n \n # DeBERTa\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The DeBERTa model was proposed in [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654) by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen It is based on Google's"
        },
        {
            "sha": "fb932ce3ec7a6aceee46673ef80f90cdabf4e991",
            "filename": "docs/source/en/model_doc/decision_transformer.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fdecision_transformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fdecision_transformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdecision_transformer.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # Decision Transformer\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The Decision Transformer model was proposed in [Decision Transformer: Reinforcement Learning via Sequence Modeling](https://arxiv.org/abs/2106.01345)  "
        },
        {
            "sha": "5b83f23cf5b3f98a629527825871ee779a41b296",
            "filename": "docs/source/en/model_doc/deformable_detr.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeformable_detr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeformable_detr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeformable_detr.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # Deformable DETR\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The Deformable DETR model was proposed in [Deformable DETR: Deformable Transformers for End-to-End Object Detection](https://arxiv.org/abs/2010.04159) by Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, Jifeng Dai."
        },
        {
            "sha": "0750d4000a442b6f3f1326c314e6aae5349738d3",
            "filename": "docs/source/en/model_doc/deit.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeit.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeit.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeit.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,12 @@ rendered properly in your Markdown viewer.\n \n # DeiT\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The DeiT model was proposed in [Training data-efficient image transformers & distillation through attention](https://arxiv.org/abs/2012.12877) by Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre"
        },
        {
            "sha": "d3c0de7b7f84f44f720e2288bbd3ba77ea2c6268",
            "filename": "docs/source/en/model_doc/deplot.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeplot.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeplot.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeplot.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # DePlot\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview \n \n DePlot was proposed in the paper [DePlot: One-shot visual language reasoning by plot-to-table translation](https://arxiv.org/abs/2212.10505) from Fangyu Liu, Julian Martin Eisenschlos, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee, Mandar Joshi, Wenhu Chen, Nigel Collier, Yasemin Altun."
        },
        {
            "sha": "07bed70880375859b029773582df6262e681c790",
            "filename": "docs/source/en/model_doc/depth_anything.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fdepth_anything.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fdepth_anything.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdepth_anything.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # Depth Anything\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The Depth Anything model was proposed in [Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data](https://arxiv.org/abs/2401.10891) by Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, Hengshuang Zhao. Depth Anything is based on the [DPT](dpt) architecture, trained on ~62 million images, obtaining state-of-the-art results for both relative and absolute depth estimation."
        },
        {
            "sha": "91cbb61907e6bfcc5e8d0a41609b28a28b3a3439",
            "filename": "docs/source/en/model_doc/depth_pro.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fdepth_pro.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fdepth_pro.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdepth_pro.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # DepthPro\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The DepthPro model was proposed in [Depth Pro: Sharp Monocular Metric Depth in Less Than a Second](https://arxiv.org/abs/2410.02073) by Aleksei Bochkovskii, AmaÃ«l Delaunoy, Hugo Germain, Marcel Santos, Yichao Zhou, Stephan R. Richter, Vladlen Koltun."
        },
        {
            "sha": "e3859341a71af9ec7ae0d74ba9cfb4882587fe6a",
            "filename": "docs/source/en/model_doc/deta.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeta.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeta.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeta.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # DETA\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n <Tip warning={true}>\n \n This model is in maintenance mode only, we don't accept any new PRs changing its code."
        },
        {
            "sha": "4614d549a180fe51d9d1947471dabc8c126fbc11",
            "filename": "docs/source/en/model_doc/detr.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fdetr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fdetr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdetr.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # DETR\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The DETR model was proposed in [End-to-End Object Detection with Transformers](https://arxiv.org/abs/2005.12872) by"
        },
        {
            "sha": "33d7e3b16d8869218a0c198e31cfbc195989989e",
            "filename": "docs/source/en/model_doc/dialogpt.md",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fdialogpt.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fdialogpt.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdialogpt.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,13 @@ rendered properly in your Markdown viewer.\n \n # DialoGPT\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n+<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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\n+\">\n+</div>\n+\n ## Overview\n \n DialoGPT was proposed in [DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation](https://arxiv.org/abs/1911.00536) by Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao,"
        },
        {
            "sha": "c4a170c265724dae6010ddb31d4e7e7cd296af13",
            "filename": "docs/source/en/model_doc/diffllama.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fdiffllama.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fdiffllama.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdiffllama.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,12 @@ rendered properly in your Markdown viewer.\n \n # DiffLlama\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The DiffLlama model was proposed in [Differential Transformer](https://arxiv.org/abs/2410.05258) by Kazuma Matsumoto and ."
        },
        {
            "sha": "cd1d67073be68770b94ff4442887853fbf788bc5",
            "filename": "docs/source/en/model_doc/dinat.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fdinat.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fdinat.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdinat.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # Dilated Neighborhood Attention Transformer\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n DiNAT was proposed in [Dilated Neighborhood Attention Transformer](https://arxiv.org/abs/2209.15001)"
        },
        {
            "sha": "5c130dabda90d37e9aba00c40dcbda7b2c6ee550",
            "filename": "docs/source/en/model_doc/dinov2.md",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fdinov2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fdinov2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdinov2.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -12,6 +12,13 @@ specific language governing permissions and limitations under the License.\n \n # DINOv2\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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\n+\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The DINOv2 model was proposed in [DINOv2: Learning Robust Visual Features without Supervision](https://arxiv.org/abs/2304.07193) by"
        },
        {
            "sha": "7151dc45356a6219126c7ead1fd7d082766c0eee",
            "filename": "docs/source/en/model_doc/dinov2_with_registers.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fdinov2_with_registers.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fdinov2_with_registers.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdinov2_with_registers.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -9,6 +9,11 @@ specific language governing permissions and limitations under the License.\n \n # DINOv2 with Registers\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The DINOv2 with Registers model was proposed in [Vision Transformers Need Registers](https://arxiv.org/abs/2309.16588) by TimothÃ©e Darcet, Maxime Oquab, Julien Mairal, Piotr Bojanowski."
        },
        {
            "sha": "66be95fa0406dcd7e8accb7c838966bda52e7ad4",
            "filename": "docs/source/en/model_doc/distilbert.md",
            "status": "modified",
            "additions": 6,
            "deletions": 9,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fdistilbert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fdistilbert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdistilbert.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -17,15 +17,12 @@ rendered properly in your Markdown viewer.\n # DistilBERT\n \n <div class=\"flex flex-wrap space-x-1\">\n-<a href=\"https://huggingface.co/models?filter=distilbert\">\n-<img alt=\"Models\" src=\"https://img.shields.io/badge/All_model_pages-distilbert-blueviolet\">\n-</a>\n-<a href=\"https://huggingface.co/spaces/docs-demos/distilbert-base-uncased\">\n-<img alt=\"Spaces\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue\">\n-</a>\n-<a href=\"https://huggingface.co/papers/1910.01108\">\n-<img alt=\"Paper page\" src=\"https://img.shields.io/badge/Paper%20page-1910.01108-green\">\n-</a>\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n+<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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\n+\">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n </div>\n \n ## Overview"
        },
        {
            "sha": "8848948375e85e5c7e1a68bf869a1469e467d4be",
            "filename": "docs/source/en/model_doc/dit.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fdit.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fdit.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdit.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,12 @@ rendered properly in your Markdown viewer.\n \n # DiT\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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\n+\">\n+</div>\n+\n ## Overview\n \n DiT was proposed in [DiT: Self-supervised Pre-training for Document Image Transformer](https://arxiv.org/abs/2203.02378) by Junlong Li, Yiheng Xu, Tengchao Lv, Lei Cui, Cha Zhang, Furu Wei."
        },
        {
            "sha": "0f6b19c90014b0eef86c9c0c57cb115443ae79f7",
            "filename": "docs/source/en/model_doc/dpr.md",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fdpr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fdpr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdpr.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -17,12 +17,9 @@ rendered properly in your Markdown viewer.\n # DPR\n \n <div class=\"flex flex-wrap space-x-1\">\n-<a href=\"https://huggingface.co/models?filter=dpr\">\n-<img alt=\"Models\" src=\"https://img.shields.io/badge/All_model_pages-dpr-blueviolet\">\n-</a>\n-<a href=\"https://huggingface.co/spaces/docs-demos/dpr-question_encoder-bert-base-multilingual\">\n-<img alt=\"Spaces\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue\">\n-</a>\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n </div>\n \n ## Overview"
        },
        {
            "sha": "7010d03cdc684d752818f54f14c782f556b53c1d",
            "filename": "docs/source/en/model_doc/dpt.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fdpt.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fdpt.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdpt.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # DPT\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The DPT model was proposed in [Vision Transformers for Dense Prediction](https://arxiv.org/abs/2103.13413) by RenÃ© Ranftl, Alexey Bochkovskiy, Vladlen Koltun."
        },
        {
            "sha": "f05ccacc3dbf596ad590ce001b9b3480565bed90",
            "filename": "docs/source/en/model_doc/efficientformer.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fefficientformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fefficientformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fefficientformer.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,11 @@ rendered properly in your Markdown viewer.\n \n # EfficientFormer\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n+</div>\n+\n <Tip warning={true}>\n \n This model is in maintenance mode only, we don't accept any new PRs changing its code."
        },
        {
            "sha": "a34378fa4709bcc49f7a8f95f9f771a729788e94",
            "filename": "docs/source/en/model_doc/efficientnet.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fefficientnet.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fefficientnet.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fefficientnet.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # EfficientNet\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The EfficientNet model was proposed in [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://arxiv.org/abs/1905.11946) "
        },
        {
            "sha": "bee883d64153d2b008b2b2dc1be0a9dffe4ef5ca",
            "filename": "docs/source/en/model_doc/electra.md",
            "status": "modified",
            "additions": 4,
            "deletions": 6,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Felectra.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Felectra.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Felectra.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -17,12 +17,10 @@ rendered properly in your Markdown viewer.\n # ELECTRA\n \n <div class=\"flex flex-wrap space-x-1\">\n-<a href=\"https://huggingface.co/models?filter=electra\">\n-<img alt=\"Models\" src=\"https://img.shields.io/badge/All_model_pages-electra-blueviolet\">\n-</a>\n-<a href=\"https://huggingface.co/spaces/docs-demos/electra_large_discriminator_squad2_512\">\n-<img alt=\"Spaces\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue\">\n-</a>\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n+<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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\n+\">\n </div>\n \n ## Overview"
        },
        {
            "sha": "4ac7d0b0c4f1ae161a49dfe95d1269024f6ca7d7",
            "filename": "docs/source/en/model_doc/emu3.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Femu3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Femu3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Femu3.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,12 @@ rendered properly in your Markdown viewer.\n \n # Emu3\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The Emu3 model was proposed in [Emu3: Next-Token Prediction is All You Need](https://arxiv.org/abs/2409.18869) by Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, Yingli Zhao, Yulong Ao, Xuebin Min, Tao Li, Boya Wu, Bo Zhao, Bowen Zhang, Liangdong Wang, Guang Liu, Zheqi He, Xi Yang, Jingjing Liu, Yonghua Lin, Tiejun Huang, Zhongyuan Wang."
        },
        {
            "sha": "893954d5cf867d54c2578960e7a3b4c71b56b6f0",
            "filename": "docs/source/en/model_doc/encodec.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fencodec.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fencodec.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fencodec.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # EnCodec\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The EnCodec neural codec model was proposed in [High Fidelity Neural Audio Compression](https://arxiv.org/abs/2210.13438) by Alexandre DÃ©fossez, Jade Copet, Gabriel Synnaeve, Yossi Adi."
        },
        {
            "sha": "d0a676fb33a6d6c60e006f60976a8fa8ee26e00e",
            "filename": "docs/source/en/model_doc/encoder-decoder.md",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fencoder-decoder.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fencoder-decoder.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fencoder-decoder.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,14 @@ rendered properly in your Markdown viewer.\n \n # Encoder Decoder Models\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n+<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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\n+\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The [`EncoderDecoderModel`] can be used to initialize a sequence-to-sequence model with any"
        },
        {
            "sha": "82f2a0d5ba817fc38f7db64b696e97980e760b3a",
            "filename": "docs/source/en/model_doc/ernie.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fernie.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fernie.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fernie.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # ERNIE\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n ERNIE is a series of powerful models proposed by baidu, especially in Chinese tasks,\n including [ERNIE1.0](https://arxiv.org/abs/1904.09223), [ERNIE2.0](https://ojs.aaai.org/index.php/AAAI/article/view/6428),"
        },
        {
            "sha": "3ce3b40c44638f3b0750dafba78396e63dac1fb4",
            "filename": "docs/source/en/model_doc/ernie_m.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fernie_m.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fernie_m.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fernie_m.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # ErnieM\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n <Tip warning={true}>\n \n This model is in maintenance mode only, we don't accept any new PRs changing its code."
        },
        {
            "sha": "6061d8eea987fd7a9faaf79c07fd0017ab030fa0",
            "filename": "docs/source/en/model_doc/esm.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fesm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fesm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fesm.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,11 @@ rendered properly in your Markdown viewer.\n \n # ESM\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n+</div>\n+\n ## Overview\n \n This page provides code and pre-trained weights for Transformer protein language models from Meta AI's Fundamental "
        },
        {
            "sha": "1197d208a2ab96241447c6e0b95f6efad0c0e959",
            "filename": "docs/source/en/model_doc/falcon.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Ffalcon.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Ffalcon.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ffalcon.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,12 @@ rendered properly in your Markdown viewer.\n \n # Falcon\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n Falcon is a class of causal decoder-only models built by [TII](https://www.tii.ae/). The largest Falcon checkpoints"
        },
        {
            "sha": "276548be77ad7e9a5e991fc62242999fc1c464f8",
            "filename": "docs/source/en/model_doc/falcon3.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Ffalcon3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Ffalcon3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ffalcon3.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,12 @@ rendered properly in your Markdown viewer.\n \n # Falcon3\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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\n+\">\n+</div>\n+\n ## Overview\n \n Falcon3 represents a natural evolution from previous releases, emphasizing expanding the models' science, math, and code capabilities. This iteration includes five base models: Falcon3-1B-Base, Falcon3-3B-Base, Falcon3-Mamba-7B-Base, Falcon3-7B-Base, and Falcon3-10B-Base. In developing these models, we incorporated several key innovations aimed at improving the models' performances while reducing training costs:"
        },
        {
            "sha": "fb6debfef921e44780f7d2e6e31e66a2be6cf14a",
            "filename": "docs/source/en/model_doc/falcon_mamba.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Ffalcon_mamba.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Ffalcon_mamba.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ffalcon_mamba.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # FalconMamba\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The FalconMamba model was proposed by TII UAE (Technology Innovation Institute) in their release."
        },
        {
            "sha": "aeb055ceae40dffbdb3cc002b3a391e89aae89b4",
            "filename": "docs/source/en/model_doc/fastspeech2_conformer.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Ffastspeech2_conformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Ffastspeech2_conformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ffastspeech2_conformer.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -12,6 +12,10 @@ specific language governing permissions and limitations under the License.\n \n # FastSpeech2Conformer\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The FastSpeech2Conformer model was proposed with the paper [Recent Developments On Espnet Toolkit Boosted By Conformer](https://arxiv.org/abs/2010.13956) by Pengcheng Guo, Florian Boyer, Xuankai Chang, Tomoki Hayashi, Yosuke Higuchi, Hirofumi Inaguma, Naoyuki Kamo, Chenda Li, Daniel Garcia-Romero, Jiatong Shi, Jing Shi, Shinji Watanabe, Kun Wei, Wangyou Zhang, and Yuekai Zhang."
        },
        {
            "sha": "0e3b9ba0738fc99679235c4eb75f62981d6b2fe4",
            "filename": "docs/source/en/model_doc/flan-t5.md",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fflan-t5.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fflan-t5.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fflan-t5.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,13 @@ rendered properly in your Markdown viewer.\n \n # FLAN-T5\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n+<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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\n+\">\n+</div>\n+\n ## Overview\n \n FLAN-T5 was released in the paper [Scaling Instruction-Finetuned Language Models](https://arxiv.org/pdf/2210.11416.pdf) - it is an enhanced version of T5 that has been finetuned in a mixture of tasks."
        },
        {
            "sha": "3b946b909b09ee9d023ffe33f80256aca11c53d3",
            "filename": "docs/source/en/model_doc/flan-ul2.md",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fflan-ul2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fflan-ul2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fflan-ul2.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,13 @@ rendered properly in your Markdown viewer.\n \n # FLAN-UL2\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n+<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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\n+\">\n+</div>\n+\n ## Overview\n \n Flan-UL2 is an encoder decoder model based on the T5 architecture. It uses the same configuration as the [UL2](ul2) model released earlier last year. "
        },
        {
            "sha": "59ab44ebff0387446030c57699e8b3f454b8b3ef",
            "filename": "docs/source/en/model_doc/flaubert.md",
            "status": "modified",
            "additions": 2,
            "deletions": 6,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fflaubert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fflaubert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fflaubert.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -17,12 +17,8 @@ rendered properly in your Markdown viewer.\n # FlauBERT\n \n <div class=\"flex flex-wrap space-x-1\">\n-<a href=\"https://huggingface.co/models?filter=flaubert\">\n-<img alt=\"Models\" src=\"https://img.shields.io/badge/All_model_pages-flaubert-blueviolet\">\n-</a>\n-<a href=\"https://huggingface.co/spaces/docs-demos/flaubert_small_cased\">\n-<img alt=\"Spaces\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue\">\n-</a>\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n </div>\n \n ## Overview"
        },
        {
            "sha": "b32f93fc8bcb1b2b79b5cd269bac1ef5279340da",
            "filename": "docs/source/en/model_doc/flava.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fflava.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fflava.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fflava.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # FLAVA\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The FLAVA model was proposed in [FLAVA: A Foundational Language And Vision Alignment Model](https://arxiv.org/abs/2112.04482) by Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela and is accepted at CVPR 2022."
        },
        {
            "sha": "fcf75e21caed25f830d3dd1d5c98d36627c6522f",
            "filename": "docs/source/en/model_doc/fnet.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Ffnet.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Ffnet.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ffnet.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # FNet\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The FNet model was proposed in [FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/abs/2105.03824) by"
        },
        {
            "sha": "5312cae4ff675e936bd282f9663b64f681fbd994",
            "filename": "docs/source/en/model_doc/focalnet.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Ffocalnet.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Ffocalnet.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ffocalnet.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # FocalNet\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The FocalNet model was proposed in [Focal Modulation Networks](https://arxiv.org/abs/2203.11926) by Jianwei Yang, Chunyuan Li, Xiyang Dai, Lu Yuan, Jianfeng Gao."
        },
        {
            "sha": "96050a153df2aaf8a3f62a669aeb66fc31ec4b86",
            "filename": "docs/source/en/model_doc/funnel.md",
            "status": "modified",
            "additions": 2,
            "deletions": 7,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Ffunnel.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Ffunnel.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ffunnel.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -17,15 +17,10 @@ rendered properly in your Markdown viewer.\n # Funnel Transformer\n \n <div class=\"flex flex-wrap space-x-1\">\n-<a href=\"https://huggingface.co/models?filter=funnel\">\n-<img alt=\"Models\" src=\"https://img.shields.io/badge/All_model_pages-funnel-blueviolet\">\n-</a>\n-<a href=\"https://huggingface.co/spaces/docs-demos/funnel-transformer-small\">\n-<img alt=\"Spaces\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue\">\n-</a>\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n </div>\n \n-\n ## Overview\n \n The Funnel Transformer model was proposed in the paper [Funnel-Transformer: Filtering out Sequential Redundancy for"
        },
        {
            "sha": "c0ea89ad19fb508cca1be90415441d973f019d67",
            "filename": "docs/source/en/model_doc/fuyu.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Ffuyu.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Ffuyu.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ffuyu.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # Fuyu\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The Fuyu model was created by [ADEPT](https://www.adept.ai/blog/fuyu-8b), and authored by Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani, SaÄŸnak TaÅŸÄ±rlar."
        },
        {
            "sha": "144bcf33886b17816fa87b6526ebb0079411d0e3",
            "filename": "docs/source/en/model_doc/gemma.md",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,14 @@ rendered properly in your Markdown viewer.\n \n # Gemma\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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\n+\">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The Gemma model was proposed in [Gemma: Open Models Based on Gemini Technology and Research](https://blog.google/technology/developers/gemma-open-models/) by Gemma Team, Google."
        },
        {
            "sha": "9cf8ff7af102adcc2383a7ceaa835cd81afebcb2",
            "filename": "docs/source/en/model_doc/gemma2.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma2.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -17,6 +17,12 @@ rendered properly in your Markdown viewer.\n \n # Gemma2\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The Gemma2 model was proposed in [Gemma2: Open Models Based on Gemini Technology and Research](https://blog.google/technology/developers/google-gemma-2/) by Gemma2 Team, Google."
        },
        {
            "sha": "825b73c5c59b5092900a08b62116cb234a935e9c",
            "filename": "docs/source/en/model_doc/git.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fgit.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fgit.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgit.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # GIT\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The GIT model was proposed in [GIT: A Generative Image-to-text Transformer for Vision and Language](https://arxiv.org/abs/2205.14100) by"
        },
        {
            "sha": "cfcd549d1493f430ef26ed1789b2e8f3f8dc8a25",
            "filename": "docs/source/en/model_doc/glm.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fglm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fglm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fglm.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,12 @@ rendered properly in your Markdown viewer.\n \n # GLM\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The GLM Model was proposed"
        },
        {
            "sha": "95ecc36bf5b75b7c5c5b5cc100e39f3737836492",
            "filename": "docs/source/en/model_doc/glpn.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fglpn.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fglpn.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fglpn.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # GLPN\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n <Tip>\n \n This is a recently introduced model so the API hasn't been tested extensively. There may be some bugs or slight"
        },
        {
            "sha": "bb14cdbcf8477cdb926a9ce8549492d6a6ec2c0f",
            "filename": "docs/source/en/model_doc/got_ocr2.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fgot_ocr2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fgot_ocr2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgot_ocr2.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # GOT-OCR2\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The GOT-OCR2 model was proposed in [General OCR Theory: Towards OCR-2.0 via a Unified End-to-end Model](https://arxiv.org/abs/2409.01704) by Haoran Wei, Chenglong Liu, Jinyue Chen, Jia Wang, Lingyu Kong, Yanming Xu, Zheng Ge, Liang Zhao, Jianjian Sun, Yuang Peng, Chunrui Han, Xiangyu Zhang."
        },
        {
            "sha": "20daa3537af0f37c48b95385dc288a1629a6814d",
            "filename": "docs/source/en/model_doc/gpt-sw3.md",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt-sw3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt-sw3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt-sw3.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,13 @@ rendered properly in your Markdown viewer.\n \n # GPT-Sw3\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n+<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAC0AAAAtCAMAAAANxBKoAAAC7lBMVEUAAADg5vYHPVgAoJH+/v76+v39/f9JbLP///9+AIgAnY3///+mcqzt8fXy9fgkXa3Ax9709fr+///9/f8qXq49qp5AaLGMwrv8/P0eW60VWawxYq8yqJzG2dytt9Wyu9elzci519Lf3O3S2efY3OrY0+Xp7PT///////+dqNCexMc6Z7AGpJeGvbenstPZ5ejQ1OfJzOLa7ejh4+/r8fT29vpccbklWK8PVa0AS6ghW63O498vYa+lsdKz1NDRt9Kw1c672tbD3tnAxt7R6OHp5vDe7OrDyuDn6vLl6/EAQKak0MgATakkppo3ZK/Bz9y8w9yzu9jey97axdvHzeG21NHH4trTwthKZrVGZLSUSpuPQJiGAI+GAI8SWKydycLL4d7f2OTi1+S9xNzL0ePT6OLGzeEAo5U0qJw/aLEAo5JFa7JBabEAp5Y4qZ2QxLyKmsm3kL2xoMOehrRNb7RIbbOZgrGre68AUqwAqZqNN5aKJ5N/lMq+qsd8kMa4pcWzh7muhLMEV69juq2kbKqgUaOTR5uMMZWLLZSGAI5VAIdEAH+ovNDHuNCnxcy3qcaYx8K8msGplrx+wLahjbYdXrV6vbMvYK9DrZ8QrZ8tqJuFms+Sos6sw8ecy8RffsNVeMCvmb43aLltv7Q4Y7EZWK4QWa1gt6meZKUdr6GOAZVeA4xPAISyveLUwtivxtKTpNJ2jcqfvcltiMiwwcfAoMVxhL+Kx7xjdrqTe60tsaNQs6KaRKACrJ6UTZwkqpqTL5pkHY4AloSgsd2ptNXPvNOOncuxxsqFl8lmg8apt8FJcr9EbryGxLqlkrkrY7dRa7ZGZLQ5t6iXUZ6PPpgVpZeJCJFKAIGareTa0+KJod3H0deY2M+esM25usmYu8d2zsJOdcBVvrCLbqcAOaaHaKQAMaScWqKBXqCXMJ2RHpiLF5NmJZAdAHN2kta11dKu1M+DkcZLdb+Mcql3TppyRJdzQ5ZtNZNlIY+DF4+voCOQAAAAZ3RSTlMABAT+MEEJ/RH+/TP+Zlv+pUo6Ifz8+fco/fz6+evr39S9nJmOilQaF/7+/f38+smmoYp6b1T+/v7++vj189zU0tDJxsGzsrKSfv34+Pf27dDOysG9t6+n/vv6+vr59uzr1tG+tZ6Qg9Ym3QAABR5JREFUSMeNlVVUG1EQhpcuxEspXqS0SKEtxQp1d3d332STTRpIQhIISQgJhODu7lAoDoUCpe7u7u7+1puGpqnCPOyZvffbOXPm/PsP9JfQgyCC+tmTABTOcbxDz/heENS7/1F+9nhvkHePG0wNDLbGWwdXL+rbLWvpmZHXD8+gMfBjTh+aSe6Gnn7lwQIOTR0c8wfX3PWgv7avbdKwf/ZoBp1Gp/PvuvXW3vw5ib7emnTW4OR+3D4jB9vjNJ/7gNvfWWeH/TO/JyYrsiKCRjVEZA3UB+96kON+DxOQ/NLE8PE5iUYgIXjFnCOlxEQMaSGVxjg4gxOnEycGz8bptuNjVx08LscIgrzH3umcn+KKtiBIyvzOO2O99aAdR8cF19oZalnCtvREUw79tCd5sow1g1UKM6kXqUx4T8wsi3sTjJ3yzDmmhenLXLpo8u45eG5y4Vvbk6kkC4LLtJMowkSQxmk4ggVJEG+7c6QpHT8vvW9X7/o7+3ELmiJi2mEzZJiz8cT6TBlanBk70cB5GGIGC1gRDdZ00yADLW1FL6gqhtvNXNG5S9gdSrk4M1qu7JAsmYshzDS4peoMrU/gT7qQdqYGZaYhxZmVbGJAm/CS/HloWyhRUlknQ9KYcExTwS80d3VNOxUZJpITYyspl0LbhArhpZCD9cRWEQuhYkNGMHToQ/2Cs6swJlb39CsllxdXX6IUKh/H5jbnSsPKjgmoaFQ1f8wRLR0UnGE/RcDEjj2jXG1WVTwUs8+zxfcrVO+vSsuOpVKxCfYZiQ0/aPKuxQbQ8lIz+DClxC8u+snlcJ7Yr1z1JPqUH0V+GDXbOwAib931Y4Imaq0NTIXPXY+N5L18GJ37SVWu+hwXff8l72Ds9XuwYIBaXPq6Shm4l+Vl/5QiOlV+uTk6YR9PxKsI9xNJny31ygK1e+nIRC1N97EGkFPI+jCpiHe5PCEy7oWqWSwRrpOvhFzcbTWMbm3ZJAOn1rUKpYIt/lDhW/5RHHteeWFN60qo98YJuoq1nK3uW5AabyspC1BcIEpOhft+SZAShYoLSvnmSfnYADUERP5jJn2h5XtsgCRuhYQqAvwTwn33+YWEKUI72HX5AtfSAZDe8F2DtPPm77afhl0EkthzuCQU0BWApgQIH9+KB0JhopMM7bJrdTRoleM2JAVNMyPF+wdoaz+XJpGoVAQ7WXUkcV7gT3oUZyi/ISIJAVKhgNp+4b4veCFhYVJw4locdSjZCp9cPUhLF9EZ3KKzURepMEtCDPP3VcWFx4UIiZIklIpFNfHpdEafIF2aRmOcrUmjohbT2WUllbmRvgfbythbQO3222fpDJoufaQPncYYuqoGtUEsCJZL6/3PR5b4syeSjZMQG/T2maGANlXT2v8S4AULWaUkCxfLyW8iW4kdka+nEMjxpL2NCwsYNBp+Q61PF43zyDg9Bm9+3NNySn78jMZUUkumqE4Gp7JmFOdP1vc8PpRrzj9+wPinCy8K1PiJ4aYbnTYpCCbDkBSbzhu2QJ1Gd82t8jI8TH51+OzvXoWbnXUOBkNW+0mWFwGcGOUVpU81/n3TOHb5oMt2FgYGjzau0Nif0Ss7Q3XB33hjjQHjHA5E5aOyIQc8CBrLdQSs3j92VG+3nNEjbkbdbBr9zm04ruvw37vh0QKOdeGIkckc80fX3KH/h7PT4BOjgCty8VZ5ux1MoO5Cf5naca2LAsEgehI+drX8o/0Nu+W0m6K/I9gGPd/dfx/EN/wN62AhsBWuAAAAAElFTkSuQmCC\n+\">\n+</div>\n+\n ## Overview\n \n The GPT-Sw3 model was first proposed in"
        },
        {
            "sha": "648fa6cb8d605491129c797ad3f20e0f0e3bff29",
            "filename": "docs/source/en/model_doc/gpt_bigcode.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_bigcode.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_bigcode.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_bigcode.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,12 @@ rendered properly in your Markdown viewer.\n \n # GPTBigCode\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The GPTBigCode model was proposed in [SantaCoder: don't reach for the stars!](https://arxiv.org/abs/2301.03988) by BigCode. The listed authors are: Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, Logesh Kumar Umapathi, Carolyn Jane Anderson, Yangtian Zi, Joel Lamy Poirier, Hailey Schoelkopf, Sergey Troshin, Dmitry Abulkhanov, Manuel Romero, Michael Lappert, Francesco De Toni, Bernardo GarcÃ­a del RÃ­o, Qian Liu, Shamik Bose, Urvashi Bhattacharyya, Terry Yue Zhuo, Ian Yu, Paulo Villegas, Marco Zocca, Sourab Mangrulkar, David Lansky, Huu Nguyen, Danish Contractor, Luis Villa, Jia Li, Dzmitry Bahdanau, Yacine Jernite, Sean Hughes, Daniel Fried, Arjun Guha, Harm de Vries, Leandro von Werra."
        },
        {
            "sha": "f90e0d18498f52452521db3bb683ef07479d54ac",
            "filename": "docs/source/en/model_doc/gpt_neo.md",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_neo.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_neo.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_neo.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,13 @@ rendered properly in your Markdown viewer.\n \n # GPT Neo\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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\n+\">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+</div>\n+\n ## Overview\n \n The GPTNeo model was released in the [EleutherAI/gpt-neo](https://github.com/EleutherAI/gpt-neo) repository by Sid"
        },
        {
            "sha": "35f12bdb2128d88f83361a861894a8d35d04eb82",
            "filename": "docs/source/en/model_doc/gpt_neox.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_neox.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_neox.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_neox.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,11 @@ rendered properly in your Markdown viewer.\n \n # GPT-NeoX\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n We introduce GPT-NeoX-20B, a 20 billion parameter autoregressive language model trained on the Pile, whose weights will"
        },
        {
            "sha": "cedfafa133e459fc79722780a43b385e26879d64",
            "filename": "docs/source/en/model_doc/gpt_neox_japanese.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_neox_japanese.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_neox_japanese.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_neox_japanese.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,11 @@ rendered properly in your Markdown viewer.\n \n # GPT-NeoX-Japanese\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+</div>\n+\n ## Overview\n \n We introduce GPT-NeoX-Japanese, which is an autoregressive language model for Japanese, trained on top of [https://github.com/EleutherAI/gpt-neox](https://github.com/EleutherAI/gpt-neox)."
        },
        {
            "sha": "8e852d931aae0a6e96622fae2d5e455d24f2d0c9",
            "filename": "docs/source/en/model_doc/gptj.md",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fgptj.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fgptj.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgptj.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,14 @@ rendered properly in your Markdown viewer.\n \n # GPT-J\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n+<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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\n+\">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+</div>\n+\n ## Overview\n \n The GPT-J model was released in the [kingoflolz/mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax) repository by Ben Wang and Aran Komatsuzaki. It is a GPT-2-like"
        },
        {
            "sha": "929e7330ceea261eabc8f94ccac8af35f66a4672",
            "filename": "docs/source/en/model_doc/gptsan-japanese.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fgptsan-japanese.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fgptsan-japanese.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgptsan-japanese.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # GPTSAN-japanese\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n <Tip warning={true}>\n \n This model is in maintenance mode only, we don't accept any new PRs changing its code."
        },
        {
            "sha": "0326bc5ad24a33b2a60520dcf8ee59186565cbf6",
            "filename": "docs/source/en/model_doc/granite.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranite.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranite.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranite.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,12 @@ rendered properly in your Markdown viewer.\n \n # Granite\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The Granite model was proposed in [Power Scheduler: A Batch Size and Token Number Agnostic Learning Rate Scheduler](https://arxiv.org/abs/2408.13359) by Yikang Shen, Matthew Stallone, Mayank Mishra, Gaoyuan Zhang, Shawn Tan, Aditya Prasad, Adriana Meza Soria, David D. Cox and Rameswar Panda."
        },
        {
            "sha": "56ba5d936c9d4fd6205ee5ed616f3b7d374236a5",
            "filename": "docs/source/en/model_doc/granitemoe.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranitemoe.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranitemoe.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranitemoe.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,12 @@ rendered properly in your Markdown viewer.\n \n # GraniteMoe\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The GraniteMoe model was proposed in [Power Scheduler: A Batch Size and Token Number Agnostic Learning Rate Scheduler](https://arxiv.org/abs/2408.13359) by Yikang Shen, Matthew Stallone, Mayank Mishra, Gaoyuan Zhang, Shawn Tan, Aditya Prasad, Adriana Meza Soria, David D. Cox and Rameswar Panda."
        },
        {
            "sha": "0d88134d4b7e608976e6f47dc1cb770afa16c8a8",
            "filename": "docs/source/en/model_doc/graphormer.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fgraphormer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fgraphormer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgraphormer.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -14,6 +14,10 @@ rendered properly in your Markdown viewer.\n \n # Graphormer\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n <Tip warning={true}>\n \n This model is in maintenance mode only, we don't accept any new PRs changing its code."
        },
        {
            "sha": "75f8a2fa32f77f869c26a23fcc938f609f7b6afc",
            "filename": "docs/source/en/model_doc/grounding-dino.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fgrounding-dino.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fgrounding-dino.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgrounding-dino.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # Grounding DINO\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The Grounding DINO model was proposed in [Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection](https://arxiv.org/abs/2303.05499) by Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, Lei Zhang. Grounding DINO extends a closed-set object detection model with a text encoder, enabling open-set object detection. The model achieves remarkable results, such as 52.5 AP on COCO zero-shot."
        },
        {
            "sha": "c77a51d8b1b78ea77190cb10892e298dcdd35d89",
            "filename": "docs/source/en/model_doc/groupvit.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fgroupvit.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fgroupvit.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgroupvit.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,11 @@ rendered properly in your Markdown viewer.\n \n # GroupViT\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The GroupViT model was proposed in [GroupViT: Semantic Segmentation Emerges from Text Supervision](https://arxiv.org/abs/2202.11094) by Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, Jan Kautz, Xiaolong Wang."
        },
        {
            "sha": "a9296eb110d5a0f476aad2adad215db99f0cc4df",
            "filename": "docs/source/en/model_doc/helium.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fhelium.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fhelium.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fhelium.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,11 @@ rendered properly in your Markdown viewer.\n \n # Helium\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n \n ## Overview\n "
        },
        {
            "sha": "aa4f535ed274305456bebb7b9d24f0298b78c0a4",
            "filename": "docs/source/en/model_doc/herbert.md",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fherbert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fherbert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fherbert.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,13 @@ rendered properly in your Markdown viewer.\n \n # HerBERT\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n+<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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\n+\">\n+</div>\n+\n ## Overview\n \n The HerBERT model was proposed in [KLEJ: Comprehensive Benchmark for Polish Language Understanding](https://www.aclweb.org/anthology/2020.acl-main.111.pdf) by Piotr Rybak, Robert Mroczkowski, Janusz Tracz, and"
        },
        {
            "sha": "a82eec950a513737dc0c3992d9e5703987761b35",
            "filename": "docs/source/en/model_doc/hiera.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fhiera.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fhiera.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fhiera.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # Hiera\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n Hiera was proposed in [Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles](https://arxiv.org/abs/2306.00989) by Chaitanya Ryali, Yuan-Ting Hu, Daniel Bolya, Chen Wei, Haoqi Fan, Po-Yao Huang, Vaibhav Aggarwal, Arkabandhu Chowdhury, Omid Poursaeed, Judy Hoffman, Jitendra Malik, Yanghao Li, Christoph Feichtenhofer"
        },
        {
            "sha": "432e127c78634fa55e617520b94ff1fa8d8e3aa2",
            "filename": "docs/source/en/model_doc/hubert.md",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fhubert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fhubert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fhubert.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,13 @@ rendered properly in your Markdown viewer.\n \n # Hubert\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n Hubert was proposed in [HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units](https://arxiv.org/abs/2106.07447) by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan"
        },
        {
            "sha": "8c43eeddaf5567067c7c7b0b2337a1248b058f59",
            "filename": "docs/source/en/model_doc/ibert.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fibert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fibert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fibert.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # I-BERT\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The I-BERT model was proposed in [I-BERT: Integer-only BERT Quantization](https://arxiv.org/abs/2101.01321) by"
        },
        {
            "sha": "2b8e471213d71e1f171da074c68f27bb73ebfcb8",
            "filename": "docs/source/en/model_doc/idefics.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fidefics.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fidefics.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fidefics.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,12 @@ rendered properly in your Markdown viewer.\n \n # IDEFICS\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The IDEFICS model was proposed in [OBELICS: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents"
        },
        {
            "sha": "8de2c92d560944491ac8ef0d96d2b64c20c3ac77",
            "filename": "docs/source/en/model_doc/idefics2.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fidefics2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fidefics2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fidefics2.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,12 @@ rendered properly in your Markdown viewer.\n \n # Idefics2\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The Idefics2 model was proposed in [What matters when building vision-language models?](https://arxiv.org/abs/2405.02246) by LÃ©o Tronchon, Hugo Laurencon, Victor Sanh. The accompanying blog post can be found [here](https://huggingface.co/blog/idefics2)."
        },
        {
            "sha": "deab4423f80c08137316e73470474f20a0477bac",
            "filename": "docs/source/en/model_doc/idefics3.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fidefics3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fidefics3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fidefics3.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,12 @@ rendered properly in your Markdown viewer.\n \n # Idefics3\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The Idefics3 model was proposed in [Building and better understanding vision-language models: insights and future directions](https://huggingface.co/papers/2408.12637) by Hugo LaurenÃ§on, AndrÃ©s Marafioti, Victor Sanh, and LÃ©o Tronchon."
        },
        {
            "sha": "a92fdc83e8ac69f219b50c2c504980649bcc03c6",
            "filename": "docs/source/en/model_doc/ijepa.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fijepa.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fijepa.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fijepa.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,11 @@ rendered properly in your Markdown viewer.\n \n # I-JEPA\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The I-JEPA model was proposed in [Image-based Joint-Embedding Predictive Architecture](https://arxiv.org/abs/2301.08243) by Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Yann LeCun, Nicolas Ballas."
        },
        {
            "sha": "7fbec62d30bb2fd90be3fd33a36a1312fae33f0e",
            "filename": "docs/source/en/model_doc/imagegpt.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fimagegpt.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fimagegpt.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fimagegpt.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -15,6 +15,10 @@ specific language governing permissions and limitations under the License. -->\n \n # ImageGPT\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The ImageGPT model was proposed in [Generative Pretraining from Pixels](https://openai.com/blog/image-gpt) by Mark"
        },
        {
            "sha": "1dfc397db7770bdcd44af12edeec57e34bd68ddb",
            "filename": "docs/source/en/model_doc/informer.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Finformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Finformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Finformer.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # Informer\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The Informer model was proposed in [Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting](https://arxiv.org/abs/2012.07436) by Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang."
        },
        {
            "sha": "4f2feb015f1f8185b479ef6e6177f6e93f847f9e",
            "filename": "docs/source/en/model_doc/instructblip.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Finstructblip.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Finstructblip.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Finstructblip.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -12,6 +12,10 @@ specific language governing permissions and limitations under the License.\n \n # InstructBLIP\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The InstructBLIP model was proposed in [InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning](https://arxiv.org/abs/2305.06500) by Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, Steven Hoi."
        },
        {
            "sha": "c26562a8530862d5df2ab0674a467dba4866be5b",
            "filename": "docs/source/en/model_doc/instructblipvideo.md",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Finstructblipvideo.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Finstructblipvideo.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Finstructblipvideo.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -12,7 +12,9 @@ specific language governing permissions and limitations under the License.\n \n # InstructBlipVideo\n \n-## Overview\n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n \n ## Overview\n "
        },
        {
            "sha": "c8d66b163b5ad96078f4aa1ecd92bf22e6992dc0",
            "filename": "docs/source/en/model_doc/jamba.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fjamba.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fjamba.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fjamba.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,12 @@ rendered properly in your Markdown viewer.\n \n # Jamba\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n Jamba is a state-of-the-art, hybrid SSM-Transformer LLM. It is the first production-scale Mamba implementation, which opens up interesting research and application opportunities. While this initial experimentation shows encouraging gains, we expect these to be further enhanced with future optimizations and explorations."
        },
        {
            "sha": "aba6577f70cd4b889d3c841a39032ae8a1752b96",
            "filename": "docs/source/en/model_doc/jetmoe.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fjetmoe.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fjetmoe.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fjetmoe.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,12 @@ rendered properly in your Markdown viewer.\n \n # JetMoe\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n **JetMoe-8B** is an 8B Mixture-of-Experts (MoE) language model developed by [Yikang Shen](https://scholar.google.com.hk/citations?user=qff5rRYAAAAJ) and [MyShell](https://myshell.ai/)."
        },
        {
            "sha": "144134d9b0709b4a7ff8ca2392ac4a15569c7dcd",
            "filename": "docs/source/en/model_doc/jukebox.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fjukebox.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fjukebox.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fjukebox.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -15,6 +15,10 @@ rendered properly in your Markdown viewer.\n -->\n # Jukebox\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n <Tip warning={true}>\n \n This model is in maintenance mode only, we don't accept any new PRs changing its code."
        },
        {
            "sha": "88a3b6bd99e1493d10b0bd50b9a9f69a92332a74",
            "filename": "docs/source/en/model_doc/kosmos-2.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fkosmos-2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fkosmos-2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fkosmos-2.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # KOSMOS-2\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The KOSMOS-2 model was proposed in [Kosmos-2: Grounding Multimodal Large Language Models to the World](https://arxiv.org/abs/2306.14824) by Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, Furu Wei."
        },
        {
            "sha": "51cc52b7f4526d2556dd9dd158f36f6af8481759",
            "filename": "docs/source/en/model_doc/layoutlm.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Flayoutlm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Flayoutlm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Flayoutlm.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,11 @@ rendered properly in your Markdown viewer.\n \n # LayoutLM\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n+</div>\n+\n <a id='Overview'></a>\n \n ## Overview"
        },
        {
            "sha": "7fc5ae36197bd76d89b5179905d2f2251e44a349",
            "filename": "docs/source/en/model_doc/layoutlmv2.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Flayoutlmv2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Flayoutlmv2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Flayoutlmv2.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # LayoutLMV2\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The LayoutLMV2 model was proposed in [LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding](https://arxiv.org/abs/2012.14740) by Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu,"
        },
        {
            "sha": "96e0a4d4bf5134459a8a43e8980184566fc90155",
            "filename": "docs/source/en/model_doc/layoutxlm.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Flayoutxlm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Flayoutxlm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Flayoutxlm.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # LayoutXLM\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n LayoutXLM was proposed in [LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding](https://arxiv.org/abs/2104.08836) by Yiheng Xu, Tengchao Lv, Lei Cui, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha"
        },
        {
            "sha": "729d5666d8a3d3be4e72c725267de4af22ffcca0",
            "filename": "docs/source/en/model_doc/led.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fled.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fled.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fled.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,11 @@ rendered properly in your Markdown viewer.\n \n # LED\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The LED model was proposed in [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150) by Iz"
        },
        {
            "sha": "af42c1533e53ebf92e033de7b4550a3a6f5d0327",
            "filename": "docs/source/en/model_doc/levit.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Flevit.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Flevit.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Flevit.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # LeViT\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The LeViT model was proposed in [LeViT: Introducing Convolutions to Vision Transformers](https://arxiv.org/abs/2104.01136) by Ben Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, HervÃ© JÃ©gou, Matthijs Douze. LeViT improves the [Vision Transformer (ViT)](vit) in performance and efficiency by a few architectural differences such as activation maps with decreasing resolutions in Transformers and the introduction of an attention bias to integrate positional information."
        },
        {
            "sha": "2474d854e030538724837e596d87e3e439a76c44",
            "filename": "docs/source/en/model_doc/lilt.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Flilt.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Flilt.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Flilt.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # LiLT\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The LiLT model was proposed in [LiLT: A Simple yet Effective Language-Independent Layout Transformer for Structured Document Understanding](https://arxiv.org/abs/2202.13669) by Jiapeng Wang, Lianwen Jin, Kai Ding."
        },
        {
            "sha": "c127e0d73c00a7d6dc93220e3f5b2babe2c7c947",
            "filename": "docs/source/en/model_doc/llama.md",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,14 @@ rendered properly in your Markdown viewer.\n \n # LLaMA\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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\n+\">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The LLaMA model was proposed in [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971) by Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample. It is a collection of foundation language models ranging from 7B to 65B parameters."
        },
        {
            "sha": "4e5f572c4b20af48a438401e63eeb95bf555f787",
            "filename": "docs/source/en/model_doc/llama2.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama2.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,12 @@ rendered properly in your Markdown viewer.\n \n # Llama2\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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\n+\">\n+</div>\n+\n ## Overview\n \n The Llama2 model was proposed in [LLaMA: Open Foundation and Fine-Tuned Chat Models](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/) by Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushka rMishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing EllenTan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, Thomas Scialom. It is a collection of foundation language models ranging from 7B to 70B parameters, with checkpoints finetuned for chat application!"
        },
        {
            "sha": "0bb5e8160c909e9b1a5e9c99647a65eda2eacc9e",
            "filename": "docs/source/en/model_doc/llama3.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama3.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,12 @@ rendered properly in your Markdown viewer.\n \n # Llama3\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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\n+\">\n+</div>\n+\n ```py3\n import transformers\n import torch"
        },
        {
            "sha": "79033ec5a189199796f8ef394829ba1e61d556f5",
            "filename": "docs/source/en/model_doc/llava.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,12 @@ rendered properly in your Markdown viewer.\n \n # LLaVa\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n LLaVa is an open-source chatbot trained by fine-tuning LlamA/Vicuna on GPT-generated multimodal instruction-following data. It is an auto-regressive language model, based on the transformer architecture. In other words, it is an multi-modal version of LLMs fine-tuned for chat / instructions."
        },
        {
            "sha": "7d85ab8b6967755836dd767a21bc19c9c411648d",
            "filename": "docs/source/en/model_doc/llava_next.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,12 @@ rendered properly in your Markdown viewer.\n \n # LLaVA-NeXT\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The LLaVA-NeXT model was proposed in [LLaVA-NeXT: Improved reasoning, OCR, and world knowledge](https://llava-vl.github.io/blog/2024-01-30-llava-next/) by Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, Yong Jae Lee. LLaVa-NeXT (also called LLaVa-1.6) improves upon [LLaVa](llava) by increasing the input image resolution and training on an improved visual instruction tuning dataset to improve OCR and common sense reasoning."
        },
        {
            "sha": "b338bbfba128ada31119ee02d3dfe20cf14cf14f",
            "filename": "docs/source/en/model_doc/llava_next_video.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next_video.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next_video.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next_video.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,12 @@ rendered properly in your Markdown viewer.\n \n # LLaVa-NeXT-Video\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The LLaVa-NeXT-Video model was proposed in [LLaVA-NeXT: A Strong Zero-shot Video Understanding Model"
        },
        {
            "sha": "77fe807d46d0083e0b1895d8b305d5f5073f87ee",
            "filename": "docs/source/en/model_doc/llava_onevision.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_onevision.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_onevision.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_onevision.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,12 @@ rendered properly in your Markdown viewer.\n \n # LLaVA-OneVision\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The LLaVA-OneVision model was proposed in [LLaVA-OneVision: Easy Visual Task Transfer](https://arxiv.org/abs/2408.03326) by <Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, Chunyuan Li"
        },
        {
            "sha": "d173a7eb32ec11abb0f777ee9439458886159980",
            "filename": "docs/source/en/model_doc/longformer.md",
            "status": "modified",
            "additions": 2,
            "deletions": 6,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Flongformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Flongformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Flongformer.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -17,12 +17,8 @@ rendered properly in your Markdown viewer.\n # Longformer\n \n <div class=\"flex flex-wrap space-x-1\">\n-<a href=\"https://huggingface.co/models?filter=longformer\">\n-<img alt=\"Models\" src=\"https://img.shields.io/badge/All_model_pages-longformer-blueviolet\">\n-</a>\n-<a href=\"https://huggingface.co/spaces/docs-demos/longformer-base-4096-finetuned-squadv1\">\n-<img alt=\"Spaces\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue\">\n-</a>\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n </div>\n \n ## Overview"
        },
        {
            "sha": "85a869f3c5948934ea42ba66339370ae62964b15",
            "filename": "docs/source/en/model_doc/longt5.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Flongt5.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Flongt5.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Flongt5.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,12 @@ rendered properly in your Markdown viewer.\n \n # LongT5\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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\n+\">\n+</div>\n+\n ## Overview\n \n The LongT5 model was proposed in [LongT5: Efficient Text-To-Text Transformer for Long Sequences](https://arxiv.org/abs/2112.07916)"
        },
        {
            "sha": "be4d5946dfcf2950afbfdb1e88a506c9383c9a5f",
            "filename": "docs/source/en/model_doc/luke.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fluke.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fluke.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fluke.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # LUKE\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The LUKE model was proposed in [LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention](https://arxiv.org/abs/2010.01057) by Ikuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda and Yuji Matsumoto."
        },
        {
            "sha": "a0f686efc35d0437c943d11bfb9f98e0aef680c9",
            "filename": "docs/source/en/model_doc/lxmert.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Flxmert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Flxmert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Flxmert.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,11 @@ rendered properly in your Markdown viewer.\n \n # LXMERT\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The LXMERT model was proposed in [LXMERT: Learning Cross-Modality Encoder Representations from Transformers](https://arxiv.org/abs/1908.07490) by Hao Tan & Mohit Bansal. It is a series of bidirectional transformer encoders"
        },
        {
            "sha": "f4f2955bb046f310195a2b50c81058dad8a99651",
            "filename": "docs/source/en/model_doc/m2m_100.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fm2m_100.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fm2m_100.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fm2m_100.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,12 @@ rendered properly in your Markdown viewer.\n \n # M2M100\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The M2M100 model was proposed in [Beyond English-Centric Multilingual Machine Translation](https://arxiv.org/abs/2010.11125) by Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky,"
        },
        {
            "sha": "db6abc38eaf1fba33945f85815a0c8dd692350da",
            "filename": "docs/source/en/model_doc/madlad-400.md",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmadlad-400.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmadlad-400.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmadlad-400.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,13 @@ rendered properly in your Markdown viewer.\n \n # MADLAD-400\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n+<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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\n+\">\n+</div>\n+\n ## Overview\n \n MADLAD-400 models were released in the paper [MADLAD-400: A Multilingual And Document-Level Large Audited Dataset](MADLAD-400: A Multilingual And Document-Level Large Audited Dataset). "
        },
        {
            "sha": "d5c0612b1ebe15de3001091f3d6459a08eebf03b",
            "filename": "docs/source/en/model_doc/mamba.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmamba.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmamba.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmamba.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # Mamba\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The Mamba model was proposed in [Mamba: Linear-Time Sequence Modeling with Selective State Spaces](https://arxiv.org/abs/2312.00752) by Albert Gu and Tri Dao."
        },
        {
            "sha": "8d88d6c02652e50c3efacc1bdc7277dcabda1376",
            "filename": "docs/source/en/model_doc/mamba2.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmamba2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmamba2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmamba2.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # Mamba 2\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The Mamba2 model was proposed in [Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality](https://arxiv.org/abs/2405.21060) by Tri Dao and Albert Gu. It is a State Space Model similar to Mamba 1, with better performances in a simplified architecture. "
        },
        {
            "sha": "80bb73d26df121000a8d6a2343142489532cbf08",
            "filename": "docs/source/en/model_doc/marian.md",
            "status": "modified",
            "additions": 4,
            "deletions": 6,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmarian.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmarian.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmarian.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -17,12 +17,10 @@ rendered properly in your Markdown viewer.\n # MarianMT\n \n <div class=\"flex flex-wrap space-x-1\">\n-<a href=\"https://huggingface.co/models?filter=marian\">\n-<img alt=\"Models\" src=\"https://img.shields.io/badge/All_model_pages-marian-blueviolet\">\n-</a>\n-<a href=\"https://huggingface.co/spaces/docs-demos/opus-mt-zh-en\">\n-<img alt=\"Spaces\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue\">\n-</a>\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n+<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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\n+\">\n </div>\n \n ## Overview"
        },
        {
            "sha": "72948da2c5af42913dae55768f3d044fe84ca375",
            "filename": "docs/source/en/model_doc/markuplm.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmarkuplm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmarkuplm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmarkuplm.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # MarkupLM\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The MarkupLM model was proposed in [MarkupLM: Pre-training of Text and Markup Language for Visually-rich Document"
        },
        {
            "sha": "37a2603c68800905cabbb5b49cd1ead121c1372e",
            "filename": "docs/source/en/model_doc/mask2former.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmask2former.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmask2former.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmask2former.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # Mask2Former\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The Mask2Former model was proposed in [Masked-attention Mask Transformer for Universal Image Segmentation](https://arxiv.org/abs/2112.01527) by Bowen Cheng, Ishan Misra, Alexander G. Schwing, Alexander Kirillov, Rohit Girdhar. Mask2Former is a unified framework for panoptic, instance and semantic segmentation and features significant performance and efficiency improvements over [MaskFormer](maskformer)."
        },
        {
            "sha": "0adbbf2285f9dc1ad625161a9d2bda7f819b05f0",
            "filename": "docs/source/en/model_doc/maskformer.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmaskformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmaskformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmaskformer.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # MaskFormer\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n <Tip>\n \n This is a recently introduced model so the API hasn't been tested extensively. There may be some bugs or slight"
        },
        {
            "sha": "f3c618953b9b4e2aa39fd22d772de865495b6212",
            "filename": "docs/source/en/model_doc/matcha.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmatcha.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmatcha.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmatcha.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # MatCha\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n MatCha has been proposed in the paper [MatCha: Enhancing Visual Language Pretraining with Math Reasoning and Chart Derendering](https://arxiv.org/abs/2212.09662), from Fangyu Liu, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee, Mandar Joshi, Yasemin Altun, Nigel Collier, Julian Martin Eisenschlos."
        },
        {
            "sha": "62356ad26402e64b6287ba12886716fa0ed6e10c",
            "filename": "docs/source/en/model_doc/mbart.md",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmbart.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmbart.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmbart.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -17,12 +17,12 @@ rendered properly in your Markdown viewer.\n # MBart and MBart-50\n \n <div class=\"flex flex-wrap space-x-1\">\n-<a href=\"https://huggingface.co/models?filter=mbart\">\n-<img alt=\"Models\" src=\"https://img.shields.io/badge/All_model_pages-mbart-blueviolet\">\n-</a>\n-<a href=\"https://huggingface.co/spaces/docs-demos/mbart-large-50-one-to-many-mmt\">\n-<img alt=\"Spaces\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue\">\n-</a>\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n+<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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\n+\">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n </div>\n \n "
        },
        {
            "sha": "a755f5a027d2051d18fa770449d80756f6455d98",
            "filename": "docs/source/en/model_doc/mctct.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmctct.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmctct.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmctct.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # M-CTC-T\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n <Tip warning={true}>\n \n This model is in maintenance mode only, so we won't accept any new PRs changing its code."
        },
        {
            "sha": "4e8ccd4b29f3b6a204c595f39ce4e2cdfb7bf256",
            "filename": "docs/source/en/model_doc/mega.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmega.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmega.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmega.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # MEGA\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n <Tip warning={true}>\n \n This model is in maintenance mode only, we don't accept any new PRs changing its code."
        },
        {
            "sha": "b032655f75472466a81368488a60c49865d195a2",
            "filename": "docs/source/en/model_doc/megatron-bert.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmegatron-bert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmegatron-bert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmegatron-bert.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # MegatronBERT\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The MegatronBERT model was proposed in [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model"
        },
        {
            "sha": "7e0ee3cb9e7c98a4dce2ed321a49572db934b689",
            "filename": "docs/source/en/model_doc/megatron_gpt2.md",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmegatron_gpt2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmegatron_gpt2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmegatron_gpt2.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,13 @@ rendered properly in your Markdown viewer.\n \n # MegatronGPT2\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n+<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAC0AAAAtCAMAAAANxBKoAAAC7lBMVEUAAADg5vYHPVgAoJH+/v76+v39/f9JbLP///9+AIgAnY3///+mcqzt8fXy9fgkXa3Ax9709fr+///9/f8qXq49qp5AaLGMwrv8/P0eW60VWawxYq8yqJzG2dytt9Wyu9elzci519Lf3O3S2efY3OrY0+Xp7PT///////+dqNCexMc6Z7AGpJeGvbenstPZ5ejQ1OfJzOLa7ejh4+/r8fT29vpccbklWK8PVa0AS6ghW63O498vYa+lsdKz1NDRt9Kw1c672tbD3tnAxt7R6OHp5vDe7OrDyuDn6vLl6/EAQKak0MgATakkppo3ZK/Bz9y8w9yzu9jey97axdvHzeG21NHH4trTwthKZrVGZLSUSpuPQJiGAI+GAI8SWKydycLL4d7f2OTi1+S9xNzL0ePT6OLGzeEAo5U0qJw/aLEAo5JFa7JBabEAp5Y4qZ2QxLyKmsm3kL2xoMOehrRNb7RIbbOZgrGre68AUqwAqZqNN5aKJ5N/lMq+qsd8kMa4pcWzh7muhLMEV69juq2kbKqgUaOTR5uMMZWLLZSGAI5VAIdEAH+ovNDHuNCnxcy3qcaYx8K8msGplrx+wLahjbYdXrV6vbMvYK9DrZ8QrZ8tqJuFms+Sos6sw8ecy8RffsNVeMCvmb43aLltv7Q4Y7EZWK4QWa1gt6meZKUdr6GOAZVeA4xPAISyveLUwtivxtKTpNJ2jcqfvcltiMiwwcfAoMVxhL+Kx7xjdrqTe60tsaNQs6KaRKACrJ6UTZwkqpqTL5pkHY4AloSgsd2ptNXPvNOOncuxxsqFl8lmg8apt8FJcr9EbryGxLqlkrkrY7dRa7ZGZLQ5t6iXUZ6PPpgVpZeJCJFKAIGareTa0+KJod3H0deY2M+esM25usmYu8d2zsJOdcBVvrCLbqcAOaaHaKQAMaScWqKBXqCXMJ2RHpiLF5NmJZAdAHN2kta11dKu1M+DkcZLdb+Mcql3TppyRJdzQ5ZtNZNlIY+DF4+voCOQAAAAZ3RSTlMABAT+MEEJ/RH+/TP+Zlv+pUo6Ifz8+fco/fz6+evr39S9nJmOilQaF/7+/f38+smmoYp6b1T+/v7++vj189zU0tDJxsGzsrKSfv34+Pf27dDOysG9t6+n/vv6+vr59uzr1tG+tZ6Qg9Ym3QAABR5JREFUSMeNlVVUG1EQhpcuxEspXqS0SKEtxQp1d3d332STTRpIQhIISQgJhODu7lAoDoUCpe7u7u7+1puGpqnCPOyZvffbOXPm/PsP9JfQgyCC+tmTABTOcbxDz/heENS7/1F+9nhvkHePG0wNDLbGWwdXL+rbLWvpmZHXD8+gMfBjTh+aSe6Gnn7lwQIOTR0c8wfX3PWgv7avbdKwf/ZoBp1Gp/PvuvXW3vw5ib7emnTW4OR+3D4jB9vjNJ/7gNvfWWeH/TO/JyYrsiKCRjVEZA3UB+96kON+DxOQ/NLE8PE5iUYgIXjFnCOlxEQMaSGVxjg4gxOnEycGz8bptuNjVx08LscIgrzH3umcn+KKtiBIyvzOO2O99aAdR8cF19oZalnCtvREUw79tCd5sow1g1UKM6kXqUx4T8wsi3sTjJ3yzDmmhenLXLpo8u45eG5y4Vvbk6kkC4LLtJMowkSQxmk4ggVJEG+7c6QpHT8vvW9X7/o7+3ELmiJi2mEzZJiz8cT6TBlanBk70cB5GGIGC1gRDdZ00yADLW1FL6gqhtvNXNG5S9gdSrk4M1qu7JAsmYshzDS4peoMrU/gT7qQdqYGZaYhxZmVbGJAm/CS/HloWyhRUlknQ9KYcExTwS80d3VNOxUZJpITYyspl0LbhArhpZCD9cRWEQuhYkNGMHToQ/2Cs6swJlb39CsllxdXX6IUKh/H5jbnSsPKjgmoaFQ1f8wRLR0UnGE/RcDEjj2jXG1WVTwUs8+zxfcrVO+vSsuOpVKxCfYZiQ0/aPKuxQbQ8lIz+DClxC8u+snlcJ7Yr1z1JPqUH0V+GDXbOwAib931Y4Imaq0NTIXPXY+N5L18GJ37SVWu+hwXff8l72Ds9XuwYIBaXPq6Shm4l+Vl/5QiOlV+uTk6YR9PxKsI9xNJny31ygK1e+nIRC1N97EGkFPI+jCpiHe5PCEy7oWqWSwRrpOvhFzcbTWMbm3ZJAOn1rUKpYIt/lDhW/5RHHteeWFN60qo98YJuoq1nK3uW5AabyspC1BcIEpOhft+SZAShYoLSvnmSfnYADUERP5jJn2h5XtsgCRuhYQqAvwTwn33+YWEKUI72HX5AtfSAZDe8F2DtPPm77afhl0EkthzuCQU0BWApgQIH9+KB0JhopMM7bJrdTRoleM2JAVNMyPF+wdoaz+XJpGoVAQ7WXUkcV7gT3oUZyi/ISIJAVKhgNp+4b4veCFhYVJw4locdSjZCp9cPUhLF9EZ3KKzURepMEtCDPP3VcWFx4UIiZIklIpFNfHpdEafIF2aRmOcrUmjohbT2WUllbmRvgfbythbQO3222fpDJoufaQPncYYuqoGtUEsCJZL6/3PR5b4syeSjZMQG/T2maGANlXT2v8S4AULWaUkCxfLyW8iW4kdka+nEMjxpL2NCwsYNBp+Q61PF43zyDg9Bm9+3NNySn78jMZUUkumqE4Gp7JmFOdP1vc8PpRrzj9+wPinCy8K1PiJ4aYbnTYpCCbDkBSbzhu2QJ1Gd82t8jI8TH51+OzvXoWbnXUOBkNW+0mWFwGcGOUVpU81/n3TOHb5oMt2FgYGjzau0Nif0Ss7Q3XB33hjjQHjHA5E5aOyIQc8CBrLdQSs3j92VG+3nNEjbkbdbBr9zm04ruvw37vh0QKOdeGIkckc80fX3KH/h7PT4BOjgCty8VZ5ux1MoO5Cf5naca2LAsEgehI+drX8o/0Nu+W0m6K/I9gGPd/dfx/EN/wN62AhsBWuAAAAAElFTkSuQmCC\n+\">\n+</div>\n+\n ## Overview\n \n The MegatronGPT2 model was proposed in [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model"
        },
        {
            "sha": "168e5bd1043d5069b1f4a6997ee6a074abbd7602",
            "filename": "docs/source/en/model_doc/mgp-str.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmgp-str.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmgp-str.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmgp-str.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # MGP-STR\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The MGP-STR model was proposed in [Multi-Granularity Prediction for Scene Text Recognition](https://arxiv.org/abs/2209.03592) by Peng Wang, Cheng Da, and Cong Yao. MGP-STR is a conceptually **simple** yet **powerful** vision Scene Text Recognition (STR) model, which is built upon the [Vision Transformer (ViT)](vit). To integrate linguistic knowledge, Multi-Granularity Prediction (MGP) strategy is proposed to inject information from the language modality into the model in an implicit way."
        },
        {
            "sha": "6e68394fcaeabca35c453347065b5f80cb989bf2",
            "filename": "docs/source/en/model_doc/mimi.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmimi.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmimi.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmimi.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,12 @@ rendered properly in your Markdown viewer.\n \n # Mimi\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The Mimi model was proposed in [Moshi: a speech-text foundation model for real-time dialogue](https://kyutai.org/Moshi.pdf) by Alexandre DÃ©fossez, Laurent MazarÃ©, Manu Orsini, AmÃ©lie Royer, Patrick PÃ©rez, HervÃ© JÃ©gou, Edouard Grave and Neil Zeghidour. Mimi is a high-fidelity audio codec model developed by the Kyutai team, that combines semantic and acoustic information into audio tokens running at 12Hz and a bitrate of 1.1kbps. In other words, it can be used to map audio waveforms into â€œaudio tokensâ€, known as â€œcodebooksâ€."
        },
        {
            "sha": "097d8888f9a579a9a14720db2355f1ac897ce6bf",
            "filename": "docs/source/en/model_doc/mistral.md",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmistral.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmistral.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmistral.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,15 @@ rendered properly in your Markdown viewer.\n \n # Mistral\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n+<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAC0AAAAtCAMAAAANxBKoAAAC7lBMVEUAAADg5vYHPVgAoJH+/v76+v39/f9JbLP///9+AIgAnY3///+mcqzt8fXy9fgkXa3Ax9709fr+///9/f8qXq49qp5AaLGMwrv8/P0eW60VWawxYq8yqJzG2dytt9Wyu9elzci519Lf3O3S2efY3OrY0+Xp7PT///////+dqNCexMc6Z7AGpJeGvbenstPZ5ejQ1OfJzOLa7ejh4+/r8fT29vpccbklWK8PVa0AS6ghW63O498vYa+lsdKz1NDRt9Kw1c672tbD3tnAxt7R6OHp5vDe7OrDyuDn6vLl6/EAQKak0MgATakkppo3ZK/Bz9y8w9yzu9jey97axdvHzeG21NHH4trTwthKZrVGZLSUSpuPQJiGAI+GAI8SWKydycLL4d7f2OTi1+S9xNzL0ePT6OLGzeEAo5U0qJw/aLEAo5JFa7JBabEAp5Y4qZ2QxLyKmsm3kL2xoMOehrRNb7RIbbOZgrGre68AUqwAqZqNN5aKJ5N/lMq+qsd8kMa4pcWzh7muhLMEV69juq2kbKqgUaOTR5uMMZWLLZSGAI5VAIdEAH+ovNDHuNCnxcy3qcaYx8K8msGplrx+wLahjbYdXrV6vbMvYK9DrZ8QrZ8tqJuFms+Sos6sw8ecy8RffsNVeMCvmb43aLltv7Q4Y7EZWK4QWa1gt6meZKUdr6GOAZVeA4xPAISyveLUwtivxtKTpNJ2jcqfvcltiMiwwcfAoMVxhL+Kx7xjdrqTe60tsaNQs6KaRKACrJ6UTZwkqpqTL5pkHY4AloSgsd2ptNXPvNOOncuxxsqFl8lmg8apt8FJcr9EbryGxLqlkrkrY7dRa7ZGZLQ5t6iXUZ6PPpgVpZeJCJFKAIGareTa0+KJod3H0deY2M+esM25usmYu8d2zsJOdcBVvrCLbqcAOaaHaKQAMaScWqKBXqCXMJ2RHpiLF5NmJZAdAHN2kta11dKu1M+DkcZLdb+Mcql3TppyRJdzQ5ZtNZNlIY+DF4+voCOQAAAAZ3RSTlMABAT+MEEJ/RH+/TP+Zlv+pUo6Ifz8+fco/fz6+evr39S9nJmOilQaF/7+/f38+smmoYp6b1T+/v7++vj189zU0tDJxsGzsrKSfv34+Pf27dDOysG9t6+n/vv6+vr59uzr1tG+tZ6Qg9Ym3QAABR5JREFUSMeNlVVUG1EQhpcuxEspXqS0SKEtxQp1d3d332STTRpIQhIISQgJhODu7lAoDoUCpe7u7u7+1puGpqnCPOyZvffbOXPm/PsP9JfQgyCC+tmTABTOcbxDz/heENS7/1F+9nhvkHePG0wNDLbGWwdXL+rbLWvpmZHXD8+gMfBjTh+aSe6Gnn7lwQIOTR0c8wfX3PWgv7avbdKwf/ZoBp1Gp/PvuvXW3vw5ib7emnTW4OR+3D4jB9vjNJ/7gNvfWWeH/TO/JyYrsiKCRjVEZA3UB+96kON+DxOQ/NLE8PE5iUYgIXjFnCOlxEQMaSGVxjg4gxOnEycGz8bptuNjVx08LscIgrzH3umcn+KKtiBIyvzOO2O99aAdR8cF19oZalnCtvREUw79tCd5sow1g1UKM6kXqUx4T8wsi3sTjJ3yzDmmhenLXLpo8u45eG5y4Vvbk6kkC4LLtJMowkSQxmk4ggVJEG+7c6QpHT8vvW9X7/o7+3ELmiJi2mEzZJiz8cT6TBlanBk70cB5GGIGC1gRDdZ00yADLW1FL6gqhtvNXNG5S9gdSrk4M1qu7JAsmYshzDS4peoMrU/gT7qQdqYGZaYhxZmVbGJAm/CS/HloWyhRUlknQ9KYcExTwS80d3VNOxUZJpITYyspl0LbhArhpZCD9cRWEQuhYkNGMHToQ/2Cs6swJlb39CsllxdXX6IUKh/H5jbnSsPKjgmoaFQ1f8wRLR0UnGE/RcDEjj2jXG1WVTwUs8+zxfcrVO+vSsuOpVKxCfYZiQ0/aPKuxQbQ8lIz+DClxC8u+snlcJ7Yr1z1JPqUH0V+GDXbOwAib931Y4Imaq0NTIXPXY+N5L18GJ37SVWu+hwXff8l72Ds9XuwYIBaXPq6Shm4l+Vl/5QiOlV+uTk6YR9PxKsI9xNJny31ygK1e+nIRC1N97EGkFPI+jCpiHe5PCEy7oWqWSwRrpOvhFzcbTWMbm3ZJAOn1rUKpYIt/lDhW/5RHHteeWFN60qo98YJuoq1nK3uW5AabyspC1BcIEpOhft+SZAShYoLSvnmSfnYADUERP5jJn2h5XtsgCRuhYQqAvwTwn33+YWEKUI72HX5AtfSAZDe8F2DtPPm77afhl0EkthzuCQU0BWApgQIH9+KB0JhopMM7bJrdTRoleM2JAVNMyPF+wdoaz+XJpGoVAQ7WXUkcV7gT3oUZyi/ISIJAVKhgNp+4b4veCFhYVJw4locdSjZCp9cPUhLF9EZ3KKzURepMEtCDPP3VcWFx4UIiZIklIpFNfHpdEafIF2aRmOcrUmjohbT2WUllbmRvgfbythbQO3222fpDJoufaQPncYYuqoGtUEsCJZL6/3PR5b4syeSjZMQG/T2maGANlXT2v8S4AULWaUkCxfLyW8iW4kdka+nEMjxpL2NCwsYNBp+Q61PF43zyDg9Bm9+3NNySn78jMZUUkumqE4Gp7JmFOdP1vc8PpRrzj9+wPinCy8K1PiJ4aYbnTYpCCbDkBSbzhu2QJ1Gd82t8jI8TH51+OzvXoWbnXUOBkNW+0mWFwGcGOUVpU81/n3TOHb5oMt2FgYGjzau0Nif0Ss7Q3XB33hjjQHjHA5E5aOyIQc8CBrLdQSs3j92VG+3nNEjbkbdbBr9zm04ruvw37vh0QKOdeGIkckc80fX3KH/h7PT4BOjgCty8VZ5ux1MoO5Cf5naca2LAsEgehI+drX8o/0Nu+W0m6K/I9gGPd/dfx/EN/wN62AhsBWuAAAAAElFTkSuQmCC\n+\">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n Mistral was introduced in the [this blogpost](https://mistral.ai/news/announcing-mistral-7b/) by Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, LÃ©lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, William El Sayed."
        },
        {
            "sha": "38c0c98ed0b95714f090dfa21179bab3d839b60e",
            "filename": "docs/source/en/model_doc/mixtral.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmixtral.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmixtral.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmixtral.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,12 @@ rendered properly in your Markdown viewer.\n \n # Mixtral\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n Mixtral-8x7B was introduced in the [Mixtral of Experts blogpost](https://mistral.ai/news/mixtral-of-experts/) by Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, LÃ©lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, William El Sayed."
        },
        {
            "sha": "77f5e211f170f0d2c2df6c0a4393543c43e96e1e",
            "filename": "docs/source/en/model_doc/mllama.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmllama.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmllama.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmllama.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # Mllama\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The Llama 3.2-Vision collection of multimodal large language models (LLMs) is a collection of pretrained and instruction-tuned image reasoning generative models in 11B and 90B sizes (text \\+ images in / text out). The Llama 3.2-Vision instruction-tuned models are optimized for visual recognition, image reasoning, captioning, and answering general questions about an image."
        },
        {
            "sha": "aae607def6f105d16b2900b3af0d9430a78edae8",
            "filename": "docs/source/en/model_doc/mluke.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmluke.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmluke.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmluke.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # mLUKE\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The mLUKE model was proposed in [mLUKE: The Power of Entity Representations in Multilingual Pretrained Language Models](https://arxiv.org/abs/2110.08151) by Ryokan Ri, Ikuya Yamada, and Yoshimasa Tsuruoka. It's a multilingual extension"
        },
        {
            "sha": "480d5bc8ddb1c2a6709b14756335a9468c7b4fe1",
            "filename": "docs/source/en/model_doc/mms.md",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmms.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmms.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmms.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,13 @@ rendered properly in your Markdown viewer.\n \n # MMS\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n+<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAC0AAAAtCAMAAAANxBKoAAAC7lBMVEUAAADg5vYHPVgAoJH+/v76+v39/f9JbLP///9+AIgAnY3///+mcqzt8fXy9fgkXa3Ax9709fr+///9/f8qXq49qp5AaLGMwrv8/P0eW60VWawxYq8yqJzG2dytt9Wyu9elzci519Lf3O3S2efY3OrY0+Xp7PT///////+dqNCexMc6Z7AGpJeGvbenstPZ5ejQ1OfJzOLa7ejh4+/r8fT29vpccbklWK8PVa0AS6ghW63O498vYa+lsdKz1NDRt9Kw1c672tbD3tnAxt7R6OHp5vDe7OrDyuDn6vLl6/EAQKak0MgATakkppo3ZK/Bz9y8w9yzu9jey97axdvHzeG21NHH4trTwthKZrVGZLSUSpuPQJiGAI+GAI8SWKydycLL4d7f2OTi1+S9xNzL0ePT6OLGzeEAo5U0qJw/aLEAo5JFa7JBabEAp5Y4qZ2QxLyKmsm3kL2xoMOehrRNb7RIbbOZgrGre68AUqwAqZqNN5aKJ5N/lMq+qsd8kMa4pcWzh7muhLMEV69juq2kbKqgUaOTR5uMMZWLLZSGAI5VAIdEAH+ovNDHuNCnxcy3qcaYx8K8msGplrx+wLahjbYdXrV6vbMvYK9DrZ8QrZ8tqJuFms+Sos6sw8ecy8RffsNVeMCvmb43aLltv7Q4Y7EZWK4QWa1gt6meZKUdr6GOAZVeA4xPAISyveLUwtivxtKTpNJ2jcqfvcltiMiwwcfAoMVxhL+Kx7xjdrqTe60tsaNQs6KaRKACrJ6UTZwkqpqTL5pkHY4AloSgsd2ptNXPvNOOncuxxsqFl8lmg8apt8FJcr9EbryGxLqlkrkrY7dRa7ZGZLQ5t6iXUZ6PPpgVpZeJCJFKAIGareTa0+KJod3H0deY2M+esM25usmYu8d2zsJOdcBVvrCLbqcAOaaHaKQAMaScWqKBXqCXMJ2RHpiLF5NmJZAdAHN2kta11dKu1M+DkcZLdb+Mcql3TppyRJdzQ5ZtNZNlIY+DF4+voCOQAAAAZ3RSTlMABAT+MEEJ/RH+/TP+Zlv+pUo6Ifz8+fco/fz6+evr39S9nJmOilQaF/7+/f38+smmoYp6b1T+/v7++vj189zU0tDJxsGzsrKSfv34+Pf27dDOysG9t6+n/vv6+vr59uzr1tG+tZ6Qg9Ym3QAABR5JREFUSMeNlVVUG1EQhpcuxEspXqS0SKEtxQp1d3d332STTRpIQhIISQgJhODu7lAoDoUCpe7u7u7+1puGpqnCPOyZvffbOXPm/PsP9JfQgyCC+tmTABTOcbxDz/heENS7/1F+9nhvkHePG0wNDLbGWwdXL+rbLWvpmZHXD8+gMfBjTh+aSe6Gnn7lwQIOTR0c8wfX3PWgv7avbdKwf/ZoBp1Gp/PvuvXW3vw5ib7emnTW4OR+3D4jB9vjNJ/7gNvfWWeH/TO/JyYrsiKCRjVEZA3UB+96kON+DxOQ/NLE8PE5iUYgIXjFnCOlxEQMaSGVxjg4gxOnEycGz8bptuNjVx08LscIgrzH3umcn+KKtiBIyvzOO2O99aAdR8cF19oZalnCtvREUw79tCd5sow1g1UKM6kXqUx4T8wsi3sTjJ3yzDmmhenLXLpo8u45eG5y4Vvbk6kkC4LLtJMowkSQxmk4ggVJEG+7c6QpHT8vvW9X7/o7+3ELmiJi2mEzZJiz8cT6TBlanBk70cB5GGIGC1gRDdZ00yADLW1FL6gqhtvNXNG5S9gdSrk4M1qu7JAsmYshzDS4peoMrU/gT7qQdqYGZaYhxZmVbGJAm/CS/HloWyhRUlknQ9KYcExTwS80d3VNOxUZJpITYyspl0LbhArhpZCD9cRWEQuhYkNGMHToQ/2Cs6swJlb39CsllxdXX6IUKh/H5jbnSsPKjgmoaFQ1f8wRLR0UnGE/RcDEjj2jXG1WVTwUs8+zxfcrVO+vSsuOpVKxCfYZiQ0/aPKuxQbQ8lIz+DClxC8u+snlcJ7Yr1z1JPqUH0V+GDXbOwAib931Y4Imaq0NTIXPXY+N5L18GJ37SVWu+hwXff8l72Ds9XuwYIBaXPq6Shm4l+Vl/5QiOlV+uTk6YR9PxKsI9xNJny31ygK1e+nIRC1N97EGkFPI+jCpiHe5PCEy7oWqWSwRrpOvhFzcbTWMbm3ZJAOn1rUKpYIt/lDhW/5RHHteeWFN60qo98YJuoq1nK3uW5AabyspC1BcIEpOhft+SZAShYoLSvnmSfnYADUERP5jJn2h5XtsgCRuhYQqAvwTwn33+YWEKUI72HX5AtfSAZDe8F2DtPPm77afhl0EkthzuCQU0BWApgQIH9+KB0JhopMM7bJrdTRoleM2JAVNMyPF+wdoaz+XJpGoVAQ7WXUkcV7gT3oUZyi/ISIJAVKhgNp+4b4veCFhYVJw4locdSjZCp9cPUhLF9EZ3KKzURepMEtCDPP3VcWFx4UIiZIklIpFNfHpdEafIF2aRmOcrUmjohbT2WUllbmRvgfbythbQO3222fpDJoufaQPncYYuqoGtUEsCJZL6/3PR5b4syeSjZMQG/T2maGANlXT2v8S4AULWaUkCxfLyW8iW4kdka+nEMjxpL2NCwsYNBp+Q61PF43zyDg9Bm9+3NNySn78jMZUUkumqE4Gp7JmFOdP1vc8PpRrzj9+wPinCy8K1PiJ4aYbnTYpCCbDkBSbzhu2QJ1Gd82t8jI8TH51+OzvXoWbnXUOBkNW+0mWFwGcGOUVpU81/n3TOHb5oMt2FgYGjzau0Nif0Ss7Q3XB33hjjQHjHA5E5aOyIQc8CBrLdQSs3j92VG+3nNEjbkbdbBr9zm04ruvw37vh0QKOdeGIkckc80fX3KH/h7PT4BOjgCty8VZ5ux1MoO5Cf5naca2LAsEgehI+drX8o/0Nu+W0m6K/I9gGPd/dfx/EN/wN62AhsBWuAAAAAElFTkSuQmCC\n+\">\n+</div>\n+\n ## Overview\n \n The MMS model was proposed in [Scaling Speech Technology to 1,000+ Languages](https://arxiv.org/abs/2305.13516) "
        },
        {
            "sha": "11a2b21b6130c713ee9fe367f3c76bbe962afd98",
            "filename": "docs/source/en/model_doc/mobilebert.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilebert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilebert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilebert.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,11 @@ rendered properly in your Markdown viewer.\n \n # MobileBERT\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The MobileBERT model was proposed in [MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices](https://arxiv.org/abs/2004.02984) by Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny"
        },
        {
            "sha": "7d94777d6f83180bc1f4c1538aa57721b94c9e09",
            "filename": "docs/source/en/model_doc/mobilenet_v1.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilenet_v1.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilenet_v1.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilenet_v1.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # MobileNet V1\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The MobileNet model was proposed in [MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications](https://arxiv.org/abs/1704.04861) by Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, Hartwig Adam."
        },
        {
            "sha": "b78a8eb72f6395ca2b82b4a3d6c896e1239451ce",
            "filename": "docs/source/en/model_doc/mobilenet_v2.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilenet_v2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilenet_v2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilenet_v2.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # MobileNet V2\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The MobileNet model was proposed in [MobileNetV2: Inverted Residuals and Linear Bottlenecks](https://arxiv.org/abs/1801.04381) by Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen."
        },
        {
            "sha": "c9054b59cbc9ba61ac80e72c01cc48473c161715",
            "filename": "docs/source/en/model_doc/mobilevit.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilevit.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilevit.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilevit.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,11 @@ rendered properly in your Markdown viewer.\n \n # MobileViT\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The MobileViT model was proposed in [MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer](https://arxiv.org/abs/2110.02178) by Sachin Mehta and Mohammad Rastegari. MobileViT introduces a new layer that replaces local processing in convolutions with global processing using transformers."
        },
        {
            "sha": "b6549666850afceeb04522dd735bee1c64c69d47",
            "filename": "docs/source/en/model_doc/mobilevitv2.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilevitv2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilevitv2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilevitv2.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # MobileViTV2\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The MobileViTV2 model was proposed in [Separable Self-attention for Mobile Vision Transformers](https://arxiv.org/abs/2206.02680) by Sachin Mehta and Mohammad Rastegari."
        },
        {
            "sha": "f7ceaae18797a883155b8c723115e0440a094e57",
            "filename": "docs/source/en/model_doc/modernbert.md",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmodernbert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmodernbert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmodernbert.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -17,12 +17,9 @@ rendered properly in your Markdown viewer.\n # ModernBERT\n \n <div class=\"flex flex-wrap space-x-1\">\n-<a href=\"https://huggingface.co/models?filter=modernbert\">\n-<img alt=\"Models\" src=\"https://img.shields.io/badge/All_model_pages-modernbert-blueviolet\">\n-</a>\n-<a href=\"https://arxiv.org/abs/2412.13663\">\n-<img alt=\"Paper page\" src=\"https://img.shields.io/badge/Paper%20page-2412.13663-green\">\n-</a>\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n </div>\n \n ## Overview"
        },
        {
            "sha": "2a4599e3d7e08465c0e82b9b0f53a2c6ae33d04a",
            "filename": "docs/source/en/model_doc/moonshine.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmoonshine.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmoonshine.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmoonshine.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,12 @@ rendered properly in your Markdown viewer.\n \n # Moonshine\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The Moonshine model was proposed in [Moonshine: Speech Recognition for Live Transcription and Voice Commands"
        },
        {
            "sha": "9302a94619593492f962b5d54cafce35d1336e92",
            "filename": "docs/source/en/model_doc/moshi.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmoshi.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmoshi.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmoshi.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,12 @@ rendered properly in your Markdown viewer.\n \n # Moshi\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The Moshi model was proposed in [Moshi: a speech-text foundation model for real-time dialogue](https://kyutai.org/Moshi.pdf) by Alexandre DÃ©fossez, Laurent MazarÃ©, Manu Orsini, AmÃ©lie Royer, Patrick PÃ©rez, HervÃ© JÃ©gou, Edouard Grave and Neil Zeghidour."
        },
        {
            "sha": "cf84e2b410752429a2523656f8d61e032ad9de38",
            "filename": "docs/source/en/model_doc/mpnet.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmpnet.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmpnet.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmpnet.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,11 @@ rendered properly in your Markdown viewer.\n \n # MPNet\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The MPNet model was proposed in [MPNet: Masked and Permuted Pre-training for Language Understanding](https://arxiv.org/abs/2004.09297) by Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, Tie-Yan Liu."
        },
        {
            "sha": "a4dbc5ea6a8deb47d184cb3ca76d17d5c6d806b4",
            "filename": "docs/source/en/model_doc/mpt.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmpt.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmpt.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmpt.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # MPT\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The MPT model was proposed by the [MosaicML](https://www.mosaicml.com/) team and released with multiple sizes and finetuned variants. The MPT models are a series of open source and commercially usable LLMs pre-trained on 1T tokens. "
        },
        {
            "sha": "a5490d5d379cf0b9507b43c911a173d0d4f4ddf8",
            "filename": "docs/source/en/model_doc/mra.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmra.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmra.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmra.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # MRA\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The MRA model was proposed in [Multi Resolution Analysis (MRA) for Approximate Self-Attention](https://arxiv.org/abs/2207.10284) by Zhanpeng Zeng, Sourav Pal, Jeffery Kline, Glenn M Fung, and Vikas Singh."
        },
        {
            "sha": "d4af9f538cb3579f8b1d6270b0dcc5f894b47743",
            "filename": "docs/source/en/model_doc/mt5.md",
            "status": "modified",
            "additions": 4,
            "deletions": 6,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmt5.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmt5.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmt5.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -17,12 +17,10 @@ rendered properly in your Markdown viewer.\n # mT5\n \n <div class=\"flex flex-wrap space-x-1\">\n-<a href=\"https://huggingface.co/models?filter=mt5\">\n-<img alt=\"Models\" src=\"https://img.shields.io/badge/All_model_pages-mt5-blueviolet\">\n-</a>\n-<a href=\"https://huggingface.co/spaces/docs-demos/mt5-small-finetuned-arxiv-cs-finetuned-arxiv-cs-full\">\n-<img alt=\"Spaces\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue\">\n-</a>\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n+<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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\n+\">\n </div>\n \n ## Overview"
        },
        {
            "sha": "6d709a963c0436c49d9bf76ae3aa55e8123ab0b4",
            "filename": "docs/source/en/model_doc/musicgen.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmusicgen.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmusicgen.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmusicgen.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,12 @@ rendered properly in your Markdown viewer.\n \n # MusicGen\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The MusicGen model was proposed in the paper [Simple and Controllable Music Generation](https://arxiv.org/abs/2306.05284)"
        },
        {
            "sha": "b1f16c4574efef76afcb7bd37b631cf8a3cce05c",
            "filename": "docs/source/en/model_doc/musicgen_melody.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmusicgen_melody.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmusicgen_melody.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmusicgen_melody.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,12 @@ rendered properly in your Markdown viewer.\n \n # MusicGen Melody\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The MusicGen Melody model was proposed in [Simple and Controllable Music Generation](https://arxiv.org/abs/2306.05284) by Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi and Alexandre DÃ©fossez."
        },
        {
            "sha": "d732977167921ecf9c2c9414fc15885774141ff5",
            "filename": "docs/source/en/model_doc/mvp.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmvp.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmvp.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmvp.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # MVP\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The MVP model was proposed in [MVP: Multi-task Supervised Pre-training for Natural Language Generation](https://arxiv.org/abs/2206.12131) by Tianyi Tang, Junyi Li, Wayne Xin Zhao and Ji-Rong Wen."
        },
        {
            "sha": "c7725ed7a56381d280509180db479aef05b30084",
            "filename": "docs/source/en/model_doc/nat.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fnat.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fnat.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fnat.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # Neighborhood Attention Transformer\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n <Tip warning={true}>\n \n This model is in maintenance mode only, we don't accept any new PRs changing its code."
        },
        {
            "sha": "13b1b9be2fbcf265e32c3c9fdb508027a0ef37c3",
            "filename": "docs/source/en/model_doc/nemotron.md",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fnemotron.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fnemotron.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fnemotron.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -14,7 +14,11 @@ specific language governing permissions and limitations under the License.\n \n # Nemotron\n \n-## Nemotron\n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n \n ### License\n "
        },
        {
            "sha": "dc815e0ecc48152295479b110952518e3fc7048b",
            "filename": "docs/source/en/model_doc/nezha.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fnezha.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fnezha.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fnezha.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # Nezha\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n <Tip warning={true}>\n \n This model is in maintenance mode only, we don't accept any new PRs changing its code."
        },
        {
            "sha": "65a4812ed6ab335f14f0a13f534605dbeba3e193",
            "filename": "docs/source/en/model_doc/nllb-moe.md",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fnllb-moe.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fnllb-moe.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fnllb-moe.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,9 @@ rendered properly in your Markdown viewer.\n \n # NLLB-MOE\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n \n ## Overview\n "
        },
        {
            "sha": "4ba2737779209a9f8443ea7a88dcf5a3ad6581f9",
            "filename": "docs/source/en/model_doc/nllb.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fnllb.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fnllb.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fnllb.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,12 @@ rendered properly in your Markdown viewer.\n \n # NLLB\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Updated tokenizer behavior \n \n **DISCLAIMER:** The default behaviour for the tokenizer was fixed and thus changed in April 2023."
        },
        {
            "sha": "06b12b5ee8e66cc6c28376aeb9933ff8d65429e9",
            "filename": "docs/source/en/model_doc/nougat.md",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fnougat.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fnougat.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fnougat.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -15,6 +15,13 @@ specific language governing permissions and limitations under the License. -->\n \n # Nougat\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n+<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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\n+\">\n+</div>\n+\n ## Overview\n \n The Nougat model was proposed in [Nougat: Neural Optical Understanding for Academic Documents](https://arxiv.org/abs/2308.13418) by"
        },
        {
            "sha": "b4c017b35fff8b29edd3dcaa7bc83002b536441e",
            "filename": "docs/source/en/model_doc/nystromformer.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fnystromformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fnystromformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fnystromformer.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a",
            "patch": "@@ -16,6 +16,10 @@ rendered properly in your Markdown viewer.\n \n # NystrÃ¶mformer\n \n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n ## Overview\n \n The NystrÃ¶mformer model was proposed in [*NystrÃ¶mformer: A NystrÃ¶m-Based Algorithm for Approximating Self-Attention*](https://arxiv.org/abs/2102.03902) by Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn"
        },
        {
            "sha": "8d722185c31f229c68f4b7216554f64ccedcea77",
            "filename": "docs/source/en/model_doc/olmo.md",
            "status": "modified",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Folmo.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Folmo.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Folmo.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a"
        },
        {
            "sha": "24030b855244c1399c3d4c8fd838aa9ea12847e3",
            "filename": "docs/source/en/model_doc/olmo2.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Folmo2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Folmo2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Folmo2.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a"
        },
        {
            "sha": "6496e44c1bd5b98dc254f90873e5dda80b6668fe",
            "filename": "docs/source/en/model_doc/olmoe.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Folmoe.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Folmoe.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Folmoe.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a"
        },
        {
            "sha": "d73fef2d8b5d8844473b474740dcd42aebbc02f8",
            "filename": "docs/source/en/model_doc/omdet-turbo.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fomdet-turbo.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fomdet-turbo.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fomdet-turbo.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a"
        },
        {
            "sha": "f1c1de791238f3fc64df10a4a4057062696a0c7b",
            "filename": "docs/source/en/model_doc/oneformer.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Foneformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Foneformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Foneformer.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a"
        },
        {
            "sha": "3b4856cd4fb6db9fb8acc203c333cd8bfce81c83",
            "filename": "docs/source/en/model_doc/open-llama.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fopen-llama.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fopen-llama.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fopen-llama.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a"
        },
        {
            "sha": "68cda34db5ab5dc1c53d9b02848f10c2384afbe0",
            "filename": "docs/source/en/model_doc/openai-gpt.md",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fopenai-gpt.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fopenai-gpt.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fopenai-gpt.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a"
        },
        {
            "sha": "f6165e495393e4d0da1e59c0bc4c8266c97671ef",
            "filename": "docs/source/en/model_doc/opt.md",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fopt.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fopt.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fopt.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a"
        },
        {
            "sha": "f01a5c59063bd1308ef85286c0f044d0d01044cb",
            "filename": "docs/source/en/model_doc/owlv2.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fowlv2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fowlv2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fowlv2.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a"
        },
        {
            "sha": "5be8ffc8f58c312036165486c202b517f89a3a3c",
            "filename": "docs/source/en/model_doc/owlvit.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fowlvit.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fowlvit.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fowlvit.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a"
        },
        {
            "sha": "3662f0fcf47b2520288963f3de844a3e58528f75",
            "filename": "docs/source/en/model_doc/paligemma.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fpaligemma.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fpaligemma.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fpaligemma.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a"
        },
        {
            "sha": "dd678dd401013b210ed57b64ee1c7e0121aa775b",
            "filename": "docs/source/en/model_doc/patchtsmixer.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fpatchtsmixer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fpatchtsmixer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fpatchtsmixer.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a"
        },
        {
            "sha": "c55ba333429906e11d09ba1b7627fa151833e7c4",
            "filename": "docs/source/en/model_doc/patchtst.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fpatchtst.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fpatchtst.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fpatchtst.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a"
        },
        {
            "sha": "46fca71ac0d98c5774502b2a60f088ea5dec35fd",
            "filename": "docs/source/en/model_doc/pegasus.md",
            "status": "modified",
            "additions": 4,
            "deletions": 7,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fpegasus.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fpegasus.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fpegasus.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a"
        },
        {
            "sha": "3f982263cdb1c50963c97ddbd588080e3218a696",
            "filename": "docs/source/en/model_doc/pegasus_x.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fpegasus_x.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fpegasus_x.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fpegasus_x.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a"
        },
        {
            "sha": "700f49d42d931ffc6c783131b5c0f1e4946b4743",
            "filename": "docs/source/en/model_doc/perceiver.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fperceiver.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fperceiver.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fperceiver.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a"
        },
        {
            "sha": "bf721f19a107eb042138e1e81577064e04304b80",
            "filename": "docs/source/en/model_doc/persimmon.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fpersimmon.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fpersimmon.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fpersimmon.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a"
        },
        {
            "sha": "097d7fdd39eebdfe95ed5409db7ba4cb6f76eed8",
            "filename": "docs/source/en/model_doc/phi.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fphi.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fphi.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fphi.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a"
        },
        {
            "sha": "82973d39c07b6aaeda5db5465d20adf52a01099e",
            "filename": "docs/source/en/model_doc/phi3.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fphi3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fphi3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fphi3.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a"
        },
        {
            "sha": "6728248f2e0a1c177e5dcf66d59bdf16981874ef",
            "filename": "docs/source/en/model_doc/phimoe.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fphimoe.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fphimoe.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fphimoe.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a"
        },
        {
            "sha": "c1c4b8742b4d9641528a729892920210f80b5036",
            "filename": "docs/source/en/model_doc/phobert.md",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fphobert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fphobert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fphobert.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a"
        },
        {
            "sha": "e912cc96cdccec416125c827463d37829cbdbf71",
            "filename": "docs/source/en/model_doc/pix2struct.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fpix2struct.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fpix2struct.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fpix2struct.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a"
        },
        {
            "sha": "f287170a0e0fb9c7d0a1a156e0a37db66b5970be",
            "filename": "docs/source/en/model_doc/pixtral.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fpixtral.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fpixtral.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fpixtral.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a"
        },
        {
            "sha": "bac567615d42331ca3c0f23e6320dffa7614628e",
            "filename": "docs/source/en/model_doc/plbart.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fplbart.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fplbart.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fplbart.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a"
        },
        {
            "sha": "bce183706a832da3aecb9c6748adacdefdc13b03",
            "filename": "docs/source/en/model_doc/poolformer.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fpoolformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fpoolformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fpoolformer.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a"
        },
        {
            "sha": "a9554b4924a9bc135aeaf3f1fdae4b3d3915dfb8",
            "filename": "docs/source/en/model_doc/pop2piano.md",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fpop2piano.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fpop2piano.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fpop2piano.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a"
        },
        {
            "sha": "b768fef72a048448e853e5ed5c298ed7d11ef3ba",
            "filename": "docs/source/en/model_doc/prophetnet.md",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fprophetnet.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fprophetnet.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fprophetnet.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a"
        },
        {
            "sha": "d4c80445bf61852038a5cfb13d2c2c84b64105b4",
            "filename": "docs/source/en/model_doc/pvt.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fpvt.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fpvt.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fpvt.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a"
        },
        {
            "sha": "deac614d38bff152c7241cb530fd132974a8bf2d",
            "filename": "docs/source/en/model_doc/pvt_v2.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fpvt_v2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fpvt_v2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fpvt_v2.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a"
        },
        {
            "sha": "76555909c76d7a9dc5ca4a241f45d10f56d01822",
            "filename": "docs/source/en/model_doc/qdqbert.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fqdqbert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fqdqbert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqdqbert.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a"
        },
        {
            "sha": "dc6201d0de5e973827f83624c8cfbafb11d42a38",
            "filename": "docs/source/en/model_doc/qwen2.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a"
        },
        {
            "sha": "b2c138999e6f3948053fd4518adfd8fe73c889ea",
            "filename": "docs/source/en/model_doc/qwen2_5_vl.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_5_vl.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_5_vl.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_5_vl.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a"
        },
        {
            "sha": "8f7bd7c3b69c60fa400d40bec46fc2177198a588",
            "filename": "docs/source/en/model_doc/qwen2_audio.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_audio.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_audio.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_audio.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a"
        },
        {
            "sha": "eaaa66aedf7a70f2801b3d818746729442dd3c30",
            "filename": "docs/source/en/model_doc/qwen2_moe.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_moe.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_moe.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_moe.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a"
        },
        {
            "sha": "37c7ad31b311d32c16493935da309b0f1ff7d2da",
            "filename": "docs/source/en/model_doc/qwen2_vl.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_vl.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_vl.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_vl.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a"
        },
        {
            "sha": "8b65da43a22eaa3c281fcdfccd05838809983b89",
            "filename": "docs/source/en/model_doc/rag.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Frag.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Frag.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Frag.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a"
        },
        {
            "sha": "b5b9102c2c64b265ecdeb9b61981012095fc2191",
            "filename": "docs/source/en/model_doc/realm.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Frealm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Frealm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Frealm.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a"
        },
        {
            "sha": "b543b35a75f03deeae69ac087668a4649c2edafe",
            "filename": "docs/source/en/model_doc/recurrent_gemma.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Frecurrent_gemma.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Frecurrent_gemma.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Frecurrent_gemma.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a"
        },
        {
            "sha": "7e403599fdb0b74ebd354c02ffb850dd28053b36",
            "filename": "docs/source/en/model_doc/reformer.md",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Freformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Freformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Freformer.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a"
        },
        {
            "sha": "f292fe0df24bf371415d45c8e791ce158ae54429",
            "filename": "docs/source/en/model_doc/regnet.md",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fregnet.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fregnet.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fregnet.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a"
        },
        {
            "sha": "319e44cf0987f0ef1e44d814fefaae6ff4d71374",
            "filename": "docs/source/en/model_doc/rembert.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Frembert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Frembert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Frembert.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a"
        },
        {
            "sha": "d7400b46c838b21cac3838200e50e86f6aa0c1ee",
            "filename": "docs/source/en/model_doc/resnet.md",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fresnet.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fresnet.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fresnet.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a"
        },
        {
            "sha": "795f81caaa72eef2ebe7c47f8956c5c8e3853290",
            "filename": "docs/source/en/model_doc/retribert.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fretribert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fretribert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fretribert.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a"
        },
        {
            "sha": "7cef8526c251385df5ce893a3ce4fb9464781080",
            "filename": "docs/source/en/model_doc/roberta-prelayernorm.md",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Froberta-prelayernorm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Froberta-prelayernorm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Froberta-prelayernorm.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a"
        },
        {
            "sha": "10a46d6f57eba940f3b5997ff18246f6c48c87c0",
            "filename": "docs/source/en/model_doc/roberta.md",
            "status": "modified",
            "additions": 5,
            "deletions": 10,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Froberta.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Froberta.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Froberta.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a"
        },
        {
            "sha": "f3797663ff70aa6bd8c237e50070512e7f079724",
            "filename": "docs/source/en/model_doc/roc_bert.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Froc_bert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Froc_bert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Froc_bert.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a"
        },
        {
            "sha": "83d01c2fc91dc36c1d9d3f6525935b22c8aa3443",
            "filename": "docs/source/en/model_doc/roformer.md",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Froformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Froformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Froformer.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a"
        },
        {
            "sha": "c80e83e7b8832465994df36707548b29e26fa384",
            "filename": "docs/source/en/model_doc/rt_detr.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Frt_detr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Frt_detr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Frt_detr.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a"
        },
        {
            "sha": "e5212d945ce7343b6af1acf4fd8e8c5557c9a9cb",
            "filename": "docs/source/en/model_doc/rt_detr_v2.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Frt_detr_v2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Frt_detr_v2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Frt_detr_v2.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a"
        },
        {
            "sha": "8b54c25204bb318223900460e703416794f2bb2a",
            "filename": "docs/source/en/model_doc/rwkv.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Frwkv.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Frwkv.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Frwkv.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a"
        },
        {
            "sha": "cd9e3f5c3c456a566c376c4b7153cda01c444d2d",
            "filename": "docs/source/en/model_doc/sam.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a"
        },
        {
            "sha": "100198e50170a2600a986f7e2a1851821deae815",
            "filename": "docs/source/en/model_doc/seamless_m4t.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fseamless_m4t.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fseamless_m4t.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fseamless_m4t.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a"
        },
        {
            "sha": "7b68d08b5f95ccfe3b1e15d4aea4027927689168",
            "filename": "docs/source/en/model_doc/seamless_m4t_v2.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fseamless_m4t_v2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fseamless_m4t_v2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fseamless_m4t_v2.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a"
        },
        {
            "sha": "093a141eaf83c8f90e4b35192d45147cb3531475",
            "filename": "docs/source/en/model_doc/segformer.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fsegformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fsegformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsegformer.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a"
        },
        {
            "sha": "1eb82b84774c11c078fa80f55fec0766a268e106",
            "filename": "docs/source/en/model_doc/seggpt.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fseggpt.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fseggpt.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fseggpt.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a"
        },
        {
            "sha": "3626d953d97da411e8f4b247b770243b18adfdfa",
            "filename": "docs/source/en/model_doc/sew-d.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fsew-d.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fsew-d.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsew-d.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a"
        },
        {
            "sha": "cfc92db0eaa13eec3eb8fb02b9a1c55007f6ac18",
            "filename": "docs/source/en/model_doc/sew.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fsew.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fsew.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsew.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a"
        },
        {
            "sha": "478e8a19a8c3fc2baa7d08d8d565a8f30bf6108e",
            "filename": "docs/source/en/model_doc/siglip.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fsiglip.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fsiglip.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsiglip.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a"
        },
        {
            "sha": "0d49d9382361634bed2513580d4a816117ac34a9",
            "filename": "docs/source/en/model_doc/siglip2.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fsiglip2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fsiglip2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsiglip2.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a"
        },
        {
            "sha": "9512fb6aa2958dd2130e9393f122a5b6002e4273",
            "filename": "docs/source/en/model_doc/smolvlm.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fsmolvlm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fsmolvlm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsmolvlm.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a"
        },
        {
            "sha": "8893adfdd4a05cbd45691ca7bccfc55aff824dbe",
            "filename": "docs/source/en/model_doc/speech-encoder-decoder.md",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fspeech-encoder-decoder.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fspeech-encoder-decoder.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fspeech-encoder-decoder.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a"
        },
        {
            "sha": "8b375374ea54611e9c40a1de28ce585d38ad45cc",
            "filename": "docs/source/en/model_doc/speech_to_text.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fspeech_to_text.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fspeech_to_text.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fspeech_to_text.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a"
        },
        {
            "sha": "acbadb137f46fbc45b7fe3252565d33fa7ac278a",
            "filename": "docs/source/en/model_doc/speecht5.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fspeecht5.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fspeecht5.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fspeecht5.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a"
        },
        {
            "sha": "0d526beff9686fab6854a0e7fc3f54909e5c734c",
            "filename": "docs/source/en/model_doc/splinter.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fsplinter.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fsplinter.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsplinter.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a"
        },
        {
            "sha": "56046e22b79920fd4653bb9714d1346ea53fa4f0",
            "filename": "docs/source/en/model_doc/squeezebert.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fsqueezebert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fsqueezebert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsqueezebert.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a"
        },
        {
            "sha": "b996b7fcf9e85db9db25ccf6318ee2288c410716",
            "filename": "docs/source/en/model_doc/stablelm.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fstablelm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fstablelm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fstablelm.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a"
        },
        {
            "sha": "c6b146bf30ed000e84124f44c1a5d172307ba053",
            "filename": "docs/source/en/model_doc/starcoder2.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fstarcoder2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fstarcoder2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fstarcoder2.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a"
        },
        {
            "sha": "38ef55ab793f7ac196cacd3fbecb839de68d6117",
            "filename": "docs/source/en/model_doc/superglue.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fsuperglue.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0f8d055ce7a218e041e20a06946bf0baa8a7d6a/docs%2Fsource%2Fen%2Fmodel_doc%2Fsuperglue.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsuperglue.md?ref=c0f8d055ce7a218e041e20a06946bf0baa8a7d6a"
        }
    ],
    "stats": {
        "total": 24684,
        "additions": 10520,
        "deletions": 14164
    }
}