{
    "author": "aymeric-roucher",
    "message": "Add token cost + runtime monitoring to Agent and HfEngine children (#34548)\n\n* Add monitoring to Agent and HfEngine children",
    "sha": "901f50458050116f8df860717ac38fe172c6809f",
    "files": [
        {
            "sha": "08c30d54fd43d5c9ffe142e90a41f5d09f26e969",
            "filename": "src/transformers/agents/agents.py",
            "status": "modified",
            "additions": 127,
            "deletions": 59,
            "changes": 186,
            "blob_url": "https://github.com/huggingface/transformers/blob/901f50458050116f8df860717ac38fe172c6809f/src%2Ftransformers%2Fagents%2Fagents.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/901f50458050116f8df860717ac38fe172c6809f/src%2Ftransformers%2Fagents%2Fagents.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fagents%2Fagents.py?ref=901f50458050116f8df860717ac38fe172c6809f",
            "patch": "@@ -17,14 +17,16 @@\n import json\n import logging\n import re\n-from typing import Any, Callable, Dict, List, Literal, Optional, Tuple, Union\n+import time\n+from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n \n from .. import is_torch_available\n from ..utils import logging as transformers_logging\n from ..utils.import_utils import is_pygments_available\n from .agent_types import AgentAudio, AgentImage\n from .default_tools import BASE_PYTHON_TOOLS, FinalAnswerTool, setup_default_tools\n from .llm_engine import HfApiEngine, MessageRole\n+from .monitoring import Monitor\n from .prompts import (\n     DEFAULT_CODE_SYSTEM_PROMPT,\n     DEFAULT_REACT_CODE_SYSTEM_PROMPT,\n@@ -353,17 +355,23 @@ class Agent:\n     def __init__(\n         self,\n         tools: Union[List[Tool], Toolbox],\n-        llm_engine: Callable = HfApiEngine(),\n-        system_prompt=DEFAULT_REACT_CODE_SYSTEM_PROMPT,\n-        tool_description_template=None,\n-        additional_args={},\n+        llm_engine: Callable = None,\n+        system_prompt: Optional[str] = None,\n+        tool_description_template: Optional[str] = None,\n+        additional_args: Dict = {},\n         max_iterations: int = 6,\n-        tool_parser=parse_json_tool_call,\n+        tool_parser: Optional[Callable] = None,\n         add_base_tools: bool = False,\n         verbose: int = 0,\n-        grammar: Dict[str, str] = None,\n-        managed_agents: List = None,\n+        grammar: Optional[Dict[str, str]] = None,\n+        managed_agents: Optional[List] = None,\n+        step_callbacks: Optional[List[Callable]] = None,\n+        monitor_metrics: bool = True,\n     ):\n+        if system_prompt is None:\n+            system_prompt = DEFAULT_REACT_CODE_SYSTEM_PROMPT\n+        if tool_parser is None:\n+            tool_parser = parse_json_tool_call\n         self.agent_name = self.__class__.__name__\n         self.llm_engine = llm_engine\n         self.system_prompt_template = system_prompt\n@@ -406,6 +414,15 @@ def __init__(\n         elif verbose == 2:\n             logger.setLevel(logging.DEBUG)\n \n+        # Initialize step callbacks\n+        self.step_callbacks = step_callbacks if step_callbacks is not None else []\n+\n+        # Initialize Monitor if monitor_metrics is True\n+        self.monitor = None\n+        if monitor_metrics:\n+            self.monitor = Monitor(self.llm_engine)\n+            self.step_callbacks.append(self.monitor.update_metrics)\n+\n     @property\n     def toolbox(self) -> Toolbox:\n         \"\"\"Get the toolbox currently available to the agent\"\"\"\n@@ -578,13 +595,19 @@ class CodeAgent(Agent):\n     def __init__(\n         self,\n         tools: List[Tool],\n-        llm_engine: Callable = HfApiEngine(),\n-        system_prompt: str = DEFAULT_CODE_SYSTEM_PROMPT,\n-        tool_description_template: str = DEFAULT_TOOL_DESCRIPTION_TEMPLATE,\n-        grammar: Dict[str, str] = None,\n+        llm_engine: Optional[Callable] = None,\n+        system_prompt: Optional[str] = None,\n+        tool_description_template: Optional[str] = None,\n+        grammar: Optional[Dict[str, str]] = None,\n         additional_authorized_imports: Optional[List[str]] = None,\n         **kwargs,\n     ):\n+        if llm_engine is None:\n+            llm_engine = HfApiEngine()\n+        if system_prompt is None:\n+            system_prompt = DEFAULT_CODE_SYSTEM_PROMPT\n+        if tool_description_template is None:\n+            tool_description_template = DEFAULT_TOOL_DESCRIPTION_TEMPLATE\n         super().__init__(\n             tools=tools,\n             llm_engine=llm_engine,\n@@ -700,15 +723,24 @@ class ReactAgent(Agent):\n     def __init__(\n         self,\n         tools: List[Tool],\n-        llm_engine: Callable = HfApiEngine(),\n-        system_prompt: str = DEFAULT_REACT_CODE_SYSTEM_PROMPT,\n-        tool_description_template: str = DEFAULT_TOOL_DESCRIPTION_TEMPLATE,\n-        grammar: Dict[str, str] = None,\n-        plan_type: Literal[tuple(SUPPORTED_PLAN_TYPES)] = SUPPORTED_PLAN_TYPES[0],\n+        llm_engine: Optional[Callable] = None,\n+        system_prompt: Optional[str] = None,\n+        tool_description_template: Optional[str] = None,\n+        grammar: Optional[Dict[str, str]] = None,\n+        plan_type: Optional[str] = None,\n         planning_interval: Optional[int] = None,\n         **kwargs,\n     ):\n-        assert plan_type in SUPPORTED_PLAN_TYPES, f\"plan type {plan_type} is not supported\"\n+        if llm_engine is None:\n+            llm_engine = HfApiEngine()\n+        if system_prompt is None:\n+            system_prompt = DEFAULT_REACT_CODE_SYSTEM_PROMPT\n+        if tool_description_template is None:\n+            tool_description_template = DEFAULT_TOOL_DESCRIPTION_TEMPLATE\n+        if plan_type is None:\n+            plan_type = SUPPORTED_PLAN_TYPES[0]\n+        else:\n+            assert plan_type in SUPPORTED_PLAN_TYPES, f\"plan type {plan_type} is not supported\"\n         super().__init__(\n             tools=tools,\n             llm_engine=llm_engine,\n@@ -776,16 +808,24 @@ def stream_run(self, task: str):\n         final_answer = None\n         iteration = 0\n         while final_answer is None and iteration < self.max_iterations:\n+            step_start_time = time.time()\n+            step_log_entry = {\"iteration\": iteration, \"start_time\": step_start_time}\n             try:\n-                step_logs = self.step()\n-                if \"final_answer\" in step_logs:\n-                    final_answer = step_logs[\"final_answer\"]\n+                self.step(step_log_entry)\n+                if \"final_answer\" in step_log_entry:\n+                    final_answer = step_log_entry[\"final_answer\"]\n             except AgentError as e:\n                 self.logger.error(e, exc_info=1)\n-                self.logs[-1][\"error\"] = e\n+                step_log_entry[\"error\"] = e\n             finally:\n+                step_end_time = time.time()\n+                step_log_entry[\"step_end_time\"] = step_end_time\n+                step_log_entry[\"step_duration\"] = step_end_time - step_start_time\n+                self.logs.append(step_log_entry)\n+                for callback in self.step_callbacks:\n+                    callback(step_log_entry)\n                 iteration += 1\n-                yield self.logs[-1]\n+                yield step_log_entry\n \n         if final_answer is None and iteration == self.max_iterations:\n             error_message = \"Reached max iterations.\"\n@@ -794,6 +834,9 @@ def stream_run(self, task: str):\n             self.logger.error(error_message, exc_info=1)\n             final_answer = self.provide_final_answer(task)\n             final_step_log[\"final_answer\"] = final_answer\n+            final_step_log[\"step_duration\"] = 0\n+            for callback in self.step_callbacks:\n+                callback(final_step_log)\n             yield final_step_log\n \n         yield final_answer\n@@ -805,16 +848,24 @@ def direct_run(self, task: str):\n         final_answer = None\n         iteration = 0\n         while final_answer is None and iteration < self.max_iterations:\n+            step_start_time = time.time()\n+            step_log_entry = {\"iteration\": iteration, \"start_time\": step_start_time}\n             try:\n                 if self.planning_interval is not None and iteration % self.planning_interval == 0:\n                     self.planning_step(task, is_first_step=(iteration == 0), iteration=iteration)\n-                step_logs = self.step()\n-                if \"final_answer\" in step_logs:\n-                    final_answer = step_logs[\"final_answer\"]\n+                self.step(step_log_entry)\n+                if \"final_answer\" in step_log_entry:\n+                    final_answer = step_log_entry[\"final_answer\"]\n             except AgentError as e:\n                 self.logger.error(e, exc_info=1)\n-                self.logs[-1][\"error\"] = e\n+                step_log_entry[\"error\"] = e\n             finally:\n+                step_end_time = time.time()\n+                step_log_entry[\"step_end_time\"] = step_end_time\n+                step_log_entry[\"step_duration\"] = step_end_time - step_start_time\n+                self.logs.append(step_log_entry)\n+                for callback in self.step_callbacks:\n+                    callback(step_log_entry)\n                 iteration += 1\n \n         if final_answer is None and iteration == self.max_iterations:\n@@ -824,6 +875,9 @@ def direct_run(self, task: str):\n             self.logger.error(error_message, exc_info=1)\n             final_answer = self.provide_final_answer(task)\n             final_step_log[\"final_answer\"] = final_answer\n+            final_step_log[\"step_duration\"] = 0\n+            for callback in self.step_callbacks:\n+                callback(final_step_log)\n \n         return final_answer\n \n@@ -937,13 +991,19 @@ class ReactJsonAgent(ReactAgent):\n     def __init__(\n         self,\n         tools: List[Tool],\n-        llm_engine: Callable = HfApiEngine(),\n-        system_prompt: str = DEFAULT_REACT_JSON_SYSTEM_PROMPT,\n-        tool_description_template: str = DEFAULT_TOOL_DESCRIPTION_TEMPLATE,\n-        grammar: Dict[str, str] = None,\n+        llm_engine: Optional[Callable] = None,\n+        system_prompt: Optional[str] = None,\n+        tool_description_template: Optional[str] = None,\n+        grammar: Optional[Dict[str, str]] = None,\n         planning_interval: Optional[int] = None,\n         **kwargs,\n     ):\n+        if llm_engine is None:\n+            llm_engine = HfApiEngine()\n+        if system_prompt is None:\n+            system_prompt = DEFAULT_REACT_JSON_SYSTEM_PROMPT\n+        if tool_description_template is None:\n+            tool_description_template = DEFAULT_TOOL_DESCRIPTION_TEMPLATE\n         super().__init__(\n             tools=tools,\n             llm_engine=llm_engine,\n@@ -954,7 +1014,7 @@ def __init__(\n             **kwargs,\n         )\n \n-    def step(self):\n+    def step(self, log_entry: Dict[str, Any]):\n         \"\"\"\n         Perform one step in the ReAct framework: the agent thinks, acts, and observes the result.\n         The errors are raised here, they are caught and logged in the run() method.\n@@ -965,9 +1025,7 @@ def step(self):\n         self.logger.debug(\"===== New step =====\")\n \n         # Add new step in logs\n-        current_step_logs = {}\n-        self.logs.append(current_step_logs)\n-        current_step_logs[\"agent_memory\"] = agent_memory.copy()\n+        log_entry[\"agent_memory\"] = agent_memory.copy()\n \n         self.logger.info(\"===== Calling LLM with this last message: =====\")\n         self.logger.info(self.prompt[-1])\n@@ -981,7 +1039,7 @@ def step(self):\n             raise AgentGenerationError(f\"Error in generating llm output: {e}.\")\n         self.logger.debug(\"===== Output message of the LLM: =====\")\n         self.logger.debug(llm_output)\n-        current_step_logs[\"llm_output\"] = llm_output\n+        log_entry[\"llm_output\"] = llm_output\n \n         # Parse\n         self.logger.debug(\"===== Extracting action =====\")\n@@ -992,8 +1050,8 @@ def step(self):\n         except Exception as e:\n             raise AgentParsingError(f\"Could not parse the given action: {e}.\")\n \n-        current_step_logs[\"rationale\"] = rationale\n-        current_step_logs[\"tool_call\"] = {\"tool_name\": tool_name, \"tool_arguments\": arguments}\n+        log_entry[\"rationale\"] = rationale\n+        log_entry[\"tool_call\"] = {\"tool_name\": tool_name, \"tool_arguments\": arguments}\n \n         # Execute\n         self.logger.warning(\"=== Agent thoughts:\")\n@@ -1011,8 +1069,8 @@ def step(self):\n                     answer = arguments\n             else:\n                 answer = arguments\n-            current_step_logs[\"final_answer\"] = answer\n-            return current_step_logs\n+            log_entry[\"final_answer\"] = answer\n+            return answer\n         else:\n             if arguments is None:\n                 arguments = {}\n@@ -1030,8 +1088,8 @@ def step(self):\n             else:\n                 updated_information = str(observation).strip()\n             self.logger.info(updated_information)\n-            current_step_logs[\"observation\"] = updated_information\n-            return current_step_logs\n+            log_entry[\"observation\"] = updated_information\n+            return log_entry\n \n \n class ReactCodeAgent(ReactAgent):\n@@ -1044,14 +1102,20 @@ class ReactCodeAgent(ReactAgent):\n     def __init__(\n         self,\n         tools: List[Tool],\n-        llm_engine: Callable = HfApiEngine(),\n-        system_prompt: str = DEFAULT_REACT_CODE_SYSTEM_PROMPT,\n-        tool_description_template: str = DEFAULT_TOOL_DESCRIPTION_TEMPLATE,\n-        grammar: Dict[str, str] = None,\n+        llm_engine: Optional[Callable] = None,\n+        system_prompt: Optional[str] = None,\n+        tool_description_template: Optional[str] = None,\n+        grammar: Optional[Dict[str, str]] = None,\n         additional_authorized_imports: Optional[List[str]] = None,\n         planning_interval: Optional[int] = None,\n         **kwargs,\n     ):\n+        if llm_engine is None:\n+            llm_engine = HfApiEngine()\n+        if system_prompt is None:\n+            system_prompt = DEFAULT_REACT_CODE_SYSTEM_PROMPT\n+        if tool_description_template is None:\n+            tool_description_template = DEFAULT_TOOL_DESCRIPTION_TEMPLATE\n         super().__init__(\n             tools=tools,\n             llm_engine=llm_engine,\n@@ -1075,21 +1139,18 @@ def __init__(\n         self.system_prompt = self.system_prompt.replace(\"<<authorized_imports>>\", str(self.authorized_imports))\n         self.custom_tools = {}\n \n-    def step(self):\n+    def step(self, log_entry: Dict[str, Any]):\n         \"\"\"\n         Perform one step in the ReAct framework: the agent thinks, acts, and observes the result.\n         The errors are raised here, they are caught and logged in the run() method.\n         \"\"\"\n         agent_memory = self.write_inner_memory_from_logs()\n \n         self.prompt = agent_memory.copy()\n-\n         self.logger.debug(\"===== New step =====\")\n \n         # Add new step in logs\n-        current_step_logs = {}\n-        self.logs.append(current_step_logs)\n-        current_step_logs[\"agent_memory\"] = agent_memory.copy()\n+        log_entry[\"agent_memory\"] = agent_memory.copy()\n \n         self.logger.info(\"===== Calling LLM with these last messages: =====\")\n         self.logger.info(self.prompt[-2:])\n@@ -1104,7 +1165,7 @@ def step(self):\n \n         self.logger.debug(\"=== Output message of the LLM:\")\n         self.logger.debug(llm_output)\n-        current_step_logs[\"llm_output\"] = llm_output\n+        log_entry[\"llm_output\"] = llm_output\n \n         # Parse\n         self.logger.debug(\"=== Extracting action ===\")\n@@ -1120,8 +1181,8 @@ def step(self):\n             error_msg = f\"Error in code parsing: {e}. Make sure to provide correct code\"\n             raise AgentParsingError(error_msg)\n \n-        current_step_logs[\"rationale\"] = rationale\n-        current_step_logs[\"tool_call\"] = {\"tool_name\": \"code interpreter\", \"tool_arguments\": code_action}\n+        log_entry[\"rationale\"] = rationale\n+        log_entry[\"tool_call\"] = {\"tool_name\": \"code interpreter\", \"tool_arguments\": code_action}\n \n         # Execute\n         self.log_rationale_code_action(rationale, code_action)\n@@ -1146,7 +1207,7 @@ def step(self):\n                 self.logger.warning(\"Last output from code snippet:\")\n                 self.logger.log(32, str(result))\n                 observation += \"Last output from code snippet:\\n\" + str(result)[:100000]\n-            current_step_logs[\"observation\"] = observation\n+            log_entry[\"observation\"] = observation\n         except Exception as e:\n             error_msg = f\"Code execution failed due to the following error:\\n{str(e)}\"\n             if \"'dict' object has no attribute 'read'\" in str(e):\n@@ -1156,8 +1217,11 @@ def step(self):\n             if line[: len(\"final_answer\")] == \"final_answer\":\n                 self.logger.log(33, \"Final answer:\")\n                 self.logger.log(32, result)\n-                current_step_logs[\"final_answer\"] = result\n-        return current_step_logs\n+                log_entry[\"final_answer\"] = result\n+        return result\n+\n+\n+LENGTH_TRUNCATE_REPORTS = 1000\n \n \n class ManagedAgent:\n@@ -1200,10 +1264,14 @@ def __call__(self, request, **kwargs):\n             answer += f\"\\n\\nFor more detail, find below a summary of this agent's work:\\nSUMMARY OF WORK FROM AGENT '{self.name}':\\n\"\n             for message in self.agent.write_inner_memory_from_logs(summary_mode=True):\n                 content = message[\"content\"]\n-                if len(str(content)) < 1000 or \"[FACTS LIST]\" in str(content):\n+                if len(str(content)) < LENGTH_TRUNCATE_REPORTS or \"[FACTS LIST]\" in str(content):\n                     answer += \"\\n\" + str(content) + \"\\n---\"\n                 else:\n-                    answer += \"\\n\" + str(content)[:1000] + \"\\n(...Step was truncated because too long)...\\n---\"\n+                    answer += (\n+                        \"\\n\"\n+                        + str(content)[:LENGTH_TRUNCATE_REPORTS]\n+                        + \"\\n(...Step was truncated because too long)...\\n---\"\n+                    )\n             answer += f\"\\nEND OF SUMMARY OF WORK FROM AGENT '{self.name}'.\"\n             return answer\n         else:"
        },
        {
            "sha": "afa4d62d059e5b47863bcc84d8c5f542a70f2e2e",
            "filename": "src/transformers/agents/llm_engine.py",
            "status": "modified",
            "additions": 95,
            "deletions": 54,
            "changes": 149,
            "blob_url": "https://github.com/huggingface/transformers/blob/901f50458050116f8df860717ac38fe172c6809f/src%2Ftransformers%2Fagents%2Fllm_engine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/901f50458050116f8df860717ac38fe172c6809f/src%2Ftransformers%2Fagents%2Fllm_engine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fagents%2Fllm_engine.py?ref=901f50458050116f8df860717ac38fe172c6809f",
            "patch": "@@ -20,7 +20,12 @@\n \n from huggingface_hub import InferenceClient\n \n+from .. import AutoTokenizer\n from ..pipelines.base import Pipeline\n+from ..utils import logging\n+\n+\n+logger = logging.get_logger(__name__)\n \n \n class MessageRole(str, Enum):\n@@ -67,46 +72,32 @@ def get_clean_message_list(message_list: List[Dict[str, str]], role_conversions:\n }\n \n \n-class HfApiEngine:\n-    \"\"\"A class to interact with Hugging Face's Inference API for language model interaction.\n-\n-    This engine allows you to communicate with Hugging Face's models using the Inference API. It can be used in both serverless mode or with a dedicated endpoint, supporting features like stop sequences and grammar customization.\n-\n-    Parameters:\n-        model (`str`, *optional*, defaults to `\"meta-llama/Meta-Llama-3.1-8B-Instruct\"`):\n-            The Hugging Face model ID to be used for inference. This can be a path or model identifier from the Hugging Face model hub.\n-        token (`str`, *optional*):\n-            The Hugging Face API token for authentication. If not provided, the class will use the token stored in the Hugging Face CLI configuration.\n-        max_tokens (`int`, *optional*, defaults to 1500):\n-            The maximum number of tokens allowed in the output.\n-        timeout (`int`, *optional*, defaults to 120):\n-            Timeout for the API request, in seconds.\n-\n-    Raises:\n-        ValueError:\n-            If the model name is not provided.\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        model: str = \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n-        token: Optional[str] = None,\n-        max_tokens: Optional[int] = 1500,\n-        timeout: Optional[int] = 120,\n+class HfEngine:\n+    def __init__(self, model_id: Optional[str] = None):\n+        self.last_input_token_count = None\n+        self.last_output_token_count = None\n+        if model_id is None:\n+            model_id = \"HuggingFaceTB/SmolLM2-1.7B-Instruct\"\n+            logger.warning(f\"Using default model for token counting: '{model_id}'\")\n+        try:\n+            self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n+        except Exception as e:\n+            logger.warning(f\"Failed to load tokenizer for model {model_id}: {e}. Loading default tokenizer instead.\")\n+            self.tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-1.7B-Instruct\")\n+\n+    def get_token_counts(self):\n+        return {\n+            \"input_token_count\": self.last_input_token_count,\n+            \"output_token_count\": self.last_output_token_count,\n+        }\n+\n+    def generate(\n+        self, messages: List[Dict[str, str]], stop_sequences: Optional[List[str]] = None, grammar: Optional[str] = None\n     ):\n-        \"\"\"Initialize the HfApiEngine.\"\"\"\n-        if not model:\n-            raise ValueError(\"Model name must be provided.\")\n-\n-        self.model = model\n-        self.client = InferenceClient(self.model, token=token, timeout=timeout)\n-        self.max_tokens = max_tokens\n+        raise NotImplementedError\n \n     def __call__(\n-        self,\n-        messages: List[Dict[str, str]],\n-        stop_sequences: Optional[List[str]] = [],\n-        grammar: Optional[str] = None,\n+        self, messages: List[Dict[str, str]], stop_sequences: Optional[List[str]] = None, grammar: Optional[str] = None\n     ) -> str:\n         \"\"\"Process the input messages and return the model's response.\n \n@@ -136,6 +127,57 @@ def __call__(\n             \"Quantum mechanics is the branch of physics that studies...\"\n             ```\n         \"\"\"\n+        if not isinstance(messages, List):\n+            raise ValueError(\"Messages should be a list of dictionaries with 'role' and 'content' keys.\")\n+        if stop_sequences is None:\n+            stop_sequences = []\n+        response = self.generate(messages, stop_sequences, grammar)\n+        self.last_input_token_count = len(self.tokenizer.apply_chat_template(messages, tokenize=True))\n+        self.last_output_token_count = len(self.tokenizer.encode(response))\n+\n+        # Remove stop sequences from LLM output\n+        for stop_seq in stop_sequences:\n+            if response[-len(stop_seq) :] == stop_seq:\n+                response = response[: -len(stop_seq)]\n+        return response\n+\n+\n+class HfApiEngine(HfEngine):\n+    \"\"\"A class to interact with Hugging Face's Inference API for language model interaction.\n+\n+    This engine allows you to communicate with Hugging Face's models using the Inference API. It can be used in both serverless mode or with a dedicated endpoint, supporting features like stop sequences and grammar customization.\n+\n+    Parameters:\n+        model (`str`, *optional*, defaults to `\"meta-llama/Meta-Llama-3.1-8B-Instruct\"`):\n+            The Hugging Face model ID to be used for inference. This can be a path or model identifier from the Hugging Face model hub.\n+        token (`str`, *optional*):\n+            Token used by the Hugging Face API for authentication.\n+            If not provided, the class will use the token stored in the Hugging Face CLI configuration.\n+        max_tokens (`int`, *optional*, defaults to 1500):\n+            The maximum number of tokens allowed in the output.\n+        timeout (`int`, *optional*, defaults to 120):\n+            Timeout for the API request, in seconds.\n+\n+    Raises:\n+        ValueError:\n+            If the model name is not provided.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        model: str = \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n+        token: Optional[str] = None,\n+        max_tokens: Optional[int] = 1500,\n+        timeout: Optional[int] = 120,\n+    ):\n+        super().__init__(model_id=model)\n+        self.model = model\n+        self.client = InferenceClient(self.model, token=token, timeout=timeout)\n+        self.max_tokens = max_tokens\n+\n+    def generate(\n+        self, messages: List[Dict[str, str]], stop_sequences: Optional[List[str]] = None, grammar: Optional[str] = None\n+    ) -> str:\n         # Get clean message list\n         messages = get_clean_message_list(messages, role_conversions=llama_role_conversions)\n \n@@ -148,41 +190,40 @@ def __call__(\n             response = self.client.chat_completion(messages, stop=stop_sequences, max_tokens=self.max_tokens)\n \n         response = response.choices[0].message.content\n-\n-        # Remove stop sequences from LLM output\n-        for stop_seq in stop_sequences:\n-            if response[-len(stop_seq) :] == stop_seq:\n-                response = response[: -len(stop_seq)]\n         return response\n \n \n-class TransformersEngine:\n+class TransformersEngine(HfEngine):\n     \"\"\"This engine uses a pre-initialized local text-generation pipeline.\"\"\"\n \n-    def __init__(self, pipeline: Pipeline):\n+    def __init__(self, pipeline: Pipeline, model_id: Optional[str] = None):\n+        super().__init__(model_id)\n         self.pipeline = pipeline\n \n-    def __call__(\n-        self, messages: List[Dict[str, str]], stop_sequences: Optional[List[str]] = None, grammar: Optional[str] = None\n+    def generate(\n+        self,\n+        messages: List[Dict[str, str]],\n+        stop_sequences: Optional[List[str]] = None,\n+        grammar: Optional[str] = None,\n+        max_length: int = 1500,\n     ) -> str:\n         # Get clean message list\n         messages = get_clean_message_list(messages, role_conversions=llama_role_conversions)\n \n         # Get LLM output\n+        if stop_sequences is not None and len(stop_sequences) > 0:\n+            stop_strings = stop_sequences\n+        else:\n+            stop_strings = None\n+\n         output = self.pipeline(\n             messages,\n-            stop_strings=stop_sequences,\n-            max_length=1500,\n+            stop_strings=stop_strings,\n+            max_length=max_length,\n             tokenizer=self.pipeline.tokenizer,\n         )\n \n         response = output[0][\"generated_text\"][-1][\"content\"]\n-\n-        # Remove stop sequences from LLM output\n-        if stop_sequences is not None:\n-            for stop_seq in stop_sequences:\n-                if response[-len(stop_seq) :] == stop_seq:\n-                    response = response[: -len(stop_seq)]\n         return response\n \n "
        },
        {
            "sha": "7126e72b5fd0606fbb4bb6f7eff46e0ad0992abd",
            "filename": "src/transformers/agents/monitoring.py",
            "status": "modified",
            "additions": 26,
            "deletions": 2,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/901f50458050116f8df860717ac38fe172c6809f/src%2Ftransformers%2Fagents%2Fmonitoring.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/901f50458050116f8df860717ac38fe172c6809f/src%2Ftransformers%2Fagents%2Fmonitoring.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fagents%2Fmonitoring.py?ref=901f50458050116f8df860717ac38fe172c6809f",
            "patch": "@@ -14,8 +14,11 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n+from ..utils import logging\n from .agent_types import AgentAudio, AgentImage, AgentText\n-from .agents import ReactAgent\n+\n+\n+logger = logging.get_logger(__name__)\n \n \n def pull_message(step_log: dict, test_mode: bool = True):\n@@ -54,7 +57,7 @@ def __init__(self, role, content, metadata=None):\n         )\n \n \n-def stream_to_gradio(agent: ReactAgent, task: str, test_mode: bool = False, **kwargs):\n+def stream_to_gradio(agent, task: str, test_mode: bool = False, **kwargs):\n     \"\"\"Runs an agent with the given task and streams the messages from the agent as gradio ChatMessages.\"\"\"\n \n     try:\n@@ -91,3 +94,24 @@ def __init__(self, role, content, metadata=None):\n         )\n     else:\n         yield ChatMessage(role=\"assistant\", content=str(final_answer))\n+\n+\n+class Monitor:\n+    def __init__(self, tracked_llm_engine):\n+        self.step_durations = []\n+        self.tracked_llm_engine = tracked_llm_engine\n+        if getattr(self.tracked_llm_engine, \"last_input_token_count\", \"Not found\") != \"Not found\":\n+            self.total_input_token_count = 0\n+            self.total_output_token_count = 0\n+\n+    def update_metrics(self, step_log):\n+        step_duration = step_log[\"step_duration\"]\n+        self.step_durations.append(step_duration)\n+        logger.info(f\"Step {len(self.step_durations)}:\")\n+        logger.info(f\"- Time taken: {step_duration:.2f} seconds (valid only if step succeeded)\")\n+\n+        if getattr(self.tracked_llm_engine, \"last_input_token_count\", None) is not None:\n+            self.total_input_token_count += self.tracked_llm_engine.last_input_token_count\n+            self.total_output_token_count += self.tracked_llm_engine.last_output_token_count\n+            logger.info(f\"- Input tokens: {self.total_input_token_count}\")\n+            logger.info(f\"- Output tokens: {self.total_output_token_count}\")"
        },
        {
            "sha": "759704612c2f4f4a2082e767ff93c66d62fa66d1",
            "filename": "src/transformers/agents/tools.py",
            "status": "modified",
            "additions": 13,
            "deletions": 12,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/901f50458050116f8df860717ac38fe172c6809f/src%2Ftransformers%2Fagents%2Ftools.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/901f50458050116f8df860717ac38fe172c6809f/src%2Ftransformers%2Fagents%2Ftools.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fagents%2Ftools.py?ref=901f50458050116f8df860717ac38fe172c6809f",
            "patch": "@@ -785,21 +785,22 @@ def launch_gradio_demo(tool_class: Tool):\n     def fn(*args, **kwargs):\n         return tool(*args, **kwargs)\n \n+    TYPE_TO_COMPONENT_CLASS_MAPPING = {\n+        \"image\": gr.Image,\n+        \"audio\": gr.Audio,\n+        \"string\": gr.Textbox,\n+        \"integer\": gr.Textbox,\n+        \"number\": gr.Textbox,\n+    }\n+\n     gradio_inputs = []\n     for input_name, input_details in tool_class.inputs.items():\n-        input_type = input_details[\"type\"]\n-        if input_type == \"image\":\n-            gradio_inputs.append(gr.Image(label=input_name))\n-        elif input_type == \"audio\":\n-            gradio_inputs.append(gr.Audio(label=input_name))\n-        elif input_type in [\"string\", \"integer\", \"number\"]:\n-            gradio_inputs.append(gr.Textbox(label=input_name))\n-        else:\n-            error_message = f\"Input type '{input_type}' not supported.\"\n-            raise ValueError(error_message)\n+        input_gradio_component_class = TYPE_TO_COMPONENT_CLASS_MAPPING[input_details[\"type\"]]\n+        new_component = input_gradio_component_class(label=input_name)\n+        gradio_inputs.append(new_component)\n \n-    gradio_output = tool_class.output_type\n-    assert gradio_output in [\"string\", \"image\", \"audio\"], f\"Output type '{gradio_output}' not supported.\"\n+    output_gradio_componentclass = TYPE_TO_COMPONENT_CLASS_MAPPING[tool_class.output_type]\n+    gradio_output = output_gradio_componentclass(label=input_name)\n \n     gr.Interface(\n         fn=fn,"
        },
        {
            "sha": "c35074270ea272136abfa52b4956e477f752801c",
            "filename": "tests/agents/test_monitoring.py",
            "status": "modified",
            "additions": 85,
            "deletions": 1,
            "changes": 86,
            "blob_url": "https://github.com/huggingface/transformers/blob/901f50458050116f8df860717ac38fe172c6809f/tests%2Fagents%2Ftest_monitoring.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/901f50458050116f8df860717ac38fe172c6809f/tests%2Fagents%2Ftest_monitoring.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fagents%2Ftest_monitoring.py?ref=901f50458050116f8df860717ac38fe172c6809f",
            "patch": "@@ -21,11 +21,95 @@\n \n \n class MonitoringTester(unittest.TestCase):\n+    def test_code_agent_metrics(self):\n+        class FakeLLMEngine:\n+            def __init__(self):\n+                self.last_input_token_count = 10\n+                self.last_output_token_count = 20\n+\n+            def __call__(self, prompt, **kwargs):\n+                return \"\"\"\n+Code:\n+```py\n+final_answer('This is the final answer.')\n+```\"\"\"\n+\n+        agent = ReactCodeAgent(\n+            tools=[],\n+            llm_engine=FakeLLMEngine(),\n+            max_iterations=1,\n+        )\n+\n+        agent.run(\"Fake task\")\n+\n+        self.assertEqual(agent.monitor.total_input_token_count, 10)\n+        self.assertEqual(agent.monitor.total_output_token_count, 20)\n+\n+    def test_json_agent_metrics(self):\n+        class FakeLLMEngine:\n+            def __init__(self):\n+                self.last_input_token_count = 10\n+                self.last_output_token_count = 20\n+\n+            def __call__(self, prompt, **kwargs):\n+                return 'Action:{\"action\": \"final_answer\", \"action_input\": {\"answer\": \"image\"}}'\n+\n+        agent = ReactJsonAgent(\n+            tools=[],\n+            llm_engine=FakeLLMEngine(),\n+            max_iterations=1,\n+        )\n+\n+        agent.run(\"Fake task\")\n+\n+        self.assertEqual(agent.monitor.total_input_token_count, 10)\n+        self.assertEqual(agent.monitor.total_output_token_count, 20)\n+\n+    def test_code_agent_metrics_max_iterations(self):\n+        class FakeLLMEngine:\n+            def __init__(self):\n+                self.last_input_token_count = 10\n+                self.last_output_token_count = 20\n+\n+            def __call__(self, prompt, **kwargs):\n+                return \"Malformed answer\"\n+\n+        agent = ReactCodeAgent(\n+            tools=[],\n+            llm_engine=FakeLLMEngine(),\n+            max_iterations=1,\n+        )\n+\n+        agent.run(\"Fake task\")\n+\n+        self.assertEqual(agent.monitor.total_input_token_count, 20)\n+        self.assertEqual(agent.monitor.total_output_token_count, 40)\n+\n+    def test_code_agent_metrics_generation_error(self):\n+        class FakeLLMEngine:\n+            def __init__(self):\n+                self.last_input_token_count = 10\n+                self.last_output_token_count = 20\n+\n+            def __call__(self, prompt, **kwargs):\n+                raise AgentError\n+\n+        agent = ReactCodeAgent(\n+            tools=[],\n+            llm_engine=FakeLLMEngine(),\n+            max_iterations=1,\n+        )\n+\n+        agent.run(\"Fake task\")\n+\n+        self.assertEqual(agent.monitor.total_input_token_count, 20)\n+        self.assertEqual(agent.monitor.total_output_token_count, 40)\n+\n     def test_streaming_agent_text_output(self):\n         def dummy_llm_engine(prompt, **kwargs):\n             return \"\"\"\n Code:\n-````\n+```py\n final_answer('This is the final answer.')\n ```\"\"\"\n "
        }
    ],
    "stats": {
        "total": 474,
        "additions": 346,
        "deletions": 128
    }
}