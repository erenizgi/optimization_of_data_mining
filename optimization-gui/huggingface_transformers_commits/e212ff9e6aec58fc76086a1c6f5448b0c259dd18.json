{
    "author": "zucchini-nlp",
    "message": "[video processor] support torchcodec and decrease cuda memory usage (#38880)\n\n* don't move the whole video to GPU\n\n* add torchcodec\n\n* add tests\n\n* make style\n\n* instrucblip as well\n\n* consistency\n\n* Update src/transformers/utils/import_utils.py\n\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\n\n* Update src/transformers/utils/import_utils.py\n\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\n\n* Update src/transformers/video_utils.py\n\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\n\n---------\n\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>",
    "sha": "e212ff9e6aec58fc76086a1c6f5448b0c259dd18",
    "files": [
        {
            "sha": "330dba0c3b857ba71c36fdfa02422dd9bb56f82d",
            "filename": "src/transformers/models/instructblipvideo/video_processing_instructblipvideo.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/e212ff9e6aec58fc76086a1c6f5448b0c259dd18/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fvideo_processing_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e212ff9e6aec58fc76086a1c6f5448b0c259dd18/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fvideo_processing_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fvideo_processing_instructblipvideo.py?ref=e212ff9e6aec58fc76086a1c6f5448b0c259dd18",
            "patch": "@@ -94,12 +94,18 @@ def _preprocess(\n         fps: Optional[int] = None,\n         num_frames: Optional[int] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n+        device: Optional[\"torch.Tensor\"] = None,\n     ) -> BatchFeature:\n         if do_sample_frames:\n             videos = [\n                 self.sample_frames(video, metadata, num_frames, fps) for video, metadata in zip(videos, video_metadata)\n             ]\n \n+        # We need to sample frames first before moving to device, if `do_sample_frames=True`. Otherwise\n+        # moving the whole video incurs high GPU mem usage for long videos\n+        if device is not None:\n+            videos = [video.to(device) for video in videos]\n+\n         # Group videos by size for batched resizing\n         grouped_videos, grouped_videos_index = group_videos_by_shape(videos)\n         resized_videos_grouped = {}"
        },
        {
            "sha": "c9be4ebb94c8ca9810981129c4ce41e2ed356241",
            "filename": "src/transformers/models/internvl/video_processing_internvl.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/e212ff9e6aec58fc76086a1c6f5448b0c259dd18/src%2Ftransformers%2Fmodels%2Finternvl%2Fvideo_processing_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e212ff9e6aec58fc76086a1c6f5448b0c259dd18/src%2Ftransformers%2Fmodels%2Finternvl%2Fvideo_processing_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fvideo_processing_internvl.py?ref=e212ff9e6aec58fc76086a1c6f5448b0c259dd18",
            "patch": "@@ -147,6 +147,7 @@ def _preprocess(\n         num_frames: Optional[int] = None,\n         initial_shift: Optional[Union[bool, float, int]] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n+        device: Optional[\"torch.Tensor\"] = None,\n     ) -> BatchFeature:\n         if do_sample_frames:\n             # Sample video frames\n@@ -155,6 +156,11 @@ def _preprocess(\n                 for video, metadata in zip(videos, video_metadata)\n             ]\n \n+        # We need to sample frames first before moving to device, if `do_sample_frames=True`. Otherwise\n+        # moving the whole video incurs high GPU mem usage for long videos\n+        if device is not None:\n+            videos = [video.to(device) for video in videos]\n+\n         # Group videos by size for batched resizing\n         grouped_videos, grouped_videos_index = group_videos_by_shape(videos)\n         resized_videos_grouped = {}"
        },
        {
            "sha": "5640b8d333813985905cced20bbdbd1c1cb1c155",
            "filename": "src/transformers/models/qwen2_vl/video_processing_qwen2_vl.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/e212ff9e6aec58fc76086a1c6f5448b0c259dd18/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fvideo_processing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e212ff9e6aec58fc76086a1c6f5448b0c259dd18/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fvideo_processing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fvideo_processing_qwen2_vl.py?ref=e212ff9e6aec58fc76086a1c6f5448b0c259dd18",
            "patch": "@@ -213,6 +213,7 @@ def _preprocess(\n         min_frames: Optional[int] = None,\n         max_frames: Optional[int] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n+        device: Optional[\"torch.Tensor\"] = None,\n         **kwargs,\n     ):\n         if do_sample_frames:\n@@ -230,6 +231,11 @@ def _preprocess(\n                 for video, metadata in zip(videos, video_metadata)\n             ]\n \n+        # We need to sample frames first before moving to device, if `do_sample_frames=True`. Otherwise\n+        # moving the whole video incurs high GPU mem usage for long videos\n+        if device is not None:\n+            videos = [video.to(device) for video in videos]\n+\n         # Group videos by size for batched resizing\n         grouped_videos, grouped_videos_index = group_videos_by_shape(videos)\n         resized_videos_grouped = {}"
        },
        {
            "sha": "730079f9b40023f6f74e95d5a787c1c8cf672430",
            "filename": "src/transformers/models/smolvlm/video_processing_smolvlm.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/e212ff9e6aec58fc76086a1c6f5448b0c259dd18/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fvideo_processing_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e212ff9e6aec58fc76086a1c6f5448b0c259dd18/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fvideo_processing_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fvideo_processing_smolvlm.py?ref=e212ff9e6aec58fc76086a1c6f5448b0c259dd18",
            "patch": "@@ -332,6 +332,7 @@ def _preprocess(\n         num_frames: Optional[int] = None,\n         skip_secs: Optional[int] = 0,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n+        device: Optional[\"torch.Tensor\"] = None,\n         **kwargs,\n     ):\n         # Group videos by size for batched resizing\n@@ -356,6 +357,11 @@ def _preprocess(\n             ]\n             durations_list = [len(video) // 24 for video in videos]\n \n+        # We need to sample frames first before moving to device, if `do_sample_frames=True`. Otherwise\n+        # moving the whole video incurs high GPU mem usage for long videos\n+        if device is not None:\n+            videos = [video.to(device) for video in videos]\n+\n         grouped_videos, grouped_videos_index = group_videos_by_shape(processed_videos)\n         resized_videos_grouped = {}\n         for shape, stacked_videos in grouped_videos.items():"
        },
        {
            "sha": "1a4232adc8c572e9edc1b1801cc6f15caa331a44",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/e212ff9e6aec58fc76086a1c6f5448b0c259dd18/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e212ff9e6aec58fc76086a1c6f5448b0c259dd18/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=e212ff9e6aec58fc76086a1c6f5448b0c259dd18",
            "patch": "@@ -158,6 +158,7 @@\n     is_torch_xpu_available,\n     is_torchao_available,\n     is_torchaudio_available,\n+    is_torchcodec_available,\n     is_torchdynamo_available,\n     is_torchvision_available,\n     is_vision_available,\n@@ -634,6 +635,16 @@ def require_torchvision(test_case):\n     return unittest.skipUnless(is_torchvision_available(), \"test requires Torchvision\")(test_case)\n \n \n+def require_torchcodec(test_case):\n+    \"\"\"\n+    Decorator marking a test that requires Torchcodec.\n+\n+    These tests are skipped when Torchcodec isn't installed.\n+\n+    \"\"\"\n+    return unittest.skipUnless(is_torchcodec_available(), \"test requires Torchvision\")(test_case)\n+\n+\n def require_torch_or_tf(test_case):\n     \"\"\"\n     Decorator marking a test that requires PyTorch or TensorFlow."
        },
        {
            "sha": "6d73b8d0325b05d8cc12a6c2ef8bf31fa0f39893",
            "filename": "src/transformers/utils/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e212ff9e6aec58fc76086a1c6f5448b0c259dd18/src%2Ftransformers%2Futils%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e212ff9e6aec58fc76086a1c6f5448b0c259dd18/src%2Ftransformers%2Futils%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2F__init__.py?ref=e212ff9e6aec58fc76086a1c6f5448b0c259dd18",
            "patch": "@@ -254,6 +254,7 @@\n     is_torch_xpu_available,\n     is_torchao_available,\n     is_torchaudio_available,\n+    is_torchcodec_available,\n     is_torchdistx_available,\n     is_torchdynamo_available,\n     is_torchdynamo_compiling,"
        },
        {
            "sha": "0fe8ba55c9e951f70d8afca97e78bdad199f5bf5",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/e212ff9e6aec58fc76086a1c6f5448b0c259dd18/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e212ff9e6aec58fc76086a1c6f5448b0c259dd18/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=e212ff9e6aec58fc76086a1c6f5448b0c259dd18",
            "patch": "@@ -119,6 +119,7 @@ def _is_package_available(pkg_name: str, return_version: bool = False) -> Union[\n _vptq_available, _vptq_version = _is_package_available(\"vptq\", return_version=True)\n _av_available = importlib.util.find_spec(\"av\") is not None\n _decord_available = importlib.util.find_spec(\"decord\") is not None\n+_torchcodec_available = importlib.util.find_spec(\"torchcodec\") is not None\n _bitsandbytes_available = _is_package_available(\"bitsandbytes\")\n _eetq_available = _is_package_available(\"eetq\")\n _fbgemm_gpu_available = _is_package_available(\"fbgemm_gpu\")\n@@ -976,6 +977,10 @@ def is_decord_available():\n     return _decord_available\n \n \n+def is_torchcodec_available():\n+    return _torchcodec_available\n+\n+\n def is_ninja_available():\n     r\"\"\"\n     Code comes from *torch.utils.cpp_extension.is_ninja_available()*. Returns `True` if the\n@@ -1502,6 +1507,14 @@ def check_torch_load_is_safe():\n Please note that you may need to restart your runtime after installation.\n \"\"\"\n \n+TORCHCODEC_IMPORT_ERROR = \"\"\"\n+{0} requires the TorchCodec (https://github.com/pytorch/torchcodec) library, but it was not found in your environment. You can install it with:\n+```\n+pip install torchcodec\n+```\n+Please note that you may need to restart your runtime after installation.\n+\"\"\"\n+\n # docstyle-ignore\n CV2_IMPORT_ERROR = \"\"\"\n {0} requires the OpenCV library but it was not found in your environment. You can install it with:\n@@ -1882,6 +1895,7 @@ def check_torch_load_is_safe():\n         (\"tokenizers\", (is_tokenizers_available, TOKENIZERS_IMPORT_ERROR)),\n         (\"torch\", (is_torch_available, PYTORCH_IMPORT_ERROR)),\n         (\"torchvision\", (is_torchvision_available, TORCHVISION_IMPORT_ERROR)),\n+        (\"torchcodec\", (is_torchcodec_available, TORCHCODEC_IMPORT_ERROR)),\n         (\"vision\", (is_vision_available, VISION_IMPORT_ERROR)),\n         (\"scipy\", (is_scipy_available, SCIPY_IMPORT_ERROR)),\n         (\"accelerate\", (is_accelerate_available, ACCELERATE_IMPORT_ERROR)),"
        },
        {
            "sha": "b21b38d34f08a30b3265df7c4b2eb798c446d728",
            "filename": "src/transformers/video_processing_utils.py",
            "status": "modified",
            "additions": 9,
            "deletions": 7,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/e212ff9e6aec58fc76086a1c6f5448b0c259dd18/src%2Ftransformers%2Fvideo_processing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e212ff9e6aec58fc76086a1c6f5448b0c259dd18/src%2Ftransformers%2Fvideo_processing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fvideo_processing_utils.py?ref=e212ff9e6aec58fc76086a1c6f5448b0c259dd18",
            "patch": "@@ -294,7 +294,6 @@ def _prepare_input_videos(\n         videos: VideoInput,\n         video_metadata: VideoMetadata = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-        device: Optional[\"torch.device\"] = None,\n     ) -> list[\"torch.Tensor\"]:\n         \"\"\"\n         Prepare the input videos for processing.\n@@ -313,10 +312,6 @@ def _prepare_input_videos(\n                 # not using F.to_tensor as it doesn't handle (C, H, W) numpy arrays\n                 video = torch.from_numpy(video).contiguous()\n \n-            # Now that we have torch tensors, we can move them to the right device\n-            if device is not None:\n-                video = video.to(device)\n-\n             processed_videos.append(video)\n         return processed_videos, batch_metadata\n \n@@ -336,10 +331,9 @@ def preprocess(\n             kwargs.setdefault(kwarg_name, getattr(self, kwarg_name, None))\n \n         input_data_format = kwargs.pop(\"input_data_format\")\n-        device = kwargs.pop(\"device\")\n         video_metadata = kwargs.pop(\"video_metadata\")\n         videos, video_metadata = self._prepare_input_videos(\n-            videos=videos, video_metadata=video_metadata, input_data_format=input_data_format, device=device\n+            videos=videos, video_metadata=video_metadata, input_data_format=input_data_format\n         )\n \n         kwargs = self._further_process_kwargs(**kwargs)\n@@ -378,6 +372,7 @@ def _preprocess(\n         fps: Optional[int] = None,\n         num_frames: Optional[int] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n+        device: Optional[\"torch.Tensor\"] = None,\n     ) -> BatchFeature:\n         if do_sample_frames:\n             # Sample video frames\n@@ -386,6 +381,11 @@ def _preprocess(\n                 for video, metadata in zip(videos, video_metadata)\n             ]\n \n+        # We need to sample frames first before moving to device, if `do_sample_frames=True`. Otherwise\n+        # moving the whole video incurs high GPU mem usage for long videos\n+        if device is not None:\n+            videos = [video.to(device) for video in videos]\n+\n         # Group videos by size for batched resizing\n         grouped_videos, grouped_videos_index = group_videos_by_shape(videos)\n         resized_videos_grouped = {}\n@@ -775,6 +775,8 @@ def to_dict(self) -> dict[str, Any]:\n             `dict[str, Any]`: Dictionary of all the attributes that make up this video processor instance.\n         \"\"\"\n         output = copy.deepcopy(self.__dict__)\n+        output.pop(\"model_valid_processing_keys\", None)\n+        output.pop(\"_valid_kwargs_names\", None)\n         output[\"video_processor_type\"] = self.__class__.__name__\n \n         return output"
        },
        {
            "sha": "ea02eefd5fbd4ab1eaf7482cb690216f5fce23bc",
            "filename": "src/transformers/video_utils.py",
            "status": "modified",
            "additions": 57,
            "deletions": 2,
            "changes": 59,
            "blob_url": "https://github.com/huggingface/transformers/blob/e212ff9e6aec58fc76086a1c6f5448b0c259dd18/src%2Ftransformers%2Fvideo_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e212ff9e6aec58fc76086a1c6f5448b0c259dd18/src%2Ftransformers%2Fvideo_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fvideo_utils.py?ref=e212ff9e6aec58fc76086a1c6f5448b0c259dd18",
            "patch": "@@ -14,6 +14,7 @@\n # limitations under the License.\n \n import os\n+import warnings\n from collections.abc import Iterable\n from contextlib import redirect_stdout\n from dataclasses import dataclass\n@@ -33,6 +34,7 @@\n     is_numpy_array,\n     is_torch_available,\n     is_torch_tensor,\n+    is_torchcodec_available,\n     is_torchvision_available,\n     is_vision_available,\n     is_yt_dlp_available,\n@@ -425,6 +427,10 @@ def sample_indices_fn(metadata, **kwargs):\n             - Numpy array of frames in RGB (shape: [num_frames, height, width, 3]).\n             - `VideoMetadata` object.\n     \"\"\"\n+    warnings.warn(\n+        \"Using `torchvision` for video decoding is deprecated and will be removed in future versions. \"\n+        \"Please use `torchcodec` instead.\"\n+    )\n     video, _, info = torchvision_io.read_video(\n         video_path,\n         start_pts=0.0,\n@@ -449,11 +455,59 @@ def sample_indices_fn(metadata, **kwargs):\n     return video, metadata\n \n \n+def read_video_torchcodec(\n+    video_path: str,\n+    sample_indices_fn: Callable,\n+    **kwargs,\n+):\n+    \"\"\"\n+    Decode the video with torchcodec decoder.\n+\n+    Args:\n+        video_path (`str`):\n+            Path to the video file.\n+        sample_indices_fn (`Callable`, *optional*):\n+            A callable function that will return indices at which the video should be sampled. If the video has to be loaded using\n+            by a different sampling technique than provided by `num_frames` or `fps` arguments, one should provide their own `sample_indices_fn`.\n+            If not provided, simple uniform sampling with fps is performed.\n+            Example:\n+            def sample_indices_fn(metadata, **kwargs):\n+                return np.linspace(0, metadata.total_num_frames - 1, num_frames, dtype=int)\n+\n+    Returns:\n+        Tuple[`torch.Tensor`, `VideoMetadata`]: A tuple containing:\n+            - Numpy array of frames in RGB (shape: [num_frames, height, width, 3]).\n+            - `VideoMetadata` object.\n+    \"\"\"\n+    # Lazy import torchcodec\n+    requires_backends(read_video_torchcodec, [\"torchcodec\"])\n+    from torchcodec.decoders import VideoDecoder\n+\n+    decoder = VideoDecoder(\n+        video_path,\n+        dimension_order=\"NHWC\",  # to be consistent with other decoders\n+        # Interestingly `exact` mode takes less than approximate when we load the whole video\n+        seek_mode=\"exact\",\n+    )\n+    metadata = VideoMetadata(\n+        total_num_frames=decoder.metadata.num_frames,\n+        fps=decoder.metadata.average_fps,\n+        duration=decoder.metadata.duration_seconds,\n+        video_backend=\"torchcodec\",\n+    )\n+    indices = sample_indices_fn(metadata=metadata, **kwargs)\n+\n+    video = decoder.get_frames_at(indices=indices).data.contiguous()\n+    metadata.frames_indices = indices\n+    return video, metadata\n+\n+\n VIDEO_DECODERS = {\n     \"decord\": read_video_decord,\n     \"opencv\": read_video_opencv,\n     \"pyav\": read_video_pyav,\n     \"torchvision\": read_video_torchvision,\n+    \"torchcodec\": read_video_torchcodec,\n }\n \n \n@@ -477,7 +531,7 @@ def load_video(\n             Number of frames to sample per second. Should be passed only when `num_frames=None`.\n             If not specified and `num_frames==None`, all frames are sampled.\n         backend (`str`, *optional*, defaults to `\"pyav\"`):\n-            The backend to use when loading the video. Can be any of [\"decord\", \"pyav\", \"opencv\", \"torchvision\"]. Defaults to \"pyav\".\n+            The backend to use when loading the video. Can be any of [\"decord\", \"pyav\", \"opencv\", \"torchvision\", \"torchcodec\"]. Defaults to \"pyav\".\n         sample_indices_fn (`Callable`, *optional*):\n             A callable function that will return indices at which the video should be sampled. If the video has to be loaded using\n             by a different sampling technique than provided by `num_frames` or `fps` arguments, one should provide their own `sample_indices_fn`.\n@@ -535,7 +589,7 @@ def sample_indices_fn_func(metadata, **fn_kwargs):\n     video_is_url = video.startswith(\"http://\") or video.startswith(\"https://\")\n     if video_is_url and backend in [\"opencv\", \"torchvision\"]:\n         raise ValueError(\n-            \"If you are trying to load a video from URL, you can decode the video only with `pyav` or `decord` as backend\"\n+            \"If you are trying to load a video from URL, you can decode the video only with `pyav`, `decord` or `torchcodec` as backend\"\n         )\n \n     if file_obj is None:\n@@ -546,6 +600,7 @@ def sample_indices_fn_func(metadata, **fn_kwargs):\n         or (not is_av_available() and backend == \"pyav\")\n         or (not is_cv2_available() and backend == \"opencv\")\n         or (not is_torchvision_available() and backend == \"torchvision\")\n+        or (not is_torchcodec_available() and backend == \"torchcodec\")\n     ):\n         raise ImportError(\n             f\"You chose backend={backend} for loading the video but the required library is not found in your environment \""
        },
        {
            "sha": "7c598222bd6b2fe6c126f6f259dab90e9a5999b6",
            "filename": "tests/utils/test_video_utils.py",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/e212ff9e6aec58fc76086a1c6f5448b0c259dd18/tests%2Futils%2Ftest_video_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e212ff9e6aec58fc76086a1c6f5448b0c259dd18/tests%2Futils%2Ftest_video_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_video_utils.py?ref=e212ff9e6aec58fc76086a1c6f5448b0c259dd18",
            "patch": "@@ -27,6 +27,7 @@\n     require_cv2,\n     require_decord,\n     require_torch,\n+    require_torchcodec,\n     require_torchvision,\n     require_vision,\n )\n@@ -261,6 +262,7 @@ def test_load_video_local(self):\n \n     @require_decord\n     @require_torchvision\n+    @require_torchcodec\n     @require_cv2\n     def test_load_video_backend_url(self):\n         video, _ = load_video(\n@@ -269,6 +271,12 @@ def test_load_video_backend_url(self):\n         )\n         self.assertEqual(video.shape, (243, 360, 640, 3))\n \n+        video, _ = load_video(\n+            \"https://huggingface.co/datasets/raushan-testing-hf/videos-test/resolve/main/sample_demo_1.mp4\",\n+            backend=\"torchcodec\",\n+        )\n+        self.assertEqual(video.shape, (243, 360, 640, 3))\n+\n         # Can't use certain backends with url\n         with self.assertRaises(ValueError):\n             video, _ = load_video(\n@@ -283,6 +291,7 @@ def test_load_video_backend_url(self):\n \n     @require_decord\n     @require_torchvision\n+    @require_torchcodec\n     @require_cv2\n     def test_load_video_backend_local(self):\n         video_file_path = hf_hub_download(\n@@ -300,6 +309,10 @@ def test_load_video_backend_local(self):\n         self.assertEqual(video.shape, (243, 360, 640, 3))\n         self.assertIsInstance(metadata, VideoMetadata)\n \n+        video, metadata = load_video(video_file_path, backend=\"torchcodec\")\n+        self.assertEqual(video.shape, (243, 360, 640, 3))\n+        self.assertIsInstance(metadata, VideoMetadata)\n+\n     def test_load_video_num_frames(self):\n         video, _ = load_video(\n             \"https://huggingface.co/datasets/raushan-testing-hf/videos-test/resolve/main/sample_demo_1.mp4\","
        }
    ],
    "stats": {
        "total": 138,
        "additions": 129,
        "deletions": 9
    }
}