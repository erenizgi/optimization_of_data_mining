{
    "author": "Cyrilvallez",
    "message": "Fix new FA2 if `is_causal` is passed explicitly (#35390)\n\n* fix\r\n\r\n* Update modeling_decision_transformer.py\r\n\r\n* Update flash_attention.py",
    "sha": "05260a1fc1c8571a2b421ce72b680d5f1bc3e5a4",
    "files": [
        {
            "sha": "a3ca4bea484d22176f49dd74d93666ce7c7e2e40",
            "filename": "src/transformers/integrations/flash_attention.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/05260a1fc1c8571a2b421ce72b680d5f1bc3e5a4/src%2Ftransformers%2Fintegrations%2Fflash_attention.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05260a1fc1c8571a2b421ce72b680d5f1bc3e5a4/src%2Ftransformers%2Fintegrations%2Fflash_attention.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fflash_attention.py?ref=05260a1fc1c8571a2b421ce72b680d5f1bc3e5a4",
            "patch": "@@ -44,6 +44,9 @@ def flash_attention_forward(\n         else:\n             target_dtype = next(layer for layer in module.modules() if isinstance(layer, torch.nn.Linear)).weight.dtype\n \n+    # FA2 always relies on the value set in the module, so remove it if present in kwargs to avoid passing it twice\n+    kwargs.pop(\"is_causal\", None)\n+\n     attn_output = _flash_attention_forward(\n         query,\n         key,"
        },
        {
            "sha": "683b683008f2da0a7d2594faea02efbbc19bd503",
            "filename": "src/transformers/models/decision_transformer/modeling_decision_transformer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/05260a1fc1c8571a2b421ce72b680d5f1bc3e5a4/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05260a1fc1c8571a2b421ce72b680d5f1bc3e5a4/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py?ref=05260a1fc1c8571a2b421ce72b680d5f1bc3e5a4",
            "patch": "@@ -285,9 +285,9 @@ def forward(\n         shape_q = (*query_states.shape[:-1], -1, self.head_dim)\n         shape_kv = (*key_states.shape[:-1], -1, self.head_dim)\n \n-        query_states = query_states.reshape(shape_q).transpose(1, 2)\n-        key_states = key_states.reshape(shape_kv).transpose(1, 2)\n-        value_states = value_states.reshape(shape_kv).transpose(1, 2)\n+        query_states = query_states.view(shape_q).transpose(1, 2)\n+        key_states = key_states.view(shape_kv).transpose(1, 2)\n+        value_states = value_states.view(shape_kv).transpose(1, 2)\n \n         if layer_past is not None:\n             past_key, past_value = layer_past"
        },
        {
            "sha": "854c21576b5048d4a3594fe76fdffeed3c3e3263",
            "filename": "src/transformers/models/gpt2/modeling_gpt2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/05260a1fc1c8571a2b421ce72b680d5f1bc3e5a4/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05260a1fc1c8571a2b421ce72b680d5f1bc3e5a4/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py?ref=05260a1fc1c8571a2b421ce72b680d5f1bc3e5a4",
            "patch": "@@ -295,9 +295,9 @@ def forward(\n         shape_q = (*query_states.shape[:-1], -1, self.head_dim)\n         shape_kv = (*key_states.shape[:-1], -1, self.head_dim)\n \n-        query_states = query_states.reshape(shape_q).transpose(1, 2)\n-        key_states = key_states.reshape(shape_kv).transpose(1, 2)\n-        value_states = value_states.reshape(shape_kv).transpose(1, 2)\n+        query_states = query_states.view(shape_q).transpose(1, 2)\n+        key_states = key_states.view(shape_kv).transpose(1, 2)\n+        value_states = value_states.view(shape_kv).transpose(1, 2)\n \n         if layer_past is not None:\n             past_key, past_value = layer_past"
        }
    ],
    "stats": {
        "total": 15,
        "additions": 9,
        "deletions": 6
    }
}