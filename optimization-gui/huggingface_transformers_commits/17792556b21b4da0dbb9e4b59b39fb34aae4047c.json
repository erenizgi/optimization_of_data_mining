{
    "author": "wejoncy",
    "message": "[save_pretrained ] Skip collecting duplicated weight (#36409)\n\n* Skip collecting duplicated weight\n\n* format",
    "sha": "17792556b21b4da0dbb9e4b59b39fb34aae4047c",
    "files": [
        {
            "sha": "1553287c92a04ff150019f0bdd8a0099b9c33195",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/17792556b21b4da0dbb9e4b59b39fb34aae4047c/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17792556b21b4da0dbb9e4b59b39fb34aae4047c/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=17792556b21b4da0dbb9e4b59b39fb34aae4047c",
            "patch": "@@ -3071,6 +3071,9 @@ def save_pretrained(\n                 # init state_dict for this shard\n                 shard_state_dict = {name: \"\" for name in shard}\n                 for module_name in shard:\n+                    # skip to collect this weight again\n+                    if shard_state_dict.get(module_name) != \"\":\n+                        continue\n                     module = module_map[module_name]\n                     # update state dict with onloaded parameters\n                     shard_state_dict = get_state_dict_from_offload(module, module_name, shard_state_dict)"
        }
    ],
    "stats": {
        "total": 3,
        "additions": 3,
        "deletions": 0
    }
}