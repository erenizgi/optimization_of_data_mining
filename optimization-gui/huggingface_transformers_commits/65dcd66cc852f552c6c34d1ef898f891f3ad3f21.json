{
    "author": "cyyever",
    "message": "ðŸš¨ [V5] Remove deprecated training arguments  (#41017)\n\n* Remove deprecated training arguments from V5\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Remove deprecated training arguments from V5\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Fix comments\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Fix code\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n---------\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>",
    "sha": "65dcd66cc852f552c6c34d1ef898f891f3ad3f21",
    "files": [
        {
            "sha": "65f6243d3dd2ef78f369798b291b1cbde9afaa9e",
            "filename": "examples/pytorch/question-answering/trainer_qa.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/65dcd66cc852f552c6c34d1ef898f891f3ad3f21/examples%2Fpytorch%2Fquestion-answering%2Ftrainer_qa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65dcd66cc852f552c6c34d1ef898f891f3ad3f21/examples%2Fpytorch%2Fquestion-answering%2Ftrainer_qa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fquestion-answering%2Ftrainer_qa.py?ref=65dcd66cc852f552c6c34d1ef898f891f3ad3f21",
            "patch": "@@ -83,7 +83,7 @@ def evaluate(self, eval_dataset=None, eval_examples=None, ignore_keys=None, metr\n             # Only the main node log the results by default\n             self.log(metrics)\n \n-        if self.args.tpu_metrics_debug or self.args.debug:\n+        if self.args.debug:\n             # tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\n             xm.master_print(met.metrics_report())\n "
        },
        {
            "sha": "a3f2b883b28d63c61ca1de4028c79e356d17c26f",
            "filename": "examples/pytorch/question-answering/trainer_seq2seq_qa.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/65dcd66cc852f552c6c34d1ef898f891f3ad3f21/examples%2Fpytorch%2Fquestion-answering%2Ftrainer_seq2seq_qa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65dcd66cc852f552c6c34d1ef898f891f3ad3f21/examples%2Fpytorch%2Fquestion-answering%2Ftrainer_seq2seq_qa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fquestion-answering%2Ftrainer_seq2seq_qa.py?ref=65dcd66cc852f552c6c34d1ef898f891f3ad3f21",
            "patch": "@@ -106,7 +106,7 @@ def evaluate(\n             # Only the main node log the results by default\n             self.log(metrics)\n \n-        if self.args.tpu_metrics_debug or self.args.debug:\n+        if self.args.debug:\n             # tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\n             xm.master_print(met.metrics_report())\n "
        },
        {
            "sha": "5e71f2a30a6d0b148febb79201df87546dfe9ffe",
            "filename": "src/transformers/training_args.py",
            "status": "modified",
            "additions": 8,
            "deletions": 227,
            "changes": 235,
            "blob_url": "https://github.com/huggingface/transformers/blob/65dcd66cc852f552c6c34d1ef898f891f3ad3f21/src%2Ftransformers%2Ftraining_args.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65dcd66cc852f552c6c34d1ef898f891f3ad3f21/src%2Ftransformers%2Ftraining_args.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftraining_args.py?ref=65dcd66cc852f552c6c34d1ef898f891f3ad3f21",
            "patch": "@@ -21,11 +21,8 @@\n from datetime import timedelta\n from enum import Enum\n from functools import cached_property\n-from pathlib import Path\n from typing import Any, Optional, Union\n \n-from huggingface_hub import get_full_repo_name\n-\n from .debug_utils import DebugOption\n from .trainer_utils import (\n     EvaluationStrategy,\n@@ -383,7 +380,7 @@ class TrainingArguments:\n             Whether to restore the callback states from the checkpoint. If `True`, will override\n             callbacks passed to the `Trainer` if they exist in the checkpoint.\"\n         use_cpu (`bool`, *optional*, defaults to `False`):\n-            Whether or not to use cpu. If set to False, we will use cuda or mps device if available.\n+            Whether or not to use cpu. If set to False, we will use the available torch device/backend.\n         seed (`int`, *optional*, defaults to 42):\n             Random seed that will be set at the beginning of training. To ensure reproducibility across runs, use the\n             [`~Trainer.model_init`] function to instantiate the model if it has some randomly initialized parameters.\n@@ -401,8 +398,6 @@ class TrainingArguments:\n         fp16_opt_level (`str`, *optional*, defaults to 'O1'):\n             For `fp16` training, Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3']. See details on\n             the [Apex documentation](https://nvidia.github.io/apex/amp).\n-        fp16_backend (`str`, *optional*, defaults to `\"auto\"`):\n-            This argument is deprecated. Use `half_precision_backend` instead.\n         half_precision_backend (`str`, *optional*, defaults to `\"auto\"`):\n             The backend to use for mixed precision training. Must be one of `\"auto\", \"apex\", \"cpu_amp\"`. `\"auto\"` will\n             use CPU/CUDA AMP or APEX depending on the PyTorch version detected, while the other choices will force the\n@@ -706,8 +701,6 @@ class TrainingArguments:\n             If True, use gradient checkpointing to save memory at the expense of slower backward pass.\n         gradient_checkpointing_kwargs (`dict`, *optional*, defaults to `None`):\n             Key word arguments to be passed to the `gradient_checkpointing_enable` method.\n-        include_inputs_for_metrics (`bool`, *optional*, defaults to `False`):\n-            This argument is deprecated. Use `include_for_metrics` instead, e.g, `include_for_metrics = [\"inputs\"]`.\n         include_for_metrics (`list[str]`, *optional*, defaults to `[]`):\n             Include additional data in the `compute_metrics` function if needed for metrics computation.\n             Possible options to add to `include_for_metrics` list:\n@@ -722,9 +715,6 @@ class TrainingArguments:\n         full_determinism (`bool`, *optional*, defaults to `False`)\n             If `True`, [`enable_full_determinism`] is called instead of [`set_seed`] to ensure reproducible results in\n             distributed training. Important: this will negatively impact the performance, so only use it for debugging.\n-        torchdynamo (`str`, *optional*):\n-            If set, the backend compiler for TorchDynamo. Possible choices are `\"eager\"`, `\"aot_eager\"`, `\"inductor\"`,\n-            `\"nvfuser\"`, `\"aot_nvfuser\"`, `\"aot_cudagraphs\"`, `\"ofi\"`, `\"fx2trt\"`, `\"onnxrt\"` and `\"ipex\"`.\n         ray_scope (`str`, *optional*, defaults to `\"last\"`):\n             The scope to use when doing hyperparameter search with Ray. By default, `\"last\"` will be used. Ray will\n             then use the last checkpoint of all trials, compare those, and select the best one. However, other options\n@@ -736,8 +726,6 @@ class TrainingArguments:\n             performing slow operations in distributed runnings. Please refer the [PyTorch documentation]\n             (https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group) for more\n             information.\n-        use_mps_device (`bool`, *optional*, defaults to `False`):\n-            This argument is deprecated.`mps` device will be used if it is available similar to `cuda` device.\n         torch_compile (`bool`, *optional*, defaults to `False`):\n             Whether or not to compile the model using PyTorch 2.0\n             [`torch.compile`](https://pytorch.org/get-started/pytorch-2.0/).\n@@ -858,25 +846,6 @@ class TrainingArguments:\n         default=8, metadata={\"help\": \"Batch size per device accelerator core/CPU for evaluation.\"}\n     )\n \n-    per_gpu_train_batch_size: Optional[int] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"Deprecated, the use of `--per_device_train_batch_size` is preferred. \"\n-                \"Batch size per GPU/TPU core/CPU for training.\"\n-            )\n-        },\n-    )\n-    per_gpu_eval_batch_size: Optional[int] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"Deprecated, the use of `--per_device_eval_batch_size` is preferred. \"\n-                \"Batch size per GPU/TPU core/CPU for evaluation.\"\n-            )\n-        },\n-    )\n-\n     gradient_accumulation_steps: int = field(\n         default=1,\n         metadata={\"help\": \"Number of updates steps to accumulate before performing a backward/update pass.\"},\n@@ -1036,23 +1005,12 @@ class TrainingArguments:\n             \"help\": \"Whether to restore the callback states from the checkpoint. If `True`, will override callbacks passed to the `Trainer` if they exist in the checkpoint.\"\n         },\n     )\n-    no_cuda: bool = field(\n-        default=False,\n-        metadata={\"help\": \"This argument is deprecated. It will be removed in version 5.0 of ðŸ¤— Transformers.\"},\n-    )\n     use_cpu: bool = field(\n         default=False,\n         metadata={\n             \"help\": \"Whether or not to use cpu. If left to False, we will use the available torch device/backend (cuda/mps/xpu/hpu etc.)\"\n         },\n     )\n-    use_mps_device: bool = field(\n-        default=False,\n-        metadata={\n-            \"help\": \"This argument is deprecated. `mps` device will be used if available similar to `cuda` device.\"\n-            \" It will be removed in version 5.0 of ðŸ¤— Transformers\"\n-        },\n-    )\n     seed: int = field(default=42, metadata={\"help\": \"Random seed that will be set at the beginning of training.\"})\n     data_seed: Optional[int] = field(default=None, metadata={\"help\": \"Random seed to be used with data samplers.\"})\n     jit_mode_eval: bool = field(\n@@ -1120,14 +1078,6 @@ class TrainingArguments:\n     tpu_num_cores: Optional[int] = field(\n         default=None, metadata={\"help\": \"TPU: Number of TPU cores (automatically passed by launcher script)\"}\n     )\n-    tpu_metrics_debug: bool = field(\n-        default=False,\n-        metadata={\n-            \"help\": (\n-                \"Deprecated, the use of `--debug tpu_metrics_debug` is preferred. TPU: Whether to print debug metrics\"\n-            )\n-        },\n-    )\n     debug: Union[str, list[DebugOption]] = field(\n         default=\"\",\n         metadata={\n@@ -1229,15 +1179,6 @@ class TrainingArguments:\n             ),\n         },\n     )\n-    fsdp_min_num_params: int = field(\n-        default=0,\n-        metadata={\n-            \"help\": (\n-                \"This parameter is deprecated. FSDP's minimum number of parameters for Default Auto Wrapping. (useful\"\n-                \" only when `fsdp` field is passed).\"\n-            )\n-        },\n-    )\n     fsdp_config: Optional[Union[dict[str, Any], str]] = field(\n         default=None,\n         metadata={\n@@ -1247,15 +1188,6 @@ class TrainingArguments:\n             )\n         },\n     )\n-    fsdp_transformer_layer_cls_to_wrap: Optional[str] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"This parameter is deprecated. Transformer layer class name (case-sensitive) to wrap, e.g,\"\n-                \" `BertLayer`, `GPTJBlock`, `T5Block` .... (useful only when `fsdp` flag is passed).\"\n-            )\n-        },\n-    )\n     accelerator_config: Optional[Union[dict, str]] = field(\n         default=None,\n         metadata={\n@@ -1293,7 +1225,6 @@ class TrainingArguments:\n         metadata={\"help\": \"The optimizer to use.\"},\n     )\n     optim_args: Optional[str] = field(default=None, metadata={\"help\": \"Optional arguments to supply to optimizer.\"})\n-    adafactor: bool = field(default=False, metadata={\"help\": \"Whether or not to replace AdamW by Adafactor.\"})\n     group_by_length: bool = field(\n         default=False,\n         metadata={\"help\": \"Whether or not to group samples of roughly the same length together when batching.\"},\n@@ -1390,12 +1321,6 @@ class TrainingArguments:\n             \"help\": \"Gradient checkpointing key word arguments such as `use_reentrant`. Will be passed to `torch.utils.checkpoint.checkpoint` through `model.gradient_checkpointing_enable`.\"\n         },\n     )\n-    include_inputs_for_metrics: bool = field(\n-        default=False,\n-        metadata={\n-            \"help\": \"This argument is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `include_for_metrics` instead.\"\n-        },\n-    )\n     include_for_metrics: list[str] = field(\n         default_factory=list,\n         metadata={\n@@ -1409,23 +1334,6 @@ class TrainingArguments:\n             \"help\": \"Whether to recursively concat inputs/losses/labels/predictions across batches. If `False`, will instead store them as lists, with each batch kept separate.\"\n         },\n     )\n-    # Deprecated arguments\n-    fp16_backend: str = field(\n-        default=\"auto\",\n-        metadata={\n-            \"help\": \"Deprecated. Use half_precision_backend instead\",\n-            \"choices\": [\"auto\", \"apex\", \"cpu_amp\"],\n-        },\n-    )\n-    push_to_hub_model_id: Optional[str] = field(\n-        default=None, metadata={\"help\": \"The name of the repository to which push the `Trainer`.\"}\n-    )\n-    push_to_hub_organization: Optional[str] = field(\n-        default=None, metadata={\"help\": \"The name of the organization in with to which push the `Trainer`.\"}\n-    )\n-    push_to_hub_token: Optional[str] = field(\n-        default=None, metadata={\"help\": \"The token to use to push to the Model Hub.\"}\n-    )\n     _n_gpu: int = field(init=False, repr=False, default=-1)\n     mp_parameters: str = field(\n         default=\"\",\n@@ -1450,12 +1358,6 @@ class TrainingArguments:\n             )\n         },\n     )\n-    torchdynamo: Optional[str] = field(\n-        default=None,\n-        metadata={\n-            \"help\": \"This argument is deprecated, use `--torch_compile_backend` instead.\",\n-        },\n-    )\n     ray_scope: Optional[str] = field(\n         default=\"last\",\n         metadata={\n@@ -1608,13 +1510,6 @@ def __post_init__(self):\n             )\n             # Go back to the underlying string or we won't be able to instantiate `IntervalStrategy` on it.\n             self.eval_strategy = self.eval_strategy.value\n-        if self.no_cuda:\n-            warnings.warn(\n-                \"using `no_cuda` is deprecated and will be removed in version 5.0 of ðŸ¤— Transformers. \"\n-                \"Use `use_cpu` instead\",\n-                FutureWarning,\n-            )\n-            self.use_cpu = self.no_cuda\n \n         self.eval_strategy = IntervalStrategy(self.eval_strategy)\n         self.logging_strategy = IntervalStrategy(self.logging_strategy)\n@@ -1631,7 +1526,7 @@ def __post_init__(self):\n                     f\"`torch_empty_cache_steps` must be an integer bigger than 0, got {self.torch_empty_cache_steps}.\"\n                 )\n \n-        # eval_steps has to be defined and non-zero, fallbacks to logging_steps if the latter is non-zero\n+        # eval_steps has to be defined and non-zero, falls back to logging_steps if the latter is non-zero\n         if self.eval_strategy == IntervalStrategy.STEPS and (self.eval_steps is None or self.eval_steps == 0):\n             if self.logging_steps > 0:\n                 logger.info(f\"using `logging_steps` to initialize `eval_steps` to {self.logging_steps}\")\n@@ -1705,16 +1600,8 @@ def __post_init__(self):\n         if self.greater_is_better is None and self.metric_for_best_model is not None:\n             self.greater_is_better = not self.metric_for_best_model.endswith(\"loss\")\n         if is_torch_available():\n-            if self.fp16_backend and self.fp16_backend != \"auto\":\n-                warnings.warn(\n-                    \"`fp16_backend` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use\"\n-                    \" `half_precision_backend` instead\",\n-                    FutureWarning,\n-                )\n-                self.half_precision_backend = self.fp16_backend\n-\n             if self.bf16 or self.bf16_full_eval:\n-                if self.use_cpu and not is_torch_available() and not is_torch_xla_available():\n+                if self.use_cpu and not is_torch_xla_available():\n                     # cpu\n                     raise ValueError(\"Your setup doesn't support bf16/(cpu, tpu, neuroncore). You need torch>=1.10\")\n                 elif not self.use_cpu:\n@@ -1755,13 +1642,6 @@ def __post_init__(self):\n                 raise ValueError(\"lr_scheduler_type reduce_lr_on_plateau requires torch>=0.2.0\")\n \n         self.optim = OptimizerNames(self.optim)\n-        if self.adafactor:\n-            warnings.warn(\n-                \"`--adafactor` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--optim\"\n-                \" adafactor` instead\",\n-                FutureWarning,\n-            )\n-            self.optim = OptimizerNames.ADAFACTOR\n \n         # We need to setup the accelerator config here *before* the first call to `self.device`\n         if is_accelerate_available():\n@@ -1789,13 +1669,6 @@ def __post_init__(self):\n         if is_torch_available():\n             self.device\n \n-        if self.torchdynamo is not None:\n-            warnings.warn(\n-                \"`torchdynamo` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use\"\n-                \" `torch_compile_backend` instead\",\n-                FutureWarning,\n-            )\n-            self.torch_compile_backend = self.torchdynamo\n         if (self.torch_compile_mode is not None or self.torch_compile_backend is not None) and not self.torch_compile:\n             self.torch_compile = True\n         if self.torch_compile and self.torch_compile_backend is None:\n@@ -1922,23 +1795,12 @@ def __post_init__(self):\n                     v = self.fsdp_config.pop(k)\n                     self.fsdp_config[k[5:]] = v\n \n-        if self.fsdp_min_num_params > 0:\n-            warnings.warn(\"using `--fsdp_min_num_params` is deprecated. Use fsdp_config instead \", FutureWarning)\n-\n-        self.fsdp_config[\"min_num_params\"] = max(self.fsdp_config.get(\"min_num_params\", 0), self.fsdp_min_num_params)\n+        self.fsdp_config[\"min_num_params\"] = self.fsdp_config.get(\"min_num_params\", 0)\n \n         # if fsdp_config[\"transformer_layer_cls_to_wrap\"] is specified as a string, convert it to a list with a single object\n         if isinstance(self.fsdp_config.get(\"transformer_layer_cls_to_wrap\", None), str):\n             self.fsdp_config[\"transformer_layer_cls_to_wrap\"] = [self.fsdp_config[\"transformer_layer_cls_to_wrap\"]]\n \n-        if self.fsdp_transformer_layer_cls_to_wrap is not None:\n-            warnings.warn(\n-                \"using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead \", FutureWarning\n-            )\n-            self.fsdp_config[\"transformer_layer_cls_to_wrap\"] = self.fsdp_config.get(\n-                \"transformer_layer_cls_to_wrap\", []\n-            ) + [self.fsdp_transformer_layer_cls_to_wrap]\n-\n         if len(self.fsdp) == 0 and self.fsdp_config[\"min_num_params\"] > 0:\n             warnings.warn(\"`min_num_params` is useful only when `--fsdp` is specified.\")\n \n@@ -2013,18 +1875,6 @@ def __post_init__(self):\n \n             os.environ[f\"{prefix}USE_ORIG_PARAMS\"] = str(self.fsdp_config.get(\"use_orig_params\", \"true\")).lower()\n \n-        if self.tpu_metrics_debug:\n-            warnings.warn(\n-                \"using `--tpu_metrics_debug` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use\"\n-                \" `--debug tpu_metrics_debug` instead\",\n-                FutureWarning,\n-            )\n-            if self.debug is None:\n-                self.debug = \" tpu_metrics_debug\"\n-            else:\n-                self.debug += \" tpu_metrics_debug\"\n-            self.tpu_metrics_debug = False\n-\n         if isinstance(self.debug, str):\n             self.debug = [DebugOption(s) for s in self.debug.split()]\n         elif self.debug is None:\n@@ -2078,41 +1928,6 @@ def __post_init__(self):\n                 \" when --dataloader_num_workers > 1.\"\n             )\n \n-        if self.push_to_hub_token is not None:\n-            warnings.warn(\n-                \"`--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use \"\n-                \"`--hub_token` instead.\",\n-                FutureWarning,\n-            )\n-            self.hub_token = self.push_to_hub_token\n-\n-        if self.push_to_hub_model_id is not None:\n-            self.hub_model_id = get_full_repo_name(\n-                self.push_to_hub_model_id, organization=self.push_to_hub_organization, token=self.hub_token\n-            )\n-            if self.push_to_hub_organization is not None:\n-                warnings.warn(\n-                    \"`--push_to_hub_model_id` and `--push_to_hub_organization` are deprecated and will be removed in \"\n-                    \"version 5 of ðŸ¤— Transformers. Use `--hub_model_id` instead and pass the full repo name to this \"\n-                    f\"argument (in this case {self.hub_model_id}).\",\n-                    FutureWarning,\n-                )\n-            else:\n-                warnings.warn(\n-                    \"`--push_to_hub_model_id` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use \"\n-                    \"`--hub_model_id` instead and pass the full repo name to this argument (in this case \"\n-                    f\"{self.hub_model_id}).\",\n-                    FutureWarning,\n-                )\n-        elif self.push_to_hub_organization is not None:\n-            self.hub_model_id = f\"{self.push_to_hub_organization}/{Path(self.output_dir).name}\"\n-            warnings.warn(\n-                \"`--push_to_hub_organization` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use \"\n-                \"`--hub_model_id` instead and pass the full repo name to this argument (in this case \"\n-                f\"{self.hub_model_id}).\",\n-                FutureWarning,\n-            )\n-\n         if self.eval_use_gather_object and not is_accelerate_available(\"0.30.0\"):\n             raise ValueError(\n                 \"--eval_use_gather_object requires Accelerate to be version of `accelerate` > 0.30.0.\"\n@@ -2126,12 +1941,6 @@ def __post_init__(self):\n                     \"This is not supported and we recommend you to update your version.\"\n                 )\n \n-        if self.include_inputs_for_metrics:\n-            logger.warning(\n-                \"Using `include_inputs_for_metrics` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Please use `include_for_metrics` list argument instead.\"\n-            )\n-            self.include_for_metrics.append(\"inputs\")\n-\n         if self.include_num_input_tokens_seen is True:\n             self.include_num_input_tokens_seen = \"all\"\n         elif self.include_num_input_tokens_seen is False:\n@@ -2140,11 +1949,6 @@ def __post_init__(self):\n     def __str__(self):\n         self_as_dict = asdict(self)\n \n-        # Remove deprecated arguments. That code should be removed once\n-        # those deprecated arguments are removed from TrainingArguments. (TODO: v5)\n-        del self_as_dict[\"per_gpu_train_batch_size\"]\n-        del self_as_dict[\"per_gpu_eval_batch_size\"]\n-\n         self_as_dict = {k: f\"<{k.upper()}>\" if k.endswith(\"_token\") else v for k, v in self_as_dict.items()}\n \n         attrs_as_str = [f\"{k}={v},\\n\" for k, v in sorted(self_as_dict.items())]\n@@ -2155,29 +1959,17 @@ def __str__(self):\n     @property\n     def train_batch_size(self) -> int:\n         \"\"\"\n-        The actual batch size for training (may differ from `per_gpu_train_batch_size` in distributed training).\n+        The actual batch size for training.\n         \"\"\"\n-        if self.per_gpu_train_batch_size:\n-            logger.warning(\n-                \"Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future \"\n-                \"version. Using `--per_device_train_batch_size` is preferred.\"\n-            )\n-        per_device_batch_size = self.per_gpu_train_batch_size or self.per_device_train_batch_size\n-        train_batch_size = per_device_batch_size * max(1, self.n_gpu)\n+        train_batch_size = self.per_device_train_batch_size * max(1, self.n_gpu)\n         return train_batch_size\n \n     @property\n     def eval_batch_size(self) -> int:\n         \"\"\"\n-        The actual batch size for evaluation (may differ from `per_gpu_eval_batch_size` in distributed training).\n+        The actual batch size for evaluation.\n         \"\"\"\n-        if self.per_gpu_eval_batch_size:\n-            logger.warning(\n-                \"Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future \"\n-                \"version. Using `--per_device_eval_batch_size` is preferred.\"\n-            )\n-        per_device_batch_size = self.per_gpu_eval_batch_size or self.per_device_eval_batch_size\n-        eval_batch_size = per_device_batch_size * max(1, self.n_gpu)\n+        eval_batch_size = self.per_device_eval_batch_size * max(1, self.n_gpu)\n         return eval_batch_size\n \n     @property\n@@ -2268,17 +2060,6 @@ def _setup_devices(self) -> \"torch.device\":\n             # Already set _n_gpu\n             pass\n         elif self.distributed_state.distributed_type == DistributedType.NO:\n-            if self.use_mps_device:\n-                warnings.warn(\n-                    \"`use_mps_device` is deprecated and will be removed in version 5.0 of ðŸ¤— Transformers. \"\n-                    \"`mps` device will be used by default if available similar to the way `cuda` device is used.\"\n-                    \"Therefore, no action from user is required. \"\n-                )\n-                if device.type != \"mps\":\n-                    raise ValueError(\n-                        \"Either you do not have an MPS-enabled device on this machine or MacOS version is not 12.3+ \"\n-                        \"or current PyTorch install was not built with MPS enabled.\"\n-                    )\n             if self.use_cpu:\n                 device = torch.device(\"cpu\")\n             elif is_torch_mps_available():"
        },
        {
            "sha": "bf8e74dcff87391b7d4efffd59d4096f88cca9c2",
            "filename": "tests/extended/test_trainer_ext.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/65dcd66cc852f552c6c34d1ef898f891f3ad3f21/tests%2Fextended%2Ftest_trainer_ext.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65dcd66cc852f552c6c34d1ef898f891f3ad3f21/tests%2Fextended%2Ftest_trainer_ext.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fextended%2Ftest_trainer_ext.py?ref=65dcd66cc852f552c6c34d1ef898f891f3ad3f21",
            "patch": "@@ -331,10 +331,7 @@ def run_trainer(\n             args += [\"--predict_with_generate\"]\n \n         if do_train:\n-            if optim == \"adafactor\":\n-                args += [\"--adafactor\"]\n-            else:\n-                args += f\"--optim {optim}\".split()\n+            args += f\"--optim {optim}\".split()\n \n         if extra_args_str is not None:\n             args += extra_args_str.split()"
        },
        {
            "sha": "0be43774a85dab354ad2315ee7e0a03e0da59f03",
            "filename": "tests/test_tokenization_common.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/65dcd66cc852f552c6c34d1ef898f891f3ad3f21/tests%2Ftest_tokenization_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65dcd66cc852f552c6c34d1ef898f891f3ad3f21/tests%2Ftest_tokenization_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_tokenization_common.py?ref=65dcd66cc852f552c6c34d1ef898f891f3ad3f21",
            "patch": "@@ -4462,7 +4462,7 @@ def test_saving_tokenizer_trainer(self):\n \n                     # Load tokenizer from a folder without legacy files\n                     tokenizer = self.rust_tokenizer_class.from_pretrained(tmp_dir)\n-                    training_args = TrainingArguments(output_dir=tmp_dir, do_train=True, no_cuda=True)\n+                    training_args = TrainingArguments(output_dir=tmp_dir, do_train=True, use_cpu=True)\n                     trainer = Trainer(model=model, args=training_args, processing_class=tokenizer)\n \n                     # Should not raise an error"
        }
    ],
    "stats": {
        "total": 246,
        "additions": 12,
        "deletions": 234
    }
}