{
    "author": "manueldeprada",
    "message": "[cache refactor] Move all the caching logic to a per-layer approach (#39106)\n\n* Squash for refactor: Replace monolithic cache classes with modular LayeredCache (#38077)\n\n- Introduces CacheLayer and Cache base classes\n- Ports Static, Dynamic, Offloaded, Quantized, Hybrid, etc. to use layers\n- Implements method/attr dispatch across layers to reduce boilerplate\n- Adds CacheProcessor hooks for offloading, quantization, etc.\n- Updates and passes tests\n\n* fix quantized, add tests\n\n* remove CacheProcessorList\n\n* raushan review, arthur review\n\n* joao review: minor things\n\n* remove cache configs, make CacheLayer a mixin (joaos review)\n\n* back to storage inside Cache()\n\n* remove cachebase for decorator\n\n* no more __getattr__\n\n* fix tests\n\n* joaos review except docs\n\n* fix ast deprecations for python 3.14: replace node.n by node.value and use `ast.Constant`\n\nMore verbose exceptions in `fix_docstring` on docstring formatting issues.\n\n* Revert \"back to storage inside Cache()\"\n\nThis reverts commit 27916bc2737806bf849ce2148cb1e66d59573913.\n\n* cyril review\n\n* simplify cache export\n\n* fix lfm2 cache\n\n* HybridChunked to layer\n\n* BC proxy object for cache.key_cache[i]=...\n\n* reorder classes\n\n* bfff come on LFM2\n\n* better tests for hybrid and hybridChunked\n\n* complete coverage for hybrid chunked caches (prefill chunking)\n\n* reimplementing HybridChunked\n\n* cyril review\n\n* fix ci\n\n* docs for cache refactor\n\n* docs\n\n* oopsie\n\n* oopsie\n\n* fix after merge\n\n* cyril review\n\n* arthur review\n\n* opsie\n\n* fix lfm2\n\n* opsie2",
    "sha": "c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2",
    "files": [
        {
            "sha": "639b3b128cb8ecfcff56d9bc24242ed8363779e1",
            "filename": "docs/source/en/cache_explanation.md",
            "status": "modified",
            "additions": 6,
            "deletions": 10,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/docs%2Fsource%2Fen%2Fcache_explanation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/docs%2Fsource%2Fen%2Fcache_explanation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fcache_explanation.md?ref=c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2",
            "patch": "@@ -82,22 +82,18 @@ When you use Transformers' [`Cache`] class, the self-attention module performs s\n \n ## Cache storage implementation\n \n-The actual storage of key-value pairs varies between cache implementations. As an example, consider the [`DynamicCache`].\n+Caches are structured as a list of layers, where each layer contains a key and value cache. The key and value caches are tensors with the shape `[batch_size, num_heads, seq_len, head_dim]`.\n \n+Layers can be of different types (e.g. `DynamicLayer`, `StaticLayer`, `SlidingWindowLayer`), which mostly changes how sequence length is handled and how the cache is updated.\n \n-In [`DynamicCache`], the key-value pairs are stored as two lists of tensors. Each tensor in the lists have the shape `[batch_size, num_heads, seq_len, head_dim]`.\n-- `key_cache`: A list of tensors, one for each layer.\n-- `value_cache`: A list of tensors, one for each layer.\n+The simplest is a `DynamicLayer` that grows as more tokens are processed. The sequence length dimension (`seq_len`) increases with each new token:\n \n-When new tokens are processed:\n-\n-1. For each layer, the new key and value states are concatenated with the existing cache.\n ```py\n-self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2)\n-self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2)\n+cache.layers[idx].keys = torch.cat([cache.layers[idx].keys, key_states], dim=-2)\n+cache.layers[idx].values = torch.cat([cache.layers[idx].values, value_states], dim=-2)\n ```\n \n-2. The cache grows dynamically as more tokens are processed. The sequence length dimension (`seq_len`) increases with each new token.\n+Other layer types like `StaticLayer` and `SlidingWindowLayer` have a fixed sequence length that is set when the cache is created. This makes them compatible with `torch.compile`. In the case of `SlidingWindowLayer`, existing tokens are shifted out of the cache when a new token is added.\n \n The example below demonstrates how to create a generation loop with [`DynamicCache`]. As discussed, the attention mask is a concatenation of past and current token values and `1` is added to the cache position for the next token.\n "
        },
        {
            "sha": "c64ba2a3ca435e46a48abbcabd0ad2d6ef1e5a1b",
            "filename": "docs/source/en/internal/generation_utils.md",
            "status": "modified",
            "additions": 52,
            "deletions": 25,
            "changes": 77,
            "blob_url": "https://github.com/huggingface/transformers/blob/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/docs%2Fsource%2Fen%2Finternal%2Fgeneration_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/docs%2Fsource%2Fen%2Finternal%2Fgeneration_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Finternal%2Fgeneration_utils.md?ref=c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2",
            "patch": "@@ -356,66 +356,93 @@ A [`Constraint`] can be used to force the generation to include specific tokens\n \n ## Caches\n \n-[[autodoc]] Cache\n+[[autodoc]] CacheLayerMixin\n     - update\n+    - get_seq_length\n+    - get_mask_sizes\n+    - get_max_cache_shape\n+    - reset\n+    - reorder_cache\n \n-[[autodoc]] CacheConfig\n-\t- update\n+[[autodoc]] DynamicLayer\n+    - update\n+    - crop\n+    - batch_repeat_interleave\n+    - batch_select_indices\n \n-[[autodoc]] QuantizedCacheConfig\n-\t- validate\n+[[autodoc]] StaticLayer\n+    - update\n \n-[[autodoc]] DynamicCache\n+[[autodoc]] SlidingWindowLayer\n+    - update\n+\n+[[autodoc]] CacheProcessor\n+    - pre_update\n+    - post_update\n+\n+[[autodoc]] OffloadedCacheProcessor\n+    - pre_update\n+\n+[[autodoc]] QuantizedCacheProcessor\n+    - post_update\n+\n+[[autodoc]] QuantoQuantizedCacheProcessor\n+    - post_update\n+\n+[[autodoc]] HQQQuantizedCacheProcessor\n+    - post_update\n+\n+[[autodoc]] Cache\n     - update\n     - get_seq_length\n+    - get_mask_sizes\n+    - get_max_cache_shape\n+    - reset\n     - reorder_cache\n+    - crop\n+    - batch_repeat_interleave\n+    - batch_select_indices\n+\n+[[autodoc]] DynamicCache\n     - to_legacy_cache\n     - from_legacy_cache\n \n [[autodoc]] QuantizedCache\n-    - update\n-    - get_seq_length\n \n [[autodoc]] QuantoQuantizedCache\n \n+[[autodoc]] QuantoQuantizedCacheProcessor\n+\n [[autodoc]] HQQQuantizedCache\n \n+[[autodoc]] HQQQuantizedCacheProcessor\n+\n [[autodoc]] OffloadedCache\n-    - update\n-    - prefetch_layer\n-    - evict_previous_layer\n \n [[autodoc]] StaticCache\n-    - update\n-    - get_seq_length\n-    - reset\n \n [[autodoc]] OffloadedStaticCache\n-    - update\n-    - get_seq_length\n-    - reset\n \n [[autodoc]] HybridCache\n-    - update\n-    - get_seq_length\n-    - reset\n+\n+[[autodoc]] HybridChunkedCache\n \n [[autodoc]] SlidingWindowCache\n-    - update\n-    - reset\n \n [[autodoc]] EncoderDecoderCache\n-    - get_seq_length\n     - to_legacy_cache\n     - from_legacy_cache\n-    - reset\n-    - reorder_cache\n \n [[autodoc]] MambaCache\n     - update_conv_state\n     - update_ssm_state\n     - reset\n \n+[[autodoc]] CacheConfig\n+\n+[[autodoc]] QuantizedCacheConfig\n+\n+\n ## Watermark Utils\n \n [[autodoc]] WatermarkingConfig"
        },
        {
            "sha": "a1b6dd81ff160a6f48001d9a330a713eb5603ea5",
            "filename": "docs/source/en/kv_cache.md",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/docs%2Fsource%2Fen%2Fkv_cache.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/docs%2Fsource%2Fen%2Fkv_cache.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fkv_cache.md?ref=c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2",
            "patch": "@@ -134,7 +134,7 @@ The [`QuantizedCache`] reduces memory requirements by quantizing the KV values t\n > [!WARNING]\n > Quantizing the cache can harm latency if the context length is short and there is enough GPU memory available for generation without enabling cache quantization. Try to find a balance between memory efficiency and latency.\n \n-Enable [`QuantizedCache`] by configuring `cache_implementation=\"quantized\"` in [`GenerationConfig`], and indicate the quantization backend in [`QuantizedCacheConfig`]. Any additional quantization related parameters should also be passed either as a dict or an instance of [`QuantizedCacheConfig`]. You should use the default values for these additional parameters unless you're running out-of-memory. In that case, consider decreasing the residual length.\n+Enable [`QuantizedCache`] by configuring `cache_implementation=\"quantized\"` in [`GenerationConfig`], and the quantization backend, as well as any additional quantization related parameters should also be passed either as a dict. You should use the default values for these additional parameters unless you're running out-of-memory. In that case, consider decreasing the residual length.\n \n <hfoptions id=\"quantized-cache\">\n <hfoption id=\"HQQQuantizedCache\">\n@@ -143,7 +143,7 @@ For [`HQQQuantizedCache`], we recommend setting the `axis-key` and `axis-value`\n \n ```py\n import torch\n-from transformers import AutoTokenizer, AutoModelForCausalLM, HQQQuantizedCache, QuantizedCacheConfig\n+from transformers import AutoTokenizer, AutoModelForCausalLM, HQQQuantizedCache\n \n tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", torch_dtype=torch.float16, device_map=\"auto\")\n@@ -161,7 +161,7 @@ For [`QuantoQuantizedCache`], we recommend setting the `axis-key` and `axis-valu\n \n ```py\n import torch\n-from transformers import AutoTokenizer, AutoModelForCausalLM, QuantoQuantizedCache, QuantizedCacheConfig\n+from transformers import AutoTokenizer, AutoModelForCausalLM, QuantoQuantizedCache\n \n tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", torch_dtype=torch.float16, device_map=\"auto\")\n@@ -275,7 +275,6 @@ from transformers.cache_utils import (\n     StaticCache,\n     SlidingWindowCache,\n     QuantoQuantizedCache,\n-    QuantizedCacheConfig,\n )\n \n model_id = \"meta-llama/Llama-2-7b-chat-hf\""
        },
        {
            "sha": "1a08a79368d306cd590a697bed03b6f23c10baf9",
            "filename": "docs/source/ko/internal/generation_utils.md",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/docs%2Fsource%2Fko%2Finternal%2Fgeneration_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/docs%2Fsource%2Fko%2Finternal%2Fgeneration_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Finternal%2Fgeneration_utils.md?ref=c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2",
            "patch": "@@ -345,12 +345,6 @@ generation_output[:2]\n [[autodoc]] Cache\n     - update\n \n-[[autodoc]] CacheConfig\n-    - update\n-\n-[[autodoc]] QuantizedCacheConfig\n-    - validate\n-\n [[autodoc]] DynamicCache\n     - update\n     - get_seq_length"
        },
        {
            "sha": "84892590b1afa9fb7dec9624adffa0b5158e90c2",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2",
            "patch": "@@ -365,15 +365,28 @@\n     ]\n     _import_structure[\"activations\"] = []\n     _import_structure[\"cache_utils\"] = [\n+        \"CacheLayerMixin\",\n+        \"DynamicLayer\",\n+        \"StaticLayer\",\n+        \"SlidingWindowLayer\",\n+        \"ChunkedSlidingLayer\",\n+        \"CacheProcessor\",\n+        \"OffloadedCacheProcessor\",\n+        \"QuantizedCacheProcessor\",\n+        \"QuantoQuantizedCacheProcessor\",\n+        \"HQQQuantizedCacheProcessor\",\n         \"Cache\",\n         \"CacheConfig\",\n         \"DynamicCache\",\n         \"EncoderDecoderCache\",\n         \"HQQQuantizedCache\",\n+        \"HQQQuantizedCacheProcessor\",\n         \"HybridCache\",\n+        \"HybridChunkedCache\",\n         \"OffloadedCache\",\n         \"OffloadedStaticCache\",\n         \"QuantizedCache\",\n+        \"QuantoQuantizedCacheProcessor\",\n         \"QuantizedCacheConfig\",\n         \"QuantoQuantizedCache\",\n         \"SinkCache\","
        },
        {
            "sha": "c8471b60e449f61122378a40033f0b370f58189e",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 1714,
            "deletions": 1749,
            "changes": 3463,
            "blob_url": "https://github.com/huggingface/transformers/blob/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2"
        },
        {
            "sha": "7d2cd21effb2c9ce0a5d1a71b17517cabf37cf31",
            "filename": "src/transformers/generation/configuration_utils.py",
            "status": "modified",
            "additions": 13,
            "deletions": 27,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py?ref=c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2",
            "patch": "@@ -44,7 +44,6 @@\n \n logger = logging.get_logger(__name__)\n METADATA_FIELDS = (\"_from_model_config\", \"_commit_hash\", \"_original_object_hash\", \"transformers_version\")\n-CACHE_CONFIG_MAPPING = {}\n NEED_SETUP_CACHE_CLASSES_MAPPING = {}\n QUANT_BACKEND_CLASSES_MAPPING = {}\n ALL_CACHE_IMPLEMENTATIONS = []\n@@ -56,16 +55,12 @@\n         HybridChunkedCache,\n         OffloadedHybridCache,\n         OffloadedStaticCache,\n-        QuantizedCacheConfig,\n         QuantoQuantizedCache,\n         SlidingWindowCache,\n         StaticCache,\n-        StaticCacheConfig,\n     )\n     from .logits_process import SynthIDTextWatermarkLogitsProcessor, WatermarkLogitsProcessor\n \n-    CACHE_CONFIG_MAPPING[\"quantized\"] = QuantizedCacheConfig\n-    CACHE_CONFIG_MAPPING[\"static\"] = StaticCacheConfig\n     NEED_SETUP_CACHE_CLASSES_MAPPING = {\n         \"static\": StaticCache,\n         \"offloaded_static\": OffloadedStaticCache,\n@@ -76,9 +71,7 @@\n         \"offloaded_hybrid_chunked\": OffloadedHybridCache,\n     }\n     QUANT_BACKEND_CLASSES_MAPPING = {\"quanto\": QuantoQuantizedCache, \"HQQ\": HQQQuantizedCache}\n-    ALL_CACHE_IMPLEMENTATIONS = (\n-        list(NEED_SETUP_CACHE_CLASSES_MAPPING.keys()) + list(CACHE_CONFIG_MAPPING.keys()) + [\"offloaded\", \"dynamic\"]\n-    )\n+    ALL_CACHE_IMPLEMENTATIONS = list(NEED_SETUP_CACHE_CLASSES_MAPPING.keys()) + [\"offloaded\", \"dynamic\", \"quantized\"]\n \n \n class GenerationMode(ExplicitEnum):\n@@ -188,10 +181,8 @@ class GenerationConfig(PushToHubMixin):\n \n             If none is specified, we will use the default cache for the model (which is often [`DynamicCache`]). See\n             our [cache documentation](https://huggingface.co/docs/transformers/en/kv_cache) for further information.\n-        cache_config (`CacheConfig` or `dict`, *optional*, default to `None`):\n-            Arguments used in the key-value cache class can be passed in `cache_config`. Can be passed as a `Dict` and\n-            it will be converted to its respective `CacheConfig` internally.\n-            Otherwise can be passed as a `CacheConfig` class matching the indicated `cache_implementation`.\n+        cache_config (`dict`, *optional*, default to `None`):\n+            Arguments used in the key-value cache class can be passed in `cache_config`.\n         return_legacy_cache (`bool`, *optional*, default to `True`):\n             Whether to return the legacy or new format of the cache when `DynamicCache` is used by default.\n \n@@ -406,10 +397,16 @@ def __init__(self, **kwargs):\n         self.use_cache = kwargs.pop(\"use_cache\", True)\n         self.cache_implementation = kwargs.pop(\"cache_implementation\", None)\n         self.cache_config = kwargs.pop(\"cache_config\", None)\n-        if self.cache_implementation is not None and self.cache_implementation in CACHE_CONFIG_MAPPING:\n-            cache_config_class = CACHE_CONFIG_MAPPING[self.cache_implementation]\n-            if isinstance(self.cache_config, dict):\n-                self.cache_config = cache_config_class.from_dict(self.cache_config)\n+        if self.cache_config is not None and not isinstance(self.cache_config, dict):\n+            warnings.warn(\n+                (\n+                    \"Passing a CacheConfig object is deprecated and will be removed in v4.55.0 in favor of a simpler dictionary.\"\n+                ),\n+                FutureWarning,\n+                stacklevel=2,\n+            )\n+            self.cache_config = self.cache_config.to_dict()\n+\n         self.return_legacy_cache = kwargs.pop(\"return_legacy_cache\", None)\n         self.prefill_chunk_size = kwargs.pop(\"prefill_chunk_size\", None)\n \n@@ -611,17 +608,6 @@ def validate(self, strict=False):\n                 f\"Invalid `cache_implementation` ({self.cache_implementation}). Choose one of: \"\n                 f\"{ALL_CACHE_IMPLEMENTATIONS}\"\n             )\n-        if self.cache_config is not None:\n-            cache_class = CACHE_CONFIG_MAPPING.get(self.cache_implementation)\n-            if cache_class is None:\n-                raise ValueError(\n-                    \"You provided a `cache_config` but the cache implementation you are using \"\n-                    f\"({self.cache_implementation}) does not require any config. Make sure to use the \"\n-                    \"correct cache implementation matching your cache config.\"\n-                )\n-            if not isinstance(self.cache_config, cache_class):\n-                self.cache_config = cache_class.from_dict(self.cache_config)\n-            self.cache_config.validate()\n         # 1.3. Performance attributes\n         if self.compile_config is not None and not isinstance(self.compile_config, CompileConfig):\n             raise ValueError("
        },
        {
            "sha": "fe38e0d2b1fd92dbb1b53e235c0afdd9236f4945",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 5,
            "deletions": 111,
            "changes": 116,
            "blob_url": "https://github.com/huggingface/transformers/blob/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2",
            "patch": "@@ -35,7 +35,6 @@\n     HybridChunkedCache,\n     OffloadedCache,\n     OffloadedHybridCache,\n-    QuantizedCacheConfig,\n )\n from ..configuration_utils import PretrainedConfig\n from ..dynamic_module_utils import (\n@@ -2082,22 +2081,22 @@ def _prepare_cache_for_generation(\n                 cache_config = (\n                     generation_config.cache_config\n                     if generation_config.cache_config is not None\n-                    else QuantizedCacheConfig()\n+                    else {\"backend\": \"quanto\"}\n                 )\n-                cache_class = QUANT_BACKEND_CLASSES_MAPPING[cache_config.backend]\n+                cache_class = QUANT_BACKEND_CLASSES_MAPPING[cache_config[\"backend\"]]\n \n-                if cache_config.backend == \"quanto\" and not is_optimum_quanto_available():\n+                if cache_config[\"backend\"] == \"quanto\" and not is_optimum_quanto_available():\n                     raise ImportError(\n                         \"You need to install optimum-quanto in order to use KV cache quantization with optimum-quanto backend. \"\n                         \"Please install it via  with `pip install optimum-quanto`\"\n                     )\n-                elif cache_config.backend == \"HQQ\" and not is_hqq_available():\n+                elif cache_config[\"backend\"] == \"HQQ\" and not is_hqq_available():\n                     raise ImportError(\n                         \"You need to install `HQQ` in order to use KV cache quantization with HQQ backend. \"\n                         \"Please install it via  with `pip install hqq`\"\n                     )\n \n-                model_kwargs[cache_name] = cache_class(cache_config)\n+                model_kwargs[cache_name] = cache_class(**cache_config)\n             elif generation_config.cache_implementation == \"offloaded\":\n                 model_kwargs[cache_name] = OffloadedCache()\n             elif generation_config.cache_implementation == \"dynamic\":\n@@ -5233,106 +5232,6 @@ def _ranking_fast(\n     return selected_idx\n \n \n-def _split(data, full_batch_size: int, split_size: int):\n-    \"\"\"\n-    Takes care of three cases:\n-    1. data is a tensor: e.g. last_hidden_state, pooler_output etc. split them on the batch_size dim\n-    2. data is a tuple: e.g. hidden_states, attentions etc. Keep the tuple as it is and split each tensor in it and\n-       return a list of tuples\n-    3. data is a tuple of tuples, e.g. past_key_values. Keep the tuple as it is and split each tuple in it and\n-       return a list of tuples of tuples\n-    (see documentation of ModelOutput)\n-    \"\"\"\n-    if data is None:\n-        return [None] * (full_batch_size // split_size)\n-    if isinstance(data, torch.Tensor):\n-        return [data[i : i + split_size] for i in range(0, full_batch_size, split_size)]\n-    # New cache format\n-    elif isinstance(data, DynamicCache) or (\n-        isinstance(data, EncoderDecoderCache) and isinstance(data.self_attention_cache, DynamicCache)\n-    ):\n-        return data.batch_split(full_batch_size, split_size)\n-    elif isinstance(data, tuple):\n-        # If the elements of the tuple are also tuples (e.g., past_key_values in our earlier example)\n-        if isinstance(data[0], tuple):\n-            return [\n-                tuple(tuple(tensor[i : i + split_size] for tensor in inner_tuple) for inner_tuple in data)\n-                for i in range(0, full_batch_size, split_size)\n-            ]\n-\n-        else:\n-            return [\n-                tuple(sub_tensor[i : i + split_size] for sub_tensor in data)\n-                for i in range(0, full_batch_size, split_size)\n-            ]\n-    else:\n-        raise TypeError(f\"Unexpected attribute type: {type(data)}\")\n-\n-\n-def _split_model_inputs(\n-    model_input: Union[ModelOutput, dict], split_size: int, full_batch_size: int, config: PretrainedConfig\n-) -> list[Union[ModelOutput, dict]]:\n-    \"\"\"\n-    Split a ModelOutput object (or its subclasses) or Dict into a list of same-class objects based on a specified split\n-    size. The input object is dict when it was prepared for forward pass and ModelOutput when it was returned from\n-    previous forward pass.\n-    \"\"\"\n-    # Edge case: if model_input is None, return a list of Nones\n-    # this happens with Whisper where encoder_outputs is None\n-    if model_input is None:\n-        return [model_input] * (full_batch_size // split_size)\n-    # Infer the class from the object\n-    model_output_cls = type(model_input)\n-    if (full_batch_size % split_size) != 0:\n-        raise ValueError(\"`full_batch_size` must be divisible by `split_size`\")\n-\n-    if split_size > full_batch_size:\n-        raise ValueError(\"`split_size` must be smaller or equal to `full_batch_size`\")\n-\n-    # Helper function to split tensors or tuples of tensors\n-\n-    # Find all the dataclass fields (e.g., last_hidden_state, pooler_output etc.) and split them\n-    keys = (\n-        model_input.__dataclass_fields__.keys() if hasattr(model_input, \"__dataclass_fields__\") else model_input.keys()\n-    )\n-    # We only keep keys that are in the model_input\n-    keys = [k for k in keys if k in model_input]\n-    # Here we can have four types of values: tensors, tuples of tensors and booleans, and encoder_outputs which is a\n-    # ModelOutput object.\n-    # bool should not be split but replicated for each split\n-    bool_keys = [k for k in keys if isinstance(model_input[k], bool) or k == \"cache_position\"]\n-    keys_to_ignore = [\"cache_position\", \"encoder_outputs\", \"logits_to_keep\"]\n-    non_bool_keys = [k for k in keys if not isinstance(model_input[k], bool) and k not in keys_to_ignore]\n-\n-    # we split the tensors and tuples of tensors\n-    data_split_list = [\n-        {k: _split(model_input[k], full_batch_size, split_size)[i] for k in non_bool_keys}\n-        for i in range(full_batch_size // split_size)\n-    ]\n-    # bool values are the same and replicated for each split\n-    bool_data = {k: model_input[k] for k in bool_keys}\n-    # encoder_outputs is a ModelOutput object and should be split by its own\n-    if \"encoder_outputs\" in model_input:\n-        encoder_outputs_split = _split_model_inputs(\n-            model_input[\"encoder_outputs\"], split_size, full_batch_size, config.get_text_config()\n-        )\n-        data_split_list = [\n-            {**data_split, \"encoder_outputs\": encoder_outputs_split[i]} for i, data_split in enumerate(data_split_list)\n-        ]\n-    # logits_to_keep should be replicated for each split, similar to bool values\n-    if \"logits_to_keep\" in model_input:\n-        data_split_list = [\n-            {**data_split, \"logits_to_keep\": model_input[\"logits_to_keep\"]} for data_split in data_split_list\n-        ]\n-\n-    # Convert each dictionary in the list to an object of the inferred class\n-    split_model_inputs: list[Union[ModelOutput, dict]] = [\n-        model_output_cls(**data_split, **bool_data) for data_split in data_split_list\n-    ]\n-\n-    return split_model_inputs\n-\n-\n def stack_model_outputs(model_outputs: list[ModelOutput], config: PretrainedConfig) -> ModelOutput:\n     \"\"\"\n     Stack a list of ModelOutput objects (or its subclasses) along the batch_size dimension. The function infers the\n@@ -5357,11 +5256,6 @@ def _concat(data):\n             return None\n         if isinstance(data[0], torch.Tensor):\n             return torch.cat(data, dim=0)\n-        # New cache format\n-        elif isinstance(data[0], DynamicCache):\n-            return DynamicCache.from_batch_splits(data)\n-        elif isinstance(data[0], EncoderDecoderCache):\n-            return EncoderDecoderCache.from_batch_splits(data)\n         elif isinstance(data[0], tuple):\n             # If the elements of the tuple are also tuples (e.g., past_key_values in our earlier example)\n             if isinstance(data[0][0], tuple):"
        },
        {
            "sha": "6fa0e6348d6638d6f9921efd157db329ac0b950a",
            "filename": "src/transformers/integrations/executorch.py",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fintegrations%2Fexecutorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fintegrations%2Fexecutorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fexecutorch.py?ref=c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2",
            "patch": "@@ -290,14 +290,14 @@ def __init__(self, model: PreTrainedModel):\n         self.model = model\n         self.static_cache = StaticCache(\n             config=self.model.config,\n-            max_batch_size=self.model.generation_config.cache_config.batch_size,\n-            max_cache_len=self.model.generation_config.cache_config.max_cache_len,\n-            device=self.model.generation_config.cache_config.device,\n+            max_batch_size=self.model.generation_config.cache_config.get(\"batch_size\"),\n+            max_cache_len=self.model.generation_config.cache_config.get(\"max_cache_len\"),\n+            device=self.model.generation_config.cache_config.get(\"device\"),\n             dtype=self.model.dtype,\n         )\n-        for i in range(len(self.static_cache.key_cache)):\n-            self.register_buffer(f\"key_cache_{i}\", self.static_cache.key_cache[i], persistent=False)\n-            self.register_buffer(f\"value_cache_{i}\", self.static_cache.value_cache[i], persistent=False)\n+        for i in range(len(self.static_cache)):\n+            self.register_buffer(f\"key_cache_{i}\", self.static_cache.layers[i].keys, persistent=False)\n+            self.register_buffer(f\"value_cache_{i}\", self.static_cache.layers[i].values, persistent=False)\n \n     def forward(self, input_ids: torch.Tensor, cache_position: torch.Tensor):\n         \"\"\"\n@@ -429,9 +429,9 @@ def __init__(\n         )\n \n         # Register all key and value cache tensors as buffers\n-        for i in range(len(self.cache.key_cache)):\n-            self.register_buffer(f\"key_cache_{i}\", self.cache.key_cache[i], persistent=False)\n-            self.register_buffer(f\"value_cache_{i}\", self.cache.value_cache[i], persistent=False)\n+        for i in range(len(self.cache)):\n+            self.register_buffer(f\"key_cache_{i}\", self.cache.layers[i].keys, persistent=False)\n+            self.register_buffer(f\"value_cache_{i}\", self.cache.layers[i].values, persistent=False)\n \n     def forward(\n         self,\n@@ -580,9 +580,9 @@ def __init__(self, model, max_static_cache_length, batch_size):\n         self.cache = EncoderDecoderCache(self.static_cache, DynamicCache())\n \n         # Register cache buffers to make them exportable\n-        for i in range(len(self.static_cache.key_cache)):\n-            self.register_buffer(f\"key_cache_{i}\", self.static_cache.key_cache[i], persistent=False)\n-            self.register_buffer(f\"value_cache_{i}\", self.static_cache.value_cache[i], persistent=False)\n+        for i in range(len(self.static_cache)):\n+            self.register_buffer(f\"key_cache_{i}\", self.static_cache.layers[i].keys, persistent=False)\n+            self.register_buffer(f\"value_cache_{i}\", self.static_cache.layers[i].values, persistent=False)\n \n     def forward(self, decoder_input_ids, encoder_hidden_states, cache_position):\n         # Get outputs from decoder"
        },
        {
            "sha": "a228e35587e5ab01689e13bc3c8aed7f92909afd",
            "filename": "src/transformers/models/bamba/modeling_bamba.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py?ref=c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2",
            "patch": "@@ -85,7 +85,7 @@ class BambaFlashAttentionKwargs(TypedDict, total=False):\n     seq_idx: torch.IntTensor\n \n \n-class HybridMambaAttentionDynamicCache(DynamicCache):\n+class HybridMambaAttentionDynamicCache(Cache):\n     \"\"\"\n     A dynamic cache that can handle both the attention cache (which has a seq_len dimension) and the mamba cache\n     (which has a constant shape regardless of seq_len).\n@@ -99,6 +99,10 @@ class HybridMambaAttentionDynamicCache(DynamicCache):\n     and `ssm_states` represents the ssm state and has a shape of `(batch_size, d_inner, d_state)`.\n     \"\"\"\n \n+    key_cache = None\n+    value_cache = None\n+    is_compileable = False\n+\n     def __init__(self, config: BambaConfig, batch_size, dtype=torch.float16, device=None):\n         super().__init__()\n         self.layers_block_type = config.layers_block_type"
        },
        {
            "sha": "02877c3d89cf92d3525436afd91ffd1e13905dbe",
            "filename": "src/transformers/models/bart/modeling_bart.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py?ref=c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2",
            "patch": "@@ -229,8 +229,8 @@ def forward(\n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.key_cache[self.layer_idx]\n-            value_states = curr_past_key_value.value_cache[self.layer_idx]\n+            key_states = curr_past_key_value.layers[self.layer_idx].keys\n+            value_states = curr_past_key_value.layers[self.layer_idx].values\n         else:\n             key_states = self.k_proj(current_states)\n             value_states = self.v_proj(current_states)"
        },
        {
            "sha": "ac3636a048a0195fbbdce6a69db3c576fb223a93",
            "filename": "src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py?ref=c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2",
            "patch": "@@ -1287,8 +1287,8 @@ def forward(\n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.key_cache[self.layer_idx]\n-            value_states = curr_past_key_value.value_cache[self.layer_idx]\n+            key_states = curr_past_key_value.layers[self.layer_idx].keys\n+            value_states = curr_past_key_value.layers[self.layer_idx].values\n         else:\n             key_states = self.k_proj(current_states)\n             value_states = self.v_proj(current_states)"
        },
        {
            "sha": "8bf976caf512f4619be3be7cc72593479c9d764f",
            "filename": "src/transformers/models/biogpt/modeling_biogpt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py?ref=c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2",
            "patch": "@@ -207,8 +207,8 @@ def forward(\n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.key_cache[self.layer_idx]\n-            value_states = curr_past_key_value.value_cache[self.layer_idx]\n+            key_states = curr_past_key_value.layers[self.layer_idx].keys\n+            value_states = curr_past_key_value.layers[self.layer_idx].values\n         else:\n             key_states = self.k_proj(current_states)\n             value_states = self.v_proj(current_states)"
        },
        {
            "sha": "445ef06b0bea557ef67a2e60760e6a424aabd7ff",
            "filename": "src/transformers/models/blenderbot/modeling_blenderbot.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py?ref=c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2",
            "patch": "@@ -228,8 +228,8 @@ def forward(\n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.key_cache[self.layer_idx]\n-            value_states = curr_past_key_value.value_cache[self.layer_idx]\n+            key_states = curr_past_key_value.layers[self.layer_idx].keys\n+            value_states = curr_past_key_value.layers[self.layer_idx].values\n         else:\n             key_states = self.k_proj(current_states)\n             value_states = self.v_proj(current_states)"
        },
        {
            "sha": "dc7eac4390c537b3b19040b6a834f71859e64ca5",
            "filename": "src/transformers/models/blenderbot_small/modeling_blenderbot_small.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py?ref=c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2",
            "patch": "@@ -212,8 +212,8 @@ def forward(\n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.key_cache[self.layer_idx]\n-            value_states = curr_past_key_value.value_cache[self.layer_idx]\n+            key_states = curr_past_key_value.layers[self.layer_idx].keys\n+            value_states = curr_past_key_value.layers[self.layer_idx].values\n         else:\n             key_states = self.k_proj(current_states)\n             value_states = self.v_proj(current_states)"
        },
        {
            "sha": "927e808e9b483f84165bdbd23ae578f02741ae96",
            "filename": "src/transformers/models/dia/modeling_dia.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py?ref=c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2",
            "patch": "@@ -349,8 +349,8 @@ def forward(\n         is_updated = past_key_values.is_updated.get(self.layer_idx) if past_key_values is not None else False\n         if past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = past_key_values.cross_attention_cache.key_cache[self.layer_idx]\n-            value_states = past_key_values.cross_attention_cache.value_cache[self.layer_idx]\n+            key_states = past_key_values.cross_attention_cache.layers[self.layer_idx].keys\n+            value_states = past_key_values.cross_attention_cache.layers[self.layer_idx].values\n         else:\n             key_states = self.k_proj(cross_attention_states).view(cross_shape).transpose(1, 2)\n             value_states = self.v_proj(cross_attention_states).view(cross_shape).transpose(1, 2)"
        },
        {
            "sha": "1c67a491ca21ae4b448728a147ac19813a9e0500",
            "filename": "src/transformers/models/dia/modular_dia.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fdia%2Fmodular_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fdia%2Fmodular_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fmodular_dia.py?ref=c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2",
            "patch": "@@ -168,8 +168,8 @@ def forward(\n         is_updated = past_key_values.is_updated.get(self.layer_idx) if past_key_values is not None else False\n         if past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = past_key_values.cross_attention_cache.key_cache[self.layer_idx]\n-            value_states = past_key_values.cross_attention_cache.value_cache[self.layer_idx]\n+            key_states = past_key_values.cross_attention_cache.layers[self.layer_idx].keys\n+            value_states = past_key_values.cross_attention_cache.layers[self.layer_idx].values\n         else:\n             key_states = self.k_proj(cross_attention_states).view(cross_shape).transpose(1, 2)\n             value_states = self.v_proj(cross_attention_states).view(cross_shape).transpose(1, 2)"
        },
        {
            "sha": "8b099342f6ee2bc7da03c9ea219f7998c8ec7e67",
            "filename": "src/transformers/models/falcon_h1/modeling_falcon_h1.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py?ref=c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2",
            "patch": "@@ -62,7 +62,7 @@\n logger = logging.get_logger(__name__)\n \n \n-class FalconHybridMambaAttentionDynamicCache(DynamicCache):\n+class FalconHybridMambaAttentionDynamicCache(Cache):\n     \"\"\"\n     A dynamic cache that can handle both the attention cache (which has a seq_len dimension) and the mamba cache\n     (which has a constant shape regardless of seq_len).\n@@ -76,6 +76,10 @@ class FalconHybridMambaAttentionDynamicCache(DynamicCache):\n     and `ssm_states` represents the ssm state and has a shape of `(batch_size, d_inner, d_state)`.\n     \"\"\"\n \n+    key_cache = None\n+    value_cache = None\n+    is_compileable = False\n+\n     def __init__(\n         self,\n         config: FalconH1Config,"
        },
        {
            "sha": "edd671729109ade701b60ecc1b488e9a76125310",
            "filename": "src/transformers/models/gemma3n/modeling_gemma3n.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py?ref=c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2",
            "patch": "@@ -1329,7 +1329,7 @@ def forward(\n \n         if self.is_kv_shared_layer and self.kv_shared_layer_index is not None and past_key_value is not None:\n             # Device of past layer may be different from current one\n-            indices = cache_position.to(past_key_value.key_cache[self.kv_shared_layer_index].device)\n+            indices = cache_position.to(past_key_value.layers[self.kv_shared_layer_index].keys.device)\n             # In this case we need special handling of the slice as the layer is of fixed small size (for full layers, we never go beyond)\n             if isinstance(past_key_value, HybridCache) and self.is_sliding:\n                 max_length = past_key_value.sliding_window\n@@ -1340,9 +1340,9 @@ def forward(\n                 )\n \n             # Device of past layer may be different from current one\n-            key_states = past_key_value.key_cache[self.kv_shared_layer_index][:, :, indices].to(query_states.device)\n-            value_states = past_key_value.value_cache[self.kv_shared_layer_index][:, :, indices].to(\n-                query_states.device\n+            key_states = past_key_value.layers[self.kv_shared_layer_index].keys[:, :, indices].to(query_states.device)\n+            value_states = (\n+                past_key_value.layers[self.kv_shared_layer_index].values[:, :, indices].to(query_states.device)\n             )\n         else:\n             key_states = self.k_proj(hidden_states).view(hidden_shape)"
        },
        {
            "sha": "015fa9e43f47692aa46b5ce179dfb49c0c49ca5e",
            "filename": "src/transformers/models/gemma3n/modular_gemma3n.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py?ref=c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2",
            "patch": "@@ -1770,7 +1770,7 @@ def forward(\n \n         if self.is_kv_shared_layer and self.kv_shared_layer_index is not None and past_key_value is not None:\n             # Device of past layer may be different from current one\n-            indices = cache_position.to(past_key_value.key_cache[self.kv_shared_layer_index].device)\n+            indices = cache_position.to(past_key_value.layers[self.kv_shared_layer_index].keys.device)\n             # In this case we need special handling of the slice as the layer is of fixed small size (for full layers, we never go beyond)\n             if isinstance(past_key_value, HybridCache) and self.is_sliding:\n                 max_length = past_key_value.sliding_window\n@@ -1781,9 +1781,9 @@ def forward(\n                 )\n \n             # Device of past layer may be different from current one\n-            key_states = past_key_value.key_cache[self.kv_shared_layer_index][:, :, indices].to(query_states.device)\n-            value_states = past_key_value.value_cache[self.kv_shared_layer_index][:, :, indices].to(\n-                query_states.device\n+            key_states = past_key_value.layers[self.kv_shared_layer_index].keys[:, :, indices].to(query_states.device)\n+            value_states = (\n+                past_key_value.layers[self.kv_shared_layer_index].values[:, :, indices].to(query_states.device)\n             )\n         else:\n             key_states = self.k_proj(hidden_states).view(hidden_shape)"
        },
        {
            "sha": "9f8c08f96f81d7d370401b5b1abc59a5d93a0dcd",
            "filename": "src/transformers/models/gptj/modeling_gptj.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py?ref=c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2",
            "patch": "@@ -701,8 +701,9 @@ def forward(\n \n                 # Ensure layer_past is on same device as hidden_states (might not be correct)\n                 if past_key_values is not None:\n-                    past_key_values.key_cache = past_key_values.key_cache.to(hidden_states.device)\n-                    past_key_values.value_cache = past_key_values.value_cache.to(hidden_states.device)\n+                    for layer in past_key_values.layers:\n+                        layer.keys = layer.keys.to(hidden_states.device)\n+                        layer.values = layer.values.to(hidden_states.device)\n \n                 # Ensure that attention_mask is always on the same device as hidden_states\n                 if causal_mask is not None:"
        },
        {
            "sha": "c42300843d749a6796d62f6cad143658c8b3c551",
            "filename": "src/transformers/models/granitemoehybrid/modeling_granitemoehybrid.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py?ref=c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2",
            "patch": "@@ -221,7 +221,7 @@ def forward(\n         return attn_output, attn_weights\n \n \n-class HybridMambaAttentionDynamicCache(DynamicCache):\n+class HybridMambaAttentionDynamicCache(Cache):\n     \"\"\"\n     A dynamic cache that can handle both the attention cache (which has a seq_len dimension) and the mamba cache\n     (which has a constant shape regardless of seq_len).\n@@ -235,6 +235,10 @@ class HybridMambaAttentionDynamicCache(DynamicCache):\n     and `ssm_states` represents the ssm state and has a shape of `(batch_size, d_inner, d_state)`.\n     \"\"\"\n \n+    key_cache = None\n+    value_cache = None\n+    is_compileable = False\n+\n     def __init__(self, config: GraniteMoeHybridConfig, batch_size, dtype=torch.float16, device=None):\n         super().__init__()\n         self.layers_block_type = config.layers_block_type"
        },
        {
            "sha": "223df2e81960f107f49544910ad0d18b65982a89",
            "filename": "src/transformers/models/informer/modeling_informer.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py?ref=c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2",
            "patch": "@@ -476,8 +476,8 @@ def forward(\n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.key_cache[self.layer_idx]\n-            value_states = curr_past_key_value.value_cache[self.layer_idx]\n+            key_states = curr_past_key_value.layers[self.layer_idx].keys\n+            value_states = curr_past_key_value.layers[self.layer_idx].values\n         else:\n             key_states = self.k_proj(current_states)\n             value_states = self.v_proj(current_states)\n@@ -593,8 +593,8 @@ def forward(\n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.key_cache[self.layer_idx]\n-            value_states = curr_past_key_value.value_cache[self.layer_idx]\n+            key_states = curr_past_key_value.layers[self.layer_idx].keys\n+            value_states = curr_past_key_value.layers[self.layer_idx].values\n         else:\n             key_states = self.k_proj(current_states)\n             value_states = self.v_proj(current_states)"
        },
        {
            "sha": "f1f73f505f44832f177e1a9f013a62bb7704308d",
            "filename": "src/transformers/models/informer/modular_informer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Finformer%2Fmodular_informer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Finformer%2Fmodular_informer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finformer%2Fmodular_informer.py?ref=c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2",
            "patch": "@@ -282,8 +282,8 @@ def forward(\n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.key_cache[self.layer_idx]\n-            value_states = curr_past_key_value.value_cache[self.layer_idx]\n+            key_states = curr_past_key_value.layers[self.layer_idx].keys\n+            value_states = curr_past_key_value.layers[self.layer_idx].values\n         else:\n             key_states = self.k_proj(current_states)\n             value_states = self.v_proj(current_states)"
        },
        {
            "sha": "8f259bb0c017df10ddb833cea8f856b7c0643b5d",
            "filename": "src/transformers/models/jamba/modeling_jamba.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py?ref=c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2",
            "patch": "@@ -183,7 +183,7 @@ def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n \n \n-class HybridMambaAttentionDynamicCache(DynamicCache):\n+class HybridMambaAttentionDynamicCache(Cache):\n     \"\"\"\n     A dynamic cache that can handle both the attention cache (which has a seq_len dimension) and the mamba cache\n     (which has a constant shape regardless of seq_len).\n@@ -197,6 +197,10 @@ class HybridMambaAttentionDynamicCache(DynamicCache):\n     and `ssm_states` represents the ssm state and has a shape of `(batch_size, d_inner, d_state)`.\n     \"\"\"\n \n+    key_cache = None\n+    value_cache = None\n+    is_compileable = False\n+\n     def __init__(self, config, batch_size, dtype=torch.float16, device=None):\n         super().__init__()\n         self.dtype = dtype"
        },
        {
            "sha": "ae2d2b051831ce512f2ea4ac1e15356478a0808e",
            "filename": "src/transformers/models/lfm2/modeling_lfm2.py",
            "status": "modified",
            "additions": 37,
            "deletions": 1,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py?ref=c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2",
            "patch": "@@ -128,14 +128,21 @@ class Lfm2HybridConvCache(DynamicCache):\n     Conv layer cache shape: `[batch_size, hidden_size, L_cache-1]`.\n     \"\"\"\n \n+    # Override @property existing in Cache\n+    max_batch_size = None\n+    is_compileable = False\n+    key_cache = None\n+    value_cache = None\n+\n     def __init__(\n         self,\n         config: Lfm2Config,\n         max_batch_size: int,\n         dtype: torch.dtype = torch.float32,\n         device: Union[torch.device, str, None] = None,\n     ):\n-        super().__init__()  # initialize key and value cache\n+        self.key_cache = []\n+        self.value_cache = []\n         self.max_batch_size = max_batch_size\n         self.layer_types = config.layer_types\n         self.first_attention_layer = self.layer_types.index(\"full_attention\")\n@@ -218,6 +225,35 @@ def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n             return 0\n         return self.key_cache[layer_idx].shape[-2]\n \n+    def get_mask_sizes(self, cache_position: torch.Tensor, layer_idx: int) -> tuple[int, int]:\n+        \"\"\"\n+        Return a tuple (kv_length, kv_offset) corresponding to the length and offset that will be returned for\n+        the given layer at `layer_idx`.\n+        The masks are then prepared according to the given lengths (kv_length, kv_offset) and patterns (i.e. sliding_window, chunk_size),\n+        for each layer.\n+        \"\"\"\n+        full_mask_kv_offset = 0\n+        query_length = cache_position.shape[0]\n+        past_seen_tokens = self.get_seq_length()\n+        kv_length = query_length + past_seen_tokens\n+        return kv_length, full_mask_kv_offset\n+\n+    def crop(self, max_length: int):\n+        \"\"\"Crop the cache to the given length\"\"\"\n+        if max_length < 0:\n+            max_length = self.get_seq_length() - abs(max_length)\n+\n+        if self.get_seq_length() <= max_length:\n+            return\n+\n+        for idx in range(len(self.key_cache)):\n+            if self.key_cache[idx].numel():\n+                self.key_cache[idx] = self.key_cache[idx][..., :max_length, :]\n+                self.value_cache[idx] = self.value_cache[idx][..., :max_length, :]\n+\n+    def __getitem__(self, layer_idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n+        return self.key_cache[layer_idx], self.value_cache[layer_idx]\n+\n     def to_legacy_cache(self) -> tuple[tuple[torch.Tensor], tuple[torch.Tensor]]:\n         raise NotImplementedError(\"Lfm2HybridConvCache does not have a legacy cache equivalent.\")\n "
        },
        {
            "sha": "9ffeb95513cbda73264896cbfb3f42dd379c4d2f",
            "filename": "src/transformers/models/lfm2/modular_lfm2.py",
            "status": "modified",
            "additions": 37,
            "deletions": 1,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodular_lfm2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodular_lfm2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodular_lfm2.py?ref=c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2",
            "patch": "@@ -89,14 +89,21 @@ class Lfm2HybridConvCache(DynamicCache):\n     Conv layer cache shape: `[batch_size, hidden_size, L_cache-1]`.\n     \"\"\"\n \n+    # Override @property existing in Cache\n+    max_batch_size = None\n+    is_compileable = False\n+    key_cache = None\n+    value_cache = None\n+\n     def __init__(\n         self,\n         config: Lfm2Config,\n         max_batch_size: int,\n         dtype: torch.dtype = torch.float32,\n         device: Union[torch.device, str, None] = None,\n     ):\n-        super().__init__()  # initialize key and value cache\n+        self.key_cache = []\n+        self.value_cache = []\n         self.max_batch_size = max_batch_size\n         self.layer_types = config.layer_types\n         self.first_attention_layer = self.layer_types.index(\"full_attention\")\n@@ -179,6 +186,35 @@ def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n             return 0\n         return self.key_cache[layer_idx].shape[-2]\n \n+    def get_mask_sizes(self, cache_position: torch.Tensor, layer_idx: int) -> tuple[int, int]:\n+        \"\"\"\n+        Return a tuple (kv_length, kv_offset) corresponding to the length and offset that will be returned for\n+        the given layer at `layer_idx`.\n+        The masks are then prepared according to the given lengths (kv_length, kv_offset) and patterns (i.e. sliding_window, chunk_size),\n+        for each layer.\n+        \"\"\"\n+        full_mask_kv_offset = 0\n+        query_length = cache_position.shape[0]\n+        past_seen_tokens = self.get_seq_length()\n+        kv_length = query_length + past_seen_tokens\n+        return kv_length, full_mask_kv_offset\n+\n+    def crop(self, max_length: int):\n+        \"\"\"Crop the cache to the given length\"\"\"\n+        if max_length < 0:\n+            max_length = self.get_seq_length() - abs(max_length)\n+\n+        if self.get_seq_length() <= max_length:\n+            return\n+\n+        for idx in range(len(self.key_cache)):\n+            if self.key_cache[idx].numel():\n+                self.key_cache[idx] = self.key_cache[idx][..., :max_length, :]\n+                self.value_cache[idx] = self.value_cache[idx][..., :max_length, :]\n+\n+    def __getitem__(self, layer_idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n+        return self.key_cache[layer_idx], self.value_cache[layer_idx]\n+\n     def to_legacy_cache(self) -> tuple[tuple[torch.Tensor], tuple[torch.Tensor]]:\n         raise NotImplementedError(\"Lfm2HybridConvCache does not have a legacy cache equivalent.\")\n "
        },
        {
            "sha": "71e5178ef573a279680fb02ae4fe6c422bd2e740",
            "filename": "src/transformers/models/longt5/modeling_longt5.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py?ref=c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2",
            "patch": "@@ -481,8 +481,8 @@ def forward(\n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.key_cache[self.layer_idx]\n-            value_states = curr_past_key_value.value_cache[self.layer_idx]\n+            key_states = curr_past_key_value.layers[self.layer_idx].keys\n+            value_states = curr_past_key_value.layers[self.layer_idx].values\n         else:\n             key_states = self.k(current_states)\n             value_states = self.v(current_states)"
        },
        {
            "sha": "59641c60846bf2788ed757c7241b3aa47676ca6b",
            "filename": "src/transformers/models/m2m_100/modeling_m2m_100.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py?ref=c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2",
            "patch": "@@ -294,8 +294,8 @@ def forward(\n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.key_cache[self.layer_idx]\n-            value_states = curr_past_key_value.value_cache[self.layer_idx]\n+            key_states = curr_past_key_value.layers[self.layer_idx].keys\n+            value_states = curr_past_key_value.layers[self.layer_idx].values\n         else:\n             key_states = self.k_proj(current_states)\n             value_states = self.v_proj(current_states)"
        },
        {
            "sha": "da09d6954c2ad66492046458bdb7e216206bbd3a",
            "filename": "src/transformers/models/mamba/modeling_mamba.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py?ref=c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2",
            "patch": "@@ -39,10 +39,6 @@\n \n logger = logging.get_logger(__name__)\n \n-if is_mambapy_available():\n-    from mambapy.pscan import pscan\n-else:\n-    pscan = None\n \n if is_mamba_ssm_available():\n     from mamba_ssm.ops.selective_scan_interface import mamba_inner_fn, selective_scan_fn\n@@ -334,6 +330,10 @@ def cuda_kernels_forward(\n \n     # fmt: off\n     def slow_forward(self, input_states, cache_params: Optional[MambaCache]=None, cache_position:Optional[torch.LongTensor]=None, attention_mask: Optional[torch.LongTensor] = None):\n+        if is_mambapy_available():\n+            from mambapy.pscan import pscan\n+        else:\n+            pscan = None\n         batch_size, seq_len, _ = input_states.shape\n         dtype = input_states.dtype\n         # 1. Gated MLP's linear projection"
        },
        {
            "sha": "8a3e7965f5067db8e38e0b06d90213a435d28815",
            "filename": "src/transformers/models/marian/modeling_marian.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py?ref=c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2",
            "patch": "@@ -229,8 +229,8 @@ def forward(\n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.key_cache[self.layer_idx]\n-            value_states = curr_past_key_value.value_cache[self.layer_idx]\n+            key_states = curr_past_key_value.layers[self.layer_idx].keys\n+            value_states = curr_past_key_value.layers[self.layer_idx].values\n         else:\n             key_states = self.k_proj(current_states)\n             value_states = self.v_proj(current_states)"
        },
        {
            "sha": "5aafbc1de6e8602d6dcaab03b8fb39575bb87a4e",
            "filename": "src/transformers/models/mbart/modeling_mbart.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py?ref=c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2",
            "patch": "@@ -238,8 +238,8 @@ def forward(\n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.key_cache[self.layer_idx]\n-            value_states = curr_past_key_value.value_cache[self.layer_idx]\n+            key_states = curr_past_key_value.layers[self.layer_idx].keys\n+            value_states = curr_past_key_value.layers[self.layer_idx].values\n         else:\n             key_states = self.k_proj(current_states)\n             value_states = self.v_proj(current_states)"
        },
        {
            "sha": "e9bb3a1c091495e15f09189018b368362be46227",
            "filename": "src/transformers/models/minimax/modeling_minimax.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py?ref=c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2",
            "patch": "@@ -108,16 +108,14 @@ def batch_repeat_interleave(self, repeats: int):\n             if self.linear_cache[layer_idx] != []:\n                 self.linear_cache[layer_idx] = self.linear_cache[layer_idx].repeat_interleave(repeats, dim=0)\n             else:\n-                self.key_cache[layer_idx] = self.key_cache[layer_idx].repeat_interleave(repeats, dim=0)\n-                self.value_cache[layer_idx] = self.value_cache[layer_idx].repeat_interleave(repeats, dim=0)\n+                self.layers[layer_idx].batch_repeat_interleave(repeats)\n \n     def batch_select_indices(self, indices: torch.Tensor):\n         for layer_idx in range(len(self)):\n             if self.linear_cache[layer_idx] != []:\n                 self.linear_cache[layer_idx] = self.linear_cache[layer_idx][indices, ...]\n             else:\n-                self.key_cache[layer_idx] = self.key_cache[layer_idx][indices, ...]\n-                self.value_cache[layer_idx] = self.value_cache[layer_idx][indices, ...]\n+                self.layers[layer_idx].batch_select_indices(indices)\n \n     def crop(self, max_length: int):\n         raise RuntimeError(\"MiniMaxCache doesnot support `crop` method\")"
        },
        {
            "sha": "6844a9d0fc6317b4b30f259be4ba4ce5aa8296c4",
            "filename": "src/transformers/models/minimax/modular_minimax.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodular_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodular_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodular_minimax.py?ref=c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2",
            "patch": "@@ -217,16 +217,14 @@ def batch_repeat_interleave(self, repeats: int):\n             if self.linear_cache[layer_idx] != []:\n                 self.linear_cache[layer_idx] = self.linear_cache[layer_idx].repeat_interleave(repeats, dim=0)\n             else:\n-                self.key_cache[layer_idx] = self.key_cache[layer_idx].repeat_interleave(repeats, dim=0)\n-                self.value_cache[layer_idx] = self.value_cache[layer_idx].repeat_interleave(repeats, dim=0)\n+                self.layers[layer_idx].batch_repeat_interleave(repeats)\n \n     def batch_select_indices(self, indices: torch.Tensor):\n         for layer_idx in range(len(self)):\n             if self.linear_cache[layer_idx] != []:\n                 self.linear_cache[layer_idx] = self.linear_cache[layer_idx][indices, ...]\n             else:\n-                self.key_cache[layer_idx] = self.key_cache[layer_idx][indices, ...]\n-                self.value_cache[layer_idx] = self.value_cache[layer_idx][indices, ...]\n+                self.layers[layer_idx].batch_select_indices(indices)\n \n     def crop(self, max_length: int):\n         raise RuntimeError(\"MiniMaxCache doesnot support `crop` method\")"
        },
        {
            "sha": "fbc8b287b6ae516bdd12c91ea5e48abe07fef121",
            "filename": "src/transformers/models/mllama/modeling_mllama.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py?ref=c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2",
            "patch": "@@ -496,8 +496,8 @@ def forward(\n                 )\n         elif cache_position[0] != 0:\n             key_states, value_states = (\n-                past_key_value.key_cache[self.layer_idx],\n-                past_key_value.value_cache[self.layer_idx],\n+                past_key_value.layers[self.layer_idx].keys,\n+                past_key_value.layers[self.layer_idx].values,\n             )\n         else:\n             raise ValueError("
        },
        {
            "sha": "c17fc883c148ce03c14a9e8b7f67d691a5bbea03",
            "filename": "src/transformers/models/moonshine/modeling_moonshine.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py?ref=c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2",
            "patch": "@@ -234,8 +234,8 @@ def forward(\n         # use key_value_states if cross attention\n         current_states = key_value_states if key_value_states is not None else hidden_states\n         if is_cross_attention and past_key_value and is_updated:\n-            key_states = past_key_value.key_cache[self.layer_idx]\n-            value_states = past_key_value.value_cache[self.layer_idx]\n+            key_states = past_key_value.layers[self.layer_idx].keys\n+            value_states = past_key_value.layers[self.layer_idx].values\n         else:\n             key_states = (\n                 self.k_proj(current_states)"
        },
        {
            "sha": "6278fab115671a853bf508557cb947d3f1de9381",
            "filename": "src/transformers/models/moonshine/modular_moonshine.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py?ref=c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2",
            "patch": "@@ -333,8 +333,8 @@ def forward(\n         # use key_value_states if cross attention\n         current_states = key_value_states if key_value_states is not None else hidden_states\n         if is_cross_attention and past_key_value and is_updated:\n-            key_states = past_key_value.key_cache[self.layer_idx]\n-            value_states = past_key_value.value_cache[self.layer_idx]\n+            key_states = past_key_value.layers[self.layer_idx].keys\n+            value_states = past_key_value.layers[self.layer_idx].values\n         else:\n             key_states = (\n                 self.k_proj(current_states)"
        },
        {
            "sha": "0d2ed6b402d0adcc33db2ba609d775e51f6a2fd0",
            "filename": "src/transformers/models/mt5/modeling_mt5.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py?ref=c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2",
            "patch": "@@ -379,8 +379,8 @@ def forward(\n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.key_cache[self.layer_idx]\n-            value_states = curr_past_key_value.value_cache[self.layer_idx]\n+            key_states = curr_past_key_value.layers[self.layer_idx].keys\n+            value_states = curr_past_key_value.layers[self.layer_idx].values\n         else:\n             key_states = self.k(current_states)\n             value_states = self.v(current_states)"
        },
        {
            "sha": "d37ee0007c5a2417ace52fa5a68b8e877ce9629a",
            "filename": "src/transformers/models/pegasus/modeling_pegasus.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py?ref=c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2",
            "patch": "@@ -228,8 +228,8 @@ def forward(\n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.key_cache[self.layer_idx]\n-            value_states = curr_past_key_value.value_cache[self.layer_idx]\n+            key_states = curr_past_key_value.layers[self.layer_idx].keys\n+            value_states = curr_past_key_value.layers[self.layer_idx].values\n         else:\n             key_states = self.k_proj(current_states)\n             value_states = self.v_proj(current_states)"
        },
        {
            "sha": "e2e04804c038ed9b0f3290ff765157bf6698a5f2",
            "filename": "src/transformers/models/pegasus_x/modeling_pegasus_x.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py?ref=c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2",
            "patch": "@@ -249,8 +249,8 @@ def forward(\n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.key_cache[self.layer_idx]\n-            value_states = curr_past_key_value.value_cache[self.layer_idx]\n+            key_states = curr_past_key_value.layers[self.layer_idx].keys\n+            value_states = curr_past_key_value.layers[self.layer_idx].values\n         else:\n             key_states = self.k_proj(current_states)\n             value_states = self.v_proj(current_states)"
        },
        {
            "sha": "4257132452b1509a8ca3024e1d00fe2b69b182be",
            "filename": "src/transformers/models/phimoe/modeling_phimoe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py?ref=c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2",
            "patch": "@@ -352,10 +352,6 @@ def forward(\n         key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n         value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n \n-        kv_seq_len = key_states.shape[-2]\n-        if past_key_value is not None:\n-            kv_seq_len += past_key_value.get_usable_length(kv_seq_len, self.layer_idx)\n-\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n "
        },
        {
            "sha": "820d59bad3e9b8705ff6856a8d516824c7e04d5f",
            "filename": "src/transformers/models/pix2struct/modeling_pix2struct.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py?ref=c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2",
            "patch": "@@ -773,8 +773,8 @@ def forward(\n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_value and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.key_cache[self.layer_idx]\n-            value_states = curr_past_key_value.value_cache[self.layer_idx]\n+            key_states = curr_past_key_value.layers[self.layer_idx].keys\n+            value_states = curr_past_key_value.layers[self.layer_idx].values\n         else:\n             key_states = self.key(current_states)\n             value_states = self.value(current_states)"
        },
        {
            "sha": "23184dd4c05d1c792c3e46288fa325b864ced8de",
            "filename": "src/transformers/models/plbart/modeling_plbart.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py?ref=c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2",
            "patch": "@@ -413,8 +413,8 @@ def forward(\n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.key_cache[self.layer_idx]\n-            value_states = curr_past_key_value.value_cache[self.layer_idx]\n+            key_states = curr_past_key_value.layers[self.layer_idx].keys\n+            value_states = curr_past_key_value.layers[self.layer_idx].values\n         else:\n             key_states = self.k_proj(current_states)\n             value_states = self.v_proj(current_states)"
        },
        {
            "sha": "bdf712a2f94986c09b636bd5049bd9f8f30f1ef0",
            "filename": "src/transformers/models/pop2piano/modeling_pop2piano.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py?ref=c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2",
            "patch": "@@ -323,8 +323,8 @@ def forward(\n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.key_cache[self.layer_idx]\n-            value_states = curr_past_key_value.value_cache[self.layer_idx]\n+            key_states = curr_past_key_value.layers[self.layer_idx].keys\n+            value_states = curr_past_key_value.layers[self.layer_idx].values\n         else:\n             key_states = self.k(current_states)\n             value_states = self.v(current_states)"
        },
        {
            "sha": "f22484047bdc0291df3871c30bc376c76f691a69",
            "filename": "src/transformers/models/switch_transformers/modeling_switch_transformers.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py?ref=c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2",
            "patch": "@@ -516,8 +516,8 @@ def forward(\n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.key_cache[self.layer_idx]\n-            value_states = curr_past_key_value.value_cache[self.layer_idx]\n+            key_states = curr_past_key_value.layers[self.layer_idx].keys\n+            value_states = curr_past_key_value.layers[self.layer_idx].values\n         else:\n             key_states = self.k(current_states)\n             value_states = self.v(current_states)"
        },
        {
            "sha": "e1570a731d33476e7f1262e91700f827e8dc105f",
            "filename": "src/transformers/models/t5/modeling_t5.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py?ref=c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2",
            "patch": "@@ -504,8 +504,8 @@ def forward(\n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.key_cache[self.layer_idx]\n-            value_states = curr_past_key_value.value_cache[self.layer_idx]\n+            key_states = curr_past_key_value.layers[self.layer_idx].keys\n+            value_states = curr_past_key_value.layers[self.layer_idx].values\n         else:\n             key_states = self.k(current_states)\n             value_states = self.v(current_states)"
        },
        {
            "sha": "9f1ad6928b14e72f22e07c6db16b939bebc70055",
            "filename": "src/transformers/models/t5gemma/modeling_t5gemma.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py?ref=c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2",
            "patch": "@@ -339,8 +339,8 @@ def forward(\n                 key_states, value_states = curr_past_key_value.update(key_states, value_states, self.layer_idx)\n                 past_key_value.is_updated[self.layer_idx] = True\n         else:\n-            key_states = curr_past_key_value.key_cache[self.layer_idx]\n-            value_states = curr_past_key_value.value_cache[self.layer_idx]\n+            key_states = curr_past_key_value.layers[self.layer_idx].keys\n+            value_states = curr_past_key_value.layers[self.layer_idx].values\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":"
        },
        {
            "sha": "74b66ad53047e915e298ed2de43fc197b375ac4c",
            "filename": "src/transformers/models/t5gemma/modular_t5gemma.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py?ref=c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2",
            "patch": "@@ -291,8 +291,8 @@ def forward(\n                 key_states, value_states = curr_past_key_value.update(key_states, value_states, self.layer_idx)\n                 past_key_value.is_updated[self.layer_idx] = True\n         else:\n-            key_states = curr_past_key_value.key_cache[self.layer_idx]\n-            value_states = curr_past_key_value.value_cache[self.layer_idx]\n+            key_states = curr_past_key_value.layers[self.layer_idx].keys\n+            value_states = curr_past_key_value.layers[self.layer_idx].values\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":"
        },
        {
            "sha": "84d5de004c517e8897bd379cdd2eacafd07b390c",
            "filename": "src/transformers/models/time_series_transformer/modeling_time_series_transformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py?ref=c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2",
            "patch": "@@ -394,8 +394,8 @@ def forward(\n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.key_cache[self.layer_idx]\n-            value_states = curr_past_key_value.value_cache[self.layer_idx]\n+            key_states = curr_past_key_value.layers[self.layer_idx].keys\n+            value_states = curr_past_key_value.layers[self.layer_idx].values\n         else:\n             key_states = self.k_proj(current_states)\n             value_states = self.v_proj(current_states)"
        },
        {
            "sha": "817ea913aed549979bbece5aca536c0b0e69801e",
            "filename": "src/transformers/models/udop/modeling_udop.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py?ref=c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2",
            "patch": "@@ -602,8 +602,8 @@ def forward(\n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.key_cache[self.layer_idx]\n-            value_states = curr_past_key_value.value_cache[self.layer_idx]\n+            key_states = curr_past_key_value.layers[self.layer_idx].keys\n+            value_states = curr_past_key_value.layers[self.layer_idx].values\n         else:\n             key_states = self.k(current_states)\n             value_states = self.v(current_states)"
        },
        {
            "sha": "cf2e999449bfec4b6f9563b3bcb15710d8500107",
            "filename": "src/transformers/models/umt5/modeling_umt5.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py?ref=c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2",
            "patch": "@@ -288,8 +288,8 @@ def forward(\n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.key_cache[self.layer_idx]\n-            value_states = curr_past_key_value.value_cache[self.layer_idx]\n+            key_states = curr_past_key_value.layers[self.layer_idx].keys\n+            value_states = curr_past_key_value.layers[self.layer_idx].values\n         else:\n             key_states = self.k(current_states)\n             value_states = self.v(current_states)"
        },
        {
            "sha": "1cdca9ba0382649a69efe0aa94376b1baff80cd0",
            "filename": "src/transformers/models/whisper/generation_whisper.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py?ref=c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2",
            "patch": "@@ -1149,8 +1149,8 @@ def split_by_batch_index(values, key, batch_idx, is_shortform, beam_indices=None\n                     for layer_idx in range(self.config.decoder_layers):\n                         layer_past_key_values = []\n                         for cache_cls in [values.self_attention_cache, values.cross_attention_cache]:\n-                            for v in [cache_cls.key_cache, cache_cls.value_cache]:\n-                                layer_past_key_values.append(v[layer_idx][batch_idx][None].cpu())\n+                            for v in [cache_cls.layers[layer_idx].keys, cache_cls.layers[layer_idx].values]:\n+                                layer_past_key_values.append(v[batch_idx][None].cpu())\n                         all_past_key_values.append(tuple(layer_past_key_values))\n                     return EncoderDecoderCache.from_legacy_cache(tuple(all_past_key_values))\n                 else:"
        },
        {
            "sha": "917113cbb00c72541814fab37869c9fed0c178c0",
            "filename": "src/transformers/models/whisper/modeling_whisper.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py?ref=c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2",
            "patch": "@@ -330,8 +330,8 @@ def forward(\n         current_states = key_value_states if key_value_states is not None else hidden_states\n         if is_cross_attention and past_key_value and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = past_key_value.key_cache[self.layer_idx]\n-            value_states = past_key_value.value_cache[self.layer_idx]\n+            key_states = past_key_value.layers[self.layer_idx].keys\n+            value_states = past_key_value.layers[self.layer_idx].values\n         else:\n             key_states = self.k_proj(current_states).view(bsz, -1, self.num_heads, self.head_dim)\n             value_states = self.v_proj(current_states).view(bsz, -1, self.num_heads, self.head_dim)"
        },
        {
            "sha": "16290ea4e1b70d646e20f3a8aab6a1a7d7e30b96",
            "filename": "src/transformers/models/zamba/modeling_zamba.py",
            "status": "modified",
            "additions": 12,
            "deletions": 1,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py?ref=c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2",
            "patch": "@@ -93,7 +93,7 @@ def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n \n \n-class ZambaHybridDynamicCache(DynamicCache):\n+class ZambaHybridDynamicCache(Cache):\n     \"\"\"\n     A dynamic cache that can handle both the attention cache (which has a seq_len dimension) and the mamba cache\n     (which has a constant shape regardless of seq_len).\n@@ -107,8 +107,13 @@ class ZambaHybridDynamicCache(DynamicCache):\n     and `ssm_states` represents the ssm state and has a shape of `(batch_size, d_inner, d_state)`.\n     \"\"\"\n \n+    key_cache = None\n+    value_cache = None\n+    is_compileable = False\n+\n     def __init__(self, config, batch_size, dtype=torch.float16, device=None):\n         self.dtype = dtype\n+        self.is_compileable = False\n         self.layers_block_type = config.layers_block_type\n         self.has_previous_state = False  # only used by mamba\n         self.intermediate_size = config.mamba_expand * config.hidden_size\n@@ -138,6 +143,12 @@ def __init__(self, config, batch_size, dtype=torch.float16, device=None):\n         self.key_cache = [torch.tensor([[]] * batch_size, device=device) for _ in range(config.num_hidden_layers)]\n         self.value_cache = [torch.tensor([[]] * batch_size, device=device) for _ in range(config.num_hidden_layers)]\n \n+    def __len__(self):\n+        return len(self.key_cache)\n+\n+    def __getitem__(self, layer_idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n+        return self.key_cache[layer_idx], self.value_cache[layer_idx]\n+\n     # Copied from transformers.models.jamba.modeling_jamba.HybridMambaAttentionDynamicCache.update\n     def update(\n         self,"
        },
        {
            "sha": "de6c9c9b96dfc467ad1af50b532158038b13d802",
            "filename": "src/transformers/models/zamba2/modeling_zamba2.py",
            "status": "modified",
            "additions": 12,
            "deletions": 2,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py?ref=c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2",
            "patch": "@@ -97,7 +97,7 @@ def extra_repr(self):\n         return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n \n \n-class Zamba2HybridDynamicCache(DynamicCache):\n+class Zamba2HybridDynamicCache(Cache):\n     \"\"\"\n     A dynamic cache that can handle both the attention cache (which has a seq_len dimension) and the mamba cache\n     (which has a constant shape regardless of seq_len).\n@@ -111,6 +111,10 @@ class Zamba2HybridDynamicCache(DynamicCache):\n     and `ssm_states` represents the ssm state and has a shape of `(batch_size, d_inner, d_state)`.\n     \"\"\"\n \n+    key_cache = None\n+    value_cache = None\n+    is_compileable = False\n+\n     def __init__(\n         self, config: Zamba2Config, batch_size: int, dtype: torch.dtype = torch.float16, device: Optional[str] = None\n     ):\n@@ -143,6 +147,12 @@ def __init__(\n         self.key_cache = [torch.tensor([[]] * batch_size, device=device) for _ in range(config.num_hidden_layers)]\n         self.value_cache = [torch.tensor([[]] * batch_size, device=device) for _ in range(config.num_hidden_layers)]\n \n+    def __len__(self):\n+        return len(self.key_cache)\n+\n+    def __getitem__(self, layer_idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n+        return self.key_cache[layer_idx], self.value_cache[layer_idx]\n+\n     def update(\n         self,\n         key_states: torch.Tensor,\n@@ -1354,7 +1364,7 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        if past_key_values and not past_key_values.has_previous_state:\n+        if past_key_values is not None and not past_key_values.has_previous_state:\n             past_key_values.has_previous_state = True\n \n         output = BaseModelOutputWithPast("
        },
        {
            "sha": "44f28f9dc3f3c73c4ac04906afb3695fd41064bc",
            "filename": "src/transformers/models/zamba2/modular_zamba2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodular_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodular_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodular_zamba2.py?ref=c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2",
            "patch": "@@ -1123,7 +1123,7 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        if past_key_values and not past_key_values.has_previous_state:\n+        if past_key_values is not None and not past_key_values.has_previous_state:\n             past_key_values.has_previous_state = True\n \n         output = BaseModelOutputWithPast("
        },
        {
            "sha": "26a5a7bf3d1e9875a86e813b9f57e9137c1064c4",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 48,
            "deletions": 42,
            "changes": 90,
            "blob_url": "https://github.com/huggingface/transformers/blob/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2",
            "patch": "@@ -1577,51 +1577,57 @@ def test_past_key_values_format(self, custom_all_cache_shapes=None):\n             # 3. Check cache shapes\n             # 3.1. Encoder-Decoder checks\n             if config.is_encoder_decoder:\n-                num_cache_decoder_layers = (\n-                    len(past_kv) if is_legacy_cache else len(past_kv.self_attention_cache.key_cache)\n-                )\n+                num_cache_decoder_layers = len(past_kv) if is_legacy_cache else len(past_kv.self_attention_cache)\n                 self.assertEqual(num_cache_decoder_layers, num_decoder_layers)\n \n                 for i in range(num_decoder_layers):\n                     if is_legacy_cache:\n                         self.assertEqual(len(past_kv[0]), 4)  # legacy check: confirm number of elements in tuple\n \n                     # Self attention\n-                    self_attention_layer_key_cache = (\n-                        past_kv[i][0] if is_legacy_cache else past_kv.self_attention_cache.key_cache[i]\n+                    self_attention_layer_keys = (\n+                        past_kv[i][0] if is_legacy_cache else past_kv.self_attention_cache.layers[i].keys\n                     )\n-                    self_attention_layer_value_cache = (\n-                        past_kv[i][1] if is_legacy_cache else past_kv.self_attention_cache.value_cache[i]\n+                    self_attention_layer_values = (\n+                        past_kv[i][1] if is_legacy_cache else past_kv.self_attention_cache.layers[i].values\n                     )\n-                    self.assertEqual(self_attention_layer_key_cache.shape, all_cache_shapes[i][0])\n-                    self.assertEqual(self_attention_layer_value_cache.shape, all_cache_shapes[i][1])\n+                    self.assertEqual(self_attention_layer_keys.shape, all_cache_shapes[i][0])\n+                    self.assertEqual(self_attention_layer_values.shape, all_cache_shapes[i][1])\n \n                     # Cross attention (ignore 3rd dim, see default shape preparation)\n-                    cross_attention_layer_key_cache = (\n-                        past_kv[i][2] if is_legacy_cache else past_kv.cross_attention_cache.key_cache[i]\n+                    cross_attention_layer_keys = (\n+                        past_kv[i][2] if is_legacy_cache else past_kv.cross_attention_cache.layers[i].keys\n                     )\n-                    cross_attention_layer_value_cache = (\n-                        past_kv[i][3] if is_legacy_cache else past_kv.cross_attention_cache.value_cache[i]\n+                    cross_attention_layer_values = (\n+                        past_kv[i][3] if is_legacy_cache else past_kv.cross_attention_cache.layers[i].values\n                     )\n-                    cross_attention_layer_key_cache = cross_attention_layer_key_cache[:, :, 0, :]\n-                    cross_attention_layer_value_cache = cross_attention_layer_value_cache[:, :, 0, :]\n-                    self.assertEqual(cross_attention_layer_key_cache.shape, all_cache_shapes[i][2])\n-                    self.assertEqual(cross_attention_layer_value_cache.shape, all_cache_shapes[i][3])\n+                    cross_attention_layer_keys = cross_attention_layer_keys[:, :, 0, :]\n+                    cross_attention_layer_values = cross_attention_layer_values[:, :, 0, :]\n+                    self.assertEqual(cross_attention_layer_keys.shape, all_cache_shapes[i][2])\n+                    self.assertEqual(cross_attention_layer_values.shape, all_cache_shapes[i][3])\n \n             # 3.2. Decoder-only checks\n             else:\n-                num_cache_decoder_layers = len(past_kv) if is_legacy_cache else len(past_kv.key_cache)\n+                num_cache_decoder_layers = len(past_kv)\n                 self.assertEqual(num_cache_decoder_layers, num_decoder_layers)\n \n                 for i in range(num_decoder_layers):\n                     if is_legacy_cache:\n                         self.assertEqual(len(past_kv[0]), 2)  # legacy check: confirm number of elements in tuple\n \n                     # Self attention\n-                    self_attention_layer_key_cache = past_kv[i][0] if is_legacy_cache else past_kv.key_cache[i]\n-                    self_attention_layer_value_cache = past_kv[i][1] if is_legacy_cache else past_kv.value_cache[i]\n-                    self.assertEqual(self_attention_layer_key_cache.shape, all_cache_shapes[i][0])\n-                    self.assertEqual(self_attention_layer_value_cache.shape, all_cache_shapes[i][1])\n+                    if is_legacy_cache:\n+                        self_attention_layer_keys = past_kv[i][0]\n+                        self_attention_layer_values = past_kv[i][1]\n+                    elif getattr(past_kv, \"layers\", None) is None:\n+                        # Cache is lot layered (i.e, Mamba derivatives)\n+                        self_attention_layer_keys = past_kv.key_cache[i]\n+                        self_attention_layer_values = past_kv.value_cache[i]\n+                    else:\n+                        self_attention_layer_keys = past_kv.layers[i].keys\n+                        self_attention_layer_values = past_kv.layers[i].values\n+                    self.assertEqual(self_attention_layer_keys.shape, all_cache_shapes[i][0])\n+                    self.assertEqual(self_attention_layer_values.shape, all_cache_shapes[i][1])\n \n     @pytest.mark.generate\n     def test_generate_from_random_inputs_embeds(self):\n@@ -1804,8 +1810,8 @@ def test_generate_from_inputs_embeds_with_static_cache(self):\n             max_length = max_new_tokens + inputs_embeds.shape[1] - 1\n             cache_shape = [batch_size, num_key_value_heads, max_length, head_dim]\n             self.assertIsInstance(outputs.past_key_values, StaticCache)\n-            self.assertEqual(len(outputs.past_key_values.key_cache), num_hidden_layers)\n-            self.assertListEqual(list(outputs.past_key_values.key_cache[0].shape), cache_shape)\n+            self.assertEqual(len(outputs.past_key_values), num_hidden_layers)\n+            self.assertListEqual(list(outputs.past_key_values.layers[0].keys.shape), cache_shape)\n \n     @pytest.mark.generate\n     def test_generate_continue_from_past_key_values(self):\n@@ -2027,8 +2033,8 @@ def test_generate_with_static_cache(self):\n                 num_hidden_layers = text_config.num_hidden_layers\n                 cache_shape = (batch_size, num_key_value_heads, max_cache_len, head_dim)\n                 self.assertTrue(isinstance(static_cache_generation.past_key_values, StaticCache))\n-                self.assertTrue(len(static_cache_generation.past_key_values.key_cache) == num_hidden_layers)\n-                self.assertTrue(static_cache_generation.past_key_values.key_cache[0].shape == cache_shape)\n+                self.assertTrue(len(static_cache_generation.past_key_values) == num_hidden_layers)\n+                self.assertTrue(static_cache_generation.past_key_values.layers[0].keys.shape == cache_shape)\n \n                 # Check 2: The outputs must be similar to the case with dynamic cache\n                 dynamic_cache_generation = model.generate(**generation_kwargs, **inputs_dict)\n@@ -2629,12 +2635,12 @@ def _check_past_key_values_for_generate(self, batch_size, decoder_past_key_value\n \n         if isinstance(decoder_past_key_values, Cache):\n             self.assertListEqual(\n-                [key_tensor.shape for key_tensor in decoder_past_key_values.key_cache],\n-                [expected_shape] * len(decoder_past_key_values.key_cache),\n+                [layer.keys.shape for layer in decoder_past_key_values.layers],\n+                [expected_shape] * len(decoder_past_key_values.layers),\n             )\n             self.assertListEqual(\n-                [value_tensor.shape for value_tensor in decoder_past_key_values.value_cache],\n-                [expected_shape] * len(decoder_past_key_values.value_cache),\n+                [layer.values.shape for layer in decoder_past_key_values.layers],\n+                [expected_shape] * len(decoder_past_key_values.layers),\n             )\n \n         # Legacy cache format checks. This branch should be removed when all models use `Cache` by default\n@@ -4040,13 +4046,13 @@ def test_generate_with_static_cache_multi_accelerator(self):\n         self.assertTrue(isinstance(results.past_key_values, StaticCache))\n \n         # check device of each layer\n-        key_cache_0 = results.past_key_values.key_cache[0]\n-        value_cache_0 = results.past_key_values.value_cache[0]\n-        self.assertTrue(key_cache_0.device == value_cache_0.device == torch.device(0))\n+        keys_0 = results.past_key_values.layers[0].keys\n+        values_0 = results.past_key_values.layers[0].values\n+        self.assertTrue(keys_0.device == values_0.device == torch.device(0))\n \n-        key_cache_1 = results.past_key_values.key_cache[1]\n-        value_cache_1 = results.past_key_values.value_cache[1]\n-        self.assertTrue(key_cache_1.device == value_cache_1.device == torch.device(1))\n+        keys_1 = results.past_key_values.layers[1].keys\n+        values_1 = results.past_key_values.layers[1].values\n+        self.assertTrue(keys_1.device == values_1.device == torch.device(1))\n \n     @pytest.mark.generate\n     @require_torch_multi_accelerator\n@@ -4118,13 +4124,13 @@ def test_init_static_cache_multi_accelerator(self):\n         results = model.generate(input_ids, past_key_values=past_key_values, **generation_kwargs)\n \n         # check device of each layer\n-        key_cache_0 = results.past_key_values.key_cache[0]\n-        value_cache_0 = results.past_key_values.value_cache[0]\n-        self.assertTrue(key_cache_0.device == value_cache_0.device == torch.device(0))\n+        keys_0 = results.past_key_values.layers[0].keys\n+        values_0 = results.past_key_values.layers[0].values\n+        self.assertTrue(keys_0.device == values_0.device == torch.device(0))\n \n-        key_cache_1 = results.past_key_values.key_cache[1]\n-        value_cache_1 = results.past_key_values.value_cache[1]\n-        self.assertTrue(key_cache_1.device == value_cache_1.device == torch.device(1))\n+        keys_1 = results.past_key_values.layers[1].keys\n+        values_1 = results.past_key_values.layers[1].values\n+        self.assertTrue(keys_1.device == values_1.device == torch.device(1))\n \n     @slow\n     def test_padding_input_contrastive_search_gpt2(self):"
        },
        {
            "sha": "0bdc6884590faaeb3e46b3dfa060944cf67f80c5",
            "filename": "tests/models/deepseek_v2/test_modeling_deepseek_v2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 8,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/tests%2Fmodels%2Fdeepseek_v2%2Ftest_modeling_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/tests%2Fmodels%2Fdeepseek_v2%2Ftest_modeling_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeepseek_v2%2Ftest_modeling_deepseek_v2.py?ref=c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2",
            "patch": "@@ -168,14 +168,9 @@ def _check_past_key_values_for_generate(self, batch_size, decoder_past_key_value\n         expected_value_shape = expected_common_shape + (config.v_head_dim,)\n \n         if isinstance(decoder_past_key_values, Cache):\n-            self.assertListEqual(\n-                [key_tensor.shape for key_tensor in decoder_past_key_values.key_cache],\n-                [expected_key_shape] * len(decoder_past_key_values.key_cache),\n-            )\n-            self.assertListEqual(\n-                [value_tensor.shape for value_tensor in decoder_past_key_values.value_cache],\n-                [expected_value_shape] * len(decoder_past_key_values.value_cache),\n-            )\n+            for layer in decoder_past_key_values.layers:\n+                self.assertEqual(layer.keys.shape, expected_key_shape)\n+                self.assertEqual(layer.values.shape, expected_value_shape)\n \n     @unittest.skip(\"Deepseek-V2 uses MLA which has a special head dim and is not compatible with StaticCache shape\")\n     def test_generate_compilation_all_outputs(self):"
        },
        {
            "sha": "87f7b2abb0e97e695423ef805cfaae7f6fb6398f",
            "filename": "tests/models/deepseek_v3/test_modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py?ref=c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2",
            "patch": "@@ -440,13 +440,11 @@ def test_past_key_values_format(self):\n         # difference: last dim\n         k_embed_dim = config.qk_nope_head_dim + config.qk_rope_head_dim\n         v_embed_dim = config.v_head_dim\n-        self_attention_key_cache_shape = (batch_size, config.num_key_value_heads, seq_length, k_embed_dim)\n-        self_attention_value_cache_shape = (batch_size, config.num_key_value_heads, seq_length, v_embed_dim)\n+        self_attention_keys_shape = (batch_size, config.num_key_value_heads, seq_length, k_embed_dim)\n+        self_attention_values_shape = (batch_size, config.num_key_value_heads, seq_length, v_embed_dim)\n         # build the full cache shapes\n         num_hidden_layers = config.num_hidden_layers\n-        all_cache_shapes = [\n-            [self_attention_key_cache_shape, self_attention_value_cache_shape] for _ in range(num_hidden_layers)\n-        ]\n+        all_cache_shapes = [[self_attention_keys_shape, self_attention_values_shape] for _ in range(num_hidden_layers)]\n         super().test_past_key_values_format(custom_all_cache_shapes=all_cache_shapes)\n \n     @require_torch_large_accelerator"
        },
        {
            "sha": "c6d1547afb40af9b213133fbdd5fa9ca458c714f",
            "filename": "tests/models/dia/test_modeling_dia.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/tests%2Fmodels%2Fdia%2Ftest_modeling_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/tests%2Fmodels%2Fdia%2Ftest_modeling_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdia%2Ftest_modeling_dia.py?ref=c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2",
            "patch": "@@ -399,12 +399,12 @@ def _check_past_key_values_for_generate(self, batch_size, decoder_past_key_value\n \n         if isinstance(decoder_past_key_values, Cache):\n             self.assertListEqual(\n-                [key_tensor.shape for key_tensor in decoder_past_key_values.key_cache],\n-                [expected_shape] * len(decoder_past_key_values.key_cache),\n+                [layer.keys.shape for layer in decoder_past_key_values.layers],\n+                [expected_shape] * len(decoder_past_key_values.layers),\n             )\n             self.assertListEqual(\n-                [value_tensor.shape for value_tensor in decoder_past_key_values.value_cache],\n-                [expected_shape] * len(decoder_past_key_values.value_cache),\n+                [layer.values.shape for layer in decoder_past_key_values.layers],\n+                [expected_shape] * len(decoder_past_key_values.layers),\n             )\n \n     def _check_scores(self, batch_size, scores, generated_length, config):"
        },
        {
            "sha": "5c538ac4b410214a3cc4799ba55bce75c45a5041",
            "filename": "tests/models/falcon_h1/test_modeling_falcon_h1.py",
            "status": "modified",
            "additions": 38,
            "deletions": 1,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/tests%2Fmodels%2Ffalcon_h1%2Ftest_modeling_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/tests%2Fmodels%2Ffalcon_h1%2Ftest_modeling_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffalcon_h1%2Ftest_modeling_falcon_h1.py?ref=c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2",
            "patch": "@@ -38,7 +38,7 @@\n if is_torch_available():\n     import torch\n \n-    from transformers import AutoTokenizer, FalconH1ForCausalLM, FalconH1Model\n+    from transformers import AutoTokenizer, Cache, FalconH1ForCausalLM, FalconH1Model\n     from transformers.models.falcon_h1.modeling_falcon_h1 import (\n         FalconHybridMambaAttentionDynamicCache,\n     )\n@@ -272,6 +272,43 @@ class FalconH1ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterM\n         {\"feature-extraction\": FalconH1Model, \"text-generation\": FalconH1ForCausalLM} if is_torch_available() else {}\n     )\n \n+    def _check_past_key_values_for_generate(self, batch_size, decoder_past_key_values, cache_length, config):\n+        self.assertIsInstance(decoder_past_key_values, (tuple, Cache))\n+\n+        # (batch, head, seq_length, head_features)\n+        expected_shape = (\n+            batch_size,\n+            config.num_key_value_heads if hasattr(config, \"num_key_value_heads\") else config.num_attention_heads,\n+            cache_length,\n+            config.hidden_size // config.num_attention_heads,\n+        )\n+\n+        if isinstance(decoder_past_key_values, Cache):\n+            self.assertListEqual(\n+                [key_tensor.shape for key_tensor in decoder_past_key_values.key_cache],\n+                [expected_shape] * len(decoder_past_key_values.key_cache),\n+            )\n+            self.assertListEqual(\n+                [value_cache.shape for value_cache in decoder_past_key_values.value_cache],\n+                [expected_shape] * len(decoder_past_key_values.value_cache),\n+            )\n+\n+        # Legacy cache format checks. This branch should be removed when all models use `Cache` by default\n+        else:\n+            self.assertListEqual(\n+                [isinstance(iter_past_key_values, tuple) for iter_past_key_values in decoder_past_key_values],\n+                [True] * len(decoder_past_key_values),\n+            )\n+            # check shape key, value\n+            self.assertListEqual(\n+                [layer_past_key_values[0].shape for layer_past_key_values in decoder_past_key_values],\n+                [expected_shape] * len(decoder_past_key_values),\n+            )\n+            self.assertListEqual(\n+                [layer_past_key_values[1].shape for layer_past_key_values in decoder_past_key_values],\n+                [expected_shape] * len(decoder_past_key_values),\n+            )\n+\n     def setUp(self):\n         self.model_tester = FalconH1ModelTester(self)\n         self.config_tester = ConfigTester(self, config_class=FalconH1Config, hidden_size=64)"
        },
        {
            "sha": "ecd2af9fdc6c9d579db7b46dccceb840b8b35697",
            "filename": "tests/models/gpt_neox/test_modeling_gpt_neox.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/tests%2Fmodels%2Fgpt_neox%2Ftest_modeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/tests%2Fmodels%2Fgpt_neox%2Ftest_modeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt_neox%2Ftest_modeling_gpt_neox.py?ref=c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2",
            "patch": "@@ -235,8 +235,8 @@ def copy_cache(cache: DynamicCache):\n             \"\"\"Deep copy a DynamicCache to reuse the same one multiple times.\"\"\"\n             new_cache = cache\n             for i in range(len(cache)):\n-                new_cache.key_cache[i] = cache.key_cache[i].clone()\n-                new_cache.value_cache[i] = cache.value_cache[i].clone()\n+                new_cache.layers[i].keys = cache.layers[i].keys.clone()\n+                new_cache.layers[i].values = cache.layers[i].values.clone()\n \n         # Cached forward once with the attention mask provided and the other time without it (which should assume full attention)\n         # We need to run both on a copy of the cache, otherwise it is modified in-place"
        },
        {
            "sha": "100787ece9ead1711cb4a7204281bd74fe3f77af",
            "filename": "tests/models/t5gemma/test_modeling_t5gemma.py",
            "status": "modified",
            "additions": 21,
            "deletions": 23,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/tests%2Fmodels%2Ft5gemma%2Ftest_modeling_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/tests%2Fmodels%2Ft5gemma%2Ftest_modeling_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ft5gemma%2Ftest_modeling_t5gemma.py?ref=c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2",
            "patch": "@@ -271,7 +271,7 @@ def create_and_check_model(\n         self.parent.assertEqual(decoder_output.size(), (self.batch_size, self.seq_length, self.hidden_size))\n         self.parent.assertIsNotNone(decoder_past)\n         self.parent.assertEqual(len(decoder_past.self_attention_cache), config.decoder.num_hidden_layers)\n-        self.parent.assertEqual(len(decoder_past.cross_attention_cache.key_cache), config.decoder.num_hidden_layers)\n+        self.parent.assertEqual(len(decoder_past.cross_attention_cache), config.decoder.num_hidden_layers)\n \n     def check_prepare_lm_labels_via_shift_left(\n         self,\n@@ -1060,51 +1060,49 @@ def test_past_key_values_format(self, custom_all_cache_shapes=None):\n             # 3. Check cache shapes\n             # 3.1. Encoder-Decoder checks\n             if config.is_encoder_decoder:\n-                num_cache_decoder_layers = (\n-                    len(past_kv) if is_legacy_cache else len(past_kv.self_attention_cache.key_cache)\n-                )\n+                num_cache_decoder_layers = len(past_kv) if is_legacy_cache else len(past_kv.self_attention_cache)\n                 self.assertEqual(num_cache_decoder_layers, num_decoder_layers)\n \n                 for i in range(num_decoder_layers):\n                     if is_legacy_cache:\n                         self.assertEqual(len(past_kv[0]), 5)  # legacy check: confirm number of elements in tuple\n \n                     # Self attention\n-                    self_attention_layer_key_cache = (\n-                        past_kv[i][0] if is_legacy_cache else past_kv.self_attention_cache.key_cache[i]\n+                    self_attention_layer_keys = (\n+                        past_kv[i][0] if is_legacy_cache else past_kv.self_attention_cache.layers[i].keys\n                     )\n-                    self_attention_layer_value_cache = (\n-                        past_kv[i][1] if is_legacy_cache else past_kv.self_attention_cache.value_cache[i]\n+                    self_attention_layer_values = (\n+                        past_kv[i][1] if is_legacy_cache else past_kv.self_attention_cache.layers[i].values\n                     )\n-                    self.assertEqual(self_attention_layer_key_cache.shape, all_cache_shapes[i][0])\n-                    self.assertEqual(self_attention_layer_value_cache.shape, all_cache_shapes[i][1])\n+                    self.assertEqual(self_attention_layer_keys.shape, all_cache_shapes[i][0])\n+                    self.assertEqual(self_attention_layer_values.shape, all_cache_shapes[i][1])\n \n                     # Cross attention (ignore 3rd dim, see default shape preparation)\n-                    cross_attention_layer_key_cache = (\n-                        past_kv[i][2] if is_legacy_cache else past_kv.cross_attention_cache.key_cache[i]\n+                    cross_attention_layer_keys = (\n+                        past_kv[i][2] if is_legacy_cache else past_kv.cross_attention_cache.layers[i].keys\n                     )\n-                    cross_attention_layer_value_cache = (\n-                        past_kv[i][3] if is_legacy_cache else past_kv.cross_attention_cache.value_cache[i]\n+                    cross_attention_layer_values = (\n+                        past_kv[i][3] if is_legacy_cache else past_kv.cross_attention_cache.layers[i].values\n                     )\n-                    cross_attention_layer_key_cache = cross_attention_layer_key_cache[:, :, 0, :]\n-                    cross_attention_layer_value_cache = cross_attention_layer_value_cache[:, :, 0, :]\n-                    self.assertEqual(cross_attention_layer_key_cache.shape, all_cache_shapes[i][2])\n-                    self.assertEqual(cross_attention_layer_value_cache.shape, all_cache_shapes[i][3])\n+                    cross_attention_layer_keys = cross_attention_layer_keys[:, :, 0, :]\n+                    cross_attention_layer_values = cross_attention_layer_values[:, :, 0, :]\n+                    self.assertEqual(cross_attention_layer_keys.shape, all_cache_shapes[i][2])\n+                    self.assertEqual(cross_attention_layer_values.shape, all_cache_shapes[i][3])\n \n             # 3.2. Decoder-only checks\n             else:\n-                num_cache_decoder_layers = len(past_kv) if is_legacy_cache else len(past_kv.key_cache)\n+                num_cache_decoder_layers = len(past_kv) if is_legacy_cache else len(past_kv)\n                 self.assertEqual(num_cache_decoder_layers, num_decoder_layers)\n \n                 for i in range(num_decoder_layers):\n                     if is_legacy_cache:\n                         self.assertEqual(len(past_kv[0]), 2)  # legacy check: confirm number of elements in tuple\n \n                     # Self attention\n-                    self_attention_layer_key_cache = past_kv[i][0] if is_legacy_cache else past_kv.key_cache[i]\n-                    self_attention_layer_value_cache = past_kv[i][1] if is_legacy_cache else past_kv.value_cache[i]\n-                    self.assertEqual(self_attention_layer_key_cache.shape, all_cache_shapes[i][0])\n-                    self.assertEqual(self_attention_layer_value_cache.shape, all_cache_shapes[i][1])\n+                    self_attention_layer_keys = past_kv[i][0] if is_legacy_cache else past_kv.layers[i].keys\n+                    self_attention_layer_values = past_kv[i][1] if is_legacy_cache else past_kv.layers[i].values\n+                    self.assertEqual(self_attention_layer_keys.shape, all_cache_shapes[i][0])\n+                    self.assertEqual(self_attention_layer_values.shape, all_cache_shapes[i][1])\n \n     @unittest.skip(\"Mismatch issue doesn't exist in T5Gemma.\")\n     def test_load_with_mismatched_shapes(self):"
        },
        {
            "sha": "8dbcc11943143476a7bb9d57a3bd251963e84381",
            "filename": "tests/utils/test_cache_utils.py",
            "status": "modified",
            "additions": 366,
            "deletions": 38,
            "changes": 404,
            "blob_url": "https://github.com/huggingface/transformers/blob/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/tests%2Futils%2Ftest_cache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2/tests%2Futils%2Ftest_cache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_cache_utils.py?ref=c338fd43b0be2c7f5d73e693fa6fb1b5e7a0bdc2",
            "patch": "@@ -36,7 +36,7 @@\n     slow,\n     torch_device,\n )\n-from transformers.utils import is_optimum_quanto_available, is_torch_greater_or_equal\n+from transformers.utils import is_hqq_available, is_optimum_quanto_available, is_torch_greater_or_equal\n \n \n if is_torch_available():\n@@ -49,8 +49,12 @@\n         DynamicCache,\n         Gemma2Config,\n         GenerationConfig,\n+        HQQQuantizedCacheProcessor,\n         HybridCache,\n+        HybridChunkedCache,\n         LlamaConfig,\n+        QuantizedCache,\n+        QuantoQuantizedCacheProcessor,\n         SlidingWindowCache,\n         StaticCache,\n         convert_and_export_with_cache,\n@@ -252,6 +256,59 @@ def test_cache_beam_search(self, cache_implementation):\n         decoded = self.tokenizer.batch_decode(gen_out.sequences, skip_special_tokens=True)\n         self.assertListEqual(decoded, EXPECTED_GENERATION)\n \n+    @parameterized.expand([(\"quanto\"), (\"HQQ\")])\n+    def test_quantized_cache_generation(self, backend):\n+        \"\"\"Tests that QuantizedCache works as expected for both `quanto` and `hqq` backends.\"\"\"\n+        if backend == \"quanto\":\n+            if not is_optimum_quanto_available():\n+                self.skipTest(\"Quanto is not available\")\n+            axis_key, axis_value = 0, 0\n+            # This output is taken from a run with the same parameters, and is known to be correct\n+            expected_generation = [\"The cat's whiskers are also a sign of anxiety.\"]\n+        elif backend == \"HQQ\":\n+            if not is_hqq_available():\n+                self.skipTest(\"HQQ is not available\")\n+            axis_key, axis_value = 1, 1\n+            # HQQ has slightly different numerics\n+            expected_generation = [\"The cat's whiskers are also a sign of anxiety.\"]\n+        else:\n+            return\n+\n+        inputs = self.tokenizer([\"The cat\"], return_tensors=\"pt\").to(self.model.device)\n+\n+        gen_out = self.model.generate(\n+            **inputs,\n+            do_sample=False,\n+            max_new_tokens=10,\n+            return_dict_in_generate=True,\n+            cache_implementation=\"quantized\",\n+            cache_config={\n+                \"backend\": backend,\n+                \"nbits\": 4,\n+                \"q_group_size\": 16,\n+                \"residual_length\": 4,\n+                \"axis_key\": axis_key,\n+                \"axis_value\": axis_value,\n+            },\n+            disable_compile=True,\n+        )\n+\n+        self.assertIsInstance(gen_out.past_key_values, QuantizedCache)\n+        processor = gen_out.past_key_values.cache_processor\n+        if backend == \"quanto\":\n+            self.assertIsInstance(processor, QuantoQuantizedCacheProcessor)\n+        elif backend == \"hqq\":\n+            self.assertIsInstance(processor, HQQQuantizedCacheProcessor)\n+\n+        decoded = self.tokenizer.batch_decode(gen_out.sequences, skip_special_tokens=True)\n+        self.assertListEqual(decoded, expected_generation)\n+\n+        self.assertTrue(len(processor._quantized_keys) > 0)\n+\n+        # Check that something is actually quantized\n+        has_been_quantized = any((q[0] if isinstance(q, tuple) else q).numel() > 0 for q in processor._quantized_keys)\n+        self.assertTrue(has_been_quantized)\n+\n     @parameterized.expand(TEST_CACHE_IMPLEMENTATIONS)\n     def test_cache_extra_left_padding(self, cache_implementation):\n         \"\"\"Tests that adding extra left-padding does not affect the generation with the cache\"\"\"\n@@ -566,7 +623,7 @@ def test_dynamic_cache_exportability(self):\n             past_key_values=DynamicCache(),\n             use_cache=True,\n         )\n-        self.assertTrue(len(res.past_key_values.key_cache) == model.config.num_hidden_layers)\n+        self.assertTrue(len(res.past_key_values) == model.config.num_hidden_layers)\n         self.assertEqual(2 * model.config.num_hidden_layers + 1, len(ep.graph_signature.output_specs))\n         self.assertEqual(\n             3,\n@@ -587,11 +644,9 @@ def test_dynamic_cache_exportability(self):\n             use_cache=True,\n         )\n         self.assertTrue(torch.allclose(res.logits, res_eager.logits, atol=1e-5))\n-        for k1, k2 in zip(res.past_key_values.key_cache, res_eager.past_key_values.key_cache):\n-            self.assertTrue(torch.allclose(k1, k2, atol=1e-5))\n-\n-        for v1, v2 in zip(res.past_key_values.value_cache, res_eager.past_key_values.value_cache):\n-            self.assertTrue(torch.allclose(v1, v2, atol=1e-5))\n+        for l1, l2 in zip(res.past_key_values.layers, res_eager.past_key_values.layers):\n+            self.assertTrue(torch.allclose(l1.keys, l2.keys, atol=1e-5))\n+            self.assertTrue(torch.allclose(l1.values, l2.values, atol=1e-5))\n \n     def test_dynamic_cache_exportability_multiple_run(self):\n         # When exporting with DynamicCache, you should export two graphs:\n@@ -615,7 +670,7 @@ def test_dynamic_cache_exportability_multiple_run(self):\n             past_key_values=DynamicCache(),\n             use_cache=True,\n         )\n-        self.assertTrue(len(res.past_key_values.key_cache) == model.config.num_hidden_layers)\n+        self.assertTrue(len(res.past_key_values) == model.config.num_hidden_layers)\n         self.assertEqual(2 * model.config.num_hidden_layers + 1, len(ep.graph_signature.output_specs))\n         self.assertEqual(\n             3,\n@@ -640,9 +695,9 @@ def test_dynamic_cache_exportability_multiple_run(self):\n         shapes = torch.export.ShapesCollection()\n         dyn = torch.export.Dim(\"seq\", max=512)\n \n-        for ix in range(len(past_key_values.key_cache)):\n-            shapes[past_key_values.key_cache[ix]] = (None, None, dyn, None)\n-            shapes[past_key_values.value_cache[ix]] = (None, None, dyn, None)\n+        for ix in range(len(past_key_values)):\n+            shapes[past_key_values.layers[ix].keys] = (None, None, dyn, None)\n+            shapes[past_key_values.layers[ix].values] = (None, None, dyn, None)\n \n         ep_second = torch.export.export(\n             model,\n@@ -683,11 +738,9 @@ def test_dynamic_cache_exportability_multiple_run(self):\n             use_cache=True,\n         )\n \n-        for k1, k2 in zip(res_export_2.past_key_values.key_cache, res_eager_2.past_key_values.key_cache):\n-            self.assertTrue(torch.allclose(k1, k2, atol=1e-5))\n-\n-        for v1, v2 in zip(res_export_2.past_key_values.value_cache, res_eager_2.past_key_values.value_cache):\n-            self.assertTrue(torch.allclose(v1, v2, atol=1e-5))\n+        for l1, l2 in zip(res_export_2.past_key_values.layers, res_eager_2.past_key_values.layers):\n+            self.assertTrue(torch.allclose(l1.keys, l2.keys, atol=1e-5))\n+            self.assertTrue(torch.allclose(l1.values, l2.values, atol=1e-5))\n \n     @unittest.skip(\"Runs on my machine locally, passed, no idea why it does not online\")\n     def test_static_cache_exportability(self):\n@@ -726,8 +779,8 @@ def test_static_cache_exportability(self):\n         self.assertEqual(model.generation_config.cache_implementation, cache_implementation)\n         self.assertEqual(model.generation_config.max_length, max_cache_len)\n         self.assertTrue(model.generation_config.cache_config is not None)\n-        self.assertEqual(model.generation_config.cache_config.batch_size, batch_size)\n-        self.assertEqual(model.generation_config.cache_config.max_cache_len, max_cache_len)\n+        self.assertEqual(model.generation_config.cache_config.get(\"batch_size\"), batch_size)\n+        self.assertEqual(model.generation_config.cache_config.get(\"max_cache_len\"), max_cache_len)\n \n         exported_program = convert_and_export_with_cache(model)\n \n@@ -830,7 +883,7 @@ def setUp(self):\n             head_dim=1,\n             hidden_size=1,\n             sliding_window=self.window_size,\n-            sliding_window_pattern=2,  # Default pattern for hybrid sliding\n+            layer_types=[\"full_attention\"] * 1,  # Static cache by default\n         )\n \n     def test_static_cache_out_of_bounds(self):\n@@ -867,7 +920,7 @@ def test_static_cache(self):\n             cache_kwargs={\"cache_position\": torch.tensor([2])},\n         )\n         self.assertEqual(\n-            static_cache.key_cache[0][0, 0, :, 0].tolist(), [1.0, 2.0, 3.0, 0.0], \"StaticCache Scenario 1 failed\"\n+            static_cache.layers[0].keys[0, 0, :, 0].tolist(), [1.0, 2.0, 3.0, 0.0], \"StaticCache Scenario 1 failed\"\n         )\n \n         # Scenario 2: Fill to capacity\n@@ -878,7 +931,7 @@ def test_static_cache(self):\n             cache_kwargs={\"cache_position\": torch.tensor([3])},\n         )\n         self.assertEqual(\n-            static_cache.key_cache[0][0, 0, :, 0].tolist(), [1.0, 2.0, 3.0, 4.0], \"StaticCache Scenario 2 failed\"\n+            static_cache.layers[0].keys[0, 0, :, 0].tolist(), [1.0, 2.0, 3.0, 4.0], \"StaticCache Scenario 2 failed\"\n         )\n \n     def test_sliding_window_cache(self):\n@@ -897,7 +950,9 @@ def test_sliding_window_cache(self):\n         result:        [3.0, 4.0, 5.0, 6.0] (keeps last window_size tokens)\n         \"\"\"\n         # Scenario 1: Update within window, no slide yet\n-        sliding_cache = SlidingWindowCache(config=self.config, max_batch_size=1, max_cache_len=self.max_cache_len)\n+        config = copy.deepcopy(self.config)\n+        config.layer_types = [\"sliding_attention\"] * config.num_hidden_layers\n+        sliding_cache = SlidingWindowCache(config=config, max_batch_size=1, max_cache_len=self.max_cache_len)\n         prefill = torch.tensor([1.0, 2.0, 0.0, 0.0])[None, None, :, None]\n         sliding_cache.update(\n             key_states=prefill,\n@@ -912,13 +967,13 @@ def test_sliding_window_cache(self):\n             cache_kwargs={\"cache_position\": torch.tensor([2]), \"sliding_window\": self.window_size},\n         )\n         self.assertEqual(\n-            sliding_cache.key_cache[0][0, 0, :, 0].tolist(),\n+            sliding_cache.layers[0].keys[0, 0, :, 0].tolist(),\n             [1.0, 2.0, 3.0, 0.0],\n             \"SlidingWindowCache Scenario 1 failed\",\n         )\n \n         # Scenario 2: Update causing slide\n-        sliding_cache = SlidingWindowCache(config=self.config, max_batch_size=1, max_cache_len=self.max_cache_len)\n+        sliding_cache = SlidingWindowCache(config=config, max_batch_size=1, max_cache_len=self.max_cache_len)\n         prefill = torch.tensor([1.0, 2.0, 3.0, 4.0])[None, None, :, None]\n         sliding_cache.update(\n             key_states=prefill,\n@@ -933,13 +988,13 @@ def test_sliding_window_cache(self):\n             cache_kwargs={\"cache_position\": torch.tensor([4]), \"sliding_window\": self.window_size},\n         )\n         self.assertEqual(\n-            sliding_cache.key_cache[0][0, 0, :, 0].tolist(),\n+            sliding_cache.layers[0].keys[0, 0, :, 0].tolist(),\n             [2.0, 3.0, 4.0, 5.0],\n             \"SlidingWindowCache Scenario 2 failed\",\n         )\n \n         # Scenario 3: Long prompt handling\n-        sliding_cache = SlidingWindowCache(config=self.config, max_batch_size=1, max_cache_len=self.max_cache_len)\n+        sliding_cache = SlidingWindowCache(config=config, max_batch_size=1, max_cache_len=self.max_cache_len)\n         long_prefill = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0, 6.0])[None, None, :, None]\n         sliding_cache.update(\n             key_states=long_prefill,\n@@ -948,13 +1003,13 @@ def test_sliding_window_cache(self):\n             cache_kwargs={\"cache_position\": torch.arange(6), \"sliding_window\": self.window_size},\n         )\n         self.assertEqual(\n-            sliding_cache.key_cache[0][0, 0, :, 0].tolist(),\n+            sliding_cache.layers[0].keys[0, 0, :, 0].tolist(),\n             [3.0, 4.0, 5.0, 6.0],\n             \"SlidingWindowCache Scenario 3 failed\",\n         )\n \n     def test_hybrid_cache_static_mode(self):\n-        \"\"\"Test HybridCache in static mode with hardcoded assertions.\n+        \"\"\"Test HybridCache with only 1 static layer.\n \n         Scenario 1: Static layer behavior\n         prefill:       [1.0, 2.0, 0.0, 0.0]\n@@ -964,7 +1019,7 @@ def test_hybrid_cache_static_mode(self):\n         update pos 3:  [1.0, 2.0, 3.0, 4.0]\n         \"\"\"\n         config = copy.deepcopy(self.config)\n-        config.sliding_window_pattern = 1  # Layer 0 is static (1 % 1 == 0)\n+        config.layer_types = [\"full_attention\"] * config.num_hidden_layers\n \n         # Scenario 1\n         hybrid_cache_static_mode = HybridCache(config=config, max_batch_size=1, max_cache_len=self.max_cache_len)\n@@ -982,7 +1037,7 @@ def test_hybrid_cache_static_mode(self):\n             cache_kwargs={\"cache_position\": torch.tensor([2])},\n         )\n         self.assertEqual(\n-            hybrid_cache_static_mode.key_cache[0][0, 0, :, 0].tolist(),\n+            hybrid_cache_static_mode.layers[0].keys[0, 0, :, 0].tolist(),\n             [1.0, 2.0, 3.0, 0.0],\n             \"HybridCache Static Scenario 1 failed\",\n         )\n@@ -995,7 +1050,7 @@ def test_hybrid_cache_static_mode(self):\n             cache_kwargs={\"cache_position\": torch.tensor([3])},\n         )\n         self.assertEqual(\n-            hybrid_cache_static_mode.key_cache[0][0, 0, :, 0].tolist(),\n+            hybrid_cache_static_mode.layers[0].keys[0, 0, :, 0].tolist(),\n             [1.0, 2.0, 3.0, 4.0],\n             \"HybridCache Static Scenario 2 failed\",\n         )\n@@ -1018,8 +1073,10 @@ def test_hybrid_cache_sliding_mode(self):\n         input:         [1.0, 2.0, 3.0, 4.0, 5.0, 6.0]\n         result:        [3.0, 4.0, 5.0, 6.0] (keeps last window_size tokens)\n         \"\"\"\n+        config = copy.deepcopy(self.config)\n+        config.layer_types = [\"sliding_attention\"] * config.num_hidden_layers\n         # Scenario 1: Update within window, no slide yet\n-        hybrid_cache = HybridCache(config=self.config, max_batch_size=1, max_cache_len=self.max_cache_len)\n+        hybrid_cache = HybridCache(config=config, max_batch_size=1, max_cache_len=self.max_cache_len)\n         prefill = torch.tensor([1.0, 2.0, 0.0, 0.0])[None, None, :, None]\n         hybrid_cache.update(\n             key_states=prefill,\n@@ -1034,13 +1091,13 @@ def test_hybrid_cache_sliding_mode(self):\n             cache_kwargs={\"cache_position\": torch.tensor([2]), \"sliding_window\": self.window_size},\n         )\n         self.assertEqual(\n-            hybrid_cache.key_cache[0][0, 0, :, 0].tolist(),\n+            hybrid_cache.layers[0].keys[0, 0, :, 0].tolist(),\n             [1.0, 2.0, 3.0, 0.0],\n             \"HybridCache Sliding Scenario 1 failed\",\n         )\n \n         # Scenario 2: Update causing first slide\n-        hybrid_cache = HybridCache(config=self.config, max_batch_size=1, max_cache_len=self.max_cache_len)\n+        hybrid_cache = HybridCache(config=config, max_batch_size=1, max_cache_len=self.max_cache_len)\n         prefill = torch.tensor([1.0, 2.0, 3.0, 4.0])[None, None, :, None]\n         hybrid_cache.update(\n             key_states=prefill,\n@@ -1055,7 +1112,7 @@ def test_hybrid_cache_sliding_mode(self):\n             cache_kwargs={\"cache_position\": torch.tensor([4]), \"sliding_window\": self.window_size},\n         )\n         self.assertEqual(\n-            hybrid_cache.key_cache[0][0, 0, :, 0].tolist(),\n+            hybrid_cache.layers[0].keys[0, 0, :, 0].tolist(),\n             [2.0, 3.0, 4.0, 5.0],\n             \"HybridCache Sliding Scenario 2 failed\",\n         )\n@@ -1068,13 +1125,13 @@ def test_hybrid_cache_sliding_mode(self):\n             cache_kwargs={\"cache_position\": torch.tensor([5]), \"sliding_window\": self.window_size},\n         )\n         self.assertEqual(\n-            hybrid_cache.key_cache[0][0, 0, :, 0].tolist(),\n+            hybrid_cache.layers[0].keys[0, 0, :, 0].tolist(),\n             [3.0, 4.0, 5.0, 6.0],\n             \"HybridCache Sliding Scenario 3 failed\",\n         )\n \n         # Scenario 4: Long prompt handling\n-        hybrid_cache = HybridCache(config=self.config, max_batch_size=1, max_cache_len=self.max_cache_len)\n+        hybrid_cache = HybridCache(config=config, max_batch_size=1, max_cache_len=self.max_cache_len)\n         long_prefill = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0, 6.0])[None, None, :, None]\n         hybrid_cache.update(\n             key_states=long_prefill,\n@@ -1083,7 +1140,278 @@ def test_hybrid_cache_sliding_mode(self):\n             cache_kwargs={\"cache_position\": torch.arange(6), \"sliding_window\": self.window_size},\n         )\n         self.assertEqual(\n-            hybrid_cache.key_cache[0][0, 0, :, 0].tolist(),\n+            hybrid_cache.layers[0].keys[0, 0, :, 0].tolist(),\n             [3.0, 4.0, 5.0, 6.0],\n             \"HybridCache Sliding Scenario 4 failed\",\n         )\n+\n+    def test_dynamic_cache(self):\n+        \"\"\"Test DynamicCache with manually prefilled states and hardcoded assertions.\n+        Scenario 1: prefill and update for one layer\n+        prefill:       [1.0, 2.0]\n+        update pos 2:  [1.0, 2.0, 3.0]\n+        Scenario 2: prefill and update for two layers independently\n+        \"\"\"\n+        prefill = torch.tensor([1.0, 2.0])[None, None, :, None]\n+        update3 = torch.tensor(3.0)[None, None, None, None]\n+        update4 = torch.tensor(4.0)[None, None, None, None]\n+\n+        # Scenario 1: prefill and update for one layer\n+        cache = DynamicCache()\n+        cache.update(prefill, prefill, 0)\n+        cache.update(update3, update3, 0)\n+        self.assertEqual(cache.layers[0].keys[0, 0, :, 0].tolist(), [1.0, 2.0, 3.0], \"DynamicCache Scenario 1 failed\")\n+        cache.update(update4, update4, 0)\n+        self.assertEqual(\n+            cache.layers[0].keys[0, 0, :, 0].tolist(), [1.0, 2.0, 3.0, 4.0], \"DynamicCache Scenario 1 (to 4) failed\"\n+        )\n+\n+        # Scenario 2: prefill and update for two layers independently\n+        prefill1 = torch.tensor([10.0, 20.0])[None, None, :, None]\n+        update3_1 = torch.tensor(30.0)[None, None, None, None]\n+        update4_1 = torch.tensor(40.0)[None, None, None, None]\n+\n+        cache = DynamicCache()\n+        cache.update(prefill, prefill, 0)\n+        cache.update(prefill1, prefill1, 1)\n+\n+        cache.update(update3, update3, 0)\n+        cache.update(update3_1, update3_1, 1)\n+        cache.update(update4, update4, 0)\n+        cache.update(update4_1, update4_1, 1)\n+        self.assertEqual(\n+            cache.layers[0].keys[0, 0, :, 0].tolist(), [1.0, 2.0, 3.0, 4.0], \"DynamicCache Scenario 2 layer 0 failed\"\n+        )\n+        self.assertEqual(\n+            cache.layers[1].keys[0, 0, :, 0].tolist(),\n+            [10.0, 20.0, 30.0, 40.0],\n+            \"DynamicCache Scenario 2 layer 1 failed\",\n+        )\n+\n+    def test_hybrid_cache(self):\n+        \"\"\"\n+        Test HybridCache with a mix of static and sliding layers,\n+        with prefill size bigger than sliding window.\n+\n+        prefill:\n+        static: [1.0, 2.0, 3.0]\n+        sliding: [10.0, 20.0, 30.0]\n+            (stores only [20.0, 30.0])\n+\n+        update pos 4:\n+        static: [1.0, 2.0, 3.0, 5.0]\n+        sliding: [30.0, 50.0]\n+        \"\"\"\n+        config = copy.deepcopy(self.config)\n+        config.num_hidden_layers = 2\n+        config.layer_types = [\"full_attention\", \"sliding_attention\"]\n+        config.sliding_window = 2\n+        hybrid_cache = HybridCache(config=config, max_batch_size=1, max_cache_len=self.max_cache_len)\n+\n+        # Prefill both layers up to cache capacity\n+        prefill_static = torch.tensor([1.0, 2.0, 3.0])[None, None, :, None]\n+        # Sliding window is 2, so it should return full [10.0, 20.0, 30.0], but store only [20.0, 30.0]\n+        prefill_sliding = torch.tensor([10.0, 20.0, 30.0])[None, None, :, None]\n+\n+        # Update static layer (layer 0)\n+        res_static = hybrid_cache.update(\n+            key_states=prefill_static,\n+            value_states=prefill_static,\n+            layer_idx=0,\n+            cache_kwargs={\"cache_position\": torch.arange(3)},\n+        )\n+\n+        # Update sliding layer (layer 1)\n+        res_sliding = hybrid_cache.update(\n+            key_states=prefill_sliding,\n+            value_states=prefill_sliding,\n+            layer_idx=1,\n+            cache_kwargs={\"cache_position\": torch.arange(3), \"sliding_window\": self.window_size},\n+        )\n+\n+        # Verify initial states\n+        self.assertEqual(\n+            hybrid_cache.layers[0].keys[0, 0, :, 0].tolist(),\n+            [1.0, 2.0, 3.0, 0.0],\n+            \"Initial static layer state is wrong\",\n+        )\n+        self.assertEqual(\n+            res_static[0][0, 0, :, 0].tolist(),\n+            [1.0, 2.0, 3.0, 0.0],\n+            \"Static layer did not return the correct value.\",\n+        )\n+        self.assertEqual(\n+            hybrid_cache.layers[1].keys[0, 0, :, 0].tolist(),\n+            [20.0, 30.0],\n+            \"Initial sliding layer state is wrong\",\n+        )\n+        self.assertEqual(\n+            res_sliding[0][0, 0, :, 0].tolist(),\n+            [10.0, 20.0, 30.0],\n+            \"Sliding layer did not return the correct value.\",\n+        )\n+\n+        # Update at position 4\n+        new_key_static = torch.tensor(5.0)[None, None, None, None]\n+        new_key_sliding = torch.tensor(50.0)[None, None, None, None]\n+\n+        # Update static layer (layer 0)\n+        hybrid_cache.update(\n+            key_states=new_key_static,\n+            value_states=new_key_static,\n+            layer_idx=0,\n+            cache_kwargs={\"cache_position\": torch.tensor([3])},\n+        )\n+\n+        # Update sliding layer (layer 1)\n+        hybrid_cache.update(\n+            key_states=new_key_sliding,\n+            value_states=new_key_sliding,\n+            layer_idx=1,\n+            cache_kwargs={\"cache_position\": torch.tensor([3])},\n+        )\n+\n+        # The static layer does not slide, so it should have updated the element at position 3\n+        self.assertEqual(\n+            hybrid_cache.layers[0].keys[0, 0, :, 0].tolist(),\n+            [1.0, 2.0, 3.0, 5.0],\n+            \"Static layer did not update as expected.\",\n+        )\n+\n+        # The sliding layer should have shifted, discarding the first element and adding the new one at the end\n+        self.assertEqual(\n+            hybrid_cache.layers[1].keys[0, 0, :, 0].tolist(),\n+            [30.0, 50.0],\n+            \"Sliding layer did not slide as expected.\",\n+        )\n+\n+    def test_hybrid_chunked_cache(self):\n+        \"\"\"\n+        Test HybridChunkedCache with both static and sliding layers and special cases:\n+            1. a pre-fill longer than the sliding window\n+            2. a single-token decoding step (normal generation)\n+            3. a multi-token decoding step after the window is already full\n+\n+        Sliding-window size: 2\n+        Static layer is full-attention.\n+        \n+        Prefill:\n+            static   : [1, 2, 3]\n+            sliding  : [10, 20, 30]   (cache keeps [20, 30])\n+        +1 token:\n+            static   : [1, 2, 3, 5]\n+            sliding  : [30, 50]       (returned [30, 50])\n+        +2 tokens:\n+            sliding  : [60, 70]       (returned [50, 60, 70])\n+        \"\"\"\n+\n+        config = copy.deepcopy(self.config)\n+        config.num_hidden_layers = 2\n+        config.layer_types = [\"full_attention\", \"sliding_attention\"]\n+        config.sliding_window = 2\n+        max_cache_len = 4\n+        chunked_cache = HybridChunkedCache(config=config, max_batch_size=1, max_cache_len=max_cache_len)\n+\n+        # 1) PREFILL (3 tokens > sliding_window)\n+        prefill_static = torch.tensor([1.0, 2.0, 3.0])[None, None, :, None]\n+        prefill_sliding = torch.tensor([10.0, 20.0, 30.0])[None, None, :, None]\n+\n+        res_static = chunked_cache.update(\n+            key_states=prefill_static,\n+            value_states=prefill_static,\n+            layer_idx=0,\n+            cache_kwargs={\"cache_position\": torch.arange(3)},\n+        )\n+        res_sliding = chunked_cache.update(\n+            key_states=prefill_sliding,\n+            value_states=prefill_sliding,\n+            layer_idx=1,\n+            cache_kwargs={\"cache_position\": torch.arange(3)},\n+        )\n+\n+        # Static layer keeps everything\n+        self.assertEqual(res_static[0][0, 0, :, 0].tolist(), [1.0, 2.0, 3.0, 0.0])\n+        # Sliding layer returned full prompt but stored the tail\n+        self.assertEqual(res_sliding[0][0, 0, :, 0].tolist(), [10.0, 20.0, 30.0])\n+        self.assertEqual(chunked_cache.layers[1].keys[0, 0, :, 0].tolist(), [20.0, 30.0])\n+\n+        # 2) ONE-TOKEN UPDATE (normal decode)\n+        new_static = torch.tensor(5.0)[None, None, None, None]\n+        new_sliding = torch.tensor(50.0)[None, None, None, None]\n+\n+        chunked_cache.update(\n+            key_states=new_static,\n+            value_states=new_static,\n+            layer_idx=0,\n+            cache_kwargs={\"cache_position\": torch.tensor([3])},\n+        )\n+        res_one = chunked_cache.update(\n+            key_states=new_sliding,\n+            value_states=new_sliding,\n+            layer_idx=1,\n+            cache_kwargs={\"cache_position\": torch.tensor([3])},\n+        )\n+\n+        self.assertEqual(chunked_cache.layers[0].keys[0, 0, :, 0].tolist(), [1.0, 2.0, 3.0, 5.0])\n+        self.assertEqual(chunked_cache.layers[1].keys[0, 0, :, 0].tolist(), [30.0, 50.0])\n+        self.assertEqual(res_one[0][0, 0, :, 0].tolist(), [30.0, 50.0])\n+\n+        # 3) TWO-TOKEN UPDATE after window is full\n+        new_sliding_2 = torch.tensor([60.0, 70.0])[None, None, :, None]\n+        res_two = chunked_cache.update(\n+            key_states=new_sliding_2,\n+            value_states=new_sliding_2,\n+            layer_idx=1,\n+            cache_kwargs={\"cache_position\": torch.tensor([4, 5])},  # arbitrary positions; ignored in full mode\n+        )\n+\n+        # Cache now keeps the latest two tokens\n+        self.assertEqual(chunked_cache.layers[1].keys[0, 0, :, 0].tolist(), [60.0, 70.0])\n+        # Returned tensor contains previous last token + new ones\n+        self.assertEqual(res_two[0][0, 0, :, 0].tolist(), [50.0, 60.0, 70.0])\n+\n+    def test_hybrid_chunked_cache_extra_cases(self):\n+        \"\"\"\n+        Covers the new cases that appear on prefill chunking:\n+        1) Not full multi-token update (cache_position[0] + update_len <= max_cache_len)\n+        2) Multi-token update crossing the window (cache_position[0] < max_cache_len  and  cache_position[0] + update_len > max_cache_len)\n+\n+        Single sliding layer, max_cache_len = 3.\n+\n+        Step 0 (prefill 2 tokens, update_len < max_cache_len\n+            cache = [10, 20,  0]         returned [10, 20, 0]\n+\n+        Step 1 (add 2 tokens, p = 2, update_len = 2,  p + update_len = 4 > max_cache_len)\n+            cache = [20, 30, 40]         returned [10, 20, 30, 40]\n+        \"\"\"\n+\n+        config = copy.deepcopy(self.config)\n+        config.num_hidden_layers = 1\n+        config.layer_types = [\"sliding_attention\"]\n+        config.sliding_window = 3\n+        cache = HybridChunkedCache(config, max_batch_size=1, max_cache_len=3)\n+\n+        # Step 0 : multi-token prefill\n+        first_chunk = torch.tensor([10.0, 20.0])[None, None, :, None]  # L = 2\n+        returned_0 = cache.update(\n+            key_states=first_chunk,\n+            value_states=first_chunk,\n+            layer_idx=0,\n+            cache_kwargs={\"cache_position\": torch.arange(2)},  # p = 0,1\n+        )\n+\n+        # internal cache should have first two tokens and a zero pad\n+        self.assertEqual(cache.layers[0].keys[0, 0, :, 0].tolist(), [10.0, 20.0, 0.0])\n+        self.assertEqual(returned_0[0][0, 0, :, 0].tolist(), [10.0, 20.0, 0.0])\n+\n+        # Step 1 : multi-token update crossing the window boundary\n+        second_chunk = torch.tensor([30.0, 40.0])[None, None, :, None]  # L = 2\n+        returned_1 = cache.update(\n+            key_states=second_chunk,\n+            value_states=second_chunk,\n+            layer_idx=0,\n+            cache_kwargs={\"cache_position\": torch.tensor([2, 3])},  # p = 2\n+        )\n+\n+        self.assertEqual(cache.layers[0].keys[0, 0, :, 0].tolist(), [20.0, 30.0, 40.0])\n+        self.assertEqual(returned_1[0][0, 0, :, 0].tolist(), [10.0, 20.0, 30.0, 40.0])"
        }
    ],
    "stats": {
        "total": 4672,
        "additions": 2505,
        "deletions": 2167
    }
}