{
    "author": "gante",
    "message": "Generate tests: modality-agnostic input preparation (#33685)",
    "sha": "d29738f5b45d9c868ef9775388700c8d08b5800d",
    "files": [
        {
            "sha": "54ea0e23b35dc2bf0050fdc6ebb5f15d4f070c84",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 172,
            "deletions": 254,
            "changes": 426,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29738f5b45d9c868ef9775388700c8d08b5800d/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29738f5b45d9c868ef9775388700c8d08b5800d/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=d29738f5b45d9c868ef9775388700c8d08b5800d",
            "patch": "@@ -94,44 +94,42 @@\n class GenerationTesterMixin:\n     model_tester = None\n     all_generative_model_classes = ()\n-    input_name = \"input_ids\"\n     max_new_tokens = 3\n \n-    def _get_input_ids_and_config(self, batch_size=2):\n+    def prepare_config_and_inputs_for_generate(self, batch_size=2):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        # TODO: @raushan or @gante, use `model.main_input_name` as the main input instead of relyinn on `input_ids`\n-        input_ids = inputs_dict.pop(self.input_name)[:batch_size, :]\n-        inputs_dict.pop(\"attention_mask\", None)\n \n-        # we don't want encoder-decoder models to start from filled decoder ids\n-        inputs_dict.pop(\"decoder_input_ids\", None)\n-        inputs_dict.pop(\"decoder_attention_mask\", None)\n-\n-        # we'll set cache use in each test differently\n-        inputs_dict.pop(\"use_cache\", None)\n-\n-        inputs_dict = {\n-            k: v[:batch_size, ...]\n+        # We don't want a few model inputs in our model input dictionary for generation tests\n+        input_keys_to_ignore = [\n+            # we don't want to mask attention heads\n+            \"head_mask\",\n+            \"decoder_head_mask\",\n+            \"cross_attn_head_mask\",\n+            # we don't want encoder-decoder models to start from filled decoder ids\n+            \"decoder_input_ids\",\n+            \"decoder_attention_mask\",\n+            # we'll set cache use in each test differently\n+            \"use_cache\",\n+            # model-specific exceptions should overload/overwrite this function\n+        ]\n+        filtered_inputs_dict = {\n+            k: v[:batch_size, ...] if isinstance(v, torch.Tensor) else v\n             for k, v in inputs_dict.items()\n-            if \"head_mask\" not in k and isinstance(v, torch.Tensor)\n+            if k not in input_keys_to_ignore\n         }\n-        if config.eos_token_id is not None and config.pad_token_id is None:\n-            # hack to allow generate for models such as GPT2 as is done in `generate()`\n-            if isinstance(config.eos_token_id, int):\n-                config.eos_token_id = [config.eos_token_id]\n-            config.pad_token_id = config.eos_token_id[0]\n \n-        if self.has_attentions:\n-            attention_mask = torch.ones_like(input_ids, dtype=torch.long)\n-        else:\n-            attention_mask = None\n-\n-        # It is important set set the eos_token_id to None to ensure that no sequences\n-        # shorter than `max_length` can be generated\n-        config.eos_token_id = None\n-        config.forced_eos_token_id = None\n+        # It is important set `eos_token_id` to `None` to avoid early stopping (would break for length-based checks)\n+        text_gen_config = config.get_text_config(decoder=True)\n+        if text_gen_config.eos_token_id is not None and text_gen_config.pad_token_id is None:\n+            text_gen_config.pad_token_id = (\n+                text_gen_config.eos_token_id\n+                if isinstance(text_gen_config.eos_token_id, int)\n+                else text_gen_config.eos_token_id[0]\n+            )\n+        text_gen_config.eos_token_id = None\n+        text_gen_config.forced_eos_token_id = None\n \n-        return config, input_ids, attention_mask, inputs_dict\n+        return config, filtered_inputs_dict\n \n     def _get_logits_processor_kwargs(self, do_sample=False, config=None):\n         logits_processor_kwargs = {\n@@ -193,8 +191,6 @@ def _get_constrained_beam_kwargs(self, num_return_sequences=1):\n     def _greedy_generate(\n         self,\n         model,\n-        input_ids,\n-        attention_mask,\n         inputs_dict,\n         output_scores=False,\n         output_logits=False,\n@@ -204,9 +200,7 @@ def _greedy_generate(\n         use_cache=True,\n     ):\n         logits_processor_kwargs = self._get_logits_processor_kwargs(do_sample=False, config=model.config)\n-        model_kwargs = {\"attention_mask\": attention_mask} if attention_mask is not None else {}\n         output_generate = model.generate(\n-            input_ids,\n             do_sample=False,\n             num_beams=1,\n             max_new_tokens=self.max_new_tokens,\n@@ -217,7 +211,6 @@ def _greedy_generate(\n             return_dict_in_generate=return_dict_in_generate,\n             use_cache=use_cache,\n             **logits_processor_kwargs,\n-            **model_kwargs,\n             **inputs_dict,\n         )\n \n@@ -226,8 +219,6 @@ def _greedy_generate(\n     def _sample_generate(\n         self,\n         model,\n-        input_ids,\n-        attention_mask,\n         inputs_dict,\n         num_return_sequences,\n         output_scores=False,\n@@ -239,9 +230,7 @@ def _sample_generate(\n     ):\n         torch.manual_seed(0)\n         logits_processor_kwargs = self._get_logits_processor_kwargs(do_sample=True, config=model.config)\n-        model_kwargs = {\"attention_mask\": attention_mask} if attention_mask is not None else {}\n         output_generate = model.generate(\n-            input_ids,\n             do_sample=True,\n             num_beams=1,\n             max_new_tokens=self.max_new_tokens,\n@@ -253,7 +242,6 @@ def _sample_generate(\n             return_dict_in_generate=return_dict_in_generate,\n             use_cache=use_cache,\n             **logits_processor_kwargs,\n-            **model_kwargs,\n             **inputs_dict,\n         )\n \n@@ -262,8 +250,6 @@ def _sample_generate(\n     def _beam_search_generate(\n         self,\n         model,\n-        input_ids,\n-        attention_mask,\n         inputs_dict,\n         beam_kwargs,\n         output_scores=False,\n@@ -274,9 +260,7 @@ def _beam_search_generate(\n         use_cache=True,\n     ):\n         logits_processor_kwargs = self._get_logits_processor_kwargs(do_sample=False, config=model.config)\n-        model_kwargs = {\"attention_mask\": attention_mask} if attention_mask is not None else {}\n         output_generate = model.generate(\n-            input_ids,\n             do_sample=False,\n             max_new_tokens=self.max_new_tokens,\n             output_scores=output_scores,\n@@ -287,7 +271,6 @@ def _beam_search_generate(\n             use_cache=use_cache,\n             **beam_kwargs,\n             **logits_processor_kwargs,\n-            **model_kwargs,\n             **inputs_dict,\n         )\n \n@@ -296,8 +279,6 @@ def _beam_search_generate(\n     def _beam_sample_generate(\n         self,\n         model,\n-        input_ids,\n-        attention_mask,\n         inputs_dict,\n         beam_kwargs,\n         output_scores=False,\n@@ -309,9 +290,7 @@ def _beam_sample_generate(\n     ):\n         torch.manual_seed(0)\n         logits_processor_kwargs = self._get_logits_processor_kwargs(do_sample=True, config=model.config)\n-        model_kwargs = {\"attention_mask\": attention_mask} if attention_mask is not None else {}\n         output_generate = model.generate(\n-            input_ids,\n             do_sample=True,\n             max_new_tokens=self.max_new_tokens,\n             output_scores=output_scores,\n@@ -322,7 +301,6 @@ def _beam_sample_generate(\n             use_cache=use_cache,\n             **beam_kwargs,\n             **logits_processor_kwargs,\n-            **model_kwargs,\n             **inputs_dict,\n         )\n \n@@ -331,8 +309,6 @@ def _beam_sample_generate(\n     def _group_beam_search_generate(\n         self,\n         model,\n-        input_ids,\n-        attention_mask,\n         inputs_dict,\n         beam_kwargs,\n         output_scores=False,\n@@ -343,9 +319,7 @@ def _group_beam_search_generate(\n         use_cache=True,\n     ):\n         logits_processor_kwargs = self._get_logits_processor_kwargs(do_sample=False, config=model.config)\n-        model_kwargs = {\"attention_mask\": attention_mask} if attention_mask is not None else {}\n         output_generate = model.generate(\n-            input_ids,\n             do_sample=False,\n             max_new_tokens=self.max_new_tokens,\n             output_scores=output_scores,\n@@ -356,7 +330,6 @@ def _group_beam_search_generate(\n             use_cache=use_cache,\n             **beam_kwargs,\n             **logits_processor_kwargs,\n-            **model_kwargs,\n             **inputs_dict,\n         )\n \n@@ -365,8 +338,6 @@ def _group_beam_search_generate(\n     def _constrained_beam_search_generate(\n         self,\n         model,\n-        input_ids,\n-        attention_mask,\n         inputs_dict,\n         constraints,\n         beam_kwargs,\n@@ -378,9 +349,7 @@ def _constrained_beam_search_generate(\n         use_cache=True,\n     ):\n         logits_processor_kwargs = self._get_logits_processor_kwargs(do_sample=False, config=model.config)\n-        model_kwargs = {\"attention_mask\": attention_mask} if attention_mask is not None else {}\n         output_generate = model.generate(\n-            input_ids,\n             do_sample=False,\n             max_new_tokens=self.max_new_tokens,\n             output_scores=output_scores,\n@@ -392,7 +361,6 @@ def _constrained_beam_search_generate(\n             use_cache=use_cache,\n             **beam_kwargs,\n             **logits_processor_kwargs,\n-            **model_kwargs,\n             **inputs_dict,\n         )\n \n@@ -401,8 +369,6 @@ def _constrained_beam_search_generate(\n     def _contrastive_generate(\n         self,\n         model,\n-        input_ids,\n-        attention_mask,\n         inputs_dict,\n         output_scores=False,\n         output_logits=False,\n@@ -417,9 +383,7 @@ def _contrastive_generate(\n         }\n \n         logits_processor_kwargs = self._get_logits_processor_kwargs(do_sample=False, config=model.config)\n-        model_kwargs = {\"attention_mask\": attention_mask} if attention_mask is not None else {}\n         output_generate = model.generate(\n-            input_ids,\n             do_sample=False,\n             num_beams=1,\n             max_new_tokens=self.max_new_tokens,\n@@ -430,7 +394,6 @@ def _contrastive_generate(\n             return_dict_in_generate=return_dict_in_generate,\n             use_cache=use_cache,\n             **logits_processor_kwargs,\n-            **model_kwargs,\n             **contrastive_search_kwargs,\n             **inputs_dict,\n         )\n@@ -440,28 +403,26 @@ def _contrastive_generate(\n     @pytest.mark.generate\n     def test_greedy_generate(self):\n         for model_class in self.all_generative_model_classes:\n-            config, input_ids, attention_mask, inputs_dict = self._get_input_ids_and_config()\n+            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n+            main_input = inputs_dict[model_class.main_input_name]\n \n             model = model_class(config).to(torch_device).eval()\n-            output_generate = self._greedy_generate(\n-                model=model, input_ids=input_ids, attention_mask=attention_mask, inputs_dict=inputs_dict\n-            )\n+            output_generate = self._greedy_generate(model=model, inputs_dict=inputs_dict)\n \n             if model.config.is_encoder_decoder:\n                 self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + 1)\n             else:\n-                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + input_ids.shape[-1])\n+                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + main_input.shape[-1])\n \n     @pytest.mark.generate\n     def test_greedy_generate_dict_outputs(self):\n         for model_class in self.all_generative_model_classes:\n-            config, input_ids, attention_mask, inputs_dict = self._get_input_ids_and_config()\n+            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n+            main_input = inputs_dict[model_class.main_input_name]\n \n             model = model_class(config).to(torch_device).eval()\n             output_generate = self._greedy_generate(\n                 model=model,\n-                input_ids=input_ids,\n-                attention_mask=attention_mask,\n                 inputs_dict=inputs_dict,\n                 output_scores=True,\n                 output_logits=True,\n@@ -477,17 +438,18 @@ def test_greedy_generate_dict_outputs(self):\n                 # Retrocompatibility check\n                 self.assertIsInstance(output_generate, GreedySearchEncoderDecoderOutput)\n             else:\n-                self.assertTrue(output_generate.sequences.shape[-1] == self.max_new_tokens + input_ids.shape[-1])\n+                self.assertTrue(output_generate.sequences.shape[-1] == self.max_new_tokens + main_input.shape[-1])\n                 self.assertIsInstance(output_generate, GenerateDecoderOnlyOutput)\n                 # Retrocompatibility check\n                 self.assertIsInstance(output_generate, GreedySearchDecoderOnlyOutput)\n \n-            self._check_outputs(output_generate, input_ids, model.config)\n+            self._check_outputs(output_generate, main_input, model.config)\n \n     @pytest.mark.generate\n     def test_greedy_generate_dict_outputs_use_cache(self):\n         for model_class in self.all_generative_model_classes:\n-            config, input_ids, attention_mask, inputs_dict = self._get_input_ids_and_config()\n+            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n+            main_input = inputs_dict[model_class.main_input_name]\n \n             if not hasattr(config, \"use_cache\"):\n                 self.skipTest(reason=f\"{model_class.__name__} doesn't support caching\")\n@@ -498,53 +460,45 @@ def test_greedy_generate_dict_outputs_use_cache(self):\n             model = model_class(config).to(torch_device).eval()\n             output_generate = self._greedy_generate(\n                 model=model,\n-                input_ids=input_ids,\n-                attention_mask=attention_mask,\n                 inputs_dict=inputs_dict,\n                 output_scores=True,\n                 output_logits=True,\n                 output_hidden_states=True,\n                 output_attentions=self.has_attentions,\n                 return_dict_in_generate=True,\n-                use_cache=True,\n+                use_cache=True,  # Enable cache\n             )\n \n             if model.config.is_encoder_decoder:\n                 self.assertTrue(output_generate.sequences.shape[-1] == self.max_new_tokens + 1)\n             else:\n-                self.assertTrue(output_generate.sequences.shape[-1] == self.max_new_tokens + input_ids.shape[-1])\n+                self.assertTrue(output_generate.sequences.shape[-1] == self.max_new_tokens + main_input.shape[-1])\n \n-            self._check_outputs(output_generate, input_ids, model.config, use_cache=True)\n+            self._check_outputs(output_generate, main_input, model.config, use_cache=True)\n \n     @pytest.mark.generate\n     def test_sample_generate(self):\n         for model_class in self.all_generative_model_classes:\n-            config, input_ids, attention_mask, inputs_dict = self._get_input_ids_and_config()\n+            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n+            main_input = inputs_dict[model_class.main_input_name]\n \n             model = model_class(config).to(torch_device).eval()\n-            output_generate = self._sample_generate(\n-                model=model,\n-                input_ids=input_ids,\n-                attention_mask=attention_mask,\n-                inputs_dict=inputs_dict,\n-                num_return_sequences=1,\n-            )\n+            output_generate = self._sample_generate(model=model, inputs_dict=inputs_dict, num_return_sequences=1)\n \n             if model.config.is_encoder_decoder:\n                 self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + 1)\n             else:\n-                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + input_ids.shape[-1])\n+                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + main_input.shape[-1])\n \n     @pytest.mark.generate\n     def test_sample_generate_dict_output(self):\n         for model_class in self.all_generative_model_classes:\n-            config, input_ids, attention_mask, inputs_dict = self._get_input_ids_and_config()\n+            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n+            main_input = inputs_dict[model_class.main_input_name]\n \n             model = model_class(config).to(torch_device).eval()\n             output_generate = self._sample_generate(\n                 model=model,\n-                input_ids=input_ids,\n-                attention_mask=attention_mask,\n                 inputs_dict=inputs_dict,\n                 num_return_sequences=2,\n                 output_scores=True,\n@@ -561,45 +515,39 @@ def test_sample_generate_dict_output(self):\n                 # Retrocompatibility check\n                 self.assertIsInstance(output_generate, SampleEncoderDecoderOutput)\n             else:\n-                self.assertTrue(output_generate.sequences.shape[-1] == self.max_new_tokens + input_ids.shape[-1])\n+                self.assertTrue(output_generate.sequences.shape[-1] == self.max_new_tokens + main_input.shape[-1])\n                 self.assertIsInstance(output_generate, GenerateDecoderOnlyOutput)\n                 # Retrocompatibility check\n                 self.assertIsInstance(output_generate, SampleDecoderOnlyOutput)\n \n-            self._check_outputs(output_generate, input_ids, model.config, num_return_sequences=2)\n+            self._check_outputs(output_generate, main_input, model.config, num_return_sequences=2)\n \n     @pytest.mark.generate\n     def test_beam_search_generate(self):\n         for model_class in self.all_generative_model_classes:\n-            config, input_ids, attention_mask, inputs_dict = self._get_input_ids_and_config()\n+            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n+            main_input = inputs_dict[model_class.main_input_name]\n \n             model = model_class(config).to(torch_device).eval()\n \n             beam_kwargs = self._get_beam_kwargs()\n-            output_generate = self._beam_search_generate(\n-                model=model,\n-                input_ids=input_ids,\n-                attention_mask=attention_mask,\n-                inputs_dict=inputs_dict,\n-                beam_kwargs=beam_kwargs,\n-            )\n+            output_generate = self._beam_search_generate(model=model, inputs_dict=inputs_dict, beam_kwargs=beam_kwargs)\n \n             if model.config.is_encoder_decoder:\n                 self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + 1)\n             else:\n-                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + input_ids.shape[-1])\n+                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + main_input.shape[-1])\n \n     @pytest.mark.generate\n     def test_beam_search_generate_dict_output(self):\n         for model_class in self.all_generative_model_classes:\n-            config, input_ids, attention_mask, inputs_dict = self._get_input_ids_and_config()\n+            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n+            main_input = inputs_dict[model_class.main_input_name]\n \n             model = model_class(config).to(torch_device).eval()\n             beam_kwargs = self._get_beam_kwargs()\n             output_generate = self._beam_search_generate(\n                 model=model,\n-                input_ids=input_ids,\n-                attention_mask=attention_mask,\n                 inputs_dict=inputs_dict,\n                 beam_kwargs=beam_kwargs,\n                 output_scores=True,\n@@ -615,20 +563,20 @@ def test_beam_search_generate_dict_output(self):\n                 # Retrocompatibility check\n                 self.assertIsInstance(output_generate, BeamSearchEncoderDecoderOutput)\n             else:\n-                self.assertTrue(output_generate.sequences.shape[-1] == self.max_new_tokens + input_ids.shape[-1])\n+                self.assertTrue(output_generate.sequences.shape[-1] == self.max_new_tokens + main_input.shape[-1])\n                 self.assertIsInstance(output_generate, GenerateBeamDecoderOnlyOutput)\n                 # Retrocompatibility check\n                 self.assertIsInstance(output_generate, BeamSearchDecoderOnlyOutput)\n \n             self._check_outputs(\n-                output_generate, input_ids, model.config, num_return_sequences=beam_kwargs[\"num_beams\"]\n+                output_generate, main_input, model.config, num_return_sequences=beam_kwargs[\"num_beams\"]\n             )\n \n     @pytest.mark.generate\n     def test_beam_search_generate_dict_outputs_use_cache(self):\n         for model_class in self.all_generative_model_classes:\n-            # enable cache\n-            config, input_ids, attention_mask, inputs_dict = self._get_input_ids_and_config()\n+            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n+            main_input = inputs_dict[model_class.main_input_name]\n \n             if not hasattr(config, \"use_cache\"):\n                 self.skipTest(reason=f\"{model_class.__name__} doesn't support caching\")\n@@ -642,25 +590,27 @@ def test_beam_search_generate_dict_outputs_use_cache(self):\n             model = model_class(config).to(torch_device).eval()\n             output_generate = self._beam_search_generate(\n                 model=model,\n-                input_ids=input_ids,\n-                attention_mask=attention_mask,\n                 inputs_dict=inputs_dict,\n                 beam_kwargs=beam_kwargs,\n                 output_scores=True,\n                 output_logits=True,\n                 output_hidden_states=True,\n                 output_attentions=self.has_attentions,\n                 return_dict_in_generate=True,\n-                use_cache=True,\n+                use_cache=True,  # Enable cache\n             )\n \n             if model.config.is_encoder_decoder:\n                 self.assertTrue(output_generate.sequences.shape[-1] == self.max_new_tokens + 1)\n             else:\n-                self.assertTrue(output_generate.sequences.shape[-1] == self.max_new_tokens + input_ids.shape[-1])\n+                self.assertTrue(output_generate.sequences.shape[-1] == self.max_new_tokens + main_input.shape[-1])\n \n             self._check_outputs(\n-                output_generate, input_ids, model.config, use_cache=True, num_return_sequences=beam_kwargs[\"num_beams\"]\n+                output_generate,\n+                main_input,\n+                model.config,\n+                use_cache=True,\n+                num_return_sequences=beam_kwargs[\"num_beams\"],\n             )\n \n     @require_accelerate\n@@ -674,16 +624,14 @@ def test_model_parallel_beam_search(self):\n             if model_class._no_split_modules is None:\n                 continue\n \n-            config, input_ids, attention_mask, inputs_dict = self._get_input_ids_and_config()\n+            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n \n             model = model_class(config).eval()\n             with tempfile.TemporaryDirectory() as tmp_dir:\n                 model.cpu().save_pretrained(tmp_dir)\n                 new_model = model_class.from_pretrained(tmp_dir, device_map=\"auto\")\n \n                 new_model.generate(\n-                    input_ids,\n-                    attention_mask=attention_mask,\n                     max_new_tokens=self.max_new_tokens,\n                     num_beams=2,\n                     **inputs_dict,\n@@ -692,22 +640,21 @@ def test_model_parallel_beam_search(self):\n     @pytest.mark.generate\n     def test_beam_sample_generate(self):\n         for model_class in self.all_generative_model_classes:\n-            config, input_ids, attention_mask, inputs_dict = self._get_input_ids_and_config()\n+            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n+            main_input = inputs_dict[model_class.main_input_name]\n \n             model = model_class(config).to(torch_device).eval()\n             beam_kwargs = self._get_beam_kwargs()\n             output_generate = self._beam_sample_generate(\n                 model=model,\n-                input_ids=input_ids,\n-                attention_mask=attention_mask,\n                 inputs_dict=inputs_dict,\n                 beam_kwargs=beam_kwargs,\n             )\n \n             if model.config.is_encoder_decoder:\n                 self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + 1)\n             else:\n-                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + input_ids.shape[-1])\n+                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + main_input.shape[-1])\n \n             # for VLMs inputs embeds won't match input ids unless images are encoded and merged with ids properly\n             # no quick fix available, since obtaining image embeddings step is very model-specific\n@@ -721,12 +668,11 @@ def test_beam_sample_generate(self):\n                     \"inputs_embeds\" in prepare_inputs_for_generation_args\n                     and \"cache_positions\" in prepare_inputs_for_generation_args\n                 ):\n-                    input_embeds = model.get_input_embeddings()(input_ids)\n+                    input_embeds = model.get_input_embeddings()(inputs_dict[\"input_ids\"])\n                     beam_kwargs.update({\"inputs_embeds\": input_embeds})\n                     output_generate2 = self._beam_sample_generate(\n                         model=model,\n                         input_ids=None,\n-                        attention_mask=attention_mask,\n                         inputs_dict={},\n                         beam_kwargs=beam_kwargs,\n                     )\n@@ -736,15 +682,14 @@ def test_beam_sample_generate(self):\n     @pytest.mark.generate\n     def test_beam_sample_generate_dict_output(self):\n         for model_class in self.all_generative_model_classes:\n-            config, input_ids, attention_mask, inputs_dict = self._get_input_ids_and_config()\n+            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n+            main_input = inputs_dict[model_class.main_input_name]\n \n             model = model_class(config).to(torch_device).eval()\n             beam_kwargs = self._get_beam_kwargs()\n \n             output_generate = self._beam_sample_generate(\n                 model=model,\n-                input_ids=input_ids,\n-                attention_mask=attention_mask,\n                 inputs_dict=inputs_dict,\n                 beam_kwargs=beam_kwargs,\n                 output_scores=True,\n@@ -761,18 +706,18 @@ def test_beam_sample_generate_dict_output(self):\n                 # Retrocompatibility check\n                 self.assertIsInstance(output_generate, BeamSampleEncoderDecoderOutput)\n             else:\n-                self.assertTrue(output_generate.sequences.shape[-1] == self.max_new_tokens + input_ids.shape[-1])\n+                self.assertTrue(output_generate.sequences.shape[-1] == self.max_new_tokens + main_input.shape[-1])\n                 self.assertIsInstance(output_generate, GenerateBeamDecoderOnlyOutput)\n                 # Retrocompatibility check\n                 self.assertIsInstance(output_generate, BeamSampleDecoderOnlyOutput)\n \n             self._check_outputs(\n-                output_generate, input_ids, model.config, num_return_sequences=beam_kwargs[\"num_beams\"]\n+                output_generate, main_input, model.config, num_return_sequences=beam_kwargs[\"num_beams\"]\n             )\n \n     @pytest.mark.generate\n     def test_generate_without_input_ids(self):\n-        config, _, _, _ = self._get_input_ids_and_config()\n+        config, _ = self.prepare_config_and_inputs_for_generate()\n \n         # if no bos token id => cannot generate from None\n         if config.bos_token_id is None:\n@@ -794,49 +739,45 @@ def test_generate_without_input_ids(self):\n     @pytest.mark.generate\n     def test_group_beam_search_generate(self):\n         for model_class in self.all_generative_model_classes:\n-            config, input_ids, attention_mask, inputs_dict = self._get_input_ids_and_config()\n+            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n+            main_input = inputs_dict[model_class.main_input_name]\n \n             model = model_class(config).to(torch_device).eval()\n             # check `generate()` and `group_beam_search()` are equal\n             beam_kwargs = self._get_diverse_beam_kwargs()\n             output_generate = self._group_beam_search_generate(\n                 model=model,\n-                input_ids=input_ids,\n-                attention_mask=attention_mask,\n                 inputs_dict=inputs_dict,\n                 beam_kwargs=beam_kwargs,\n             )\n             if model.config.is_encoder_decoder:\n                 self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + 1)\n             else:\n-                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + input_ids.shape[-1])\n+                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + main_input.shape[-1])\n \n             # check `group_beam_search` for higher than 1 `num_return_sequences`\n             num_return_sequences = 2\n             beam_kwargs = self._get_diverse_beam_kwargs(num_return_sequences=num_return_sequences)\n             output_generate = self._group_beam_search_generate(\n                 model=model,\n-                input_ids=input_ids,\n-                attention_mask=attention_mask,\n                 inputs_dict=inputs_dict,\n                 beam_kwargs=beam_kwargs,\n             )\n             if model.config.is_encoder_decoder:\n                 self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + 1)\n             else:\n-                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + input_ids.shape[-1])\n+                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + main_input.shape[-1])\n \n     @pytest.mark.generate\n     def test_group_beam_search_generate_dict_output(self):\n         for model_class in self.all_generative_model_classes:\n-            config, input_ids, attention_mask, inputs_dict = self._get_input_ids_and_config()\n+            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n+            main_input = inputs_dict[model_class.main_input_name]\n \n             model = model_class(config).to(torch_device).eval()\n             beam_kwargs = self._get_diverse_beam_kwargs()\n             output_generate = self._group_beam_search_generate(\n                 model=model,\n-                input_ids=input_ids,\n-                attention_mask=attention_mask,\n                 inputs_dict=inputs_dict,\n                 beam_kwargs=beam_kwargs,\n                 output_scores=True,\n@@ -852,21 +793,22 @@ def test_group_beam_search_generate_dict_output(self):\n                 # Retrocompatibility check\n                 self.assertIsInstance(output_generate, BeamSearchEncoderDecoderOutput)\n             else:\n-                self.assertTrue(output_generate.sequences.shape[-1] == self.max_new_tokens + input_ids.shape[-1])\n+                self.assertTrue(output_generate.sequences.shape[-1] == self.max_new_tokens + main_input.shape[-1])\n                 self.assertIsInstance(output_generate, GenerateBeamDecoderOnlyOutput)\n                 # Retrocompatibility check\n                 self.assertIsInstance(output_generate, BeamSearchDecoderOnlyOutput)\n \n             self._check_outputs(\n-                output_generate, input_ids, model.config, num_return_sequences=beam_kwargs[\"num_beams\"]\n+                output_generate, main_input, model.config, num_return_sequences=beam_kwargs[\"num_beams\"]\n             )\n \n-    # TODO: @gante\n+    # TODO: @gante check why it is flaky\n     @is_flaky()\n     @pytest.mark.generate\n     def test_constrained_beam_search_generate(self):\n         for model_class in self.all_generative_model_classes:\n-            config, input_ids, attention_mask, inputs_dict = self._get_input_ids_and_config()\n+            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n+            main_input = inputs_dict[model_class.main_input_name]\n \n             model = model_class(config).to(torch_device).eval()\n \n@@ -882,8 +824,6 @@ def test_constrained_beam_search_generate(self):\n             beam_kwargs = self._get_constrained_beam_kwargs()\n             output_generate = self._constrained_beam_search_generate(\n                 model=model,\n-                input_ids=input_ids,\n-                attention_mask=attention_mask,\n                 inputs_dict=inputs_dict,\n                 constraints=constraints,\n                 beam_kwargs=beam_kwargs,\n@@ -892,7 +832,7 @@ def test_constrained_beam_search_generate(self):\n             if model.config.is_encoder_decoder:\n                 self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + 1)\n             else:\n-                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + input_ids.shape[-1])\n+                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + main_input.shape[-1])\n \n             for generation_output in output_generate:\n                 self._check_sequence_inside_sequence(force_tokens, generation_output)\n@@ -908,8 +848,6 @@ def test_constrained_beam_search_generate(self):\n \n             output_generate = self._constrained_beam_search_generate(\n                 model=model,\n-                input_ids=input_ids,\n-                attention_mask=attention_mask,\n                 inputs_dict=inputs_dict,\n                 constraints=constraints,\n                 beam_kwargs=beam_kwargs,\n@@ -918,15 +856,16 @@ def test_constrained_beam_search_generate(self):\n             if model.config.is_encoder_decoder:\n                 self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + 1)\n             else:\n-                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + input_ids.shape[-1])\n+                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + main_input.shape[-1])\n \n             for generation_output in output_generate:\n                 self._check_sequence_inside_sequence(force_tokens, generation_output)\n \n     @pytest.mark.generate\n     def test_constrained_beam_search_generate_dict_output(self):\n         for model_class in self.all_generative_model_classes:\n-            config, input_ids, attention_mask, inputs_dict = self._get_input_ids_and_config()\n+            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n+            main_input = inputs_dict[model_class.main_input_name]\n \n             model = model_class(config).to(torch_device).eval()\n \n@@ -941,8 +880,6 @@ def test_constrained_beam_search_generate_dict_output(self):\n             beam_kwargs = self._get_constrained_beam_kwargs()\n             output_generate = self._constrained_beam_search_generate(\n                 model=model,\n-                input_ids=input_ids,\n-                attention_mask=attention_mask,\n                 inputs_dict=inputs_dict,\n                 constraints=constraints,\n                 beam_kwargs=beam_kwargs,\n@@ -960,13 +897,13 @@ def test_constrained_beam_search_generate_dict_output(self):\n                 # Retrocompatibility check\n                 self.assertIsInstance(output_generate, BeamSearchEncoderDecoderOutput)\n             else:\n-                self.assertTrue(output_generate.sequences.shape[-1] == self.max_new_tokens + input_ids.shape[-1])\n+                self.assertTrue(output_generate.sequences.shape[-1] == self.max_new_tokens + main_input.shape[-1])\n                 self.assertIsInstance(output_generate, GenerateBeamDecoderOnlyOutput)\n                 # Retrocompatibility check\n                 self.assertIsInstance(output_generate, BeamSearchDecoderOnlyOutput)\n \n             self._check_outputs(\n-                output_generate, input_ids, model.config, num_return_sequences=beam_kwargs[\"num_beams\"]\n+                output_generate, main_input, model.config, num_return_sequences=beam_kwargs[\"num_beams\"]\n             )\n \n     @pytest.mark.generate\n@@ -979,7 +916,8 @@ def test_contrastive_generate(self):\n             if any(model_name in model_class.__name__.lower() for model_name in [\"fsmt\", \"reformer\"]):\n                 self.skipTest(reason=\"Won't fix: old model with different cache format\")\n \n-            config, input_ids, attention_mask, inputs_dict = self._get_input_ids_and_config()\n+            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n+            main_input = inputs_dict[model_class.main_input_name]\n \n             # NOTE: contrastive search only works with cache on at the moment.\n             if not hasattr(config, \"use_cache\"):\n@@ -990,15 +928,13 @@ def test_contrastive_generate(self):\n             model = model_class(config).to(torch_device).eval()\n             output_generate = self._contrastive_generate(\n                 model=model,\n-                input_ids=input_ids,\n-                attention_mask=attention_mask,\n                 inputs_dict=inputs_dict,\n-                use_cache=True,\n+                use_cache=True,  # Enable cache\n             )\n             if model.config.is_encoder_decoder:\n                 self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + 1)\n             else:\n-                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + input_ids.shape[-1])\n+                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + main_input.shape[-1])\n \n     @pytest.mark.generate\n     def test_contrastive_generate_dict_outputs_use_cache(self):\n@@ -1010,7 +946,8 @@ def test_contrastive_generate_dict_outputs_use_cache(self):\n             if any(model_name in model_class.__name__.lower() for model_name in [\"fsmt\", \"reformer\"]):\n                 self.skipTest(reason=\"Won't fix: old model with different cache format\")\n \n-            config, input_ids, attention_mask, inputs_dict = self._get_input_ids_and_config()\n+            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n+            main_input = inputs_dict[model_class.main_input_name]\n \n             # NOTE: contrastive search only works with cache on at the moment.\n             if not hasattr(config, \"use_cache\"):\n@@ -1020,23 +957,21 @@ def test_contrastive_generate_dict_outputs_use_cache(self):\n             model = model_class(config).to(torch_device).eval()\n             output_generate = self._contrastive_generate(\n                 model=model,\n-                input_ids=input_ids,\n-                attention_mask=attention_mask,\n                 inputs_dict=inputs_dict,\n                 output_scores=True,\n                 output_logits=True,\n                 output_hidden_states=True,\n                 output_attentions=self.has_attentions,\n                 return_dict_in_generate=True,\n-                use_cache=True,\n+                use_cache=True,  # Enable cache\n             )\n \n             if model.config.is_encoder_decoder:\n                 self.assertTrue(output_generate.sequences.shape[-1] == self.max_new_tokens + 1)\n             else:\n-                self.assertTrue(output_generate.sequences.shape[-1] == self.max_new_tokens + input_ids.shape[-1])\n+                self.assertTrue(output_generate.sequences.shape[-1] == self.max_new_tokens + main_input.shape[-1])\n \n-            self._check_outputs(output_generate, input_ids, model.config, use_cache=True)\n+            self._check_outputs(output_generate, main_input, model.config, use_cache=True)\n \n     @pytest.mark.generate\n     def test_contrastive_generate_low_memory(self):\n@@ -1050,7 +985,7 @@ def test_contrastive_generate_low_memory(self):\n             if any(model_name in model_class.__name__.lower() for model_name in [\"gptbigcode\"]):\n                 self.skipTest(reason=\"TODO: fix me\")\n \n-            config, input_ids, attention_mask, inputs_dict = self._get_input_ids_and_config(batch_size=1)\n+            config, inputs_dict = self.prepare_config_and_inputs_for_generate(batch_size=1)\n \n             # NOTE: contrastive search only works with cache on at the moment.\n             if not hasattr(config, \"use_cache\"):\n@@ -1062,23 +997,19 @@ def test_contrastive_generate_low_memory(self):\n             model = model_class(config).to(torch_device).eval()\n \n             low_output = model.generate(\n-                input_ids,\n                 top_k=4,\n                 penalty_alpha=0.6,\n                 low_memory=True,\n                 max_new_tokens=self.max_new_tokens,\n-                attention_mask=attention_mask,\n                 **inputs_dict,\n                 use_cache=True,\n             )\n \n             high_output = model.generate(\n-                input_ids,\n                 top_k=4,\n                 penalty_alpha=0.6,\n                 low_memory=False,\n                 max_new_tokens=self.max_new_tokens,\n-                attention_mask=attention_mask,\n                 **inputs_dict,\n                 use_cache=True,\n             )\n@@ -1105,7 +1036,8 @@ def test_beam_search_low_memory(self):\n                 ]\n             ):\n                 self.skipTest(reason=\"May fix in the future: need model-specific fixes\")\n-            config, input_ids, _, _ = self._get_input_ids_and_config(batch_size=2)\n+\n+            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n             # batch_size=1 is ok, but batch_size>1 will cause non-identical output\n \n             config.use_cache = True\n@@ -1115,7 +1047,7 @@ def test_beam_search_low_memory(self):\n             model = model_class(config).to(torch_device).eval()\n \n             low_output = model.generate(\n-                input_ids,\n+                **inputs_dict,\n                 max_new_tokens=8,\n                 num_beams=5,\n                 early_stopping=True,\n@@ -1124,7 +1056,7 @@ def test_beam_search_low_memory(self):\n             )\n \n             high_output = model.generate(\n-                input_ids,\n+                **inputs_dict,\n                 max_new_tokens=8,\n                 num_beams=5,\n                 early_stopping=True,\n@@ -1169,7 +1101,8 @@ def test_assisted_decoding_matches_greedy_search(self, assistant_type):\n                 self.skipTest(reason=\"May fix in the future: need model-specific fixes\")\n \n             # enable cache\n-            config, input_ids, attention_mask, inputs_dict = self._get_input_ids_and_config(batch_size=1)\n+            config, inputs_dict = self.prepare_config_and_inputs_for_generate(batch_size=1)\n+            main_input = inputs_dict[model_class.main_input_name]\n \n             # NOTE: assisted generation only works with cache on at the moment.\n             if not hasattr(config, \"use_cache\"):\n@@ -1195,9 +1128,7 @@ def test_assisted_decoding_matches_greedy_search(self, assistant_type):\n                 \"return_dict_in_generate\": True,\n                 \"use_cache\": True,\n             }\n-            output_greedy = model.generate(\n-                input_ids, attention_mask=attention_mask, **generation_kwargs, **inputs_dict\n-            )\n+            output_greedy = model.generate(**generation_kwargs, **inputs_dict)\n \n             # test with the same assistant model or randomly init one\n             # in the first case all candidate tokens are accepted, in the second none is accepted\n@@ -1209,15 +1140,13 @@ def test_assisted_decoding_matches_greedy_search(self, assistant_type):\n             assistant_model.generation_config.num_assistant_tokens = 2  # see b)\n             assistant_model.generation_config.num_assistant_tokens_schedule = \"constant\"  # see b)\n             generation_kwargs.update({\"assistant_model\": assistant_model})\n-            output_assisted = model.generate(\n-                input_ids, attention_mask=attention_mask, **generation_kwargs, **inputs_dict\n-            )\n+            output_assisted = model.generate(**generation_kwargs, **inputs_dict)\n \n             # The two outputs must match and their shape must be as expected\n \n             self.assertListEqual(output_greedy.sequences.tolist(), output_assisted.sequences.tolist())\n             for output in (output_greedy, output_assisted):\n-                self._check_outputs(output, input_ids, model.config, use_cache=True)\n+                self._check_outputs(output, main_input, model.config, use_cache=True)\n \n     @is_flaky()\n     @pytest.mark.generate\n@@ -1246,7 +1175,8 @@ def test_prompt_lookup_decoding_matches_greedy_search(self):\n                 self.skipTest(reason=\"May fix in the future: need model-specific fixes\")\n \n             # enable cache\n-            config, input_ids, attention_mask, inputs_dict = self._get_input_ids_and_config(batch_size=1)\n+            config, inputs_dict = self.prepare_config_and_inputs_for_generate(batch_size=1)\n+            main_input = inputs_dict[model_class.main_input_name]\n \n             # NOTE: assisted generation only works with cache on at the moment.\n             if not hasattr(config, \"use_cache\"):\n@@ -1273,20 +1203,16 @@ def test_prompt_lookup_decoding_matches_greedy_search(self):\n                 \"use_cache\": True,\n             }\n \n-            output_greedy = model.generate(\n-                input_ids, attention_mask=attention_mask, **generation_kwargs, **inputs_dict\n-            )\n+            output_greedy = model.generate(**generation_kwargs, **inputs_dict)\n \n             generation_kwargs.update({\"prompt_lookup_num_tokens\": 2})  # see b)\n-            output_prompt_lookup = model.generate(\n-                input_ids, attention_mask=attention_mask, **generation_kwargs, **inputs_dict\n-            )\n+            output_prompt_lookup = model.generate(**generation_kwargs, **inputs_dict)\n \n             # The two outputs must match and their shape must be as expected\n \n             self.assertListEqual(output_greedy.sequences.tolist(), output_prompt_lookup.sequences.tolist())\n             for output in (output_greedy, output_prompt_lookup):\n-                self._check_outputs(output, input_ids, model.config, use_cache=True)\n+                self._check_outputs(output, main_input, model.config, use_cache=True)\n \n     @pytest.mark.generate\n     def test_dola_decoding_sample(self):\n@@ -1302,7 +1228,8 @@ def test_dola_decoding_sample(self):\n                 self.skipTest(\"DoLa is not supported for models that don't return layerwise hidden states\")\n \n             # enable cache if the model is not openai-gpt, xlnet, cpm, or xlm\n-            config, input_ids, attention_mask, inputs_dict = self._get_input_ids_and_config()\n+            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n+            main_input = inputs_dict[model_class.main_input_name]\n \n             # Encoder-decoder models are not supported\n             if config.is_encoder_decoder:\n@@ -1326,12 +1253,11 @@ def test_dola_decoding_sample(self):\n                 \"output_hidden_states\": True,\n                 \"output_attentions\": self.has_attentions,\n                 \"return_dict_in_generate\": True,\n-                \"use_cache\": hasattr(config, \"use_cache\"),  # Some models don't support the cache\n+                \"use_cache\": getattr(config, \"use_cache\", False),  # Some models don't support the cache\n+                \"dola_layers\": \"low\",\n             }\n-            generation_kwargs.update({\"dola_layers\": \"low\"})\n-            model_kwargs = {\"attention_mask\": attention_mask} if attention_mask is not None else {}\n-            output_dola = model.generate(input_ids, **model_kwargs, **generation_kwargs, **inputs_dict)\n-            self._check_outputs(output_dola, input_ids, model.config, use_cache=hasattr(config, \"use_cache\"))\n+            output_dola = model.generate(**generation_kwargs, **inputs_dict)\n+            self._check_outputs(output_dola, main_input, model.config, use_cache=getattr(config, \"use_cache\", False))\n \n     @pytest.mark.generate\n     def test_assisted_decoding_sample(self):\n@@ -1359,7 +1285,8 @@ def test_assisted_decoding_sample(self):\n                 self.skipTest(reason=\"May fix in the future: need model-specific fixes\")\n \n             # enable cache\n-            config, input_ids, attention_mask, inputs_dict = self._get_input_ids_and_config(batch_size=1)\n+            config, inputs_dict = self.prepare_config_and_inputs_for_generate(batch_size=1)\n+            main_input = inputs_dict[model_class.main_input_name]\n \n             # NOTE: assisted generation only works with cache on at the moment.\n             if not hasattr(config, \"use_cache\"):\n@@ -1389,11 +1316,9 @@ def test_assisted_decoding_sample(self):\n                 \"return_dict_in_generate\": True,\n                 \"use_cache\": True,\n             }\n-            output_assisted = model.generate(\n-                input_ids, attention_mask=attention_mask, **generation_kwargs, **inputs_dict\n-            )\n+            output_assisted = model.generate(**generation_kwargs, **inputs_dict)\n \n-            self._check_outputs(output_assisted, input_ids, config, use_cache=True)\n+            self._check_outputs(output_assisted, main_input, config, use_cache=True)\n \n     @pytest.mark.generate\n     def test_prompt_lookup_decoding_stops_at_eos(self):\n@@ -1429,7 +1354,8 @@ def test_generate_with_head_masking(self):\n         \"\"\"Test designed for encoder-decoder models to ensure the attention head masking is used.\"\"\"\n         attention_names = [\"encoder_attentions\", \"decoder_attentions\", \"cross_attentions\"]\n         for model_class in self.all_generative_model_classes:\n-            config, input_ids, attention_mask, inputs_dict = self._get_input_ids_and_config()\n+            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n+\n             # We want to test only encoder-decoder models\n             if not config.is_encoder_decoder:\n                 continue\n@@ -1452,8 +1378,6 @@ def test_generate_with_head_masking(self):\n \n             for attn_name, (name, mask) in zip(attention_names, head_masking.items()):\n                 out = model.generate(\n-                    input_ids,\n-                    attention_mask=attention_mask,\n                     num_beams=1,\n                     output_attentions=self.has_attentions,\n                     return_dict_in_generate=True,\n@@ -1482,7 +1406,7 @@ def test_left_padding_compatibility(self):\n         # - The model must be a decoder-only architecture (encoder-based architectures use right-padding)\n         decoder_only_classes = []\n         for model_class in self.all_generative_model_classes:\n-            config, _, _, _ = self._get_input_ids_and_config()\n+            config, _ = self.prepare_config_and_inputs_for_generate()\n             if config.is_encoder_decoder:\n                 continue\n             else:\n@@ -1515,7 +1439,12 @@ def _prepare_model_kwargs(input_ids, attention_mask, signature):\n             return model_kwargs\n \n         for model_class in decoder_only_classes:\n-            config, input_ids, attention_mask, _ = self._get_input_ids_and_config()\n+            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n+            input_ids = inputs_dict[\"input_ids\"]\n+            attention_mask = inputs_dict.get(\"attention_mask\")\n+            if attention_mask is None:\n+                attention_mask = torch.ones_like(input_ids)\n+\n             model = model_class(config).to(torch_device).eval()\n             signature = inspect.signature(model.forward).parameters.keys()\n \n@@ -1618,7 +1547,7 @@ def test_generate_from_inputs_embeds_decoder_only(self):\n         # When supported, tests that the decoder model can generate from `inputs_embeds` instead of `input_ids`\n         # if fails, you should probably update the `prepare_inputs_for_generation` function\n         for model_class in self.all_generative_model_classes:\n-            config, input_ids, _, _ = self._get_input_ids_and_config()\n+            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n \n             # Ignore:\n             # a) eos (to always output 20 tokens) and pad (so we don't try to infer the attn mask from the input_ids,\n@@ -1639,6 +1568,8 @@ def test_generate_from_inputs_embeds_decoder_only(self):\n             if \"inputs_embeds\" not in inspect.signature(model.prepare_inputs_for_generation).parameters.keys():\n                 continue\n \n+            input_ids = inputs_dict.pop(\"input_ids\")\n+\n             # Traditional way of generating text\n             outputs_from_ids = model.generate(\n                 input_ids, max_new_tokens=5, return_dict_in_generate=True, output_scores=True\n@@ -1689,17 +1620,20 @@ def test_generate_from_inputs_embeds_with_static_cache(self):\n             if not model_class._supports_static_cache:\n                 self.skipTest(reason=\"This model does not support the static cache format\")\n \n-            config, input_ids, attention_mask, inputs_dict = self._get_input_ids_and_config()\n+            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n+\n             if config.is_encoder_decoder:\n                 self.skipTest(reason=\"This model is encoder-decoder and has Encoder-Decoder Cache\")\n \n             model = model_class(config).to(torch_device).eval()\n             if \"inputs_embeds\" not in inspect.signature(model.prepare_inputs_for_generation).parameters.keys():\n                 self.skipTest(reason=\"This model does not support `inputs_embeds` in generation\")\n \n+            input_ids = inputs_dict.pop(\"input_ids\")\n+\n             model.config.use_cache = True\n             model.config.is_decoder = True\n-            batch_size, seq_length = input_ids.shape\n+            batch_size = input_ids.shape[0]\n             max_cache_len = 30\n \n             # here we force to not stop at eos and go until max-length\n@@ -1724,9 +1658,7 @@ def test_generate_from_inputs_embeds_with_static_cache(self):\n             num_hidden_layers = text_config.num_hidden_layers\n \n             inputs_embeds = model.get_input_embeddings()(input_ids)\n-            outputs = model.generate(\n-                inputs_embeds=inputs_embeds, attention_mask=attention_mask, **generation_kwargs, **inputs_dict\n-            )\n+            outputs = model.generate(inputs_embeds=inputs_embeds, **generation_kwargs, **inputs_dict)\n \n             # we should get `max_length` in shape, not `max_length - embeds_length`\n             cache_shape = (batch_size, num_key_value_heads, max_cache_len, head_dim)\n@@ -1827,7 +1759,7 @@ def test_new_cache_format(self, num_beams, do_sample):\n             if not model_class._supports_cache_class:\n                 self.skipTest(reason=\"This model does not support the new cache format\")\n \n-            config, input_ids, attention_mask, inputs_dict = self._get_input_ids_and_config()\n+            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n \n             model = model_class(config).to(torch_device).eval()\n             generation_kwargs = {\n@@ -1842,24 +1774,16 @@ def test_new_cache_format(self, num_beams, do_sample):\n             # Sets seed before calling `generate` for the case with do_sample=True\n             seed = torch.randint(0, 1000000, (1,)).item()\n             set_seed(seed)\n-            legacy_results = model.generate(\n-                input_ids, attention_mask=attention_mask, **generation_kwargs, **inputs_dict\n-            )\n+            legacy_results = model.generate(**generation_kwargs, **inputs_dict)\n             set_seed(seed)\n             num_hidden_layers = config.get_text_config().num_hidden_layers\n             if config.is_encoder_decoder:\n                 cache_cls = EncoderDecoderCache\n                 past_key_values = cache_cls(DynamicCache(num_hidden_layers), DynamicCache(num_hidden_layers))\n             else:\n                 cache_cls = DynamicCache\n-                past_key_values = cache_cls(num_hidden_layers)\n-            new_results = model.generate(\n-                input_ids,\n-                attention_mask=attention_mask,\n-                past_key_values=past_key_values,\n-                **generation_kwargs,\n-                **inputs_dict,\n-            )\n+                past_key_values = cache_cls()\n+            new_results = model.generate(past_key_values=past_key_values, **generation_kwargs, **inputs_dict)\n \n             # The two sets of generated sequences must match, despite the cache format between forward passes being\n             # different\n@@ -1906,12 +1830,15 @@ def test_generate_with_static_cache(self):\n             if not model_class._supports_static_cache:\n                 self.skipTest(reason=\"This model does not support the static cache format\")\n \n-            config, input_ids, attention_mask, inputs_dict = self._get_input_ids_and_config()\n+            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n+            main_input = inputs_dict[model_class.main_input_name]\n+\n             if config.is_encoder_decoder:\n                 self.skipTest(reason=\"This model is encoder-decoder and has Encoder-Decoder Cache\")\n \n             config.is_decoder = True\n-            batch_size, seq_length = input_ids.shape\n+            batch_size = main_input.shape[0]\n+            seq_length = main_input.shape[-1]\n             max_new_tokens = 20\n \n             model = model_class(config).to(torch_device).eval()\n@@ -1934,7 +1861,7 @@ def test_generate_with_static_cache(self):\n                 else config.num_key_value_heads\n             )\n             num_hidden_layers = config.num_hidden_layers\n-            results = model.generate(input_ids, attention_mask=attention_mask, **generation_kwargs, **inputs_dict)\n+            results = model.generate(**generation_kwargs, **inputs_dict)\n \n             cache_shape = (batch_size, num_key_value_heads, max_cache_len, head_dim)\n             self.assertTrue(isinstance(results.past_key_values, StaticCache))\n@@ -1948,7 +1875,7 @@ def test_generate_with_quant_cache(self):\n             if not model_class._supports_quantized_cache:\n                 self.skipTest(reason=\"This model does not support the quantized cache format\")\n \n-            config, input_ids, attention_mask, inputs_dict = self._get_input_ids_and_config()\n+            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n             config.is_decoder = True\n \n             model = model_class(config).to(torch_device).eval()\n@@ -1961,23 +1888,17 @@ def test_generate_with_quant_cache(self):\n                 \"use_cache\": True,\n             }\n \n-            results = model.generate(input_ids, attention_mask=attention_mask, **generation_kwargs, **inputs_dict)\n+            results = model.generate(**generation_kwargs, **inputs_dict)\n             self.assertTrue(isinstance(results.past_key_values, QuantoQuantizedCache))\n \n             # passing past key values of different type should raise Error\n             with self.assertRaises(ValueError):\n-                num_hidden_layers = config.get_text_config().num_hidden_layers\n-                model.generate(\n-                    input_ids,\n-                    attention_mask=attention_mask,\n-                    past_key_valyes=DynamicCache(num_hidden_layers),\n-                    **generation_kwargs,\n-                )\n+                model.generate(past_key_valyes=DynamicCache(), **generation_kwargs, **inputs_dict)\n \n             # setting incorrect cache_config args should raise an Error, i.e. nbits=60 does not make sense\n             generation_kwargs[\"cache_config\"] = {\"nbits\": 60, \"q_group_size\": 8, \"residual_length\": 128}\n             with self.assertRaises(ValueError):\n-                model.generate(input_ids, attention_mask=attention_mask, **generation_kwargs)\n+                model.generate(**generation_kwargs, **inputs_dict)\n \n     @pytest.mark.generate\n     @require_torch_gpu\n@@ -2040,7 +1961,7 @@ def test_generate_methods_with_num_logits_to_keep(self):\n             if \"num_logits_to_keep\" not in set(inspect.signature(model_class.forward).parameters.keys()):\n                 self.skipTest(reason=\"This model does not support `num_logits_to_keep` argument.\")\n \n-            config, input_ids, attention_mask, inputs_dict = self._get_input_ids_and_config()\n+            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n             config.use_cache = True\n             config.is_decoder = True\n \n@@ -2054,13 +1975,9 @@ def test_generate_methods_with_num_logits_to_keep(self):\n             }\n \n             # Setting num_logits_to_keep at 0 keeps all logits (old behavior)\n-            with_all_logits = model.generate(\n-                input_ids, attention_mask=attention_mask, **generation_kwargs, **inputs_dict, num_logits_to_keep=0\n-            )\n+            with_all_logits = model.generate(**generation_kwargs, **inputs_dict, num_logits_to_keep=0)\n             # By default, num_logits_to_keep is automatically set to 1 if not provided (new behavior)\n-            without_all_logits = model.generate(\n-                input_ids, attention_mask=attention_mask, **inputs_dict, **generation_kwargs\n-            )\n+            without_all_logits = model.generate(**inputs_dict, **generation_kwargs)\n             self.assertEqual(with_all_logits.tolist(), without_all_logits.tolist())\n \n     @pytest.mark.generate\n@@ -2072,7 +1989,7 @@ def test_assisted_decoding_with_num_logits_to_keep(self):\n             if model_class._is_stateful:\n                 self.skipTest(reason=\"Stateful models don't support assisted generation\")\n \n-            config, input_ids, attention_mask, inputs_dict = self._get_input_ids_and_config(batch_size=1)\n+            config, inputs_dict = self.prepare_config_and_inputs_for_generate(batch_size=1)\n             config.use_cache = True\n             config.is_decoder = True\n \n@@ -2089,13 +2006,9 @@ def test_assisted_decoding_with_num_logits_to_keep(self):\n \n             assistant_model.generation_config.assistant_confidence_threshold = None\n             # Setting num_logits_to_keep at 0 keeps all logits (old behavior)\n-            with_all_logits = model.generate(\n-                input_ids, attention_mask=attention_mask, **generation_kwargs, **inputs_dict, num_logits_to_keep=0\n-            )\n+            with_all_logits = model.generate(**generation_kwargs, **inputs_dict, num_logits_to_keep=0)\n             # By default, num_logits_to_keep is automatically set to 1 if not provided (new behavior)\n-            without_all_logits = model.generate(\n-                input_ids, attention_mask=attention_mask, **inputs_dict, **generation_kwargs\n-            )\n+            without_all_logits = model.generate(**inputs_dict, **generation_kwargs)\n             self.assertEqual(with_all_logits.tolist(), without_all_logits.tolist())\n \n     @pytest.mark.generate\n@@ -2107,15 +2020,20 @@ def test_inherits_generation_mixin(self):\n         for model_class in self.all_generative_model_classes:\n             self.assertTrue(\"GenerationMixin\" in str(model_class.__bases__))\n \n-    def _check_outputs(self, output, input_ids, config, use_cache=False, num_return_sequences=1):\n-        batch_size, seq_length = input_ids.shape\n+    def _check_outputs(self, output, main_input, config, use_cache=False, num_return_sequences=1):\n+        batch_size = main_input.shape[0]\n+        seq_length = main_input.shape[-1]\n         config = config.text_config if hasattr(config, \"text_config\") else config\n         num_sequences_in_output = batch_size * num_return_sequences\n \n         gen_len = (\n             output.sequences.shape[-1] - 1 if config.is_encoder_decoder else output.sequences.shape[-1] - seq_length\n         )\n \n+        # in some models we subsample the sequence length in inner layers\n+        if hasattr(self.model_tester, \"get_subsampled_output_lengths\"):\n+            seq_length = self.model_tester.get_subsampled_output_lengths(seq_length)\n+\n         # scores\n         self._check_scores(num_sequences_in_output, output.scores, length=gen_len, config=config)\n "
        },
        {
            "sha": "eae9ee9fbf58eaf42fd03d79849797f1baf59750",
            "filename": "tests/models/bigbird_pegasus/test_modeling_bigbird_pegasus.py",
            "status": "modified",
            "additions": 7,
            "deletions": 22,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29738f5b45d9c868ef9775388700c8d08b5800d/tests%2Fmodels%2Fbigbird_pegasus%2Ftest_modeling_bigbird_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29738f5b45d9c868ef9775388700c8d08b5800d/tests%2Fmodels%2Fbigbird_pegasus%2Ftest_modeling_bigbird_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbigbird_pegasus%2Ftest_modeling_bigbird_pegasus.py?ref=d29738f5b45d9c868ef9775388700c8d08b5800d",
            "patch": "@@ -283,28 +283,6 @@ def is_pipeline_test_to_skip(\n \n         return False\n \n-    # overwrite from GenerationTesterMixin to solve problem\n-    # with conflicting random seeds\n-    def _get_input_ids_and_config(self, batch_size=2):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.attention_type = \"original_full\"\n-\n-        input_ids = inputs_dict.pop(self.input_name)\n-        _ = inputs_dict.pop(\"attention_mask\", None)\n-        _ = inputs_dict.pop(\"decoder_input_ids\", None)\n-        _ = inputs_dict.pop(\"decoder_attention_mask\", None)\n-        attention_mask = torch.ones_like(input_ids, dtype=torch.long)\n-\n-        # cut to half length & take max batch_size 3\n-        sequence_length = input_ids.shape[-1] // 2\n-        input_ids = input_ids[:batch_size, :sequence_length]\n-        attention_mask = attention_mask[:batch_size, :sequence_length]\n-\n-        if config.eos_token_id is not None and config.pad_token_id is None:\n-            # hack to allow generate for models such as GPT2 as is done in `generate()`\n-            config.pad_token_id = config.eos_token_id\n-        return config, input_ids, attention_mask, inputs_dict\n-\n     def setUp(self):\n         self.model_tester = BigBirdPegasusModelTester(self)\n         self.config_tester = ConfigTester(self, config_class=BigBirdPegasusConfig)\n@@ -485,6 +463,13 @@ def test_for_change_to_full_attn(self):\n     def test_load_save_without_tied_weights(self):\n         pass\n \n+    def test_generate_with_head_masking(self):\n+        # overwritten to temporarily switch the attention type to `original_full`\n+        original_self_attention_type = self.model_tester.attention_type\n+        self.model_tester.attention_type = \"original_full\"\n+        super().test_generate_with_head_masking()\n+        self.model_tester.attention_type = original_self_attention_type\n+\n \n @require_torch\n @require_sentencepiece"
        },
        {
            "sha": "aad26ef147e83e65dfc8023ed31f3b23b8e1d8df",
            "filename": "tests/models/chameleon/test_modeling_chameleon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29738f5b45d9c868ef9775388700c8d08b5800d/tests%2Fmodels%2Fchameleon%2Ftest_modeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29738f5b45d9c868ef9775388700c8d08b5800d/tests%2Fmodels%2Fchameleon%2Ftest_modeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fchameleon%2Ftest_modeling_chameleon.py?ref=d29738f5b45d9c868ef9775388700c8d08b5800d",
            "patch": "@@ -116,7 +116,7 @@ def prepare_config_and_inputs(self):\n \n         input_mask = None\n         if self.use_input_mask:\n-            input_mask = torch.tril(torch.ones(self.batch_size, self.seq_length)).to(torch_device)\n+            input_mask = torch.tril(torch.ones_like(input_ids).to(torch_device))\n \n         sequence_labels = None\n         token_labels = None"
        },
        {
            "sha": "7d12dd3d873bfcd798fd23e2d4eaa21f68069af6",
            "filename": "tests/models/cohere/test_modeling_cohere.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29738f5b45d9c868ef9775388700c8d08b5800d/tests%2Fmodels%2Fcohere%2Ftest_modeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29738f5b45d9c868ef9775388700c8d08b5800d/tests%2Fmodels%2Fcohere%2Ftest_modeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcohere%2Ftest_modeling_cohere.py?ref=d29738f5b45d9c868ef9775388700c8d08b5800d",
            "patch": "@@ -95,7 +95,7 @@ def prepare_config_and_inputs(self):\n \n         input_mask = None\n         if self.use_input_mask:\n-            input_mask = torch.tril(torch.ones(self.batch_size, self.seq_length)).to(torch_device)\n+            input_mask = torch.tril(torch.ones_like(input_ids).to(torch_device))\n \n         token_type_ids = None\n         if self.use_token_type_ids:"
        },
        {
            "sha": "e3b729d2f101f81e8a1c03727e968e7fac4c50be",
            "filename": "tests/models/dac/test_modeling_dac.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29738f5b45d9c868ef9775388700c8d08b5800d/tests%2Fmodels%2Fdac%2Ftest_modeling_dac.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29738f5b45d9c868ef9775388700c8d08b5800d/tests%2Fmodels%2Fdac%2Ftest_modeling_dac.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdac%2Ftest_modeling_dac.py?ref=d29738f5b45d9c868ef9775388700c8d08b5800d",
            "patch": "@@ -123,7 +123,6 @@ class DacModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     test_headmasking = False\n     test_resize_embeddings = False\n     pipeline_model_mapping = {\"feature-extraction\": DacModel} if is_torch_available() else {}\n-    input_name = \"input_values\"\n \n     def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):\n         # model does not have attention and does not support returning hidden states"
        },
        {
            "sha": "2aac4dba82e8970971575a2a76a69e92939214cd",
            "filename": "tests/models/encodec/test_modeling_encodec.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29738f5b45d9c868ef9775388700c8d08b5800d/tests%2Fmodels%2Fencodec%2Ftest_modeling_encodec.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29738f5b45d9c868ef9775388700c8d08b5800d/tests%2Fmodels%2Fencodec%2Ftest_modeling_encodec.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fencodec%2Ftest_modeling_encodec.py?ref=d29738f5b45d9c868ef9775388700c8d08b5800d",
            "patch": "@@ -141,7 +141,6 @@ class EncodecModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase)\n     test_headmasking = False\n     test_resize_embeddings = False\n     pipeline_model_mapping = {\"feature-extraction\": EncodecModel} if is_torch_available() else {}\n-    input_name = \"input_values\"\n \n     def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):\n         # model does not have attention and does not support returning hidden states"
        },
        {
            "sha": "6422133d75eb137070c403d419b281f9c05c644b",
            "filename": "tests/models/gemma/test_modeling_gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29738f5b45d9c868ef9775388700c8d08b5800d/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29738f5b45d9c868ef9775388700c8d08b5800d/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py?ref=d29738f5b45d9c868ef9775388700c8d08b5800d",
            "patch": "@@ -119,7 +119,7 @@ def prepare_config_and_inputs(self):\n \n         input_mask = None\n         if self.use_input_mask:\n-            input_mask = torch.tril(torch.ones(self.batch_size, self.seq_length)).to(torch_device)\n+            input_mask = torch.tril(torch.ones_like(input_ids).to(torch_device))\n \n         token_type_ids = None\n         if self.use_token_type_ids:"
        },
        {
            "sha": "9b25698f6401068ba45a9b505c3e130d88d9a0f8",
            "filename": "tests/models/granite/test_modeling_granite.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29738f5b45d9c868ef9775388700c8d08b5800d/tests%2Fmodels%2Fgranite%2Ftest_modeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29738f5b45d9c868ef9775388700c8d08b5800d/tests%2Fmodels%2Fgranite%2Ftest_modeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgranite%2Ftest_modeling_granite.py?ref=d29738f5b45d9c868ef9775388700c8d08b5800d",
            "patch": "@@ -106,7 +106,7 @@ def prepare_config_and_inputs(self):\n \n         input_mask = None\n         if self.use_input_mask:\n-            input_mask = torch.tril(torch.ones(self.batch_size, self.seq_length)).to(torch_device)\n+            input_mask = torch.tril(torch.ones_like(input_ids).to(torch_device))\n \n         token_type_ids = None\n         if self.use_token_type_ids:"
        },
        {
            "sha": "d5d0cee6daa1cd7a247bac44899c911045b398ab",
            "filename": "tests/models/granitemoe/test_modeling_granitemoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29738f5b45d9c868ef9775388700c8d08b5800d/tests%2Fmodels%2Fgranitemoe%2Ftest_modeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29738f5b45d9c868ef9775388700c8d08b5800d/tests%2Fmodels%2Fgranitemoe%2Ftest_modeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgranitemoe%2Ftest_modeling_granitemoe.py?ref=d29738f5b45d9c868ef9775388700c8d08b5800d",
            "patch": "@@ -105,7 +105,7 @@ def prepare_config_and_inputs(self):\n \n         input_mask = None\n         if self.use_input_mask:\n-            input_mask = torch.tril(torch.ones(self.batch_size, self.seq_length)).to(torch_device)\n+            input_mask = torch.tril(torch.ones_like(input_ids).to(torch_device))\n \n         token_type_ids = None\n         if self.use_token_type_ids:"
        },
        {
            "sha": "f1eb2b3929b69b1422a9778c52a9a63598702935",
            "filename": "tests/models/led/test_modeling_led.py",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29738f5b45d9c868ef9775388700c8d08b5800d/tests%2Fmodels%2Fled%2Ftest_modeling_led.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29738f5b45d9c868ef9775388700c8d08b5800d/tests%2Fmodels%2Fled%2Ftest_modeling_led.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fled%2Ftest_modeling_led.py?ref=d29738f5b45d9c868ef9775388700c8d08b5800d",
            "patch": "@@ -338,13 +338,11 @@ def test_global_attention(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs_for_common()\n         self.model_tester.check_global_attention(*config_and_inputs)\n \n-    def _get_input_ids_and_config(self, batch_size=2):\n-        config, input_ids, attention_mask, inputs_dict = GenerationTesterMixin._get_input_ids_and_config(\n-            self, batch_size=batch_size\n-        )\n+    def prepare_config_and_inputs_for_generate(self, *args, **kwargs):\n+        config, inputs_dict = super().prepare_config_and_inputs_for_generate(*args, **kwargs)\n         # LED computes attention scores based on mask indices if `is_global`\n         inputs_dict.pop(\"global_attention_mask\")\n-        return config, input_ids, attention_mask, inputs_dict\n+        return config, inputs_dict\n \n     # LEDForSequenceClassification does not support inputs_embeds\n     def test_inputs_embeds(self):"
        },
        {
            "sha": "6b273bce7a1f566fa590b35789d3bf4fb2200f19",
            "filename": "tests/models/llama/test_modeling_llama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29738f5b45d9c868ef9775388700c8d08b5800d/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29738f5b45d9c868ef9775388700c8d08b5800d/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py?ref=d29738f5b45d9c868ef9775388700c8d08b5800d",
            "patch": "@@ -112,7 +112,7 @@ def prepare_config_and_inputs(self):\n \n         input_mask = None\n         if self.use_input_mask:\n-            input_mask = torch.tril(torch.ones(self.batch_size, self.seq_length)).to(torch_device)\n+            input_mask = torch.tril(torch.ones_like(input_ids).to(torch_device))\n \n         token_type_ids = None\n         if self.use_token_type_ids:"
        },
        {
            "sha": "ab6184ce2bbed84dcc01651a90955aec16c067be",
            "filename": "tests/models/mimi/test_modeling_mimi.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29738f5b45d9c868ef9775388700c8d08b5800d/tests%2Fmodels%2Fmimi%2Ftest_modeling_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29738f5b45d9c868ef9775388700c8d08b5800d/tests%2Fmodels%2Fmimi%2Ftest_modeling_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmimi%2Ftest_modeling_mimi.py?ref=d29738f5b45d9c868ef9775388700c8d08b5800d",
            "patch": "@@ -170,7 +170,6 @@ class MimiModelTest(ModelTesterMixin, unittest.TestCase):\n     test_headmasking = False\n     test_resize_embeddings = False\n     test_torchscript = False\n-    input_name = \"input_values\"\n \n     def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):\n         # model does support returning hidden states"
        },
        {
            "sha": "88140b1a20f8106e1a0ca742236766a4aed4dec6",
            "filename": "tests/models/mistral/test_modeling_mistral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29738f5b45d9c868ef9775388700c8d08b5800d/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29738f5b45d9c868ef9775388700c8d08b5800d/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py?ref=d29738f5b45d9c868ef9775388700c8d08b5800d",
            "patch": "@@ -112,7 +112,7 @@ def prepare_config_and_inputs(self):\n \n         input_mask = None\n         if self.use_input_mask:\n-            input_mask = torch.tril(torch.ones(self.batch_size, self.seq_length)).to(torch_device)\n+            input_mask = torch.tril(torch.ones_like(input_ids).to(torch_device))\n \n         token_type_ids = None\n         if self.use_token_type_ids:"
        },
        {
            "sha": "836d38e904cb80f850ffcc21ed84a8c25de6e4b3",
            "filename": "tests/models/mixtral/test_modeling_mixtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29738f5b45d9c868ef9775388700c8d08b5800d/tests%2Fmodels%2Fmixtral%2Ftest_modeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29738f5b45d9c868ef9775388700c8d08b5800d/tests%2Fmodels%2Fmixtral%2Ftest_modeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmixtral%2Ftest_modeling_mixtral.py?ref=d29738f5b45d9c868ef9775388700c8d08b5800d",
            "patch": "@@ -108,7 +108,7 @@ def prepare_config_and_inputs(self):\n \n         input_mask = None\n         if self.use_input_mask:\n-            input_mask = torch.tril(torch.ones(self.batch_size, self.seq_length)).to(torch_device)\n+            input_mask = torch.tril(torch.ones_like(input_ids).to(torch_device))\n \n         token_type_ids = None\n         if self.use_token_type_ids:"
        },
        {
            "sha": "cc30238c8df9f5979abc7a741f3f5a8283fcbca3",
            "filename": "tests/models/musicgen/test_modeling_musicgen.py",
            "status": "modified",
            "additions": 14,
            "deletions": 218,
            "changes": 232,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29738f5b45d9c868ef9775388700c8d08b5800d/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29738f5b45d9c868ef9775388700c8d08b5800d/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py?ref=d29738f5b45d9c868ef9775388700c8d08b5800d",
            "patch": "@@ -60,10 +60,6 @@\n         MusicgenModel,\n         set_seed,\n     )\n-    from transformers.generation import (\n-        GenerateDecoderOnlyOutput,\n-        GenerateEncoderDecoderOutput,\n-    )\n \n \n def _config_zero_init(config):\n@@ -124,6 +120,7 @@ def __init__(\n         pad_token_id=99,\n         bos_token_id=99,\n         num_codebooks=4,\n+        audio_channels=1,\n     ):\n         self.parent = parent\n         self.batch_size = batch_size\n@@ -141,6 +138,7 @@ def __init__(\n         self.pad_token_id = pad_token_id\n         self.bos_token_id = bos_token_id\n         self.num_codebooks = num_codebooks\n+        self.audio_channels = audio_channels\n \n     def prepare_config_and_inputs(self):\n         input_ids = ids_tensor([self.batch_size * self.num_codebooks, self.seq_length], self.vocab_size)\n@@ -166,6 +164,7 @@ def get_config(self):\n             bos_token_id=self.bos_token_id,\n             num_codebooks=self.num_codebooks,\n             tie_word_embeddings=False,\n+            audio_channels=self.audio_channels,\n         )\n         return config\n \n@@ -282,47 +281,15 @@ def test_tie_model_weights(self):\n     def test_tied_weights_keys(self):\n         pass\n \n-    def _get_input_ids_and_config(self, batch_size=2):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        input_ids = inputs_dict[\"input_ids\"]\n-\n-        _ = inputs_dict.pop(\"attention_mask\", None)\n-        inputs_dict = {\n-            k: v[:batch_size, ...]\n-            for k, v in inputs_dict.items()\n-            if \"head_mask\" not in k and isinstance(v, torch.Tensor)\n-        }\n-\n-        # take max batch_size\n-        sequence_length = input_ids.shape[-1]\n-        input_ids = input_ids[: batch_size * config.num_codebooks, :]\n-\n-        attention_mask = torch.ones((batch_size, sequence_length), dtype=torch.long)\n-        return config, input_ids, attention_mask, inputs_dict\n-\n     def _get_logits_processor_kwargs(self, do_sample=False, config=None):\n         logits_processor_kwargs = {}\n         return logits_processor_kwargs\n \n     def test_greedy_generate_stereo_outputs(self):\n-        for model_class in self.greedy_sample_model_classes:\n-            config, input_ids, attention_mask, inputs_dict = self._get_input_ids_and_config()\n-            config.audio_channels = 2\n-            model = model_class(config).to(torch_device).eval()\n-            output_generate = self._greedy_generate(\n-                model=model,\n-                input_ids=input_ids.to(torch_device),\n-                attention_mask=attention_mask.to(torch_device),\n-                output_scores=True,\n-                output_hidden_states=True,\n-                output_attentions=True,\n-                return_dict_in_generate=True,\n-                inputs_dict={},\n-            )\n-\n-            self.assertIsInstance(output_generate, GenerateDecoderOnlyOutput)\n-\n-            self.assertNotIn(config.pad_token_id, output_generate)\n+        original_audio_channels = self.model_tester.audio_channels\n+        self.model_tester.audio_channels = 2\n+        super().test_greedy_generate_dict_outputs()\n+        self.model_tester.audio_channels = original_audio_channels\n \n     @require_flash_attn\n     @require_torch_gpu\n@@ -998,6 +965,7 @@ def __init__(\n         num_codebooks=4,\n         num_filters=4,\n         codebook_size=128,\n+        audio_channels=1,\n     ):\n         self.parent = parent\n         self.batch_size = batch_size\n@@ -1017,6 +985,7 @@ def __init__(\n         self.num_codebooks = num_codebooks\n         self.num_filters = num_filters\n         self.codebook_size = codebook_size\n+        self.audio_channels = audio_channels\n \n     def prepare_config_and_inputs(self):\n         input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n@@ -1052,6 +1021,7 @@ def get_config(self):\n             bos_token_id=self.bos_token_id,\n             num_codebooks=self.num_codebooks,\n             tie_word_embeddings=False,\n+            audio_channels=self.audio_channels,\n         )\n         config = MusicgenConfig.from_sub_models_config(text_encoder_config, audio_encoder_config, decoder_config)\n         return config\n@@ -1415,170 +1385,10 @@ def test_model_get_set_embeddings(self):\n             lm_heads = model.get_output_embeddings()\n             self.assertTrue(lm_heads is None or isinstance(lm_heads[0], torch.nn.Linear))\n \n-    def _get_input_ids_and_config(self, batch_size=2):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        input_ids = inputs_dict[\"input_ids\"]\n-\n-        # take max batch_size\n-        sequence_length = input_ids.shape[-1]\n-        input_ids = input_ids[:batch_size, :]\n-        attention_mask = torch.ones((batch_size, sequence_length), dtype=torch.long)\n-\n-        return config, input_ids, attention_mask\n-\n-    # override since the `input_ids` cannot be used as the `decoder_input_ids` for musicgen (input / outputs are\n-    # different modalities -> different shapes)\n-    def _greedy_generate(\n-        self,\n-        model,\n-        input_ids,\n-        attention_mask,\n-        output_scores=False,\n-        output_attentions=False,\n-        output_hidden_states=False,\n-        return_dict_in_generate=False,\n-    ):\n-        model_kwargs = {\"attention_mask\": attention_mask} if attention_mask is not None else {}\n-        output_generate = model.generate(\n-            input_ids,\n-            do_sample=False,\n-            num_beams=1,\n-            max_new_tokens=self.max_new_tokens,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            output_scores=output_scores,\n-            return_dict_in_generate=return_dict_in_generate,\n-            remove_invalid_values=True,\n-            **model_kwargs,\n-        )\n-\n-        return output_generate\n-\n-    # override since the `input_ids` cannot be used as the `decoder_input_ids` for musicgen (input / outputs are\n-    # different modalities -> different shapes)\n-    def _sample_generate(\n-        self,\n-        model,\n-        input_ids,\n-        attention_mask,\n-        num_return_sequences,\n-        output_scores=False,\n-        output_attentions=False,\n-        output_hidden_states=False,\n-        return_dict_in_generate=False,\n-    ):\n-        torch.manual_seed(0)\n-        model_kwargs = {\"attention_mask\": attention_mask} if attention_mask is not None else {}\n-        output_generate = model.generate(\n-            input_ids,\n-            do_sample=True,\n-            num_beams=1,\n-            max_new_tokens=self.max_new_tokens,\n-            num_return_sequences=num_return_sequences,\n-            output_scores=output_scores,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict_in_generate=return_dict_in_generate,\n-            remove_invalid_values=True,\n-            **model_kwargs,\n-        )\n-\n-        return output_generate\n-\n     def _get_logits_processor_kwargs(self, do_sample=False, config=None):\n         logits_processor_kwargs = {}\n         return logits_processor_kwargs\n \n-    def test_greedy_generate_dict_outputs(self):\n-        for model_class in self.greedy_sample_model_classes:\n-            # disable cache\n-            config, input_ids, attention_mask = self._get_input_ids_and_config()\n-            config.use_cache = False\n-            model = model_class(config).to(torch_device).eval()\n-            output_generate = self._greedy_generate(\n-                model=model,\n-                input_ids=input_ids.to(torch_device),\n-                attention_mask=attention_mask.to(torch_device),\n-                output_scores=True,\n-                output_hidden_states=True,\n-                output_attentions=True,\n-                return_dict_in_generate=True,\n-            )\n-\n-            self.assertIsInstance(output_generate, GenerateEncoderDecoderOutput)\n-\n-            self.assertNotIn(config.pad_token_id, output_generate)\n-\n-    def test_greedy_generate_dict_outputs_use_cache(self):\n-        for model_class in self.greedy_sample_model_classes:\n-            # enable cache\n-            config, input_ids, attention_mask = self._get_input_ids_and_config()\n-\n-            config.use_cache = True\n-            config.is_decoder = True\n-            model = model_class(config).to(torch_device).eval()\n-            output_generate = self._greedy_generate(\n-                model=model,\n-                input_ids=input_ids.to(torch_device),\n-                attention_mask=attention_mask.to(torch_device),\n-                output_scores=True,\n-                output_hidden_states=True,\n-                output_attentions=True,\n-                return_dict_in_generate=True,\n-            )\n-\n-            self.assertIsInstance(output_generate, GenerateEncoderDecoderOutput)\n-\n-    def test_sample_generate(self):\n-        for model_class in self.greedy_sample_model_classes:\n-            config, input_ids, attention_mask = self._get_input_ids_and_config()\n-            model = model_class(config).to(torch_device).eval()\n-\n-            # check `generate()` and `sample()` are equal\n-            output_generate = self._sample_generate(\n-                model=model,\n-                input_ids=input_ids.to(torch_device),\n-                attention_mask=attention_mask.to(torch_device),\n-                num_return_sequences=1,\n-            )\n-            self.assertIsInstance(output_generate, torch.Tensor)\n-\n-    def test_sample_generate_dict_output(self):\n-        for model_class in self.greedy_sample_model_classes:\n-            # disable cache\n-            config, input_ids, attention_mask = self._get_input_ids_and_config()\n-            config.use_cache = False\n-            model = model_class(config).to(torch_device).eval()\n-\n-            output_generate = self._sample_generate(\n-                model=model,\n-                input_ids=input_ids.to(torch_device),\n-                attention_mask=attention_mask.to(torch_device),\n-                num_return_sequences=3,\n-                output_scores=True,\n-                output_hidden_states=True,\n-                output_attentions=True,\n-                return_dict_in_generate=True,\n-            )\n-\n-            self.assertIsInstance(output_generate, GenerateEncoderDecoderOutput)\n-\n-    def test_generate_without_input_ids(self):\n-        config, _, _ = self._get_input_ids_and_config()\n-\n-        # if no bos token id => cannot generate from None\n-        if config.bos_token_id is None:\n-            self.skipTest(reason=\"bos_token_id is None\")\n-\n-        for model_class in self.greedy_sample_model_classes:\n-            model = model_class(config).to(torch_device)\n-            model.eval()\n-\n-            output_ids_generate = model.generate(\n-                do_sample=False, max_new_tokens=self.max_new_tokens, remove_invalid_values=True\n-            )\n-            self.assertIsNotNone(output_ids_generate)\n-\n     @require_torch_fp16\n     @require_torch_accelerator  # not all operations are supported in fp16 on CPU\n     def test_generate_fp16(self):\n@@ -1595,24 +1405,10 @@ def test_generate_fp16(self):\n             )\n \n     def test_greedy_generate_stereo_outputs(self):\n-        for model_class in self.greedy_sample_model_classes:\n-            config, input_ids, attention_mask = self._get_input_ids_and_config()\n-            config.audio_channels = 2\n-\n-            model = model_class(config).to(torch_device).eval()\n-            output_generate = self._greedy_generate(\n-                model=model,\n-                input_ids=input_ids.to(torch_device),\n-                attention_mask=attention_mask.to(torch_device),\n-                output_scores=True,\n-                output_hidden_states=True,\n-                output_attentions=True,\n-                return_dict_in_generate=True,\n-            )\n-\n-            self.assertIsInstance(output_generate, GenerateEncoderDecoderOutput)\n-\n-            self.assertNotIn(config.pad_token_id, output_generate)\n+        original_audio_channels = self.model_tester.audio_channels\n+        self.model_tester.audio_channels = 2\n+        super().test_greedy_generate_dict_outputs()\n+        self.model_tester.audio_channels = original_audio_channels\n \n     @unittest.skip(\n         reason=\"MusicgenModel is actually not the base of MusicgenForCausalLM as the latter is a composit model\""
        },
        {
            "sha": "35af9fe0768da8a23b11380dedd5790d9765794a",
            "filename": "tests/models/musicgen_melody/test_modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 14,
            "deletions": 216,
            "changes": 230,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29738f5b45d9c868ef9775388700c8d08b5800d/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29738f5b45d9c868ef9775388700c8d08b5800d/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py?ref=d29738f5b45d9c868ef9775388700c8d08b5800d",
            "patch": "@@ -61,9 +61,6 @@\n         MusicgenMelodyModel,\n         set_seed,\n     )\n-    from transformers.generation import (\n-        GenerateDecoderOnlyOutput,\n-    )\n \n if is_torchaudio_available():\n     from transformers import MusicgenMelodyProcessor\n@@ -124,6 +121,7 @@ def __init__(\n         bos_token_id=99,\n         num_codebooks=4,\n         conditional_seq_length=4,\n+        audio_channels=1,\n     ):\n         self.parent = parent\n         self.batch_size = batch_size\n@@ -143,6 +141,7 @@ def __init__(\n         self.num_codebooks = num_codebooks\n         self.conditional_seq_length = conditional_seq_length\n         self.encoder_seq_length = conditional_seq_length + seq_length\n+        self.audio_channels = audio_channels\n \n     def prepare_config_and_inputs(self):\n         input_ids = ids_tensor([self.batch_size * self.num_codebooks, self.seq_length], self.vocab_size)\n@@ -168,6 +167,7 @@ def get_config(self):\n             bos_token_id=self.bos_token_id,\n             num_codebooks=self.num_codebooks,\n             tie_word_embeddings=False,\n+            audio_channels=self.audio_channels,\n         )\n         return config\n \n@@ -285,46 +285,15 @@ def test_tie_model_weights(self):\n     def test_tied_weights_keys(self):\n         pass\n \n-    def _get_input_ids_and_config(self, batch_size=2):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        input_ids = inputs_dict[\"input_ids\"]\n-\n-        _ = inputs_dict.pop(\"attention_mask\", None)\n-        inputs_dict = {\n-            k: v[:batch_size, ...]\n-            for k, v in inputs_dict.items()\n-            if \"head_mask\" not in k and isinstance(v, torch.Tensor)\n-        }\n-\n-        # take max batch_size\n-        sequence_length = input_ids.shape[-1]\n-        input_ids = input_ids[: batch_size * config.num_codebooks, :]\n-\n-        attention_mask = torch.ones((batch_size, sequence_length), dtype=torch.long)\n-        return config, input_ids, attention_mask, inputs_dict\n-\n     def _get_logits_processor_kwargs(self, do_sample=False, config=None):\n         logits_processor_kwargs = {}\n         return logits_processor_kwargs\n \n     def test_greedy_generate_stereo_outputs(self):\n-        for model_class in self.greedy_sample_model_classes:\n-            config, input_ids, attention_mask, _ = self._get_input_ids_and_config()\n-            config.audio_channels = 2\n-            model = model_class(config).to(torch_device).eval()\n-            output_generate = self._greedy_generate(\n-                model=model,\n-                input_ids=input_ids.to(torch_device),\n-                attention_mask=attention_mask.to(torch_device),\n-                output_scores=True,\n-                output_hidden_states=True,\n-                output_attentions=True,\n-                return_dict_in_generate=True,\n-                inputs_dict={},\n-            )\n-\n-            self.assertIsInstance(output_generate, GenerateDecoderOnlyOutput)\n-            self.assertNotIn(config.pad_token_id, output_generate)\n+        original_audio_channels = self.model_tester.audio_channels\n+        self.model_tester.audio_channels = 2\n+        super().test_greedy_generate_dict_outputs()\n+        self.model_tester.audio_channels = original_audio_channels\n \n     @require_flash_attn\n     @require_torch_gpu\n@@ -996,6 +965,7 @@ def __init__(\n         codebook_size=128,\n         conditional_seq_length=3,\n         chroma_length=24,\n+        audio_channels=1,\n     ):\n         self.parent = parent\n         self.batch_size = batch_size\n@@ -1018,6 +988,7 @@ def __init__(\n         self.conditional_seq_length = conditional_seq_length\n         self.chroma_length = chroma_length\n         self.encoder_seq_length = conditional_seq_length + seq_length\n+        self.audio_channels = audio_channels\n \n     def prepare_config_and_inputs(self):\n         input_ids = ids_tensor([self.batch_size, self.conditional_seq_length], self.vocab_size)\n@@ -1053,6 +1024,7 @@ def get_config(self):\n             bos_token_id=self.bos_token_id,\n             num_codebooks=self.num_codebooks,\n             tie_word_embeddings=False,\n+            audio_channels=self.audio_channels,\n         )\n         config = MusicgenMelodyConfig.from_sub_models_config(\n             text_encoder_config, audio_encoder_config, decoder_config, chroma_length=self.chroma_length\n@@ -1399,170 +1371,10 @@ def test_model_get_set_embeddings(self):\n             lm_heads = model.get_output_embeddings()\n             self.assertTrue(lm_heads is None or isinstance(lm_heads[0], torch.nn.Linear))\n \n-    def _get_input_ids_and_config(self, batch_size=2):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        input_ids = inputs_dict[\"input_ids\"]\n-\n-        # take max batch_size\n-        sequence_length = input_ids.shape[-1]\n-        input_ids = input_ids[:batch_size, :]\n-        attention_mask = torch.ones((batch_size, sequence_length), dtype=torch.long)\n-\n-        return config, input_ids, attention_mask\n-\n-    # override since the `input_ids` cannot be used as the `decoder_input_ids` for musicgen_melody (input / outputs are\n-    # different modalities -> different shapes)\n-    def _greedy_generate(\n-        self,\n-        model,\n-        input_ids,\n-        attention_mask,\n-        output_scores=False,\n-        output_attentions=False,\n-        output_hidden_states=False,\n-        return_dict_in_generate=False,\n-    ):\n-        model_kwargs = {\"attention_mask\": attention_mask} if attention_mask is not None else {}\n-        output_generate = model.generate(\n-            input_ids,\n-            do_sample=False,\n-            num_beams=1,\n-            max_new_tokens=self.max_new_tokens,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            output_scores=output_scores,\n-            return_dict_in_generate=return_dict_in_generate,\n-            remove_invalid_values=True,\n-            **model_kwargs,\n-        )\n-\n-        return output_generate\n-\n-    # override since the `input_ids` cannot be used as the `decoder_input_ids` for musicgen_melody (input / outputs are\n-    # different modalities -> different shapes)\n-    def _sample_generate(\n-        self,\n-        model,\n-        input_ids,\n-        attention_mask,\n-        num_return_sequences,\n-        output_scores=False,\n-        output_attentions=False,\n-        output_hidden_states=False,\n-        return_dict_in_generate=False,\n-    ):\n-        torch.manual_seed(0)\n-        model_kwargs = {\"attention_mask\": attention_mask} if attention_mask is not None else {}\n-        output_generate = model.generate(\n-            input_ids,\n-            do_sample=True,\n-            num_beams=1,\n-            max_new_tokens=self.max_new_tokens,\n-            num_return_sequences=num_return_sequences,\n-            output_scores=output_scores,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict_in_generate=return_dict_in_generate,\n-            remove_invalid_values=True,\n-            **model_kwargs,\n-        )\n-\n-        return output_generate\n-\n     def _get_logits_processor_kwargs(self, do_sample=False, config=None):\n         logits_processor_kwargs = {}\n         return logits_processor_kwargs\n \n-    def test_greedy_generate_dict_outputs(self):\n-        for model_class in self.greedy_sample_model_classes:\n-            # disable cache\n-            config, input_ids, attention_mask = self._get_input_ids_and_config()\n-            config.use_cache = False\n-            model = model_class(config).to(torch_device).eval()\n-            output_generate = self._greedy_generate(\n-                model=model,\n-                input_ids=input_ids.to(torch_device),\n-                attention_mask=attention_mask.to(torch_device),\n-                output_scores=True,\n-                output_hidden_states=True,\n-                output_attentions=True,\n-                return_dict_in_generate=True,\n-            )\n-\n-            self.assertIsInstance(output_generate, GenerateDecoderOnlyOutput)\n-\n-            self.assertNotIn(config.pad_token_id, output_generate)\n-\n-    def test_greedy_generate_dict_outputs_use_cache(self):\n-        for model_class in self.greedy_sample_model_classes:\n-            # enable cache\n-            config, input_ids, attention_mask = self._get_input_ids_and_config()\n-\n-            config.use_cache = True\n-            config.is_decoder = True\n-            model = model_class(config).to(torch_device).eval()\n-            output_generate = self._greedy_generate(\n-                model=model,\n-                input_ids=input_ids.to(torch_device),\n-                attention_mask=attention_mask.to(torch_device),\n-                output_scores=True,\n-                output_hidden_states=True,\n-                output_attentions=True,\n-                return_dict_in_generate=True,\n-            )\n-\n-            self.assertIsInstance(output_generate, GenerateDecoderOnlyOutput)\n-\n-    def test_sample_generate(self):\n-        for model_class in self.greedy_sample_model_classes:\n-            config, input_ids, attention_mask = self._get_input_ids_and_config()\n-            model = model_class(config).to(torch_device).eval()\n-\n-            # check `generate()` and `sample()` are equal\n-            output_generate = self._sample_generate(\n-                model=model,\n-                input_ids=input_ids.to(torch_device),\n-                attention_mask=attention_mask.to(torch_device),\n-                num_return_sequences=1,\n-            )\n-            self.assertIsInstance(output_generate, torch.Tensor)\n-\n-    def test_sample_generate_dict_output(self):\n-        for model_class in self.greedy_sample_model_classes:\n-            # disable cache\n-            config, input_ids, attention_mask = self._get_input_ids_and_config()\n-            config.use_cache = False\n-            model = model_class(config).to(torch_device).eval()\n-\n-            output_generate = self._sample_generate(\n-                model=model,\n-                input_ids=input_ids.to(torch_device),\n-                attention_mask=attention_mask.to(torch_device),\n-                num_return_sequences=3,\n-                output_scores=True,\n-                output_hidden_states=True,\n-                output_attentions=True,\n-                return_dict_in_generate=True,\n-            )\n-\n-            self.assertIsInstance(output_generate, GenerateDecoderOnlyOutput)\n-\n-    def test_generate_without_input_ids(self):\n-        config, _, _ = self._get_input_ids_and_config()\n-\n-        # if no bos token id => cannot generate from None\n-        if config.bos_token_id is None:\n-            self.skipTest(reason=\"bos_token_id is None\")\n-\n-        for model_class in self.greedy_sample_model_classes:\n-            model = model_class(config).to(torch_device)\n-            model.eval()\n-\n-            output_ids_generate = model.generate(\n-                do_sample=False, max_new_tokens=self.max_new_tokens, remove_invalid_values=True\n-            )\n-            self.assertIsNotNone(output_ids_generate)\n-\n     @require_torch_fp16\n     @require_torch_accelerator  # not all operations are supported in fp16 on CPU\n     def test_generate_fp16(self):\n@@ -1579,24 +1391,10 @@ def test_generate_fp16(self):\n             )\n \n     def test_greedy_generate_stereo_outputs(self):\n-        for model_class in self.greedy_sample_model_classes:\n-            config, input_ids, attention_mask = self._get_input_ids_and_config()\n-            config.audio_channels = 2\n-\n-            model = model_class(config).to(torch_device).eval()\n-            output_generate = self._greedy_generate(\n-                model=model,\n-                input_ids=input_ids.to(torch_device),\n-                attention_mask=attention_mask.to(torch_device),\n-                output_scores=True,\n-                output_hidden_states=True,\n-                output_attentions=True,\n-                return_dict_in_generate=True,\n-            )\n-\n-            self.assertIsInstance(output_generate, GenerateDecoderOnlyOutput)\n-\n-            self.assertNotIn(config.pad_token_id, output_generate)\n+        original_audio_channels = self.model_tester.audio_channels\n+        self.model_tester.audio_channels = 2\n+        super().test_greedy_generate_dict_outputs()\n+        self.model_tester.audio_channels = original_audio_channels\n \n     @unittest.skip(\n         reason=\"MusicgenMelodyModel is actually not the base of MusicgenMelodyForCausalLM as the latter is a composit model\""
        },
        {
            "sha": "43e0b7afb49f8a298a05207d1fa0e2e81343e71b",
            "filename": "tests/models/olmo/test_modeling_olmo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29738f5b45d9c868ef9775388700c8d08b5800d/tests%2Fmodels%2Folmo%2Ftest_modeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29738f5b45d9c868ef9775388700c8d08b5800d/tests%2Fmodels%2Folmo%2Ftest_modeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Folmo%2Ftest_modeling_olmo.py?ref=d29738f5b45d9c868ef9775388700c8d08b5800d",
            "patch": "@@ -101,7 +101,7 @@ def prepare_config_and_inputs(self):\n \n         input_mask = None\n         if self.use_input_mask:\n-            input_mask = torch.tril(torch.ones(self.batch_size, self.seq_length)).to(torch_device)\n+            input_mask = torch.tril(torch.ones_like(input_ids).to(torch_device))\n \n         token_type_ids = None\n         if self.use_token_type_ids:"
        },
        {
            "sha": "9c3af5723ee18b9038fd6b18b20209a60ad889db",
            "filename": "tests/models/olmoe/test_modeling_olmoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29738f5b45d9c868ef9775388700c8d08b5800d/tests%2Fmodels%2Folmoe%2Ftest_modeling_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29738f5b45d9c868ef9775388700c8d08b5800d/tests%2Fmodels%2Folmoe%2Ftest_modeling_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Folmoe%2Ftest_modeling_olmoe.py?ref=d29738f5b45d9c868ef9775388700c8d08b5800d",
            "patch": "@@ -111,7 +111,7 @@ def prepare_config_and_inputs(self):\n \n         input_mask = None\n         if self.use_input_mask:\n-            input_mask = torch.tril(torch.ones(self.batch_size, self.seq_length)).to(torch_device)\n+            input_mask = torch.tril(torch.ones_like(input_ids).to(torch_device))\n \n         token_type_ids = None\n         if self.use_token_type_ids:"
        },
        {
            "sha": "600c5b8a2f7342d4cd36bee60591fd4f515b7b7f",
            "filename": "tests/models/persimmon/test_modeling_persimmon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29738f5b45d9c868ef9775388700c8d08b5800d/tests%2Fmodels%2Fpersimmon%2Ftest_modeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29738f5b45d9c868ef9775388700c8d08b5800d/tests%2Fmodels%2Fpersimmon%2Ftest_modeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpersimmon%2Ftest_modeling_persimmon.py?ref=d29738f5b45d9c868ef9775388700c8d08b5800d",
            "patch": "@@ -110,7 +110,7 @@ def prepare_config_and_inputs(self):\n \n         input_mask = None\n         if self.use_input_mask:\n-            input_mask = torch.tril(torch.ones(self.batch_size, self.seq_length)).to(torch_device)\n+            input_mask = torch.tril(torch.ones_like(input_ids).to(torch_device))\n \n         token_type_ids = None\n         if self.use_token_type_ids:"
        },
        {
            "sha": "1186717a78cc00b979ae3b7bef8ba907b19d2f70",
            "filename": "tests/models/phi3/test_modeling_phi3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29738f5b45d9c868ef9775388700c8d08b5800d/tests%2Fmodels%2Fphi3%2Ftest_modeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29738f5b45d9c868ef9775388700c8d08b5800d/tests%2Fmodels%2Fphi3%2Ftest_modeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fphi3%2Ftest_modeling_phi3.py?ref=d29738f5b45d9c868ef9775388700c8d08b5800d",
            "patch": "@@ -151,7 +151,7 @@ def prepare_config_and_inputs(self):\n \n         input_mask = None\n         if self.use_input_mask:\n-            input_mask = torch.tril(torch.ones(self.batch_size, self.seq_length)).to(torch_device)\n+            input_mask = torch.tril(torch.ones_like(input_ids).to(torch_device))\n \n         token_type_ids = None\n         if self.use_token_type_ids:"
        },
        {
            "sha": "95bf2cce6d3a4a95827e3a5b7d1a72b21c9a67f2",
            "filename": "tests/models/qwen2/test_modeling_qwen2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29738f5b45d9c868ef9775388700c8d08b5800d/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29738f5b45d9c868ef9775388700c8d08b5800d/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py?ref=d29738f5b45d9c868ef9775388700c8d08b5800d",
            "patch": "@@ -116,7 +116,7 @@ def prepare_config_and_inputs(self):\n \n         input_mask = None\n         if self.use_input_mask:\n-            input_mask = torch.tril(torch.ones(self.batch_size, self.seq_length)).to(torch_device)\n+            input_mask = torch.tril(torch.ones_like(input_ids).to(torch_device))\n \n         token_type_ids = None\n         if self.use_token_type_ids:"
        },
        {
            "sha": "e8eb915a328aa141c7f59b4555e245869b1e8763",
            "filename": "tests/models/qwen2_moe/test_modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29738f5b45d9c868ef9775388700c8d08b5800d/tests%2Fmodels%2Fqwen2_moe%2Ftest_modeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29738f5b45d9c868ef9775388700c8d08b5800d/tests%2Fmodels%2Fqwen2_moe%2Ftest_modeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_moe%2Ftest_modeling_qwen2_moe.py?ref=d29738f5b45d9c868ef9775388700c8d08b5800d",
            "patch": "@@ -134,7 +134,7 @@ def prepare_config_and_inputs(self):\n \n         input_mask = None\n         if self.use_input_mask:\n-            input_mask = torch.tril(torch.ones(self.batch_size, self.seq_length)).to(torch_device)\n+            input_mask = torch.tril(torch.ones_like(input_ids).to(torch_device))\n \n         token_type_ids = None\n         if self.use_token_type_ids:"
        },
        {
            "sha": "d2f658f56bd81b0aa86dc65375839b3bd7013ac0",
            "filename": "tests/models/recurrent_gemma/test_modeling_recurrent_gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29738f5b45d9c868ef9775388700c8d08b5800d/tests%2Fmodels%2Frecurrent_gemma%2Ftest_modeling_recurrent_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29738f5b45d9c868ef9775388700c8d08b5800d/tests%2Fmodels%2Frecurrent_gemma%2Ftest_modeling_recurrent_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frecurrent_gemma%2Ftest_modeling_recurrent_gemma.py?ref=d29738f5b45d9c868ef9775388700c8d08b5800d",
            "patch": "@@ -103,7 +103,7 @@ def prepare_config_and_inputs(self):\n \n         input_mask = None\n         if self.use_input_mask:\n-            input_mask = torch.tril(torch.ones(self.batch_size, self.seq_length)).to(torch_device)\n+            input_mask = torch.tril(torch.ones_like(input_ids).to(torch_device))\n \n         token_type_ids = None\n         if self.use_token_type_ids:"
        },
        {
            "sha": "d837742e9ccd64d9353978a383016a89e5d8a951",
            "filename": "tests/models/reformer/test_modeling_reformer.py",
            "status": "modified",
            "additions": 6,
            "deletions": 11,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29738f5b45d9c868ef9775388700c8d08b5800d/tests%2Fmodels%2Freformer%2Ftest_modeling_reformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29738f5b45d9c868ef9775388700c8d08b5800d/tests%2Fmodels%2Freformer%2Ftest_modeling_reformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Freformer%2Ftest_modeling_reformer.py?ref=d29738f5b45d9c868ef9775388700c8d08b5800d",
            "patch": "@@ -684,20 +684,15 @@ def _check_hidden_states_for_generate(\n     def test_left_padding_compatibility(self):\n         pass\n \n-    def _get_input_ids_and_config(self, batch_size=2):\n+    def prepare_config_and_inputs_for_generate(self, *args, **kwargs):\n         # override because overwise we hit max possible seq length for model (4*8=32)\n         # decreasing the seq_length in tester causes errors for \"training_tests\", those need exactly max seq length\n         # NOTE: seq_length has to be multiple of 4, otherwise it fails for other tests\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        input_ids = inputs_dict.pop(self.input_name)\n-        _ = inputs_dict.pop(\"attention_mask\", None)\n-        _ = inputs_dict.pop(\"decoder_input_ids\", None)\n-        _ = inputs_dict.pop(\"decoder_attention_mask\", None)\n-        input_ids = input_ids[:batch_size, :16]\n-        attention_mask = torch.ones_like(input_ids, dtype=torch.long)[:batch_size, :16]\n-        config.eos_token_id = None\n-        config.forced_eos_token_id = None\n-        return config, input_ids, attention_mask, inputs_dict\n+        original_sequence_length = self.model_tester.seq_length\n+        self.model_tester.seq_length = 16\n+        test_inputs = super().prepare_config_and_inputs_for_generate(*args, **kwargs)\n+        self.model_tester.seq_length = original_sequence_length\n+        return test_inputs\n \n \n @require_torch"
        },
        {
            "sha": "cb09d44421f4820a3ca2812f9a31faf47eac5880",
            "filename": "tests/models/seamless_m4t/test_modeling_seamless_m4t.py",
            "status": "modified",
            "additions": 0,
            "deletions": 22,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29738f5b45d9c868ef9775388700c8d08b5800d/tests%2Fmodels%2Fseamless_m4t%2Ftest_modeling_seamless_m4t.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29738f5b45d9c868ef9775388700c8d08b5800d/tests%2Fmodels%2Fseamless_m4t%2Ftest_modeling_seamless_m4t.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fseamless_m4t%2Ftest_modeling_seamless_m4t.py?ref=d29738f5b45d9c868ef9775388700c8d08b5800d",
            "patch": "@@ -360,8 +360,6 @@ class SeamlessM4TModelWithSpeechInputTest(ModelTesterMixin, unittest.TestCase):\n     )\n     all_generative_model_classes = (SeamlessM4TForSpeechToText,) if is_torch_available() else ()\n \n-    input_name = \"input_features\"\n-\n     def setUp(self):\n         self.model_tester = SeamlessM4TModelTester(self, input_modality=\"speech\")\n         self.config_tester = ConfigTester(self, config_class=SeamlessM4TConfig)\n@@ -379,26 +377,6 @@ def test_model_from_pretrained(self):\n         model = SeamlessM4TModel.from_pretrained(model_name)\n         self.assertIsNotNone(model)\n \n-    def _get_input_ids_and_config(self, batch_size=2):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        input_ids = inputs_dict[self.input_name]\n-\n-        # cut to half length & take max batch_size 3\n-        sequence_length = input_ids.shape[-1] // 2\n-        input_ids = input_ids[:batch_size, :sequence_length]\n-\n-        # generate max 3 tokens\n-        max_length = input_ids.shape[-1] + 3\n-        if config.eos_token_id is not None and config.pad_token_id is None:\n-            # hack to allow generate for models such as GPT2 as is done in `generate()`\n-            if isinstance(config.eos_token_id, int):\n-                config.eos_token_id = [config.eos_token_id]\n-            config.pad_token_id = config.eos_token_id[0]\n-\n-        attention_mask = torch.ones(input_ids.shape[:2], dtype=torch.long)[:batch_size, :sequence_length]\n-\n-        return config, input_ids.float(), attention_mask, max_length\n-\n     def test_initialization(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n "
        },
        {
            "sha": "451fff0b35fb8c77b5e931cc93e3ec55d3e610ce",
            "filename": "tests/models/seamless_m4t_v2/test_modeling_seamless_m4t_v2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 22,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29738f5b45d9c868ef9775388700c8d08b5800d/tests%2Fmodels%2Fseamless_m4t_v2%2Ftest_modeling_seamless_m4t_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29738f5b45d9c868ef9775388700c8d08b5800d/tests%2Fmodels%2Fseamless_m4t_v2%2Ftest_modeling_seamless_m4t_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fseamless_m4t_v2%2Ftest_modeling_seamless_m4t_v2.py?ref=d29738f5b45d9c868ef9775388700c8d08b5800d",
            "patch": "@@ -376,8 +376,6 @@ class SeamlessM4Tv2ModelWithSpeechInputTest(ModelTesterMixin, unittest.TestCase)\n     )\n     all_generative_model_classes = (SeamlessM4Tv2ForSpeechToText,) if is_torch_available() else ()\n \n-    input_name = \"input_features\"\n-\n     def setUp(self):\n         self.model_tester = SeamlessM4Tv2ModelTester(self, input_modality=\"speech\")\n         self.config_tester = ConfigTester(self, config_class=SeamlessM4Tv2Config)\n@@ -395,26 +393,6 @@ def test_model_from_pretrained(self):\n         model = SeamlessM4Tv2Model.from_pretrained(model_name)\n         self.assertIsNotNone(model)\n \n-    def _get_input_ids_and_config(self, batch_size=2):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        input_ids = inputs_dict[self.input_name]\n-\n-        # cut to half length & take max batch_size 3\n-        sequence_length = input_ids.shape[-1] // 2\n-        input_ids = input_ids[:batch_size, :sequence_length]\n-\n-        # generate max 3 tokens\n-        max_length = input_ids.shape[-1] + 3\n-        if config.eos_token_id is not None and config.pad_token_id is None:\n-            # hack to allow generate for models such as GPT2 as is done in `generate()`\n-            if isinstance(config.eos_token_id, int):\n-                config.eos_token_id = [config.eos_token_id]\n-            config.pad_token_id = config.eos_token_id[0]\n-\n-        attention_mask = torch.ones(input_ids.shape[:2], dtype=torch.long)[:batch_size, :sequence_length]\n-\n-        return config, input_ids.float(), attention_mask, max_length\n-\n     def test_initialization(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n "
        },
        {
            "sha": "50446d4628af8c6978e7c2a74ab6dafcb814f953",
            "filename": "tests/models/speech_to_text/test_modeling_speech_to_text.py",
            "status": "modified",
            "additions": 6,
            "deletions": 54,
            "changes": 60,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29738f5b45d9c868ef9775388700c8d08b5800d/tests%2Fmodels%2Fspeech_to_text%2Ftest_modeling_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29738f5b45d9c868ef9775388700c8d08b5800d/tests%2Fmodels%2Fspeech_to_text%2Ftest_modeling_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fspeech_to_text%2Ftest_modeling_speech_to_text.py?ref=d29738f5b45d9c868ef9775388700c8d08b5800d",
            "patch": "@@ -282,20 +282,6 @@ class Speech2TextModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTest\n     test_pruning = False\n     test_missing_keys = False\n \n-    input_name = \"input_features\"\n-\n-    def _get_input_ids_and_config(self, batch_size=2):\n-        config, input_ids, attention_mask, inputs_dict = GenerationTesterMixin._get_input_ids_and_config(self)\n-\n-        # `input_ids` is actually `input_features` which is a 3D tensor.\n-        # We must overwrite the mask to make it 2D since the original `_get_input_ids_and_config` creates an\n-        # attention mask of the same shape as `input_ids`.\n-        if len(attention_mask.shape) > 2:\n-            sequence_length = input_ids.shape[1]\n-            attention_mask = torch.ones((batch_size, sequence_length), dtype=torch.long, device=attention_mask.device)\n-\n-        return config, input_ids, attention_mask, inputs_dict\n-\n     def setUp(self):\n         self.model_tester = Speech2TextModelTester(self)\n         self.config_tester = ConfigTester(self, config_class=Speech2TextConfig)\n@@ -632,46 +618,12 @@ def test_resize_embeddings_untied(self):\n     def test_generate_without_input_ids(self):\n         pass\n \n-    def _check_outputs(self, output, input_ids, config, use_cache=False, num_return_sequences=1):\n-        batch_size, seq_length = input_ids.shape[:2]\n-        subsampled_seq_length = self.model_tester.get_subsampled_output_lengths(seq_length)\n-        num_sequences_in_output = batch_size * num_return_sequences\n-        gen_len = (\n-            output.sequences.shape[-1] - 1 if config.is_encoder_decoder else output.sequences.shape[-1] - seq_length\n-        )\n-\n-        # scores\n-        self._check_scores(num_sequences_in_output, output.scores, length=gen_len, config=config)\n-\n-        # Attentions\n-        # encoder\n-        self._check_encoder_attention_for_generate(\n-            output.encoder_attentions, batch_size, config, subsampled_seq_length\n-        )\n-        # decoder\n-        self._check_attentions_for_generate(\n-            num_sequences_in_output,\n-            output.decoder_attentions,\n-            min_length=1,\n-            max_length=output.sequences.shape[-1],\n-            config=config,\n-            use_cache=use_cache,\n-        )\n-\n-        # Hidden States\n-        # encoder\n-        self._check_encoder_hidden_states_for_generate(\n-            output.encoder_hidden_states, batch_size, config, subsampled_seq_length\n-        )\n-\n-        # decoder\n-        self._check_hidden_states_for_generate(\n-            num_sequences_in_output,\n-            output.decoder_hidden_states,\n-            min_length=1,\n-            max_length=output.sequences.shape[-1],\n-            config=config,\n-            use_cache=use_cache,\n+    def _check_outputs(self, output, main_input, config, use_cache=False, num_return_sequences=1):\n+        # In this model, the index of `batch_size` and `sequence_length`` in `main_input` is different: they are the\n+        # first two dimensions of the tensor.\n+        main_input = main_input[:, :, 0]\n+        super()._check_outputs(\n+            output, main_input, config, use_cache=use_cache, num_return_sequences=num_return_sequences\n         )\n \n     def _create_and_check_torchscript(self, config, inputs_dict):"
        },
        {
            "sha": "97abf1a2cf2c2c1993d987805e8ccbda9eb37f07",
            "filename": "tests/models/speecht5/test_modeling_speecht5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29738f5b45d9c868ef9775388700c8d08b5800d/tests%2Fmodels%2Fspeecht5%2Ftest_modeling_speecht5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29738f5b45d9c868ef9775388700c8d08b5800d/tests%2Fmodels%2Fspeecht5%2Ftest_modeling_speecht5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fspeecht5%2Ftest_modeling_speecht5.py?ref=d29738f5b45d9c868ef9775388700c8d08b5800d",
            "patch": "@@ -177,8 +177,6 @@ class SpeechT5ModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase\n     test_headmasking = False\n     test_resize_embeddings = False\n \n-    input_name = \"input_values\"\n-\n     def setUp(self):\n         self.model_tester = SpeechT5ModelTester(self)\n         self.config_tester = ConfigTester(self, config_class=SpeechT5Config, hidden_size=37)\n@@ -375,8 +373,6 @@ class SpeechT5ForSpeechToTextTest(ModelTesterMixin, unittest.TestCase):\n     test_pruning = False\n     test_headmasking = False\n \n-    input_name = \"input_values\"\n-\n     def setUp(self):\n         self.model_tester = SpeechT5ForSpeechToTextTester(self)\n         self.config_tester = ConfigTester(self, config_class=SpeechT5Config, hidden_size=37)\n@@ -895,8 +891,6 @@ class SpeechT5ForTextToSpeechTest(ModelTesterMixin, unittest.TestCase):\n     test_pruning = False\n     test_headmasking = False\n \n-    input_name = \"input_ids\"\n-\n     def setUp(self):\n         self.model_tester = SpeechT5ForTextToSpeechTester(self)\n         self.config_tester = ConfigTester(self, config_class=SpeechT5Config, hidden_size=37)\n@@ -1441,8 +1435,6 @@ class SpeechT5ForSpeechToSpeechTest(ModelTesterMixin, unittest.TestCase):\n     test_headmasking = False\n     test_resize_embeddings = False\n \n-    input_name = \"input_values\"\n-\n     def setUp(self):\n         self.model_tester = SpeechT5ForSpeechToSpeechTester(self)\n         self.config_tester = ConfigTester(self, config_class=SpeechT5Config, hidden_size=37)\n@@ -1854,8 +1846,6 @@ class SpeechT5HifiGanTest(ModelTesterMixin, unittest.TestCase):\n     is_encoder_decoder = False\n     has_attentions = False\n \n-    input_name = \"spectrogram\"\n-\n     def setUp(self):\n         self.model_tester = SpeechT5HifiGanTester(self)\n         self.config_tester = ConfigTester(self, config_class=SpeechT5HifiGanConfig)"
        },
        {
            "sha": "c88fda6fb84e06ca9de65fb636c21d9e3264abda",
            "filename": "tests/models/stablelm/test_modeling_stablelm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29738f5b45d9c868ef9775388700c8d08b5800d/tests%2Fmodels%2Fstablelm%2Ftest_modeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29738f5b45d9c868ef9775388700c8d08b5800d/tests%2Fmodels%2Fstablelm%2Ftest_modeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fstablelm%2Ftest_modeling_stablelm.py?ref=d29738f5b45d9c868ef9775388700c8d08b5800d",
            "patch": "@@ -113,7 +113,7 @@ def prepare_config_and_inputs(self):\n \n         input_mask = None\n         if self.use_input_mask:\n-            input_mask = torch.tril(torch.ones(self.batch_size, self.seq_length)).to(torch_device)\n+            input_mask = torch.tril(torch.ones_like(input_ids).to(torch_device))\n \n         token_type_ids = None\n         if self.use_token_type_ids:"
        },
        {
            "sha": "7ab7faa90ea0900667f1ef004438a4a35be05aa6",
            "filename": "tests/models/starcoder2/test_modeling_starcoder2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29738f5b45d9c868ef9775388700c8d08b5800d/tests%2Fmodels%2Fstarcoder2%2Ftest_modeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29738f5b45d9c868ef9775388700c8d08b5800d/tests%2Fmodels%2Fstarcoder2%2Ftest_modeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fstarcoder2%2Ftest_modeling_starcoder2.py?ref=d29738f5b45d9c868ef9775388700c8d08b5800d",
            "patch": "@@ -107,7 +107,7 @@ def prepare_config_and_inputs(self):\n \n         input_mask = None\n         if self.use_input_mask:\n-            input_mask = torch.tril(torch.ones(self.batch_size, self.seq_length)).to(torch_device)\n+            input_mask = torch.tril(torch.ones_like(input_ids).to(torch_device))\n \n         token_type_ids = None\n         if self.use_token_type_ids:"
        },
        {
            "sha": "037f1b1e2188a407cdeae9a66bdef28eecc1e848",
            "filename": "tests/models/t5/test_modeling_tf_t5.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29738f5b45d9c868ef9775388700c8d08b5800d/tests%2Fmodels%2Ft5%2Ftest_modeling_tf_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29738f5b45d9c868ef9775388700c8d08b5800d/tests%2Fmodels%2Ft5%2Ftest_modeling_tf_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ft5%2Ftest_modeling_tf_t5.py?ref=d29738f5b45d9c868ef9775388700c8d08b5800d",
            "patch": "@@ -470,7 +470,7 @@ def test_greedy_xla_generate_simple(self):\n         self.assertListEqual(expected_output_string, output_strings_xla)\n \n     @slow\n-    def test_greedy_generate(self):\n+    def test_t5_greedy_generate(self):\n         model = TFT5ForConditionalGeneration.from_pretrained(\"google-t5/t5-small\")\n         tokenizer = T5Tokenizer.from_pretrained(\"google-t5/t5-small\")\n \n@@ -520,7 +520,7 @@ def test_sample_xla_generate_simple(self):\n             self.assertListEqual(expected_output_string_xla, output_strings_xla)\n \n     @slow\n-    def test_sample_generate(self):\n+    def test_t5_sample_generate(self):\n         model = TFT5ForConditionalGeneration.from_pretrained(\"google-t5/t5-small\")\n         tokenizer = T5Tokenizer.from_pretrained(\"google-t5/t5-small\")\n "
        },
        {
            "sha": "84d28c645874d11f8eb6cf1259c238431e45dea8",
            "filename": "tests/models/univnet/test_modeling_univnet.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29738f5b45d9c868ef9775388700c8d08b5800d/tests%2Fmodels%2Funivnet%2Ftest_modeling_univnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29738f5b45d9c868ef9775388700c8d08b5800d/tests%2Fmodels%2Funivnet%2Ftest_modeling_univnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Funivnet%2Ftest_modeling_univnet.py?ref=d29738f5b45d9c868ef9775388700c8d08b5800d",
            "patch": "@@ -118,8 +118,6 @@ class UnivNetModelTest(ModelTesterMixin, unittest.TestCase):\n     is_encoder_decoder = False\n     has_attentions = False\n \n-    input_name = \"input_features\"\n-\n     def setUp(self):\n         self.model_tester = UnivNetModelTester(self)\n         self.config_tester = ConfigTester("
        },
        {
            "sha": "366194090953f5e91b6d6ce14fd7a61a5f5a2782",
            "filename": "tests/models/vits/test_modeling_vits.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29738f5b45d9c868ef9775388700c8d08b5800d/tests%2Fmodels%2Fvits%2Ftest_modeling_vits.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29738f5b45d9c868ef9775388700c8d08b5800d/tests%2Fmodels%2Fvits%2Ftest_modeling_vits.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvits%2Ftest_modeling_vits.py?ref=d29738f5b45d9c868ef9775388700c8d08b5800d",
            "patch": "@@ -167,8 +167,6 @@ class VitsModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     test_torchscript = False\n     has_attentions = False\n \n-    input_name = \"input_ids\"\n-\n     def setUp(self):\n         self.model_tester = VitsModelTester(self)\n         self.config_tester = ConfigTester(self, config_class=VitsConfig, hidden_size=37)"
        },
        {
            "sha": "c719fcf989dae3c8db7e4dc5f0ced4ca6ea512fa",
            "filename": "tests/models/whisper/test_modeling_whisper.py",
            "status": "modified",
            "additions": 0,
            "deletions": 46,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29738f5b45d9c868ef9775388700c8d08b5800d/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29738f5b45d9c868ef9775388700c8d08b5800d/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py?ref=d29738f5b45d9c868ef9775388700c8d08b5800d",
            "patch": "@@ -395,8 +395,6 @@ class WhisperModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMi\n     # `0.5` is for `test_disk_offload` (which also works for `test_model_parallelism`)\n     model_split_percents = [0.5, 0.8, 0.9]\n \n-    input_name = \"input_features\"\n-\n     # TODO: Fix the failed tests\n     def is_pipeline_test_to_skip(\n         self, pipeline_test_casse_name, config_class, model_architecture, tokenizer_name, processor_name\n@@ -868,48 +866,6 @@ def test_resize_embeddings_untied(self):\n     def test_generate_without_input_ids(self):\n         pass\n \n-    def _check_outputs(self, output, input_ids, config, use_cache=False, num_return_sequences=1):\n-        batch_size, mel, seq_length = input_ids.shape\n-        subsampled_seq_length = self.model_tester.get_subsampled_output_lengths(seq_length)\n-        num_sequences_in_output = batch_size * num_return_sequences\n-        gen_len = (\n-            output.sequences.shape[-1] - 1 if config.is_encoder_decoder else output.sequences.shape[-1] - seq_length\n-        )\n-\n-        # scores\n-        self._check_scores(num_sequences_in_output, output.scores, length=gen_len, config=config)\n-\n-        # Attentions\n-        # encoder\n-        self._check_encoder_attention_for_generate(\n-            output.encoder_attentions, batch_size, config, subsampled_seq_length\n-        )\n-        # decoder\n-        self._check_attentions_for_generate(\n-            num_sequences_in_output,\n-            output.decoder_attentions,\n-            min_length=1,\n-            max_length=output.sequences.shape[-1],\n-            config=config,\n-            use_cache=use_cache,\n-        )\n-\n-        # Hidden States\n-        # encoder\n-        self._check_encoder_hidden_states_for_generate(\n-            output.encoder_hidden_states, batch_size, config, subsampled_seq_length\n-        )\n-\n-        # decoder\n-        self._check_hidden_states_for_generate(\n-            num_sequences_in_output,\n-            output.decoder_hidden_states,\n-            min_length=1,\n-            max_length=output.sequences.shape[-1],\n-            config=config,\n-            use_cache=use_cache,\n-        )\n-\n     @require_flash_attn\n     @require_torch_gpu\n     @pytest.mark.flash_attn_test\n@@ -3511,8 +3467,6 @@ class WhisperEncoderModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.\n     test_pruning = False\n     test_missing_keys = False\n \n-    input_name = \"input_features\"\n-\n     def setUp(self):\n         self.model_tester = WhisperEncoderModelTester(self)\n         self.config_tester = ConfigTester(self, config_class=WhisperConfig)"
        }
    ],
    "stats": {
        "total": 1147,
        "additions": 241,
        "deletions": 906
    }
}