{
    "author": "rootonchair",
    "message": "Add Fast Image Processor for PoolFormer (#37182)\n\n* support poolformer fast image processor\n\n* support test for crop_pct=None\n\n* run make style\n\n* Apply suggestions from code review\n\n* rename test\n\n---------\n\nCo-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>",
    "sha": "dea1919be4bb41ab6779b802caf427a8dae64924",
    "files": [
        {
            "sha": "60573162d686a4cf000bbb1fd0850ca31614bf9f",
            "filename": "docs/source/en/model_doc/poolformer.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/dea1919be4bb41ab6779b802caf427a8dae64924/docs%2Fsource%2Fen%2Fmodel_doc%2Fpoolformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/dea1919be4bb41ab6779b802caf427a8dae64924/docs%2Fsource%2Fen%2Fmodel_doc%2Fpoolformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fpoolformer.md?ref=dea1919be4bb41ab6779b802caf427a8dae64924",
            "patch": "@@ -73,6 +73,11 @@ If you're interested in submitting a resource to be included here, please feel f\n [[autodoc]] PoolFormerImageProcessor\n     - preprocess\n \n+## PoolFormerImageProcessorFast\n+\n+[[autodoc]] PoolFormerImageProcessorFast\n+    - preprocess\n+\n ## PoolFormerModel\n \n [[autodoc]] PoolFormerModel"
        },
        {
            "sha": "d0ac43f7471ba6de2ee2351932bbe6564fc34e05",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/dea1919be4bb41ab6779b802caf427a8dae64924/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dea1919be4bb41ab6779b802caf427a8dae64924/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=dea1919be4bb41ab6779b802caf427a8dae64924",
            "patch": "@@ -131,7 +131,7 @@\n             (\"phi4_multimodal\", \"Phi4MultimodalImageProcessorFast\"),\n             (\"pix2struct\", (\"Pix2StructImageProcessor\",)),\n             (\"pixtral\", (\"PixtralImageProcessor\", \"PixtralImageProcessorFast\")),\n-            (\"poolformer\", (\"PoolFormerImageProcessor\",)),\n+            (\"poolformer\", (\"PoolFormerImageProcessor\", \"PoolFormerImageProcessorFast\")),\n             (\"prompt_depth_anything\", (\"PromptDepthAnythingImageProcessor\",)),\n             (\"pvt\", (\"PvtImageProcessor\", \"PvtImageProcessorFast\")),\n             (\"pvt_v2\", (\"PvtImageProcessor\", \"PvtImageProcessorFast\")),"
        },
        {
            "sha": "2e80bb47c302c88ffe33450d48a3b6ac6cf2ffc0",
            "filename": "src/transformers/models/poolformer/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/dea1919be4bb41ab6779b802caf427a8dae64924/src%2Ftransformers%2Fmodels%2Fpoolformer%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dea1919be4bb41ab6779b802caf427a8dae64924/src%2Ftransformers%2Fmodels%2Fpoolformer%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpoolformer%2F__init__.py?ref=dea1919be4bb41ab6779b802caf427a8dae64924",
            "patch": "@@ -21,6 +21,7 @@\n     from .configuration_poolformer import *\n     from .feature_extraction_poolformer import *\n     from .image_processing_poolformer import *\n+    from .image_processing_poolformer_fast import *\n     from .modeling_poolformer import *\n else:\n     import sys"
        },
        {
            "sha": "37e81a5f5f9e6c1351bf790182763814946fa76c",
            "filename": "src/transformers/models/poolformer/image_processing_poolformer_fast.py",
            "status": "added",
            "additions": 270,
            "deletions": 0,
            "changes": 270,
            "blob_url": "https://github.com/huggingface/transformers/blob/dea1919be4bb41ab6779b802caf427a8dae64924/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fimage_processing_poolformer_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dea1919be4bb41ab6779b802caf427a8dae64924/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fimage_processing_poolformer_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fimage_processing_poolformer_fast.py?ref=dea1919be4bb41ab6779b802caf427a8dae64924",
            "patch": "@@ -0,0 +1,270 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Fast Image processor class for PoolFormer.\"\"\"\n+\n+from typing import Optional, Union\n+\n+from ...image_processing_utils_fast import (\n+    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n+    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING_PREPROCESS,\n+    BaseImageProcessorFast,\n+    BatchFeature,\n+    DefaultFastImageProcessorKwargs,\n+)\n+from ...image_transforms import (\n+    ChannelDimension,\n+    get_resize_output_image_size,\n+    get_size_with_aspect_ratio,\n+    group_images_by_shape,\n+    reorder_images,\n+)\n+from ...image_utils import (\n+    IMAGENET_DEFAULT_MEAN,\n+    IMAGENET_DEFAULT_STD,\n+    ImageInput,\n+    PILImageResampling,\n+    SizeDict,\n+    get_image_size_for_max_height_width,\n+)\n+from ...processing_utils import Unpack\n+from ...utils import (\n+    TensorType,\n+    add_start_docstrings,\n+    is_torch_available,\n+    is_torchvision_available,\n+    is_torchvision_v2_available,\n+)\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+if is_torchvision_available():\n+    if is_torchvision_v2_available():\n+        from torchvision.transforms.v2 import functional as F\n+    else:\n+        from torchvision.transforms import functional as F\n+\n+\n+class PoolFormerFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n+    crop_pct: Optional[float]\n+\n+\n+@add_start_docstrings(\n+    \"Constructs a fast PoolFormer image processor.\",\n+    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n+    \"\"\"\n+        crop_pct (`float`, *optional*, defaults to `self.crop_pct`):\n+            Percentage of the image to crop. Only has an effect if `do_resize` is set to `True`.\n+    \"\"\",\n+)\n+class PoolFormerImageProcessorFast(BaseImageProcessorFast):\n+    resample = PILImageResampling.BICUBIC\n+    image_mean = IMAGENET_DEFAULT_MEAN\n+    image_std = IMAGENET_DEFAULT_STD\n+    size = {\"shortest_edge\": 224}\n+    default_to_square = False\n+    crop_size = {\"height\": 224, \"width\": 224}\n+    crop_pct = 0.9\n+    do_resize = True\n+    do_center_crop = True\n+    do_rescale = True\n+    do_normalize = True\n+    valid_kwargs = PoolFormerFastImageProcessorKwargs\n+\n+    def __init__(self, **kwargs: Unpack[PoolFormerFastImageProcessorKwargs]):\n+        super().__init__(**kwargs)\n+\n+    @add_start_docstrings(\n+        BASE_IMAGE_PROCESSOR_FAST_DOCSTRING_PREPROCESS,\n+        \"\"\"\n+            crop_pct (`float`, *optional*, defaults to `self.crop_pct`):\n+                Percentage of the image to crop. Only has an effect if `do_resize` is set to `True`.\n+        \"\"\",\n+    )\n+    def preprocess(self, images: ImageInput, **kwargs: Unpack[PoolFormerFastImageProcessorKwargs]) -> BatchFeature:\n+        return super().preprocess(images, **kwargs)\n+\n+    def resize(\n+        self,\n+        image: \"torch.Tensor\",\n+        size: SizeDict,\n+        crop_pct: Optional[float] = None,\n+        interpolation: \"F.InterpolationMode\" = None,\n+        antialias: bool = True,\n+        **kwargs,\n+    ) -> \"torch.Tensor\":\n+        \"\"\"\n+        Resize an image.\n+\n+        If crop_pct is unset:\n+            - size is `{\"height\": h, \"width\": w}`: the image is resized to `(h, w)`.\n+            - size is `{\"shortest_edge\": s}`: the shortest edge of the image is resized to s whilst maintaining the\n+              aspect ratio.\n+\n+        if crop_pct is set:\n+            - size is `{\"height\": h, \"width\": w}`: the image is resized to `(int(floor(h/crop_pct)),\n+              int(floor(w/crop_pct)))`\n+            - size is `{\"height\": c, \"width\": c}`: the shortest edge of the image is resized to `int(floor(c/crop_pct)`\n+              whilst maintaining the aspect ratio.\n+            - size is `{\"shortest_edge\": c}`: the shortest edge of the image is resized to `int(floor(c/crop_pct)`\n+              whilst maintaining the aspect ratio.\n+\n+        Args:\n+            image (`torch.Tensor`):\n+                Image to resize.\n+            size (`SizeDict`):\n+                Dictionary in the format `{\"height\": int, \"width\": int}` specifying the size of the output image.\n+            crop_pct (`float`, *optional*):\n+                Percentage of the image that will be cropped from the center. If set, the image is resized\n+            resample (`InterpolationMode`, *optional*, defaults to `InterpolationMode.BILINEAR`):\n+                `InterpolationMode` filter to use when resizing the image e.g. `InterpolationMode.BICUBIC`.\n+\n+        Returns:\n+            `torch.Tensor`: The resized image.\n+        \"\"\"\n+        interpolation = interpolation if interpolation is not None else F.InterpolationMode.BILINEAR\n+        if crop_pct is not None:\n+            if size.shortest_edge:\n+                scale_size = int(size.shortest_edge / crop_pct)\n+            elif size.height and size.width:\n+                if size.height == size.width:\n+                    scale_size = int(size.height / crop_pct)\n+                else:\n+                    scale_size = (int(size.height / crop_pct), int(size.width / crop_pct))\n+            else:\n+                raise ValueError(\"Invalid size for resize: {}\".format(size))\n+\n+            new_size = get_resize_output_image_size(\n+                image,\n+                size=scale_size,\n+                default_to_square=False,\n+                input_data_format=ChannelDimension.FIRST,\n+            )\n+        else:\n+            if size.shortest_edge and size.longest_edge:\n+                # Resize the image so that the shortest edge or the longest edge is of the given size\n+                # while maintaining the aspect ratio of the original image.\n+                new_size = get_size_with_aspect_ratio(\n+                    image.size()[-2:],\n+                    size.shortest_edge,\n+                    size.longest_edge,\n+                )\n+            elif size.shortest_edge:\n+                new_size = get_resize_output_image_size(\n+                    image,\n+                    size=size.shortest_edge,\n+                    default_to_square=False,\n+                    input_data_format=ChannelDimension.FIRST,\n+                )\n+            elif size.max_height and size.max_width:\n+                new_size = get_image_size_for_max_height_width(image.size()[-2:], size.max_height, size.max_width)\n+            elif size.height and size.width:\n+                new_size = (size.height, size.width)\n+            else:\n+                raise ValueError(\n+                    \"Size must contain 'height' and 'width' keys, or 'max_height' and 'max_width', or 'shortest_edge' key. Got\"\n+                    f\" {size}.\"\n+                )\n+        return F.resize(image, new_size, interpolation=interpolation, antialias=antialias)\n+\n+    def center_crop(\n+        self,\n+        image: \"torch.Tensor\",\n+        size: SizeDict,\n+        **kwargs,\n+    ) -> \"torch.Tensor\":\n+        \"\"\"\n+        Center crop an image to `(size[\"height\"], size[\"width\"])`. If the input size is smaller than `crop_size` along\n+        any edge, the image is padded with 0's and then center cropped.\n+\n+        Args:\n+            image (`\"torch.Tensor\"`):\n+                Image to center crop.\n+            size (`Dict[str, int]`):\n+                Size of the output image.\n+\n+        Returns:\n+            `torch.Tensor`: The center cropped image.\n+        \"\"\"\n+        if size.height is None or size.width is None:\n+            raise ValueError(f\"The size dictionary must have keys 'height' and 'width'. Got {size.keys()}\")\n+        image_height, image_width = image.shape[-2:]\n+        crop_height, crop_width = size.height, size.width\n+\n+        if crop_width > image_width or crop_height > image_height:\n+            padding_ltrb = [\n+                (crop_width - image_width) // 2 if crop_width > image_width else 0,\n+                (crop_height - image_height) // 2 if crop_height > image_height else 0,\n+                (crop_width - image_width + 1) // 2 if crop_width > image_width else 0,\n+                (crop_height - image_height + 1) // 2 if crop_height > image_height else 0,\n+            ]\n+            image = F.pad(image, padding_ltrb, fill=0)  # PIL uses fill value 0\n+            image_height, image_width = image.shape[-2:]\n+            if crop_width == image_width and crop_height == image_height:\n+                return image\n+\n+        crop_top = int((image_height - crop_height) / 2.0)\n+        crop_left = int((image_width - crop_width) / 2.0)\n+        return F.crop(image, crop_top, crop_left, crop_height, crop_width)\n+\n+    def _preprocess(\n+        self,\n+        images: list[\"torch.Tensor\"],\n+        do_resize: bool,\n+        size: SizeDict,\n+        crop_pct: float,\n+        interpolation: Optional[\"F.InterpolationMode\"],\n+        do_center_crop: bool,\n+        crop_size: SizeDict,\n+        do_rescale: bool,\n+        rescale_factor: float,\n+        do_normalize: bool,\n+        image_mean: Optional[Union[float, list[float]]],\n+        image_std: Optional[Union[float, list[float]]],\n+        return_tensors: Optional[Union[str, TensorType]],\n+        **kwargs,\n+    ) -> BatchFeature:\n+        # Group images by size for batched resizing\n+        grouped_images, grouped_images_index = group_images_by_shape(images)\n+        resized_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            if do_resize:\n+                stacked_images = self.resize(\n+                    image=stacked_images, size=size, crop_pct=crop_pct, interpolation=interpolation\n+                )\n+            resized_images_grouped[shape] = stacked_images\n+        resized_images = reorder_images(resized_images_grouped, grouped_images_index)\n+\n+        # Group images by size for further processing\n+        # Needed in case do_resize is False, or resize returns images with different sizes\n+        grouped_images, grouped_images_index = group_images_by_shape(resized_images)\n+        processed_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            if do_center_crop:\n+                stacked_images = self.center_crop(stacked_images, crop_size)\n+            # Fused rescale and normalize\n+            stacked_images = self.rescale_and_normalize(\n+                stacked_images, do_rescale, rescale_factor, do_normalize, image_mean, image_std\n+            )\n+            processed_images_grouped[shape] = stacked_images\n+\n+        processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n+        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n+\n+        return BatchFeature(data={\"pixel_values\": processed_images}, tensor_type=return_tensors)\n+\n+\n+__all__ = [\"PoolFormerImageProcessorFast\"]"
        },
        {
            "sha": "bfaebd92d6b1b5c7e5681af9eda97109a053a537",
            "filename": "tests/models/poolformer/test_image_processing_poolformer.py",
            "status": "modified",
            "additions": 28,
            "deletions": 14,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/dea1919be4bb41ab6779b802caf427a8dae64924/tests%2Fmodels%2Fpoolformer%2Ftest_image_processing_poolformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dea1919be4bb41ab6779b802caf427a8dae64924/tests%2Fmodels%2Fpoolformer%2Ftest_image_processing_poolformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpoolformer%2Ftest_image_processing_poolformer.py?ref=dea1919be4bb41ab6779b802caf427a8dae64924",
            "patch": "@@ -15,14 +15,17 @@\n import unittest\n \n from transformers.testing_utils import require_torch, require_vision\n-from transformers.utils import is_vision_available\n+from transformers.utils import is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n \n \n if is_vision_available():\n     from transformers import PoolFormerImageProcessor\n \n+    if is_torchvision_available():\n+        from transformers import PoolFormerImageProcessorFast\n+\n \n class PoolFormerImageProcessingTester:\n     def __init__(\n@@ -85,6 +88,7 @@ def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=F\n @require_vision\n class PoolFormerImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n     image_processing_class = PoolFormerImageProcessor if is_vision_available() else None\n+    fast_image_processing_class = PoolFormerImageProcessorFast if is_torchvision_available() else None\n \n     def setUp(self):\n         super().setUp()\n@@ -95,19 +99,29 @@ def image_processor_dict(self):\n         return self.image_processor_tester.prepare_image_processor_dict()\n \n     def test_image_processor_properties(self):\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        self.assertTrue(hasattr(image_processing, \"do_resize_and_center_crop\"))\n-        self.assertTrue(hasattr(image_processing, \"size\"))\n-        self.assertTrue(hasattr(image_processing, \"crop_pct\"))\n-        self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n-        self.assertTrue(hasattr(image_processing, \"image_mean\"))\n-        self.assertTrue(hasattr(image_processing, \"image_std\"))\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            self.assertTrue(hasattr(image_processing, \"do_resize_and_center_crop\"))\n+            self.assertTrue(hasattr(image_processing, \"size\"))\n+            self.assertTrue(hasattr(image_processing, \"crop_pct\"))\n+            self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n+            self.assertTrue(hasattr(image_processing, \"image_mean\"))\n+            self.assertTrue(hasattr(image_processing, \"image_std\"))\n \n     def test_image_processor_from_dict_with_kwargs(self):\n-        image_processor = self.image_processing_class.from_dict(self.image_processor_dict)\n-        self.assertEqual(image_processor.size, {\"shortest_edge\": 30})\n-        self.assertEqual(image_processor.crop_size, {\"height\": 30, \"width\": 30})\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict)\n+            self.assertEqual(image_processor.size, {\"shortest_edge\": 30})\n+            self.assertEqual(image_processor.crop_size, {\"height\": 30, \"width\": 30})\n+\n+            image_processor = self.image_processing_class.from_dict(self.image_processor_dict, size=42, crop_size=84)\n+            self.assertEqual(image_processor.size, {\"shortest_edge\": 42})\n+            self.assertEqual(image_processor.crop_size, {\"height\": 84, \"width\": 84})\n \n-        image_processor = self.image_processing_class.from_dict(self.image_processor_dict, size=42, crop_size=84)\n-        self.assertEqual(image_processor.size, {\"shortest_edge\": 42})\n-        self.assertEqual(image_processor.crop_size, {\"height\": 84, \"width\": 84})\n+\n+@require_torch\n+@require_vision\n+class PoolFormerImageProcessingNoCropPctTest(PoolFormerImageProcessingTest):\n+    def setUp(self):\n+        super().setUp()\n+        self.image_processor_tester = PoolFormerImageProcessingTester(self, crop_pct=None)"
        }
    ],
    "stats": {
        "total": 320,
        "additions": 305,
        "deletions": 15
    }
}