{
    "author": "zucchini-nlp",
    "message": "[transformers x vLLM] standardize processors (#37915)\n\n* standardize\n\n* fix tests\n\n* batch update some processors, not final yet\n\n* oke, now I tested that everything indeed runs. Still needs prettification\n\n* emu3\n\n* fixup\n\n* gemma3 but it doesn't generate anything\n\n* fuyu\n\n* update\n\n* why?\n\n* Update src/transformers/models/aya_vision/processing_aya_vision.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* address comments\n\n* bc\n\n* why do we need to guard import this every time?\n\n* i hate guarded imports\n\n* i am blind\n\n---------\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>",
    "sha": "9e1017b479d0390f30c6376169a236264da2ec47",
    "files": [
        {
            "sha": "eb9badef1ef803d69d05a86cd7ebc5cc34248a62",
            "filename": "src/transformers/models/aria/image_processing_aria.py",
            "status": "modified",
            "additions": 21,
            "deletions": 0,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e1017b479d0390f30c6376169a236264da2ec47/src%2Ftransformers%2Fmodels%2Faria%2Fimage_processing_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e1017b479d0390f30c6376169a236264da2ec47/src%2Ftransformers%2Fmodels%2Faria%2Fimage_processing_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fimage_processing_aria.py?ref=9e1017b479d0390f30c6376169a236264da2ec47",
            "patch": "@@ -500,5 +500,26 @@ def get_image_patches(\n         ]\n         return patches\n \n+    def get_number_of_image_patches(self, height: int, width: int, images_kwargs=None):\n+        \"\"\"\n+        A utility that returns number of image patches for a given image size.\n+\n+        Args:\n+            height (`int`):\n+                Height of the input image.\n+            width (`int`):\n+                Width of the input image.\n+            images_kwargs (`dict`, *optional*)\n+                Any kwargs to override defaults of the image processor.\n+        Returns:\n+            `int`: Number of patches per image.\n+        \"\"\"\n+        split_image = images_kwargs.get(\"split_image\", None) or self.split_image\n+        max_image_size = images_kwargs.get(\"max_image_size\", None) or self.max_image_size\n+\n+        resized_height, resized_width = select_best_resolution((height, width), self.split_resolutions)\n+        num_patches = 1 if not split_image else resized_height // max_image_size * resized_width // max_image_size\n+        return num_patches\n+\n \n __all__ = [\"AriaImageProcessor\"]"
        },
        {
            "sha": "acf5c65403dffbd1c8a5b5808fd7adacb04bd4d3",
            "filename": "src/transformers/models/aria/modular_aria.py",
            "status": "modified",
            "additions": 58,
            "deletions": 6,
            "changes": 64,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e1017b479d0390f30c6376169a236264da2ec47/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e1017b479d0390f30c6376169a236264da2ec47/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py?ref=9e1017b479d0390f30c6376169a236264da2ec47",
            "patch": "@@ -34,7 +34,7 @@\n )\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_utils import PreTrainedModel\n-from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n+from ...processing_utils import MultiModalData, ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils import PreTokenizedInput, TextInput\n from ...utils import LossKwargs, TensorType, auto_docstring, can_return_tuple, logging\n from ...utils.import_utils import is_torch_available\n@@ -884,11 +884,33 @@ def get_image_patches(\n         ]\n         return patches\n \n+    def get_number_of_image_patches(self, height: int, width: int, images_kwargs=None):\n+        \"\"\"\n+        A utility that returns number of image patches for a given image size.\n+\n+        Args:\n+            height (`int`):\n+                Height of the input image.\n+            width (`int`):\n+                Width of the input image.\n+            images_kwargs (`dict`, *optional*)\n+                Any kwargs to override defaults of the image processor.\n+        Returns:\n+            `int`: Number of patches per image.\n+        \"\"\"\n+        split_image = images_kwargs.get(\"split_image\", None) or self.split_image\n+        max_image_size = images_kwargs.get(\"max_image_size\", None) or self.max_image_size\n+\n+        resized_height, resized_width = select_best_resolution((height, width), self.split_resolutions)\n+        num_patches = 1 if not split_image else resized_height // max_image_size * resized_width // max_image_size\n+        return num_patches\n+\n \n class AriaProcessorKwargs(ProcessingKwargs, total=False):\n     _defaults = {\n         \"text_kwargs\": {\n             \"padding\": False,\n+            \"return_mm_token_type_ids\": False,\n         },\n         \"images_kwargs\": {\n             \"max_image_size\": 980,\n@@ -978,10 +1000,7 @@ def __call__(\n             raise ValueError(\"Invalid input text. Please provide a string, or a list of strings\")\n \n         if images is not None:\n-            image_inputs = self.image_processor(\n-                images,\n-                **output_kwargs[\"images_kwargs\"],\n-            )\n+            image_inputs = self.image_processor(images, **output_kwargs[\"images_kwargs\"])\n             # expand the image_token according to the num_crops and tokens per image\n             tokens_per_image = self.size_conversion[image_inputs.pixel_values.shape[2]]\n             prompt_strings = []\n@@ -995,11 +1014,44 @@ def __call__(\n             prompt_strings = text\n \n         return_tensors = output_kwargs[\"text_kwargs\"].pop(\"return_tensors\", None)\n-        text_inputs = self.tokenizer(prompt_strings, **output_kwargs[\"text_kwargs\"])\n+        return_mm_token_type_ids = output_kwargs[\"text_kwargs\"].pop(\"return_mm_token_type_ids\", False)\n+        text_inputs = self.tokenizer(prompt_strings, **output_kwargs[\"text_kwargs\"], return_tensors=None)\n         self._check_special_mm_tokens(prompt_strings, text_inputs, modalities=[\"image\"])\n \n+        if return_mm_token_type_ids:\n+            array_ids = np.array(text_inputs[\"input_ids\"])\n+            mm_token_type_ids = np.zeros_like(text_inputs[\"input_ids\"])\n+            mm_token_type_ids[array_ids == self.image_token_id] = 1\n+            text_inputs[\"mm_token_type_ids\"] = mm_token_type_ids.tolist()\n+\n         return BatchFeature(data={**text_inputs, **image_inputs}, tensor_type=return_tensors)\n \n+    def _get_num_multimodal_tokens(self, image_sizes=None, **kwargs):\n+        \"\"\"\n+        Computes the number of placeholder tokens needed for multimodal inputs with the given sizes.\n+        Args:\n+            image_sizes (`List[List[int]]`, *optional*):\n+                The input sizes formatted as (height, width) per each image.\n+        Returns:\n+            `MultiModalData`: A `MultiModalData` object holding number of tokens per each of the provided\n+            input modalities, along with other useful data.\n+        \"\"\"\n+\n+        vision_data = {}\n+        if image_sizes is not None:\n+            images_kwargs = AriaProcessorKwargs._defaults.get(\"images_kwargs\", {})\n+            images_kwargs.update(kwargs)\n+\n+            max_size = images_kwargs.get(\"max_image_size\", None) or self.image_processor.max_image_size\n+            num_image_patches = [\n+                self.image_processor.get_number_of_image_patches(*image_size, images_kwargs)\n+                for image_size in image_sizes\n+            ]\n+            num_image_tokens = [self.size_conversion[max_size] * num_patches for num_patches in num_image_patches]\n+            vision_data.update({\"num_image_tokens\": num_image_tokens, \"num_image_patches\": num_image_patches})\n+\n+        return MultiModalData(**vision_data)\n+\n     def batch_decode(self, *args, **kwargs):\n         \"\"\"\n         This method forwards all its arguments to LlamaTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please"
        },
        {
            "sha": "6fe5ea3d5992c9b1cc90996467f68479af32b6c9",
            "filename": "src/transformers/models/aria/processing_aria.py",
            "status": "modified",
            "additions": 39,
            "deletions": 6,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e1017b479d0390f30c6376169a236264da2ec47/src%2Ftransformers%2Fmodels%2Faria%2Fprocessing_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e1017b479d0390f30c6376169a236264da2ec47/src%2Ftransformers%2Fmodels%2Faria%2Fprocessing_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fprocessing_aria.py?ref=9e1017b479d0390f30c6376169a236264da2ec47",
            "patch": "@@ -20,9 +20,11 @@\n # limitations under the License.\n from typing import Dict, List, Optional, Union\n \n+import numpy as np\n+\n from ...image_processing_utils import BatchFeature\n from ...image_utils import ImageInput\n-from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n+from ...processing_utils import MultiModalData, ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils import PreTokenizedInput, TextInput\n from ...utils import TensorType\n from ..auto import AutoTokenizer\n@@ -32,6 +34,7 @@ class AriaProcessorKwargs(ProcessingKwargs, total=False):\n     _defaults = {\n         \"text_kwargs\": {\n             \"padding\": False,\n+            \"return_mm_token_type_ids\": False,\n         },\n         \"images_kwargs\": {\n             \"max_image_size\": 980,\n@@ -121,10 +124,7 @@ def __call__(\n             raise ValueError(\"Invalid input text. Please provide a string, or a list of strings\")\n \n         if images is not None:\n-            image_inputs = self.image_processor(\n-                images,\n-                **output_kwargs[\"images_kwargs\"],\n-            )\n+            image_inputs = self.image_processor(images, **output_kwargs[\"images_kwargs\"])\n             # expand the image_token according to the num_crops and tokens per image\n             tokens_per_image = self.size_conversion[image_inputs.pixel_values.shape[2]]\n             prompt_strings = []\n@@ -138,11 +138,44 @@ def __call__(\n             prompt_strings = text\n \n         return_tensors = output_kwargs[\"text_kwargs\"].pop(\"return_tensors\", None)\n-        text_inputs = self.tokenizer(prompt_strings, **output_kwargs[\"text_kwargs\"])\n+        return_mm_token_type_ids = output_kwargs[\"text_kwargs\"].pop(\"return_mm_token_type_ids\", False)\n+        text_inputs = self.tokenizer(prompt_strings, **output_kwargs[\"text_kwargs\"], return_tensors=None)\n         self._check_special_mm_tokens(prompt_strings, text_inputs, modalities=[\"image\"])\n \n+        if return_mm_token_type_ids:\n+            array_ids = np.array(text_inputs[\"input_ids\"])\n+            mm_token_type_ids = np.zeros_like(text_inputs[\"input_ids\"])\n+            mm_token_type_ids[array_ids == self.image_token_id] = 1\n+            text_inputs[\"mm_token_type_ids\"] = mm_token_type_ids.tolist()\n+\n         return BatchFeature(data={**text_inputs, **image_inputs}, tensor_type=return_tensors)\n \n+    def _get_num_multimodal_tokens(self, image_sizes=None, **kwargs):\n+        \"\"\"\n+        Computes the number of placeholder tokens needed for multimodal inputs with the given sizes.\n+        Args:\n+            image_sizes (`List[List[int]]`, *optional*):\n+                The input sizes formatted as (height, width) per each image.\n+        Returns:\n+            `MultiModalData`: A `MultiModalData` object holding number of tokens per each of the provided\n+            input modalities, along with other useful data.\n+        \"\"\"\n+\n+        vision_data = {}\n+        if image_sizes is not None:\n+            images_kwargs = AriaProcessorKwargs._defaults.get(\"images_kwargs\", {})\n+            images_kwargs.update(kwargs)\n+\n+            max_size = images_kwargs.get(\"max_image_size\", None) or self.image_processor.max_image_size\n+            num_image_patches = [\n+                self.image_processor.get_number_of_image_patches(*image_size, images_kwargs)\n+                for image_size in image_sizes\n+            ]\n+            num_image_tokens = [self.size_conversion[max_size] * num_patches for num_patches in num_image_patches]\n+            vision_data.update({\"num_image_tokens\": num_image_tokens, \"num_image_patches\": num_image_patches})\n+\n+        return MultiModalData(**vision_data)\n+\n     def batch_decode(self, *args, **kwargs):\n         \"\"\"\n         This method forwards all its arguments to LlamaTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please"
        },
        {
            "sha": "9b68d4fcf51448a780cfca16647f002339e01f98",
            "filename": "src/transformers/models/aya_vision/processing_aya_vision.py",
            "status": "modified",
            "additions": 54,
            "deletions": 11,
            "changes": 65,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e1017b479d0390f30c6376169a236264da2ec47/src%2Ftransformers%2Fmodels%2Faya_vision%2Fprocessing_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e1017b479d0390f30c6376169a236264da2ec47/src%2Ftransformers%2Fmodels%2Faya_vision%2Fprocessing_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fprocessing_aya_vision.py?ref=9e1017b479d0390f30c6376169a236264da2ec47",
            "patch": "@@ -13,22 +13,23 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-\n from typing import List, Optional, Union\n \n-from transformers.processing_utils import (\n-    ImagesKwargs,\n-    ProcessingKwargs,\n-    ProcessorMixin,\n-    Unpack,\n-)\n-from transformers.tokenization_utils_base import PreTokenizedInput, TextInput\n+import numpy as np\n \n from ...image_processing_utils import BatchFeature\n from ...image_utils import (\n     ImageInput,\n     make_flat_list_of_images,\n )\n+from ...processing_utils import (\n+    ImagesKwargs,\n+    MultiModalData,\n+    ProcessingKwargs,\n+    ProcessorMixin,\n+    Unpack,\n+)\n+from ...tokenization_utils_base import PreTokenizedInput, TextInput\n \n \n class AyaVisionImagesKwargs(ImagesKwargs, total=False):\n@@ -43,6 +44,7 @@ class AyaVisionProcessorKwargs(ProcessingKwargs, total=False):\n         \"text_kwargs\": {\n             \"padding_side\": \"left\",\n             \"padding\": True,\n+            \"return_mm_token_type_ids\": False,\n         },\n         \"images_kwargs\": {\n             \"crop_to_patches\": True,\n@@ -121,7 +123,6 @@ def __init__(\n         super().__init__(image_processor, tokenizer, chat_template=chat_template)\n \n         self.image_token = image_token\n-        self.image_token_id = tokenizer.convert_tokens_to_ids(self.image_token)\n         self.patch_size = patch_size * downsample_factor\n         self.img_size = img_size\n \n@@ -131,6 +132,10 @@ def __init__(\n         self.img_line_break_token = img_line_break_token\n         self.tile_token = tile_token\n         self.tile_global_token = tile_global_token\n+        self.image_token_id = tokenizer.convert_tokens_to_ids(self.img_patch_token)\n+        self.image_ids = tokenizer.convert_tokens_to_ids(\n+            [img_patch_token, tile_token, tile_global_token, start_of_img_token, end_of_img_token]\n+        )\n \n     def _prompt_split_image(self, num_patches):\n         \"\"\"\n@@ -226,11 +231,49 @@ def __call__(\n             text = processed_text\n \n         return_tensors = output_kwargs[\"text_kwargs\"].pop(\"return_tensors\", None)\n-        text_inputs = self.tokenizer(text, **output_kwargs[\"text_kwargs\"])\n-        self._check_special_mm_tokens(text, text_inputs, modalities=[\"image\"])\n+        return_mm_token_type_ids = output_kwargs[\"text_kwargs\"].pop(\"return_mm_token_type_ids\", False)\n+        text_inputs = self.tokenizer(text, **output_kwargs[\"text_kwargs\"], return_tensors=None)\n+\n+        if return_mm_token_type_ids:\n+            array_ids = np.array(text_inputs[\"input_ids\"])\n+            mm_token_type_ids = np.zeros_like(text_inputs[\"input_ids\"])\n+            mm_token_type_ids[np.isin(array_ids, self.image_ids)] = 1\n+            text_inputs[\"mm_token_type_ids\"] = mm_token_type_ids.tolist()\n \n         return BatchFeature(data={**text_inputs, **image_inputs}, tensor_type=return_tensors)\n \n+    def _get_num_multimodal_tokens(self, image_sizes=None, **kwargs):\n+        \"\"\"\n+        Computes the number of placeholder tokens needed for multimodal inputs with the given sizes.\n+\n+        Args:\n+            image_sizes (`List[List[int]]`, *optional*):\n+                The input sizes formatted as (height, width) per each image.\n+\n+        Returns:\n+            `MultiModalData`: A `MultiModalData` object holding number of tokens per each of the provided\n+            input modalities, along with other useful data.\n+        \"\"\"\n+\n+        vision_data = {}\n+        if image_sizes is not None:\n+            images_kwargs = AyaVisionProcessorKwargs._defaults.get(\"images_kwargs\", {})\n+            images_kwargs.update(kwargs)\n+\n+            num_image_patches = [\n+                self.image_processor.get_number_of_image_patches(*image_size, images_kwargs)\n+                for image_size in image_sizes\n+            ]\n+\n+            token_per_patch = (self.img_size // self.patch_size) ** 2\n+            num_image_tokens = [\n+                token_per_patch + 3 + sum(token_per_patch + 1 for _ in range(1, num_patches))\n+                for num_patches in num_image_patches\n+            ]  # Add +3 and +1 for BOI/EOI and image tile tokens\n+            vision_data.update({\"num_image_tokens\": num_image_tokens, \"num_image_patches\": num_image_patches})\n+\n+        return MultiModalData(**vision_data)\n+\n     def batch_decode(self, *args, **kwargs):\n         \"\"\"\n         This method forwards all its arguments to PreTrainedTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please"
        },
        {
            "sha": "8f55eeb26a4819310a3bd23705fea18e5c23d6cb",
            "filename": "src/transformers/models/chameleon/processing_chameleon.py",
            "status": "modified",
            "additions": 51,
            "deletions": 6,
            "changes": 57,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e1017b479d0390f30c6376169a236264da2ec47/src%2Ftransformers%2Fmodels%2Fchameleon%2Fprocessing_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e1017b479d0390f30c6376169a236264da2ec47/src%2Ftransformers%2Fmodels%2Fchameleon%2Fprocessing_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fprocessing_chameleon.py?ref=9e1017b479d0390f30c6376169a236264da2ec47",
            "patch": "@@ -18,9 +18,18 @@\n \n from typing import List, Optional, Union\n \n+import numpy as np\n+\n from ...feature_extraction_utils import BatchFeature\n from ...image_utils import ImageInput\n-from ...processing_utils import ProcessingKwargs, ProcessorMixin, TextKwargs, Unpack, _validate_images_text_input_order\n+from ...processing_utils import (\n+    MultiModalData,\n+    ProcessingKwargs,\n+    ProcessorMixin,\n+    TextKwargs,\n+    Unpack,\n+    _validate_images_text_input_order,\n+)\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n \n \n@@ -34,6 +43,7 @@ class ChameleonProcessorKwargs(ProcessingKwargs, total=False):\n         \"text_kwargs\": {\n             \"padding\": False,\n             \"return_for_text_completion\": False,\n+            \"return_mm_token_type_ids\": False,\n         },\n         \"common_kwargs\": {\n             \"return_tensors\": \"pt\",\n@@ -73,6 +83,10 @@ def __init__(self, image_processor, tokenizer, image_seq_length: int = 1024, ima\n             tokenizer.boi_token if hasattr(tokenizer, \"boi_token\") else \"<racm3:break>\"\n         )  # fixed tokens for start and end, so can hardcode\n         self.image_end_token = tokenizer.eoi_token if hasattr(tokenizer, \"eoi_token\") else \"<eoss>\"\n+        self.image_token_id = tokenizer.convert_tokens_to_ids(self.image_token)\n+        self.image_start_token_id = tokenizer.convert_tokens_to_ids(self.image_start_token)\n+        self.image_end_token_id = tokenizer.convert_tokens_to_ids(self.image_end_token)\n+        self.image_ids = [self.image_token_id, self.image_start_token_id, self.image_end_token_id]\n \n         super().__init__(image_processor, tokenizer)\n \n@@ -141,14 +155,45 @@ def __call__(\n                 sample += self.tokenizer.sep_token  # special Chameleon treatment to add sep for chat mode\n             prompt_strings.append(sample)\n \n+        image_inputs = {}\n+        if images is not None:\n+            image_inputs = self.image_processor(images, **output_kwargs[\"images_kwargs\"])\n+\n         return_tensors = output_kwargs[\"text_kwargs\"].pop(\"return_tensors\", None)\n-        data = self.tokenizer(prompt_strings, **output_kwargs[\"text_kwargs\"])\n-        self._check_special_mm_tokens(prompt_strings, data, modalities=[\"image\"])\n+        return_mm_token_type_ids = output_kwargs[\"text_kwargs\"].pop(\"return_mm_token_type_ids\", False)\n+        text_inputs = self.tokenizer(prompt_strings, **output_kwargs[\"text_kwargs\"], return_tensors=None)\n+        self._check_special_mm_tokens(prompt_strings, text_inputs, modalities=[\"image\"])\n \n-        if images is not None:\n-            data[\"pixel_values\"] = self.image_processor(images, **output_kwargs[\"images_kwargs\"])[\"pixel_values\"]\n+        if return_mm_token_type_ids:\n+            array_ids = np.array(text_inputs[\"input_ids\"])\n+            mm_token_type_ids = np.zeros_like(text_inputs[\"input_ids\"])\n+            mm_token_type_ids[np.isin(array_ids, self.image_ids)] = 1\n+            text_inputs[\"mm_token_type_ids\"] = mm_token_type_ids.tolist()\n+\n+        return BatchFeature(data={**text_inputs, **image_inputs}, tensor_type=return_tensors)\n+\n+    def _get_num_multimodal_tokens(self, image_sizes=None, **kwargs):\n+        \"\"\"\n+        Computes the number of placeholder tokens needed for multimodal inputs with the given sizes.\n+\n+        Args:\n+            image_sizes (`List[List[int]]`, *optional*):\n+                The input sizes formatted as (height, width) per each image.\n+\n+        Returns:\n+            `MultiModalData`: A `MultiModalData` object holding number of tokens per each of the provided\n+            input modalities, along with other useful data.\n+        \"\"\"\n+\n+        vision_data = {}\n+        if image_sizes is not None:\n+            # add 2 for BOI and EOI tokens\n+            num_image_tokens = [self.image_seq_length + 2] * len(image_sizes)\n+            num_image_patches = [1] * len(image_sizes)\n+\n+            vision_data.update({\"num_image_tokens\": num_image_tokens, \"num_image_patches\": num_image_patches})\n \n-        return BatchFeature(data=data, tensor_type=return_tensors)\n+        return MultiModalData(**vision_data)\n \n     # Copied from transformers.models.clip.processing_clip.CLIPProcessor.batch_decode with CLIP->Llama\n     def batch_decode(self, *args, **kwargs):"
        },
        {
            "sha": "03b76bf9595f479696f0d74f9cbd44c803e85616",
            "filename": "src/transformers/models/colpali/processing_colpali.py",
            "status": "modified",
            "additions": 20,
            "deletions": 1,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e1017b479d0390f30c6376169a236264da2ec47/src%2Ftransformers%2Fmodels%2Fcolpali%2Fprocessing_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e1017b479d0390f30c6376169a236264da2ec47/src%2Ftransformers%2Fmodels%2Fcolpali%2Fprocessing_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolpali%2Fprocessing_colpali.py?ref=9e1017b479d0390f30c6376169a236264da2ec47",
            "patch": "@@ -24,7 +24,7 @@\n \n from ...feature_extraction_utils import BatchFeature\n from ...image_utils import ImageInput, is_valid_image, make_flat_list_of_images\n-from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n+from ...processing_utils import MultiModalData, ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import AddedToken, PreTokenizedInput, TextInput\n from ...utils import is_torch_available\n \n@@ -256,6 +256,25 @@ def __call__(\n \n             return batch_query\n \n+    def _get_num_multimodal_tokens(self, image_sizes=None, **kwargs):\n+        \"\"\"\n+        Computes the number of placeholder tokens needed for multimodal inputs with the given sizes.\n+\n+        Args:\n+            image_sizes (List[List[str]], *optional*):\n+                The input sizes formatted as (height, width) per each image.\n+        Returns:\n+            Dict[str, List[int]]: A dictionary mapping each modality (\"image\", \"video\", \"audio\")\n+            to a list containing the number of placeholder tokens required. If the model doesn't accept\n+            a certain modality or no input sizes are provided, the dict value is set to an empty list.\n+        \"\"\"\n+        vision_data = {}\n+        if image_sizes is not None:\n+            num_image_tokens = [self.image_seq_length] * len(image_sizes)\n+            num_image_patches = [1] * len(image_sizes)\n+            vision_data.update({\"num_image_tokens\": num_image_tokens, \"num_image_patches\": num_image_patches})\n+        return MultiModalData(**vision_data)\n+\n     def batch_decode(self, *args, **kwargs):\n         \"\"\"\n         This method forwards all its arguments to GemmaTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please"
        },
        {
            "sha": "dd1928ac8a8dd6b8a11015ba8b67c835fa862f9e",
            "filename": "src/transformers/models/emu3/processing_emu3.py",
            "status": "modified",
            "additions": 53,
            "deletions": 6,
            "changes": 59,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e1017b479d0390f30c6376169a236264da2ec47/src%2Ftransformers%2Fmodels%2Femu3%2Fprocessing_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e1017b479d0390f30c6376169a236264da2ec47/src%2Ftransformers%2Fmodels%2Femu3%2Fprocessing_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fprocessing_emu3.py?ref=9e1017b479d0390f30c6376169a236264da2ec47",
            "patch": "@@ -16,10 +16,17 @@\n \n from typing import List, Optional, Union\n \n+import numpy as np\n+\n from ...image_processing_utils import BatchFeature\n from ...image_utils import ImageInput\n-from ...processing_utils import ImagesKwargs, ProcessingKwargs, ProcessorMixin, TextKwargs, Unpack\n+from ...processing_utils import ImagesKwargs, MultiModalData, ProcessingKwargs, ProcessorMixin, TextKwargs, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n+from ...utils import is_vision_available\n+\n+\n+if is_vision_available():\n+    from .image_processing_emu3 import smart_resize\n \n \n class Emu3TextKwargs(TextKwargs, total=False):\n@@ -37,6 +44,7 @@ class Emu3ProcessorKwargs(ProcessingKwargs, total=False):\n     _defaults = {\n         \"text_kwargs\": {\n             \"return_for_image_generation\": False,\n+            \"return_mm_token_type_ids\": False,\n         },\n         \"images_kwargs\": {\n             \"ratio\": \"1:1\",\n@@ -166,7 +174,7 @@ def __call__(\n \n                     image_placeholder = f\"{image_start_tokens}{height}*{width}{self.fake_token_around_image}{'<placeholder>' * image_seq_length}{image_end_tokens}\"\n                     sample = sample.replace(self.image_token, image_placeholder, 1)\n-                    sample = f\"{self.bos_token}{sample}\"  # add BOS because PT tokenizer doesn't add it\n+                    sample = f\"{self.bos_token}{sample}\"  # add BOS because GPT tokenizer doesn't add it\n                 prompt_strings.append(sample)\n             text = [sample.replace(\"<placeholder>\", self.image_token) for sample in prompt_strings]\n \n@@ -179,12 +187,51 @@ def __call__(\n \n         # else just generate from text-only input, and we do no special treatment for text\n         return_tensors = output_kwargs[\"text_kwargs\"].pop(\"return_tensors\", None)\n-        data = self.tokenizer(text, **output_kwargs[\"text_kwargs\"])\n-        self._check_special_mm_tokens(text, data, modalities=[\"image\"])\n+        return_mm_token_type_ids = output_kwargs[\"text_kwargs\"].pop(\"return_mm_token_type_ids\", False)\n+        text_inputs = self.tokenizer(text, **output_kwargs[\"text_kwargs\"], return_tensors=None)\n+        self._check_special_mm_tokens(text, text_inputs, modalities=[\"image\"])\n \n-        data.update(**image_features)\n+        if return_mm_token_type_ids:\n+            array_ids = np.array(text_inputs[\"input_ids\"])\n+            mm_token_type_ids = np.zeros_like(text_inputs[\"input_ids\"])\n+            mm_token_type_ids[array_ids == self.image_token_id] = 1\n+            text_inputs[\"mm_token_type_ids\"] = mm_token_type_ids.tolist()\n+\n+        return BatchFeature(data={**text_inputs, **image_features}, tensor_type=return_tensors)\n+\n+    def _get_num_multimodal_tokens(self, image_sizes=None, **kwargs):\n+        \"\"\"\n+        Computes the number of placeholder tokens needed for multimodal inputs with the given sizes.\n+\n+        Args:\n+            image_sizes (`List[List[int]]`, *optional*):\n+                The input sizes formatted as (height, width) per each image.\n+\n+        Returns:\n+            `MultiModalData`: A `MultiModalData` object holding number of tokens per each of the provided\n+            input modalities, along with other useful data.\n+        \"\"\"\n \n-        return BatchFeature(data=data, tensor_type=return_tensors)\n+        vision_data = {}\n+        if image_sizes is not None:\n+            num_image_tokens = []\n+            for height, width in image_sizes:\n+                height, width = smart_resize(\n+                    height,\n+                    width,\n+                    self.image_processor.spatial_factor,\n+                    self.image_processor.min_pixels,\n+                    self.image_processor.max_pixels,\n+                )\n+                height = height // self.downsample_ratio\n+                width = width // self.downsample_ratio\n+                image_seq_length = height * (width + 1)  # +1 for extra row when converting to BPE in modeling code\n+                num_image_tokens.append(image_seq_length)\n+\n+            num_image_patches = [1] * len(image_sizes)\n+            vision_data.update({\"num_image_tokens\": num_image_tokens, \"num_image_patches\": num_image_patches})\n+\n+        return MultiModalData(**vision_data)\n \n     def calculate_generate_size(self, ratio, image_area, spatial_factor):\n         width, height = map(int, ratio.split(\":\"))"
        },
        {
            "sha": "590cf8f8d174c7f7834fff8ba882ebb6cfd1bd75",
            "filename": "src/transformers/models/fuyu/modeling_fuyu.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e1017b479d0390f30c6376169a236264da2ec47/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e1017b479d0390f30c6376169a236264da2ec47/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py?ref=9e1017b479d0390f30c6376169a236264da2ec47",
            "patch": "@@ -130,7 +130,7 @@ def gather_continuous_embeddings(\n             )\n         return output_embeddings\n \n-    def get_image_features(self, pixel_values: torch.FloatTensor):\n+    def get_image_features(self, pixel_values: torch.FloatTensor, **kwargs):\n         \"\"\"\n         Encodes images into continuous embeddings that can be forwarded to the language model.\n "
        },
        {
            "sha": "1b7fbab0e30744b74ec4ce7c73b90260e593b876",
            "filename": "src/transformers/models/fuyu/processing_fuyu.py",
            "status": "modified",
            "additions": 65,
            "deletions": 5,
            "changes": 70,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e1017b479d0390f30c6376169a236264da2ec47/src%2Ftransformers%2Fmodels%2Ffuyu%2Fprocessing_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e1017b479d0390f30c6376169a236264da2ec47/src%2Ftransformers%2Fmodels%2Ffuyu%2Fprocessing_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fprocessing_fuyu.py?ref=9e1017b479d0390f30c6376169a236264da2ec47",
            "patch": "@@ -22,7 +22,13 @@\n import numpy as np\n \n from ...image_utils import ImageInput\n-from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack, _validate_images_text_input_order\n+from ...processing_utils import (\n+    MultiModalData,\n+    ProcessingKwargs,\n+    ProcessorMixin,\n+    Unpack,\n+    _validate_images_text_input_order,\n+)\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n from ...utils import is_torch_available, logging, requires_backends\n from ...utils.import_utils import requires\n@@ -64,6 +70,7 @@ class FuyuProcessorKwargs(ProcessingKwargs, total=False):\n             \"return_token_type_ids\": False,\n             \"return_length\": False,\n             \"verbose\": True,\n+            \"return_mm_token_type_ids\": False,\n         },\n         \"images_kwargs\": {},\n     }\n@@ -355,6 +362,8 @@ def __init__(self, image_processor, tokenizer, **kwargs):\n         self.max_position_embeddings = 16384  # TODO Can't derive this from model files: where to set it?\n         self.pad_token_id = 0\n         self.dummy_image_index = -1\n+        self.image_token_id = tokenizer.encode(\"|SPEAKER|\", add_special_tokens=False)[1]\n+        self.image_newline_id = tokenizer.encode(\"|NEWLINE|\", add_special_tokens=False)[1]\n \n     def _left_pad_inputs_with_attention_mask(self, model_inputs: List[Dict], return_attention_mask: bool):\n         max_length_input_ids = max(entry[\"input_ids\"].shape[1] for entry in model_inputs)\n@@ -403,6 +412,11 @@ def _left_pad_inputs_with_attention_mask(self, model_inputs: List[Dict], return_\n         for key in batched_keys:\n             batched_inputs[key] = torch.cat(batched_inputs[key], dim=0)\n \n+        # Cast images to tensor as well, if only one image passed and no padding needed\n+        # NOTE: vLLM expects all processor outputs to be a tensor\n+        if len(batched_inputs[\"image_patches\"]) == 1:\n+            batched_inputs[\"image_patches\"] = torch.cat(batched_inputs[\"image_patches\"], dim=0)\n+\n         return batched_inputs\n \n     def get_sample_encoding(\n@@ -517,6 +531,7 @@ def __call__(\n             tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n             **kwargs,\n         )\n+        return_mm_token_type_ids = output_kwargs[\"text_kwargs\"].pop(\"return_mm_token_type_ids\", False)\n \n         if not output_kwargs[\"text_kwargs\"].setdefault(\"return_attention_mask\", True):\n             raise ValueError(\"`return_attention_mask=False` is not supported for this model.\")\n@@ -550,8 +565,6 @@ def __call__(\n \n         # --- Use self.tokenizer to get the ids of special tokens to insert into image ids ---\n \n-        image_placeholder_id = self.tokenizer(\"|SPEAKER|\", add_special_tokens=False)[\"input_ids\"][1]\n-        image_newline_id = self.tokenizer(\"|NEWLINE|\", add_special_tokens=False)[\"input_ids\"][1]\n         tensor_batch_images = torch.stack([img[0] for img in batch_images]).unsqueeze(1)\n \n         # --- Use self.image_processor again to obtain the full token ids and batch inputs ---\n@@ -565,16 +578,63 @@ def __call__(\n                 scale_factors=[scale_factor],\n                 image_unpadded_heights=torch.tensor([image_unpadded_height]),\n                 image_unpadded_widths=torch.tensor([image_unpadded_width]),\n-                image_placeholder_id=image_placeholder_id,\n-                image_newline_id=image_newline_id,\n+                image_placeholder_id=self.image_token_id,\n+                image_newline_id=self.image_newline_id,\n                 tensor_batch_images=tensor_batch_image.unsqueeze(0),\n             )\n             all_encodings.append(sample_encoding)\n+\n         batch_encoding = self._left_pad_inputs_with_attention_mask(\n             model_inputs=all_encodings, return_attention_mask=True\n         )\n+        if return_mm_token_type_ids:\n+            input_ids = batch_encoding[\"input_ids\"]\n+            mm_token_type_ids = torch.zeros_like(input_ids)\n+            mm_token_type_ids[input_ids == self.image_token_id] = 1\n+            mm_token_type_ids[input_ids == self.image_newline_id] = 1\n+            batch_encoding[\"mm_token_type_ids\"] = mm_token_type_ids\n+\n         return FuyuBatchFeature(data=batch_encoding)\n \n+    def _get_num_multimodal_tokens(self, image_sizes=None, **kwargs):\n+        \"\"\"\n+        Computes the number of placeholder tokens needed for multimodal inputs with the given sizes.\n+\n+        Args:\n+            image_sizes (`List[List[int]]`, *optional*):\n+                The input sizes formatted as (height, width) per each image.\n+\n+        Returns:\n+            `MultiModalData`: A `MultiModalData` object holding number of tokens per each of the provided\n+            input modalities, along with other useful data.\n+        \"\"\"\n+\n+        vision_data = {}\n+        if image_sizes is not None:\n+            size = kwargs.get(\"size\", None) or self.image_processor.size\n+            padded_height, padded_width = size[\"height\"], size[\"width\"]\n+\n+            num_image_tokens = []\n+            num_image_patches = [1] * len(image_sizes)\n+            for image_size in image_sizes:\n+                height_scale_factor = padded_height / image_size[0]\n+                width_scale_factor = padded_width / image_size[1]\n+                optimal_scale_factor = min(height_scale_factor, width_scale_factor)\n+\n+                # We can use torch here because Fuyu processor has hard dependency on torch\n+                model_image_input = self.image_processor.preprocess_with_tokenizer_info(\n+                    image_input=torch.zeros(1, 1, 3, padded_height, padded_width),\n+                    image_present=torch.ones(1, 1, 1),\n+                    image_unpadded_h=torch.tensor([[int(image_size[0] * optimal_scale_factor)]]),\n+                    image_unpadded_w=torch.tensor([[int(image_size[1] * optimal_scale_factor)]]),\n+                    image_placeholder_id=0,  # dummy ids, we can be sure `id=0` is never out-of-range\n+                    image_newline_id=0,\n+                    variable_sized=True,\n+                )\n+                num_image_tokens.append(model_image_input[\"image_input_ids\"][0][0].shape[-1])\n+            vision_data.update({\"num_image_tokens\": num_image_tokens, \"num_image_patches\": num_image_patches})\n+        return MultiModalData(**vision_data)\n+\n     def post_process_box_coordinates(self, outputs, target_sizes=None):\n         \"\"\"\n         Transforms raw coordinates detected by [`FuyuForCausalLM`] to the original images' coordinate space."
        },
        {
            "sha": "c8ac5e97146f123334bd72960ffb17044208a11a",
            "filename": "src/transformers/models/gemma3/processing_gemma3.py",
            "status": "modified",
            "additions": 33,
            "deletions": 7,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e1017b479d0390f30c6376169a236264da2ec47/src%2Ftransformers%2Fmodels%2Fgemma3%2Fprocessing_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e1017b479d0390f30c6376169a236264da2ec47/src%2Ftransformers%2Fmodels%2Fgemma3%2Fprocessing_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fprocessing_gemma3.py?ref=9e1017b479d0390f30c6376169a236264da2ec47",
            "patch": "@@ -20,7 +20,7 @@\n \n from ...feature_extraction_utils import BatchFeature\n from ...image_utils import ImageInput, make_nested_list_of_images\n-from ...processing_utils import ImagesKwargs, ProcessingKwargs, ProcessorMixin, Unpack\n+from ...processing_utils import ImagesKwargs, MultiModalData, ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n from ...utils import to_py_obj\n \n@@ -38,6 +38,7 @@ class Gemma3ProcessorKwargs(ProcessingKwargs, total=False):\n     _defaults = {\n         \"text_kwargs\": {\n             \"padding\": False,\n+            \"return_mm_token_type_ids\": True,\n         },\n         \"images_kwargs\": {\n             \"do_pan_and_scan\": False,\n@@ -137,17 +138,42 @@ def __call__(\n             text = [prompt.replace(self.boi_token, self.full_image_sequence) for prompt in text]\n \n         return_tensors = output_kwargs[\"text_kwargs\"].pop(\"return_tensors\", None)\n-        text_inputs = self.tokenizer(text=text, **output_kwargs[\"text_kwargs\"], return_tensors=\"np\")\n+        return_mm_token_type_ids = output_kwargs[\"text_kwargs\"].pop(\"return_mm_token_type_ids\", False)\n+        text_inputs = self.tokenizer(text=text, **output_kwargs[\"text_kwargs\"])\n         self._check_special_mm_tokens(text, text_inputs, modalities=[\"image\"])\n \n         # Add token type ids manually, as tokenizer can't do arbitrary position token types\n-        array_ids = text_inputs[\"input_ids\"]\n-        mm_token_type_ids = np.zeros_like(text_inputs[\"input_ids\"])\n-        mm_token_type_ids[array_ids == self.image_token_id] = 1\n-        text_inputs = {k: v.tolist() for k, v in text_inputs.items()}  # in case user requested list inputs\n-        text_inputs[\"token_type_ids\"] = mm_token_type_ids.tolist()\n+        if return_mm_token_type_ids:\n+            array_ids = np.array(text_inputs[\"input_ids\"])\n+            mm_token_type_ids = np.zeros_like(array_ids)\n+            mm_token_type_ids[array_ids == self.image_token_id] = 1\n+            text_inputs[\"token_type_ids\"] = mm_token_type_ids.tolist()\n+\n         return BatchFeature(data={**text_inputs, **image_inputs}, tensor_type=return_tensors)\n \n+    def _get_num_multimodal_tokens(self, image_sizes=None, **kwargs):\n+        \"\"\"\n+        Computes the number of placeholder tokens needed for multimodal inputs with the given sizes.\n+\n+        Args:\n+            image_sizes (`List[List[int]]`, *optional*):\n+                The input sizes formatted as (height, width) per each image.\n+\n+        Returns:\n+            `MultiModalData`: A `MultiModalData` object holding number of tokens per each of the provided\n+            input modalities, along with other useful data.\n+        \"\"\"\n+\n+        vision_data = {}\n+        if image_sizes is not None:\n+            # NOTE: no image cropping supported yet\n+            num_image_tokens = [self.image_seq_length] * len(image_sizes)\n+            num_image_patches = [1] * len(image_sizes)\n+\n+            vision_data.update({\"num_image_tokens\": num_image_tokens, \"num_image_patches\": num_image_patches})\n+\n+        return MultiModalData(**vision_data)\n+\n     # Copied from transformers.models.clip.processing_clip.CLIPProcessor.batch_decode with CLIP->Gemma\n     def batch_decode(self, *args, **kwargs):\n         \"\"\""
        },
        {
            "sha": "d706c0f34039dbfce8d40508d21a8422f5df6731",
            "filename": "src/transformers/models/got_ocr2/image_processing_got_ocr2.py",
            "status": "modified",
            "additions": 28,
            "deletions": 0,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e1017b479d0390f30c6376169a236264da2ec47/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e1017b479d0390f30c6376169a236264da2ec47/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2.py?ref=9e1017b479d0390f30c6376169a236264da2ec47",
            "patch": "@@ -491,5 +491,33 @@ def crop_image_to_patches(\n \n         return processed_images\n \n+    def get_number_of_image_patches(self, height: int, width: int, images_kwargs=None):\n+        \"\"\"\n+        A utility that returns number patches for a given image size.\n+\n+        Args:\n+            height (`int`):\n+                Height of the input image.\n+            width (`int`):\n+                Width of the input image.\n+            images_kwargs (`dict`, *optional*)\n+                Any kwargs to override defaults of the image processor.\n+        Returns:\n+            `int`: Number of patches per image.\n+        \"\"\"\n+        min_patches = images_kwargs.get(\"min_patches\", None) or self.min_patches\n+        max_patches = images_kwargs.get(\"max_patches\", None) or self.max_patches\n+        patch_size = images_kwargs.get(\"size\", None) or self.size\n+        crop_to_patches = images_kwargs.get(\"crop_to_patches\", None) or self.crop_to_patches\n+\n+        num_patches = 1\n+        if crop_to_patches and max_patches > 1:\n+            num_columns, num_rows = get_optimal_tiled_canvas(\n+                (height, width), (patch_size[\"height\"], patch_size[\"width\"]), min_patches, max_patches\n+            )\n+            num_patches += num_columns * num_rows\n+\n+        return num_patches\n+\n \n __all__ = [\"GotOcr2ImageProcessor\"]"
        },
        {
            "sha": "95179d7a94c3ae48210ddd927bfa784a6111985a",
            "filename": "src/transformers/models/got_ocr2/image_processing_got_ocr2_fast.py",
            "status": "modified",
            "additions": 28,
            "deletions": 0,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e1017b479d0390f30c6376169a236264da2ec47/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e1017b479d0390f30c6376169a236264da2ec47/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2_fast.py?ref=9e1017b479d0390f30c6376169a236264da2ec47",
            "patch": "@@ -228,5 +228,33 @@ def _preprocess(\n             data={\"pixel_values\": processed_images, \"num_patches\": num_patches}, tensor_type=return_tensors\n         )\n \n+    def get_number_of_image_tokens(self, height: int, width: int, images_kwargs=None):\n+        \"\"\"\n+        A utility that returns number patches for a given image size.\n+\n+        Args:\n+            height (`int`):\n+                Height of the input image.\n+            width (`int`):\n+                Width of the input image.\n+            images_kwargs (`dict`, *optional*)\n+                Any kwargs to override defaults of the image processor.\n+        Returns:\n+            `int`: Number of patches per image.\n+        \"\"\"\n+        min_patches = images_kwargs.get(\"min_patches\", None) or self.min_patches\n+        max_patches = images_kwargs.get(\"max_patches\", None) or self.max_patches\n+        patch_size = images_kwargs.get(\"size\", None) or self.size\n+        crop_to_patches = images_kwargs.get(\"crop_to_patches\", None) or self.crop_to_patches\n+\n+        num_patches = 1\n+        if crop_to_patches and max_patches > 1:\n+            num_columns, num_rows = get_optimal_tiled_canvas(\n+                (height, width), (patch_size[\"height\"], patch_size[\"width\"]), min_patches, max_patches\n+            )\n+            num_patches += num_columns * num_rows\n+\n+        return num_patches\n+\n \n __all__ = [\"GotOcr2ImageProcessorFast\"]"
        },
        {
            "sha": "e84c4157b2ac63d9fbcfc503e9c8105146076698",
            "filename": "src/transformers/models/idefics3/image_processing_idefics3.py",
            "status": "modified",
            "additions": 41,
            "deletions": 0,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e1017b479d0390f30c6376169a236264da2ec47/src%2Ftransformers%2Fmodels%2Fidefics3%2Fimage_processing_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e1017b479d0390f30c6376169a236264da2ec47/src%2Ftransformers%2Fmodels%2Fidefics3%2Fimage_processing_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fimage_processing_idefics3.py?ref=9e1017b479d0390f30c6376169a236264da2ec47",
            "patch": "@@ -850,5 +850,46 @@ def preprocess(\n \n         return encoding\n \n+    def get_number_of_image_patches(self, height: int, width: int, images_kwargs=None):\n+        \"\"\"\n+        A utility that returns number of image patches for a given image size.\n+\n+        Args:\n+            height (`int`):\n+                Height of the input image.\n+            width (`int`):\n+                Width of the input image.\n+            images_kwargs (`dict`, *optional*)\n+                Any kwargs to override defaults of the image processor.\n+        Returns:\n+            `int`: Number of patches per image.\n+        \"\"\"\n+        do_image_splitting = images_kwargs.get(\"do_image_splitting\", None) or self.do_image_splitting\n+        max_image_size = images_kwargs.get(\"max_image_size\", None) or self.max_image_size\n+        size = images_kwargs.get(\"size\", None) or self.size\n+\n+        if do_image_splitting:\n+            height, width = _resize_output_size_rescale_to_max_len(height, width, max_len=size[\"longest_edge\"])\n+            height, width = _resize_output_size_scale_below_upper_bound(height, width, max_len=4096)\n+            aspect_ratio = width / height\n+\n+            if width >= height:\n+                resized_width = math.ceil(width / max_image_size[\"longest_edge\"]) * max_image_size[\"longest_edge\"]\n+                resized_height = int(width / aspect_ratio)\n+                resized_height = math.ceil(height / max_image_size[\"longest_edge\"]) * max_image_size[\"longest_edge\"]\n+            elif height > width:\n+                resized_height = math.ceil(height / max_image_size[\"longest_edge\"]) * max_image_size[\"longest_edge\"]\n+                resized_width = int(height * aspect_ratio)\n+                resized_width = math.ceil(width / max_image_size[\"longest_edge\"]) * max_image_size[\"longest_edge\"]\n+\n+            max_height = max_width = max_image_size[\"longest_edge\"]\n+            if resized_height > max_height or resized_width > max_width:\n+                # Calculate the number of splits\n+                num_rows = math.ceil(resized_height / max_height)\n+                num_cols = math.ceil(resized_width / max_width)\n+                num_patches = num_rows * num_cols + 1\n+\n+        return num_patches\n+\n \n __all__ = [\"Idefics3ImageProcessor\"]"
        },
        {
            "sha": "5d058ec8245c9521276b7ad909ecfff13e57fd28",
            "filename": "src/transformers/models/idefics3/processing_idefics3.py",
            "status": "modified",
            "additions": 71,
            "deletions": 2,
            "changes": 73,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e1017b479d0390f30c6376169a236264da2ec47/src%2Ftransformers%2Fmodels%2Fidefics3%2Fprocessing_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e1017b479d0390f30c6376169a236264da2ec47/src%2Ftransformers%2Fmodels%2Fidefics3%2Fprocessing_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fprocessing_idefics3.py?ref=9e1017b479d0390f30c6376169a236264da2ec47",
            "patch": "@@ -16,13 +16,16 @@\n Processor class for Idefics3.\n \"\"\"\n \n+import math\n import re\n from itertools import accumulate\n from typing import TYPE_CHECKING, Dict, List, Optional, Union\n \n+import numpy as np\n+\n from ...feature_extraction_utils import BatchFeature\n from ...image_utils import ImageInput, is_valid_image, load_image\n-from ...processing_utils import ImagesKwargs, ProcessingKwargs, ProcessorMixin, Unpack\n+from ...processing_utils import ImagesKwargs, MultiModalData, ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import AddedToken, BatchEncoding, TextInput\n from ...utils import logging\n \n@@ -98,6 +101,7 @@ class Idefics3ProcessorKwargs(ProcessingKwargs, total=False):\n             \"add_special_tokens\": True,\n             \"padding\": False,\n             \"is_split_into_words\": False,\n+            \"return_mm_token_type_ids\": False,\n         },\n         \"images_kwargs\": {\n             \"return_row_col_info\": True,\n@@ -146,6 +150,12 @@ def __init__(\n         self.end_of_utterance_token = AddedToken(\"<end_of_utterance>\", normalized=False, special=True).content\n         self.global_image_tag = \"<global-img>\"  # https://github.com/huggingface/transformers/pull/32473/files/8063e5e17362571b693f1db95167f5443a3be1b2#r1734825341\n         self.image_seq_len = image_seq_len\n+        self.image_token_id = tokenizer.convert_tokens_to_ids(self.image_token)\n+        self.fake_image_token_id = tokenizer.convert_tokens_to_ids(self.fake_image_token)\n+        self.global_image_token_id = tokenizer.convert_tokens_to_ids(self.global_image_tag)\n+        self.row_col_ids = [\n+            tokenizer.convert_tokens_to_ids(f\"<row_{i + 1}_col_{j + 1}>\") for i in range(6) for j in range(6)\n+        ]\n \n         # This regex matches one or more occurrences of <global-img> tags (optionally surrounded by newline characters)\n         # or <row_x_col_y> tags (where x and y are digits, also optionally surrounded by newline characters).\n@@ -241,6 +251,7 @@ def __call__(\n         )\n \n         image_seq_len = image_seq_len if image_seq_len is not None else self.image_seq_len\n+        return_mm_token_type_ids = output_kwargs[\"text_kwargs\"].pop(\"return_mm_token_type_ids\", False)\n         return_tensors = output_kwargs[\"text_kwargs\"].pop(\"return_tensors\", None)\n \n         n_images_in_text = []\n@@ -302,9 +313,11 @@ def __call__(\n                 global_img_token = self.global_image_tag\n \n                 prompt_strings = []\n+                batch_image_seq_lengths = []\n                 for sample, sample_rows, sample_cols in zip(text, image_rows, image_cols):\n                     # Replace the image token with fake tokens around the expanded image token sequence of length `image_seq_len`\n                     image_prompt_strings = []\n+                    image_seq_lengths = []\n                     for n_rows, n_cols in zip(sample_rows, sample_cols):\n                         image_prompt_string = get_image_prompt_string(\n                             n_rows,\n@@ -314,8 +327,12 @@ def __call__(\n                             fake_token_around_image=fake_image_token,\n                             global_img_token=global_img_token,\n                         )\n+                        # Add +2 and +3 for special BOI/EOI/fake_image_wrapper tokens\n+                        row_length = (self.image_seq_len + 2) * n_cols + 1\n+                        image_seq_lengths.append((self.image_seq_len + 3) + row_length * n_rows)\n                         image_prompt_strings.append(image_prompt_string)\n \n+                    batch_image_seq_lengths.append(image_seq_lengths)\n                     split_sample = sample.split(image_token)\n                     if len(split_sample) == 0:\n                         raise ValueError(\"The image token should be present in the text.\")\n@@ -338,7 +355,59 @@ def __call__(\n             text_inputs = self.tokenizer(text=text, **output_kwargs[\"text_kwargs\"])\n             inputs.update(text_inputs)\n \n-        return BatchFeature(inputs, tensor_type=return_tensors)\n+        if return_mm_token_type_ids:\n+            array_ids = np.array(inputs[\"input_ids\"])\n+            mm_token_type_ids = np.zeros_like(array_ids)\n+            for i, seq_lengths in enumerate(batch_image_seq_lengths):\n+                image_start_positions = np.where(array_ids[i] == self.fake_image_token_id)[0]\n+                j = 0\n+                for seq_len in seq_lengths:\n+                    if j >= len(image_start_positions):\n+                        break\n+                    start = image_start_positions[j]\n+                    end = start + seq_len\n+                    mm_token_type_ids[i, start:end] = 1\n+                    j = np.searchsorted(image_start_positions, end)\n+\n+            inputs[\"mm_token_type_ids\"] = mm_token_type_ids.tolist()\n+\n+        return BatchFeature(data=inputs, tensor_type=return_tensors)\n+\n+    def _get_num_multimodal_tokens(self, image_sizes=None, **kwargs):\n+        \"\"\"\n+        Computes the number of placeholder tokens needed for multimodal inputs with the given sizes.\n+\n+        Args:\n+            image_sizes (`List[List[int]]`, *optional*):\n+                The input sizes formatted as (height, width) per each image.\n+\n+        Returns:\n+            `MultiModalData`: A `MultiModalData` object holding number of tokens per each of the provided\n+            input modalities, along with other useful data.\n+        \"\"\"\n+\n+        vision_data = {}\n+        if image_sizes is not None:\n+            images_kwargs = Idefics3ProcessorKwargs._defaults.get(\"images_kwargs\", {})\n+            images_kwargs.update(kwargs)\n+\n+            num_image_patches = [\n+                self.image_processor.get_number_of_image_patches(*image_size, images_kwargs)\n+                for image_size in image_sizes\n+            ]\n+\n+            base_image_length = self.image_seq_len + 3\n+            col_length = self.image_seq_len + 2\n+            num_image_tokens = []\n+\n+            for num_patches in num_image_patches:\n+                num_cols = num_rows = int(math.sqrt(num_patches - 1))\n+                row_length = col_length * num_cols + 1\n+                num_image_tokens.append(base_image_length + (row_length * num_rows))\n+\n+            vision_data.update({\"num_image_tokens\": num_image_tokens, \"num_image_patches\": num_image_patches})\n+\n+        return MultiModalData(**vision_data)\n \n     def batch_decode(self, *args, **kwargs):\n         \"\"\""
        },
        {
            "sha": "addb45a688ad4d410b75b4dd8e775bff85ccd5fe",
            "filename": "src/transformers/models/internvl/processing_internvl.py",
            "status": "modified",
            "additions": 47,
            "deletions": 9,
            "changes": 56,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e1017b479d0390f30c6376169a236264da2ec47/src%2Ftransformers%2Fmodels%2Finternvl%2Fprocessing_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e1017b479d0390f30c6376169a236264da2ec47/src%2Ftransformers%2Fmodels%2Finternvl%2Fprocessing_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fprocessing_internvl.py?ref=9e1017b479d0390f30c6376169a236264da2ec47",
            "patch": "@@ -13,25 +13,24 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-\n from typing import List, Optional, Union\n \n import numpy as np\n \n-from transformers.processing_utils import (\n-    ImagesKwargs,\n-    ProcessingKwargs,\n-    ProcessorMixin,\n-    Unpack,\n-)\n-from transformers.tokenization_utils_base import PreTokenizedInput, TextInput\n-\n from ...image_processing_utils import BatchFeature\n from ...image_utils import (\n     ImageInput,\n     concatenate_list,\n     make_flat_list_of_images,\n )\n+from ...processing_utils import (\n+    ImagesKwargs,\n+    MultiModalData,\n+    ProcessingKwargs,\n+    ProcessorMixin,\n+    Unpack,\n+)\n+from ...tokenization_utils_base import PreTokenizedInput, TextInput\n from ...video_utils import VideoInput, VideoMetadata, load_video, make_batched_videos\n \n \n@@ -46,6 +45,7 @@ class InternVLProcessorKwargs(ProcessingKwargs, total=False):\n     _defaults = {\n         \"text_kwargs\": {\n             \"padding_side\": \"left\",\n+            \"return_mm_token_type_ids\": False,\n         },\n         \"images_kwargs\": {\n             \"crop_to_patches\": True,\n@@ -94,9 +94,12 @@ def __init__(\n         self.image_seq_length = image_seq_length\n         self.start_image_token = tokenizer.start_image_token\n         self.end_image_token = tokenizer.end_image_token\n+        self.start_image_token_id = tokenizer.start_image_token_id\n+        self.end_image_token_id = tokenizer.end_image_token_id\n         self.image_token = tokenizer.context_image_token\n         self.video_token = tokenizer.video_token\n         self.image_token_id = tokenizer.context_image_token_id\n+        self.image_ids = [self.image_token_id, self.start_image_token_id, self.end_image_token_id]\n \n         super().__init__(image_processor, tokenizer, video_processor, chat_template=chat_template, **kwargs)\n \n@@ -261,11 +264,46 @@ def __call__(\n             image_videos_inputs = {\"pixel_values\": concatenate_list(image_video_patches)}\n \n         return_tensors = output_kwargs[\"text_kwargs\"].pop(\"return_tensors\", None)\n+        return_mm_token_type_ids = output_kwargs[\"text_kwargs\"].pop(\"return_mm_token_type_ids\", None)\n         text_inputs = self.tokenizer(text, **output_kwargs[\"text_kwargs\"])\n         self._check_special_mm_tokens(text, text_inputs, modalities=[\"image\"])\n \n+        if return_mm_token_type_ids:\n+            array_ids = np.array(text_inputs[\"input_ids\"])\n+            mm_token_type_ids = np.zeros_like(text_inputs[\"input_ids\"])\n+            mm_token_type_ids[np.isin(array_ids, self.image_ids)] = 1\n+            text_inputs[\"mm_token_type_ids\"] = mm_token_type_ids.tolist()\n+\n         return BatchFeature(data={**text_inputs, **image_videos_inputs}, tensor_type=return_tensors)\n \n+    def _get_num_multimodal_tokens(self, image_sizes=None, **kwargs):\n+        \"\"\"\n+        Computes the number of placeholder tokens needed for multimodal inputs with the given sizes.\n+\n+        Args:\n+            image_sizes (`List[List[int]]`, *optional*):\n+                The input sizes formatted as (height, width) per each image.\n+\n+        Returns:\n+            `MultiModalData`: A `MultiModalData` object holding number of tokens per each of the provided\n+            input modalities, along with other useful data.\n+        \"\"\"\n+\n+        vision_data = {}\n+        if image_sizes is not None:\n+            images_kwargs = InternVLProcessorKwargs._defaults.get(\"images_kwargs\", {})\n+            images_kwargs.update(kwargs)\n+\n+            num_image_patches = [\n+                self.image_processor.get_number_of_image_tokens(*image_size, images_kwargs)\n+                for image_size in image_sizes\n+            ]\n+            # Add 2 for BOI and EOI tokens\n+            num_image_tokens = [2 + (self.image_seq_length * num_patches) for num_patches in num_image_patches]\n+            vision_data.update({\"num_image_tokens\": num_image_tokens, \"num_image_patches\": num_image_patches})\n+\n+        return MultiModalData(**vision_data)\n+\n     def sample_indices_fn(\n         self, metadata: VideoMetadata, num_frames: Optional[int] = None, initial_shift: Union[bool, float, int] = True\n     ):"
        },
        {
            "sha": "d0ad3d0af179a4f22da62de2e7fe50a50101e37a",
            "filename": "src/transformers/models/llava/processing_llava.py",
            "status": "modified",
            "additions": 51,
            "deletions": 10,
            "changes": 61,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e1017b479d0390f30c6376169a236264da2ec47/src%2Ftransformers%2Fmodels%2Fllava%2Fprocessing_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e1017b479d0390f30c6376169a236264da2ec47/src%2Ftransformers%2Fmodels%2Fllava%2Fprocessing_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fprocessing_llava.py?ref=9e1017b479d0390f30c6376169a236264da2ec47",
            "patch": "@@ -18,9 +18,17 @@\n \n from typing import List, Union\n \n+import numpy as np\n+\n from ...feature_extraction_utils import BatchFeature\n from ...image_utils import ImageInput, get_image_size, to_numpy_array\n-from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack, _validate_images_text_input_order\n+from ...processing_utils import (\n+    MultiModalData,\n+    ProcessingKwargs,\n+    ProcessorMixin,\n+    Unpack,\n+    _validate_images_text_input_order,\n+)\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n from ...utils import logging\n \n@@ -30,9 +38,7 @@\n \n class LlavaProcessorKwargs(ProcessingKwargs, total=False):\n     _defaults = {\n-        \"text_kwargs\": {\n-            \"padding\": False,\n-        },\n+        \"text_kwargs\": {\"padding\": False, \"return_mm_token_type_ids\": False},\n         \"images_kwargs\": {},\n     }\n \n@@ -89,11 +95,7 @@ def __init__(\n         self.num_additional_image_tokens = num_additional_image_tokens\n         self.vision_feature_select_strategy = vision_feature_select_strategy\n         self.image_token = tokenizer.image_token if hasattr(tokenizer, \"image_token\") else image_token\n-        self.image_token_id = (\n-            tokenizer.image_token_id\n-            if getattr(tokenizer, \"image_token_id\", None)\n-            else tokenizer.convert_tokens_to_ids(self.image_token)\n-        )\n+        self.image_token_id = tokenizer.encode(self.image_token, add_special_tokens=False)[0]\n         super().__init__(image_processor, tokenizer, chat_template=chat_template)\n \n     def __call__(\n@@ -174,10 +176,49 @@ def __call__(\n                 prompt_strings.append(sample)\n \n         return_tensors = output_kwargs[\"text_kwargs\"].pop(\"return_tensors\", None)\n-        text_inputs = self.tokenizer(prompt_strings, **output_kwargs[\"text_kwargs\"])\n+        return_mm_token_type_ids = output_kwargs[\"text_kwargs\"].pop(\"return_mm_token_type_ids\", False)\n+        text_inputs = self.tokenizer(prompt_strings, **output_kwargs[\"text_kwargs\"], return_tensors=None)\n         self._check_special_mm_tokens(prompt_strings, text_inputs, modalities=[\"image\"])\n+\n+        if return_mm_token_type_ids:\n+            array_ids = np.array(text_inputs[\"input_ids\"])\n+            mm_token_type_ids = np.zeros_like(text_inputs[\"input_ids\"])\n+            mm_token_type_ids[array_ids == self.image_token_id] = 1\n+            text_inputs[\"mm_token_type_ids\"] = mm_token_type_ids.tolist()\n+\n         return BatchFeature(data={**text_inputs, **image_inputs}, tensor_type=return_tensors)\n \n+    def _get_num_multimodal_tokens(self, image_sizes=None, **kwargs):\n+        \"\"\"\n+        Computes the number of placeholder tokens needed for multimodal inputs with the given sizes.\n+\n+        Args:\n+            image_sizes (`List[List[int]]`, *optional*):\n+                The input sizes formatted as (height, width) per each image.\n+\n+        Returns:\n+            `MultiModalData`: A `MultiModalData` object holding number of tokens per each of the provided\n+            input modalities, along with other useful data.\n+        \"\"\"\n+\n+        vision_data = {}\n+        if image_sizes is not None:\n+            images_kwargs = LlavaProcessorKwargs._defaults.get(\"images_kwargs\", {})\n+            images_kwargs.update(kwargs)\n+            crop_size = images_kwargs.get(\"crop_size\", None) or self.image_processor.crop_size\n+            resized_height, resized_width = crop_size[\"height\"], crop_size[\"width\"]\n+\n+            num_image_tokens = (resized_height // self.patch_size) * (resized_width // self.patch_size)\n+            num_image_tokens += self.num_additional_image_tokens\n+            if self.vision_feature_select_strategy == \"default\":\n+                num_image_tokens -= 1\n+\n+            num_image_tokens = [num_image_tokens] * len(image_sizes)\n+            num_image_patches = [1] * len(image_sizes)\n+            vision_data.update({\"num_image_tokens\": num_image_tokens, \"num_image_patches\": num_image_patches})\n+\n+        return MultiModalData(**vision_data)\n+\n     # Copied from transformers.models.clip.processing_clip.CLIPProcessor.batch_decode with CLIP->Llama\n     def batch_decode(self, *args, **kwargs):\n         \"\"\""
        },
        {
            "sha": "7a3eb43b7be1d910dd9b3337746841620b99b09f",
            "filename": "src/transformers/models/llava_next/processing_llava_next.py",
            "status": "modified",
            "additions": 59,
            "deletions": 1,
            "changes": 60,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e1017b479d0390f30c6376169a236264da2ec47/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e1017b479d0390f30c6376169a236264da2ec47/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py?ref=9e1017b479d0390f30c6376169a236264da2ec47",
            "patch": "@@ -18,10 +18,18 @@\n \n from typing import List, Union\n \n+import numpy as np\n+\n from ...feature_extraction_utils import BatchFeature\n from ...image_processing_utils import select_best_resolution\n from ...image_utils import ImageInput, get_image_size, to_numpy_array\n-from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack, _validate_images_text_input_order\n+from ...processing_utils import (\n+    MultiModalData,\n+    ProcessingKwargs,\n+    ProcessorMixin,\n+    Unpack,\n+    _validate_images_text_input_order,\n+)\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n from ...utils import logging\n \n@@ -33,6 +41,7 @@ class LlavaNextProcessorKwargs(ProcessingKwargs, total=False):\n     _defaults = {\n         \"text_kwargs\": {\n             \"padding\": False,\n+            \"return_mm_token_type_ids\": False,\n         },\n         \"images_kwargs\": {\n             \"do_pad\": True,\n@@ -172,9 +181,16 @@ def __call__(\n             prompt_strings = [sample.replace(\"<placeholder>\", self.image_token) for sample in prompt_strings]\n \n         return_tensors = output_kwargs[\"text_kwargs\"].pop(\"return_tensors\", None)\n+        return_mm_token_type_ids = output_kwargs[\"text_kwargs\"].pop(\"return_mm_token_type_ids\", None)\n         text_inputs = self.tokenizer(prompt_strings, **output_kwargs[\"text_kwargs\"])\n         self._check_special_mm_tokens(prompt_strings, text_inputs, modalities=[\"image\"])\n \n+        if return_mm_token_type_ids:\n+            array_ids = np.array(text_inputs[\"input_ids\"])\n+            mm_token_type_ids = np.zeros_like(text_inputs[\"input_ids\"])\n+            mm_token_type_ids[array_ids == self.image_token_id] = 1\n+            text_inputs[\"mm_token_type_ids\"] = mm_token_type_ids.tolist()\n+\n         return BatchFeature(data={**text_inputs, **image_inputs}, tensor_type=return_tensors)\n \n     def _get_number_of_features(self, orig_height: int, orig_width: int, height: int, width: int) -> int:\n@@ -219,6 +235,48 @@ def _get_unpadded_features(self, height, width, patches_height, patches_width, s\n         newline_features = current_height\n         return (unpadded_features, newline_features)\n \n+    def _get_num_multimodal_tokens(self, image_sizes=None, **kwargs):\n+        \"\"\"\n+        Computes the number of placeholder tokens needed for multimodal inputs with the given sizes.\n+        Args:\n+            image_sizes (List[List[str]], *optional*):\n+                The input sizes formatted as (height, width) per each image.\n+            video_sizes (List[List[str]], *optional*):\n+                The input sizes formatted as (num_frames, height, width) per each video.\n+            audio_lengths (List[int], *optional*):\n+                The input length formatted as per each audio.\n+        Returns:\n+            Dict[str, List[int]]: A dictionary mapping each modality (\"image\", \"video\", \"audio\")\n+            to a list containing the number of placeholder tokens required. If the model doesn't accept\n+            a certain modality or no input sizes are provided, the dict value is set to an empty list.\n+        \"\"\"\n+        vision_data = {}\n+        if image_sizes is not None:\n+            images_kwargs = LlavaNextProcessorKwargs._defaults.get(\"images_kwargs\", {})\n+            images_kwargs.update(kwargs)\n+\n+            size = images_kwargs.get(\"size\", None) or self.image_processor.size\n+            size = (\n+                (size[\"shortest_edge\"], size[\"shortest_edge\"])\n+                if \"shortest_edge\" in size\n+                else (min(size[\"height\"], size[\"width\"]), min(size[\"height\"], size[\"width\"]))\n+            )\n+            processed_height, processed_width = size\n+\n+            batch_num_image_tokens = []\n+            num_image_patches = [1] * len(image_sizes)  # llava-next doesn't batch pixels as Idefics, thus `1` patch`\n+            for image_size in image_sizes:\n+                orig_height, orig_width = image_size\n+                num_image_tokens = self._get_number_of_features(\n+                    orig_height, orig_width, processed_height, processed_width\n+                )\n+                if self.vision_feature_select_strategy == \"default\":\n+                    num_image_tokens -= 1\n+                batch_num_image_tokens.append(num_image_tokens)\n+            vision_data.update({\"num_image_tokens\": batch_num_image_tokens, \"num_image_patches\": num_image_patches})\n+\n+        return MultiModalData(**vision_data)\n+\n     # Copied from transformers.models.clip.processing_clip.CLIPProcessor.batch_decode with CLIP->Llama\n     def batch_decode(self, *args, **kwargs):\n         \"\"\""
        },
        {
            "sha": "224b6e37cca4320ec7f4fcb8f14fb782886c5d97",
            "filename": "src/transformers/models/llava_onevision/processing_llava_onevision.py",
            "status": "modified",
            "additions": 51,
            "deletions": 1,
            "changes": 52,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e1017b479d0390f30c6376169a236264da2ec47/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e1017b479d0390f30c6376169a236264da2ec47/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py?ref=9e1017b479d0390f30c6376169a236264da2ec47",
            "patch": "@@ -24,7 +24,7 @@\n from ...feature_extraction_utils import BatchFeature\n from ...image_processing_utils import select_best_resolution\n from ...image_utils import ImageInput, get_image_size, to_numpy_array\n-from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n+from ...processing_utils import MultiModalData, ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n from ...utils import logging\n from ...video_utils import VideoInput\n@@ -38,6 +38,7 @@ class LlavaOnevisionProcessorKwargs(ProcessingKwargs, total=False):\n     _defaults = {\n         \"text_kwargs\": {\n             \"padding\": False,\n+            \"return_mm_token_type_ids\": False,\n         },\n         \"image_kwargs\": {},\n         \"videos_kwargs\": {},\n@@ -196,9 +197,16 @@ def __call__(\n             text = [sample.replace(self.video_token, self.video_token * num_video_tokens) for sample in text]\n \n         return_tensors = output_kwargs[\"text_kwargs\"].pop(\"return_tensors\", None)\n+        return_mm_token_type_ids = output_kwargs[\"text_kwargs\"].pop(\"return_mm_token_type_ids\", None)\n         text_inputs = self.tokenizer(text, **output_kwargs[\"text_kwargs\"])\n         self._check_special_mm_tokens(text, text_inputs, modalities=[\"image\"])\n \n+        if return_mm_token_type_ids:\n+            array_ids = np.array(text_inputs[\"input_ids\"])\n+            mm_token_type_ids = np.zeros_like(text_inputs[\"input_ids\"])\n+            mm_token_type_ids[array_ids == self.image_token_id] = 1\n+            text_inputs[\"mm_token_type_ids\"] = mm_token_type_ids.tolist()\n+\n         return BatchFeature(data={**text_inputs, **image_inputs, **video_inputs}, tensor_type=return_tensors)\n \n     def _expand_image_tokens(\n@@ -285,6 +293,48 @@ def _get_unpadded_features(self, height, width, patches_height, patches_width, s\n \n         return (unpadded_features, newline_features)\n \n+    def _get_num_multimodal_tokens(self, image_sizes=None, video_sizes=None, **kwargs):\n+        \"\"\"\n+        Computes the number of placeholder tokens needed for multimodal inputs with the given sizes.\n+        Args:\n+            image_sizes (List[List[str]], *optional*):\n+                The input sizes formatted as (height, width) per each image.\n+            video_sizes (List[List[str]], *optional*):\n+                The input sizes formatted as (num_frames, height, width) per each video.\n+            audio_lengths (List[int], *optional*):\n+                The input length formatted as per each audio.\n+        Returns:\n+            Dict[str, List[int]]: A dictionary mapping each modality (\"image\", \"video\", \"audio\")\n+            to a list containing the number of placeholder tokens required. If the model doesn't accept\n+            a certain modality or no input sizes are provided, the dict value is set to an empty list.\n+        \"\"\"\n+        vision_data = {}\n+        if image_sizes is not None:\n+            images_kwargs = LlavaOnevisionProcessorKwargs._defaults.get(\"images_kwargs\", {})\n+            images_kwargs.update(kwargs)\n+\n+            size = images_kwargs.get(\"size\", None) or self.image_processor.size\n+            size = (\n+                (size[\"shortest_edge\"], size[\"shortest_edge\"])\n+                if \"shortest_edge\" in size\n+                else (min(size[\"height\"], size[\"width\"]), min(size[\"height\"], size[\"width\"]))\n+            )\n+            processed_height, processed_width = size\n+\n+            batch_num_image_tokens = []\n+            num_image_patches = [1] * len(image_sizes)  # llava-ov doesn't batch pixels as Idefics, thus `1` patch`\n+            for image_size in image_sizes:\n+                orig_height, orig_width = image_size\n+                num_image_tokens = self._get_number_of_features(\n+                    orig_height, orig_width, processed_height, processed_width\n+                )\n+                if self.vision_feature_select_strategy == \"default\":\n+                    num_image_tokens -= 1\n+                batch_num_image_tokens.append(num_image_tokens)\n+            vision_data.update({\"num_image_tokens\": batch_num_image_tokens, \"num_image_patches\": num_image_patches})\n+\n+        return MultiModalData(**vision_data)\n+\n     # Copied from transformers.models.clip.processing_clip.CLIPProcessor.batch_decode with CLIP->Llama\n     def batch_decode(self, *args, **kwargs):\n         \"\"\""
        },
        {
            "sha": "c18a698c2378738037e6a4033e1222699fb41633",
            "filename": "src/transformers/models/paligemma/processing_paligemma.py",
            "status": "modified",
            "additions": 31,
            "deletions": 0,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e1017b479d0390f30c6376169a236264da2ec47/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fprocessing_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e1017b479d0390f30c6376169a236264da2ec47/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fprocessing_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fprocessing_paligemma.py?ref=9e1017b479d0390f30c6376169a236264da2ec47",
            "patch": "@@ -18,10 +18,13 @@\n \n from typing import List, Optional, Union\n \n+import numpy as np\n+\n from ...feature_extraction_utils import BatchFeature\n from ...image_utils import ImageInput, is_valid_image, make_flat_list_of_images\n from ...processing_utils import (\n     ImagesKwargs,\n+    MultiModalData,\n     ProcessingKwargs,\n     ProcessorMixin,\n     TextKwargs,\n@@ -56,6 +59,7 @@ class PaliGemmaProcessorKwargs(ProcessingKwargs, total=False):\n     _defaults = {\n         \"text_kwargs\": {\n             \"padding\": False,\n+            \"return_mm_token_type_ids\": False,\n         },\n         \"images_kwargs\": {\n             \"data_format\": \"channels_first\",\n@@ -299,6 +303,7 @@ def __call__(\n         pixel_values = self.image_processor(images, **output_kwargs[\"images_kwargs\"])[\"pixel_values\"]\n \n         return_tensors = output_kwargs[\"text_kwargs\"].pop(\"return_tensors\", None)\n+        return_mm_token_type_ids = output_kwargs[\"text_kwargs\"].pop(\"return_mm_token_type_ids\", None)\n         inputs = self.tokenizer(\n             input_strings,\n             text_pair=suffix,\n@@ -312,8 +317,34 @@ def __call__(\n         if return_token_type_ids:\n             labels = inputs[\"input_ids\"].masked_fill(inputs[\"token_type_ids\"] == 0, -100)\n             return_data.update({\"labels\": labels})\n+\n+        if return_mm_token_type_ids:\n+            array_ids = np.array(return_data[\"input_ids\"])\n+            mm_token_type_ids = np.zeros_like(return_data[\"input_ids\"])\n+            mm_token_type_ids[array_ids == self.image_token_id] = 1\n+            return_data[\"mm_token_type_ids\"] = mm_token_type_ids.tolist()\n+\n         return BatchFeature(data=return_data, tensor_type=return_tensors)\n \n+    def _get_num_multimodal_tokens(self, image_sizes=None, **kwargs):\n+        \"\"\"\n+        Computes the number of placeholder tokens needed for multimodal inputs with the given sizes.\n+\n+        Args:\n+            image_sizes (List[List[str]], *optional*):\n+                The input sizes formatted as (height, width) per each image.\n+        Returns:\n+            Dict[str, List[int]]: A dictionary mapping each modality (\"image\", \"video\", \"audio\")\n+            to a list containing the number of placeholder tokens required. If the model doesn't accept\n+            a certain modality or no input sizes are provided, the dict value is set to an empty list.\n+        \"\"\"\n+        vision_data = {}\n+        if image_sizes is not None:\n+            num_image_tokens = [self.image_seq_length] * len(image_sizes)\n+            num_image_patches = [1] * len(image_sizes)\n+            vision_data.update({\"num_image_tokens\": num_image_tokens, \"num_image_patches\": num_image_patches})\n+        return MultiModalData(**vision_data)\n+\n     # Copied from transformers.models.clip.processing_clip.CLIPProcessor.batch_decode with CLIP->Gemma\n     def batch_decode(self, *args, **kwargs):\n         \"\"\""
        },
        {
            "sha": "aa5681f28b3edbde8a09345ba41d1cdac071eca3",
            "filename": "src/transformers/models/pixtral/processing_pixtral.py",
            "status": "modified",
            "additions": 64,
            "deletions": 3,
            "changes": 67,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e1017b479d0390f30c6376169a236264da2ec47/src%2Ftransformers%2Fmodels%2Fpixtral%2Fprocessing_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e1017b479d0390f30c6376169a236264da2ec47/src%2Ftransformers%2Fmodels%2Fpixtral%2Fprocessing_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fprocessing_pixtral.py?ref=9e1017b479d0390f30c6376169a236264da2ec47",
            "patch": "@@ -18,11 +18,23 @@\n \n from typing import List, Union\n \n+import numpy as np\n+\n from ...feature_extraction_utils import BatchFeature\n from ...image_utils import ImageInput, is_valid_image, load_image\n-from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack, _validate_images_text_input_order\n+from ...processing_utils import (\n+    MultiModalData,\n+    ProcessingKwargs,\n+    ProcessorMixin,\n+    Unpack,\n+    _validate_images_text_input_order,\n+)\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n-from ...utils import logging\n+from ...utils import is_vision_available, logging\n+\n+\n+if is_vision_available():\n+    from .image_processing_pixtral import get_resize_output_image_size\n \n \n logger = logging.get_logger(__name__)\n@@ -32,6 +44,7 @@ class PixtralProcessorKwargs(ProcessingKwargs, total=False):\n     _defaults = {\n         \"text_kwargs\": {\n             \"padding\": False,\n+            \"return_mm_token_type_ids\": False,\n         },\n         \"images_kwargs\": {},\n         \"common_kwargs\": {\n@@ -106,6 +119,10 @@ def __init__(\n         self.image_token_id = tokenizer.convert_tokens_to_ids(self.image_token)\n         self.image_break_token = image_break_token\n         self.image_end_token = image_end_token\n+        self.image_token_id = tokenizer.convert_tokens_to_ids(self.image_token)\n+        self.image_break_token_id = tokenizer.convert_tokens_to_ids(self.image_break_token)\n+        self.image_end_token_id = tokenizer.convert_tokens_to_ids(self.image_end_token)\n+        self.image_ids = [self.image_token_id, self.image_break_token_id, self.image_end_token_id]\n         super().__init__(image_processor, tokenizer, chat_template=chat_template)\n \n     def __call__(\n@@ -213,10 +230,54 @@ def __call__(\n                 prompt_strings.append(sample)\n \n         return_tensors = output_kwargs[\"text_kwargs\"].pop(\"return_tensors\", None)\n-        text_inputs = self.tokenizer(prompt_strings, **output_kwargs[\"text_kwargs\"])\n+        return_mm_token_type_ids = output_kwargs[\"text_kwargs\"].pop(\"return_mm_token_type_ids\", False)\n+        text_inputs = self.tokenizer(prompt_strings, **output_kwargs[\"text_kwargs\"], return_tensors=None)\n         self._check_special_mm_tokens(prompt_strings, text_inputs, modalities=[\"image\"])\n+\n+        if return_mm_token_type_ids:\n+            array_ids = np.array(text_inputs[\"input_ids\"])\n+            mm_token_type_ids = np.zeros_like(text_inputs[\"input_ids\"])\n+            mm_token_type_ids[np.isin(array_ids, self.image_ids)] = 1\n+            text_inputs[\"mm_token_type_ids\"] = mm_token_type_ids.tolist()\n+\n         return BatchFeature(data={**text_inputs, **image_inputs}, tensor_type=return_tensors)\n \n+    def _get_num_multimodal_tokens(self, image_sizes=None, **kwargs):\n+        \"\"\"\n+        Computes the number of placeholder tokens needed for multimodal inputs with the given sizes.\n+\n+        Args:\n+            image_sizes (`List[List[int]]`, *optional*):\n+                The input sizes formatted as (height, width) per each image.\n+\n+        Returns:\n+            `MultiModalData`: A `MultiModalData` object holding number of tokens per each of the provided\n+            input modalities, along with other useful data.\n+        \"\"\"\n+        vision_data = {}\n+        if image_sizes is not None:\n+            images_kwargs = PixtralProcessorKwargs._defaults.get(\"images_kwargs\", {})\n+            images_kwargs.update(kwargs)\n+\n+            size = images_kwargs.get(\"size\", None) or self.image_processor.size\n+            patch_size = self.patch_size * self.spatial_merge_size\n+\n+            num_image_tokens = []\n+            for height, width in image_sizes:\n+                resized_height, resized_width = get_resize_output_image_size(\n+                    image=np.zeros((height, width, 3)),\n+                    size=(size[\"longest_edge\"], size[\"longest_edge\"]),\n+                    patch_size=(patch_size, patch_size),\n+                )\n+                num_height_tokens = resized_height // patch_size\n+                num_width_tokens = resized_width // patch_size\n+                num_image_tokens.append((num_width_tokens + 1) * num_height_tokens)\n+\n+            num_image_patches = [1] * len(image_sizes)\n+            vision_data.update({\"num_image_tokens\": num_image_tokens, \"num_image_patches\": num_image_patches})\n+\n+        return MultiModalData(**vision_data)\n+\n     # Copied from transformers.models.clip.processing_clip.CLIPProcessor.batch_decode with CLIP->Llama\n     def batch_decode(self, *args, **kwargs):\n         \"\"\""
        },
        {
            "sha": "3a711a2805f42bda5848aa8a94dd8fa98c63de14",
            "filename": "src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py",
            "status": "modified",
            "additions": 48,
            "deletions": 1,
            "changes": 49,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e1017b479d0390f30c6376169a236264da2ec47/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e1017b479d0390f30c6376169a236264da2ec47/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py?ref=9e1017b479d0390f30c6376169a236264da2ec47",
            "patch": "@@ -22,6 +22,7 @@\n from dataclasses import dataclass\n from typing import List, Optional, Tuple, Union\n \n+import numpy as np\n import torch\n import torch.nn as nn\n import torch.nn.functional as F\n@@ -48,7 +49,7 @@\n from ...feature_extraction_utils import BatchFeature\n from ...image_utils import ImageInput\n from ...modeling_flash_attention_utils import is_flash_attn_available\n-from ...processing_utils import ProcessingKwargs, Unpack, VideosKwargs\n+from ...processing_utils import MultiModalData, ProcessingKwargs, Unpack, VideosKwargs\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n from ...utils import is_torchdynamo_compiling, logging\n from ...video_utils import VideoInput\n@@ -925,6 +926,7 @@ class Qwen2_5_VLProcessorKwargs(ProcessingKwargs, total=False):\n     _defaults = {\n         \"text_kwargs\": {\n             \"padding\": False,\n+            \"return_mm_token_type_ids\": False,\n         },\n         \"videos_kwargs\": {\"fps\": 2.0},\n     }\n@@ -1050,11 +1052,56 @@ def __call__(\n                 text[i] = text[i].replace(\"<|placeholder|>\", self.video_token)\n \n         return_tensors = output_kwargs[\"text_kwargs\"].pop(\"return_tensors\", None)\n+        return_mm_token_type_ids = output_kwargs[\"text_kwargs\"].pop(\"return_mm_token_type_ids\", None)\n         text_inputs = self.tokenizer(text, **output_kwargs[\"text_kwargs\"])\n         self._check_special_mm_tokens(text, text_inputs, modalities=[\"image\", \"video\"])\n \n+        if return_mm_token_type_ids:\n+            array_ids = np.array(text_inputs[\"input_ids\"])\n+            mm_token_type_ids = np.zeros_like(text_inputs[\"input_ids\"])\n+            mm_token_type_ids[array_ids == self.image_token_id] = 1\n+            text_inputs[\"mm_token_type_ids\"] = mm_token_type_ids.tolist()\n+\n         return BatchFeature(data={**text_inputs, **image_inputs, **videos_inputs}, tensor_type=return_tensors)\n \n+    def _get_num_multimodal_tokens(self, image_sizes=None, video_sizes=None, **kwargs):\n+        \"\"\"\n+        Computes the number of placeholder tokens needed for multimodal inputs with the given sizes.\n+        Args:\n+            image_sizes (`List[List[int]]`, *optional*):\n+                The input sizes formatted as (height, width) per each image.\n+            video_sizes (`List[List[int]]`, *optional*):\n+                The input sizes formatted as (num_frames, height, width) per each video.\n+        Returns:\n+            `MultiModalData`: A `MultiModalData` object holding number of tokens per each of the provided\n+            input modalities, along with other useful data.\n+        \"\"\"\n+\n+        vision_data = {}\n+        if image_sizes is not None:\n+            images_kwargs = Qwen2_5_VLProcessorKwargs._defaults.get(\"images_kwargs\", {})\n+            images_kwargs.update(kwargs)\n+            merge_size = images_kwargs.get(\"merge_size\", None) or self.image_processor.merge_size\n+\n+            num_image_patches = [\n+                self.image_processor.get_number_of_image_patches(*image_size, images_kwargs)\n+                for image_size in image_sizes\n+            ]\n+            num_image_tokens = [(num_patches // merge_size**2) for num_patches in num_image_patches]\n+            vision_data.update({\"num_image_tokens\": num_image_tokens, \"num_image_patches\": num_image_patches})\n+\n+        if video_sizes is not None:\n+            videos_kwargs = Qwen2_5_VLProcessorKwargs._defaults.get(\"videos_kwargs\", {})\n+            videos_kwargs.update(kwargs)\n+            num_video_patches = [\n+                self.video_processor.get_number_of_video_patches(*video_size, videos_kwargs)\n+                for video_size in video_sizes\n+            ]\n+            num_video_tokens = [(num_patches // merge_size**2) for num_patches in num_video_patches]\n+            vision_data[\"num_video_tokens\"] = num_video_tokens\n+\n+        return MultiModalData(**vision_data)\n+\n \n __all__ = [\n     \"Qwen2_5_VLConfig\","
        },
        {
            "sha": "e3671dc3c4366e35be7728a0f97bcb85667cd9c6",
            "filename": "src/transformers/models/qwen2_5_vl/processing_qwen2_5_vl.py",
            "status": "modified",
            "additions": 49,
            "deletions": 1,
            "changes": 50,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e1017b479d0390f30c6376169a236264da2ec47/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fprocessing_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e1017b479d0390f30c6376169a236264da2ec47/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fprocessing_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fprocessing_qwen2_5_vl.py?ref=9e1017b479d0390f30c6376169a236264da2ec47",
            "patch": "@@ -25,9 +25,11 @@\n # limitations under the License.\n from typing import List, Optional, Union\n \n+import numpy as np\n+\n from ...feature_extraction_utils import BatchFeature\n from ...image_utils import ImageInput\n-from ...processing_utils import ImagesKwargs, ProcessingKwargs, ProcessorMixin, Unpack, VideosKwargs\n+from ...processing_utils import ImagesKwargs, MultiModalData, ProcessingKwargs, ProcessorMixin, Unpack, VideosKwargs\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n from ...video_utils import VideoInput\n \n@@ -50,6 +52,7 @@ class Qwen2_5_VLProcessorKwargs(ProcessingKwargs, total=False):\n     _defaults = {\n         \"text_kwargs\": {\n             \"padding\": False,\n+            \"return_mm_token_type_ids\": False,\n         },\n         \"videos_kwargs\": {\"fps\": 2.0},\n     }\n@@ -188,11 +191,56 @@ def __call__(\n                 text[i] = text[i].replace(\"<|placeholder|>\", self.video_token)\n \n         return_tensors = output_kwargs[\"text_kwargs\"].pop(\"return_tensors\", None)\n+        return_mm_token_type_ids = output_kwargs[\"text_kwargs\"].pop(\"return_mm_token_type_ids\", None)\n         text_inputs = self.tokenizer(text, **output_kwargs[\"text_kwargs\"])\n         self._check_special_mm_tokens(text, text_inputs, modalities=[\"image\", \"video\"])\n \n+        if return_mm_token_type_ids:\n+            array_ids = np.array(text_inputs[\"input_ids\"])\n+            mm_token_type_ids = np.zeros_like(text_inputs[\"input_ids\"])\n+            mm_token_type_ids[array_ids == self.image_token_id] = 1\n+            text_inputs[\"mm_token_type_ids\"] = mm_token_type_ids.tolist()\n+\n         return BatchFeature(data={**text_inputs, **image_inputs, **videos_inputs}, tensor_type=return_tensors)\n \n+    def _get_num_multimodal_tokens(self, image_sizes=None, video_sizes=None, **kwargs):\n+        \"\"\"\n+        Computes the number of placeholder tokens needed for multimodal inputs with the given sizes.\n+        Args:\n+            image_sizes (`List[List[int]]`, *optional*):\n+                The input sizes formatted as (height, width) per each image.\n+            video_sizes (`List[List[int]]`, *optional*):\n+                The input sizes formatted as (num_frames, height, width) per each video.\n+        Returns:\n+            `MultiModalData`: A `MultiModalData` object holding number of tokens per each of the provided\n+            input modalities, along with other useful data.\n+        \"\"\"\n+\n+        vision_data = {}\n+        if image_sizes is not None:\n+            images_kwargs = Qwen2_5_VLProcessorKwargs._defaults.get(\"images_kwargs\", {})\n+            images_kwargs.update(kwargs)\n+            merge_size = images_kwargs.get(\"merge_size\", None) or self.image_processor.merge_size\n+\n+            num_image_patches = [\n+                self.image_processor.get_number_of_image_patches(*image_size, images_kwargs)\n+                for image_size in image_sizes\n+            ]\n+            num_image_tokens = [(num_patches // merge_size**2) for num_patches in num_image_patches]\n+            vision_data.update({\"num_image_tokens\": num_image_tokens, \"num_image_patches\": num_image_patches})\n+\n+        if video_sizes is not None:\n+            videos_kwargs = Qwen2_5_VLProcessorKwargs._defaults.get(\"videos_kwargs\", {})\n+            videos_kwargs.update(kwargs)\n+            num_video_patches = [\n+                self.video_processor.get_number_of_video_patches(*video_size, videos_kwargs)\n+                for video_size in video_sizes\n+            ]\n+            num_video_tokens = [(num_patches // merge_size**2) for num_patches in num_video_patches]\n+            vision_data[\"num_video_tokens\"] = num_video_tokens\n+\n+        return MultiModalData(**vision_data)\n+\n     def batch_decode(self, *args, **kwargs):\n         \"\"\"\n         This method forwards all its arguments to Qwen2TokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please"
        },
        {
            "sha": "48e8594b1281914a51188f7e00b0e2b81fa9bad7",
            "filename": "src/transformers/models/qwen2_vl/image_processing_qwen2_vl.py",
            "status": "modified",
            "additions": 26,
            "deletions": 0,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e1017b479d0390f30c6376169a236264da2ec47/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e1017b479d0390f30c6376169a236264da2ec47/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl.py?ref=9e1017b479d0390f30c6376169a236264da2ec47",
            "patch": "@@ -490,5 +490,31 @@ def preprocess(\n \n         return BatchFeature(data=data, tensor_type=return_tensors)\n \n+    def get_number_of_image_patches(self, height: int, width: int, images_kwargs=None):\n+        \"\"\"\n+        A utility that returns number of image patches for a given image size.\n+\n+        Args:\n+            height (`int`):\n+                Height of the input image.\n+            width (`int`):\n+                Width of the input image.\n+            images_kwargs (`dict`, *optional*)\n+                Any kwargs to override defaults of the image processor.\n+        Returns:\n+            `int`: Number of image patches per image.\n+        \"\"\"\n+        min_pixels = images_kwargs.get(\"min_pixels\", None) or self.size[\"shortest_edge\"]\n+        max_pixels = images_kwargs.get(\"max_pixels\", None) or self.size[\"longest_edge\"]\n+        patch_size = images_kwargs.get(\"patch_size\", None) or self.patch_size\n+        merge_size = images_kwargs.get(\"merge_size\", None) or self.merge_size\n+\n+        factor = patch_size * merge_size\n+        resized_height, resized_width = smart_resize(\n+            height, width, factor, min_pixels=min_pixels, max_pixels=max_pixels\n+        )\n+        grid_h, grid_w = resized_height // patch_size, resized_width // patch_size\n+        return grid_h * grid_w\n+\n \n __all__ = [\"Qwen2VLImageProcessor\"]"
        },
        {
            "sha": "7c27694c759070f5ac95f760cce4b898d8a385f6",
            "filename": "src/transformers/models/qwen2_vl/image_processing_qwen2_vl_fast.py",
            "status": "modified",
            "additions": 26,
            "deletions": 0,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e1017b479d0390f30c6376169a236264da2ec47/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e1017b479d0390f30c6376169a236264da2ec47/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py?ref=9e1017b479d0390f30c6376169a236264da2ec47",
            "patch": "@@ -402,5 +402,31 @@ def preprocess(\n \n         return BatchFeature(data=data, tensor_type=return_tensors)\n \n+    def get_number_of_image_patches(self, height: int, width: int, images_kwargs=None):\n+        \"\"\"\n+        A utility that returns number of image patches for a given image size.\n+\n+        Args:\n+            height (`int`):\n+                Height of the input image.\n+            width (`int`):\n+                Width of the input image.\n+            images_kwargs (`dict`, *optional*)\n+                Any kwargs to override defaults of the image processor.\n+        Returns:\n+            `int`: Number of image patches per image.\n+        \"\"\"\n+        min_pixels = images_kwargs.get(\"min_pixels\", None) or self.size[\"shortest_edge\"]\n+        max_pixels = images_kwargs.get(\"max_pixels\", None) or self.size[\"longest_edge\"]\n+        patch_size = images_kwargs.get(\"patch_size\", None) or self.patch_size\n+        merge_size = images_kwargs.get(\"merge_size\", None) or self.merge_size\n+\n+        factor = patch_size * merge_size\n+        resized_height, resized_width = smart_resize(\n+            height, width, factor, min_pixels=min_pixels, max_pixels=max_pixels\n+        )\n+        grid_h, grid_w = resized_height // patch_size, resized_width // patch_size\n+        return grid_h * grid_w\n+\n \n __all__ = [\"Qwen2VLImageProcessorFast\"]"
        },
        {
            "sha": "dd4cafa132ef236c504a0450eaed40a117534a17",
            "filename": "src/transformers/models/qwen2_vl/processing_qwen2_vl.py",
            "status": "modified",
            "additions": 51,
            "deletions": 2,
            "changes": 53,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e1017b479d0390f30c6376169a236264da2ec47/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fprocessing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e1017b479d0390f30c6376169a236264da2ec47/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fprocessing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fprocessing_qwen2_vl.py?ref=9e1017b479d0390f30c6376169a236264da2ec47",
            "patch": "@@ -23,9 +23,11 @@\n \n from typing import List, Optional, Union\n \n+import numpy as np\n+\n from ...feature_extraction_utils import BatchFeature\n from ...image_utils import ImageInput\n-from ...processing_utils import ImagesKwargs, ProcessingKwargs, ProcessorMixin, Unpack\n+from ...processing_utils import ImagesKwargs, MultiModalData, ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n from ...utils import logging\n from ...video_utils import VideoInput\n@@ -47,6 +49,7 @@ class Qwen2VLProcessorKwargs(ProcessingKwargs, total=False):\n     _defaults = {\n         \"text_kwargs\": {\n             \"padding\": False,\n+            \"return_mm_token_type_ids\": False,\n         },\n     }\n \n@@ -172,10 +175,56 @@ def __call__(\n                 text[i] = text[i].replace(\"<|placeholder|>\", self.video_token)\n \n         return_tensors = output_kwargs[\"text_kwargs\"].pop(\"return_tensors\", None)\n-        text_inputs = self.tokenizer(text, **output_kwargs[\"text_kwargs\"])\n+        return_mm_token_type_ids = output_kwargs[\"text_kwargs\"].pop(\"return_mm_token_type_ids\", False)\n+        text_inputs = self.tokenizer(text, **output_kwargs[\"text_kwargs\"], return_tensors=None)\n         self._check_special_mm_tokens(text, text_inputs, modalities=[\"image\", \"video\"])\n+\n+        if return_mm_token_type_ids:\n+            array_ids = np.array(text_inputs[\"input_ids\"])\n+            mm_token_type_ids = np.zeros_like(text_inputs[\"input_ids\"])\n+            mm_token_type_ids[array_ids == self.image_token_id] = 1\n+            text_inputs[\"mm_token_type_ids\"] = mm_token_type_ids.tolist()\n+\n         return BatchFeature(data={**text_inputs, **image_inputs, **videos_inputs}, tensor_type=return_tensors)\n \n+    def _get_num_multimodal_tokens(self, image_sizes=None, video_sizes=None, **kwargs):\n+        \"\"\"\n+        Computes the number of placeholder tokens needed for multimodal inputs with the given sizes.\n+        Args:\n+            image_sizes (`List[List[int]]`, *optional*):\n+                The input sizes formatted as (height, width) per each image.\n+            video_sizes (`List[List[int]]`, *optional*):\n+                The input sizes formatted as (num_frames, height, width) per each video.\n+        Returns:\n+            `MultiModalData`: A `MultiModalData` object holding number of tokens per each of the provided\n+            input modalities, along with other useful data.\n+        \"\"\"\n+\n+        vision_data = {}\n+        if image_sizes is not None:\n+            images_kwargs = Qwen2VLProcessorKwargs._defaults.get(\"images_kwargs\", {})\n+            images_kwargs.update(kwargs)\n+            merge_size = images_kwargs.get(\"merge_size\", None) or self.image_processor.merge_size\n+\n+            num_image_patches = [\n+                self.image_processor.get_number_of_image_patches(*image_size, images_kwargs)\n+                for image_size in image_sizes\n+            ]\n+            num_image_tokens = [(num_patches // merge_size**2) for num_patches in num_image_patches]\n+            vision_data.update({\"num_image_tokens\": num_image_tokens, \"num_image_patches\": num_image_patches})\n+\n+        if video_sizes is not None:\n+            videos_kwargs = Qwen2VLProcessorKwargs._defaults.get(\"videos_kwargs\", {})\n+            videos_kwargs.update(kwargs)\n+            num_video_patches = [\n+                self.video_processor.get_number_of_video_patches(*video_size, videos_kwargs)\n+                for video_size in video_sizes\n+            ]\n+            num_video_tokens = [(num_patches // merge_size**2) for num_patches in num_video_patches]\n+            vision_data[\"num_video_tokens\"] = num_video_tokens\n+\n+        return MultiModalData(**vision_data)\n+\n     def batch_decode(self, *args, **kwargs):\n         \"\"\"\n         This method forwards all its arguments to Qwen2TokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please"
        },
        {
            "sha": "991459887b2f74283ff399f2603039bfd6bd2172",
            "filename": "src/transformers/models/qwen2_vl/video_processing_qwen2_vl.py",
            "status": "modified",
            "additions": 30,
            "deletions": 0,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e1017b479d0390f30c6376169a236264da2ec47/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fvideo_processing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e1017b479d0390f30c6376169a236264da2ec47/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fvideo_processing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fvideo_processing_qwen2_vl.py?ref=9e1017b479d0390f30c6376169a236264da2ec47",
            "patch": "@@ -204,5 +204,35 @@ def _preprocess(\n             tensor_type=return_tensors,\n         )\n \n+    def get_num_of_video_patches(self, num_frames: int, height: int, width: int, videos_kwargs=None):\n+        \"\"\"\n+        A utility that returns number of video patches a given video size.\n+\n+        Args:\n+            num_frames (`int`):\n+                Number of frames in the input video.\n+            height (`int`):\n+                Height of the input video.\n+            width (`int`):\n+                Width of the input video.\n+            videos_kwargs (`dict`, *optional*)\n+                Any kwargs to override defaults of the video processor.\n+        Returns:\n+            `Tuple(int, int)`: Number of placeholder tokens required and number of patches per image.\n+        \"\"\"\n+        min_pixels = videos_kwargs.get(\"min_pixels\", None) or self.size[\"shortest_edge\"]\n+        max_pixels = videos_kwargs.get(\"max_pixels\", None) or self.size[\"longest_edge\"]\n+        patch_size = videos_kwargs.get(\"patch_size\", None) or self.patch_size\n+        merge_size = videos_kwargs.get(\"merge_size\", None) or self.merge_size\n+        temporal_patch_size = videos_kwargs.get(\"temporal_patch_size\", None) or self.temporal_patch_size\n+\n+        factor = patch_size * merge_size\n+        resized_height, resized_width = smart_resize(\n+            height, width, factor, min_pixels=min_pixels, max_pixels=max_pixels\n+        )\n+        grid_h, grid_w = resized_height // patch_size, resized_width // patch_size\n+        grid_t = num_frames // temporal_patch_size\n+        return grid_t * grid_h * grid_w\n+\n \n __all__ = [\"Qwen2VLVideoProcessor\"]"
        },
        {
            "sha": "0d788e201150a83363fa2adeeb4d56074bbc424a",
            "filename": "src/transformers/models/smolvlm/image_processing_smolvlm.py",
            "status": "modified",
            "additions": 41,
            "deletions": 0,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e1017b479d0390f30c6376169a236264da2ec47/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fimage_processing_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e1017b479d0390f30c6376169a236264da2ec47/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fimage_processing_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fimage_processing_smolvlm.py?ref=9e1017b479d0390f30c6376169a236264da2ec47",
            "patch": "@@ -847,5 +847,46 @@ def preprocess(\n \n         return encoding\n \n+    def get_number_of_image_patches(self, height: int, width: int, images_kwargs=None):\n+        \"\"\"\n+        A utility that returns number of image patches for a given image size.\n+\n+        Args:\n+            height (`int`):\n+                Height of the input image.\n+            width (`int`):\n+                Width of the input image.\n+            images_kwargs (`dict`, *optional*)\n+                Any kwargs to override defaults of the image processor.\n+        Returns:\n+            `int`: Number of patches per image.\n+        \"\"\"\n+        do_image_splitting = images_kwargs.get(\"do_image_splitting\", None) or self.do_image_splitting\n+        max_image_size = images_kwargs.get(\"max_image_size\", None) or self.max_image_size\n+        size = images_kwargs.get(\"size\", None) or self.size\n+\n+        if do_image_splitting:\n+            height, width = _resize_output_size_rescale_to_max_len(height, width, max_len=size[\"longest_edge\"])\n+            height, width = _resize_output_size_scale_below_upper_bound(height, width, max_len=4096)\n+            aspect_ratio = width / height\n+\n+            if width >= height:\n+                resized_width = math.ceil(width / max_image_size[\"longest_edge\"]) * max_image_size[\"longest_edge\"]\n+                resized_height = int(width / aspect_ratio)\n+                resized_height = math.ceil(height / max_image_size[\"longest_edge\"]) * max_image_size[\"longest_edge\"]\n+            elif height > width:\n+                resized_height = math.ceil(height / max_image_size[\"longest_edge\"]) * max_image_size[\"longest_edge\"]\n+                resized_width = int(height * aspect_ratio)\n+                resized_width = math.ceil(width / max_image_size[\"longest_edge\"]) * max_image_size[\"longest_edge\"]\n+\n+            max_height = max_width = max_image_size[\"longest_edge\"]\n+            if resized_height > max_height or resized_width > max_width:\n+                # Calculate the number of splits\n+                num_rows = math.ceil(resized_height / max_height)\n+                num_cols = math.ceil(resized_width / max_width)\n+                num_patches = num_rows * num_cols + 1\n+\n+        return num_patches\n+\n \n __all__ = [\"SmolVLMImageProcessor\"]"
        },
        {
            "sha": "add9b4e2ad2c4c0b15237f0d0b323094b156bf5e",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 30,
            "deletions": 0,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e1017b479d0390f30c6376169a236264da2ec47/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e1017b479d0390f30c6376169a236264da2ec47/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=9e1017b479d0390f30c6376169a236264da2ec47",
            "patch": "@@ -22,6 +22,7 @@\n import sys\n import typing\n import warnings\n+from dataclasses import dataclass\n from pathlib import Path\n from typing import Any, Dict, List, Optional, TypedDict, Union\n \n@@ -120,6 +121,8 @@ class TextKwargs(TypedDict, total=False):\n             Whether or not to print more information and warnings.\n         padding_side (`str`, *optional*):\n             The side on which padding will be applied.\n+        return_mm_token_type_ids (`bool`, *optional*):\n+            Whether to return multimodal token type ids indicating mm placeholder token positions.\n     \"\"\"\n \n     text_pair: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]]\n@@ -140,6 +143,7 @@ class TextKwargs(TypedDict, total=False):\n     return_length: Optional[bool]\n     verbose: Optional[bool]\n     padding_side: Optional[str]\n+    return_mm_token_type_ids: Optional[bool]\n \n \n class ImagesKwargs(TypedDict, total=False):\n@@ -455,6 +459,32 @@ class AllKwargsForChatTemplate(\n     }\n \n \n+@dataclass\n+class MultiModalData:\n+    \"\"\"\n+    Dataclass that holds extra useful data for processing\n+    multimodal data. Processors currently cannot return keys,\n+    unless it is used in model's forward. Thus we have helper\n+    methods that calculate and return useful data from processing\n+    input multimodals (images/videos).\n+    Note that this dataclass is aimed to be used only in vLLM\n+    and we might change its API in the future.\n+    \"\"\"\n+\n+    num_image_tokens: list[int] = None\n+    num_video_tokens: list[int] = None\n+    num_audio_tokens: list[int] = None\n+    num_image_patches: list[int] = None\n+\n+    def __contains__(self, key):\n+        return hasattr(self, key) and getattr(self, key) is not None\n+\n+    def __getitem__(self, key):\n+        if hasattr(self, key):\n+            return getattr(self, key)\n+        raise AttributeError(f\"{self.__class__.__name__} has no attribute {key}\")\n+\n+\n class ProcessorMixin(PushToHubMixin):\n     \"\"\"\n     This is a mixin used to provide saving/loading functionality for all processor classes."
        }
    ],
    "stats": {
        "total": 1246,
        "additions": 1167,
        "deletions": 79
    }
}