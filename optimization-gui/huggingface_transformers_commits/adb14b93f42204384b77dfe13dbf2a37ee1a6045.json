{
    "author": "fabxoe",
    "message": "ğŸŒ [i18n-KO] Translated `model_doc/llama3.md` to Korean (#33635)\n\n* docs: ko: model_doc/llama3.md\r\n\r\n* fix: resolve suggestions\r\n\r\n* fix: resolve suggestions\r\n\r\nCo-authored-by: Chaewon Song <chaewon1019@ewhain.net>\r\n\r\n* fix: resolve suggestions\r\n\r\nCo-authored-by: HyeokJun SHIN <96534680+jun048098@users.noreply.github.com>\r\n\r\n* fix: resolve suggestions\r\n\r\n* fix: resolve suggestions\r\n\r\nCo-authored-by: Chaewon Song <chaewon1019@ewhain.net>\r\n\r\n* fix: resolve suggestions\r\n\r\nCo-authored-by: Ahnjj_DEV <ahnjj.dev@gmail.com>\r\n\r\n* fix: resolve suggestions\r\n\r\nCo-authored-by: Ahnjj_DEV <ahnjj.dev@gmail.com>\r\n\r\n* fix: resolve suggestions\r\n\r\n---------\r\n\r\nCo-authored-by: Chaewon Song <chaewon1019@ewhain.net>\r\nCo-authored-by: HyeokJun SHIN <96534680+jun048098@users.noreply.github.com>\r\nCo-authored-by: Ahnjj_DEV <ahnjj.dev@gmail.com>",
    "sha": "adb14b93f42204384b77dfe13dbf2a37ee1a6045",
    "files": [
        {
            "sha": "7c0e936d6ddbfcf397bbf50d66f2892c17569c60",
            "filename": "docs/source/ko/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/adb14b93f42204384b77dfe13dbf2a37ee1a6045/docs%2Fsource%2Fko%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/adb14b93f42204384b77dfe13dbf2a37ee1a6045/docs%2Fsource%2Fko%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2F_toctree.yml?ref=adb14b93f42204384b77dfe13dbf2a37ee1a6045",
            "patch": "@@ -424,6 +424,8 @@\n         title: LLaMA\n       - local: model_doc/llama2\n         title: LLaMA2\n+      - local: model_doc/llama3\n+        title: LLaMA3\n       - local: in_translation\n         title: (ë²ˆì—­ì¤‘) Longformer\n       - local: in_translation"
        },
        {
            "sha": "ca819bfcabaf882e54f5f175fdad3732e1e22cd1",
            "filename": "docs/source/ko/model_doc/llama3.md",
            "status": "added",
            "additions": 80,
            "deletions": 0,
            "changes": 80,
            "blob_url": "https://github.com/huggingface/transformers/blob/adb14b93f42204384b77dfe13dbf2a37ee1a6045/docs%2Fsource%2Fko%2Fmodel_doc%2Fllama3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/adb14b93f42204384b77dfe13dbf2a37ee1a6045/docs%2Fsource%2Fko%2Fmodel_doc%2Fllama3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fllama3.md?ref=adb14b93f42204384b77dfe13dbf2a37ee1a6045",
            "patch": "@@ -0,0 +1,80 @@\n+<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# Llama3[[llama3]]\n+\n+```py3\n+import transformers\n+import torch\n+\n+model_id = \"meta-llama/Meta-Llama-3-8B\"\n+\n+pipeline = transformers.pipeline(\"text-generation\", model=model_id, model_kwargs={\"torch_dtype\": torch.bfloat16}, device_map=\"auto\")\n+pipeline(\"Hey how are you doing today?\")\n+```\n+\n+## ê°œìš”[[overview]]\n+\n+ë¼ë§ˆ3 ëª¨ë¸ì€ Meta AI íŒ€ì´ ì œì•ˆí•œ [ë©”íƒ€ ë¼ë§ˆ3 ì†Œê°œ: í˜„ì¬ê¹Œì§€ ê°€ì¥ ìœ ëŠ¥í•œ ê³µê°œ ê°€ëŠ¥ LLM](https://ai.meta.com/blog/meta-llama-3/)ì—ì„œ ì†Œê°œë˜ì—ˆìŠµë‹ˆë‹¤.\n+\n+í•´ë‹¹ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ì˜ ì´ˆë¡ì…ë‹ˆë‹¤:\n+\n+*ì˜¤ëŠ˜, ê´‘ë²”ìœ„í•œ ì‚¬ìš©ì„ ìœ„í•´ ì´ìš© ê°€ëŠ¥í•œ ë¼ë§ˆì˜ ì°¨ì„¸ëŒ€ ëª¨ë¸ì¸ ë©”íƒ€ ë¼ë§ˆ3ì˜ ì²« ë‘ ëª¨ë¸ì„ ê³µìœ í•˜ê²Œ ë˜ì–´ ê¸°ì©ë‹ˆë‹¤. ì´ë²ˆ ì¶œì‹œëŠ” 8Bì™€ 70B ë§¤ê°œë³€ìˆ˜ë¥¼ ê°€ì§„ ì‚¬ì „ í›ˆë ¨ ë° ì§€ì‹œ ë¯¸ì„¸ ì¡°ì •ëœ ì–¸ì–´ ëª¨ë¸ì„ íŠ¹ì§•ìœ¼ë¡œ í•˜ë©°, ê´‘ë²”ìœ„í•œ ì‚¬ìš© ì‚¬ë¡€ë¥¼ ì§€ì›í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë¼ë§ˆì˜ ì´ ì°¨ì„¸ëŒ€ ëª¨ë¸ì€ ë‹¤ì–‘í•œ ì‚°ì—… ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìµœì²¨ë‹¨ì˜ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ë©°, ê°œì„ ëœ ì¶”ë¡  ëŠ¥ë ¥ì„ í¬í•¨í•œ ìƒˆë¡œìš´ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì´ê²ƒë“¤ì´ ë‹¨ì—°ì½” í•´ë‹¹ í´ë˜ìŠ¤ì—ì„œ ìµœê³ ì˜ ì˜¤í”ˆ ì†ŒìŠ¤ ëª¨ë¸ì´ë¼ê³  ë¯¿ìŠµë‹ˆë‹¤. ì˜¤ëœ ê°œë°©ì  ì ‘ê·¼ ë°©ì‹ì„ ì§€ì§€í•˜ë©°, ìš°ë¦¬ëŠ” ë¼ë§ˆ3ë¥¼ ì»¤ë®¤ë‹ˆí‹° ê¸°ì—¬ìë“¤ì—ê²Œ ë§¡ê¸°ê³  ìˆìŠµë‹ˆë‹¤. ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ê°œë°œì ë„êµ¬, í‰ê°€, ì¶”ë¡  ìµœì í™” ë“±ì— ì´ë¥´ê¸°ê¹Œì§€ AI ìŠ¤íƒ ì „ë°˜ì— ê±¸ì¹œ ë‹¤ìŒ í˜ì‹ ì˜ ë¬¼ê²°ì„ ì´‰ë°œí•˜ê¸¸ í¬ë§í•©ë‹ˆë‹¤. ì—¬ëŸ¬ë¶„ì´ ë¬´ì—‡ì„ ë§Œë“¤ì§€ ê¸°ëŒ€í•˜ë©° ì—¬ëŸ¬ë¶„ì˜ í”¼ë“œë°±ì„ ê³ ëŒ€í•©ë‹ˆë‹¤.*\n+\n+ë¼ë§ˆ3 ëª¨ë¸ì˜ ëª¨ë“  ì²´í¬í¬ì¸íŠ¸ëŠ” [ì´ê³³](https://huggingface.co/models?search=llama3)ì—ì„œ í™•ì¸í•˜ì„¸ìš”.\n+ì›ë³¸ ì½”ë“œëŠ” [ì´ê³³](https://github.com/meta-llama/llama3)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+## ì‚¬ìš© íŒ[[usage-tips]]\n+\n+<Tip warning={true}>\n+\n+`ë¼ë§ˆ3` ëª¨ë¸ë“¤ì€ `bfloat16`ë¥¼ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨ë˜ì—ˆì§€ë§Œ, ì›ë˜ì˜ ì¶”ë¡ ì€ `float16`ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. Hubì— ì—…ë¡œë“œëœ ì²´í¬í¬ì¸íŠ¸ë“¤ì€ `torch_dtype = 'float16'`ì„ ì‚¬ìš©í•˜ëŠ”ë°, ì´ëŠ” `AutoModel` APIê°€ ì²´í¬í¬ì¸íŠ¸ë¥¼ `torch.float32`ì—ì„œ `torch.float16`ìœ¼ë¡œ ë³€í™˜í•˜ëŠ”ë° ì´ìš©ë©ë‹ˆë‹¤. \n+\n+ `model = AutoModelForCausalLM.from_pretrained(\"path\", torch_dtype = \"auto\")`ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ì´ˆê¸°í™”í•  ë•Œ, ì˜¨ë¼ì¸ ê°€ì¤‘ì¹˜ì˜ `dtype`ëŠ” `torch_dtype=\"auto\"`ë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” í•œ ëŒ€ë¶€ë¶„ ë¬´ê´€í•©ë‹ˆë‹¤. ê·¸ ì´ìœ ëŠ” ëª¨ë¸ì´ ë¨¼ì € ë‹¤ìš´ë¡œë“œë˜ê³ (ì˜¨ë¼ì¸ ì²´í¬í¬ì¸íŠ¸ì˜ `dtype`ë¥¼ ì‚¬ìš©), ê·¸ ë‹¤ìŒ `torch`ì˜ `dtype`ìœ¼ë¡œ ë³€í™˜ë˜ì–´(`torch.float32`ê°€ ë¨), ë§ˆì§€ë§‰ìœ¼ë¡œ configì— `torch_dtype`ì´ ì œê³µëœ ê²½ìš° ê°€ì¤‘ì¹˜ê°€ ì‚¬ìš©ë˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\n+\n+`float16`ìœ¼ë¡œ ëª¨ë¸ì„ í›ˆë ¨í•˜ëŠ” ê²ƒì€ ê¶Œì¥ë˜ì§€ ì•Šìœ¼ë©° `nan`ì„ ìƒì„±í•˜ëŠ” ê²ƒìœ¼ë¡œ ì•Œë ¤ì ¸ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ëª¨ë“  ëª¨ë¸ì€ `bfloat16`ìœ¼ë¡œ í›ˆë ¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.\n+\n+</Tip>\n+\n+íŒ:\n+\n+- ë¼ë§ˆ3 ëª¨ë¸ì„ ìœ„í•œ ê°€ì¤‘ì¹˜ëŠ” [ì´ í¼](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)ì„ ì±„ìš°ë©´ì„œ ì–»ì–´ì ¸ì•¼ í•©ë‹ˆë‹¤.\n+- ì•„í‚¤í…ì²˜ëŠ” ë¼ë§ˆ2ì™€ ì •í™•íˆ ê°™ìŠµë‹ˆë‹¤.\n+- í† í¬ë‚˜ì´ì €ëŠ” [tiktoken](https://github.com/openai/tiktoken) (sentencepiece êµ¬í˜„ì— ê¸°ë°˜í•œ ë¼ë§ˆ2 ì™€ëŠ” ë‹¤ë¥´ê²Œ)ì— ê¸°ë°˜í•œ BPE ëª¨ë¸ì…ë‹ˆë‹¤. tiktoken ê¸°ë°˜ í† í¬ë‚˜ì´ì €ê°€ sebtencepiece ê¸°ë°˜ ë°©ì‹ê³¼ ë‹¤ë¥¸ì ì€ ì…ë ¥ í† í°ì´ vocabì— ì´ë¯¸ ì¡´ì¬í•  ë•Œ BPE ë³‘í•© ë£°ì„ ë¬´ì‹œí•˜ê³  ì‹±ê¸€ í† í°ìœ¼ë¡œ í† í¬ë‚˜ì´ì§•í•œë‹¤ëŠ” ì ì—ì„œ ê°€ì¥ í° ì°¨ì´ë¥¼ ë³´ì…ë‹ˆë‹¤. ìì„¸íˆ ë§í•˜ë©´ `\"hugging\"`ì´ vocabì— ì¡´ì¬í•˜ê³  ê¸°ì¡´ì— ë³‘í•©ì´ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´, `[\"hug\",\"ging\"]` ì²˜ëŸ¼ ë‘ í† í°ìœ¼ë¡œ ë” ì‘ì€ ë‹¨ìœ„ì˜ ë‹¨ì–´ë¥¼ ê°€ì§€ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, í•˜ë‚˜ì˜ í† í°ë§Œì„ ìë™ìœ¼ë¡œ ë¦¬í„´í•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.\n+- ê¸°ë³¸ ëª¨ë¸ì€ íŒ¨ë”© í† í°ì´ ì—†ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•˜ëŠ” `pad_id = -1`ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ê°™ì€ ë¡œì§ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìœ¼ë‹ˆ `tokenizer.add_special_tokens({\"pad_token\":\"<pad>\"})`ë¥¼ ì‚¬ìš©í•˜ì—¬ í† í°ì„ ì¶”ê°€í•˜ê³  ì„ë² ë”© í¬ê¸°ë„ í™•ì‹¤íˆ ì¡°ì •í•´ì•¼ í•©ë‹ˆë‹¤. `model.config.pad_token_id`ë„ ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤. ëª¨ë¸ì˜ `embed_tokens` ë ˆì´ì–´ëŠ” `self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.config.padding_idx)`ë¡œ ì´ˆê¸°í™”ë˜ë©°, íŒ¨ë”© í† í°ì„ ì¸ì½”ë”©í•˜ëŠ” ê²ƒì´ 0(zero)ë¥¼ ì¶œë ¥í•˜ê²Œ í•  ê²ƒì¸ì§€ ê·¸ë˜ì„œ ì´ˆê¸°í™”ê°€ ì¶”ì²œë ë•Œ ì´ë¥¼ í†µí™”ì‹œí‚¬ ê²ƒì¸ì§€ë¥¼ ì •í•˜ê²Œ í•©ë‹ˆë‹¤. \n+- ì›ë³¸ ì²´í¬í¬ì¸íŠ¸ëŠ” ì´ [ë³€í™˜ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/convert_llama_weights_to_hf.py)ë¥¼ ì´ìš©í•´ì„œ ë³€í™˜ ê°€ëŠ¥í•©ë‹ˆë‹¤. ìŠ¤í¬ë¦½íŠ¸ëŠ” ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ í˜¸ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n+    \n+    ```bash\n+    python src/transformers/models/llama/convert_llama_weights_to_hf.py \\\n+        --input_dir /path/to/downloaded/llama/weights --model_size 7B --output_dir /output/path --llama_version 3\n+    ```\n+\n+- ë³€í™˜ í›„, ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ëŠ” ë‹¤ìŒì„ í†µí•´ ë¡œë“œëœë‹¤.\n+\n+    ```python\n+    from transformers import AutoModelForCausalLM, AutoTokenizer\n+    \n+    tokenizer = AutoTokenizer.from_pretrained(\"/output/path\")\n+    model = AutoModelForCausalLM.from_pretrained(\"/output/path\")\n+    ```\n+\n+    ì´ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰ì‹œí‚¤ë ¤ë©´ ëª¨ë¸ ì „ì²´ë¥¼ float16 ì •ë°€ë„ë¡œ í˜¸ìŠ¤íŒ…í•  ìˆ˜ ìˆëŠ” ì¶©ë¶„í•œ ë©”ì¸ë©”ëª¨ë¦¬ê°€ í•„ìš”í•˜ë‹¤ëŠ” ì ì„ ìœ ì˜í•˜ì„¸ìš”. ê°€ì¥ í° ë²„ì „ì´ ì—¬ëŸ¬ ì²´í¬í¬ì¸íŠ¸ë¡œ ë‚˜ë‰˜ì–´ ìˆë”ë¼ë„, ê° ì²´í¬í¬ì¸íŠ¸ê°€ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ ì¼ë¶€ë¥¼ í¬í•¨í•˜ê³  ìˆê¸° ë•Œë¬¸ì— ì´ë¥¼ ëª¨ë‘ RAMì— ë¡œë“œí•´ì•¼ í•©ë‹ˆë‹¤. 75B ëª¨ë¸ì„ ì˜ˆë¡œ ë“¤ë©´ ëŒ€ëµ 145GBì˜ RAMì´ í•„ìš”í•©ë‹ˆë‹¤. \n+\n+- `attn_implementation=\"flash_attention_2\"`ë¥¼ í†µí•´ì„œ í”Œë˜ì‹œ ì–´í…ì…˜2ë¥¼ ì‚¬ìš©í•  ë•Œ, `from_pretrained` í´ë˜ìŠ¤ ë©”ì„œë“œì— `torch_dtype`ë¥¼ ì „ë‹¬í•˜ì§€ ë§ê³  ìë™ í˜¼í•© ì •ë°€ë„(Automatic Mixed-Precision) í•™ìŠµì„ ì‚¬ìš©í•˜ì„¸ìš”. `Trainer`ë¥¼ ì‚¬ìš©í•  ë•ŒëŠ” ë‹¨ìˆœíˆ `fp16` ë˜ëŠ” `bf16`ì„ `True`ë¡œ ì„¤ì •í•˜ë©´ ë©ë‹ˆë‹¤. ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ë°˜ë“œì‹œ `torch.autocast`ë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤. í”Œë˜ì‹œ ì–´í…ì…˜ì€ `fp16`ê³¼ `bf16` ë°ì´í„° ìœ í˜•ë§Œ ì§€ì›í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\n+\n+## ìë£Œ[[resources]]\n+\n+[ë¼ë§ˆ2](./llama2) ë¬¸ì„œ í˜ì´ì§€ì—ì„œëŠ” ì´ë¯¸ ìˆ˜ ë§ì€ ë©‹ì§€ê³  ìœ ìµí•œ ìë£Œë“¤ì„ ì œê³µí•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ê³³ì— ë¼ë§ˆ3ì— ëŒ€í•œ ìƒˆë¡œìš´ ìë£Œë¥¼ ë”í•´ì£¼ì‹¤ ì»¨íŠ¸ë¦¬ë·°í„°ë“¤ì„ ì´ˆëŒ€í•©ë‹ˆë‹¤! ğŸ¤—"
        }
    ],
    "stats": {
        "total": 82,
        "additions": 82,
        "deletions": 0
    }
}