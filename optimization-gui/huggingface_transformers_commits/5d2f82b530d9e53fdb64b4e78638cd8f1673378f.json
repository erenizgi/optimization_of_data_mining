{
    "author": "avihu111",
    "message": "Fix GraniteMoeHybrid in transformers v5 (#42872)\n\n* apply_rotary_pos_emb should be called\n\n* fix position_embeddings usage in granitemoehybrid\n\n* setting `self.rotary_emb` to None only in hybrid models. Safer, since all modules are highly modular.\n\n* minor\n\n* adding `position_embedding_type` to the config.\n\n* review cleanup\n\n* modeling too\n\n* rewrite conditionally applying rope\n\n* resolve rotary_emb issue",
    "sha": "5d2f82b530d9e53fdb64b4e78638cd8f1673378f",
    "files": [
        {
            "sha": "55961674404b46f144f1d4b4be74b5ecf0022656",
            "filename": "src/transformers/models/granitemoehybrid/configuration_granitemoehybrid.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/5d2f82b530d9e53fdb64b4e78638cd8f1673378f/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fconfiguration_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5d2f82b530d9e53fdb64b4e78638cd8f1673378f/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fconfiguration_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fconfiguration_granitemoehybrid.py?ref=5d2f82b530d9e53fdb64b4e78638cd8f1673378f",
            "patch": "@@ -92,6 +92,8 @@ class GraniteMoeHybridConfig(PreTrainedConfig):\n             allow the model to output the auxiliary loss.\n         router_aux_loss_coef (`float`, *optional*, defaults to 0.001): router auxiliary loss coefficient\n         shared_intermediate_size (`int`, *optional*, defaults to 1024): intermediate size for shared experts.\n+        position_embedding_type (`str`, *optional*):\n+            Positional embedding type to be used; defaults to None. Allowed options: `[None, \"rope\"]`\n         layer_types (`List`, *optional*): list of strings to be used as layer types.\n             Allowed choices: \"mamba\", \"attention\".\n         mamba_n_heads (`int`, *optional*, defaults to 128):\n@@ -159,6 +161,7 @@ def __init__(\n         output_router_logits: Optional[bool] = False,\n         router_aux_loss_coef: Optional[float] = 0.001,\n         shared_intermediate_size: Optional[int] = 1024,\n+        position_embedding_type: Optional[str] = None,\n         layer_types: Optional[list[str]] = None,\n         mamba_n_heads: Optional[int] = 128,\n         mamba_n_groups: Optional[int] = 1,\n@@ -198,6 +201,7 @@ def __init__(\n         self.output_router_logits = output_router_logits\n         self.router_aux_loss_coef = router_aux_loss_coef\n         self.shared_intermediate_size = shared_intermediate_size\n+        self.position_embedding_type = position_embedding_type\n         self.rope_parameters = rope_parameters\n \n         mamba_intermediate = mamba_expand * hidden_size"
        },
        {
            "sha": "d59ebe088b03624201a7d08b2e67effc3650e3a2",
            "filename": "src/transformers/models/granitemoehybrid/modeling_granitemoehybrid.py",
            "status": "modified",
            "additions": 9,
            "deletions": 2,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/5d2f82b530d9e53fdb64b4e78638cd8f1673378f/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5d2f82b530d9e53fdb64b4e78638cd8f1673378f/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py?ref=5d2f82b530d9e53fdb64b4e78638cd8f1673378f",
            "patch": "@@ -165,6 +165,7 @@ def forward(\n         attention_mask: Optional[torch.Tensor],\n         past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # None or rope embeddings\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n         input_shape = hidden_states.shape[:-1]\n@@ -174,6 +175,10 @@ def forward(\n         key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n         value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n \n+        if position_embeddings is not None:\n+            cos, sin = position_embeddings\n+            query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n         if past_key_values is not None:\n             cache_kwargs = {\"cache_position\": cache_position}\n             key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n@@ -1265,7 +1270,7 @@ def __init__(self, config: GraniteMoeHybridConfig):\n             [GraniteMoeHybridDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n         )\n         self.norm = GraniteMoeHybridRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n-        self.rotary_emb = GraniteMoeHybridRotaryEmbedding(config=config)\n+        self.rotary_emb = GraniteMoeHybridRotaryEmbedding(config) if config.position_embedding_type == \"rope\" else None\n         self.gradient_checkpointing = False\n         self.embedding_multiplier = config.embedding_multiplier\n \n@@ -1313,7 +1318,9 @@ def forward(\n \n         # embed positions\n         hidden_states = inputs_embeds\n-        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+        position_embeddings = None\n+        if self.rotary_emb is not None:\n+            position_embeddings = self.rotary_emb(hidden_states, position_ids)\n \n         for decoder_layer in self.layers:\n             # Depending on the layer type we opt for 2D base attention mask (Mamba) or 4D causal mask (Attention)"
        },
        {
            "sha": "a2b9fe97a0e7096c5fb16a925055abcc9227a7d2",
            "filename": "src/transformers/models/granitemoehybrid/modular_granitemoehybrid.py",
            "status": "modified",
            "additions": 10,
            "deletions": 1,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/5d2f82b530d9e53fdb64b4e78638cd8f1673378f/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodular_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5d2f82b530d9e53fdb64b4e78638cd8f1673378f/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodular_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodular_granitemoehybrid.py?ref=5d2f82b530d9e53fdb64b4e78638cd8f1673378f",
            "patch": "@@ -39,6 +39,7 @@\n     GraniteMoeSharedModel,\n     GraniteMoeSharedMoE,\n     GraniteMoeSharedPreTrainedModel,\n+    apply_rotary_pos_emb,\n     eager_attention_forward,\n )\n from .configuration_granitemoehybrid import GraniteMoeHybridConfig\n@@ -57,6 +58,7 @@ def forward(  # FIME: @ARTHUR this forward is also classic: attention nope\n         attention_mask: Optional[torch.Tensor],\n         past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # None or rope embeddings\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n         input_shape = hidden_states.shape[:-1]\n@@ -66,6 +68,10 @@ def forward(  # FIME: @ARTHUR this forward is also classic: attention nope\n         key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n         value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n \n+        if position_embeddings is not None:\n+            cos, sin = position_embeddings\n+            query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n         if past_key_values is not None:\n             cache_kwargs = {\"cache_position\": cache_position}\n             key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n@@ -203,6 +209,7 @@ def __init__(self, config: GraniteMoeHybridConfig):\n             [GraniteMoeHybridDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n         )\n         self.embedding_multiplier = config.embedding_multiplier\n+        self.rotary_emb = GraniteMoeHybridRotaryEmbedding(config) if config.position_embedding_type == \"rope\" else None\n \n     @auto_docstring\n     @check_model_inputs\n@@ -245,7 +252,9 @@ def forward(\n \n         # embed positions\n         hidden_states = inputs_embeds\n-        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+        position_embeddings = None\n+        if self.rotary_emb is not None:\n+            position_embeddings = self.rotary_emb(hidden_states, position_ids)\n \n         for decoder_layer in self.layers:\n             # Depending on the layer type we opt for 2D base attention mask (Mamba) or 4D causal mask (Attention)"
        }
    ],
    "stats": {
        "total": 26,
        "additions": 23,
        "deletions": 3
    }
}