{
    "author": "laurentd-lunit",
    "message": "[fix] LlavaNextProcessor '_get_unpadded_features' method (#33263)\n\n* [fix] LlavaNextProcessor '_get_unpadded_features' method\r\n\r\n* [tests] add test_image_token_filling\r\n\r\n* [chore] style + comment\r\n\r\n* [minor] improve readability\r\n\r\n* [chore] run make fix-copies",
    "sha": "d70347726577e9823e35c11883e98c5b2c520b37",
    "files": [
        {
            "sha": "f84578d1f3466eab19129f7900f183915e8b8ae9",
            "filename": "src/transformers/models/llava_next/processing_llava_next.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d70347726577e9823e35c11883e98c5b2c520b37/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d70347726577e9823e35c11883e98c5b2c520b37/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py?ref=d70347726577e9823e35c11883e98c5b2c520b37",
            "patch": "@@ -199,8 +199,8 @@ def _get_unpadded_features(self, height, width, patches_height, patches_width, s\n         because it divided each image into patches depending on its resolution. Therefore we need to calculate how many\n         patches an image is divided into and get the number of features from that.\n         \"\"\"\n-        current_width = patches_height * scale_height\n-        current_height = patches_width * scale_width\n+        current_height = patches_height * scale_height\n+        current_width = patches_width * scale_width\n \n         original_aspect_ratio = width / height\n         current_aspect_ratio = current_width / current_height"
        },
        {
            "sha": "c8b58ce7982f8bea5d857d8bed658fd996287e1b",
            "filename": "tests/models/llava_next/test_processor_llava_next.py",
            "status": "modified",
            "additions": 28,
            "deletions": 0,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/d70347726577e9823e35c11883e98c5b2c520b37/tests%2Fmodels%2Fllava_next%2Ftest_processor_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d70347726577e9823e35c11883e98c5b2c520b37/tests%2Fmodels%2Fllava_next%2Ftest_processor_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next%2Ftest_processor_llava_next.py?ref=d70347726577e9823e35c11883e98c5b2c520b37",
            "patch": "@@ -13,6 +13,8 @@\n # limitations under the License.\n import unittest\n \n+import torch\n+\n from transformers.testing_utils import require_vision\n from transformers.utils import is_vision_available\n \n@@ -39,3 +41,29 @@ def test_chat_template(self):\n \n         formatted_prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n         self.assertEqual(expected_prompt, formatted_prompt)\n+\n+    def test_image_token_filling(self):\n+        processor = AutoProcessor.from_pretrained(\"llava-hf/llava-v1.6-vicuna-7b-hf\")\n+        processor.patch_size = 14\n+        processor.vision_feature_select_strategy = \"default\"\n+        # Important to check with non square image\n+        image = torch.randint(0, 2, (3, 500, 316))\n+        expected_image_tokens = 1526\n+        image_token_index = 32000\n+\n+        messages = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\"type\": \"image\"},\n+                    {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n+                ],\n+            },\n+        ]\n+        inputs = processor(\n+            text=[processor.apply_chat_template(messages)],\n+            images=[image],\n+            return_tensors=\"pt\",\n+        )\n+        image_tokens = (inputs[\"input_ids\"] == image_token_index).sum().item()\n+        self.assertEqual(expected_image_tokens, image_tokens)"
        }
    ],
    "stats": {
        "total": 32,
        "additions": 30,
        "deletions": 2
    }
}