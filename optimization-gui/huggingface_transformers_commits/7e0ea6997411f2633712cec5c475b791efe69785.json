{
    "author": "stas00",
    "message": "HF Trainer: ALST/Ulysses sequence parallelism integration via HF Accelerate (#41832)\n\n* HF Trainer: ALST/Ulysses sequence parallelism integration via HF Accelerate\n\nSigned-off-by: Stas Bekman <stas.bekman@snowflake.com>\n\n* make it work + tests\n\nSigned-off-by: Stas Bekman <stas.bekman@snowflake.com>\n\n* cleanup\n\nSigned-off-by: Stas Bekman <stas.bekman@snowflake.com>\n\n* undo\n\nSigned-off-by: Stas Bekman <stas.bekman@snowflake.com>\n\n* normalize\n\nSigned-off-by: Stas Bekman <stas.bekman@snowflake.com>\n\n* always return cp_size\n\nSigned-off-by: Stas Bekman <stas.bekman@snowflake.com>\n\n* cleanup\n\nSigned-off-by: Stas Bekman <stas.bekman@snowflake.com>\n\n* extract code into _deepspeed_cp_compute_loss\n\nSigned-off-by: Stas Bekman <stas.bekman@snowflake.com>\n\n* fix\n\nSigned-off-by: Stas Bekman <stas.bekman@snowflake.com>\n\n* ALST/Ulysses sequence parallelism docs\n\n* typo\n\n* add link to UlyssesSPDataLoaderAdapter\n\n* adapt to renaming to SP\n\nSigned-off-by: Stas Bekman <stas.bekman@snowflake.com>\n\n* improve\n\nSigned-off-by: Stas Bekman <stas.bekman@snowflake.com>\n\n* fix\n\nSigned-off-by: Stas Bekman <stas.bekman@snowflake.com>\n\n* Update docs/source/en/deepspeed.md\n\nCo-authored-by: Kashif Rasul <kashif.rasul@gmail.com>\n\n* address comments\n\nSigned-off-by: Stas Bekman <stas.bekman@snowflake.com>\n\n* address comments\n\nSigned-off-by: Stas Bekman <stas.bekman@snowflake.com>\n\n* Update src/transformers/trainer.py\n\nCo-authored-by: Kashif Rasul <kashif.rasul@gmail.com>\n\n* address comments\n\nSigned-off-by: Stas Bekman <stas.bekman@snowflake.com>\n\n* address comments\n\nSigned-off-by: Stas Bekman <stas.bekman@snowflake.com>\n\n* Update src/transformers/trainer.py\n\nCo-authored-by: Kashif Rasul <kashif.rasul@gmail.com>\n\n* Update src/transformers/trainer.py\n\nCo-authored-by: Kashif Rasul <kashif.rasul@gmail.com>\n\n* style\n\nSigned-off-by: Stas Bekman <stas.bekman@snowflake.com>\n\n* Update docs/source/en/deepspeed.md\n\nCo-authored-by: Kashif Rasul <kashif.rasul@gmail.com>\n\n* Update docs/source/en/deepspeed.md\n\nCo-authored-by: Kashif Rasul <kashif.rasul@gmail.com>\n\n* Account for Sequence Parallelism (SP) dataloader adapter effect\n\n* Update src/transformers/trainer.py\n\nCo-authored-by: Kashif Rasul <kashif.rasul@gmail.com>\n\n* Update docs/source/en/deepspeed.md\n\nCo-authored-by: Kashif Rasul <kashif.rasul@gmail.com>\n\n* Update docs/source/en/deepspeed.md\n\nCo-authored-by: Kashif Rasul <kashif.rasul@gmail.com>\n\n* model_accepts_loss_kwargs to False\n\n* better comment\n\n* Apply suggestion from @kashif\n\n* Apply suggestion from @kashif\n\n* Apply suggestions from code review\n\n* Apply suggestion from @kashif\n\n* Apply suggestion from @kashif\n\n* Apply suggestion from @kashif\n\n* Update src/transformers/trainer.py\n\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\n\n* Update src/transformers/training_args.py\n\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\n\n* Apply suggestion from @kashif\n\n* Apply suggestion from @kashif\n\n---------\n\nSigned-off-by: Stas Bekman <stas.bekman@snowflake.com>\nCo-authored-by: Stas Bekman <stas.bekman@snowflake.com>\nCo-authored-by: Kashif Rasul <kashif.rasul@gmail.com>\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>",
    "sha": "7e0ea6997411f2633712cec5c475b791efe69785",
    "files": [
        {
            "sha": "bee280fdc3282434eaf328f1bfbdc4b851347418",
            "filename": "docs/source/en/deepspeed.md",
            "status": "modified",
            "additions": 102,
            "deletions": 0,
            "changes": 102,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e0ea6997411f2633712cec5c475b791efe69785/docs%2Fsource%2Fen%2Fdeepspeed.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e0ea6997411f2633712cec5c475b791efe69785/docs%2Fsource%2Fen%2Fdeepspeed.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fdeepspeed.md?ref=7e0ea6997411f2633712cec5c475b791efe69785",
            "patch": "@@ -368,6 +368,108 @@ The example ZeRO-3 and ZeRO-Infinity config below sets most of the parameter val\n }\n ```\n \n+### Sequence Parallelism\n+\n+DeepSpeed's ALST/Ulysses sequence parallelism enables training with very long sequences by splitting the sequence across multiple GPUs. This is particularly useful for training large language models with very long sequence lengths.\n+\n+Arctic Long Sequence Training (ALST) uses a combination of sharding inputs along the sequence dimension and attention head parallelism. With this approach, you can train models with sequence lengths up to 500K tokens on a single H100 GPU, 3.7M on a single node, or 15M tokens on just four nodes with Llama-8B. The implementation described here enables one component of the full ALST system. For additional optimizations like TiledMLP and activation checkpoint offloading, refer to the [DeepSpeed ALST tutorial](https://www.deepspeed.ai/tutorials/ulysses-alst-sequence-parallelism/).\n+\n+> [!TIP]\n+> For more detailed information about sequence parallelism, see the Accelerate [Sequence Parallelism](https://huggingface.co/docs/accelerate/concept_guides/sequence_parallelism) guide.\n+\n+To enable ALST/Ulysses sequence parallelism with [`Trainer`], configure `parallelism_config` in [`TrainingArguments`]. Sequence parallelism is configured via Accelerate's `ParallelismConfig` and requires an Accelerate version higher than 1.12.0.\n+\n+```py\n+from accelerate.utils import ParallelismConfig, DeepSpeedSequenceParallelConfig\n+\n+# Example: 4 GPUs with sp_size=4, dp_replicate_size=1 (no data parallelism)\n+# Ensure total_size = dp_replicate_size * dp_shard_size * sp_size = 1 * 1 * 4 = 4 GPUs\n+parallelism_config = ParallelismConfig(\n+    sp_backend=\"deepspeed\",\n+    sp_size=4,  # Number of GPUs to split sequence across\n+    dp_replicate_size=1,  # Explicit: no data parallelism\n+    sp_handler=DeepSpeedSequenceParallelConfig(\n+        sp_seq_length_is_variable=True,\n+        sp_attn_implementation=\"sdpa\",\n+    ),\n+)\n+\n+training_args = TrainingArguments(\n+    ...,\n+    deepspeed=\"path/to/deepspeed_config.json\",\n+    parallelism_config=parallelism_config,\n+)\n+```\n+\n+You can also configure sequence parallelism using an Accelerate config file.\n+\n+```yaml\n+distributed_type: DEEPSPEED\n+deepspeed_config:\n+  deepspeed_config_file: path/to/ds_config.json\n+machine_rank: 0\n+num_machines: 1\n+num_processes: 4  # Total number of processes\n+parallelism_config:\n+  parallelism_config_sp_size: 4  # Sequence parallel size\n+  parallelism_config_dp_replicate_size: 1  # Must be: dp_replicate_size * dp_shard_size * sp_size = num_processes\n+  parallelism_config_sp_backend: deepspeed\n+  parallelism_config_sp_seq_length_is_variable: true\n+  parallelism_config_sp_attn_implementation: sdpa\n+```\n+\n+Important configuration parameters include the following.\n+\n+* `sp_backend` must be set to `\"deepspeed\"` to use ALST/Ulysses sequence parallelism.\n+* `sp_size` is the degree of sequence parallelism. For example, `sp_size=4` means 4 GPUs will process a single sequence in parallel. You need at least 2 GPUs to enable sequence parallelism. **Data feeding**: Each rank receives a unique data stream from the DataLoader (like DP). **Batch size calculation**: The effective `dp_world_size = world_size / sp_size`. So with 4 GPUs and `sp_size=4`, each of the 4 ranks gets different samples from the DataLoader, but `dp_world_size=1` for total batch size calculations\n+* `sp_seq_length_is_variable` determines how sequence lengths are handled. When set to `True` (recommended), the implementation adapts to varying sequence lengths between batches. When `False`, all sequences must be padded to a fixed length specified by `sp_seq_length`.\n+* `sp_attn_implementation` specifies the attention implementation to use. Supported values are `\"sdpa\"`, `\"flash_attention_2\"`, or `\"flash_attention_3\"`. Flash Attention is recommended for best performance, especially with multiple samples in a batch, because SDPA may incorrectly attend across sample boundaries.\n+\n+> [!WARNING]\n+> Sequence parallelism requires your model to use one of the supported attention implementations (`sdpa`, `flash_attention_2`, or `flash_attention_3`). The `eager` attention implementation is not supported because it doesn't properly handle `position_ids`.\n+\n+When using sequence parallelism, ensure your sequences are properly padded. Use `pad_to_multiple_of` in your data collator to ensure sequences are divisible by `sp_size`. For example, with `sp_size=4`, set `pad_to_multiple_of=4` or higher.\n+\n+```py\n+from transformers import DataCollatorForLanguageModeling\n+\n+data_collator = DataCollatorForLanguageModeling(\n+    tokenizer=tokenizer,\n+    mlm=False,\n+    pad_to_multiple_of=4,  # Ensure sequences are divisible by sp_size\n+)\n+```\n+\n+When using `sp_size` with multiple GPUs, you **must** explicitly set `dp_replicate_size` or `dp_shard_size` to ensure `total_size = dp_replicate_size * dp_shard_size * sp_size` equals your total number of GPUs. For example, with 8 GPUs and `sp_size=4`, you must set `dp_replicate_size=2` (since 2 × 1 × 4 = 8):\n+\n+```py\n+parallelism_config = ParallelismConfig(\n+    sp_backend=\"deepspeed\",\n+    sp_size=4,\n+    dp_replicate_size=2,\n+    sp_handler=DeepSpeedSequenceParallelConfig(\n+        sp_seq_length_is_variable=True,\n+        sp_attn_implementation=\"flash_attention_2\",\n+    ),\n+)\n+```\n+\n+[`Trainer`] automatically handles the special requirements for sequence parallelism including:\n+\n+* Adapting the data loader via DeepSpeed's [`UlyssesSPDataLoaderAdapter`](https://github.com/deepspeedai/DeepSpeed/blob/master/deepspeed/runtime/sequence_parallel/ulysses_sp.py) to shard sequences across GPUs. **Important**: Unlike Tensor Parallelism (where all ranks must receive identical data), each rank with SP receives a unique data stream from the DataLoader (similar to DP). The adapter handles distributing sequence chunks across SP ranks internally, so your DataLoader should continue feeding different samples to each rank.\n+* Generating `position_ids` when not provided\n+* Creating `shift_labels` for causal language modeling\n+* Aggregating loss across sequence parallel ranks with proper masking for `-100` labels\n+\n+You can launch training with sequence parallelism using the `accelerate launch` command.\n+\n+```bash\n+accelerate launch --config_file alst_config.yaml your_training_script.py \\\n+--output_dir output_dir \\\n+--per_device_train_batch_size 1 \\\n+--gradient_accumulation_steps 1\n+```\n+\n ## Training features\n \n DeepSpeed supports many training features that can be configured in the config file. This section describes some of the most important features."
        },
        {
            "sha": "9e5263eed1d2581b1685a698bcd8910b53b6c00a",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 15,
            "deletions": 4,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e0ea6997411f2633712cec5c475b791efe69785/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e0ea6997411f2633712cec5c475b791efe69785/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=7e0ea6997411f2633712cec5c475b791efe69785",
            "patch": "@@ -21,6 +21,7 @@\n import gc\n import importlib\n import inspect\n+import json\n import logging\n import multiprocessing\n import os\n@@ -2005,14 +2006,12 @@ def get_env(self):\n         paths = [self.repo_root_dir_str, self.src_dir_str]\n         if \"/examples\" in self.test_file_dir_str:\n             paths.append(self.examples_dir_str)\n-        else:\n-            paths.append(self.tests_dir_str)\n         paths.append(env.get(\"PYTHONPATH\", \"\"))\n \n         env[\"PYTHONPATH\"] = \":\".join(paths)\n         return env\n \n-    def get_auto_remove_tmp_dir(self, tmp_dir=None, before=None, after=None):\n+    def get_auto_remove_tmp_dir(self, tmp_dir=None, before=None, after=None, return_pathlib_obj=False):\n         \"\"\"\n         Args:\n             tmp_dir (`string`, *optional*):\n@@ -2032,6 +2031,8 @@ def get_auto_remove_tmp_dir(self, tmp_dir=None, before=None, after=None):\n             after (`bool`, *optional*):\n                 If `True`, delete the `tmp_dir` at the end of the test if `False`, leave the `tmp_dir` and its contents\n                 intact at the end of the test.\n+            return_pathlib_obj (`bool`, *optional*):\n+                If `True` will return a pathlib.Path object\n \n         Returns:\n             tmp_dir(`string`): either the same value as passed via *tmp_dir* or the path to the auto-selected tmp dir\n@@ -2078,7 +2079,7 @@ def get_auto_remove_tmp_dir(self, tmp_dir=None, before=None, after=None):\n             # register for deletion\n             self.teardown_tmp_dirs.append(tmp_dir)\n \n-        return tmp_dir\n+        return Path(tmp_dir).resolve() if return_pathlib_obj else tmp_dir\n \n     def python_one_liner_max_rss(self, one_liner_str):\n         \"\"\"\n@@ -4076,3 +4077,13 @@ def use_one_line_repr(obj):\n     cache[(id(obj), indent, mode, prefix)] = output\n \n     return output\n+\n+\n+def write_file(file, content):\n+    with open(file, \"w\") as f:\n+        f.write(content)\n+\n+\n+def read_json_file(file):\n+    with open(file, \"r\") as fh:\n+        return json.load(fh)"
        },
        {
            "sha": "c97c2bb7846b09a4b8c996ca2b774dece2ef60ef",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 121,
            "deletions": 16,
            "changes": 137,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e0ea6997411f2633712cec5c475b791efe69785/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e0ea6997411f2633712cec5c475b791efe69785/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=7e0ea6997411f2633712cec5c475b791efe69785",
            "patch": "@@ -603,6 +603,11 @@ def __init__(\n                 k.kind == inspect.Parameter.VAR_KEYWORD for k in forward_params.values()\n             )\n \n+        # Override for Sequence Parallelism: SP computes its own good_tokens count, so skip num_items_in_batch calculation\n+        pc = getattr(self.accelerator, \"parallelism_config\", None)\n+        if pc is not None and pc.sp_backend == \"deepspeed\" and pc.sp_enabled:\n+            self.model_accepts_loss_kwargs = False\n+\n         self.neftune_noise_alpha = args.neftune_noise_alpha\n \n         self.compute_metrics = compute_metrics\n@@ -2163,6 +2168,22 @@ def train(\n                 ignore_keys_for_eval=ignore_keys_for_eval,\n             )\n \n+    def get_sp_size(self) -> int:\n+        \"\"\"Get the sequence parallel size\"\"\"\n+        if getattr(self.accelerator, \"parallelism_config\", None) is None:\n+            return 1\n+        else:\n+            pc = self.accelerator.parallelism_config\n+            return pc.sp_size\n+\n+    def get_cp_size(self) -> int:\n+        \"\"\"Get the context parallel size\"\"\"\n+        if getattr(self.accelerator, \"parallelism_config\", None) is None:\n+            return 1\n+        else:\n+            pc = self.accelerator.parallelism_config\n+            return pc.cp_size\n+\n     def get_tp_size(self) -> int:\n         \"\"\"Get the tensor parallel size from either the model or DeepSpeed config.\"\"\"\n \n@@ -2180,8 +2201,19 @@ def get_tp_size(self) -> int:\n     def get_total_train_batch_size(self, args) -> int:\n         \"\"\"Calculates total batch size (micro_batch * grad_accum * dp_world_size).\n \n-        Note: Only considers DP and TP (dp_world_size = world_size // tp_size).\"\"\"\n-        dp_world_size = args.world_size // self.get_tp_size()\n+        Accounts for all parallelism dimensions: TP, CP, and SP.\n+\n+        Formula: dp_world_size = world_size // (tp_size * cp_size * sp_size)\n+\n+        Where:\n+        - TP (Tensor Parallelism): Model layers split across GPUs\n+        - CP (Context Parallelism): Sequences split using Ring Attention (FSDP2)\n+        - SP (Sequence Parallelism): Sequences split using ALST/Ulysses (DeepSpeed)\n+\n+        All dimensions are separate and multiplicative: world_size = dp_size * tp_size * cp_size * sp_size\n+        \"\"\"\n+\n+        dp_world_size = args.world_size // self.get_tp_size() // self.get_cp_size() // self.get_sp_size()\n         return self._train_batch_size * args.gradient_accumulation_steps * dp_world_size\n \n     def _inner_training_loop(\n@@ -2305,6 +2337,11 @@ def _inner_training_loop(\n         else:\n             self.optimizer = self.accelerator.prepare(self.optimizer)\n \n+        # since DataLoader was Accelerate prepared w/o a model arg in the same call, we now have to complete the DL wrapping for ALST/UlyssesSP, after model has been prepared\n+        pc = getattr(self.accelerator, \"parallelism_config\", None)\n+        if pc is not None and pc.sp_backend == \"deepspeed\" and pc.sp_enabled:\n+            train_dataloader = self.accelerator.deepspeed_ulysses_dl_adapter(train_dataloader, model)\n+\n         if self.is_fsdp_enabled:\n             self.model = self.model_wrapped = model\n \n@@ -3639,23 +3676,30 @@ def _prepare_context_parallel_inputs(self, model, inputs: dict[str, Union[torch.\n             getattr(self.accelerator, \"parallelism_config\", None) is not None\n             and self.accelerator.parallelism_config.cp_enabled\n         ):\n-            if hasattr(model, \"config\"):\n-                if model.config._attn_implementation != \"sdpa\":\n-                    raise ValueError(\n-                        f\"Context parallelism is supported only with SDPA attention, you are using {model.config._attn_implementation}.\"\n-                    )\n+            if self.accelerator.parallelism_config.cp_backend == \"torch\":\n+                if hasattr(model, \"config\"):\n+                    if model.config._attn_implementation != \"sdpa\":\n+                        raise ValueError(\n+                            f\"Context parallelism is supported only with SDPA attention, you are using {model.config._attn_implementation}.\"\n+                        )\n+\n+                if \"shift_labels\" not in inputs:\n+                    logger.warning_once(\"Shift labels not found in the inputs, shifting manually\")\n+                    if \"labels\" in inputs:\n+                        _ignore_index = -100\n+                        labels = nn.functional.pad(inputs[\"labels\"], (0, 1), value=_ignore_index)\n+                        inputs[\"shift_labels\"] = labels[:, 1:].contiguous()\n+\n+            # note: we don't do anything for accelerator.parallelism_config.sp_backend == \"deepspeed\" since:\n+            # - accelerator.parallelism_config performs the `model.config._attn_implementation` checks already and it supports more than `dspa`\n+            # - UlyssesSPDataLoaderAdapter called from Accelerate performs the `shift_label` creation - must not interfere\n+            # - position_ids generation should be done by HF Trainer if it wasn't done by the user\n \n             if \"position_ids\" not in inputs:\n                 logger.warning_once(\"Position IDs not found in the inputs, generating manually\")\n                 inputs[\"position_ids\"] = torch.arange(\n                     inputs[\"input_ids\"].size(1), device=inputs[\"input_ids\"].device\n                 ).expand(inputs[\"input_ids\"].size(0), -1)\n-            if \"shift_labels\" not in inputs:\n-                logger.warning_once(\"Shift labels not found in the inputs, shifting manually\")\n-                if \"labels\" in inputs:\n-                    _ignore_index = -100\n-                    labels = nn.functional.pad(inputs[\"labels\"], (0, 1), value=_ignore_index)\n-                    inputs[\"shift_labels\"] = labels[:, 1:].contiguous()\n \n             buffers = []\n             buffer_seq_dims = []\n@@ -3824,6 +3868,10 @@ def compute_loss(\n         Subclass and override for custom behavior. If you are not using `num_items_in_batch` when computing your loss,\n         make sure to overwrite `self.model_accepts_loss_kwargs` to `False`. Otherwise, the loss calculating might be slightly inaccurate when performing gradient accumulation.\n         \"\"\"\n+        pc = getattr(self.accelerator, \"parallelism_config\", None)\n+        if pc is not None and pc.sp_backend == \"deepspeed\" and pc.sp_enabled:\n+            return self._deepspeed_sp_compute_loss(model, inputs, return_outputs, pc)\n+\n         if (self.label_smoother is not None or self.compute_loss_func is not None) and \"labels\" in inputs:\n             labels = inputs.pop(\"labels\")\n         else:\n@@ -3877,6 +3925,55 @@ def compute_loss(\n \n         return (loss, outputs) if return_outputs else loss\n \n+    def _deepspeed_sp_compute_loss(self, model, inputs, return_outputs, pc):\n+        \"\"\"\n+        How the loss is computed by Trainer under sequence parallelism with sp_backend==\"deepspeed\" and sp_size>1.\n+        Performs weighted loss aggregation across SP ranks, accounting for varying numbers of valid tokens per rank\n+        (e.g., when some ranks receive only padding or prompt tokens that are masked with -100).\n+\n+        Args:\n+            model (`nn.Module`):\n+                The model to compute the loss for.\n+            inputs (`dict[str, Union[torch.Tensor, Any]]`):\n+                The input data for the model. Must include \"shift_labels\" key.\n+            return_outputs (`bool`, *optional*, defaults to `False`):\n+                Whether to return the model outputs along with the loss.\n+            pc (`accelerate.parallelism_config.ParallelismConfig`):\n+                self.accelerator.parallelism_config object (not None)\n+\n+        Returns:\n+            The loss of the model along with its output if return_outputs was set to True\n+        \"\"\"\n+\n+        unwrapped_model = self.accelerator.unwrap_model(model)\n+\n+        outputs = model(**inputs)\n+        shift_labels = inputs[\"shift_labels\"]\n+        loss = unwrapped_model.loss_function(\n+            logits=outputs.logits,\n+            labels=None,\n+            shift_labels=shift_labels,\n+            vocab_size=unwrapped_model.config.vocab_size,\n+        )\n+\n+        sp_group = self.accelerator.torch_device_mesh[\"sp\"].get_group()\n+        sp_world_size = pc.sp_size\n+        # differentiable weighted per-shard-loss aggregation across ranks\n+        losses_per_rank = torch.distributed.nn.functional.all_gather(loss, group=sp_group)\n+        # special dealing with SFT that has prompt tokens that aren't used in loss computation\n+        good_tokens = (shift_labels != -100).view(-1).sum()\n+        good_tokens_per_rank = torch.distributed.nn.functional.all_gather(good_tokens, group=sp_group)\n+        # Skip ranks with zero valid tokens\n+        total_loss = sum(\n+            losses_per_rank[rank] * good_tokens_per_rank[rank]\n+            for rank in range(sp_world_size)\n+            if good_tokens_per_rank[rank] > 0\n+        )\n+        total_good_tokens = sum(good_tokens_per_rank)\n+        loss = total_loss / max(total_good_tokens, 1)\n+\n+        return (loss, outputs) if return_outputs else loss\n+\n     def is_local_process_zero(self) -> bool:\n         \"\"\"\n         Whether or not this process is the local (e.g., on one machine if training in a distributed fashion on several\n@@ -3917,7 +4014,9 @@ def save_model(self, output_dir: Optional[str] = None, _internal_call: bool = Fa\n             Path(os.path.join(output_dir, \"user_content.pt\")).touch()\n         # We are in N-D parallelism if we have parallelism_config set, so we check accelerate if we're on a to_save rank\n         elif getattr(self.accelerator, \"parallelism_config\", None) is not None:\n-            if self.accelerator.should_save_model:\n+            # DeepSpeed SP already handles checkpoint saving below, so skip manual save in that case\n+            pc = getattr(self.accelerator, \"parallelism_config\")\n+            if self.accelerator.should_save_model and not (pc.sp_enabled and pc.sp_backend == \"deepspeed\"):\n                 self._save(output_dir)\n         # If we drop to here, we're in 1D parallelism, so all ranks need to go to `save_pretrained`\n         elif (tp_size := getattr(self.model, \"_tp_size\", 0)) is not None and tp_size > 1:\n@@ -4986,9 +5085,10 @@ def create_accelerator_and_postprocess(self):\n \n         # We defer compatibility checks to accelerator\n         if self.args.parallelism_config is not None:\n-            if not is_accelerate_available(\"1.10.1\"):\n+            min_accelerate_version = \"1.12.0\"\n+            if not is_accelerate_available(min_accelerate_version):\n                 raise ImportError(\n-                    \"ParallelismConfig requires accelerate v1.10.1 and above. Please upgrade accelerate to use this feature.\"\n+                    f\"ParallelismConfig requires accelerate>={min_accelerate_version}). Please upgrade accelerate to use this feature.\"\n                 )\n             args[\"parallelism_config\"] = self.args.parallelism_config\n \n@@ -5182,6 +5282,11 @@ def set_initial_training_values(\n         epoch_based = max_steps < 0\n         len_dataloader = len(dataloader) if has_length(dataloader) else None\n \n+        # Account for Sequence Parallelism (SP) dataloader adapter's effect\n+        sp_size = self.get_sp_size()\n+        if sp_size > 1 and len_dataloader is not None:\n+            len_dataloader = len_dataloader * sp_size\n+\n         # Case 2: We have a dataloader length and can extrapolate\n         if len_dataloader is not None:\n             num_update_steps_per_epoch = max("
        },
        {
            "sha": "8c7f1a5573c3a6f2cc5881f944746bc987fa0a5b",
            "filename": "src/transformers/training_args.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e0ea6997411f2633712cec5c475b791efe69785/src%2Ftransformers%2Ftraining_args.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e0ea6997411f2633712cec5c475b791efe69785/src%2Ftransformers%2Ftraining_args.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftraining_args.py?ref=7e0ea6997411f2633712cec5c475b791efe69785",
            "patch": "@@ -1139,7 +1139,7 @@ class TrainingArguments:\n     )\n     parallelism_config: Optional[ParallelismConfig] = field(\n         default=None,\n-        metadata={\"help\": (\"Parallelism configuration for the training run. Requires Accelerate `1.10.1`\")},\n+        metadata={\"help\": (\"Parallelism configuration for the training run. Requires Accelerate `1.12.0`\")},\n     )\n     deepspeed: Optional[Union[dict, str]] = field(\n         default=None,"
        },
        {
            "sha": "2f24325f3ab4be8c3b05730b810dba0a3960b405",
            "filename": "tests/deepspeed/test_alst_ulysses_sp.py",
            "status": "added",
            "additions": 214,
            "deletions": 0,
            "changes": 214,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e0ea6997411f2633712cec5c475b791efe69785/tests%2Fdeepspeed%2Ftest_alst_ulysses_sp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e0ea6997411f2633712cec5c475b791efe69785/tests%2Fdeepspeed%2Ftest_alst_ulysses_sp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fdeepspeed%2Ftest_alst_ulysses_sp.py?ref=7e0ea6997411f2633712cec5c475b791efe69785",
            "patch": "@@ -0,0 +1,214 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import json\n+import sys\n+\n+from transformers import is_torch_available\n+from transformers.testing_utils import (\n+    TestCasePlus,\n+    execute_subprocess_async,\n+    read_json_file,\n+    require_accelerate,\n+    require_torch_multi_accelerator,\n+    slow,\n+    write_file,\n+)\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+    from transformers import (\n+        AutoModelForCausalLM,\n+        AutoTokenizer,\n+        DataCollatorForLanguageModeling,\n+        HfArgumentParser,\n+        Trainer,\n+        TrainingArguments,\n+    )\n+\n+\n+class TestTrainerALSTUlyssesSP(TestCasePlus):\n+    \"\"\"Test Trainer with ALST/Ulysses sequence parallelism enabled via accelerate's ParallelismConfig.\"\"\"\n+\n+    @require_torch_multi_accelerator\n+    @require_accelerate\n+    @slow\n+    def test_sp_equivalence(self):\n+        \"\"\"Test that ALST/Ulysses sequence parallelism produces the same losses as without it.\"\"\"\n+\n+        # shared setup\n+        world_size = 2\n+        script_path = __file__  # self.test_file_dir} / \"test_alst_ulysses_sp.py\"\n+        ds_config_path = self.test_file_dir / \"ds_config_zero2.json\"\n+\n+        # step 1. Run with SP enabled (sp_size=world_size)\n+        sp_yes_output_dir = self.get_auto_remove_tmp_dir(return_pathlib_obj=True)\n+        sp_yes_accelerate_config_path = sp_yes_output_dir / \"context_parallel_config.yaml\"\n+        sp_yes_losses_path = sp_yes_output_dir / \"sp_yes_losses.json\"\n+        write_file(\n+            sp_yes_accelerate_config_path,\n+            f\"\"\"\n+distributed_type: DEEPSPEED\n+deepspeed_config:\n+  deepspeed_config_file: {ds_config_path}\n+machine_rank: 0\n+num_machines: 1\n+num_processes: {world_size}\n+parallelism_config:\n+  parallelism_config_sp_size: {world_size}\n+  parallelism_config_sp_backend: deepspeed\n+  parallelism_config_sp_seq_length_is_variable: true\n+  parallelism_config_sp_attn_implementation: sdpa\n+                   \"\"\",\n+        )\n+\n+        cmd_sp = f\"\"\"\n+            accelerate launch\n+            --config_file {sp_yes_accelerate_config_path}\n+            {script_path}\n+            --output_dir {sp_yes_output_dir}\n+            --report_to none\n+            --max_steps 10\n+            --per_device_train_batch_size 1\n+            --gradient_accumulation_steps 1\n+            --logging_steps 1\n+            --remove_unused_columns False\n+            --seed 42\n+            --loss_output_file {sp_yes_losses_path}\n+        \"\"\".split()\n+\n+        execute_subprocess_async(cmd_sp, env=self.get_env())\n+\n+        # step 2. Run without SP enabled (sp_size=world_size)\n+        sp_no_output_dir = self.get_auto_remove_tmp_dir(return_pathlib_obj=True)\n+        sp_no_accelerate_config_path = sp_no_output_dir / \"context_parallel_config.yaml\"\n+        sp_no_losses_path = sp_no_output_dir / \"sp_yes_losses.json\"\n+        write_file(\n+            sp_no_accelerate_config_path,\n+            f\"\"\"\n+distributed_type: DEEPSPEED\n+deepspeed_config:\n+  deepspeed_config_file: {ds_config_path}\n+machine_rank: 0\n+num_machines: 1\n+num_processes: {world_size}\n+                   \"\"\",\n+        )\n+\n+        cmd_sp = f\"\"\"\n+            accelerate launch\n+            --config_file {sp_no_accelerate_config_path}\n+            {script_path}\n+            --output_dir {sp_no_output_dir}\n+            --report_to none\n+            --max_steps 10\n+            --per_device_train_batch_size 1\n+            --gradient_accumulation_steps 1\n+            --logging_steps 1\n+            --remove_unused_columns False\n+            --seed 42\n+            --loss_output_file {sp_no_losses_path}\n+        \"\"\".split()\n+\n+        execute_subprocess_async(cmd_sp, env=self.get_env())\n+\n+        # Compare losses - should be very close since SP just splits sequence computation\n+        sp_yes_losses = read_json_file(sp_yes_losses_path)\n+        sp_no_losses = read_json_file(sp_no_losses_path)\n+\n+        assert len(sp_yes_losses) == len(sp_no_losses), (\n+            f\"Different number of losses: SP has {len(sp_yes_losses)}, no-SP has {len(sp_no_losses)}\"\n+        )\n+\n+        # ALST/UlyssesSP should produce very similar results (small numerical differences expected)\n+        # The differences come from:\n+        # - Different gradient reduction patterns in distributed training\n+        # - BF16 mixed precision accumulated differences\n+        sp_yes_losses_tensor = torch.tensor(sp_yes_losses)\n+        sp_no_losses_tensor = torch.tensor(sp_no_losses)\n+        torch.testing.assert_close(\n+            sp_yes_losses_tensor,\n+            sp_no_losses_tensor,\n+            atol=2e-2,\n+            rtol=2e-5,\n+            msg=f\"SP-enabled losses {sp_yes_losses} do not match SP-disabled losses {sp_no_losses}\",\n+        )\n+\n+\n+if __name__ == \"__main__\":\n+    model_name = \"hf-internal-testing/tiny-random-LlamaForCausalLM\"\n+\n+    # Parse custom arguments (not TrainingArguments parameters)\n+    loss_output_file = None\n+\n+    if \"--loss_output_file\" in sys.argv:\n+        idx = sys.argv.index(\"--loss_output_file\")\n+        loss_output_file = sys.argv[idx + 1]\n+        sys.argv.pop(idx)\n+        sys.argv.pop(idx)\n+\n+    parser = HfArgumentParser((TrainingArguments,))\n+    training_args = parser.parse_args_into_dataclasses()[0]\n+\n+    tokenizer = AutoTokenizer.from_pretrained(model_name)\n+    if tokenizer.pad_token is None:\n+        tokenizer.pad_token = tokenizer.eos_token\n+\n+    model = AutoModelForCausalLM.from_pretrained(\n+        model_name,\n+        attn_implementation=\"sdpa\",  # SP requires SDPA or FA\n+    )\n+    # fix the outdated testing model config\n+    model.generation_config.pad_token_id = 1\n+\n+    # Create simple dataset: just tokenize some text\n+    texts = [\n+        \"The quick brown fox jumps over the lazy dog. \" * 10,\n+        \"Hello world, this is a test sentence for training. \" * 10,\n+    ] * 4  # 8 samples total\n+\n+    def tokenize_function(examples):\n+        return tokenizer(examples, max_length=128, truncation=True, padding=\"max_length\")\n+\n+    train_dataset = [tokenize_function(text) for text in texts]\n+\n+    # Use standard DataCollatorForLanguageModeling for causal LM\n+    # pad_to_multiple_of=4 ensures sequences are divisible by sp_size * 2 (for sp_size=2)\n+    # Trainer will automatically generate position_ids and shift_labels as needed\n+    data_collator = DataCollatorForLanguageModeling(\n+        tokenizer=tokenizer,\n+        mlm=False,  # Causal language modeling\n+        pad_to_multiple_of=4,\n+    )\n+\n+    trainer = Trainer(\n+        model=model,\n+        args=training_args,\n+        train_dataset=train_dataset,\n+        data_collator=data_collator,\n+    )\n+\n+    # Train for a few steps\n+    trainer.train()\n+\n+    # Verify training completed\n+    assert trainer.state.global_step > 0, \"Training should have completed at least one step\"\n+\n+    # Save losses to file if requested (for equivalence testing)\n+    if loss_output_file and training_args.process_index == 0:\n+        losses = [log[\"loss\"] for log in trainer.state.log_history if \"loss\" in log]\n+        with open(loss_output_file, \"w\") as f:\n+            json.dump(losses, f)"
        }
    ],
    "stats": {
        "total": 474,
        "additions": 453,
        "deletions": 21
    }
}