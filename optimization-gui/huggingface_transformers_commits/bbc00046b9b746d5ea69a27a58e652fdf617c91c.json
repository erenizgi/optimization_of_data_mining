{
    "author": "ydshieh",
    "message": "Fix flaky `test_custom_4d_attention_mask` (#35606)\n\n* fix\r\n\r\n* fix\r\n\r\n---------\r\n\r\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "bbc00046b9b746d5ea69a27a58e652fdf617c91c",
    "files": [
        {
            "sha": "89587d303eba429c49211acf7a37947329b34027",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 8,
            "deletions": 2,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/bbc00046b9b746d5ea69a27a58e652fdf617c91c/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bbc00046b9b746d5ea69a27a58e652fdf617c91c/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=bbc00046b9b746d5ea69a27a58e652fdf617c91c",
            "patch": "@@ -1431,14 +1431,20 @@ def set_model_tester_for_less_flaky_test(test_case):\n         and target_num_hidden_layers is not None\n     ):\n         test_case.model_tester.vision_config = copy.deepcopy(test_case.model_tester.vision_config)\n-        test_case.model_tester.vision_config[\"num_hidden_layers\"] = target_num_hidden_layers\n+        if isinstance(test_case.model_tester.vision_config, dict):\n+            test_case.model_tester.vision_config[\"num_hidden_layers\"] = 1\n+        else:\n+            test_case.model_tester.vision_config.num_hidden_layers = 1\n     if (\n         hasattr(test_case.model_tester, \"text_config\")\n         and \"num_hidden_layers\" in test_case.model_tester.text_config\n         and target_num_hidden_layers is not None\n     ):\n         test_case.model_tester.text_config = copy.deepcopy(test_case.model_tester.text_config)\n-        test_case.model_tester.text_config[\"num_hidden_layers\"] = target_num_hidden_layers\n+        if isinstance(test_case.model_tester.text_config, dict):\n+            test_case.model_tester.text_config[\"num_hidden_layers\"] = 1\n+        else:\n+            test_case.model_tester.text_config.num_hidden_layers = 1\n \n     # A few model class specific handling\n "
        },
        {
            "sha": "6a9b8523f9e426924e587bb3737dd8afd8f488ec",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bbc00046b9b746d5ea69a27a58e652fdf617c91c/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bbc00046b9b746d5ea69a27a58e652fdf617c91c/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=bbc00046b9b746d5ea69a27a58e652fdf617c91c",
            "patch": "@@ -4707,13 +4707,17 @@ def test_custom_4d_attention_mask(self):\n                 reason=\"Model architecture has no generative classes, and thus not necessarily supporting 4D masks\"\n             )\n \n+        set_model_tester_for_less_flaky_test(self)\n+\n         for model_class in self.all_generative_model_classes:\n             if not model_class._supports_static_cache:\n                 self.skipTest(f\"{model_class.__name__} is not guaranteed to work with custom 4D attention masks\")\n             config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+            set_config_for_less_flaky_test(config)\n             if getattr(config, \"sliding_window\", 0) is not None and getattr(config, \"sliding_window\", 0) > 0:\n                 self.skipTest(f\"{model_class.__name__} with sliding window attention is not supported by this test\")\n             model = model_class(config).to(device=torch_device, dtype=torch.float32)\n+            set_model_for_less_flaky_test(model)\n \n             (\n                 input_ids,"
        }
    ],
    "stats": {
        "total": 14,
        "additions": 12,
        "deletions": 2
    }
}