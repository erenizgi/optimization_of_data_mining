{
    "author": "yijun-lee",
    "message": "ğŸŒ [i18n-KO] Translated `chameleon.md` to Korean (#33799)\n\n* docs: ko: chameleon.md\r\n\r\n* feat: nmt draft\r\n\r\n* fix: manual edits\r\n\r\n* fix: resolve suggestions\r\n\r\nCo-authored-by: Jiwook Han <33192762+mreraser@users.noreply.github.com>\r\nCo-authored-by: Chulhwa (Evan) Han <cjfghk5697@ajou.ac.kr>\r\n\r\n---------\r\n\r\nCo-authored-by: Jiwook Han <33192762+mreraser@users.noreply.github.com>\r\nCo-authored-by: Chulhwa (Evan) Han <cjfghk5697@ajou.ac.kr>",
    "sha": "178d707b7e80c04bf7ae9ecd6dc5ae29c88029a7",
    "files": [
        {
            "sha": "a1e58161fe2035a1e73b8972796638019eab0d43",
            "filename": "docs/source/ko/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/178d707b7e80c04bf7ae9ecd6dc5ae29c88029a7/docs%2Fsource%2Fko%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/178d707b7e80c04bf7ae9ecd6dc5ae29c88029a7/docs%2Fsource%2Fko%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2F_toctree.yml?ref=178d707b7e80c04bf7ae9ecd6dc5ae29c88029a7",
            "patch": "@@ -667,6 +667,8 @@\n         title: (ë²ˆì—­ì¤‘) BLIP-2\n       - local: in_translation\n         title: (ë²ˆì—­ì¤‘) BridgeTower\n+      - local: model_doc/chameleon\n+        title: Chameleon\n       - local: in_translation\n         title: (ë²ˆì—­ì¤‘) Chinese-CLIP\n       - local: in_translation"
        },
        {
            "sha": "14a18a09765bd4cd737dcf40f0a8ffa82f173e3f",
            "filename": "docs/source/ko/model_doc/chameleon.md",
            "status": "added",
            "additions": 186,
            "deletions": 0,
            "changes": 186,
            "blob_url": "https://github.com/huggingface/transformers/blob/178d707b7e80c04bf7ae9ecd6dc5ae29c88029a7/docs%2Fsource%2Fko%2Fmodel_doc%2Fchameleon.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/178d707b7e80c04bf7ae9ecd6dc5ae29c88029a7/docs%2Fsource%2Fko%2Fmodel_doc%2Fchameleon.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fchameleon.md?ref=178d707b7e80c04bf7ae9ecd6dc5ae29c88029a7",
            "patch": "@@ -0,0 +1,186 @@\n+<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# Chameleon [[chameleon]]\n+\n+## ê°œìš” [[overview]]\n+\n+Chameleon ëª¨ë¸ì€ META AI Chameleon íŒ€ì˜ ë…¼ë¬¸ [Chameleon: Mixed-Modal Early-Fusion Foundation Models](https://arxiv.org/abs/2405.09818v1)ì—ì„œ ì œì•ˆë˜ì—ˆìŠµë‹ˆë‹¤. Chameleonì€ ë²¡í„° ì–‘ìí™”ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ í† í°í™”í•¨ìœ¼ë¡œì¨ ë©€í‹°ëª¨ë‹¬ ì¶œë ¥ì„ ìƒì„±í•  ìˆ˜ ìˆëŠ” ë¹„ì „-ì–¸ì–´ ëª¨ë¸ì…ë‹ˆë‹¤. ì´ ëª¨ë¸ì€ êµì°¨ëœ í˜•ì‹ì„ í¬í•¨í•œ ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ìœ¼ë©°, í…ìŠ¤íŠ¸ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤. ì´ë¯¸ì§€ ìƒì„± ëª¨ë“ˆì€ ì•„ì§ ê³µê°œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n+\n+ë…¼ë¬¸ì˜ ì´ˆë¡ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n+\n+*ìš°ë¦¬ëŠ” ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ ì„ì˜ì˜ ìˆœì„œë¡œ ì´í•´í•˜ê³  ìƒì„±í•  ìˆ˜ ìˆëŠ” early-fusion í† í° ê¸°ë°˜ì˜ í˜¼í•© ëª¨ë‹¬(mixed-modal) ëª¨ë¸ì˜ ì¼ì¢…ì¸ Chameleonì„ ì†Œê°œí•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì´ˆê¸°ë¶€í„° ì•ˆì •ì ì¸ í›ˆë ¨ ì ‘ê·¼ë²•, ì •ë ¬ ë°©ë²•, ê·¸ë¦¬ê³  early-fusion, í† í° ê¸°ë°˜, í˜¼í•© ëª¨ë‹¬ ì„¤ì •ì— ë§ì¶˜ ì•„í‚¤í…ì²˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ì œì‹œí•©ë‹ˆë‹¤. ì´ ëª¨ë¸ë“¤ì€ ì‹œê°ì  ì§ˆë¬¸ ì‘ë‹µ, ì´ë¯¸ì§€ ìº¡ì…˜ ìƒì„±, í…ìŠ¤íŠ¸ ìƒì„±, ì´ë¯¸ì§€ ìƒì„±, ì¥ë¬¸ í˜¼í•© ëª¨ë‹¬ ìƒì„± ë“± í¬ê´„ì ì¸ ì‘ì—… ë²”ìœ„ì—ì„œ í‰ê°€ë˜ì—ˆìŠµë‹ˆë‹¤. Chameleonì€ ë‹¨ì¼ ëª¨ë¸ì—ì„œ ì´ë¯¸ì§€ ìº¡ì…˜ ìƒì„± ì‘ì—…ì—ì„œì˜ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ í¬í•¨í•œ ê´‘ë²”ìœ„í•˜ê³  ì¼ë°˜ì ìœ¼ë¡œ ì ìš© ê°€ëŠ¥í•œ ëŠ¥ë ¥ì„ ë³´ì—¬ì£¼ë©°, í…ìŠ¤íŠ¸ ì „ìš© ì‘ì—…ì—ì„œ Llama-2ë¥¼ ëŠ¥ê°€í•˜ë©´ì„œ Mixtral 8x7Bì™€ Gemini-Proì™€ ê°™ì€ ëª¨ë¸ë“¤ ì‚¬ì´ì—ì„œë„ ê²½ìŸë ¥ì„ ê°–ì¶”ê³  ìˆìŠµë‹ˆë‹¤. ê·¸ë¦¬ê³  ìƒë‹¹í•œ ì„±ëŠ¥ì˜ ì´ë¯¸ì§€ ìƒì„±ë„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ë˜í•œ í”„ë¡¬í”„íŠ¸ë‚˜ ì¶œë ¥ì— ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ì˜ í˜¼í•© ì‹œí€€ìŠ¤ê°€ í¬í•¨ëœ ìƒˆë¡œìš´ ì¥ë¬¸ í˜¼í•© ëª¨ë‹¬ ìƒì„± í‰ê°€ì—ì„œ, ì¸ê°„ì˜ íŒë‹¨ì— ë”°ë¥´ë©´ Gemini Proì™€ GPT-4Vë¥¼ í¬í•¨í•œ í›¨ì”¬ ë” í° ëª¨ë¸ì˜ ì„±ëŠ¥ê³¼ ë™ë“±í•˜ê±°ë‚˜ ì´ë¥¼ ëŠ¥ê°€í•©ë‹ˆë‹¤. Chameleonì€ ì™„ì „í•œ ë©€í‹°ëª¨ë‹¬ ë¬¸ì„œì˜ í†µí•© ëª¨ë¸ë§ì—ì„œ ì¤‘ìš”í•œ ë°œì „ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.*\n+\n+<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/chameleon_arch.png\"\n+alt=\"drawing\" width=\"600\"/>\n+\n+<small>Chameleonì€ ì´ë¯¸ì§€ë¥¼ ì´ì‚°ì ì¸ í† í°ìœ¼ë¡œ ë³€í™˜í•˜ê¸° ìœ„í•´ ë²¡í„° ì–‘ìí™” ëª¨ë“ˆì„ í†µí•©í•©ë‹ˆë‹¤. ì´ëŠ” ìê¸°íšŒê·€ transformerë¥¼ ì‚¬ìš©í•œ ì´ë¯¸ì§€ ìƒì„±ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. <a href=\"https://arxiv.org/abs/2405.09818v1\">ì›ë³¸ ë…¼ë¬¸</a>ì—ì„œ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.</small>\n+\n+ì´ ëª¨ë¸ì€ [joaogante](https://huggingface.co/joaogante)ì™€ [RaushanTurganbay](https://huggingface.co/RaushanTurganbay)ê°€ ê¸°ì—¬í–ˆìŠµë‹ˆë‹¤. ì›ë³¸ ì½”ë“œëŠ” [ì—¬ê¸°](https://github.com/facebookresearch/chameleon)ì—ì„œ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+## ì‚¬ìš© íŒ [[usage-tips]]\n+\n+- ë” ì •í™•í•œ ê²°ê³¼ë¥¼ ìœ„í•´, ë°°ì¹˜ ìƒì„± ì‹œ `padding_side=\"left\"`ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤. ìƒì„±í•˜ê¸° ì „ì— `processor.tokenizer.padding_side = \"left\"`ë¡œ ì„¤ì •í•˜ì‹­ì‹œì˜¤.\n+\n+- Chameleonì€ ì•ˆì „ì„± ì •ë ¬ì„ ìœ„í•´ íŠœë‹ë˜ì—ˆìŒì„ ìœ ì˜í•˜ì‹­ì‹œì˜¤. ëª¨ë¸ì´ ì‘ë‹µì„ ê±°ë¶€í•˜ëŠ” ê²½ìš°, ì—´ë¦° ì§ˆë¬¸ë³´ë‹¤ëŠ” ë” êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸ì„ í•´ë³´ì„¸ìš”.\n+\n+- Chameleonì€ ì±„íŒ… í˜•ì‹ìœ¼ë¡œ ìƒì„±í•˜ë¯€ë¡œ, ìƒì„±ëœ í…ìŠ¤íŠ¸ëŠ” í•­ìƒ \"assistant's turn\"ìœ¼ë¡œ í‘œì‹œë©ë‹ˆë‹¤. í”„ë¡œì„¸ì„œë¥¼ í˜¸ì¶œí•  ë•Œ `return_for_text_completion=True`ë¥¼ ì „ë‹¬í•˜ì—¬ í…ìŠ¤íŠ¸ ì™„ì„± ìƒì„±ì„ í™œì„±í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+> [!NOTE]\n+> Transformersì—ì„œì˜ Chameleon êµ¬í˜„ì€ ì´ë¯¸ì§€ ì„ë² ë”©ì„ ë³‘í•©í•  ìœ„ì¹˜ë¥¼ ë‚˜íƒ€ë‚´ê¸° ìœ„í•´ íŠ¹ë³„í•œ ì´ë¯¸ì§€ í† í°ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. íŠ¹ë³„í•œ ì´ë¯¸ì§€ í† í°ì„ ìœ„í•´ ìƒˆë¡œìš´ í† í°ì„ ì¶”ê°€í•˜ì§€ ì•Šê³  ì˜ˆì•½ëœ í† í° ì¤‘ í•˜ë‚˜ì¸ `<reserved08707>`ë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. ì˜¬ë°”ë¥¸ ìƒì„±ì„ ìœ„í•´ í”„ë¡¬í”„íŠ¸ì—ì„œ ì´ë¯¸ì§€ê°€ ì„ë² ë”©ë  ìœ„ì¹˜ì— `<image>`ë¥¼ ì¶”ê°€í•´ì•¼ í•©ë‹ˆë‹¤.\n+\n+## ì‚¬ìš© ì˜ˆì œ [[usage-example]]\n+\n+### ë‹¨ì¼ ì´ë¯¸ì§€ ì¶”ë¡  [[single-image-inference]]\n+\n+Chameleonì€ ê²Œì´í‹°ë“œ(gated) ëª¨ë¸ì´ë¯€ë¡œ Hugging Face Hubì— ëŒ€í•œ ì•¡ì„¸ìŠ¤ ê¶Œí•œì´ ìˆê³  í† í°ìœ¼ë¡œ ë¡œê·¸ì¸í–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”. ë‹¤ìŒì€ ëª¨ë¸ì„ ë¡œë“œí•˜ê³  ë°˜ì •ë°€ë„(`torch.bfloat16`)ë¡œ ì¶”ë¡ í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤:\n+\n+```python\n+from transformers import ChameleonProcessor, ChameleonForConditionalGeneration\n+import torch\n+from PIL import Image\n+import requests\n+\n+processor = ChameleonProcessor.from_pretrained(\"facebook/chameleon-7b\")\n+model = ChameleonForConditionalGeneration.from_pretrained(\"facebook/chameleon-7b\", torch_dtype=torch.bfloat16, device_map=\"cuda\")\n+\n+# ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ì¤€ë¹„\n+url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n+image = Image.open(requests.get(url, stream=True).raw)\n+prompt = \"ì´ ì´ë¯¸ì§€ì—ì„œ ë¬´ì—‡ì„ ë³´ë‚˜ìš”?<image>\"\n+\n+inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(model.device, dtype=torch.bfloat16)\n+\n+# í”„ë¡¬í”„íŠ¸ë¥¼ ìê¸°íšŒê·€ì ìœ¼ë¡œ ì™„ì„±\n+output = model.generate(**inputs, max_new_tokens=50)\n+print(processor.decode(output[0], skip_special_tokens=True))\n+```\n+\n+### ë‹¤ì¤‘ ì´ë¯¸ì§€ ì¶”ë¡  [[multi-image-inference]]\n+\n+Chameleonì€ ì—¬ëŸ¬ ì´ë¯¸ì§€ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ë“¤ì´ë©°, ì´ë¯¸ì§€ë“¤ì€ ë™ì¼í•œ í”„ë¡¬í”„íŠ¸ì— ì†í•˜ê±°ë‚˜ ë‹¤ë¥¸ í”„ë¡¬í”„íŠ¸ì— ì†í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤(ë°°ì¹˜ ì¶”ë¡ ì—ì„œ). ë‹¤ìŒì€ ê·¸ ë°©ë²•ì…ë‹ˆë‹¤:\n+\n+```python\n+from transformers import ChameleonProcessor, ChameleonForConditionalGeneration\n+import torch\n+from PIL import Image\n+import requests\n+\n+processor = ChameleonProcessor.from_pretrained(\"facebook/chameleon-7b\")\n+\n+model = ChameleonForConditionalGeneration.from_pretrained(\"facebook/chameleon-7b\", torch_dtype=torch.bfloat16, device_map=\"cuda\")\n+\n+# ì„¸ ê°€ì§€ ë‹¤ë¥¸ ì´ë¯¸ì§€ ê°€ì ¸ì˜¤ê¸°\n+url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n+image_stop = Image.open(requests.get(url, stream=True).raw)\n+\n+url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+image_cats = Image.open(requests.get(url, stream=True).raw)\n+\n+url = \"https://huggingface.co/microsoft/kosmos-2-patch14-224/resolve/main/snowman.jpg\"\n+image_snowman = Image.open(requests.get(url, stream=True).raw)\n+\n+# ë°°ì¹˜ëœ í”„ë¡¬í”„íŠ¸ ì¤€ë¹„: ì²« ë²ˆì§¸ëŠ” ë‹¤ì¤‘ ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸ì´ê³  ë‘ ë²ˆì§¸ëŠ” ë‹¨ì¼ ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸ì…ë‹ˆë‹¤\n+prompts = [\n+    \"ì´ ì´ë¯¸ì§€ë“¤ì€ ë¬´ì—‡ì´ ê³µí†µì ì¸ê°€ìš”?<image><image>\",\n+    \"<image>ì´ ì´ë¯¸ì§€ì— ë¬´ì—‡ì´ ë‚˜íƒ€ë‚˜ ìˆë‚˜ìš”?\"\n+]\n+\n+# ì´ë¯¸ì§€ë“¤ì„ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ì—ì„œ ì‚¬ìš©ë˜ì–´ì•¼ í•˜ëŠ” ìˆœì„œëŒ€ë¡œ ì…ë ¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n+# ê° \"<image>\" í† í°ì€ í•˜ë‚˜ì˜ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•˜ë©°, ë‹¤ìŒ \"<image>\" í† í°ì€ ë‹¤ìŒ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤\n+inputs = processor(images=[image_stop, image_cats, image_snowman], text=prompts, padding=True, return_tensors=\"pt\").to(device=\"cuda\", dtype=torch.bfloat16)\n+\n+# ìƒì„±\n+generate_ids = model.generate(**inputs, max_new_tokens=50)\n+processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n+```\n+\n+## ëª¨ë¸ ìµœì í™” [[model-optimization]]\n+\n+### Bitsandbytesë¥¼ ì‚¬ìš©í•œ ì–‘ìí™” [[quantization-using-bitsandbytes]]\n+\n+ëª¨ë¸ì€ 8ë¹„íŠ¸ ë˜ëŠ” 4ë¹„íŠ¸ë¡œ ë¡œë“œí•  ìˆ˜ ìˆìœ¼ë©°, ì´ëŠ” ì›ë³¸ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ìœ ì§€í•˜ë©´ì„œ ë©”ëª¨ë¦¬ ìš”êµ¬ ì‚¬í•­ì„ í¬ê²Œ ì¤„ì—¬ì¤ë‹ˆë‹¤. ë¨¼ì € bitsandbytesë¥¼ ì„¤ì¹˜í•˜ê³ (`pip install bitsandbytes`), ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì§€ì›í•˜ëŠ” GPU/ê°€ì†ê¸°ë¥¼ ì‚¬ìš© ì¤‘ì¸ì§€ í™•ì¸í•˜ì‹­ì‹œì˜¤.\n+\n+<Tip>\n+\n+bitsandbytesëŠ” CUDA ì´ì™¸ì˜ ì—¬ëŸ¬ ë°±ì—”ë“œë¥¼ ì§€ì›í•˜ë„ë¡ ë¦¬íŒ©í„°ë§ë˜ê³  ìˆìŠµë‹ˆë‹¤. í˜„ì¬ ROCm(AMD GPU) ë° Intel CPU êµ¬í˜„ì´ ì„±ìˆ™ ë‹¨ê³„ì´ë©°, Intel XPUëŠ” ì§„í–‰ ì¤‘ì´ê³  Apple Silicon ì§€ì›ì€ Q4/Q1ì— ì˜ˆìƒë©ë‹ˆë‹¤. ì„¤ì¹˜ ì§€ì¹¨ ë° ìµœì‹  ë°±ì—”ë“œ ì—…ë°ì´íŠ¸ëŠ” [ì´ ë§í¬](https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend)ë¥¼ ë°©ë¬¸í•˜ì„¸ìš”.\n+\n+ì „ì²´ ê³µê°œ ì „ì— ë²„ê·¸ë¥¼ ì‹ë³„í•˜ëŠ” ë° ë„ì›€ì´ ë˜ëŠ” í”¼ë“œë°±ì„ í™˜ì˜í•©ë‹ˆë‹¤! ìì„¸í•œ ë‚´ìš©ê³¼ í”¼ë“œë°±ì€ [ì´ ë¬¸ì„œ](https://huggingface.co/docs/bitsandbytes/main/en/non_cuda_backends)ë¥¼ í™•ì¸í•˜ì„¸ìš”.\n+\n+</Tip>\n+\n+ìœ„ì˜ ì½”ë“œ ìŠ¤ë‹ˆí«ì„ ë‹¤ìŒê³¼ ê°™ì´ ë³€ê²½í•˜ë©´ ë©ë‹ˆë‹¤:\n+\n+```python\n+from transformers import ChameleonForConditionalGeneration, BitsAndBytesConfig\n+\n+# ëª¨ë¸ ì–‘ìí™” ë°©ì‹ ì§€ì •\n+quantization_config = BitsAndBytesConfig(\n+    load_in_4bit=True,\n+    bnb_4bit_quant_type=\"nf4\",\n+    bnb_4bit_compute_dtype=torch.bfloat16,\n+)\n+\n+model = ChameleonForConditionalGeneration.from_pretrained(\"facebook/chameleon-7b\", quantization_config=quantization_config, device_map=\"cuda\")\n+```\n+\n+### Flash-Attention 2ì™€ SDPAë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒì„± ì†ë„ í–¥ìƒ [[use-flash-attention-2-and-sdpa-to-further-speed-up-generation]]\n+\n+ì´ ëª¨ë¸ì€ ìµœì í™”ë¥¼ ìœ„í•´ Flash-Attention 2ì™€ PyTorchì˜ [`torch.nn.functional.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)ë¥¼ ëª¨ë‘ ì§€ì›í•©ë‹ˆë‹¤. SDPAëŠ” ëª¨ë¸ì„ ë¡œë“œí•  ë•Œ ê¸°ë³¸ ì˜µì…˜ì…ë‹ˆë‹¤. Flash Attention 2ë¡œ ì „í™˜í•˜ë ¤ë©´ ë¨¼ì € flash-attnì„ ì„¤ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤. í•´ë‹¹ íŒ¨í‚¤ì§€ ì„¤ì¹˜ì— ëŒ€í•´ì„œëŠ” [ì›ë³¸ ë¦¬í¬ì§€í† ë¦¬](https://github.com/Dao-AILab/flash-attention)ë¥¼ ì°¸ê³ í•˜ì‹­ì‹œì˜¤. ìœ„ì˜ ì½”ë“œ ìŠ¤ë‹ˆí«ì„ ë‹¤ìŒê³¼ ê°™ì´ ë³€ê²½í•˜ë©´ ë©ë‹ˆë‹¤:\n+\n+```python\n+from transformers import ChameleonForConditionalGeneration\n+\n+model_id = \"facebook/chameleon-7b\"\n+model = ChameleonForConditionalGeneration.from_pretrained(\n+    model_id,\n+    torch_dtype=torch.bfloat16,\n+    low_cpu_mem_usage=True,\n+    attn_implementation=\"flash_attention_2\"\n+).to(0)\n+```\n+\n+## ChameleonConfig [[transformers.ChameleonConfig]]\n+\n+[[autodoc]] ChameleonConfig\n+\n+## ChameleonVQVAEConfig [[transformers.ChameleonVQVAEConfig]]\n+\n+[[autodoc]] ChameleonVQVAEConfig\n+\n+## ChameleonProcessor [[transformers.ChameleonProcessor]]\n+\n+[[autodoc]] ChameleonProcessor\n+\n+## ChameleonImageProcessor [[transformers.ChameleonImageProcessor]]\n+\n+[[autodoc]] ChameleonImageProcessor\n+    - preprocess\n+\n+## ChameleonVQVAE [[transformers.ChameleonVQVAE]]\n+\n+[[autodoc]] ChameleonVQVAE\n+    - forward\n+\n+## ChameleonModel [[transformers.ChameleonModel]]\n+\n+[[autodoc]] ChameleonModel\n+    - forward\n+\n+## ChameleonForConditionalGeneration [[transformers.ChameleonForConditionalGeneration]]\n+\n+[[autodoc]] ChameleonForConditionalGeneration\n+    - forward"
        }
    ],
    "stats": {
        "total": 188,
        "additions": 188,
        "deletions": 0
    }
}