{
    "author": "yao-matrix",
    "message": "[4/N]more docs to device agnostic (#40355)\n\n* more docs to device agnostic\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\n\n* more\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\n\n* 1\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\n\n* 2\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\n\n* Update vitpose.md\n\n* Update camembert.md\n\n* Update camembert.md\n\n---------\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>",
    "sha": "7c1169e21fcf953b9670ac1e79c6e157bcaedc5c",
    "files": [
        {
            "sha": "cbb0c8e702b19ca545b2323f40870720eba129a2",
            "filename": "docs/source/en/generation_strategies.md",
            "status": "modified",
            "additions": 41,
            "deletions": 25,
            "changes": 66,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fgeneration_strategies.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fgeneration_strategies.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fgeneration_strategies.md?ref=7c1169e21fcf953b9670ac1e79c6e157bcaedc5c",
            "patch": "@@ -32,12 +32,14 @@ Greedy search works well for tasks with relatively short outputs where creativit\n \n ```py\n import torch\n-from transformers import AutoModelForCausalLM, AutoTokenizer\n+from transformers import AutoModelForCausalLM, AutoTokenizer, infer_device\n+\n+device = infer_device()\n \n tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n-inputs = tokenizer(\"Hugging Face is an open-source company\", return_tensors=\"pt\").to(\"cuda\")\n+inputs = tokenizer(\"Hugging Face is an open-source company\", return_tensors=\"pt\").to(device)\n \n-model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", torch_dtype=torch.float16).to(\"cuda\")\n+model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", torch_dtype=torch.float16).to(device)\n # explicitly set to default length because Llama2 generation length is 4096\n outputs = model.generate(**inputs, max_new_tokens=20)\n tokenizer.batch_decode(outputs, skip_special_tokens=True)\n@@ -52,12 +54,14 @@ Enable multinomial sampling with `do_sample=True` and `num_beams=1`.\n \n ```py\n import torch\n-from transformers import AutoModelForCausalLM, AutoTokenizer\n+from transformers import AutoModelForCausalLM, AutoTokenizer, infer_device\n+\n+device = infer_device()\n \n tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n-inputs = tokenizer(\"Hugging Face is an open-source company\", return_tensors=\"pt\").to(\"cuda\")\n+inputs = tokenizer(\"Hugging Face is an open-source company\", return_tensors=\"pt\").to(device)\n \n-model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", torch_dtype=torch.float16).to(\"cuda\")\n+model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", torch_dtype=torch.float16).to(device)\n # explicitly set to 100 because Llama2 generation length is 4096\n outputs = model.generate(**inputs, max_new_tokens=50, do_sample=True, num_beams=1)\n tokenizer.batch_decode(outputs, skip_special_tokens=True)\n@@ -75,12 +79,14 @@ Enable beam search with the `num_beams` parameter (should be greater than 1 othe\n \n ```py\n import torch\n-from transformers import AutoModelForCausalLM, AutoTokenizer\n+from transformers import AutoModelForCausalLM, AutoTokenizer, infer_device\n+\n+device = infer_device()\n \n tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n-inputs = tokenizer(\"Hugging Face is an open-source company\", return_tensors=\"pt\").to(\"cuda\")\n+inputs = tokenizer(\"Hugging Face is an open-source company\", return_tensors=\"pt\").to(device)\n \n-model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", torch_dtype=torch.float16).to(\"cuda\")\n+model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", torch_dtype=torch.float16).to(device)\n # explicitly set to 100 because Llama2 generation length is 4096\n outputs = model.generate(**inputs, max_new_tokens=50, num_beams=2)\n tokenizer.batch_decode(outputs, skip_special_tokens=True)\n@@ -160,12 +166,14 @@ Enable prompt lookup decoding with the `prompt_lookup_num_tokens` parameter.\n \n ```py\n import torch\n-from transformers import AutoModelForCausalLM, AutoTokenizer\n+from transformers import AutoModelForCausalLM, AutoTokenizer, infer_device\n+\n+device = infer_device()\n \n tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM-1.7B\")\n-model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM-1.7B\", torch_dtype=torch.float16).to(\"cuda\")\n-assistant_model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM-135M\", torch_dtype=torch.float16).to(\"cuda\")\n-inputs = tokenizer(\"Hugging Face is an open-source company\", return_tensors=\"pt\").to(\"cuda\")\n+model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM-1.7B\", torch_dtype=torch.float16).to(device)\n+assistant_model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM-135M\", torch_dtype=torch.float16).to(device)\n+inputs = tokenizer(\"Hugging Face is an open-source company\", return_tensors=\"pt\").to(device)\n \n outputs = model.generate(**inputs, assistant_model=assistant_model, max_new_tokens=20, prompt_lookup_num_tokens=5)\n tokenizer.batch_decode(outputs, skip_special_tokens=True)\n@@ -226,12 +234,14 @@ Enable contrastive search with the `penalty_alpha` and `top_k` parameters. The `\n \n ```py\n import torch\n-from transformers import AutoModelForCausalLM, AutoTokenizer\n+from transformers import AutoModelForCausalLM, AutoTokenizer, infer_device\n+\n+device = infer_device()\n \n tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n-inputs = tokenizer(\"Hugging Face is an open-source company\", return_tensors=\"pt\").to(\"cuda\")\n+inputs = tokenizer(\"Hugging Face is an open-source company\", return_tensors=\"pt\").to(device)\n \n-model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", torch_dtype=torch.float16).to(\"cuda\")\n+model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", torch_dtype=torch.float16).to(device)\n # explicitly set to 100 because Llama2 generation length is 4096\n outputs = model.generate(**inputs, max_new_tokens=100, penalty_alpha=0.6, top_k=4)\n tokenizer.batch_decode(outputs, skip_special_tokens=True)\n@@ -262,11 +272,13 @@ Enable DoLa with the following parameters.\n \n ```py\n import torch\n-from transformers import AutoModelForCausalLM, AutoTokenizer\n+from transformers import AutoModelForCausalLM, AutoTokenizer, infer_device\n+\n+device = infer_device()\n \n tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM-1.7B\")\n-model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM-1.7B\", torch_dtype=torch.float16).to(\"cuda\")\n-inputs = tokenizer(\"What is the highest peak in the world??\", return_tensors=\"pt\").to(\"cuda\")\n+model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM-1.7B\", torch_dtype=torch.float16).to(device)\n+inputs = tokenizer(\"What is the highest peak in the world??\", return_tensors=\"pt\").to(device)\n \n outputs = model.generate(**inputs, max_new_tokens=50, dola_layers=\"high\", do_sample=False)\n tokenizer.batch_decode(outputs, skip_special_tokens=True)\n@@ -280,11 +292,13 @@ Contrast layers 18 and 20 with the final layer.\n \n ```py\n import torch\n-from transformers import AutoModelForCausalLM, AutoTokenizer\n+from transformers import AutoModelForCausalLM, AutoTokenizer, infer_device\n+\n+device = infer_device()\n \n tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM-1.7B\")\n-model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM-1.7B\", torch_dtype=torch.float16).to(\"cuda\")\n-inputs = tokenizer(\"What is the highest peak in the world?\", return_tensors=\"pt\").to(\"cuda\")\n+model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM-1.7B\", torch_dtype=torch.float16).to(device)\n+inputs = tokenizer(\"What is the highest peak in the world?\", return_tensors=\"pt\").to(device)\n \n outputs = model.generate(**inputs, max_new_tokens=50, dola_layers=[18,20], do_sample=False, repetition_penalty=1.2)\n tokenizer.batch_decode(outputs[:, inputs.input_ids.shape[-1]:], skip_special_tokens=True)\n@@ -302,12 +316,14 @@ Enable diverse beam search with the `num_beams`, `num_beam_groups` and `diversit\n \n ```py\n import torch\n-from transformers import AutoModelForCausalLM, AutoTokenizer\n+from transformers import AutoModelForCausalLM, AutoTokenizer, infer_device\n+\n+device = infer_device()\n \n tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n-inputs = tokenizer(\"Hugging Face is an open-source company\", return_tensors=\"pt\").to(\"cuda\")\n+inputs = tokenizer(\"Hugging Face is an open-source company\", return_tensors=\"pt\").to(device)\n \n-model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", torch_dtype=torch.float16).to(\"cuda\")\n+model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", torch_dtype=torch.float16).to(device)\n # explicitly set to 100 because Llama2 generation length is 4096\n outputs = model.generate(**inputs, max_new_tokens=50, num_beams=6, num_beam_groups=3, diversity_penalty=1.0, do_sample=False)\n tokenizer.batch_decode(outputs, skip_special_tokens=True)"
        },
        {
            "sha": "a08f57426b6adf8a29b522b05f1d02e848ea2632",
            "filename": "docs/source/en/llm_tutorial.md",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fllm_tutorial.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fllm_tutorial.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fllm_tutorial.md?ref=7c1169e21fcf953b9670ac1e79c6e157bcaedc5c",
            "patch": "@@ -56,7 +56,7 @@ Tokenize your input, and set the [`~PreTrainedTokenizer.padding_side`] parameter\n \n ```py\n tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\", padding_side=\"left\")\n-model_inputs = tokenizer([\"A list of colors: red, blue\"], return_tensors=\"pt\").to(\"cuda\")\n+model_inputs = tokenizer([\"A list of colors: red, blue\"], return_tensors=\"pt\").to(model.device)\n ```\n \n Pass the inputs to [`~GenerationMixin.generate`] to generate tokens, and [`~PreTrainedTokenizer.batch_decode`] the generated tokens back to text.\n@@ -164,7 +164,7 @@ The section below covers some common issues you may encounter during text genera\n [`~GenerationMixin.generate`] returns up to 20 tokens by default unless otherwise specified in a models [`GenerationConfig`]. It is highly recommended to manually set the number of generated tokens with the [`max_new_tokens`] parameter to control the output length. [Decoder-only](https://hf.co/learn/nlp-course/chapter1/6?fw=pt) models returns the initial prompt along with the generated tokens.\n \n ```py\n-model_inputs = tokenizer([\"A sequence of numbers: 1, 2\"], return_tensors=\"pt\").to(\"cuda\")\n+model_inputs = tokenizer([\"A sequence of numbers: 1, 2\"], return_tensors=\"pt\").to(model.device)\n ```\n \n <hfoptions id=\"output-length\">\n@@ -195,7 +195,7 @@ The default decoding strategy in [`~GenerationMixin.generate`] is *greedy search\n For example, enable a [multinomial sampling](./generation_strategies#multinomial-sampling) strategy to generate more diverse outputs. Refer to the [Generation strategy](./generation_strategies) guide for more decoding strategies.\n \n ```py\n-model_inputs = tokenizer([\"I am a cat.\"], return_tensors=\"pt\").to(\"cuda\")\n+model_inputs = tokenizer([\"I am a cat.\"], return_tensors=\"pt\").to(model.device)\n ```\n \n <hfoptions id=\"decoding\">\n@@ -227,7 +227,7 @@ Inputs need to be padded if they don't have the same length. But LLMs aren't tra\n ```py\n model_inputs = tokenizer(\n     [\"1, 2, 3\", \"A, B, C, D, E\"], padding=True, return_tensors=\"pt\"\n-).to(\"cuda\")\n+).to(model.device)\n generated_ids = model.generate(**model_inputs)\n tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n '1, 2, 33333333333'\n@@ -241,7 +241,7 @@ tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\", padding_s\n tokenizer.pad_token = tokenizer.eos_token\n model_inputs = tokenizer(\n     [\"1, 2, 3\", \"A, B, C, D, E\"], padding=True, return_tensors=\"pt\"\n-).to(\"cuda\")\n+).to(model.device)\n generated_ids = model.generate(**model_inputs)\n tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n '1, 2, 3, 4, 5, 6,'\n@@ -270,7 +270,7 @@ model = AutoModelForCausalLM.from_pretrained(\n \n ```py\n prompt = \"\"\"How many cats does it take to change a light bulb? Reply as a pirate.\"\"\"\n-model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n+model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(model.device)\n input_length = model_inputs.input_ids.shape[1]\n generated_ids = model.generate(**model_inputs, max_new_tokens=50)\n print(tokenizer.batch_decode(generated_ids[:, input_length:], skip_special_tokens=True)[0])\n@@ -288,7 +288,7 @@ messages = [\n     },\n     {\"role\": \"user\", \"content\": \"How many cats does it take to change a light bulb?\"},\n ]\n-model_inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n+model_inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n input_length = model_inputs.shape[1]\n generated_ids = model.generate(model_inputs, do_sample=True, max_new_tokens=50)\n print(tokenizer.batch_decode(generated_ids[:, input_length:], skip_special_tokens=True)[0])"
        },
        {
            "sha": "fb4b80970a2f519637554c4c6993a69a6df6b506",
            "filename": "docs/source/en/model_doc/bert.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fmodel_doc%2Fbert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fmodel_doc%2Fbert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbert.md?ref=7c1169e21fcf953b9670ac1e79c6e157bcaedc5c",
            "patch": "@@ -65,7 +65,7 @@ model = AutoModelForMaskedLM.from_pretrained(\n     device_map=\"auto\",\n     attn_implementation=\"sdpa\"\n )\n-inputs = tokenizer(\"Plants create [MASK] through a process known as photosynthesis.\", return_tensors=\"pt\").to(\"cuda\")\n+inputs = tokenizer(\"Plants create [MASK] through a process known as photosynthesis.\", return_tensors=\"pt\").to(model.device)\n \n with torch.no_grad():\n     outputs = model(**inputs)"
        },
        {
            "sha": "a52d5e9b2d9924665d34d81316e18c57b37b91c9",
            "filename": "docs/source/en/model_doc/camembert.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fmodel_doc%2Fcamembert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fmodel_doc%2Fcamembert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcamembert.md?ref=7c1169e21fcf953b9670ac1e79c6e157bcaedc5c",
            "patch": "@@ -60,7 +60,7 @@ from transformers import AutoTokenizer, AutoModelForMaskedLM\n \n tokenizer = AutoTokenizer.from_pretrained(\"camembert-base\")\n model = AutoModelForMaskedLM.from_pretrained(\"camembert-base\", torch_dtype=\"auto\", device_map=\"auto\", attn_implementation=\"sdpa\")\n-inputs = tokenizer(\"Le camembert est un délicieux fromage <mask>.\", return_tensors=\"pt\").to(\"cuda\")\n+inputs = tokenizer(\"Le camembert est un délicieux fromage <mask>.\", return_tensors=\"pt\").to(model.device)\n \n with torch.no_grad():\n     outputs = model(**inputs)\n@@ -101,7 +101,7 @@ model = AutoModelForMaskedLM.from_pretrained(\n )\n tokenizer = AutoTokenizer.from_pretrained(\"almanach/camembert-large\")\n \n-inputs = tokenizer(\"Le camembert est un délicieux fromage <mask>.\", return_tensors=\"pt\").to(\"cuda\")\n+inputs = tokenizer(\"Le camembert est un délicieux fromage <mask>.\", return_tensors=\"pt\").to(model.device)\n \n with torch.no_grad():\n     outputs = model(**inputs)"
        },
        {
            "sha": "c128ca316b7397d02a92c0de6294b159f5f5e10b",
            "filename": "docs/source/en/model_doc/colqwen2.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fmodel_doc%2Fcolqwen2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fmodel_doc%2Fcolqwen2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcolqwen2.md?ref=7c1169e21fcf953b9670ac1e79c6e157bcaedc5c",
            "patch": "@@ -50,7 +50,7 @@ model_name = \"vidore/colqwen2-v1.0-hf\"\n model = ColQwen2ForRetrieval.from_pretrained(\n     model_name,\n     torch_dtype=torch.bfloat16,\n-    device_map=\"auto\",  # \"cpu\", \"cuda\", or \"mps\" for Apple Silicon\n+    device_map=\"auto\",  # \"cpu\", \"cuda\", \"xpu\" or \"mps\" for Apple Silicon\n     attn_implementation=\"flash_attention_2\" if is_flash_attn_2_available() else \"sdpa\",\n )\n processor = ColQwen2Processor.from_pretrained(model_name)\n@@ -107,10 +107,10 @@ import requests\n import torch\n from PIL import Image\n \n-from transformers import BitsAndBytesConfig, ColQwen2ForRetrieval, ColQwen2Processor\n-\n+from transformers import BitsAndBytesConfig, ColQwen2ForRetrieval, ColQwen2Processor, infer_device\n \n model_name = \"vidore/colqwen2-v1.0-hf\"\n+device = infer_device()\n \n # 4-bit quantization configuration\n bnb_config = BitsAndBytesConfig(\n@@ -123,7 +123,7 @@ bnb_config = BitsAndBytesConfig(\n model = ColQwen2ForRetrieval.from_pretrained(\n     model_name,\n     quantization_config=bnb_config,\n-    device_map=\"cuda\",\n+    device_map=device,\n ).eval()\n \n processor = ColQwen2Processor.from_pretrained(model_name)"
        },
        {
            "sha": "1ee2b63dd7150ec63c8d9a4f08312ad32172a207",
            "filename": "docs/source/en/model_doc/csm.md",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fmodel_doc%2Fcsm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fmodel_doc%2Fcsm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcsm.md?ref=7c1169e21fcf953b9670ac1e79c6e157bcaedc5c",
            "patch": "@@ -59,7 +59,7 @@ inputs = processor.apply_chat_template(\n     conversation,\n     tokenize=True,\n     return_dict=True,\n-).to(device)\n+).to(model.device)\n \n # infer the model\n audio = model.generate(**inputs, output_audio=True)\n@@ -104,7 +104,7 @@ inputs = processor.apply_chat_template(\n     conversation,\n     tokenize=True,\n     return_dict=True,\n-).to(device)\n+).to(model.device)\n \n # infer the model\n audio = model.generate(**inputs, output_audio=True)\n@@ -161,7 +161,7 @@ inputs = processor.apply_chat_template(\n     conversation,\n     tokenize=True,\n     return_dict=True,\n-).to(device)\n+).to(model.device)\n \n audio = model.generate(**inputs, output_audio=True)\n processor.save_audio(audio, [f\"speech_batch_idx_{i}.wav\" for i in range(len(audio))])\n@@ -251,7 +251,7 @@ padded_inputs_1 = processor.apply_chat_template(\n     conversation,\n     tokenize=True,\n     return_dict=True,\n-).to(device)\n+).to(model.device)\n \n print(\"\\n\" + \"=\"*50)\n print(\"First generation - compiling and recording CUDA graphs...\")\n@@ -292,7 +292,7 @@ padded_inputs_2 = processor.apply_chat_template(\n     conversation,\n     tokenize=True,\n     return_dict=True,\n-).to(device)\n+).to(model.device)\n \n print(\"\\n\" + \"=\"*50)\n print(\"Generation with other inputs!\")\n@@ -337,7 +337,7 @@ inputs = processor.apply_chat_template(\n     tokenize=True,\n     return_dict=True,\n     output_labels=True,\n-).to(device)\n+).to(model.device)\n \n out = model(**inputs)\n out.loss.backward()"
        },
        {
            "sha": "807db0a029ead7cd4709779d618a17dfe8358cdd",
            "filename": "docs/source/en/model_doc/deberta-v2.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeberta-v2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeberta-v2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeberta-v2.md?ref=7c1169e21fcf953b9670ac1e79c6e157bcaedc5c",
            "patch": "@@ -69,7 +69,7 @@ model = AutoModelForSequenceClassification.from_pretrained(\n     device_map=\"auto\"\n )\n \n-inputs = tokenizer(\"DeBERTa-v2 is great at understanding context!\", return_tensors=\"pt\").to(\"cuda\")\n+inputs = tokenizer(\"DeBERTa-v2 is great at understanding context!\", return_tensors=\"pt\").to(model.device)\n outputs = model(**inputs)\n \n logits = outputs.logits\n@@ -110,7 +110,7 @@ model = AutoModelForSequenceClassification.from_pretrained(\n     torch_dtype=\"float16\"\n )\n \n-inputs = tokenizer(\"DeBERTa-v2 is great at understanding context!\", return_tensors=\"pt\").to(\"cuda\")\n+inputs = tokenizer(\"DeBERTa-v2 is great at understanding context!\", return_tensors=\"pt\").to(model.device)\n outputs = model(**inputs)\n logits = outputs.logits\n predicted_class_id = logits.argmax().item()"
        },
        {
            "sha": "2d99bdbfd2103e4ba459dbaa12f9f6318a4cdfdf",
            "filename": "docs/source/en/model_doc/deberta.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeberta.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeberta.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeberta.md?ref=7c1169e21fcf953b9670ac1e79c6e157bcaedc5c",
            "patch": "@@ -71,7 +71,7 @@ inputs = tokenizer(\n     \"A soccer game with multiple people playing.\",\n     \"Some people are playing a sport.\",\n     return_tensors=\"pt\"\n-).to(\"cuda\")\n+).to(model.device)\n \n with torch.no_grad():\n     logits = model(**inputs).logits"
        },
        {
            "sha": "c1c697d8fe82a9f1a3450a6cc15f4bf7a1dd9b7d",
            "filename": "docs/source/en/model_doc/ernie.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fmodel_doc%2Fernie.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fmodel_doc%2Fernie.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fernie.md?ref=7c1169e21fcf953b9670ac1e79c6e157bcaedc5c",
            "patch": "@@ -66,7 +66,7 @@ model = AutoModelForMaskedLM.from_pretrained(\n     torch_dtype=torch.float16,\n     device_map=\"auto\"\n )\n-inputs = tokenizer(\"巴黎是[MASK]国的首都。\", return_tensors=\"pt\").to(\"cuda\")\n+inputs = tokenizer(\"巴黎是[MASK]国的首都。\", return_tensors=\"pt\").to(model.device)\n \n with torch.no_grad():\n     outputs = model(**inputs)"
        },
        {
            "sha": "3c7606868ac0b4e9ba408195f2c99b62af1a2199",
            "filename": "docs/source/en/model_doc/gemma3n.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma3n.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma3n.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma3n.md?ref=7c1169e21fcf953b9670ac1e79c6e157bcaedc5c",
            "patch": "@@ -103,7 +103,7 @@ inputs = processor.apply_chat_template(\n     return_dict=True,\n     return_tensors=\"pt\",\n     add_generation_prompt=True,\n-).to(\"cuda\")\n+).to(model.device)\n \n output = model.generate(**inputs, max_new_tokens=50, cache_implementation=\"static\")\n print(processor.decode(output[0], skip_special_tokens=True))"
        },
        {
            "sha": "ec1a9c06c5d320b02b835a9a3b52503a295fbb6b",
            "filename": "docs/source/en/model_doc/gpt_neo.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_neo.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_neo.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_neo.md?ref=7c1169e21fcf953b9670ac1e79c6e157bcaedc5c",
            "patch": "@@ -55,7 +55,7 @@ from transformers import AutoModelForCausalLM, AutoTokenizer\n model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\", torch_dtype=torch.float16, device_map=\"auto\", attn_implementation=\"flash_attention_2\")\n tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n \n-input_ids = tokenizer(\"Hello, I'm a language model\", return_tensors=\"pt\").to(\"cuda\")\n+input_ids = tokenizer(\"Hello, I'm a language model\", return_tensors=\"pt\").to(model.device)\n \n output = model.generate(**input_ids)\n print(tokenizer.decode(output[0], skip_special_tokens=True))\n@@ -93,7 +93,7 @@ model = AutoModelForCausalLM.from_pretrained(\n )\n \n tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-2.7B\")\n-inputs = tokenizer(\"Hello, I'm a language model\", return_tensors=\"pt\").to(\"cuda\")\n+inputs = tokenizer(\"Hello, I'm a language model\", return_tensors=\"pt\").to(model.device)\n outputs = model.generate(**inputs, max_new_tokens=100)\n print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n ```"
        },
        {
            "sha": "5f26cd52e916ec51faa57c9708726fada8d82786",
            "filename": "docs/source/en/model_doc/gpt_neox.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_neox.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_neox.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_neox.md?ref=7c1169e21fcf953b9670ac1e79c6e157bcaedc5c",
            "patch": "@@ -38,7 +38,7 @@ generous the support of [CoreWeave](https://www.coreweave.com/).\n GPT-NeoX-20B was trained with fp16, thus it is recommended to initialize the model as follows:\n \n ```python\n-model = GPTNeoXForCausalLM.from_pretrained(\"EleutherAI/gpt-neox-20b\").half().cuda()\n+model = GPTNeoXForCausalLM.from_pretrained(\"EleutherAI/gpt-neox-20b\", device_map=\"auto\", torch_dtype=torch.float16)\n ```\n \n GPT-NeoX-20B also has a different tokenizer from the one used in GPT-J-6B and GPT-Neo. The new tokenizer allocates"
        },
        {
            "sha": "0d7f74bf0f00507e4f4903c4f9374a8d1d36413a",
            "filename": "docs/source/en/model_doc/grounding-dino.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fmodel_doc%2Fgrounding-dino.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fmodel_doc%2Fgrounding-dino.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgrounding-dino.md?ref=7c1169e21fcf953b9670ac1e79c6e157bcaedc5c",
            "patch": "@@ -50,10 +50,10 @@ Here's how to use the model for zero-shot object detection:\n \n >>> import torch\n >>> from PIL import Image\n->>> from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n+>>> from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection, infer_device\n \n >>> model_id = \"IDEA-Research/grounding-dino-tiny\"\n->>> device = \"cuda\"\n+>>> device = infer_device()\n \n >>> processor = AutoProcessor.from_pretrained(model_id)\n >>> model = AutoModelForZeroShotObjectDetection.from_pretrained(model_id).to(device)\n@@ -63,7 +63,7 @@ Here's how to use the model for zero-shot object detection:\n >>> # Check for cats and remote controls\n >>> text_labels = [[\"a cat\", \"a remote control\"]]\n \n->>> inputs = processor(images=image, text=text_labels, return_tensors=\"pt\").to(device)\n+>>> inputs = processor(images=image, text=text_labels, return_tensors=\"pt\").to(model.device)\n >>> with torch.no_grad():\n ...     outputs = model(**inputs)\n "
        },
        {
            "sha": "c53f67e91b47e028ec2954ef3f9bccdd678927dd",
            "filename": "docs/source/en/model_doc/kosmos2_5.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fmodel_doc%2Fkosmos2_5.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fmodel_doc%2Fkosmos2_5.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fkosmos2_5.md?ref=7c1169e21fcf953b9670ac1e79c6e157bcaedc5c",
            "patch": "@@ -46,10 +46,10 @@ import re\n import torch\n import requests\n from PIL import Image, ImageDraw\n-from transformers import AutoProcessor, Kosmos2_5ForConditionalGeneration\n+from transformers import AutoProcessor, Kosmos2_5ForConditionalGeneration, infer_device\n \n repo = \"ydshieh/kosmos-2.5\"\n-device = \"cuda:0\"\n+device = f\"{infer_device()}:0\"\n dtype = torch.bfloat16\n model = Kosmos2_5ForConditionalGeneration.from_pretrained(repo, device_map=device, torch_dtype=dtype)\n processor = AutoProcessor.from_pretrained(repo)\n@@ -85,10 +85,10 @@ import re\n import torch\n import requests\n from PIL import Image, ImageDraw\n-from transformers import AutoProcessor, Kosmos2_5ForConditionalGeneration\n+from transformers import AutoProcessor, Kosmos2_5ForConditionalGeneration, infer_device\n \n repo = \"ydshieh/kosmos-2.5\"\n-device = \"cuda:0\"\n+device = f\"{infer_device()}:0\"\n dtype = torch.bfloat16\n model = Kosmos2_5ForConditionalGeneration.from_pretrained(repo, device_map=device, torch_dtype=dtype)\n processor = AutoProcessor.from_pretrained(repo)"
        },
        {
            "sha": "4c089c376cc77603703f2847708a192664a4af43",
            "filename": "docs/source/en/model_doc/llama4.md",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama4.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama4.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama4.md?ref=7c1169e21fcf953b9670ac1e79c6e157bcaedc5c",
            "patch": "@@ -205,7 +205,7 @@ We will work to enable running with `device_map=\"auto\"` and flex-attention witho\n tensor-parallel in the future.\n \n ```py\n-from transformers import Llama4ForConditionalGeneration, AutoTokenizer\n+from transformers import Llama4ForConditionalGeneration, AutoTokenizer, infer_device\n import torch\n import time\n \n@@ -228,7 +228,9 @@ messages = [\n ]\n input_ids = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\")\n \n-torch.cuda.synchronize()\n+device = infer_device()\n+torch_device_module = getattr(torch, device, torch.cuda)\n+torch_device_module.synchronize()\n start = time.time()\n out = model.generate(\n     input_ids.to(model.device),\n@@ -238,7 +240,7 @@ out = model.generate(\n )\n print(time.time()-start)\n print(tokenizer.batch_decode(out[:, input_ids.shape[-1]:]))\n-print(f\"{torch.cuda.max_memory_allocated(model.device) / 1024**3:.2f} GiB\")\n+print(f\"{torch_device_module.max_memory_allocated(model.device) / 1024**3:.2f} GiB\")\n ```\n \n </hfoption>"
        },
        {
            "sha": "248b024de0069e8ac53e5571b9bc678b45389ec6",
            "filename": "docs/source/en/model_doc/llava_onevision.md",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_onevision.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_onevision.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_onevision.md?ref=7c1169e21fcf953b9670ac1e79c6e157bcaedc5c",
            "patch": "@@ -115,14 +115,16 @@ The original code can be found [here](https://github.com/LLaVA-VL/LLaVA-NeXT/tre\n Here's how to load the model and perform inference in half-precision (`torch.float16`):\n \n ```python\n-from transformers import AutoProcessor, LlavaOnevisionForConditionalGeneration\n+from transformers import AutoProcessor, LlavaOnevisionForConditionalGeneration, infer_device\n import torch\n \n+device = f\"{infer_device}:0\"\n+\n processor = AutoProcessor.from_pretrained(\"llava-hf/llava-onevision-qwen2-7b-ov-hf\") \n model = LlavaOnevisionForConditionalGeneration.from_pretrained(\n     \"llava-hf/llava-onevision-qwen2-7b-ov-hf\",\n     torch_dtype=torch.float16,\n-    device_map=\"cuda:0\"\n+    device_map=device\n )\n \n # prepare image and text prompt, using the appropriate prompt template\n@@ -137,7 +139,7 @@ conversation = [\n     },\n ]\n inputs = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\")\n-inputs = inputs.to(\"cuda:0\", torch.float16)\n+inputs = inputs.to(model.device, torch.float16)\n \n # autoregressively complete prompt\n output = model.generate(**inputs, max_new_tokens=100)"
        },
        {
            "sha": "1ad9742834adcbf87924fdd7ff10ffc7465bf2bb",
            "filename": "docs/source/en/model_doc/marian.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fmodel_doc%2Fmarian.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fmodel_doc%2Fmarian.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmarian.md?ref=7c1169e21fcf953b9670ac1e79c6e157bcaedc5c",
            "patch": "@@ -69,7 +69,7 @@ from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")\n model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\", torch_dtype=torch.float16, attn_implementation=\"sdpa\", device_map=\"auto\")\n \n-inputs = tokenizer(\"Hello, how are you?\", return_tensors=\"pt\").to(\"cuda\")\n+inputs = tokenizer(\"Hello, how are you?\", return_tensors=\"pt\").to(model.device)\n outputs = model.generate(**inputs, cache_implementation=\"static\")\n print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n "
        },
        {
            "sha": "be81143925cb58933caca0176e0d3304fb22b531",
            "filename": "docs/source/en/model_doc/mistral.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fmodel_doc%2Fmistral.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fmodel_doc%2Fmistral.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmistral.md?ref=7c1169e21fcf953b9670ac1e79c6e157bcaedc5c",
            "patch": "@@ -69,7 +69,7 @@ The example below demonstrates how to chat with [`Pipeline`] or the [`AutoModel`\n ...     {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n ... ]\n \n->>> model_inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n+>>> model_inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(model.device)\n \n >>> generated_ids = model.generate(model_inputs, max_new_tokens=100, do_sample=True)\n >>> tokenizer.batch_decode(generated_ids)[0]\n@@ -113,7 +113,7 @@ The example below uses [bitsandbytes](../quantization/bitsandbytes) to only quan\n ...     {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n ... ]\n \n->>> model_inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n+>>> model_inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(model.device)\n \n >>> generated_ids = model.generate(model_inputs, max_new_tokens=100, do_sample=True)\n >>> tokenizer.batch_decode(generated_ids)[0]"
        },
        {
            "sha": "f0f8da72f3919110ecde774a55d5395a007ecb60",
            "filename": "docs/source/en/model_doc/mobilebert.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilebert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilebert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilebert.md?ref=7c1169e21fcf953b9670ac1e79c6e157bcaedc5c",
            "patch": "@@ -62,7 +62,7 @@ model = AutoModelForMaskedLM.from_pretrained(\n     torch_dtype=torch.float16,\n     device_map=\"auto\",\n )\n-inputs = tokenizer(\"The capital of France is [MASK].\", return_tensors=\"pt\").to(\"cuda\")\n+inputs = tokenizer(\"The capital of France is [MASK].\", return_tensors=\"pt\").to(model.device)\n \n with torch.no_grad():\n     outputs = model(**inputs)"
        },
        {
            "sha": "103f8047d39845b2e7aea34bafb3b85f0c7e6c07",
            "filename": "docs/source/en/model_doc/nemotron.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fmodel_doc%2Fnemotron.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fmodel_doc%2Fnemotron.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fnemotron.md?ref=7c1169e21fcf953b9670ac1e79c6e157bcaedc5c",
            "patch": "@@ -57,13 +57,13 @@ The following code provides an example of how to load the Minitron-4B model and\n \n ```python\n import torch\n-from transformers import AutoTokenizer, AutoModelForCausalLM\n+from transformers import AutoTokenizer, AutoModelForCausalLM, infer_device\n \n # Load the tokenizer and model\n model_path = 'nvidia/Minitron-4B-Base'\n tokenizer  = AutoTokenizer.from_pretrained(model_path)\n \n-device = 'cuda'\n+device = infer_device()\n dtype  = torch.bfloat16\n model  = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=dtype, device_map=device)\n \n@@ -150,4 +150,4 @@ If you find our work helpful, please consider citing our paper:\n ## NemotronForTokenClassification\n \n [[autodoc]] NemotronForTokenClassification\n-    - forward\n\\ No newline at end of file\n+    - forward"
        },
        {
            "sha": "9e5d8c06e2ec54d066cfb5ef97ada8771a8a5837",
            "filename": "docs/source/en/model_doc/nllb.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fmodel_doc%2Fnllb.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fmodel_doc%2Fnllb.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fnllb.md?ref=7c1169e21fcf953b9670ac1e79c6e157bcaedc5c",
            "patch": "@@ -89,7 +89,7 @@ model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-1.3B\"\n tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-1.3B\")\n \n article = \"UN Chief says there is no military solution in Syria\"\n-inputs = tokenizer(article, return_tensors=\"pt\").to(\"cuda\")\n+inputs = tokenizer(article, return_tensors=\"pt\").to(model.device)\n translated_tokens = model.generate(\n     **inputs, forced_bos_token_id=tokenizer.convert_tokens_to_ids(\"fra_Latn\"), max_length=30,\n )"
        },
        {
            "sha": "d13bdd3353baa48094e3fb8683a27413822a6bf4",
            "filename": "docs/source/en/model_doc/opt.md",
            "status": "modified",
            "additions": 6,
            "deletions": 10,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fmodel_doc%2Fopt.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fmodel_doc%2Fopt.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fopt.md?ref=7c1169e21fcf953b9670ac1e79c6e157bcaedc5c",
            "patch": "@@ -55,15 +55,12 @@ pipeline(\"Once upon a time, in a land far, far away,\", max_length=50, num_return\n import torch\n from transformers import AutoModelForCausalLM, AutoTokenizer\n \n-device = \"cuda\"\n-\n-model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", torch_dtype=torch.float16, attn_implementation=\"sdpa\")\n+model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", torch_dtype=torch.float16, device_map=\"auto\", attn_implementation=\"sdpa\")\n tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n \n prompt = (\"Once upon a time, in a land far, far away, \")\n \n-model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n-model.to(device)\n+model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(model.device)\n \n generated_ids = model.generate(**model_inputs, max_new_tokens=30, do_sample=False)\n tokenizer.batch_decode(generated_ids)[0]\n@@ -83,18 +80,17 @@ The example below uses [bitsandbytes](..quantization/bitsandbytes) to quantize t\n \n ```py\n import torch\n-from transformers import BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM\n+from transformers import BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM, infer_device\n \n-device = \"cuda\"\n+device = infer_device()\n \n bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n-model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-13b\", torch_dtype=torch.float16, attn_implementation=\"sdpa\", quantization_config=bnb_config)\n+model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-13b\", torch_dtype=torch.float16, attn_implementation=\"sdpa\", quantization_config=bnb_config).to(device)\n tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-13b\")\n \n prompt = (\"Once upon a time, in a land far, far away, \")\n \n-model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n-model.to(device)\n+model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(model.device)\n \n generated_ids = model.generate(**model_inputs, max_new_tokens=30, do_sample=False)\n tokenizer.batch_decode(generated_ids)[0]"
        },
        {
            "sha": "f1aaa6dbed35845054297a87299e78b4593a6a40",
            "filename": "docs/source/en/model_doc/pegasus_x.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fmodel_doc%2Fpegasus_x.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fmodel_doc%2Fpegasus_x.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fpegasus_x.md?ref=7c1169e21fcf953b9670ac1e79c6e157bcaedc5c",
            "patch": "@@ -73,7 +73,7 @@ input_text = \"\"\"Plants are among the most remarkable and essential life forms on\n Through photosynthesis, plants capture energy from sunlight using a green pigment called chlorophyll, which is located in specialized cell structures called chloroplasts. In the presence of light, plants absorb carbon dioxide from the atmosphere through small pores in their leaves called stomata, and take in water from the soil through their root systems.\n These ingredients are then transformed into glucose, a type of sugar that serves as a source of chemical energy, and oxygen, which is released as a byproduct into the atmosphere. The glucose produced during photosynthesis is not just used immediately; plants also store it as starch or convert it into other organic compounds like cellulose, which is essential for building their cellular structure.\n This energy reserve allows them to grow, develop leaves, produce flowers, bear fruit, and carry out various physiological processes throughout their lifecycle.\"\"\"\n-input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n+input_ids = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n \n output = model.generate(**input_ids, cache_implementation=\"static\")\n print(tokenizer.decode(output[0], skip_special_tokens=True))\n@@ -115,7 +115,7 @@ input_text = \"\"\"Plants are among the most remarkable and essential life forms on\n Through photosynthesis, plants capture energy from sunlight using a green pigment called chlorophyll, which is located in specialized cell structures called chloroplasts. In the presence of light, plants absorb carbon dioxide from the atmosphere through small pores in their leaves called stomata, and take in water from the soil through their root systems.\n These ingredients are then transformed into glucose, a type of sugar that serves as a source of chemical energy, and oxygen, which is released as a byproduct into the atmosphere. The glucose produced during photosynthesis is not just used immediately; plants also store it as starch or convert it into other organic compounds like cellulose, which is essential for building their cellular structure.\n This energy reserve allows them to grow, develop leaves, produce flowers, bear fruit, and carry out various physiological processes throughout their lifecycle.\"\"\"\n-input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n+input_ids = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n \n output = model.generate(**input_ids, cache_implementation=\"static\")\n print(tokenizer.decode(output[0], skip_special_tokens=True))"
        },
        {
            "sha": "3b44db55363fa6d077be18a103bd8a1619a4db32",
            "filename": "docs/source/en/model_doc/phi4_multimodal.md",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fmodel_doc%2Fphi4_multimodal.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fmodel_doc%2Fphi4_multimodal.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fphi4_multimodal.md?ref=7c1169e21fcf953b9670ac1e79c6e157bcaedc5c",
            "patch": "@@ -47,10 +47,10 @@ print(result[0]['generated_text'])\n \n ```python\n import torch\n-from transformers import AutoModelForCausalLM, AutoProcessor, GenerationConfig\n+from transformers import AutoModelForCausalLM, AutoProcessor, GenerationConfig, infer_device\n \n model_path = \"microsoft/Phi-4-multimodal-instruct\"\n-device = \"cuda:0\"\n+device = f\"{infer_device()}:0\"\n \n processor = AutoProcessor.from_pretrained(model_path)\n model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device, torch_dtype=torch.float16)\n@@ -74,7 +74,7 @@ inputs = processor.apply_chat_template(\n     tokenize=True,\n     return_dict=True,\n     return_tensors=\"pt\",\n-).to(device)\n+).to(model.device)\n \n generate_ids = model.generate(\n     **inputs,\n@@ -97,10 +97,10 @@ The example below demonstrates inference with an audio and text input.\n \n ```py\n import torch\n-from transformers import AutoModelForCausalLM, AutoProcessor, GenerationConfig\n+from transformers import AutoModelForCausalLM, AutoProcessor, GenerationConfig, infer_device\n \n model_path = \"microsoft/Phi-4-multimodal-instruct\"\n-device = \"cuda:0\"\n+device = f\"{infer_device()}:0\"\n \n processor = AutoProcessor.from_pretrained(model_path)\n model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device,  torch_dtype=torch.float16)\n@@ -124,7 +124,7 @@ inputs = processor.apply_chat_template(\n     tokenize=True,\n     return_dict=True,\n     return_tensors=\"pt\",\n-).to(device)\n+).to(model.device)\n \n generate_ids = model.generate(\n     **inputs,"
        },
        {
            "sha": "4c934d92d5fcc4924a38b91daa0627f8c8937f3f",
            "filename": "docs/source/en/model_doc/qdqbert.md",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fmodel_doc%2Fqdqbert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fmodel_doc%2Fqdqbert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqdqbert.md?ref=7c1169e21fcf953b9670ac1e79c6e157bcaedc5c",
            "patch": "@@ -102,8 +102,10 @@ tensors. After setting up the tensor quantizers, one can use the following examp\n ...         module.load_calib_amax()\n ...         module.enable_quant()\n \n->>> # If running on GPU, it needs to call .cuda() again because new tensors will be created by calibration process\n->>> model.cuda()\n+>>> # If running on accelerator, it needs to call `.to(xx)` again because new tensors will be created by calibration process\n+>>> from transformers import infer_device\n+>>> device = infer_device()\n+>>> model.to(device)\n \n >>> # Keep running the quantized model\n >>> # ..."
        },
        {
            "sha": "230bab508d7cd61983a719e72fc61b4cfe8402e5",
            "filename": "docs/source/en/model_doc/qwen2_5_vl.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_5_vl.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_5_vl.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_5_vl.md?ref=7c1169e21fcf953b9670ac1e79c6e157bcaedc5c",
            "patch": "@@ -99,7 +99,7 @@ inputs = processor.apply_chat_template(\n     tokenize=True,\n     return_dict=True,\n     return_tensors=\"pt\"\n-).to(\"cuda\")\n+).to(model.device)\n \n generated_ids = model.generate(**inputs, max_new_tokens=128)\n generated_ids_trimmed = ["
        },
        {
            "sha": "7cdcd52119c0b947757a399e2b42a791137c5b12",
            "filename": "docs/source/en/model_doc/qwen2_audio.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_audio.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_audio.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_audio.md?ref=7c1169e21fcf953b9670ac1e79c6e157bcaedc5c",
            "patch": "@@ -109,7 +109,7 @@ for message in conversation:\n                 )\n \n inputs = processor(text=text, audios=audios, return_tensors=\"pt\", padding=True)\n-inputs.input_ids = inputs.input_ids.to(\"cuda\")\n+inputs.input_ids = inputs.input_ids.to(model.device)\n \n generate_ids = model.generate(**inputs, max_length=256)\n generate_ids = generate_ids[:, inputs.input_ids.size(1):]\n@@ -157,7 +157,7 @@ for message in conversation:\n                 )\n \n inputs = processor(text=text, audios=audios, return_tensors=\"pt\", padding=True)\n-inputs.input_ids = inputs.input_ids.to(\"cuda\")\n+inputs.input_ids = inputs.input_ids.to(model.device)\n \n generate_ids = model.generate(**inputs, max_length=256)\n generate_ids = generate_ids[:, inputs.input_ids.size(1):]\n@@ -212,8 +212,8 @@ for conversation in conversations:\n                     )\n \n inputs = processor(text=text, audios=audios, return_tensors=\"pt\", padding=True)\n-inputs['input_ids'] = inputs['input_ids'].to(\"cuda\")\n-inputs.input_ids = inputs.input_ids.to(\"cuda\")\n+inputs['input_ids'] = inputs['input_ids'].to(model.device)\n+inputs.input_ids = inputs.input_ids.to(model.device)\n \n generate_ids = model.generate(**inputs, max_length=256)\n generate_ids = generate_ids[:, inputs.input_ids.size(1):]"
        },
        {
            "sha": "546aa0a0ca88b75b65ed927f801227e0b6186c39",
            "filename": "docs/source/en/model_doc/sam2.md",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam2.md?ref=7c1169e21fcf953b9670ac1e79c6e157bcaedc5c",
            "patch": "@@ -67,12 +67,12 @@ SAM2 can be used for automatic mask generation to segment all objects in an imag\n You can segment objects by providing a single point click on the object you want to segment:\n \n ```python\n->>> from transformers import Sam2Processor, Sam2Model\n+>>> from transformers import Sam2Processor, Sam2Model, infer_device\n >>> import torch\n >>> from PIL import Image\n >>> import requests\n \n->>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+>>> device = infer_device()\n \n >>> model = Sam2Model.from_pretrained(\"facebook/sam2.1-hiera-large\").to(device)\n >>> processor = Sam2Processor.from_pretrained(\"facebook/sam2.1-hiera-large\")\n@@ -83,7 +83,7 @@ You can segment objects by providing a single point click on the object you want\n >>> input_points = [[[[500, 375]]]]  # Single point click, 4 dimensions (image_dim, object_dim, point_per_object_dim, coordinates)\n >>> input_labels = [[[1]]]  # 1 for positive click, 0 for negative click, 3 dimensions (image_dim, object_dim, point_label)\n \n->>> inputs = processor(images=raw_image, input_points=input_points, input_labels=input_labels, return_tensors=\"pt\").to(device)\n+>>> inputs = processor(images=raw_image, input_points=input_points, input_labels=input_labels, return_tensors=\"pt\").to(model.device)\n \n >>> with torch.no_grad():\n ...     outputs = model(**inputs)\n@@ -155,12 +155,12 @@ Generated masks for 2 objects\n Process multiple images simultaneously for improved efficiency:\n \n ```python\n->>> from transformers import Sam2Processor, Sam2Model\n+>>> from transformers import Sam2Processor, Sam2Model, infer_device\n >>> import torch\n >>> from PIL import Image\n >>> import requests\n \n->>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+>>> device = infer_device()\n \n >>> model = Sam2Model.from_pretrained(\"facebook/sam2.1-hiera-large\").to(device)\n >>> processor = Sam2Processor.from_pretrained(\"facebook/sam2.1-hiera-large\")\n@@ -176,7 +176,7 @@ Process multiple images simultaneously for improved efficiency:\n >>> input_points = [[[[500, 375]]], [[[770, 200]]]]  # One point for each image\n >>> input_labels = [[[1]], [[1]]]  # Positive clicks for both images\n \n->>> inputs = processor(images=raw_images, input_points=input_points, input_labels=input_labels, return_tensors=\"pt\").to(device)\n+>>> inputs = processor(images=raw_images, input_points=input_points, input_labels=input_labels, return_tensors=\"pt\").to(model.device)\n \n >>> with torch.no_grad():\n ...     outputs = model(**inputs, multimask_output=False)"
        },
        {
            "sha": "577d63b2afeb64f4fd88df6083f94ba79e342435",
            "filename": "docs/source/en/model_doc/siglip.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fmodel_doc%2Fsiglip.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fmodel_doc%2Fsiglip.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsiglip.md?ref=7c1169e21fcf953b9670ac1e79c6e157bcaedc5c",
            "patch": "@@ -67,7 +67,7 @@ url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/\n image = Image.open(requests.get(url, stream=True).raw)\n candidate_labels = [\"a Pallas cat\", \"a lion\", \"a Siberian tiger\"]\n texts = [f'This is a photo of {label}.' for label in candidate_labels]\n-inputs = processor(text=texts, images=image, padding=\"max_length\", return_tensors=\"pt\").to(\"cuda\")\n+inputs = processor(text=texts, images=image, padding=\"max_length\", return_tensors=\"pt\").to(model.device)\n \n with torch.no_grad():\n     outputs = model(**inputs)\n@@ -98,7 +98,7 @@ url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/\n image = Image.open(requests.get(url, stream=True).raw)\n candidate_labels = [\"a Pallas cat\", \"a lion\", \"a Siberian tiger\"]\n texts = [f'This is a photo of {label}.' for label in candidate_labels]\n-inputs = processor(text=texts, images=image, padding=\"max_length\", return_tensors=\"pt\").to(\"cuda\")\n+inputs = processor(text=texts, images=image, padding=\"max_length\", return_tensors=\"pt\").to(model.device)\n \n with torch.no_grad():\n     outputs = model(**inputs)"
        },
        {
            "sha": "a3e4b93e7faae4b172be964f4f92b06753900ab6",
            "filename": "docs/source/en/model_doc/t5.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fmodel_doc%2Ft5.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fmodel_doc%2Ft5.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ft5.md?ref=7c1169e21fcf953b9670ac1e79c6e157bcaedc5c",
            "patch": "@@ -66,7 +66,7 @@ model = AutoModelForSeq2SeqLM.from_pretrained(\n     device_map=\"auto\"\n     )\n \n-input_ids = tokenizer(\"translate English to French: The weather is nice today.\", return_tensors=\"pt\").to(\"cuda\")\n+input_ids = tokenizer(\"translate English to French: The weather is nice today.\", return_tensors=\"pt\").to(model.device)\n \n output = model.generate(**input_ids, cache_implementation=\"static\")\n print(tokenizer.decode(output[0], skip_special_tokens=True))\n@@ -100,7 +100,7 @@ model = AutoModelForSeq2SeqLM.from_pretrained(\n )\n \n tokenizer = AutoTokenizer.from_pretrained(\"google/t5-v1_1-xl\")\n-input_ids = tokenizer(\"translate English to French: The weather is nice today.\", return_tensors=\"pt\").to(\"cuda\")\n+input_ids = tokenizer(\"translate English to French: The weather is nice today.\", return_tensors=\"pt\").to(model.device)\n \n output = model.generate(**input_ids, cache_implementation=\"static\")\n print(tokenizer.decode(output[0], skip_special_tokens=True))"
        },
        {
            "sha": "4599061403fdbc3b6eabc57a61e834c3fbe0dc8f",
            "filename": "docs/source/en/model_doc/vit.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fmodel_doc%2Fvit.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fmodel_doc%2Fvit.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvit.md?ref=7c1169e21fcf953b9670ac1e79c6e157bcaedc5c",
            "patch": "@@ -70,7 +70,7 @@ model = AutoModelForImageClassification.from_pretrained(\n )\n url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n image = Image.open(requests.get(url, stream=True).raw)\n-inputs = image_processor(image, return_tensors=\"pt\").to(\"cuda\")\n+inputs = image_processor(image, return_tensors=\"pt\").to(model.device)\n \n with torch.no_grad():\n   logits = model(**inputs).logits"
        },
        {
            "sha": "524c09da6f56da1617554a4c7fe1d2eb1989d070",
            "filename": "docs/source/en/model_doc/vitpose.md",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fmodel_doc%2Fvitpose.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fmodel_doc%2Fvitpose.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvitpose.md?ref=7c1169e21fcf953b9670ac1e79c6e157bcaedc5c",
            "patch": "@@ -36,9 +36,9 @@ import requests\n import numpy as np\n import supervision as sv\n from PIL import Image\n-from transformers import AutoProcessor, RTDetrForObjectDetection, VitPoseForPoseEstimation\n+from transformers import AutoProcessor, RTDetrForObjectDetection, VitPoseForPoseEstimation, infer_device\n \n-device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+device = infer_device()\n \n url = \"https://www.fcbarcelona.com/fcbarcelona/photo/2021/01/31/3c55a19f-dfc1-4451-885e-afd14e890a11/mini_2021-01-31-BARCELONA-ATHLETIC-BILBAOI-30.JPG\"\n image = Image.open(requests.get(url, stream=True).raw)\n@@ -47,7 +47,7 @@ image = Image.open(requests.get(url, stream=True).raw)\n person_image_processor = AutoProcessor.from_pretrained(\"PekingU/rtdetr_r50vd_coco_o365\")\n person_model = RTDetrForObjectDetection.from_pretrained(\"PekingU/rtdetr_r50vd_coco_o365\", device_map=device)\n \n-inputs = person_image_processor(images=image, return_tensors=\"pt\").to(device)\n+inputs = person_image_processor(images=image, return_tensors=\"pt\").to(person_model.device)\n \n with torch.no_grad():\n     outputs = person_model(**inputs)\n@@ -69,7 +69,7 @@ person_boxes[:, 3] = person_boxes[:, 3] - person_boxes[:, 1]\n image_processor = AutoProcessor.from_pretrained(\"usyd-community/vitpose-base-simple\")\n model = VitPoseForPoseEstimation.from_pretrained(\"usyd-community/vitpose-base-simple\", device_map=device)\n \n-inputs = image_processor(image, boxes=[person_boxes], return_tensors=\"pt\").to(device)\n+inputs = image_processor(image, boxes=[person_boxes], return_tensors=\"pt\").to(model.device)\n \n with torch.no_grad():\n     outputs = model(**inputs)\n@@ -162,14 +162,14 @@ image_pose_result = pose_results[0]\n - ViTPose++ has 6 different MoE expert heads (COCO validation `0`, AiC `1`, MPII `2`, AP-10K `3`, APT-36K `4`, COCO-WholeBody `5`) which supports 6 different datasets. Pass a specific value corresponding to the dataset to the `dataset_index` to indicate which expert to use.\n \n     ```py\n-    from transformers import AutoProcessor, VitPoseForPoseEstimation\n+    from transformers import AutoProcessor, VitPoseForPoseEstimation, infer_device\n \n-    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+    device = infer_device()\n \n     image_processor = AutoProcessor.from_pretrained(\"usyd-community/vitpose-plus-base\")\n     model = VitPoseForPoseEstimation.from_pretrained(\"usyd-community/vitpose-plus-base\", device=device)\n \n-    inputs = image_processor(image, boxes=[person_boxes], return_tensors=\"pt\").to(device)\n+    inputs = image_processor(image, boxes=[person_boxes], return_tensors=\"pt\").to(model.device)\n     dataset_index = torch.tensor([0], device=device) # must be a tensor of shape (batch_size,)\n \n     with torch.no_grad():\n@@ -300,4 +300,4 @@ Refer to resources below to learn more about using ViTPose.\n ## VitPoseForPoseEstimation\n \n [[autodoc]] VitPoseForPoseEstimation\n-    - forward\n\\ No newline at end of file\n+    - forward"
        },
        {
            "sha": "663354c25ac36e7ea9055ed493178c3a4ef193bd",
            "filename": "docs/source/en/model_doc/whisper.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fmodel_doc%2Fwhisper.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fmodel_doc%2Fwhisper.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fwhisper.md?ref=7c1169e21fcf953b9670ac1e79c6e157bcaedc5c",
            "patch": "@@ -71,7 +71,7 @@ model = WhisperForConditionalGeneration.from_pretrained(\n     torch_dtype=torch.float16,\n     device_map=\"auto\",\n     attn_implementation=\"sdpa\"\n-).to(\"cuda\")\n+)\n \n ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n audio_sample = ds[0][\"audio\"]\n@@ -81,7 +81,7 @@ input_features = processor(\n     sampling_rate=audio_sample[\"sampling_rate\"],\n     return_tensors=\"pt\"\n ).input_features\n-input_features = input_features.to(\"cuda\", dtype=torch.float16)\n+input_features = input_features.to(model.device, dtype=torch.float16)\n \n predicted_ids = model.generate(input_features, cache_implementation=\"static\")\n transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)"
        },
        {
            "sha": "fc315b07a6f6e9e012033dd1b1054809581bd2bd",
            "filename": "docs/source/en/model_doc/xlm-roberta-xl.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlm-roberta-xl.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlm-roberta-xl.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlm-roberta-xl.md?ref=7c1169e21fcf953b9670ac1e79c6e157bcaedc5c",
            "patch": "@@ -65,7 +65,7 @@ model = AutoModelForMaskedLM.from_pretrained(\n     device_map=\"auto\",  \n     attn_implementation=\"sdpa\"  \n )  \n-inputs = tokenizer(\"Bonjour, je suis un modèle <mask>.\", return_tensors=\"pt\").to(\"cuda\")  \n+inputs = tokenizer(\"Bonjour, je suis un modèle <mask>.\", return_tensors=\"pt\").to(model.device)  \n \n with torch.no_grad():  \n     outputs = model(**inputs)  \n@@ -106,7 +106,7 @@ model = AutoModelForMaskedLM.from_pretrained(\n     attn_implementation=\"sdpa\",\n     quantization_config=quantization_config\n )\n-inputs = tokenizer(\"Bonjour, je suis un modèle <mask>.\", return_tensors=\"pt\").to(\"cuda\")\n+inputs = tokenizer(\"Bonjour, je suis un modèle <mask>.\", return_tensors=\"pt\").to(model.device)\n \n with torch.no_grad():\n     outputs = model(**inputs)"
        },
        {
            "sha": "74aeeefd4af395fb21eeeb5637d5df11ef99094d",
            "filename": "docs/source/en/model_doc/xlm-roberta.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlm-roberta.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlm-roberta.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlm-roberta.md?ref=7c1169e21fcf953b9670ac1e79c6e157bcaedc5c",
            "patch": "@@ -68,7 +68,7 @@ model = AutoModelForMaskedLM.from_pretrained(\n )\n \n # Prepare input\n-inputs = tokenizer(\"Bonjour, je suis un modèle <mask>.\", return_tensors=\"pt\").to(\"cuda\")\n+inputs = tokenizer(\"Bonjour, je suis un modèle <mask>.\", return_tensors=\"pt\").to(model.device)\n \n with torch.no_grad():\n     outputs = model(**inputs)\n@@ -113,7 +113,7 @@ model = AutoModelForMaskedLM.from_pretrained(\n     quantization_config=quantization_config\n )\n \n-inputs = tokenizer(\"Bonjour, je suis un modèle <mask>.\", return_tensors=\"pt\").to(\"cuda\")\n+inputs = tokenizer(\"Bonjour, je suis un modèle <mask>.\", return_tensors=\"pt\").to(model.device)\n outputs = model.generate(**inputs, max_new_tokens=100)\n print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n ```"
        },
        {
            "sha": "b1cc47ed7280df976ece1556ad57411b8d7e4616",
            "filename": "docs/source/en/model_doc/xlm.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlm.md?ref=7c1169e21fcf953b9670ac1e79c6e157bcaedc5c",
            "patch": "@@ -63,7 +63,7 @@ model = AutoModelForMaskedLM.from_pretrained(\n     torch_dtype=torch.float16,\n     device_map=\"auto\",\n )\n-inputs = tokenizer(\"Hello, I'm a <mask> model.\", return_tensors=\"pt\").to(\"cuda\")\n+inputs = tokenizer(\"Hello, I'm a <mask> model.\", return_tensors=\"pt\").to(model.device)\n \n with torch.no_grad():\n     outputs = model(**inputs)"
        },
        {
            "sha": "0741838e2eab38e49278316f862187fd5c438b64",
            "filename": "docs/source/en/perf_infer_gpu_one.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md?ref=7c1169e21fcf953b9670ac1e79c6e157bcaedc5c",
            "patch": "@@ -48,7 +48,7 @@ tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B\")\n model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B\", device_map=\"auto\", quantization_config=quantization_config)\n \n prompt = \"Hello, my llama is cute\"\n-inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n+inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n generated_ids = model.generate(**inputs)\n outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n ```\n@@ -197,10 +197,10 @@ from torch.nn.attention import SDPBackend, sdpa_kernel\n from transformers import AutoModelForCausalLM, AutoTokenizer\n \n tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B\")\n-model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B\", device_map=\"auto\").to(\"cuda\")\n+model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B\", device_map=\"auto\")\n \n input_text = \"Hello, my llama is cute\"\n-inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n+inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n \n with sdpa_kernel(SDPBackend.FLASH_ATTENTION):\n     outputs = model.generate(**inputs)"
        },
        {
            "sha": "5e38334f6de5ac79311222758a32b561eb290a44",
            "filename": "docs/source/en/pipeline_tutorial.md",
            "status": "modified",
            "additions": 16,
            "deletions": 8,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fpipeline_tutorial.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fpipeline_tutorial.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fpipeline_tutorial.md?ref=7c1169e21fcf953b9670ac1e79c6e157bcaedc5c",
            "patch": "@@ -39,9 +39,11 @@ pipeline(\"the secret to baking a really good cake is \")\n When you have more than one input, pass them as a list.\n \n ```py\n-from transformers import pipeline\n+from transformers import pipeline, infer_device\n+\n+device = infer_device()\n \n-pipeline = pipeline(task=\"text-generation\", model=\"google/gemma-2-2b\", device=\"cuda\")\n+pipeline = pipeline(task=\"text-generation\", model=\"google/gemma-2-2b\", device=device)\n pipeline([\"the secret to baking a really good cake is \", \"a baguette is \"])\n [[{'generated_text': 'the secret to baking a really good cake is 1. the right ingredients 2. the'}],\n  [{'generated_text': 'a baguette is 100% bread.\\n\\na baguette is 100%'}]]\n@@ -171,9 +173,11 @@ pipeline(\"the secret to baking a really good cake is \")\n In the example below, when there are 4 inputs and `batch_size` is set to 2, [`Pipeline`] passes a batch of 2 inputs to the model at a time.\n \n ```py\n-from transformers import pipeline\n+from transformers import pipeline, infer_device()\n+\n+device = infer_device()\n \n-pipeline = pipeline(task=\"text-generation\", model=\"google/gemma-2-2b\", device=\"cuda\", batch_size=2)\n+pipeline = pipeline(task=\"text-generation\", model=\"google/gemma-2-2b\", device=device, batch_size=2)\n pipeline([\"the secret to baking a really good cake is\", \"a baguette is\", \"paris is the\", \"hotdogs are\"])\n [[{'generated_text': 'the secret to baking a really good cake is to use a good cake mix.\\n\\ni’'}],\n  [{'generated_text': 'a baguette is'}],\n@@ -184,13 +188,15 @@ pipeline([\"the secret to baking a really good cake is\", \"a baguette is\", \"paris\n Another good use case for batch inference is for streaming data in [`Pipeline`].\n \n ```py\n-from transformers import pipeline\n+from transformers import pipeline, infer_device\n from transformers.pipelines.pt_utils import KeyDataset\n import datasets\n \n+device = infer_device()\n+\n # KeyDataset is a utility that returns the item in the dict returned by the dataset\n dataset = datasets.load_dataset(\"imdb\", name=\"plain_text\", split=\"unsupervised\")\n-pipeline = pipeline(task=\"text-classification\", model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\", device=\"cuda\")\n+pipeline = pipeline(task=\"text-classification\", model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\", device=device)\n for out in pipeline(KeyDataset(dataset, \"text\"), batch_size=8, truncation=\"only_first\"):\n     print(out)\n ```\n@@ -296,11 +302,13 @@ For inference with large datasets, you can iterate directly over the dataset its\n \n ```py\n from transformers.pipelines.pt_utils import KeyDataset\n-from transformers import pipeline\n+from transformers import pipeline, infer_device\n from datasets import load_dataset\n \n+device = infer_device()\n+\n dataset = datasets.load_dataset(\"imdb\", name=\"plain_text\", split=\"unsupervised\")\n-pipeline = pipeline(task=\"text-classification\", model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\", device=\"cuda\")\n+pipeline = pipeline(task=\"text-classification\", model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\", device=device)\n for out in pipeline(KeyDataset(dataset, \"text\"), batch_size=8, truncation=\"only_first\"):\n     print(out)\n ```"
        },
        {
            "sha": "bf47ea238e9bca478cfc69da8c099c0f4942fb17",
            "filename": "docs/source/en/quantization/awq.md",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fquantization%2Fawq.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fquantization%2Fawq.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Fawq.md?ref=7c1169e21fcf953b9670ac1e79c6e157bcaedc5c",
            "patch": "@@ -51,16 +51,18 @@ Identify an AWQ-quantized model by checking the `quant_method` key in the models\n \n Load the AWQ-quantized model with [`~PreTrainedModel.from_pretrained`]. This automatically sets the other weights to fp16 by default for performance reasons. Use the `torch_dtype` parameter to load these other weights in a different format.\n \n-If the model is loaded on the CPU, use the `device_map` parameter to move it to a GPU.\n+If the model is loaded on the CPU, use the `device_map` parameter to move it to an accelerator.\n \n ```py\n-from transformers import AutoModelForCausalLM, AutoTokenizer\n+from transformers import AutoModelForCausalLM, AutoTokenizer, infer_device\n import torch\n \n+device = f\"{infer_device()}:0\"\n+\n model = AutoModelForCausalLM.from_pretrained(\n   \"TheBloke/zephyr-7B-alpha-AWQ\",\n   torch_dtype=torch.float32,\n-  device_map=\"cuda:0\"\n+  device_map=device\n )\n ```\n "
        },
        {
            "sha": "b8c020f0b5fb83c512ab3053753fe633841a152c",
            "filename": "docs/source/en/quantization/torchao.md",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fquantization%2Ftorchao.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Fquantization%2Ftorchao.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Ftorchao.md?ref=7c1169e21fcf953b9670ac1e79c6e157bcaedc5c",
            "patch": "@@ -112,7 +112,7 @@ quantized_model = AutoModelForCausalLM.from_pretrained(\n \n tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n input_text = \"What are we having for dinner?\"\n-input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n+input_ids = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n \n # auto-compile the quantized model with `cache_implementation=\"static\"` to get speed up\n output = quantized_model.generate(**input_ids, max_new_tokens=10, cache_implementation=\"static\")\n@@ -140,7 +140,7 @@ quantized_model = AutoModelForCausalLM.from_pretrained(\n \n tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n input_text = \"What are we having for dinner?\"\n-input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n+input_ids = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n \n # auto-compile the quantized model with `cache_implementation=\"static\"` to get speed up\n output = quantized_model.generate(**input_ids, max_new_tokens=10, cache_implementation=\"static\")\n@@ -165,13 +165,13 @@ quantization_config = TorchAoConfig(quant_type=quant_config)\n quantized_model = AutoModelForCausalLM.from_pretrained(\n     \"RedHatAI/Sparse-Llama-3.1-8B-2of4\",\n     torch_dtype=torch.float16,\n-    device_map=\"cuda\",\n+    device_map=\"auto\",\n     quantization_config=quantization_config\n )\n \n tokenizer = AutoTokenizer.from_pretrained(\"RedHatAI/Sparse-Llama-3.1-8B-2of4\")\n input_text = \"What are we having for dinner?\"\n-input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n+input_ids = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n \n # auto-compile the quantized model with `cache_implementation=\"static\"` to get speed up\n output = quantized_model.generate(**input_ids, max_new_tokens=10, cache_implementation=\"static\")\n@@ -204,7 +204,7 @@ quantized_model = AutoModelForCausalLM.from_pretrained(\n \n tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n input_text = \"What are we having for dinner?\"\n-input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n+input_ids = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n \n # auto-compile the quantized model with `cache_implementation=\"static\"` to get speed up\n output = quantized_model.generate(**input_ids, max_new_tokens=10, cache_implementation=\"static\")\n@@ -239,7 +239,7 @@ quantized_model = AutoModelForCausalLM.from_pretrained(\n \n tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n input_text = \"What are we having for dinner?\"\n-input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n+input_ids = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n \n # auto-compile the quantized model with `cache_implementation=\"static\"` to get speed up\n output = quantized_model.generate(**input_ids, max_new_tokens=10, cache_implementation=\"static\")\n@@ -264,13 +264,13 @@ quantization_config = TorchAoConfig(quant_type=quant_config)\n quantized_model = AutoModelForCausalLM.from_pretrained(\n     \"RedHatAI/Sparse-Llama-3.1-8B-2of4\",\n     torch_dtype=torch.float16,\n-    device_map=\"cuda\",\n+    device_map=\"auto\",\n     quantization_config=quantization_config\n )\n \n tokenizer = AutoTokenizer.from_pretrained(\"RedHatAI/Sparse-Llama-3.1-8B-2of4\")\n input_text = \"What are we having for dinner?\"\n-input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n+input_ids = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n \n # auto-compile the quantized model with `cache_implementation=\"static\"` to get speed up\n output = quantized_model.generate(**input_ids, max_new_tokens=10, cache_implementation=\"static\")\n@@ -303,7 +303,7 @@ quantized_model = AutoModelForCausalLM.from_pretrained(\n \n tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n input_text = \"What are we having for dinner?\"\n-input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"xpu\")\n+input_ids = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n \n # auto-compile the quantized model with `cache_implementation=\"static\"` to get speed up\n output = quantized_model.generate(**input_ids, max_new_tokens=10, cache_implementation=\"static\")\n@@ -334,7 +334,7 @@ quantized_model = AutoModelForCausalLM.from_pretrained(\n \n tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n input_text = \"What are we having for dinner?\"\n-input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"xpu\")\n+input_ids = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n \n # auto-compile the quantized model with `cache_implementation=\"static\"` to get speed up\n output = quantized_model.generate(**input_ids, max_new_tokens=10, cache_implementation=\"static\")"
        },
        {
            "sha": "eee3a8d46f2257b8fb4950d4ea3abc9228c6b2cd",
            "filename": "docs/source/en/tasks/image_text_to_text.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Ftasks%2Fimage_text_to_text.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Ftasks%2Fimage_text_to_text.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fimage_text_to_text.md?ref=7c1169e21fcf953b9670ac1e79c6e157bcaedc5c",
            "patch": "@@ -38,10 +38,10 @@ pip install -q transformers accelerate flash_attn\n Let's initialize the model and the processor.\n \n ```python\n-from transformers import AutoProcessor, AutoModelForImageTextToText\n+from transformers import AutoProcessor, AutoModelForImageTextToText, infer_device\n import torch\n \n-device = torch.device(\"cuda\")\n+device = torch.device(infer_device())\n model = AutoModelForImageTextToText.from_pretrained(\n     \"HuggingFaceM4/idefics2-8b\",\n     torch_dtype=torch.bfloat16,\n@@ -301,7 +301,7 @@ from transformers import AutoModelForImageTextToText, QuantoConfig\n model_id = \"HuggingFaceM4/idefics2-8b\"\n quantization_config = QuantoConfig(weights=\"int8\")\n quantized_model = AutoModelForImageTextToText.from_pretrained(\n-    model_id, device_map=\"cuda\", quantization_config=quantization_config\n+    model_id, device_map=\"auto\", quantization_config=quantization_config\n )\n ```\n "
        },
        {
            "sha": "5f66e68c24524dcdfe4c6d307e0794dac1ef711e",
            "filename": "docs/source/en/tasks/mask_generation.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Ftasks%2Fmask_generation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Ftasks%2Fmask_generation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fmask_generation.md?ref=7c1169e21fcf953b9670ac1e79c6e157bcaedc5c",
            "patch": "@@ -189,7 +189,7 @@ inputs = processor(\n         image,\n         input_boxes=[[[box]]],\n         return_tensors=\"pt\"\n-    ).to(\"cuda\")\n+    ).to(model.device)\n \n with torch.no_grad():\n     outputs = model(**inputs)"
        },
        {
            "sha": "b387a8320dfc57ff87b93f7394aea1df8f267201",
            "filename": "docs/source/en/tasks/video_classification.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Ftasks%2Fvideo_classification.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Ftasks%2Fvideo_classification.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fvideo_classification.md?ref=7c1169e21fcf953b9670ac1e79c6e157bcaedc5c",
            "patch": "@@ -464,7 +464,7 @@ Load a video for inference:\n The simplest way to try out your fine-tuned model for inference is to use it in a [`pipeline`](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.VideoClassificationPipeline). Instantiate a `pipeline` for video classification with your model, and pass your video to it:\n \n ```py\n->>> from transformers import pipeline\n+>>> from transformers import pipeline, infer_device\n \n >>> video_cls = pipeline(model=\"my_awesome_video_cls_model\")\n >>> video_cls(\"https://huggingface.co/datasets/sayakpaul/ucf101-subset/resolve/main/v_BasketballDunk_g14_c06.avi\")\n@@ -489,7 +489,7 @@ You can also manually replicate the results of the `pipeline` if you'd like.\n ...         ),  # this can be skipped if you don't have labels available.\n ...     }\n \n-...     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n+...     device = torch.device(infer_device())\n ...     inputs = {k: v.to(device) for k, v in inputs.items()}\n ...     model = model.to(device)\n "
        },
        {
            "sha": "0536c487c51891292abd5194ad1da8b1a7758286",
            "filename": "docs/source/en/tasks/video_text_to_text.md",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Ftasks%2Fvideo_text_to_text.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c1169e21fcf953b9670ac1e79c6e157bcaedc5c/docs%2Fsource%2Fen%2Ftasks%2Fvideo_text_to_text.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fvideo_text_to_text.md?ref=7c1169e21fcf953b9670ac1e79c6e157bcaedc5c",
            "patch": "@@ -46,8 +46,7 @@ model_id = \"llava-hf/llava-interleave-qwen-0.5b-hf\"\n \n processor = LlavaProcessor.from_pretrained(model_id)\n \n-model = LlavaForConditionalGeneration.from_pretrained(model_id, torch_dtype=torch.float16)\n-model.to(\"cuda\") # can also be xpu, mps, npu etc. depending on your hardware accelerator\n+model = LlavaForConditionalGeneration.from_pretrained(model_id, device_map=\"auto\", torch_dtype=torch.float16)\n ```\n \n Some models directly consume the `<video>` token, and others accept `<image>` tokens equal to the number of sampled frames. This model handles videos in the latter fashion. We will write a simple utility to handle image tokens, and another utility to get a video from a url and sample frames from it. \n@@ -144,4 +143,4 @@ print(processor.decode(output[0][2:], skip_special_tokens=True)[len(user_prompt)\n \n And voila! \n \n-To learn more about chat templates and token streaming for video-text-to-text models, refer to the [image-text-to-text](../tasks/image_text_to_text) task guide because these models work similarly.\n\\ No newline at end of file\n+To learn more about chat templates and token streaming for video-text-to-text models, refer to the [image-text-to-text](../tasks/image_text_to_text) task guide because these models work similarly."
        }
    ],
    "stats": {
        "total": 343,
        "additions": 185,
        "deletions": 158
    }
}