{
    "author": "yao-matrix",
    "message": "make aya vision 5 integration tests pass on xpu (#37990)\n\n* 5 aya vision integration pass on XPU\n\nSigned-off-by: Yao Matrix <matrix.yao@intel.com>\n\n* fix style\n\nSigned-off-by: Yao Matrix <matrix.yao@intel.com>\n\n---------\n\nSigned-off-by: Yao Matrix <matrix.yao@intel.com>\nCo-authored-by: Yih-Dar <2521628+ydshieh@users.noreply.github.com>",
    "sha": "038f8fc159355b6bae1a554d592df94cb61f2753",
    "files": [
        {
            "sha": "c35058abd6221f0ca479f08a704a66b9e4cdf05f",
            "filename": "tests/models/aya_vision/test_modeling_aya_vision.py",
            "status": "modified",
            "additions": 42,
            "deletions": 10,
            "changes": 52,
            "blob_url": "https://github.com/huggingface/transformers/blob/038f8fc159355b6bae1a554d592df94cb61f2753/tests%2Fmodels%2Faya_vision%2Ftest_modeling_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/038f8fc159355b6bae1a554d592df94cb61f2753/tests%2Fmodels%2Faya_vision%2Ftest_modeling_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faya_vision%2Ftest_modeling_aya_vision.py?ref=038f8fc159355b6bae1a554d592df94cb61f2753",
            "patch": "@@ -25,10 +25,12 @@\n     is_vision_available,\n )\n from transformers.testing_utils import (\n+    Expectations,\n     cleanup,\n+    require_deterministic_for_xpu,\n     require_read_token,\n     require_torch,\n-    require_torch_gpu,\n+    require_torch_accelerator,\n     slow,\n     torch_device,\n )\n@@ -332,7 +334,7 @@ def tearDown(self):\n         cleanup(torch_device, gc_collect=True)\n \n     @slow\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_small_model_integration_forward(self):\n         processor = AutoProcessor.from_pretrained(self.model_checkpoint)\n         model = AyaVisionForConditionalGeneration.from_pretrained(\n@@ -366,7 +368,8 @@ def test_small_model_integration_forward(self):\n         )\n \n     @slow\n-    @require_torch_gpu\n+    @require_torch_accelerator\n+    @require_deterministic_for_xpu\n     def test_small_model_integration_generate_text_only(self):\n         processor = AutoProcessor.from_pretrained(self.model_checkpoint)\n         model = AyaVisionForConditionalGeneration.from_pretrained(\n@@ -390,11 +393,19 @@ def test_small_model_integration_generate_text_only(self):\n                 generate_ids[0, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True\n             )\n         print(\"decoded_output\", decoded_output)\n-        expected_output = \"Whispers on the breeze,\\nLeaves dance under moonlit skies,\\nNature's quiet song.\"\n+\n+        expected_outputs = Expectations(\n+            {\n+                (\"xpu\", 3): \"Whispers on the breeze,\\nLeaves dance under moonlit sky,\\nNature's quiet song.\",\n+                (\"cuda\", 7): \"Whispers on the breeze,\\nLeaves dance under moonlit skies,\\nNature's quiet song.\",\n+            }\n+        )  # fmt: skip\n+        expected_output = expected_outputs.get_expectation()\n+\n         self.assertEqual(decoded_output, expected_output)\n \n     @slow\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_small_model_integration_generate_chat_template(self):\n         processor = AutoProcessor.from_pretrained(self.model_checkpoint)\n         model = AyaVisionForConditionalGeneration.from_pretrained(\n@@ -423,7 +434,7 @@ def test_small_model_integration_generate_chat_template(self):\n         self.assertEqual(decoded_output, expected_output)\n \n     @slow\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_small_model_integration_batched_generate(self):\n         processor = AutoProcessor.from_pretrained(self.model_checkpoint)\n         model = AyaVisionForConditionalGeneration.from_pretrained(\n@@ -459,7 +470,14 @@ def test_small_model_integration_batched_generate(self):\n         # Check first output\n         decoded_output = processor.decode(output[0, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True)\n         print(\"decoded_output\", decoded_output)\n-        expected_output = \"Wooden path to water,\\nMountains echo in stillness,\\nPeaceful forest scene.\"  # fmt: skip\n+        expected_outputs = Expectations(\n+            {\n+                (\"xpu\", 3): \"Wooden path to water,\\nMountains echo in stillness,\\nPeaceful forest lake.\",\n+                (\"cuda\", 7): \"Wooden path to water,\\nMountains echo in stillness,\\nPeaceful forest scene.\",\n+            }\n+        )  # fmt: skip\n+        expected_output = expected_outputs.get_expectation()\n+\n         self.assertEqual(\n             decoded_output,\n             expected_output,\n@@ -478,7 +496,8 @@ def test_small_model_integration_batched_generate(self):\n         )\n \n     @slow\n-    @require_torch_gpu\n+    @require_torch_accelerator\n+    @require_deterministic_for_xpu\n     def test_small_model_integration_batched_generate_multi_image(self):\n         processor = AutoProcessor.from_pretrained(self.model_checkpoint)\n         model = AyaVisionForConditionalGeneration.from_pretrained(\n@@ -523,7 +542,14 @@ def test_small_model_integration_batched_generate_multi_image(self):\n         # Check first output\n         decoded_output = processor.decode(output[0, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True)\n         # Batching seems to alter the output slightly, but it is also the case in the original implementation. This seems to be expected: https://github.com/huggingface/transformers/issues/23017#issuecomment-1649630232\n-        expected_output = \"Wooden path to water,\\nMountains echo in stillness,\\nPeaceful forest scene.\"  # fmt: skip\n+        expected_outputs = Expectations(\n+            {\n+                (\"xpu\", 3): \"Wooden path to water,\\nMountains echo in stillness,\\nPeaceful forest lake.\",\n+                (\"cuda\", 7): \"Wooden path to water,\\nMountains echo in stillness,\\nPeaceful forest scene.\",\n+            }\n+        )  # fmt: skip\n+        expected_output = expected_outputs.get_expectation()\n+\n         print(\"decoded_output\", decoded_output)\n         self.assertEqual(\n             decoded_output,\n@@ -534,7 +560,13 @@ def test_small_model_integration_batched_generate_multi_image(self):\n         # Check second output\n         decoded_output = processor.decode(output[1, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True)\n         print(\"decoded_output\", decoded_output)\n-        expected_output = \"The first image showcases the Statue of Liberty, a colossal neoclassical sculpture on Liberty Island in New York Harbor. Standing at a\"  # fmt: skip\n+        expected_outputs = Expectations(\n+            {\n+                (\"xpu\", 3): \"The first image showcases the Statue of Liberty, a colossal neoclassical sculpture on Liberty Island in New York Harbor. Standing at \",\n+                (\"cuda\", 7): \"The first image showcases the Statue of Liberty, a colossal neoclassical sculpture on Liberty Island in New York Harbor. Standing at a\",\n+            }\n+        )  # fmt: skip\n+        expected_output = expected_outputs.get_expectation()\n         self.assertEqual(\n             decoded_output,\n             expected_output,"
        }
    ],
    "stats": {
        "total": 52,
        "additions": 42,
        "deletions": 10
    }
}