{
    "author": "ydshieh",
    "message": "Fix `Gemma2IntegrationTest` (#38492)\n\n* fix\n\n* fix\n\n* skip-ci\n\n* skip-ci\n\n* skip-ci\n\n* skip-ci\n\n* skip-ci\n\n* skip-ci\n\n* skip-ci\n\n* skip-ci\n\n* skip-ci\n\n* skip-ci\n\n* skip-ci\n\n* update\n\n* fix\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "ccc859620a3b36c822ab93df46e7384141c92aaa",
    "files": [
        {
            "sha": "478b8a16ad3d490a90faeaceccc7d0fdeda46f63",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc859620a3b36c822ab93df46e7384141c92aaa/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc859620a3b36c822ab93df46e7384141c92aaa/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=ccc859620a3b36c822ab93df46e7384141c92aaa",
            "patch": "@@ -116,6 +116,7 @@\n     is_peft_available,\n     is_phonemizer_available,\n     is_pretty_midi_available,\n+    is_psutil_available,\n     is_pyctcdecode_available,\n     is_pytesseract_available,\n     is_pytest_available,\n@@ -1053,6 +1054,19 @@ def require_torch_gpu(test_case):\n     return unittest.skipUnless(torch_device == \"cuda\", \"test requires CUDA\")(test_case)\n \n \n+def require_large_cpu_ram(test_case, memory: float = 80):\n+    \"\"\"Decorator marking a test that requires a CPU RAM with more than `memory` GiB of memory.\"\"\"\n+    if not is_psutil_available():\n+        return test_case\n+\n+    import psutil\n+\n+    return unittest.skipUnless(\n+        psutil.virtual_memory().total / 1024**3 > memory,\n+        f\"test requires a machine with more than {memory} GiB of CPU RAM memory\",\n+    )(test_case)\n+\n+\n def require_torch_large_gpu(test_case, memory: float = 20):\n     \"\"\"Decorator marking a test that requires a CUDA GPU with more than `memory` GiB of memory.\"\"\"\n     if torch_device != \"cuda\":"
        },
        {
            "sha": "02a33d8f611ab332663dbdce82aeb48759ec52a9",
            "filename": "tests/models/cohere2/test_modeling_cohere2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc859620a3b36c822ab93df46e7384141c92aaa/tests%2Fmodels%2Fcohere2%2Ftest_modeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc859620a3b36c822ab93df46e7384141c92aaa/tests%2Fmodels%2Fcohere2%2Ftest_modeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcohere2%2Ftest_modeling_cohere2.py?ref=ccc859620a3b36c822ab93df46e7384141c92aaa",
            "patch": "@@ -24,6 +24,7 @@\n from transformers.generation.configuration_utils import GenerationConfig\n from transformers.testing_utils import (\n     Expectations,\n+    is_flash_attn_2_available,\n     require_flash_attn,\n     require_read_token,\n     require_torch,\n@@ -282,6 +283,9 @@ def test_generation_beyond_sliding_window(self, attn_implementation: str):\n         we need to correctly slice the attention mask in all cases (because we use a HybridCache).\n         Outputs for every attention functions should be coherent and identical.\n         \"\"\"\n+        if attn_implementation == \"flash_attention_2\" and not is_flash_attn_2_available():\n+            self.skipTest(\"FlashAttention2 is required for this test.\")\n+\n         if torch_device == \"xpu\" and attn_implementation == \"flash_attention_2\":\n             self.skipTest(reason=\"Intel XPU doesn't support falsh_attention_2 as of now.\")\n "
        },
        {
            "sha": "f8490db76bae4dcb653657e1621cd3fc829d6634",
            "filename": "tests/models/gemma2/test_modeling_gemma2.py",
            "status": "modified",
            "additions": 43,
            "deletions": 18,
            "changes": 61,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc859620a3b36c822ab93df46e7384141c92aaa/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc859620a3b36c822ab93df46e7384141c92aaa/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py?ref=ccc859620a3b36c822ab93df46e7384141c92aaa",
            "patch": "@@ -23,13 +23,17 @@\n from transformers import AutoModelForCausalLM, AutoTokenizer, Gemma2Config, is_torch_available, pipeline\n from transformers.generation.configuration_utils import GenerationConfig\n from transformers.testing_utils import (\n+    Expectations,\n+    cleanup,\n+    is_flash_attn_2_available,\n     require_flash_attn,\n+    require_large_cpu_ram,\n     require_read_token,\n     require_torch,\n     require_torch_accelerator,\n-    require_torch_gpu,\n+    require_torch_large_accelerator,\n+    require_torch_large_gpu,\n     slow,\n-    tooslow,\n     torch_device,\n )\n \n@@ -177,7 +181,13 @@ def test_flash_attn_2_equivalence(self):\n class Gemma2IntegrationTest(unittest.TestCase):\n     input_text = [\"Hello I am doing\", \"Hi today\"]\n \n-    @tooslow\n+    def setUp(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n+    def tearDown(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n+    @require_torch_large_accelerator\n     @require_read_token\n     def test_model_9b_bf16(self):\n         model_id = \"google/gemma-2-9b\"\n@@ -198,7 +208,7 @@ def test_model_9b_bf16(self):\n \n         self.assertEqual(output_text, EXPECTED_TEXTS)\n \n-    @tooslow\n+    @require_torch_large_accelerator\n     @require_read_token\n     def test_model_9b_fp16(self):\n         model_id = \"google/gemma-2-9b\"\n@@ -220,7 +230,7 @@ def test_model_9b_fp16(self):\n         self.assertEqual(output_text, EXPECTED_TEXTS)\n \n     @require_read_token\n-    @tooslow\n+    @require_torch_large_accelerator\n     def test_model_9b_pipeline_bf16(self):\n         # See https://github.com/huggingface/transformers/pull/31747 -- pipeline was broken for Gemma2 before this PR\n         model_id = \"google/gemma-2-9b\"\n@@ -246,10 +256,15 @@ def test_model_2b_pipeline_bf16_flex_attention(self):\n         # See https://github.com/huggingface/transformers/pull/31747 -- pipeline was broken for Gemma2 before this PR\n         model_id = \"google/gemma-2-2b\"\n         # EXPECTED_TEXTS should match the same non-pipeline test, minus the special tokens\n-        EXPECTED_TEXTS = [\n-            \"Hello I am doing a project on the 1960s and I am trying to find out what the average\",\n-            \"Hi today I'm going to be talking about the 10 best anime of all time.\\n\\n1\",\n-        ]\n+        EXPECTED_BATCH_TEXTS = Expectations(\n+            {\n+                (\"cuda\", 8): [\n+                    \"Hello I am doing a project on the 1960s and I am trying to find out what the average\",\n+                    \"Hi today I'm going to be talking about the 10 most powerful characters in the Naruto series.\",\n+                ]\n+            }\n+        )\n+        EXPECTED_BATCH_TEXT = EXPECTED_BATCH_TEXTS.get_expectation()\n \n         model = AutoModelForCausalLM.from_pretrained(\n             model_id, low_cpu_mem_usage=True, torch_dtype=torch.bfloat16, attn_implementation=\"flex_attention\"\n@@ -259,21 +274,20 @@ def test_model_2b_pipeline_bf16_flex_attention(self):\n \n         output = pipe(self.input_text, max_new_tokens=20, do_sample=False, padding=True)\n \n-        self.assertEqual(output[0][0][\"generated_text\"], EXPECTED_TEXTS[0])\n-        self.assertEqual(output[1][0][\"generated_text\"], EXPECTED_TEXTS[1])\n+        self.assertEqual(output[0][0][\"generated_text\"], EXPECTED_BATCH_TEXT[0])\n+        self.assertEqual(output[1][0][\"generated_text\"], EXPECTED_BATCH_TEXT[1])\n \n     @require_read_token\n     @require_flash_attn\n-    @require_torch_gpu\n+    @require_torch_large_gpu\n     @mark.flash_attn_test\n     @slow\n-    @tooslow\n     def test_model_9b_flash_attn(self):\n         # See https://github.com/huggingface/transformers/issues/31953 --- flash attn was generating garbage for gemma2, especially in long context\n         model_id = \"google/gemma-2-9b\"\n         EXPECTED_TEXTS = [\n             '<bos>Hello I am doing a project on the 1918 flu pandemic and I am trying to find out how many people died in the United States. I have found a few sites that say 500,000 but I am not sure if that is correct. I have also found a site that says 675,000 but I am not sure if that is correct either. I am trying to find out how many people died in the United States. I have found a few',\n-            \"<pad><pad><bos>Hi today I'm going to be talking about the history of the United States. The United States of America is a country in North America. It is the third largest country in the world by total area and the third most populous country with over 320 million people. The United States is a federal republic consisting of 50 states and a federal district. The 48 contiguous states and the district of Columbia are in central North America between Canada and Mexico. The state of Alaska is in the\"\n+            \"<pad><pad><bos>Hi today I'm going to be talking about the history of the United States. The United States of America is a country in North America. It is the third largest country in the world by total area and the third most populous country with over 320 million people. The United States is a federal republic composed of 50 states and a federal district. The 48 contiguous states and the district of Columbia are in central North America between Canada and Mexico. The state of Alaska is in the\",\n         ]  # fmt: skip\n \n         model = AutoModelForCausalLM.from_pretrained(\n@@ -299,9 +313,17 @@ def test_export_static_cache(self):\n         )\n \n         tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\", pad_token=\"</s>\", padding_side=\"right\")\n-        EXPECTED_TEXT_COMPLETION = [\n-            \"Hello I am doing a project for my school and I need to know how to make a program that will take a number\",\n-        ]\n+        EXPECTED_TEXT_COMPLETIONS = Expectations(\n+            {\n+                (\"cuda\", 7): [\n+                    \"Hello I am doing a project for my school and I need to know how to make a program that will take a number\"\n+                ],\n+                (\"cuda\", 8): [\n+                    \"Hello I am doing a project for my class and I am having trouble with the code. I am trying to make a\"\n+                ],\n+            }\n+        )\n+        EXPECTED_TEXT_COMPLETION = EXPECTED_TEXT_COMPLETIONS.get_expectation()\n         max_generation_length = tokenizer(EXPECTED_TEXT_COMPLETION, return_tensors=\"pt\", padding=True)[\n             \"input_ids\"\n         ].shape[-1]\n@@ -343,6 +365,7 @@ def test_export_static_cache(self):\n \n     @slow\n     @require_read_token\n+    @require_large_cpu_ram\n     def test_export_hybrid_cache(self):\n         from transformers.integrations.executorch import TorchExportableModuleForDecoderOnlyLM\n         from transformers.pytorch_utils import is_torch_greater_or_equal\n@@ -379,8 +402,8 @@ def test_export_hybrid_cache(self):\n         eager_generated_text = tokenizer.decode(eager_outputs[0], skip_special_tokens=True)\n         self.assertEqual(export_generated_text, eager_generated_text)\n \n+    @require_torch_large_accelerator\n     @require_read_token\n-    @tooslow\n     def test_model_9b_bf16_flex_attention(self):\n         model_id = \"google/gemma-2-9b\"\n         EXPECTED_TEXTS = [\n@@ -407,6 +430,8 @@ def test_generation_beyond_sliding_window(self, attn_implementation: str):\n         we need to correctly slice the attention mask in all cases (because we use a HybridCache).\n         Outputs for every attention functions should be coherent and identical.\n         \"\"\"\n+        if attn_implementation == \"flash_attention_2\" and not is_flash_attn_2_available():\n+            self.skipTest(\"FlashAttention2 is required for this test.\")\n \n         if torch_device == \"xpu\" and attn_implementation == \"flash_attention_2\":\n             self.skipTest(reason=\"Intel XPU doesn't support falsh_attention_2 as of now.\")"
        }
    ],
    "stats": {
        "total": 79,
        "additions": 61,
        "deletions": 18
    }
}