{
    "author": "gante",
    "message": "[Chat] Add Chat from TRL üêà  (#35714)\n\n* tmp commit\r\n\r\n* add working chat\r\n\r\n* add docts\r\n\r\n* docs 2\r\n\r\n* use auto dtype by default",
    "sha": "b3d672246993affc4179b2b7ed3ee09913a66c37",
    "files": [
        {
            "sha": "d387ba069c8f0033da3f12dc6f835da9c9cd4d7e",
            "filename": "docs/source/en/chat_templating.md",
            "status": "modified",
            "additions": 100,
            "deletions": 93,
            "changes": 193,
            "blob_url": "https://github.com/huggingface/transformers/blob/b3d672246993affc4179b2b7ed3ee09913a66c37/docs%2Fsource%2Fen%2Fchat_templating.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b3d672246993affc4179b2b7ed3ee09913a66c37/docs%2Fsource%2Fen%2Fchat_templating.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fchat_templating.md?ref=b3d672246993affc4179b2b7ed3ee09913a66c37",
            "patch": "@@ -23,8 +23,8 @@ of text (as is the case with a standard language model), the model instead conti\n of one or more **messages**, each of which includes a **role**, like \"user\" or \"assistant\", as well as message text.\n \n Much like tokenization, different models expect very different input formats for chat. This is the reason we added\n-**chat templates** as a feature. Chat templates are part of the tokenizer for text-only LLMs or processor for multimodal LLMs. They specify how to convert conversations, \n-represented as lists of messages, into a single tokenizable string in the format that the model expects. \n+**chat templates** as a feature. Chat templates are part of the tokenizer for text-only LLMs or processor for multimodal LLMs. They specify how to convert conversations,\n+represented as lists of messages, into a single tokenizable string in the format that the model expects.\n \n Let's make this concrete with a quick example using the `mistralai/Mistral-7B-Instruct-v0.1` model:\n \n@@ -42,8 +42,8 @@ Let's make this concrete with a quick example using the `mistralai/Mistral-7B-In\n \"<s>[INST] Hello, how are you? [/INST]I'm doing great. How can I help you today?</s> [INST] I'd like to show off how chat templating works! [/INST]\"\n ```\n \n-Notice how the tokenizer has added the control tokens [INST] and [/INST] to indicate the start and end of \n-user messages (but not assistant messages!), and the entire chat is condensed into a single string. \n+Notice how the tokenizer has added the control tokens [INST] and [/INST] to indicate the start and end of\n+user messages (but not assistant messages!), and the entire chat is condensed into a single string.\n If we use `tokenize=True`, which is the default setting, that string will also be tokenized for us.\n \n Now, try the same code, but swap in the `HuggingFaceH4/zephyr-7b-beta` model instead, and you should get:\n@@ -59,17 +59,24 @@ I'd like to show off how chat templating works!</s>\n \n Both Zephyr and Mistral-Instruct were fine-tuned from the same base model, `Mistral-7B-v0.1`. However, they were trained\n with totally different chat formats. Without chat templates, you would have to write manual formatting code for each\n-model, and it's very easy to make minor errors that hurt performance! Chat templates handle the details of formatting \n+model, and it's very easy to make minor errors that hurt performance! Chat templates handle the details of formatting\n for you, allowing you to write universal code that works for any model.\n \n+<Tip>\n+\n+Chat templates are a critical component of our [`transformers-cli chat` CLI](quicktour#chat-with-text-generation-models).\n+You can apply the learnings of this guide there as well.\n+\n+</Tip>\n+\n \n ## How do I use chat templates?\n \n As you can see in the example above, chat templates are easy to use. Simply build a list of messages, with `role`\n and `content` keys, and then pass it to the [`~PreTrainedTokenizer.apply_chat_template`] or [`~ProcessorMixin.apply_chat_template`] method\n depending on what type of model you are using. Once you do that,\n you'll get output that's ready to go! When using chat templates as input for model generation, it's also a good idea\n-to use `add_generation_prompt=True` to add a [generation prompt](#what-are-generation-prompts). \n+to use `add_generation_prompt=True` to add a [generation prompt](#what-are-generation-prompts).\n \n ## Usage with text-only LLMs\n Here's an example of preparing input for `model.generate()`, using `Zephyr` again:\n@@ -91,29 +98,29 @@ messages = [\n tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n print(tokenizer.decode(tokenized_chat[0]))\n ```\n-This will yield a string in the input format that Zephyr expects. \n+This will yield a string in the input format that Zephyr expects.\n ```text\n <|system|>\n-You are a friendly chatbot who always responds in the style of a pirate</s> \n+You are a friendly chatbot who always responds in the style of a pirate</s>\n <|user|>\n-How many helicopters can a human eat in one sitting?</s> \n+How many helicopters can a human eat in one sitting?</s>\n <|assistant|>\n ```\n \n Now that our input is formatted correctly for Zephyr, we can use the model to generate a response to the user's question:\n \n ```python\n-outputs = model.generate(tokenized_chat, max_new_tokens=128) \n+outputs = model.generate(tokenized_chat, max_new_tokens=128)\n print(tokenizer.decode(outputs[0]))\n ```\n \n This will yield:\n \n ```text\n <|system|>\n-You are a friendly chatbot who always responds in the style of a pirate</s> \n+You are a friendly chatbot who always responds in the style of a pirate</s>\n <|user|>\n-How many helicopters can a human eat in one sitting?</s> \n+How many helicopters can a human eat in one sitting?</s>\n <|assistant|>\n Matey, I'm afraid I must inform ye that humans cannot eat helicopters. Helicopters are not food, they are flying machines. Food is meant to be eaten, like a hearty plate o' grog, a savory bowl o' stew, or a delicious loaf o' bread. But helicopters, they be for transportin' and movin' around, not for eatin'. So, I'd say none, me hearties. None at all.\n ```\n@@ -152,7 +159,7 @@ print(processor.batch_decode(processed_chat[\"input_ids\"][:, :30]))\n This yields a string in LLaVAs expected input format with many `<image>` tokens at the end.\n The `<image>` tokens are placeholders and each one will be replaced by image embeddings when the mode is run in the forward call. The `processed_chat` can be further passed into [`~GenerationMixin.generate`] to generate text.\n ```text\n-'<|im_start|>system \n+'<|im_start|>system\n You are a friendly chatbot who always responds in the style of a pirate<|im_end|><|im_start|>user <image><image><image><image><image><image><image><image>'\n ```\n \n@@ -162,7 +169,7 @@ Arr, 'twas easy after all!\n \n Yes, there is! Our text generation pipelines support chat inputs, which makes it easy to use chat models. In the past,\n we used to use a dedicated \"ConversationalPipeline\" class, but this has now been deprecated and its functionality\n-has been merged into the [`TextGenerationPipeline`]. Let's try the `Zephyr` example again, but this time using \n+has been merged into the [`TextGenerationPipeline`]. Let's try the `Zephyr` example again, but this time using\n a pipeline:\n \n ```python\n@@ -227,9 +234,9 @@ Can I ask a question?<|im_end|>\n ```\n \n Note that this time, we've added the tokens that indicate the start of a bot response. This ensures that when the model\n-generates text it will write a bot response instead of doing something unexpected, like continuing the user's \n-message. Remember, chat models are still just language models - they're trained to continue text, and chat is just a \n-special kind of text to them! You need to guide them with appropriate control tokens, so they know what they're \n+generates text it will write a bot response instead of doing something unexpected, like continuing the user's\n+message. Remember, chat models are still just language models - they're trained to continue text, and chat is just a\n+special kind of text to them! You need to guide them with appropriate control tokens, so they know what they're\n supposed to be doing.\n \n Not all models require generation prompts. Some models, like LLaMA, don't have any\n@@ -241,7 +248,7 @@ effect that `add_generation_prompt` has will depend on the template being used.\n When passing a list of messages to `apply_chat_template` or `TextGenerationPipeline`, you can choose\n to format the chat so the model will continue the final message in the chat instead of starting a new one. This is done\n by removing any end-of-sequence tokens that indicate the end of the final message, so that the model will simply\n-extend the final message when it begins to generate text. This is useful for \"prefilling\" the model's response. \n+extend the final message when it begins to generate text. This is useful for \"prefilling\" the model's response.\n \n Here's an example:\n \n@@ -266,9 +273,9 @@ get an error if you try!\n <Tip>\n \n The default behaviour of `TextGenerationPipeline` is to set `add_generation_prompt=True` so that it starts a new\n-message. However, if the final message in the input chat has the \"assistant\" role, it will assume that this message is \n-a prefill and switch to `continue_final_message=True` instead, because most models do not support multiple \n-consecutive assistant messages. You can override this behaviour by explicitly passing the `continue_final_message` \n+message. However, if the final message in the input chat has the \"assistant\" role, it will assume that this message is\n+a prefill and switch to `continue_final_message=True` instead, because most models do not support multiple\n+consecutive assistant messages. You can override this behaviour by explicitly passing the `continue_final_message`\n argument when calling the pipeline.\n \n </Tip>\n@@ -277,8 +284,8 @@ argument when calling the pipeline.\n \n Yes! This is a good way to ensure that the chat template matches the tokens the model sees during training.\n We recommend that you apply the chat template as a preprocessing step for your dataset. After this, you\n-can simply continue like any other language model training task. When training, you should usually set \n-`add_generation_prompt=False`, because the added tokens to prompt an assistant response will not be helpful during \n+can simply continue like any other language model training task. When training, you should usually set\n+`add_generation_prompt=False`, because the added tokens to prompt an assistant response will not be helpful during\n training. Let's see an example:\n \n ```python\n@@ -312,8 +319,8 @@ From here, just continue training like you would with a standard language modell\n \n <Tip>\n \n-By default, some tokenizers add special tokens like `<bos>` and `<eos>` to text they tokenize. Chat templates should \n-already include all the special tokens they need, and so additional special tokens will often be incorrect or \n+By default, some tokenizers add special tokens like `<bos>` and `<eos>` to text they tokenize. Chat templates should\n+already include all the special tokens they need, and so additional special tokens will often be incorrect or\n duplicated, which will hurt model performance.\n \n Therefore, if you format text with `apply_chat_template(tokenize=False)`, you should set the argument\n@@ -326,7 +333,7 @@ Therefore, if you format text with `apply_chat_template(tokenize=False)`, you sh\n The only argument that `apply_chat_template` requires is `messages`. However, you can pass any keyword\n argument to `apply_chat_template` and it will be accessible inside the template. This gives you a lot of freedom to use\n chat templates for many things. There are no restrictions on the names or the format of these arguments - you can pass\n-strings, lists, dicts or whatever else you want. \n+strings, lists, dicts or whatever else you want.\n \n That said, there are some common use-cases for these extra arguments,\n such as passing tools for function calling, or documents for retrieval-augmented generation. In these common cases,\n@@ -349,7 +356,7 @@ def current_time():\n def multiply(a: float, b: float):\n     \"\"\"\n     A function that multiplies two numbers\n-    \n+\n     Args:\n         a: The first number to multiply\n         b: The second number to multiply\n@@ -369,8 +376,8 @@ correctly as tools. Specifically, you should follow these rules:\n \n - The function should have a descriptive name\n - Every argument must have a type hint\n-- The function must have a docstring in the standard Google style (in other words, an initial function description  \n-  followed by an `Args:` block that describes the arguments, unless the function does not have any arguments. \n+- The function must have a docstring in the standard Google style (in other words, an initial function description\n+  followed by an `Args:` block that describes the arguments, unless the function does not have any arguments.\n - Do not include types in the `Args:` block. In other words, write `a: The first number to multiply`, not\n   `a (int): The first number to multiply`. Type hints should go in the function header instead.\n - The function can have a return type and a `Returns:` block in the docstring. However, these are optional\n@@ -412,7 +419,7 @@ Next, let's define a list of tools:\n def get_current_temperature(location: str, unit: str) -> float:\n     \"\"\"\n     Get the current temperature at a location.\n-    \n+\n     Args:\n         location: The location to get the temperature for, in the format \"City, Country\"\n         unit: The unit to return the temperature in. (choices: [\"celsius\", \"fahrenheit\"])\n@@ -424,7 +431,7 @@ def get_current_temperature(location: str, unit: str) -> float:\n def get_current_wind_speed(location: str) -> float:\n     \"\"\"\n     Get the current wind speed in km/h at a given location.\n-    \n+\n     Args:\n         location: The location to get the temperature for, in the format \"City, Country\"\n     Returns:\n@@ -469,8 +476,8 @@ the temperature in France should certainly be displayed in Celsius.\n \n The output format above is specific to the `Hermes-2-Pro` model we're using in this example. Other models may emit different\n tool call formats, and you may need to do some manual parsing at this step. For example, `Llama-3.1` models will emit\n-slightly different JSON, with `parameters` instead of `arguments`. Regardless of the format the model outputs, you \n-should add the tool call to the conversation in the format below, with `tool_calls`, `function` and `arguments` keys. \n+slightly different JSON, with `parameters` instead of `arguments`. Regardless of the format the model outputs, you\n+should add the tool call to the conversation in the format below, with `tool_calls`, `function` and `arguments` keys.\n \n </Tip>\n \n@@ -489,7 +496,7 @@ a dict, but in the OpenAI API it's a JSON string. Passing a string may cause err\n </Tip>\n \n Now that we've added the tool call to the conversation, we can call the function and append the result to the\n-conversation. Since we're just using a dummy function for this example that always returns 22.0, we can just append \n+conversation. Since we're just using a dummy function for this example that always returns 22.0, we can just append\n that result directly.\n \n ```python\n@@ -500,7 +507,7 @@ messages.append({\"role\": \"tool\", \"name\": \"get_current_temperature\", \"content\": \"\n \n Some model architectures, notably Mistral/Mixtral, also require a `tool_call_id` here, which should be\n 9 randomly-generated alphanumeric characters, and assigned to the `id` key of the tool call\n-dictionary. The same key should also be assigned to the `tool_call_id` key of the tool response dictionary below, so \n+dictionary. The same key should also be assigned to the `tool_call_id` key of the tool response dictionary below, so\n that tool calls can be matched to tool responses. So, for Mistral/Mixtral models, the code above would be:\n \n ```python\n@@ -532,13 +539,13 @@ And we get:\n The current temperature in Paris, France is 22.0 ¬∞ Celsius.<|im_end|>\n ```\n \n-Although this was a simple demo with dummy tools and a single call, the same technique works with \n+Although this was a simple demo with dummy tools and a single call, the same technique works with\n multiple real tools and longer conversations. This can be a powerful way to extend the capabilities of conversational\n agents with real-time information, computational tools like calculators, or access to large databases.\n \n ### Understanding tool schemas\n \n-Each function you pass to the `tools` argument of `apply_chat_template` is converted into a \n+Each function you pass to the `tools` argument of `apply_chat_template` is converted into a\n [JSON schema](https://json-schema.org/learn/getting-started-step-by-step). These schemas\n are then passed to the model chat template. In other words, tool-use models do not see your functions directly, and they\n never see the actual code inside them. What they care about is the function **definitions** and the **arguments** they\n@@ -547,7 +554,7 @@ to read their outputs, detect if they have requested to use a tool, pass their a\n return the response in the chat.\n \n Generating JSON schemas to pass to the template should be automatic and invisible as long as your functions\n-follow the specification above, but if you encounter problems, or you simply want more control over the conversion, \n+follow the specification above, but if you encounter problems, or you simply want more control over the conversion,\n you can handle the conversion manually. Here is an example of a manual schema conversion.\n \n ```python\n@@ -556,7 +563,7 @@ from transformers.utils import get_json_schema\n def multiply(a: float, b: float):\n     \"\"\"\n     A function that multiplies two numbers\n-    \n+\n     Args:\n         a: The first number to multiply\n         b: The second number to multiply\n@@ -571,41 +578,41 @@ This will yield:\n \n ```json\n {\n-  \"type\": \"function\", \n+  \"type\": \"function\",\n   \"function\": {\n-    \"name\": \"multiply\", \n-    \"description\": \"A function that multiplies two numbers\", \n+    \"name\": \"multiply\",\n+    \"description\": \"A function that multiplies two numbers\",\n     \"parameters\": {\n-      \"type\": \"object\", \n+      \"type\": \"object\",\n       \"properties\": {\n         \"a\": {\n-          \"type\": \"number\", \n+          \"type\": \"number\",\n           \"description\": \"The first number to multiply\"\n-        }, \n+        },\n         \"b\": {\n           \"type\": \"number\",\n           \"description\": \"The second number to multiply\"\n         }\n-      }, \n+      },\n       \"required\": [\"a\", \"b\"]\n     }\n   }\n }\n ```\n \n-If you wish, you can edit these schemas, or even write them from scratch yourself without using `get_json_schema` at \n-all. JSON schemas can be passed directly to the `tools` argument of \n+If you wish, you can edit these schemas, or even write them from scratch yourself without using `get_json_schema` at\n+all. JSON schemas can be passed directly to the `tools` argument of\n `apply_chat_template` - this gives you a lot of power to define precise schemas for more complex functions. Be careful,\n-though - the more complex your schemas, the more likely the model is to get confused when dealing with them! We \n-recommend simple function signatures where possible, keeping arguments (and especially complex, nested arguments) \n+though - the more complex your schemas, the more likely the model is to get confused when dealing with them! We\n+recommend simple function signatures where possible, keeping arguments (and especially complex, nested arguments)\n to a minimum.\n \n Here is an example of defining schemas by hand, and passing them directly to `apply_chat_template`:\n \n ```python\n # A simple function that takes no arguments\n current_time = {\n-  \"type\": \"function\", \n+  \"type\": \"function\",\n   \"function\": {\n     \"name\": \"current_time\",\n     \"description\": \"Get the current local time as a string.\",\n@@ -621,18 +628,18 @@ multiply = {\n   'type': 'function',\n   'function': {\n     'name': 'multiply',\n-    'description': 'A function that multiplies two numbers', \n+    'description': 'A function that multiplies two numbers',\n     'parameters': {\n-      'type': 'object', \n+      'type': 'object',\n       'properties': {\n         'a': {\n           'type': 'number',\n           'description': 'The first number to multiply'\n-        }, \n+        },\n         'b': {\n           'type': 'number', 'description': 'The second number to multiply'\n         }\n-      }, \n+      },\n       'required': ['a', 'b']\n     }\n   }\n@@ -647,7 +654,7 @@ model_input = tokenizer.apply_chat_template(\n ## Advanced: Retrieval-augmented generation\n \n \"Retrieval-augmented generation\" or \"RAG\" LLMs can search a corpus of documents for information before responding\n-to a query. This allows models to vastly expand their knowledge base beyond their limited context size. Our \n+to a query. This allows models to vastly expand their knowledge base beyond their limited context size. Our\n recommendation for RAG models is that their template\n should accept a `documents` argument. This should be a list of documents, where each \"document\"\n is a single dict with `title` and `contents` keys, both of which are strings. Because this format is much simpler\n@@ -672,7 +679,7 @@ conversation = [\n # Define documents for retrieval-based generation\n documents = [\n     {\n-        \"title\": \"The Moon: Our Age-Old Foe\", \n+        \"title\": \"The Moon: Our Age-Old Foe\",\n         \"text\": \"Man has always dreamed of destroying the moon. In this essay, I shall...\"\n     },\n     {\n@@ -690,7 +697,7 @@ input_ids = tokenizer.apply_chat_template(\n     add_generation_prompt=True,\n     return_tensors=\"pt\").to(device)\n \n-# Generate a response \n+# Generate a response\n gen_tokens = model.generate(\n     input_ids,\n     max_new_tokens=100,\n@@ -750,8 +757,8 @@ Effectively, the template does three things:\n    an assistant response.\n \n This is a pretty simple template but Jinja gives you a lot of flexibility to do more complex things! Let's see a Jinja\n-template that can format inputs similarly to the way LLaMA formats them (note that the real LLaMA template includes \n-handling for default system messages and slightly different system message handling in general - don't use this one \n+template that can format inputs similarly to the way LLaMA formats them (note that the real LLaMA template includes\n+handling for default system messages and slightly different system message handling in general - don't use this one\n in your actual code!)\n \n ```\n@@ -774,7 +781,7 @@ distinguishable to the model because of the tokens they're wrapped in.\n \n ### How do I create a chat template?\n \n-Simple, just write a jinja template and set `tokenizer.chat_template`. You may find it easier to start with an \n+Simple, just write a jinja template and set `tokenizer.chat_template`. You may find it easier to start with an\n existing template from another model and simply edit it for your needs! For example, we could take the LLaMA template\n above and add \"[ASST]\" and \"[/ASST]\" to assistant messages:\n \n@@ -802,13 +809,13 @@ tokenizer.chat_template = template  # Set the new template\n tokenizer.push_to_hub(\"model_name\")  # Upload your new template to the Hub!\n ```\n \n-The method [`~PreTrainedTokenizer.apply_chat_template`] which uses your chat template is called by the [`TextGenerationPipeline`] class, so \n+The method [`~PreTrainedTokenizer.apply_chat_template`] which uses your chat template is called by the [`TextGenerationPipeline`] class, so\n once you set the correct chat template, your model will automatically become compatible with [`TextGenerationPipeline`].\n \n <Tip>\n If you're fine-tuning a model for chat, in addition to setting a chat template, you should probably add any new chat\n-control tokens as special tokens in the tokenizer. Special tokens are never split, \n-ensuring that your control tokens are always handled as single tokens rather than being tokenized in pieces. You \n+control tokens as special tokens in the tokenizer. Special tokens are never split,\n+ensuring that your control tokens are always handled as single tokens rather than being tokenized in pieces. You\n should also set the tokenizer's `eos_token` attribute to the token that marks the end of assistant generations in your\n template. This will ensure that text generation tools can correctly figure out when to stop generating text.\n </Tip>\n@@ -836,13 +843,13 @@ trying to put it all in a single template where possible!\n \n When setting the template for a model that's already been trained for chat, you should ensure that the template\n exactly matches the message formatting that the model saw during training, or else you will probably experience\n-performance degradation. This is true even if you're training the model further - you will probably get the best \n+performance degradation. This is true even if you're training the model further - you will probably get the best\n performance if you keep the chat tokens constant. This is very analogous to tokenization - you generally get the\n best performance for inference or fine-tuning when you precisely match the tokenization used during training.\n \n If you're training a model from scratch, or fine-tuning a base language model for chat, on the other hand,\n you have a lot of freedom to choose an appropriate template! LLMs are smart enough to learn to handle lots of different\n-input formats. One popular choice is the `ChatML` format, and this is a good, flexible choice for many use-cases. \n+input formats. One popular choice is the `ChatML` format, and this is a good, flexible choice for many use-cases.\n It looks like this:\n \n ```\n@@ -888,25 +895,25 @@ Once the attribute is set, that's it, you're done! `tokenizer.apply_chat_templat\n model, which means it is also automatically supported in places like `TextGenerationPipeline`!\n \n By ensuring that models have this attribute, we can make sure that the whole community gets to use the full power of\n-open-source models. Formatting mismatches have been haunting the field and silently harming performance for too long - \n+open-source models. Formatting mismatches have been haunting the field and silently harming performance for too long -\n it's time to put an end to them!\n \n ## Advanced: Template writing tips\n \n <Tip>\n \n The easiest way to get started with writing Jinja templates is to take a look at some existing ones. You can use\n-`print(tokenizer.chat_template)` for any chat model to see what template it's using. In general, models that support tool use have \n+`print(tokenizer.chat_template)` for any chat model to see what template it's using. In general, models that support tool use have\n much more complex templates than other models - so when you're just getting started, they're probably a bad example\n-to learn from! You can also take a look at the \n+to learn from! You can also take a look at the\n [Jinja documentation](https://jinja.palletsprojects.com/en/3.1.x/templates/#synopsis) for details\n of general Jinja formatting and syntax.\n \n </Tip>\n \n-Jinja templates in `transformers` are identical to Jinja templates elsewhere. The main thing to know is that \n-the conversation history will be accessible inside your template as a variable called `messages`.  \n-You will be able to access `messages` in your template just like you can in Python, which means you can loop over \n+Jinja templates in `transformers` are identical to Jinja templates elsewhere. The main thing to know is that\n+the conversation history will be accessible inside your template as a variable called `messages`.\n+You will be able to access `messages` in your template just like you can in Python, which means you can loop over\n it with `{% for message in messages %}` or access individual messages with `{{ messages[0] }}`, for example.\n \n You can also use the following tips to write clean, efficient Jinja templates:\n@@ -936,7 +943,7 @@ and indentation may end up being included in the output, which is probably not w\n \n ### Special variables\n \n-Inside your template, you will have access several special variables. The most important of these is `messages`, \n+Inside your template, you will have access several special variables. The most important of these is `messages`,\n which contains the chat history as a list of message dicts. However, there are several others. Not every\n variable will be used in every template. The most common other variables are:\n \n@@ -970,7 +977,7 @@ There are multiple implementations of Jinja in various languages. They generally\n but a key difference is that when you're writing a template in Python you can use Python methods, such as\n `.lower()` on strings or `.items()` on dicts. This will break if someone tries to use your template on a non-Python\n implementation of Jinja. Non-Python implementations are particularly common in deployment environments, where JS\n-and Rust are very popular. \n+and Rust are very popular.\n \n Don't panic, though! There are a few easy changes you can make to your templates to ensure they're compatible across\n all implementations of Jinja:\n@@ -1002,21 +1009,21 @@ Here is an example of a template that formats messages ChatML-style, with genera\n ```\n \n The exact content of the assistant header will depend on your specific model, but it should always be **the string\n-that represents the start of an assistant message**, so that if the user applies your template with \n+that represents the start of an assistant message**, so that if the user applies your template with\n `add_generation_prompt=True` and then generates text, the model will write an assistant response. Also note that some\n-models do not need a generation prompt, because assistant messages always begin immediately after user messages. \n+models do not need a generation prompt, because assistant messages always begin immediately after user messages.\n This is particularly common for LLaMA and Mistral models, where assistant messages begin immediately after the `[/INST]`\n token that ends user messages. In these cases, the template can ignore the `add_generation_prompt` flag.\n \n Generation prompts are important! If your model requires a generation prompt but it is not set in the template, then\n-model generations will likely be severely degraded, or the model may display unusual behaviour like continuing \n-the final user message! \n+model generations will likely be severely degraded, or the model may display unusual behaviour like continuing\n+the final user message!\n \n ### Writing and debugging larger templates\n \n-When this feature was introduced, most templates were quite small, the Jinja equivalent of a \"one-liner\" script. \n+When this feature was introduced, most templates were quite small, the Jinja equivalent of a \"one-liner\" script.\n However, with new models and features like tool-use and RAG, some templates can be 100 lines long or more. When\n-writing templates like these, it's a good idea to write them in a separate file, using a text editor. You can easily \n+writing templates like these, it's a good idea to write them in a separate file, using a text editor. You can easily\n extract a chat template to a file:\n \n ```python\n@@ -1035,7 +1042,7 @@ identify the source of issues.\n \n ### Writing templates for tools\n \n-Although chat templates do not enforce a specific API for tools (or for anything, really), we recommend \n+Although chat templates do not enforce a specific API for tools (or for anything, really), we recommend\n template authors try to stick to a standard API where possible. The whole point of chat templates is to allow code\n to be transferable across models, so deviating from the standard tools API means users will have to write\n custom code to use tools with your model. Sometimes it's unavoidable, but often with clever templating you can\n@@ -1045,30 +1052,30 @@ Below, we'll list the elements of the standard API, and give tips on writing tem\n \n #### Tool definitions\n \n-Your template should expect that the variable `tools` will either be null (if no tools are passed), or is a list \n+Your template should expect that the variable `tools` will either be null (if no tools are passed), or is a list\n of JSON schema dicts. Our chat template methods allow users to pass tools as either JSON schema or Python functions, but when\n-functions are passed, we automatically generate JSON schema and pass that to your template. As a result, the \n+functions are passed, we automatically generate JSON schema and pass that to your template. As a result, the\n `tools` variable that your template receives will always be a list of JSON schema. Here is\n a sample tool JSON schema:\n \n ```json\n {\n-  \"type\": \"function\", \n+  \"type\": \"function\",\n   \"function\": {\n-    \"name\": \"multiply\", \n-    \"description\": \"A function that multiplies two numbers\", \n+    \"name\": \"multiply\",\n+    \"description\": \"A function that multiplies two numbers\",\n     \"parameters\": {\n-      \"type\": \"object\", \n+      \"type\": \"object\",\n       \"properties\": {\n         \"a\": {\n-          \"type\": \"number\", \n+          \"type\": \"number\",\n           \"description\": \"The first number to multiply\"\n-        }, \n+        },\n         \"b\": {\n           \"type\": \"number\",\n           \"description\": \"The second number to multiply\"\n         }\n-      }, \n+      },\n       \"required\": [\"a\", \"b\"]\n     }\n   }\n@@ -1092,13 +1099,13 @@ specific format - your model will probably need different formatting!\n \n The specific tokens and tool descriptions your template renders should of course be chosen to match the ones your model\n was trained with. There is no requirement that your **model** understands JSON schema input, only that your template can translate\n-JSON schema into your model's format. For example, [Command-R](https://huggingface.co/CohereForAI/c4ai-command-r-plus-08-2024) \n-was trained with tools defined using Python function headers, but the Command-R tool template accepts JSON schema, \n+JSON schema into your model's format. For example, [Command-R](https://huggingface.co/CohereForAI/c4ai-command-r-plus-08-2024)\n+was trained with tools defined using Python function headers, but the Command-R tool template accepts JSON schema,\n converts types internally and renders the input tools as Python headers. You can do a lot with templates!\n \n #### Tool calls\n \n-Tool calls, if present, will be a list attached to a message with the \"assistant\" role. Note that `tool_calls` is \n+Tool calls, if present, will be a list attached to a message with the \"assistant\" role. Note that `tool_calls` is\n always a list, even though most tool-calling models only support single tool calls at a time, which means\n the list will usually only have a single element. Here is a sample message dict containing a tool call:\n "
        },
        {
            "sha": "ff257c567cabca4f4825d74a572a5f97ada693da",
            "filename": "docs/source/en/generation_strategies.md",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/b3d672246993affc4179b2b7ed3ee09913a66c37/docs%2Fsource%2Fen%2Fgeneration_strategies.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b3d672246993affc4179b2b7ed3ee09913a66c37/docs%2Fsource%2Fen%2Fgeneration_strategies.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fgeneration_strategies.md?ref=b3d672246993affc4179b2b7ed3ee09913a66c37",
            "patch": "@@ -41,6 +41,13 @@ This guide describes:\n * common decoding strategies and their main parameters\n * saving and sharing custom generation configurations with your fine-tuned model on ü§ó Hub\n \n+<Tip>\n+\n+`generate()` is a critical component of our [`transformers-cli chat` CLI](quicktour#chat-with-text-generation-models).\n+You can apply the learnings of this guide there as well.\n+\n+</Tip>\n+\n ## Default text generation configuration\n \n A decoding strategy for a model is defined in its generation configuration. When using pre-trained models for inference"
        },
        {
            "sha": "dbec07084d1553e9c84361d82fc06c54550c80a4",
            "filename": "docs/source/en/llm_tutorial.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b3d672246993affc4179b2b7ed3ee09913a66c37/docs%2Fsource%2Fen%2Fllm_tutorial.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b3d672246993affc4179b2b7ed3ee09913a66c37/docs%2Fsource%2Fen%2Fllm_tutorial.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fllm_tutorial.md?ref=b3d672246993affc4179b2b7ed3ee09913a66c37",
            "patch": "@@ -23,6 +23,12 @@ LLMs, or Large Language Models, are the key component behind text generation. In\n \n Autoregressive generation is the inference-time procedure of iteratively calling a model with its own generated outputs, given a few initial inputs. In ü§ó Transformers, this is handled by the [`~generation.GenerationMixin.generate`] method, which is available to all models with generative capabilities.\n \n+<Tip>\n+\n+If you want to jump straight to chatting with a model, [try our `transformers-cli chat` CLI](quicktour#chat-with-text-generation-models).\n+\n+</Tip>\n+\n This tutorial will show you how to:\n \n * Generate text with an LLM"
        },
        {
            "sha": "a40798a164ffd7059dc0503c74c48d0a25f07897",
            "filename": "docs/source/en/quicktour.md",
            "status": "modified",
            "additions": 26,
            "deletions": 0,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/b3d672246993affc4179b2b7ed3ee09913a66c37/docs%2Fsource%2Fen%2Fquicktour.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b3d672246993affc4179b2b7ed3ee09913a66c37/docs%2Fsource%2Fen%2Fquicktour.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquicktour.md?ref=b3d672246993affc4179b2b7ed3ee09913a66c37",
            "patch": "@@ -553,6 +553,32 @@ All models are a standard [`tf.keras.Model`](https://www.tensorflow.org/api_docs\n    >>> model.fit(tf_dataset)  # doctest: +SKIP\n    ```\n \n+\n+## Chat with text generation models\n+\n+If you're working with a model that generates text as an output, you can also engage in a multi-turn conversation with\n+it through the `transformers-cli chat` command. This is the fastest way to interact with a model, e.g. for a\n+qualitative assessment (aka vibe check).\n+\n+This CLI is implemented on top of our `AutoClass` abstraction, leveraging our [text generation](llm_tutorial.md) and\n+[chat](chat_templating.md) tooling, and thus will be compatible with any ü§ó Transformers model. If you have the library\n+[installed](installation.md), you can launch the chat session on your terminal with\n+\n+```\n+transformers-cli chat --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct\n+```\n+\n+For a full list of options to launch the chat, type\n+\n+```\n+transformers-cli chat -h\n+```\n+\n+After the chat is launched, you will enter an interactive session with the model. There are special commands for this\n+session as well, such as `clear` to reset the conversation. Type `help` at any moment to display all special chat\n+commands, and `exit` to terminate the session.\n+\n+\n ## What's next?\n \n Now that you've completed the ü§ó Transformers quick tour, check out our guides and learn how to do more specific things like writing a custom model, fine-tuning a model for a task, and how to train a model with a script. If you're interested in learning more about ü§ó Transformers core concepts, grab a cup of coffee and take a look at our Conceptual Guides!"
        },
        {
            "sha": "ed69328591525e9c240776179bead633eb5cdb5a",
            "filename": "src/transformers/commands/chat.py",
            "status": "added",
            "additions": 539,
            "deletions": 0,
            "changes": 539,
            "blob_url": "https://github.com/huggingface/transformers/blob/b3d672246993affc4179b2b7ed3ee09913a66c37/src%2Ftransformers%2Fcommands%2Fchat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b3d672246993affc4179b2b7ed3ee09913a66c37/src%2Ftransformers%2Fcommands%2Fchat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcommands%2Fchat.py?ref=b3d672246993affc4179b2b7ed3ee09913a66c37",
            "patch": "@@ -0,0 +1,539 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\n+import copy\n+import json\n+import os\n+import platform\n+import re\n+import time\n+from argparse import ArgumentParser, Namespace\n+from dataclasses import dataclass, field\n+from threading import Thread\n+from typing import Optional\n+\n+import torch\n+import yaml\n+from rich.console import Console\n+from rich.live import Live\n+from rich.markdown import Markdown\n+\n+from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TextIteratorStreamer\n+\n+from . import BaseTransformersCLICommand\n+\n+\n+if platform.system() != \"Windows\":\n+    import pwd\n+\n+\n+HELP_STRING = \"\"\"\\\n+\n+**TRANSFORMERS CHAT INTERFACE**\n+\n+The chat interface is a simple tool to try out a chat model.\n+\n+Besides talking to the model there are several commands:\n+- **help**: show this help message\n+- **clear**: clears the current conversation and start a new one\n+- **example {NAME}**: load example named `{NAME}` from the config and use it as the user input\n+- **set {SETTING_NAME}={SETTING_VALUE};**: change the system prompt or generation settings (multiple settings are separated by a ';').\n+- **reset**: same as clear but also resets the generation configs to defaults if they have been changed by **set**\n+- **save {SAVE_NAME} (optional)**: save the current chat and settings to file by default to `./chat_history/{MODEL_NAME}/chat_{DATETIME}.yaml` or `{SAVE_NAME}` if provided\n+- **exit**: closes the interface\n+\"\"\"\n+\n+SUPPORTED_GENERATION_KWARGS = [\n+    \"max_new_tokens\",\n+    \"do_sample\",\n+    \"num_beams\",\n+    \"temperature\",\n+    \"top_p\",\n+    \"top_k\",\n+    \"repetition_penalty\",\n+]\n+\n+SETTING_RE = r\"^set\\s+[A-Za-z\\s_]+=[A-Za-z\\d\\s.!\\\"#$%&'()*+,-/:<=>?@\\[\\]^_`{|}~]+(?:;\\s*[A-Za-z\\s_]+=[A-Za-z\\d\\s.!\\\"#$%&'()*+,-/:<=>?@\\[\\]^_`{|}~]+)*$\"\n+\n+DEFAULT_EXAMPLES = {\n+    \"llama\": {\"text\": \"There is a Llama in my lawn, how can I get rid of it?\"},\n+    \"code\": {\n+        \"text\": \"Write a Python function that integrates any Python function f(x) numerically over an arbitrary interval [x_start, x_end].\"\n+    },\n+    \"helicopter\": {\"text\": \"How many helicopters can a human eat in one sitting?\"},\n+    \"numbers\": {\"text\": \"Count to 10 but skip every number ending with an 'e'\"},\n+    \"birds\": {\"text\": \"Why aren't birds real?\"},\n+    \"socks\": {\"text\": \"Why is it important to eat socks after meditating?\"},\n+}\n+\n+\n+def get_username():\n+    if platform.system() == \"Windows\":\n+        return os.getlogin()\n+    else:\n+        return pwd.getpwuid(os.getuid()).pw_name\n+\n+\n+def create_default_filename(model_name):\n+    time_str = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n+    return f\"{model_name}/chat_{time_str}.json\"\n+\n+\n+def save_chat(chat, args, filename):\n+    output_dict = {}\n+    output_dict[\"settings\"] = vars(args)\n+    output_dict[\"chat_history\"] = chat\n+\n+    folder = args.save_folder\n+\n+    if filename is None:\n+        filename = create_default_filename(args.model_name_or_path)\n+        filename = os.path.join(folder, filename)\n+    os.makedirs(os.path.dirname(filename), exist_ok=True)\n+\n+    with open(filename, \"w\") as f:\n+        json.dump(output_dict, f, indent=4)\n+    return os.path.abspath(filename)\n+\n+\n+def clear_chat_history(system_prompt):\n+    if system_prompt is None:\n+        chat = []\n+    else:\n+        chat = [{\"role\": \"system\", \"content\": system_prompt}]\n+    return chat\n+\n+\n+def parse_settings(user_input, current_args, interface):\n+    settings = user_input[4:].strip().split(\";\")\n+    settings = [(setting.split(\"=\")[0], setting[len(setting.split(\"=\")[0]) + 1 :]) for setting in settings]\n+    settings = dict(settings)\n+    error = False\n+\n+    for name in settings:\n+        if hasattr(current_args, name):\n+            try:\n+                if isinstance(getattr(current_args, name), bool):\n+                    if settings[name] == \"True\":\n+                        settings[name] = True\n+                    elif settings[name] == \"False\":\n+                        settings[name] = False\n+                    else:\n+                        raise ValueError\n+                else:\n+                    settings[name] = type(getattr(current_args, name))(settings[name])\n+            except ValueError:\n+                interface.print_red(\n+                    f\"Cannot cast setting {name} (={settings[name]}) to {type(getattr(current_args, name))}.\"\n+                )\n+        else:\n+            interface.print_red(f\"There is no '{name}' setting.\")\n+\n+    if error:\n+        interface.print_red(\"There was an issue parsing the settings. No settings have been changed.\")\n+        return current_args, False\n+    else:\n+        for name in settings:\n+            setattr(current_args, name, settings[name])\n+            interface.print_green(f\"Set {name} to {settings[name]}.\")\n+\n+        time.sleep(1.5)  # so the user has time to read the changes\n+        return current_args, True\n+\n+\n+def get_quantization_config(model_args) -> Optional[BitsAndBytesConfig]:\n+    if model_args.load_in_4bit:\n+        quantization_config = BitsAndBytesConfig(\n+            load_in_4bit=True,\n+            bnb_4bit_compute_dtype=model_args.torch_dtype,  # For consistency with model weights, we use the same value as `torch_dtype`\n+            bnb_4bit_quant_type=model_args.bnb_4bit_quant_type,\n+            bnb_4bit_use_double_quant=model_args.use_bnb_nested_quant,\n+            bnb_4bit_quant_storage=model_args.torch_dtype,\n+        )\n+    elif model_args.load_in_8bit:\n+        quantization_config = BitsAndBytesConfig(\n+            load_in_8bit=True,\n+        )\n+    else:\n+        quantization_config = None\n+\n+    return quantization_config\n+\n+\n+def load_model_and_tokenizer(args):\n+    tokenizer = AutoTokenizer.from_pretrained(\n+        args.model_name_or_path,\n+        revision=args.model_revision,\n+        trust_remote_code=args.trust_remote_code,\n+    )\n+\n+    torch_dtype = args.torch_dtype if args.torch_dtype in [\"auto\", None] else getattr(torch, args.torch_dtype)\n+    quantization_config = get_quantization_config(args)\n+    model_kwargs = {\n+        \"revision\": args.model_revision,\n+        \"attn_implementation\": args.attn_implementation,\n+        \"torch_dtype\": torch_dtype,\n+        \"device_map\": \"auto\",\n+        \"quantization_config\": quantization_config,\n+    }\n+    model = AutoModelForCausalLM.from_pretrained(\n+        args.model_name_or_path, trust_remote_code=args.trust_remote_code, **model_kwargs\n+    )\n+\n+    if getattr(model, \"hf_device_map\", None) is None:\n+        model = model.to(args.device)\n+\n+    return model, tokenizer\n+\n+\n+def parse_eos_tokens(tokenizer, eos_tokens, eos_token_ids):\n+    if tokenizer.pad_token_id is None:\n+        pad_token_id = tokenizer.eos_token_id\n+    else:\n+        pad_token_id = tokenizer.pad_token_id\n+\n+    all_eos_token_ids = []\n+\n+    if eos_tokens is not None:\n+        all_eos_token_ids.extend(tokenizer.convert_tokens_to_ids(eos_tokens.split(\",\")))\n+\n+    if eos_token_ids is not None:\n+        all_eos_token_ids.extend([int(token_id) for token_id in eos_token_ids.split(\",\")])\n+\n+    if len(all_eos_token_ids) == 0:\n+        all_eos_token_ids.append(tokenizer.eos_token_id)\n+\n+    return pad_token_id, all_eos_token_ids\n+\n+\n+class RichInterface:\n+    def __init__(self, model_name=None, user_name=None):\n+        self._console = Console()\n+        if model_name is None:\n+            self.model_name = \"assistant\"\n+        else:\n+            self.model_name = model_name\n+        if user_name is None:\n+            self.user_name = \"user\"\n+        else:\n+            self.user_name = user_name\n+\n+    def stream_output(self, output_stream):\n+        \"\"\"Stream output from a role.\"\"\"\n+        # This method is originally from the FastChat CLI: https://github.com/lm-sys/FastChat/blob/main/fastchat/serve/cli.py\n+        # Create a Live context for updating the console output\n+        text = \"\"\n+        self._console.print(f\"[bold blue]<{self.model_name}>:\")\n+        with Live(console=self._console, refresh_per_second=4) as live:\n+            # Read lines from the stream\n+            for i, outputs in enumerate(output_stream):\n+                if not outputs or i == 0:\n+                    continue\n+                text += outputs\n+                # Render the accumulated text as Markdown\n+                # NOTE: this is a workaround for the rendering \"unstandard markdown\"\n+                #  in rich. The chatbots output treat \"\\n\" as a new line for\n+                #  better compatibility with real-world text. However, rendering\n+                #  in markdown would break the format. It is because standard markdown\n+                #  treat a single \"\\n\" in normal text as a space.\n+                #  Our workaround is adding two spaces at the end of each line.\n+                #  This is not a perfect solution, as it would\n+                #  introduce trailing spaces (only) in code block, but it works well\n+                #  especially for console output, because in general the console does not\n+                #  care about trailing spaces.\n+                lines = []\n+                for line in text.splitlines():\n+                    lines.append(line)\n+                    if line.startswith(\"```\"):\n+                        # Code block marker - do not add trailing spaces, as it would\n+                        #  break the syntax highlighting\n+                        lines.append(\"\\n\")\n+                    else:\n+                        lines.append(\"  \\n\")\n+                markdown = Markdown(\"\".join(lines).strip(), code_theme=\"github-dark\")\n+                # Update the Live console output\n+                live.update(markdown)\n+        self._console.print()\n+        return text\n+\n+    def input(self):\n+        input = self._console.input(f\"[bold red]<{self.user_name}>:\\n\")\n+        self._console.print()\n+        return input\n+\n+    def clear(self):\n+        self._console.clear()\n+\n+    def print_user_message(self, text):\n+        self._console.print(f\"[bold red]<{self.user_name}>:[/ bold red]\\n{text}\")\n+        self._console.print()\n+\n+    def print_green(self, text):\n+        self._console.print(f\"[bold green]{text}\")\n+        self._console.print()\n+\n+    def print_red(self, text):\n+        self._console.print(f\"[bold red]{text}\")\n+        self._console.print()\n+\n+    def print_help(self):\n+        self._console.print(Markdown(HELP_STRING))\n+        self._console.print()\n+\n+\n+@dataclass\n+class ChatArguments:\n+    r\"\"\"\n+    Arguments for the chat script.\n+\n+    Args:\n+        model_name_or_path (`str`):\n+            Name of the pre-trained model.\n+        user (`str` or `None`, *optional*, defaults to `None`):\n+            Username to display in chat interface.\n+        system_prompt (`str` or `None`, *optional*, defaults to `None`):\n+            System prompt.\n+        save_folder (`str`, *optional*, defaults to `\"./chat_history/\"`):\n+            Folder to save chat history.\n+        device (`str`, *optional*, defaults to `\"cpu\"`):\n+            Device to use for inference.\n+        examples_path (`str` or `None`, *optional*, defaults to `None`):\n+            Path to a yaml file with examples.\n+        max_new_tokens (`int`, *optional*, defaults to `256`):\n+            Maximum number of tokens to generate.\n+        do_sample (`bool`, *optional*, defaults to `True`):\n+            Whether to sample outputs during generation.\n+        num_beams (`int`, *optional*, defaults to `1`):\n+            Number of beams for beam search.\n+        temperature (`float`, *optional*, defaults to `1.0`):\n+            Temperature parameter for generation.\n+        top_k (`int`, *optional*, defaults to `50`):\n+            Value of k for top-k sampling.\n+        top_p (`float`, *optional*, defaults to `1.0`):\n+            Value of p for nucleus sampling.\n+        repetition_penalty (`float`, *optional*, defaults to `1.0`):\n+            Repetition penalty.\n+        eos_tokens (`str` or `None`, *optional*, defaults to `None`):\n+            EOS tokens to stop the generation. If multiple they should be comma separated.\n+        eos_token_ids (`str` or `None`, *optional*, defaults to `None`):\n+            EOS token IDs to stop the generation. If multiple they should be comma separated.\n+        model_revision (`str`, *optional*, defaults to `\"main\"`):\n+            Specific model version to use (can be a branch name, tag name or commit id).\n+        torch_dtype (`str` or `None`, *optional*, defaults to `None`):\n+            Override the default `torch.dtype` and load the model under this dtype. If `'auto'` is passed, the dtype\n+            will be automatically derived from the model's weights.\n+        trust_remote_code (`bool`, *optional*, defaults to `False`):\n+            Whether to trust remote code when loading a model.\n+        attn_implementation (`str` or `None`, *optional*, defaults to `None`):\n+            Which attention implementation to use; you can run --attn_implementation=flash_attention_2, in which case\n+            you must install this manually by running `pip install flash-attn --no-build-isolation`.\n+        load_in_8bit (`bool`, *optional*, defaults to `False`):\n+            Whether to use 8 bit precision for the base model - works only with LoRA.\n+        load_in_4bit (`bool`, *optional*, defaults to `False`):\n+            Whether to use 4 bit precision for the base model - works only with LoRA.\n+        bnb_4bit_quant_type (`str`, *optional*, defaults to `\"nf4\"`):\n+            Quantization type.\n+        use_bnb_nested_quant (`bool`, *optional*, defaults to `False`):\n+            Whether to use nested quantization.\n+    \"\"\"\n+\n+    # General settings\n+    model_name_or_path: str = field(metadata={\"help\": \"Name of the pre-trained model.\"})\n+    user: Optional[str] = field(default=None, metadata={\"help\": \"Username to display in chat interface.\"})\n+    system_prompt: Optional[str] = field(default=None, metadata={\"help\": \"System prompt.\"})\n+    save_folder: str = field(default=\"./chat_history/\", metadata={\"help\": \"Folder to save chat history.\"})\n+    device: str = field(default=\"cpu\", metadata={\"help\": \"Device to use for inference.\"})\n+    examples_path: Optional[str] = field(default=None, metadata={\"help\": \"Path to a yaml file with examples.\"})\n+\n+    # Generation settings\n+    max_new_tokens: int = field(default=256, metadata={\"help\": \"Maximum number of tokens to generate.\"})\n+    do_sample: bool = field(default=True, metadata={\"help\": \"Whether to sample outputs during generation.\"})\n+    num_beams: int = field(default=1, metadata={\"help\": \"Number of beams for beam search.\"})\n+    temperature: float = field(default=1.0, metadata={\"help\": \"Temperature parameter for generation.\"})\n+    top_k: int = field(default=50, metadata={\"help\": \"Value of k for top-k sampling.\"})\n+    top_p: float = field(default=1.0, metadata={\"help\": \"Value of p for nucleus sampling.\"})\n+    repetition_penalty: float = field(default=1.0, metadata={\"help\": \"Repetition penalty.\"})\n+    eos_tokens: Optional[str] = field(\n+        default=None,\n+        metadata={\"help\": \"EOS tokens to stop the generation. If multiple they should be comma separated.\"},\n+    )\n+    eos_token_ids: Optional[str] = field(\n+        default=None,\n+        metadata={\"help\": \"EOS token IDs to stop the generation. If multiple they should be comma separated.\"},\n+    )\n+\n+    # Model loading\n+    model_revision: str = field(\n+        default=\"main\",\n+        metadata={\"help\": \"Specific model version to use (can be a branch name, tag name or commit id).\"},\n+    )\n+    torch_dtype: Optional[str] = field(\n+        default=\"auto\",\n+        metadata={\n+            \"help\": \"Override the default `torch.dtype` and load the model under this dtype. If `'auto'` is passed, \"\n+            \"the dtype will be automatically derived from the model's weights.\",\n+            \"choices\": [\"auto\", \"bfloat16\", \"float16\", \"float32\"],\n+        },\n+    )\n+    trust_remote_code: bool = field(\n+        default=False, metadata={\"help\": \"Whether to trust remote code when loading a model.\"}\n+    )\n+    attn_implementation: Optional[str] = field(\n+        default=None,\n+        metadata={\n+            \"help\": \"Which attention implementation to use; you can run --attn_implementation=flash_attention_2, in \"\n+            \"which case you must install this manually by running `pip install flash-attn --no-build-isolation`.\"\n+        },\n+    )\n+    load_in_8bit: bool = field(\n+        default=False,\n+        metadata={\"help\": \"Whether to use 8 bit precision for the base model - works only with LoRA.\"},\n+    )\n+    load_in_4bit: bool = field(\n+        default=False,\n+        metadata={\"help\": \"Whether to use 4 bit precision for the base model - works only with LoRA.\"},\n+    )\n+    bnb_4bit_quant_type: str = field(default=\"nf4\", metadata={\"help\": \"Quantization type.\", \"choices\": [\"fp4\", \"nf4\"]})\n+    use_bnb_nested_quant: bool = field(default=False, metadata={\"help\": \"Whether to use nested quantization.\"})\n+\n+\n+def chat_command_factory(args: Namespace):\n+    \"\"\"\n+    Factory function used to chat with a local model.\n+    \"\"\"\n+    return ChatCommand(args)\n+\n+\n+class ChatCommand(BaseTransformersCLICommand):\n+    @staticmethod\n+    def register_subcommand(parser: ArgumentParser):\n+        \"\"\"\n+        Register this command to argparse so it's available for the transformer-cli\n+\n+        Args:\n+            parser: Root parser to register command-specific arguments\n+        \"\"\"\n+        dataclass_types = (ChatArguments,)\n+        chat_parser = parser.add_parser(\"chat\", help=HELP_STRING, dataclass_types=dataclass_types)\n+        chat_parser.set_defaults(func=chat_command_factory)\n+\n+    def __init__(self, args):\n+        self.args = args\n+\n+    def run(self):\n+        args = self.args\n+        if args.examples_path is None:\n+            examples = DEFAULT_EXAMPLES\n+        else:\n+            with open(args.examples_path) as f:\n+                examples = yaml.safe_load(f)\n+\n+        current_args = copy.deepcopy(args)\n+\n+        if args.user is None:\n+            user = get_username()\n+        else:\n+            user = args.user\n+\n+        model, tokenizer = load_model_and_tokenizer(args)\n+        generation_streamer = TextIteratorStreamer(tokenizer, skip_special_tokens=True, skip_prompt=True)\n+\n+        pad_token_id, eos_token_ids = parse_eos_tokens(tokenizer, args.eos_tokens, args.eos_token_ids)\n+\n+        interface = RichInterface(model_name=args.model_name_or_path, user_name=user)\n+        interface.clear()\n+        chat = clear_chat_history(current_args.system_prompt)\n+        while True:\n+            try:\n+                user_input = interface.input()\n+\n+                if user_input == \"clear\":\n+                    chat = clear_chat_history(current_args.system_prompt)\n+                    interface.clear()\n+                    continue\n+\n+                if user_input == \"help\":\n+                    interface.print_help()\n+                    continue\n+\n+                if user_input == \"exit\":\n+                    break\n+\n+                if user_input == \"reset\":\n+                    interface.clear()\n+                    current_args = copy.deepcopy(args)\n+                    chat = clear_chat_history(current_args.system_prompt)\n+                    continue\n+\n+                if user_input.startswith(\"save\") and len(user_input.split()) < 2:\n+                    split_input = user_input.split()\n+\n+                    if len(split_input) == 2:\n+                        filename = split_input[1]\n+                    else:\n+                        filename = None\n+                    filename = save_chat(chat, current_args, filename)\n+                    interface.print_green(f\"Chat saved in {filename}!\")\n+                    continue\n+\n+                if re.match(SETTING_RE, user_input):\n+                    current_args, success = parse_settings(user_input, current_args, interface)\n+                    if success:\n+                        chat = []\n+                        interface.clear()\n+                        continue\n+\n+                if user_input.startswith(\"example\") and len(user_input.split()) == 2:\n+                    example_name = user_input.split()[1]\n+                    if example_name in examples:\n+                        interface.clear()\n+                        chat = []\n+                        interface.print_user_message(examples[example_name][\"text\"])\n+                        user_input = examples[example_name][\"text\"]\n+                    else:\n+                        interface.print_red(\n+                            f\"Example {example_name} not found in list of available examples: {list(examples.keys())}.\"\n+                        )\n+                        continue\n+\n+                chat.append({\"role\": \"user\", \"content\": user_input})\n+\n+                inputs = tokenizer.apply_chat_template(chat, return_tensors=\"pt\", add_generation_prompt=True).to(\n+                    model.device\n+                )\n+                attention_mask = torch.ones_like(inputs)\n+                generation_kwargs = {\n+                    \"inputs\": inputs,\n+                    \"attention_mask\": attention_mask,\n+                    \"streamer\": generation_streamer,\n+                    \"max_new_tokens\": current_args.max_new_tokens,\n+                    \"do_sample\": current_args.do_sample,\n+                    \"num_beams\": current_args.num_beams,\n+                    \"temperature\": current_args.temperature,\n+                    \"top_k\": current_args.top_k,\n+                    \"top_p\": current_args.top_p,\n+                    \"repetition_penalty\": current_args.repetition_penalty,\n+                    \"pad_token_id\": pad_token_id,\n+                    \"eos_token_id\": eos_token_ids,\n+                }\n+\n+                thread = Thread(target=model.generate, kwargs=generation_kwargs)\n+                thread.start()\n+                model_output = interface.stream_output(generation_streamer)\n+                thread.join()\n+                chat.append({\"role\": \"assistant\", \"content\": model_output})\n+\n+            except KeyboardInterrupt:\n+                break"
        },
        {
            "sha": "dde88797f69bfa243757691ab7bd4af9eef5b3f2",
            "filename": "src/transformers/commands/transformers_cli.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b3d672246993affc4179b2b7ed3ee09913a66c37/src%2Ftransformers%2Fcommands%2Ftransformers_cli.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b3d672246993affc4179b2b7ed3ee09913a66c37/src%2Ftransformers%2Fcommands%2Ftransformers_cli.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcommands%2Ftransformers_cli.py?ref=b3d672246993affc4179b2b7ed3ee09913a66c37",
            "patch": "@@ -13,9 +13,10 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from argparse import ArgumentParser\n+from transformers import HfArgumentParser\n \n from .add_new_model_like import AddNewModelLikeCommand\n+from .chat import ChatCommand\n from .convert import ConvertCommand\n from .download import DownloadCommand\n from .env import EnvironmentCommand\n@@ -26,10 +27,11 @@\n \n \n def main():\n-    parser = ArgumentParser(\"Transformers CLI tool\", usage=\"transformers-cli <command> [<args>]\")\n+    parser = HfArgumentParser(prog=\"Transformers CLI tool\", usage=\"transformers-cli <command> [<args>]\")\n     commands_parser = parser.add_subparsers(help=\"transformers-cli command helpers\")\n \n     # Register commands\n+    ChatCommand.register_subcommand(commands_parser)\n     ConvertCommand.register_subcommand(commands_parser)\n     DownloadCommand.register_subcommand(commands_parser)\n     EnvironmentCommand.register_subcommand(commands_parser)"
        },
        {
            "sha": "625c6b90f511416069808a434bc92cd563d8cbcb",
            "filename": "src/transformers/hf_argparser.py",
            "status": "modified",
            "additions": 13,
            "deletions": 8,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/b3d672246993affc4179b2b7ed3ee09913a66c37/src%2Ftransformers%2Fhf_argparser.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b3d672246993affc4179b2b7ed3ee09913a66c37/src%2Ftransformers%2Fhf_argparser.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fhf_argparser.py?ref=b3d672246993affc4179b2b7ed3ee09913a66c37",
            "patch": "@@ -114,18 +114,23 @@ class HfArgumentParser(ArgumentParser):\n     The class is designed to play well with the native argparse. In particular, you can add more (non-dataclass backed)\n     arguments to the parser after initialization and you'll get the output back after parsing as an additional\n     namespace. Optional: To create sub argument groups use the `_argument_group_name` attribute in the dataclass.\n+\n+    Args:\n+        dataclass_types (`DataClassType` or `Iterable[DataClassType]`, *optional*):\n+            Dataclass type, or list of dataclass types for which we will \"fill\" instances with the parsed args.\n+        kwargs (`Dict[str, Any]`, *optional*):\n+            Passed to `argparse.ArgumentParser()` in the regular way.\n     \"\"\"\n \n     dataclass_types: Iterable[DataClassType]\n \n-    def __init__(self, dataclass_types: Union[DataClassType, Iterable[DataClassType]], **kwargs):\n-        \"\"\"\n-        Args:\n-            dataclass_types:\n-                Dataclass type, or list of dataclass types for which we will \"fill\" instances with the parsed args.\n-            kwargs (`Dict[str, Any]`, *optional*):\n-                Passed to `argparse.ArgumentParser()` in the regular way.\n-        \"\"\"\n+    def __init__(self, dataclass_types: Optional[Union[DataClassType, Iterable[DataClassType]]] = None, **kwargs):\n+        # Make sure dataclass_types is an iterable\n+        if dataclass_types is None:\n+            dataclass_types = []\n+        elif not isinstance(dataclass_types, Iterable):\n+            dataclass_types = [dataclass_types]\n+\n         # To make the default appear when using --help\n         if \"formatter_class\" not in kwargs:\n             kwargs[\"formatter_class\"] = ArgumentDefaultsHelpFormatter"
        }
    ],
    "stats": {
        "total": 798,
        "additions": 695,
        "deletions": 103
    }
}