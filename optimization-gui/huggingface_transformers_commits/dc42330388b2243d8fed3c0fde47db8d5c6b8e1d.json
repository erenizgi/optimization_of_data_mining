{
    "author": "sywangyi",
    "message": "fix crash in tiiuae/falcon-11B-vlm image-to-text generation (#34728)\n\nSigned-off-by: Wang, Yi <yi.a.wang@intel.com>",
    "sha": "dc42330388b2243d8fed3c0fde47db8d5c6b8e1d",
    "files": [
        {
            "sha": "faea670ecbf428b2c65518b3bd417ddfb7618728",
            "filename": "src/transformers/models/falcon/modeling_falcon.py",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/dc42330388b2243d8fed3c0fde47db8d5c6b8e1d/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dc42330388b2243d8fed3c0fde47db8d5c6b8e1d/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py?ref=dc42330388b2243d8fed3c0fde47db8d5c6b8e1d",
            "patch": "@@ -1277,12 +1277,18 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        num_logits_to_keep: int = 0,\n     ) -> Union[Tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\n             `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\n             are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\n+\n+        num_logits_to_keep (`int`, *optional*):\n+            Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+            `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n+            token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n         \"\"\"\n \n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n@@ -1302,7 +1308,7 @@ def forward(\n         )\n         hidden_states = transformer_outputs[0]\n \n-        lm_logits = self.lm_head(hidden_states)\n+        lm_logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n \n         loss = None\n         if labels is not None:"
        }
    ],
    "stats": {
        "total": 8,
        "additions": 7,
        "deletions": 1
    }
}