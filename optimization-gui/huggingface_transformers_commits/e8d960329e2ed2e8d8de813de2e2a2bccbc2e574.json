{
    "author": "drisspg",
    "message": "Add option for ao base configs (#36526)",
    "sha": "e8d960329e2ed2e8d8de813de2e2a2bccbc2e574",
    "files": [
        {
            "sha": "31e2d4f0201c3c0a3063975da60d8571b12b7de4",
            "filename": "docs/source/en/quantization/torchao.md",
            "status": "modified",
            "additions": 80,
            "deletions": 3,
            "changes": 83,
            "blob_url": "https://github.com/huggingface/transformers/blob/e8d960329e2ed2e8d8de813de2e2a2bccbc2e574/docs%2Fsource%2Fen%2Fquantization%2Ftorchao.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e8d960329e2ed2e8d8de813de2e2a2bccbc2e574/docs%2Fsource%2Fen%2Fquantization%2Ftorchao.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Ftorchao.md?ref=e8d960329e2ed2e8d8de813de2e2a2bccbc2e574",
            "patch": "@@ -20,18 +20,95 @@ Install torchao with the following command.\n pip install --upgrade torch torchao transformers\n ```\n \n-torchao supports many quantization types for different data types (int4, float8, weight only, etc.), but the Transformers integration only currently supports int8 weight quantization and int8 dynamic quantization of weights.\n+torchao supports many quantization types for different data types (int4, float8, weight only, etc.).\n+Starting with version 0.10.0, torchao provides enhanced flexibility through the `AOBaseConfig` API, allowing for more customized quantization configurations.\n+And full access to the techniques offered in the torchao library.\n \n You can manually choose the quantization types and settings or automatically select the quantization types.\n \n <hfoptions id=\"torchao\">\n <hfoption id=\"manual\">\n \n+\n Create a [`TorchAoConfig`] and specify the quantization type and `group_size` of the weights to quantize. Set the `cache_implementation` to `\"static\"` to automatically [torch.compile](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html) the forward method.\n \n > [!TIP]\n > Run the quantized model on a CPU by changing `device_map` to `\"cpu\"` and `layout` to `Int4CPULayout()`. This is only available in torchao 0.8.0+.\n \n+In torchao 0.10.0+, you can use the more flexible `AOBaseConfig` approach instead of string identifiers:\n+\n+```py\n+import torch\n+from transformers import TorchAoConfig, AutoModelForCausalLM, AutoTokenizer\n+from torchao.quantization import Int4WeightOnlyConfig\n+\n+# Using AOBaseConfig instance (torchao >= 0.10.0)\n+quant_config = Int4WeightOnlyConfig(group_size=128)\n+quantization_config = TorchAoConfig(quant_type=quant_config)\n+\n+# Load and quantize the model\n+quantized_model = AutoModelForCausalLM.from_pretrained(\n+    \"meta-llama/Meta-Llama-3-8B\",\n+    torch_dtype=\"auto\",\n+    device_map=\"auto\",\n+    quantization_config=quantization_config\n+)\n+\n+tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n+input_text = \"What are we having for dinner?\"\n+input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n+\n+# auto-compile the quantized model with `cache_implementation=\"static\"` to get speed up\n+output = quantized_model.generate(**input_ids, max_new_tokens=10, cache_implementation=\"static\")\n+print(tokenizer.decode(output[0], skip_special_tokens=True))\n+```\n+\n+## Available Quantization Schemes\n+\n+TorchAO provides a variety of quantization configurations:\n+\n+- `Int4WeightOnlyConfig`\n+- `Int8WeightOnlyConfig`\n+- `Int8DynamicActivationInt8WeightConfig`\n+- `Float8WeightOnlyConfig`\n+\n+Each configuration can be further customized with parameters such as `group_size`, `scheme`, and `layout` to optimize for specific hardware and model architectures.\n+\n+For a complete list of available configurations, see our [quantization API documentation](https://github.com/pytorch/ao/blob/main/torchao/quantization/quant_api.py).\n+\n+> **⚠️ DEPRECATION WARNING**\n+>\n+> Starting with version 0.10.0, the string-based API for quantization configuration (e.g., `TorchAoConfig(\"int4_weight_only\", group_size=128)`) is **deprecated** and will be removed in a future release.\n+>\n+> Please use the new `AOBaseConfig`-based approach instead:\n+>\n+> ```python\n+> # Old way (deprecated)\n+> quantization_config = TorchAoConfig(\"int4_weight_only\", group_size=128)\n+>\n+> # New way (recommended)\n+> from torchao.quantization import Int4WeightOnlyConfig\n+> quant_config = Int4WeightOnlyConfig(group_size=128)\n+> quantization_config = TorchAoConfig(quant_type=quant_config)\n+> ```\n+>\n+> The new API offers greater flexibility, better type safety, and access to the full range of features available in torchao.\n+>\n+> ## Migration Guide\n+>\n+> Here's how to migrate from common string identifiers to their `AOBaseConfig` equivalents:\n+>\n+> | Old String API | New `AOBaseConfig` API |\n+> |----------------|------------------------|\n+> | `\"int4_weight_only\"` | `Int4WeightOnlyConfig()` |\n+> | `\"int8_weight_only\"` | `Int8WeightOnlyConfig()` |\n+> | `\"int8_dynamic_activation_int8_weight\"` | `Int8DynamicActivationInt8WeightConfig()` |\n+>\n+> All configuration objects accept parameters for customization (e.g., `group_size`, `scheme`, `layout`).\n+\n+\n+Below is the API for for torchao < `0.9.0`\n+\n ```py\n import torch\n from transformers import TorchAoConfig, AutoModelForCausalLM, AutoTokenizer\n@@ -78,7 +155,7 @@ print(\"bf16 model:\", benchmark_fn(bf16_model.generate, **input_ids, max_new_toke\n \n The [autoquant](https://pytorch.org/ao/stable/generated/torchao.quantization.autoquant.html#torchao.quantization.autoquant) API automatically chooses a quantization type for quantizable layers (`nn.Linear`) by micro-benchmarking on input type and shape and compiling a single linear layer.\n \n-Create a [`TorchAoConfig`] and set to `\"autoquant\"`. Set the `cache_implementation` to `\"static\"` to automatically [torch.compile](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html) the forward method. Finally, call `finalize_autoquant` on the quantized model to finalize the quantization and log the input shapes. \n+Create a [`TorchAoConfig`] and set to `\"autoquant\"`. Set the `cache_implementation` to `\"static\"` to automatically [torch.compile](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html) the forward method. Finally, call `finalize_autoquant` on the quantized model to finalize the quantization and log the input shapes.\n \n > [!TIP]\n > Run the quantized model on a CPU by changing `device_map` to `\"cpu\"` and `layout` to `Int4CPULayout()`. This is only available in torchao 0.8.0+.\n@@ -131,7 +208,7 @@ print(\"bf16 model:\", benchmark_fn(bf16_model.generate, **input_ids, max_new_toke\n \n ## Serialization\n \n-torchao implements [torch.Tensor subclasses](https://pytorch.org/docs/stable/notes/extending.html#subclassing-torch-tensor) for maximum flexibility in supporting new quantized torch.Tensor formats. [Safetensors](https://huggingface.co/docs/safetensors/en/index) serialization and deserialization does not work with torchaco.\n+torchao implements [torch.Tensor subclasses](https://pytorch.org/docs/stable/notes/extending.html#subclassing-torch-tensor) for maximum flexibility in supporting new quantized torch.Tensor formats. [Safetensors](https://huggingface.co/docs/safetensors/en/index) serialization and deserialization does not work with torchao.\n \n To avoid arbitrary user code execution, torchao sets `weights_only=True` in [torch.load](https://pytorch.org/docs/stable/generated/torch.load.html) to ensure only tensors are loaded. Any known user functions can be whitelisted with [add_safe_globals](https://pytorch.org/docs/stable/notes/serialization.html#torch.serialization.add_safe_globals).\n "
        },
        {
            "sha": "1dddaf19d44f63531d88eefcfdde391847d00366",
            "filename": "src/transformers/quantizers/quantizer_torchao.py",
            "status": "modified",
            "additions": 40,
            "deletions": 5,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/e8d960329e2ed2e8d8de813de2e2a2bccbc2e574/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e8d960329e2ed2e8d8de813de2e2a2bccbc2e574/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py?ref=e8d960329e2ed2e8d8de813de2e2a2bccbc2e574",
            "patch": "@@ -12,6 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n import importlib\n+import re\n import types\n from typing import TYPE_CHECKING, Optional, Union\n \n@@ -27,6 +28,7 @@\n from typing import Any, Dict, List\n \n from ..utils import is_torch_available, is_torchao_available, logging\n+from ..utils.quantization_config import TorchAoConfig\n \n \n if is_torch_available():\n@@ -36,6 +38,21 @@\n logger = logging.get_logger(__name__)\n \n \n+def fuzzy_match_size(config_name: str) -> Optional[str]:\n+    \"\"\"\n+    Extract the size digit from strings like \"4weight\", \"8weight\".\n+    Returns the digit as an integer if found, otherwise None.\n+    \"\"\"\n+    config_name = config_name.lower()\n+\n+    str_match = re.search(r\"(\\d)weight\", config_name)\n+\n+    if str_match:\n+        return str_match.group(1)\n+\n+    return None\n+\n+\n # Finds the parent of a node module named \"name\"\n def find_parent(model, name):\n     module_tree = name.split(\".\")[:-1]\n@@ -121,10 +138,28 @@ def update_torch_dtype(self, torch_dtype):\n                 torch_dtype = torch.float32\n         return torch_dtype\n \n-    def adjust_target_dtype(self, target_dtype: \"torch.dtype\") -> \"torch.dtype\":\n+    def adjust_target_dtype(self, torch_dtype: \"torch.dtype\") -> \"torch.dtype\":\n         if version.parse(importlib.metadata.version(\"accelerate\")) > version.parse(\"0.19.0\"):\n             from accelerate.utils import CustomDtype\n \n+            # Import AOBaseConfig directly since we know we have the right version\n+            if self.quantization_config._get_ao_version() >= version.Version(\"0.10.0\"):\n+                from torchao.core.config import AOBaseConfig\n+\n+                quant_type = self.quantization_config.quant_type\n+                if isinstance(quant_type, AOBaseConfig):\n+                    # Extract size digit using fuzzy match on the class name\n+                    config_name = quant_type.__class__.__name__\n+                    size_digit = fuzzy_match_size(config_name)\n+\n+                    # Map the extracted digit to appropriate dtype\n+                    if size_digit == \"4\":\n+                        return CustomDtype.INT4\n+                    else:\n+                        # Default to int8\n+                        return torch.int8\n+\n+            # Original mapping for non-AOBaseConfig types\n             map_to_target_dtype = {\n                 \"int4_weight_only\": CustomDtype.INT4,\n                 \"int8_weight_only\": torch.int8,\n@@ -194,14 +229,14 @@ def create_quantized_param(\n         from torchao.quantization import quantize_\n \n         module, tensor_name = get_module_from_name(model, param_name)\n-\n         if self.pre_quantized:\n             module._parameters[tensor_name] = torch.nn.Parameter(param_value.to(device=target_device))\n             if isinstance(module, nn.Linear):\n                 module.extra_repr = types.MethodType(_linear_extra_repr, module)\n         else:\n+            assert isinstance(self.quantization_config, TorchAoConfig)\n             module._parameters[tensor_name] = torch.nn.Parameter(param_value).to(device=target_device)\n-            quantize_(module, self.quantization_config.get_apply_tensor_subclass())\n+            quantize_(module, self.quantization_config.get_quantize_config())\n \n     def _process_model_after_weight_loading(self, model, **kwargs):\n         \"\"\"No process required for torchao quantized model\"\"\"\n@@ -216,7 +251,7 @@ def _process_model_after_weight_loading(self, model, **kwargs):\n             return model\n         return\n \n-    def is_serializable(self, safe_serialization=None):\n+    def is_serializable(self, safe_serialization=None) -> bool:\n         if safe_serialization:\n             logger.warning(\n                 \"torchao quantized model does not support safe serialization, \"\n@@ -237,7 +272,7 @@ def is_serializable(self, safe_serialization=None):\n         return _is_torchao_serializable\n \n     @property\n-    def is_trainable(self):\n+    def is_trainable(self) -> bool:\n         supported_quant_types_for_training = [\n             \"int8_weight_only\",\n             \"int8_dynamic_activation_int8_weight\","
        },
        {
            "sha": "4f2724f1c8a30e11676c0ad4b57ee116101f0659",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/e8d960329e2ed2e8d8de813de2e2a2bccbc2e574/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e8d960329e2ed2e8d8de813de2e2a2bccbc2e574/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=e8d960329e2ed2e8d8de813de2e2a2bccbc2e574",
            "patch": "@@ -95,6 +95,7 @@ def _is_package_available(pkg_name: str, return_version: bool = False) -> Union[\n XLA_FSDPV2_MIN_VERSION = \"2.2.0\"\n HQQ_MIN_VERSION = \"0.2.1\"\n VPTQ_MIN_VERSION = \"0.0.4\"\n+TORCHAO_MIN_VERSION = \"0.4.0\"\n \n \n _accelerate_available, _accelerate_version = _is_package_available(\"accelerate\", return_version=True)\n@@ -191,7 +192,7 @@ def _is_package_available(pkg_name: str, return_version: bool = False) -> Union[\n _timm_available = _is_package_available(\"timm\")\n _tokenizers_available = _is_package_available(\"tokenizers\")\n _torchaudio_available = _is_package_available(\"torchaudio\")\n-_torchao_available = _is_package_available(\"torchao\")\n+_torchao_available, _torchao_version = _is_package_available(\"torchao\", return_version=True)\n _torchdistx_available = _is_package_available(\"torchdistx\")\n _torchvision_available, _torchvision_version = _is_package_available(\"torchvision\", return_version=True)\n _mlx_available = _is_package_available(\"mlx\")\n@@ -1277,8 +1278,8 @@ def is_torchaudio_available():\n     return _torchaudio_available\n \n \n-def is_torchao_available():\n-    return _torchao_available\n+def is_torchao_available(min_version: str = TORCHAO_MIN_VERSION):\n+    return _torchao_available and version.parse(_torchao_version) >= version.parse(min_version)\n \n \n def is_speech_available():"
        },
        {
            "sha": "152572223fdf0ec3c7eabb695c2c48581e4bae66",
            "filename": "src/transformers/utils/quantization_config.py",
            "status": "modified",
            "additions": 130,
            "deletions": 73,
            "changes": 203,
            "blob_url": "https://github.com/huggingface/transformers/blob/e8d960329e2ed2e8d8de813de2e2a2bccbc2e574/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e8d960329e2ed2e8d8de813de2e2a2bccbc2e574/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fquantization_config.py?ref=e8d960329e2ed2e8d8de813de2e2a2bccbc2e574",
            "patch": "@@ -1455,11 +1455,18 @@ def post_init(self):\n \n @dataclass\n class TorchAoConfig(QuantizationConfigMixin):\n+    quant_method: QuantizationMethod\n+    quant_type: Union[str, \"AOBaseConfig\"]  # noqa: F821\n+    modules_to_not_convert: Optional[List]\n+    quant_type_kwargs: Dict[str, Any]\n+\n     \"\"\"This is a config class for torchao quantization/sparsity techniques.\n \n     Args:\n-        quant_type (`str`):\n-            The type of quantization we want to use, currently supporting: `int4_weight_only`, `int8_weight_only`, `int8_dynamic_activation_int8_weight` and `autoquant`.\n+        quant_type (`Union[str, AOBaseConfig]`):\n+            The type of quantization we want to use. Can be either:\n+            - A string: currently supporting: `int4_weight_only`, `int8_weight_only` and `int8_dynamic_activation_int8_weight`.\n+            - An AOBaseConfig instance: for more advanced configuration options.\n         modules_to_not_convert (`list`, *optional*, default to `None`):\n             The list of modules to not quantize, useful for quantizing models that explicitly require to have\n             some modules left in their original precision.\n@@ -1471,9 +1478,12 @@ class TorchAoConfig(QuantizationConfigMixin):\n     Example:\n \n     ```python\n-    from transformers import AutoModelForCausalLM, AutoTokenizer, TorchAoConfig\n+    # AOBaseConfig-based configuration\n+    config = Int4WeightOnlyConfig(group_size=32)\n+    quantization_config = TorchAoConfig(config)\n+    model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cuda\", torch_dtype=torch.bfloat16, quantization_config=quantization_config)\n \n-    # specific quantization method\n+    # String-based configuration\n     quantization_config = TorchAoConfig(\"int4_weight_only\", group_size=32)\n     # int4_weight_only quant is only working with *torch.bfloat16* dtype right now\n     model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cuda\", torch_dtype=torch.bfloat16, quantization_config=quantization_config)\n@@ -1496,105 +1506,152 @@ class TorchAoConfig(QuantizationConfigMixin):\n     if hasattr(quantized_model, \"finalize_autoquant\"):\n       print(\"finalizing autoquant\")\n       quantized_model.finalize_autoquant()\n+\n     ```\n     \"\"\"\n \n-    def __init__(self, quant_type: str, modules_to_not_convert: Optional[List] = None, **kwargs):\n+    def __init__(\n+        self,\n+        quant_type: Union[str, \"AOBaseConfig\"],  # noqa: F821\n+        modules_to_not_convert: Optional[List] = None,\n+        **kwargs,\n+    ):\n         self.quant_method = QuantizationMethod.TORCHAO\n         self.quant_type = quant_type\n         self.modules_to_not_convert = modules_to_not_convert\n-        # when we load from serailized config, \"quant_type_kwargs\" will be the key\n-        if \"quant_type_kwargs\" in kwargs:\n-            self.quant_type_kwargs = kwargs[\"quant_type_kwargs\"]\n-        else:\n-            self.quant_type_kwargs = kwargs\n-\n+        self.quant_type_kwargs = kwargs.get(\"quant_type_kwargs\", kwargs)\n         self.post_init()\n \n+    @staticmethod\n+    def _get_ao_version() -> version.Version:\n+        \"\"\"Centralized check for TorchAO availability and version requirements.\"\"\"\n+        if not is_torchao_available():\n+            raise ValueError(\"TorchAoConfig requires torchao to be installed. Install with `pip install torchao`\")\n+\n+        return version.parse(importlib.metadata.version(\"torchao\"))\n+\n     def post_init(self):\n-        r\"\"\"\n-        Safety checker that arguments are correct - also replaces some NoneType arguments with their default values.\n-        \"\"\"\n-        if is_torchao_available():\n-            if not version.parse(importlib.metadata.version(\"torchao\")) >= version.parse(\"0.7.0\"):\n-                raise ValueError(\"Requires torchao 0.7.0 version and above\")\n+        \"\"\"Validate configuration and set defaults.\"\"\"\n+        ao_version = self._get_ao_version()\n+\n+        # Handle quant_type based on type and version\n+        if isinstance(self.quant_type, str):\n+            self._validate_string_quant_type()\n+        elif ao_version >= version.parse(\"0.10.0\"):\n+            from torchao.quantization.quant_api import AOBaseConfig\n+\n+            if not isinstance(self.quant_type, AOBaseConfig):\n+                raise ValueError(\n+                    f\"quant_type must be either a string or an AOBaseConfig instance, got {type(self.quant_type)}\"\n+                )\n         else:\n             raise ValueError(\n-                \"TorchAoConfig requires torchao to be installed, please install with `pip install torchao`\"\n+                f\"In torchao < 0.10.0, quant_type must be a string. Got {type(self.quant_type)}. \"\n+                f\"Please upgrade to torchao >= 0.10.0 to use AOBaseConfig instances.\"\n             )\n \n-        _STR_TO_METHOD = self._get_torchao_quant_type_to_method()\n-        if self.quant_type not in _STR_TO_METHOD.keys():\n+    def _validate_string_quant_type(self):\n+        \"\"\"Validate string quant_type and its kwargs.\"\"\"\n+        methods = self._get_torchao_quant_type_to_method()\n+\n+        if self.quant_type not in methods:\n             raise ValueError(\n-                f\"Requested quantization type: {self.quant_type} is not supported yet, please add support in TorchAoConfig and TorchAoHfQuantizer.\"\n+                f\"Unsupported string quantization type: {self.quant_type}. \"\n+                f\"Supported types: {', '.join(methods.keys())}\"\n             )\n \n-        method = _STR_TO_METHOD[self.quant_type]\n+        # Validate kwargs against method signature\n+        method = methods[self.quant_type]\n         sig = signature(method)\n-        all_kwargs = [\n+        valid_kwargs = {\n             param.name\n             for param in sig.parameters.values()\n             if param.kind in [Parameter.KEYWORD_ONLY, Parameter.POSITIONAL_OR_KEYWORD]\n-        ]\n-        for k in self.quant_type_kwargs:\n-            if k not in all_kwargs:\n-                raise ValueError(\n-                    f\"Unexpected keyword arg: {k} for API: {method}, accepted keyword args are: {all_kwargs}\"\n-                )\n-\n-    def _get_torchao_quant_type_to_method(self):\n-        if is_torchao_available():\n-            from torchao.quantization import (\n-                autoquant,\n-                int4_weight_only,\n-                int8_dynamic_activation_int8_weight,\n-                int8_weight_only,\n-            )\n+        }\n \n-            return {\n-                \"int4_weight_only\": int4_weight_only,\n-                \"int8_weight_only\": int8_weight_only,\n-                \"int8_dynamic_activation_int8_weight\": int8_dynamic_activation_int8_weight,\n-                \"autoquant\": autoquant,\n-            }\n-        else:\n+        invalid_kwargs = set(self.quant_type_kwargs) - valid_kwargs\n+        if invalid_kwargs:\n             raise ValueError(\n-                \"TorchAoConfig requires torchao to be installed, please install with `pip install torchao`\"\n+                f\"Unexpected keyword arg for {self.quant_type}: {', '.join(invalid_kwargs)}. \"\n+                f\"Valid kwargs: {', '.join(valid_kwargs)}\"\n             )\n \n-    def get_apply_tensor_subclass(self):\n-        _STR_TO_METHOD = self._get_torchao_quant_type_to_method()\n-        quant_type_kwargs = self.quant_type_kwargs.copy()\n-        if (\n-            not torch.cuda.is_available()\n-            and is_torchao_available()\n-            and self.quant_type == \"int4_weight_only\"\n-            and version.parse(importlib.metadata.version(\"torchao\")) >= version.parse(\"0.8.0\")\n-        ):\n-            from torchao.dtypes import Int4CPULayout\n-\n-            quant_type_kwargs[\"layout\"] = Int4CPULayout()\n-        return _STR_TO_METHOD[self.quant_type](**quant_type_kwargs)\n+    def _get_torchao_quant_type_to_method(self):\n+        \"\"\"Get mapping of quant_type strings to their corresponding methods.\"\"\"\n+        from torchao.quantization import (\n+            autoquant,\n+            int4_weight_only,\n+            int8_dynamic_activation_int8_weight,\n+            int8_weight_only,\n+        )\n \n-    def __repr__(self):\n-        config_dict = self.to_dict()\n-        return f\"{self.__class__.__name__} {json.dumps(config_dict, indent=2, sort_keys=True)}\\n\"\n+        return {\n+            \"int4_weight_only\": int4_weight_only,\n+            \"int8_weight_only\": int8_weight_only,\n+            \"int8_dynamic_activation_int8_weight\": int8_dynamic_activation_int8_weight,\n+            \"autoquant\": autoquant,\n+        }\n \n-    def to_dict(self) -> Dict[str, Any]:\n-        \"\"\"\n-        Serializes this instance to a Python dictionary, converting any `torchao.dtypes.Layout`\n-        dataclasses to simple dicts.\n+    def get_quantize_config(self):\n+        \"\"\"Create the appropriate quantization method based on configuration.\"\"\"\n+        if isinstance(self.quant_type, str):\n+            methods = self._get_torchao_quant_type_to_method()\n+            quant_type_kwargs = self.quant_type_kwargs.copy()\n+            if (\n+                not torch.cuda.is_available()\n+                and is_torchao_available()\n+                and self.quant_type == \"int4_weight_only\"\n+                and version.parse(importlib.metadata.version(\"torchao\")) >= version.parse(\"0.8.0\")\n+            ):\n+                from torchao.dtypes import Int4CPULayout\n+\n+                quant_type_kwargs[\"layout\"] = Int4CPULayout()\n+\n+            return methods[self.quant_type](**quant_type_kwargs)\n+        else:\n+            return self.quant_type\n \n-        Returns:\n-            `Dict[str, Any]`: Dictionary of all the attributes that make up this configuration instance.\n-        \"\"\"\n+    def to_dict(self):\n+        \"\"\"Convert configuration to a dictionary.\"\"\"\n         d = super().to_dict()\n-        if \"quant_type_kwargs\" in d and \"layout\" in d[\"quant_type_kwargs\"]:\n-            layout = d[\"quant_type_kwargs\"][\"layout\"]\n-            layout = dataclasses.asdict(layout)\n-            d[\"quant_type_kwargs\"][\"layout\"] = layout\n+\n+        if isinstance(self.quant_type, str):\n+            # Handle layout serialization if present\n+            if \"quant_type_kwargs\" in d and \"layout\" in d[\"quant_type_kwargs\"]:\n+                d[\"quant_type_kwargs\"][\"layout\"] = dataclasses.asdict(d[\"quant_type_kwargs\"][\"layout\"])\n+        else:\n+            # Handle AOBaseConfig serialization\n+            from torchao.core.config import config_to_dict\n+\n+            # For now we assume there is 1 config per Transfomer, however in the future\n+            # We may want to support a config per fqn.\n+            d[\"quant_type\"] = {\"default\": config_to_dict(self.quant_type)}\n+\n         return d\n \n+    @classmethod\n+    def from_dict(cls, config_dict, return_unused_kwargs=False, **kwargs):\n+        \"\"\"Create configuration from a dictionary.\"\"\"\n+        ao_verison = cls._get_ao_version()\n+        assert ao_verison >= version.parse(\n+            \"0.10.0\"\n+        ), \"TorchAoConfig requires torchao >= 0.10.0 for construction from dict\"\n+        config_dict = config_dict.copy()\n+        quant_type = config_dict.pop(\"quant_type\")\n+        # Check if we only have one key which is \"default\"\n+        # In the future we may update this\n+        assert (\n+            len(quant_type) == 1 and \"default\" in quant_type\n+        ), \"Expected only one key 'default' in quant_type dictionary\"\n+        quant_type = quant_type[\"default\"]\n+\n+        # Deserialize quant_type if needed\n+        from torchao.core.config import config_from_dict\n+\n+        quant_type = config_from_dict(quant_type)\n+\n+        return cls(quant_type=quant_type, **config_dict)\n+\n \n @dataclass\n class BitNetConfig(QuantizationConfigMixin):"
        },
        {
            "sha": "037bf1506f1f63a102bad603d428e898c5bbc322",
            "filename": "tests/quantization/torchao_integration/test_torchao.py",
            "status": "modified",
            "additions": 37,
            "deletions": 1,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/e8d960329e2ed2e8d8de813de2e2a2bccbc2e574/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e8d960329e2ed2e8d8de813de2e2a2bccbc2e574/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py?ref=e8d960329e2ed2e8d8de813de2e2a2bccbc2e574",
            "patch": "@@ -85,7 +85,7 @@ def test_post_init_check(self):\n         Test kwargs validations in TorchAoConfig\n         \"\"\"\n         _ = TorchAoConfig(\"int4_weight_only\")\n-        with self.assertRaisesRegex(ValueError, \"is not supported yet\"):\n+        with self.assertRaisesRegex(ValueError, \"Unsupported string quantization type\"):\n             _ = TorchAoConfig(\"fp6\")\n \n         with self.assertRaisesRegex(ValueError, \"Unexpected keyword arg\"):\n@@ -408,5 +408,41 @@ class TorchAoSerializationW8GPUTest(TorchAoSerializationTest):\n     device = \"cuda:0\"\n \n \n+@require_torch_gpu\n+@require_torchao_version_greater_or_equal(\"0.10.0\")\n+class TorchAoSerializationFP8GPUTest(TorchAoSerializationTest):\n+    ORIGINAL_EXPECTED_OUTPUT = \"What are we having for dinner?\\n\\nJessica: (smiling)\"\n+    SERIALIZED_EXPECTED_OUTPUT = ORIGINAL_EXPECTED_OUTPUT\n+    device = \"cuda:0\"\n+\n+    def setUp(self):\n+        if not torch.cuda.is_available() or torch.cuda.get_device_capability()[0] < 9:\n+            raise unittest.SkipTest(\"CUDA compute capability 9.0 or higher required for FP8 tests\")\n+\n+        from torchao.quantization import Float8WeightOnlyConfig\n+\n+        self.quant_scheme = Float8WeightOnlyConfig()\n+        self.quant_scheme_kwargs = {}\n+        super().setUp()\n+\n+\n+@require_torch_gpu\n+@require_torchao_version_greater_or_equal(\"0.10.0\")\n+class TorchAoSerializationA8W4Test(TorchAoSerializationTest):\n+    ORIGINAL_EXPECTED_OUTPUT = \"What are we having for dinner?\\n\\nJessica: (smiling)\"\n+    SERIALIZED_EXPECTED_OUTPUT = ORIGINAL_EXPECTED_OUTPUT\n+    device = \"cuda:0\"\n+\n+    def setUp(self):\n+        if not torch.cuda.is_available() or torch.cuda.get_device_capability()[0] < 9:\n+            raise unittest.SkipTest(\"CUDA compute capability 9.0 or higher required for FP8 tests\")\n+\n+        from torchao.quantization import Int8DynamicActivationInt4WeightConfig\n+\n+        self.quant_scheme = Int8DynamicActivationInt4WeightConfig()\n+        self.quant_scheme_kwargs = {}\n+        super().setUp()\n+\n+\n if __name__ == \"__main__\":\n     unittest.main()"
        }
    ],
    "stats": {
        "total": 376,
        "additions": 291,
        "deletions": 85
    }
}