{
    "author": "MekkCyber",
    "message": "Fix auto-round hfoption  (#37759)\n\nfix",
    "sha": "b2d70e9c492a6c3070b1a467f6c9dc04116f8504",
    "files": [
        {
            "sha": "6f8d152f520c05af42fe6681fd0281d9bd1ab512",
            "filename": "docs/source/en/quantization/auto_round.md",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2d70e9c492a6c3070b1a467f6c9dc04116f8504/docs%2Fsource%2Fen%2Fquantization%2Fauto_round.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2d70e9c492a6c3070b1a467f6c9dc04116f8504/docs%2Fsource%2Fen%2Fquantization%2Fauto_round.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Fauto_round.md?ref=b2d70e9c492a6c3070b1a467f6c9dc04116f8504",
            "patch": "@@ -184,7 +184,7 @@ inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n print(tokenizer.decode(model.generate(**inputs, max_new_tokens=50, do_sample=False)[0]))\n ```\n \n-<hfoption>\n+</hfoption>\n \n <hfoption id=\"inference xpu\">\n \n@@ -203,7 +203,7 @@ inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n print(tokenizer.decode(model.generate(**inputs, max_new_tokens=50, do_sample=False)[0]))\n ```\n \n-<hfoption>\n+</hfoption>\n \n <hfoption id=\"inference cuda\">\n \n@@ -222,7 +222,7 @@ inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n print(tokenizer.decode(model.generate(**inputs, max_new_tokens=50, do_sample=False)[0]))\n ```\n \n-<hfoption>\n+</hfoption>\n \n <hfoption id=\"inference backend\">\n \n@@ -245,7 +245,7 @@ inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n print(tokenizer.decode(model.generate(**inputs, max_new_tokens=50, do_sample=False)[0]))\n ```\n \n-<hfoption>\n+</hfoption>\n \n \n <hfoption id=\"format convert\">\n@@ -266,9 +266,9 @@ inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n print(tokenizer.decode(model.generate(**inputs, max_new_tokens=50, do_sample=False)[0]))\n ```\n \n-<hfoption>\n+</hfoption>\n \n-<hfoptions>\n+</hfoptions>\n \n ## Issues\n "
        }
    ],
    "stats": {
        "total": 12,
        "additions": 6,
        "deletions": 6
    }
}