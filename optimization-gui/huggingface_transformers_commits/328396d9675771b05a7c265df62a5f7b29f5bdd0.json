{
    "author": "liangel-02",
    "message": "[torchao] safetensors (#42529)\n\n* torchao safetensors\n\n* working\n\n* update torchao safetensors\n\n* after rebase\n\n* remove regex\n\n* make safetensors check more robust\n\n---------\n\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>",
    "sha": "328396d9675771b05a7c265df62a5f7b29f5bdd0",
    "files": [
        {
            "sha": "c87c35724b61cacfde9b0f71fb18775a54a7d2d6",
            "filename": "src/transformers/integrations/torchao.py",
            "status": "modified",
            "additions": 32,
            "deletions": 38,
            "changes": 70,
            "blob_url": "https://github.com/huggingface/transformers/blob/328396d9675771b05a7c265df62a5f7b29f5bdd0/src%2Ftransformers%2Fintegrations%2Ftorchao.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/328396d9675771b05a7c265df62a5f7b29f5bdd0/src%2Ftransformers%2Fintegrations%2Ftorchao.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ftorchao.py?ref=328396d9675771b05a7c265df62a5f7b29f5bdd0",
            "patch": "@@ -32,7 +32,7 @@\n \n if is_torchao_available():\n     TORCHAO_VERSION = version.parse(importlib.metadata.version(\"torchao\"))\n-    if version.parse(importlib.metadata.version(\"torchao\")) >= version.parse(\"0.14.0\"):\n+    if version.parse(importlib.metadata.version(\"torchao\")) >= version.parse(\"0.15.0\"):\n         from torchao.prototype.safetensors.safetensors_support import (\n             unflatten_tensor_state_dict,\n         )\n@@ -210,61 +210,55 @@ def __init__(self, hf_quantizer):\n     def convert(\n         self,\n         input_dict: dict[str, torch.Tensor],\n+        source_patterns: list[str] | None = None,\n         model: Optional[torch.nn.Module] = None,\n         full_layer_name: str | None = None,\n         missing_keys=None,\n         **kwargs,\n     ) -> dict[str, torch.Tensor]:\n-        if isinstance(self.hf_quantizer.quantization_config.quant_type, str):\n-            is_int_4 = \"int4\" in self.hf_quantizer.quantization_config.quant_type\n-        else:\n-            config_name = self.hf_quantizer.quantization_config.quant_type.__class__.__name__\n-            is_int_4 = fuzzy_match_size(config_name) == \"4\"\n-\n-        # Simple case if we gather layermsnorm weights, we can just return the value since they are not quantized\n-        if \"weight:_data\" in input_dict.keys():\n-            value = (\n-                input_dict[\"weight:_data\"][0]\n-                if isinstance(input_dict[\"weight:_data\"], list)\n-                else input_dict[\"weight:_data\"]\n-            )\n-            return {full_layer_name: value}\n-\n-        is_unsafe_serialization = \":\" not in list(input_dict.keys())[0]\n+        \"\"\"\n+        Consolidates tensor subclass components before reconstructing the object\n+\n+        For example:\n+            input_dict: {\n+                \"_weight_qdata\": torch.Tensor,\n+                \"_weight_scale\": torch.Tensor,\n+            }\n+            full_layer_name: \"model.layers.0.self_attn.k_proj.weight\"\n+\n+            Given this, we reconstruct a Float8Tensor instance using the qdata and scale\n+            and return it as a dictionary with the full_layer_name as the key and the recovered\n+            Float8Tensor instance as the value.\n+        \"\"\"\n+        is_unsafe_serialization = list(input_dict.keys())[0] not in source_patterns\n \n         param_data = {}\n+        layer_name = \".\".join(full_layer_name.split(\".\")[:-1])\n         if is_unsafe_serialization:\n             if isinstance(input_dict[\"weight\"], list):\n                 weight = input_dict[\"weight\"][0]\n             else:\n                 weight = input_dict[\"weight\"]\n         else:\n-            if isinstance(input_dict[\"weight:qdata\"], list):\n-                param_data[f\"{full_layer_name}:qdata\"] = input_dict[\"weight:qdata\"][0]\n-            else:\n-                param_data[f\"{full_layer_name}:qdata\"] = input_dict[\"weight:qdata\"]\n-\n-            if isinstance(input_dict[\"weight:scale\"], list):\n-                param_data[f\"{full_layer_name}:scale\"] = input_dict[\"weight:scale\"][0]\n-            else:\n-                param_data[f\"{full_layer_name}:scale\"] = input_dict[\"weight:scale\"]\n-\n-            if is_int_4:\n-                if isinstance(input_dict[\"weight:zero_point\"], list):\n-                    param_data[f\"{full_layer_name}:zero_point\"] = input_dict[\"weight:zero_point\"][0]\n-                else:\n-                    param_data[f\"{full_layer_name}:zero_point\"] = input_dict[\"weight:zero_point\"]\n+            for suffix in input_dict.keys():\n+                if len(input_dict[suffix]) != 1:\n+                    raise ValueError(\n+                        f\"Expected a single tensor for {suffix} but got {len(input_dict[suffix])} tensors instead\"\n+                    )\n+                param_data[f\"{layer_name}.{suffix}\"] = input_dict[suffix][0]\n \n-        # If it's a bias, no need to do anything special (except removing the \":_data\" part of the key, but was\n-        # already done) - if it's unsafe-serialized (i.e. not safetensors), not need for anything either\n+        # If it's unsafe-serialized (i.e. not safetensors), no need for anything\n         if is_unsafe_serialization:\n             return {full_layer_name: weight}\n         # Sanity check for the new serialization format\n-        elif not (TORCHAO_VERSION >= version.parse(\"0.14.0\") and is_metadata_torchao(self.hf_quantizer.metadata)):\n-            # print(\"metadata\", self.hf_quantizer.metadata)\n-            raise ValueError(\"To use `safetensors` serialization, you should have `torchao>=0.14.0` installed\")\n+        elif not (TORCHAO_VERSION >= version.parse(\"0.15.0\") and is_metadata_torchao(self.hf_quantizer.metadata)):\n+            raise ValueError(\"To use `safetensors` serialization, you should have `torchao>=0.15.0` installed\")\n \n-        new_param = unflatten_tensor_state_dict(param_data, self.hf_quantizer.metadata)[full_layer_name]\n+        unflattened_state_dict, leftover_state_dict = unflatten_tensor_state_dict(\n+            param_data, self.hf_quantizer.metadata\n+        )\n+        assert not leftover_state_dict  # there should be no unprocessed tensors\n+        new_param = unflattened_state_dict[full_layer_name]\n \n         module, _ = get_module_from_name(model, full_layer_name)\n         # Add repr to the module"
        },
        {
            "sha": "b0d2b4727522a59d992995f19ac63edf60361afc",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/328396d9675771b05a7c265df62a5f7b29f5bdd0/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/328396d9675771b05a7c265df62a5f7b29f5bdd0/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=328396d9675771b05a7c265df62a5f7b29f5bdd0",
            "patch": "@@ -2104,7 +2104,6 @@ def set_decoder(self, decoder):\n         possible_module_names = [\"language_model\", \"text_model\", \"decoder\"]\n         for name in possible_module_names:\n             if hasattr(self, name):\n-                print(name)\n                 setattr(self, name, decoder)\n                 return\n "
        },
        {
            "sha": "2f0ad512f0cba7fd55ea9ac5b41bd23557d3fd95",
            "filename": "src/transformers/quantizers/quantizer_torchao.py",
            "status": "modified",
            "additions": 19,
            "deletions": 28,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/328396d9675771b05a7c265df62a5f7b29f5bdd0/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/328396d9675771b05a7c265df62a5f7b29f5bdd0/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py?ref=328396d9675771b05a7c265df62a5f7b29f5bdd0",
            "patch": "@@ -40,9 +40,7 @@\n     import torch.nn as nn\n \n if is_torchao_available():\n-    import torchao\n-\n-    if version.parse(importlib.metadata.version(\"torchao\")) >= version.parse(\"0.14.0\"):\n+    if version.parse(importlib.metadata.version(\"torchao\")) >= version.parse(\"0.15.0\"):\n         from torchao.prototype.safetensors.safetensors_support import (\n             flatten_tensor_state_dict,\n             unflatten_tensor_state_dict,\n@@ -88,11 +86,6 @@ def _linear_extra_repr(self):\n \n \n if is_torchao_available():\n-    SUPPORTED_SAFE_SERIALIZATION_CONFIGS = [\n-        torchao.quantization.Float8WeightOnlyConfig,\n-        torchao.quantization.Float8DynamicActivationFloat8WeightConfig,\n-    ]\n-\n     TORCHAO_VERSION = version.parse(importlib.metadata.version(\"torchao\"))\n \n \n@@ -171,12 +164,12 @@ def get_state_dict_and_metadata(self, model, safe_serialization: bool | None = F\n         If the model is safe serializable, we flatten the state dict of tensor subclasses so that it is compatible with\n         the safetensors format.\n         \"\"\"\n-        if type(self.quantization_config.quant_type) in SUPPORTED_SAFE_SERIALIZATION_CONFIGS and safe_serialization:\n-            if TORCHAO_VERSION >= version.parse(\"0.14.0\"):\n+        if safe_serialization:\n+            if TORCHAO_VERSION >= version.parse(\"0.15.0\"):\n                 return flatten_tensor_state_dict(model.state_dict())\n             else:\n                 raise RuntimeError(\n-                    f\"In order to use safetensors with torchao, please use torchao version >= 0.14.0. Current version: {TORCHAO_VERSION}\"\n+                    f\"In order to use safetensors with torchao, please use torchao version >= 0.15.0. Current version: {TORCHAO_VERSION}\"\n                 )\n         else:\n             return None, {}\n@@ -249,8 +242,6 @@ def param_needs_quantization(self, model: \"PreTrainedModel\", param_name: str, **\n         # check if the param_name is not in self.modules_to_not_convert\n         if any(key + \".\" in param_name or key == param_name for key in self.modules_to_not_convert):\n             return False\n-        elif any(param_name.endswith(f\":{x}\") for x in self.full_ao_keys):\n-            return True\n \n         # we only quantize the weight of nn.Linear and nn.Embedding\n         module, tensor_name = get_module_from_name(model, param_name)\n@@ -306,8 +297,8 @@ def create_quantized_param(\n                 )\n                 return\n             # Sanity check for the new serialization format\n-            elif not (TORCHAO_VERSION >= version.parse(\"0.14.0\") and is_metadata_torchao(self.metadata)):\n-                raise ValueError(\"To use `safetensors` serialization, you should have `torchao>=0.14.0` installed\")\n+            elif not (TORCHAO_VERSION >= version.parse(\"0.15.0\") and is_metadata_torchao(self.metadata)):\n+                raise ValueError(\"To use `safetensors` serialization, you should have `torchao>=0.15.0` installed\")\n \n             # Save the states for later quantization when they are all gathered\n             if not hasattr(self, \"ao_params\"):\n@@ -452,13 +443,10 @@ def _process_model_after_weight_loading(self, model, **kwargs):\n \n     def is_serializable(self, safe_serialization=None) -> bool:\n         if safe_serialization:\n-            _is_torchao_serializable = type(\n-                self.quantization_config.quant_type\n-            ) in SUPPORTED_SAFE_SERIALIZATION_CONFIGS and TORCHAO_VERSION >= version.parse(\"0.14.0\")\n-            if not _is_torchao_serializable:\n+            _is_torchao_serializable = TORCHAO_VERSION >= version.parse(\"0.15.0\")\n+            if not TORCHAO_VERSION >= version.parse(\"0.15.0\"):\n                 logger.warning(\n-                    f\"torchao quantized model only supports safe serialization for {SUPPORTED_SAFE_SERIALIZATION_CONFIGS}, \\\n-                    and torchao version >= 0.14.0, please set `safe_serialization` to False for \\\n+                    f\"torchao quantized model only supports safe serialization for torchao version >= 0.15.0, please set `safe_serialization` to False for \\\n                     {type(self.quantization_config.quant_type)} and {TORCHAO_VERSION}.\"\n                 )\n             return _is_torchao_serializable\n@@ -548,15 +536,18 @@ def get_weight_conversions(self):\n         if self.pre_quantized:\n             return [\n                 WeightConverter(\n-                    source_patterns=[\"weight:qdata\", \"weight:scale\", \"weight:zero_point\"],\n-                    target_patterns=\"weight\",\n-                    operations=[TorchAoDeserialize(self)],\n-                ),\n-                WeightConverter(\n-                    source_patterns=[\"weight:_data\"],\n+                    # TODO: incr flexibility by generalizing the source patterns to match the format of \"_weight_\"\n+                    # note that the matching logic is greedy, so for ex, if _weight_scale is before _weight_scale_and_zero in this list, it will match _weight_scale always (this is incorrect)\n+                    # thus, the order of source_patterns is intentional\n+                    source_patterns=[\n+                        \"_weight_qdata\",\n+                        \"_weight_scale_and_zero\",\n+                        \"_weight_scale\",\n+                        \"_weight_zero_point\",\n+                        \"_weight_act_pre_scale\",\n+                    ],\n                     target_patterns=\"weight\",\n                     operations=[TorchAoDeserialize(self)],\n                 ),\n-                # used for unsafe serialization\n             ]\n         return []"
        },
        {
            "sha": "694cfd2715b46a8f11906fcd34a04646d7e59a2f",
            "filename": "tests/quantization/torchao_integration/test_torchao.py",
            "status": "modified",
            "additions": 12,
            "deletions": 1,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/328396d9675771b05a7c265df62a5f7b29f5bdd0/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/328396d9675771b05a7c265df62a5f7b29f5bdd0/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py?ref=328396d9675771b05a7c265df62a5f7b29f5bdd0",
            "patch": "@@ -725,6 +725,7 @@ def check_serialization_expected_output(self, device, expected_output, safe_seri\n         dtype = torch.bfloat16 if isinstance(self.quant_scheme, Int4WeightOnlyConfig) else \"auto\"\n         with tempfile.TemporaryDirectory() as tmpdirname:\n             self.quantized_model.save_pretrained(tmpdirname, safe_serialization=safe_serialization)\n+\n             loaded_quantized_model = AutoModelForCausalLM.from_pretrained(\n                 tmpdirname, dtype=dtype, device_map=device, torch_dtype=dtype, use_safetensors=safe_serialization\n             )\n@@ -738,7 +739,7 @@ def test_serialization_expected_output(self):\n \n \n @require_torchao\n-@require_torchao_version_greater_or_equal(\"0.14.0\")\n+@require_torchao_version_greater_or_equal(\"0.15.0\")\n class TorchAoSafeSerializationTest(TorchAoSerializationTest):\n     # called only once for all test in this class\n     @classmethod\n@@ -763,6 +764,16 @@ def tearDown(self):\n                 \"What are we having for dinner?\\n\\nJess: (smiling) I\",\n             ),\n             (torchao.quantization.Float8WeightOnlyConfig(), \"What are we having for dinner?\\n\\nJessica: (smiling)\"),\n+            (Int4WeightOnlyConfig(), \"What are we having for dinner?\"),\n+            (\n+                Int4WeightOnlyConfig(int4_packing_format=\"tile_packed_to_4d\"),\n+                \"What are we having for dinner?\\nRed, white, and green beans,\",\n+            ),\n+            (\n+                torchao.quantization.Int8DynamicActivationIntxWeightConfig(),\n+                \"What are we having for dinner?\\n\\nJessica: (smiling)\",\n+            ),\n+            (torchao.quantization.IntxWeightOnlyConfig(), \"What are we having for dinner?\\n\\nJessica: (smiling)\"),\n         ]\n         if is_torchao_available()\n         else []"
        }
    ],
    "stats": {
        "total": 131,
        "additions": 63,
        "deletions": 68
    }
}