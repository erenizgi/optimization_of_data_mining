{
    "author": "ydshieh",
    "message": "fix `mistral3` tests (#38989)\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "839893c86bf372ee35b2c8dd750d3cdc21a995f5",
    "files": [
        {
            "sha": "595044a6fd37975ce07e428f652f4527144a8385",
            "filename": "tests/models/mistral3/test_modeling_mistral3.py",
            "status": "modified",
            "additions": 9,
            "deletions": 14,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/839893c86bf372ee35b2c8dd750d3cdc21a995f5/tests%2Fmodels%2Fmistral3%2Ftest_modeling_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/839893c86bf372ee35b2c8dd750d3cdc21a995f5/tests%2Fmodels%2Fmistral3%2Ftest_modeling_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmistral3%2Ftest_modeling_mistral3.py?ref=839893c86bf372ee35b2c8dd750d3cdc21a995f5",
            "patch": "@@ -307,6 +307,7 @@ def tearDown(self):\n     @require_read_token\n     def test_mistral3_integration_generate_text_only(self):\n         processor = AutoProcessor.from_pretrained(self.model_checkpoint)\n+        processor.chat_template = processor.chat_template.replace('strftime_now(\"%Y-%m-%d\")', '\"2025-06-20\"')\n \n         messages = [\n             {\n@@ -329,7 +330,6 @@ def test_mistral3_integration_generate_text_only(self):\n         expected_outputs = Expectations(\n             {\n                 (\"xpu\", 3): \"Sure, here is a haiku for you:\\n\\nWhispers of the breeze,\\nCherry blossoms softly fall,\\nSpring's gentle embrace.\",\n-                (\"cuda\", 7): \"Sure, here is a haiku for you:\\n\\nWhispers of the breeze,\\nCherry blossoms softly fall,\\nSpring's gentle embrace.\",\n                 (\"cuda\", 8): \"Sure, here is a haiku for you:\\n\\nWhispers of the breeze,\\nCherry blossoms softly fall,\\nSpring's gentle embrace.\",\n             }\n         )  # fmt: skip\n@@ -339,6 +339,7 @@ def test_mistral3_integration_generate_text_only(self):\n     @require_read_token\n     def test_mistral3_integration_generate(self):\n         processor = AutoProcessor.from_pretrained(self.model_checkpoint)\n+        processor.chat_template = processor.chat_template.replace('strftime_now(\"%Y-%m-%d\")', '\"2025-06-20\"')\n         messages = [\n             {\n                 \"role\": \"user\",\n@@ -361,18 +362,17 @@ def test_mistral3_integration_generate(self):\n         expected_outputs = Expectations(\n             {\n                 (\"xpu\", 3): \"The image features two cats resting on a pink blanket. The cat on the left is a kitten\",\n-                (\"cuda\", 7): \"The image features two cats resting on a pink blanket. The cat on the left is a kitten\",\n-                (\"cuda\", 8): \"The image features two cats resting on a pink blanket. The cat on the left is a small kit\",\n+                (\"cuda\", 8): 'The image features two cats lying on a pink surface, which appears to be a couch or a bed',\n             }\n         )  # fmt: skip\n         expected_output = expected_outputs.get_expectation()\n-\n         self.assertEqual(decoded_output, expected_output)\n \n     @require_read_token\n     @require_deterministic_for_xpu\n     def test_mistral3_integration_batched_generate(self):\n         processor = AutoProcessor.from_pretrained(self.model_checkpoint)\n+        processor.chat_template = processor.chat_template.replace('strftime_now(\"%Y-%m-%d\")', '\"2025-06-20\"')\n         messages = [\n             [\n                 {\n@@ -408,8 +408,7 @@ def test_mistral3_integration_batched_generate(self):\n         expected_outputs = Expectations(\n             {\n                 (\"xpu\", 3): \"Calm lake's mirror gleams,\\nWhispering pines stand in silence,\\nPath to peace begins.\",\n-                (\"cuda\", 7): \"Calm waters reflect\\nWhispering pines stand in silence\\nPath to peace begins\",\n-                (\"cuda\", 8): \"Calm waters reflect\\nWhispering pines stand in silence\\nPath to peace begins\",\n+                (\"cuda\", 8): \"Wooden path to calm,\\nReflections whisper secrets,\\nNature's peace unfolds.\",\n             }\n         )  # fmt: skip\n         expected_output = expected_outputs.get_expectation()\n@@ -424,8 +423,7 @@ def test_mistral3_integration_batched_generate(self):\n         expected_outputs = Expectations(\n             {\n                 (\"xpu\", 3): \"The image depicts a vibrant urban scene in what appears to be Chinatown. The focal point is a traditional Chinese archway\",\n-                (\"cuda\", 7): 'The image depicts a vibrant street scene in Chinatown, likely in a major city. The focal point is a traditional Chinese',\n-                (\"cuda\", 8): 'The image depicts a vibrant street scene in what appears to be Chinatown in a major city. The focal point is a',\n+                (\"cuda\", 8): 'The image depicts a street scene in what appears to be a Chinatown district. The focal point is a traditional Chinese arch',\n             }\n         )  # fmt: skip\n         expected_output = expected_outputs.get_expectation()\n@@ -439,6 +437,7 @@ def test_mistral3_integration_batched_generate(self):\n     @require_deterministic_for_xpu\n     def test_mistral3_integration_batched_generate_multi_image(self):\n         processor = AutoProcessor.from_pretrained(self.model_checkpoint)\n+        processor.chat_template = processor.chat_template.replace('strftime_now(\"%Y-%m-%d\")', '\"2025-06-20\"')\n \n         # Prepare inputs\n         messages = [\n@@ -482,9 +481,7 @@ def test_mistral3_integration_batched_generate_multi_image(self):\n         decoded_output = processor.decode(gen_tokens[0], skip_special_tokens=True)\n         expected_outputs = Expectations(\n             {\n-                (\"xpu\", 3): \"Still lake reflects skies,\\nWooden path to nature's heart,\\nSilence speaks volumes.\",\n-                (\"cuda\", 7): \"Calm waters reflect\\nWhispering pines stand in silence\\nPath to peace begins\",\n-                (\"cuda\", 8): \"Calm waters reflect\\nWhispering pines stand in silence\\nPath to peace begins\",\n+                (\"cuda\", 8): 'Calm waters reflect\\nWooden path to distant shore\\nSilence in the scene',\n             }\n         )  # fmt: skip\n         expected_output = expected_outputs.get_expectation()\n@@ -499,12 +496,10 @@ def test_mistral3_integration_batched_generate_multi_image(self):\n         expected_outputs = Expectations(\n             {\n                 (\"xpu\", 3): \"Certainly! The images depict two iconic landmarks:\\n\\n1. The first image shows the Statue of Liberty in New York City.\",\n-                (\"cuda\", 7): \"Certainly! The images depict the following landmarks:\\n\\n1. The first image shows the Statue of Liberty and the New York City\",\n-                (\"cuda\", 8): \"Certainly! The images depict the following landmarks:\\n\\n1. The first image shows the Statue of Liberty and the New York City\",\n+                (\"cuda\", 8): 'Certainly! The images depict two famous landmarks in the United States:\\n\\n1. The first image shows the Statue of Liberty,',\n             }\n         )  # fmt: skip\n         expected_output = expected_outputs.get_expectation()\n-\n         self.assertEqual(\n             decoded_output,\n             expected_output,"
        }
    ],
    "stats": {
        "total": 23,
        "additions": 9,
        "deletions": 14
    }
}