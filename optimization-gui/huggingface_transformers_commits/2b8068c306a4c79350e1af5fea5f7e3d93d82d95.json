{
    "author": "Aravind-11",
    "message": "T5 migration to new masking interface (#41804)\n\n* Refactor: migrate T5 attention masking to masking_utils interface\n\n* Refactor: migrate T5 attention masking to masking_utils interface\n\n* create_bidirectional_mask function with appropriate paramaters\n\n* create_bidirectional_mask function with appropriate paramaters\n\n* fixup executorch + import\n\n* revert causal masks\n\n* rm executorch stuff\n\n* add causal mask with non vmap\n\n* copies\n\n* remove unnecessary import\n\n---------\n\nCo-authored-by: Vasqu <antonprogamer@gmail.com>\nCo-authored-by: Anton Vlasjuk <73884904+vasqu@users.noreply.github.com>",
    "sha": "2b8068c306a4c79350e1af5fea5f7e3d93d82d95",
    "files": [
        {
            "sha": "fc400924c7a89ac138eb8b1a1deba18a8a0bad64",
            "filename": "src/transformers/models/mt5/modeling_mt5.py",
            "status": "modified",
            "additions": 21,
            "deletions": 162,
            "changes": 183,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b8068c306a4c79350e1af5fea5f7e3d93d82d95/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b8068c306a4c79350e1af5fea5f7e3d93d82d95/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py?ref=2b8068c306a4c79350e1af5fea5f7e3d93d82d95",
            "patch": "@@ -25,7 +25,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n-from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...masking_utils import create_bidirectional_mask, create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutput,\n@@ -41,18 +41,11 @@\n     DUMMY_INPUTS,\n     DUMMY_MASK,\n     auto_docstring,\n-    is_torch_flex_attn_available,\n-    is_torchdynamo_compiling,\n     logging,\n )\n from .configuration_mt5 import MT5Config\n \n \n-if is_torch_flex_attn_available():\n-    from torch.nn.attention.flex_attention import BlockMask\n-\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -735,40 +728,31 @@ def forward(\n                 past_key_values_length, past_key_values_length + seq_length, device=inputs_embeds.device\n             )\n \n-        if attention_mask is None and not is_torchdynamo_compiling():\n-            # required mask seq length can be calculated via length of past cache\n-            mask_seq_length = past_key_values_length + seq_length\n-            attention_mask = torch.ones(batch_size, mask_seq_length, device=inputs_embeds.device)\n-\n         if self.config.is_decoder:\n-            causal_mask = self._update_causal_mask(\n-                attention_mask,\n-                inputs_embeds,\n-                cache_position,\n-                past_key_values.self_attention_cache\n+            attention_mask = create_causal_mask(\n+                config=self.config,\n+                input_embeds=inputs_embeds,\n+                attention_mask=attention_mask,\n+                cache_position=cache_position,\n+                past_key_values=past_key_values.self_attention_cache\n                 if isinstance(past_key_values, EncoderDecoderCache)\n                 else past_key_values,\n-                output_attentions,\n             )\n-        elif attention_mask is not None:\n-            causal_mask = attention_mask[:, None, None, :]\n-            causal_mask = causal_mask.to(dtype=inputs_embeds.dtype)\n-            causal_mask = (1.0 - causal_mask) * torch.finfo(inputs_embeds.dtype).min\n         else:\n-            causal_mask = None\n+            attention_mask = create_bidirectional_mask(\n+                config=self.config,\n+                input_embeds=inputs_embeds,\n+                attention_mask=attention_mask,\n+            )\n \n-        # If a 2D or 3D attention mask is provided for the cross-attention\n-        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n+        encoder_extended_attention_mask = None\n         if self.is_decoder and encoder_hidden_states is not None:\n-            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n-            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n-            if encoder_attention_mask is None:\n-                encoder_attention_mask = torch.ones(\n-                    encoder_hidden_shape, device=inputs_embeds.device, dtype=torch.long\n-                )\n-            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n-        else:\n-            encoder_extended_attention_mask = None\n+            encoder_extended_attention_mask = create_bidirectional_mask(\n+                config=self.config,\n+                input_embeds=inputs_embeds,\n+                attention_mask=encoder_attention_mask,\n+                encoder_hidden_states=encoder_hidden_states,\n+            )\n \n         all_hidden_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n@@ -778,13 +762,13 @@ def forward(\n \n         hidden_states = self.dropout(inputs_embeds)\n \n-        for i, layer_module in enumerate(self.block):\n+        for layer_module in self.block:\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n             layer_outputs = layer_module(\n                 hidden_states,\n-                causal_mask,\n+                attention_mask,\n                 position_bias,\n                 encoder_hidden_states,\n                 encoder_extended_attention_mask,\n@@ -837,131 +821,6 @@ def forward(\n             cross_attentions=all_cross_attentions,\n         )\n \n-    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._update_causal_mask\n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n-        input_tensor: torch.Tensor,\n-        cache_position: torch.Tensor,\n-        past_key_values: Cache,\n-        output_attentions: bool = False,\n-    ):\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and (attention_mask == 0.0).any():\n-                return attention_mask\n-            return None\n-        if self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            return attention_mask\n-\n-        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n-        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n-        # to infer the attention mask.\n-        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n-\n-        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n-            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n-                attention_mask,\n-                inputs_embeds=input_tensor,\n-                past_key_values_length=past_seen_tokens,\n-                is_training=self.training,\n-            ):\n-                return None\n-\n-        dtype = input_tensor.dtype\n-        sequence_length = input_tensor.shape[1]\n-        if using_compilable_cache:\n-            target_length = past_key_values.get_max_cache_shape()\n-        else:\n-            target_length = (\n-                attention_mask.shape[-1]\n-                if isinstance(attention_mask, torch.Tensor)\n-                else past_seen_tokens + sequence_length + 1\n-            )\n-\n-        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-            attention_mask,\n-            sequence_length=sequence_length,\n-            target_length=target_length,\n-            dtype=dtype,\n-            cache_position=cache_position,\n-            batch_size=input_tensor.shape[0],\n-        )\n-\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-            and not output_attentions\n-        ):\n-            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n-            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n-            # Details: https://github.com/pytorch/pytorch/issues/110213\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n-\n-        return causal_mask\n-\n-    @staticmethod\n-    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask\n-\n \n @auto_docstring\n class MT5Model(MT5PreTrainedModel):"
        },
        {
            "sha": "0394e0a772ec192c82b2e926817642f598ccab6c",
            "filename": "src/transformers/models/t5/modeling_t5.py",
            "status": "modified",
            "additions": 21,
            "deletions": 163,
            "changes": 184,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b8068c306a4c79350e1af5fea5f7e3d93d82d95/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b8068c306a4c79350e1af5fea5f7e3d93d82d95/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py?ref=2b8068c306a4c79350e1af5fea5f7e3d93d82d95",
            "patch": "@@ -25,7 +25,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n-from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...masking_utils import create_bidirectional_mask, create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutput,\n@@ -41,19 +41,11 @@\n     DUMMY_INPUTS,\n     DUMMY_MASK,\n     auto_docstring,\n-    is_torch_flex_attn_available,\n-    is_torchdynamo_compiling,\n     logging,\n )\n from .configuration_t5 import T5Config\n \n \n-if is_torch_flex_attn_available():\n-    from torch.nn.attention.flex_attention import BlockMask\n-\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -738,40 +730,31 @@ def forward(\n                 past_key_values_length, past_key_values_length + seq_length, device=inputs_embeds.device\n             )\n \n-        if attention_mask is None and not is_torchdynamo_compiling():\n-            # required mask seq length can be calculated via length of past cache\n-            mask_seq_length = past_key_values_length + seq_length\n-            attention_mask = torch.ones(batch_size, mask_seq_length, device=inputs_embeds.device)\n-\n         if self.config.is_decoder:\n-            causal_mask = self._update_causal_mask(\n-                attention_mask,\n-                inputs_embeds,\n-                cache_position,\n-                past_key_values.self_attention_cache\n+            attention_mask = create_causal_mask(\n+                config=self.config,\n+                input_embeds=inputs_embeds,\n+                attention_mask=attention_mask,\n+                cache_position=cache_position,\n+                past_key_values=past_key_values.self_attention_cache\n                 if isinstance(past_key_values, EncoderDecoderCache)\n                 else past_key_values,\n-                output_attentions,\n             )\n-        elif attention_mask is not None:\n-            causal_mask = attention_mask[:, None, None, :]\n-            causal_mask = causal_mask.to(dtype=inputs_embeds.dtype)\n-            causal_mask = (1.0 - causal_mask) * torch.finfo(inputs_embeds.dtype).min\n         else:\n-            causal_mask = None\n+            attention_mask = create_bidirectional_mask(\n+                config=self.config,\n+                input_embeds=inputs_embeds,\n+                attention_mask=attention_mask,\n+            )\n \n-        # If a 2D or 3D attention mask is provided for the cross-attention\n-        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n+        encoder_extended_attention_mask = None\n         if self.is_decoder and encoder_hidden_states is not None:\n-            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n-            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n-            if encoder_attention_mask is None:\n-                encoder_attention_mask = torch.ones(\n-                    encoder_hidden_shape, device=inputs_embeds.device, dtype=torch.long\n-                )\n-            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n-        else:\n-            encoder_extended_attention_mask = None\n+            encoder_extended_attention_mask = create_bidirectional_mask(\n+                config=self.config,\n+                input_embeds=inputs_embeds,\n+                attention_mask=encoder_attention_mask,\n+                encoder_hidden_states=encoder_hidden_states,\n+            )\n \n         all_hidden_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n@@ -781,13 +764,13 @@ def forward(\n \n         hidden_states = self.dropout(inputs_embeds)\n \n-        for i, layer_module in enumerate(self.block):\n+        for layer_module in self.block:\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n             layer_outputs = layer_module(\n                 hidden_states,\n-                causal_mask,\n+                attention_mask,\n                 position_bias,\n                 encoder_hidden_states,\n                 encoder_extended_attention_mask,\n@@ -840,131 +823,6 @@ def forward(\n             cross_attentions=all_cross_attentions,\n         )\n \n-    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._update_causal_mask\n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n-        input_tensor: torch.Tensor,\n-        cache_position: torch.Tensor,\n-        past_key_values: Cache,\n-        output_attentions: bool = False,\n-    ):\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and (attention_mask == 0.0).any():\n-                return attention_mask\n-            return None\n-        if self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            return attention_mask\n-\n-        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n-        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n-        # to infer the attention mask.\n-        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n-\n-        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n-            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n-                attention_mask,\n-                inputs_embeds=input_tensor,\n-                past_key_values_length=past_seen_tokens,\n-                is_training=self.training,\n-            ):\n-                return None\n-\n-        dtype = input_tensor.dtype\n-        sequence_length = input_tensor.shape[1]\n-        if using_compilable_cache:\n-            target_length = past_key_values.get_max_cache_shape()\n-        else:\n-            target_length = (\n-                attention_mask.shape[-1]\n-                if isinstance(attention_mask, torch.Tensor)\n-                else past_seen_tokens + sequence_length + 1\n-            )\n-\n-        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-            attention_mask,\n-            sequence_length=sequence_length,\n-            target_length=target_length,\n-            dtype=dtype,\n-            cache_position=cache_position,\n-            batch_size=input_tensor.shape[0],\n-        )\n-\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-            and not output_attentions\n-        ):\n-            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n-            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n-            # Details: https://github.com/pytorch/pytorch/issues/110213\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n-\n-        return causal_mask\n-\n-    @staticmethod\n-    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask\n-\n \n @auto_docstring\n class T5Model(T5PreTrainedModel):"
        }
    ],
    "stats": {
        "total": 367,
        "additions": 42,
        "deletions": 325
    }
}