{
    "author": "MekkCyber",
    "message": "Skip Falcon 7B GGML Test  (#35783)\n\nskip test",
    "sha": "b80e334e719de76ca47b964b06b62abf7144cddc",
    "files": [
        {
            "sha": "ad5cdb17fe0b17f60d284b0b22fde5a40f76d7b0",
            "filename": "tests/quantization/ggml/test_ggml.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/b80e334e719de76ca47b964b06b62abf7144cddc/tests%2Fquantization%2Fggml%2Ftest_ggml.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b80e334e719de76ca47b964b06b62abf7144cddc/tests%2Fquantization%2Fggml%2Ftest_ggml.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fggml%2Ftest_ggml.py?ref=b80e334e719de76ca47b964b06b62abf7144cddc",
            "patch": "@@ -636,6 +636,7 @@ def test_falcon7b_q2_k(self):\n         EXPECTED_TEXT = 'Hello,\\nI am trying to use the \"get_post_meta\"'\n         self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)\n \n+    @unittest.skip(\"The test causes a torch.OutOfMemoryError on the CI but it passes with enough memory\")\n     def test_falcon7b_weights_conversion_fp16(self):\n         quantized_model = AutoModelForCausalLM.from_pretrained(\n             self.falcon7b_model_id_fp16,"
        }
    ],
    "stats": {
        "total": 1,
        "additions": 1,
        "deletions": 0
    }
}