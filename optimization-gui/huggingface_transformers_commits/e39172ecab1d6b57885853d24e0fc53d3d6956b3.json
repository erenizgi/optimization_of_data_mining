{
    "author": "ydshieh",
    "message": "Fix `llava_next` tests (#38813)\n\n* fix\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "e39172ecab1d6b57885853d24e0fc53d3d6956b3",
    "files": [
        {
            "sha": "cb573913e4a4ed7c77c1c4aca8b4cd9ccd66d8b6",
            "filename": "tests/models/llava_next/test_modeling_llava_next.py",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/e39172ecab1d6b57885853d24e0fc53d3d6956b3/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e39172ecab1d6b57885853d24e0fc53d3d6956b3/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py?ref=e39172ecab1d6b57885853d24e0fc53d3d6956b3",
            "patch": "@@ -392,7 +392,7 @@ def test_small_model_integration_test(self):\n             load_in_4bit=True,\n         )\n \n-        inputs = self.processor(images=self.image, text=self.prompt, return_tensors=\"pt\")\n+        inputs = self.processor(images=self.image, text=self.prompt, return_tensors=\"pt\").to(torch_device)\n \n         # verify inputs against original implementation\n         filepath = hf_hub_download(\n@@ -415,11 +415,13 @@ def test_small_model_integration_test(self):\n         )\n         check_torch_load_is_safe()\n         original_pixel_values = torch.load(filepath, map_location=\"cpu\", weights_only=True)\n-        assert torch.allclose(original_pixel_values, inputs.pixel_values.half())\n+        assert torch.allclose(\n+            original_pixel_values, inputs.pixel_values.to(device=\"cpu\", dtype=original_pixel_values.dtype)\n+        )\n \n         # verify generation\n         output = model.generate(**inputs, max_new_tokens=100)\n-        EXPECTED_DECODED_TEXT = '[INST]  \\nWhat is shown in this image? [/INST] The image appears to be a radar chart, which is a type of multi-dimensional plot that displays values for multiple quantitative variables represented on axes starting from the same point. This particular radar chart is showing the performance of various models or systems across different metrics or datasets.\\n\\nThe chart is divided into several sections, each representing a different model or dataset. The axes represent different metrics or datasets, such as \"MMM-Vet,\" \"MMM-Bench,\" \"L'  # fmt: skip\n+        EXPECTED_DECODED_TEXT = '[INST]  \\nWhat is shown in this image? [/INST] The image appears to be a radar chart, which is a type of multi-dimensional plot that displays values for multiple quantitative variables represented on axes starting from the same point. This particular radar chart is showing the performance of various models or systems across different metrics or datasets.\\n\\nThe chart is divided into several sections, each representing a different model or dataset. The axes represent different metrics or datasets, such as \"MMM-Vet,\" \"MMM-Bench,\" \"L'\n \n         self.assertEqual(\n             self.processor.decode(output[0], skip_special_tokens=True),\n@@ -511,7 +513,7 @@ def test_small_model_integration_test_batch_different_resolutions(self):\n \n         # verify generation\n         output = model.generate(**inputs, max_new_tokens=50)\n-        EXPECTED_DECODED_TEXT = '[INST]  \\nWhat is shown in this image? [/INST] The image shows two deer, likely fawns, in a grassy area with trees in the background. The setting appears to be a forest or woodland, and the time of day seems to be either dawn or dusk, given the soft'  # fmt: skip\n+        EXPECTED_DECODED_TEXT = \"[INST]  \\nWhat is shown in this image? [/INST] The image shows two deer, likely fawns, in a grassy area with trees in the background. The setting appears to be a forest or woodland, and the photo is taken during what seems to be either dawn or dusk, given\"\n         self.assertEqual(\n             self.processor.decode(output[0], skip_special_tokens=True),\n             EXPECTED_DECODED_TEXT,"
        }
    ],
    "stats": {
        "total": 10,
        "additions": 6,
        "deletions": 4
    }
}