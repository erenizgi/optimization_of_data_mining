{
    "author": "winglian",
    "message": "fix flex attn when optional args aren't passed (#37327)",
    "sha": "54538ebee3005951e62c16d2e9235cb5dfa19f83",
    "files": [
        {
            "sha": "d5b30d555dab3ad98495ab885bd439acd22cb87a",
            "filename": "src/transformers/integrations/flex_attention.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/54538ebee3005951e62c16d2e9235cb5dfa19f83/src%2Ftransformers%2Fintegrations%2Fflex_attention.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54538ebee3005951e62c16d2e9235cb5dfa19f83/src%2Ftransformers%2Fintegrations%2Fflex_attention.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fflex_attention.py?ref=54538ebee3005951e62c16d2e9235cb5dfa19f83",
            "patch": "@@ -100,6 +100,11 @@ def make_flex_block_causal_mask(\n     Returns:\n         BlockMask\n     \"\"\"\n+    batch_size, total_seq_len = attention_mask_2d.shape\n+    if not key_length:\n+        key_length = total_seq_len\n+    if not query_length:\n+        query_length = total_seq_len\n     attention_mask_2d = torch.nn.functional.pad(attention_mask_2d, value=0, pad=(0, key_length))\n     device = attention_mask_2d.device\n     document_ids = attention_mask_2d.clone()\n@@ -139,7 +144,7 @@ def mask_mod(batch_idx, head_idx, q_idx, kv_idx):\n         mask_mod = causal_mask_mod\n     return create_block_causal_mask_flex(\n         mask_mod=mask_mod,\n-        B=1,\n+        B=batch_size,\n         H=None,  # attention head\n         Q_LEN=query_length,\n         KV_LEN=key_length,"
        }
    ],
    "stats": {
        "total": 7,
        "additions": 6,
        "deletions": 1
    }
}