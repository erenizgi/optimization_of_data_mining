{
    "author": "Cyrilvallez",
    "message": "Fix mask slicing for models with HybridCache (#35681)\n\n* correctly slice\r\n\r\n* check mask\r\n\r\n* Update modular_gemma2.py\r\n\r\n* fix\r\n\r\n* add tests\r\n\r\n* fix typo\r\n\r\n* finally fix mask slicing\r\n\r\n* Finally correctly slice in all cases!!\r\n\r\n* add test for all attention functions\r\n\r\n* small fix in tests\r\n\r\n* trick around dynamo tracing issue\r\n\r\n* last update\r\n\r\n* more robust\r\n\r\n* kwargs propagation\r\n\r\n* make it explicit for checkpointing\r\n\r\n* apply modular",
    "sha": "3f860dba553d09a8eb96fded8d940c98a9a86854",
    "files": [
        {
            "sha": "6078f7d99a2907d1683d98f7c78eae1a37957fee",
            "filename": "src/transformers/models/cohere2/modeling_cohere2.py",
            "status": "modified",
            "additions": 38,
            "deletions": 5,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f860dba553d09a8eb96fded8d940c98a9a86854/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f860dba553d09a8eb96fded8d940c98a9a86854/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py?ref=3f860dba553d09a8eb96fded8d940c98a9a86854",
            "patch": "@@ -260,6 +260,11 @@ def forward(\n             }\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n+            # Here we need to slice as we use a static cache by default, but FA2 does not support it\n+            if attention_mask is not None and self.config._attn_implementation == \"flash_attention_2\":\n+                seq_len = attention_mask.shape[-1]\n+                key_states, value_states = key_states[:, :, :seq_len, :], value_states[:, :, :seq_len, :]\n+\n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n             if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n@@ -323,6 +328,7 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n+        last_cache_position: int = 0,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n@@ -343,21 +349,30 @@ def forward(\n                 (see `past_key_values`).\n             cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n                 Indices depicting the position of the input sequence tokens in the sequence\n+            last_cache_position (`int`): equivalent to `cache_position[-1]` but allow indexing without breaking dynamo tracing\n         \"\"\"\n \n         if self.is_sliding and attention_mask is not None:  # efficient SDPA and no padding\n-            # Flash-attn is a 2D tensor\n+            # In prefill, we may be larger than sliding window\n+            effective_seq_len = max(cache_position.shape[0], self.sliding_window)\n+            # For FA2, the mask is 2D and is of shape [bs, processed_tokens] (not [bs, max_cache_len]),\n+            # thus we must slice from the right (at most `effective_seq_len` elements)\n             if self.config._attn_implementation == \"flash_attention_2\":\n-                if past_key_value is not None:  # when decoding\n-                    attention_mask = attention_mask[:, -self.sliding_window :]\n+                attention_mask = attention_mask[:, -effective_seq_len:]\n+            # Otherwise, the mask is 4D of shape [bs, 1, query_len, max_cache_len] thus we must slice\n+            # from the left, with an offset if we are beyond the sliding window\n             else:\n                 min_dtype = torch.finfo(hidden_states.dtype).min\n                 sliding_window_mask = torch.tril(\n                     torch.ones_like(attention_mask, dtype=torch.bool), diagonal=-self.sliding_window\n                 )\n                 attention_mask = torch.where(sliding_window_mask, min_dtype, attention_mask)\n-                if attention_mask.shape[-1] <= 1:  # when decoding\n-                    attention_mask = attention_mask[:, :, :, -self.sliding_window :]\n+                # In case we are beyond the sliding window, we need to correctly offset the mask slicing\n+                # `last_cache_position` is equivalent to `cache_position[-1]` but without breaking dynamo\n+                offset = last_cache_position - effective_seq_len\n+                # Should only be used when beyond the sliding window (i.e. offset > 0)\n+                offset = max(0, offset)\n+                attention_mask = attention_mask[:, :, :, offset : offset + effective_seq_len]\n \n         residual = hidden_states\n \n@@ -557,6 +572,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        last_cache_position: Optional[int] = None,\n         **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[Tuple, BaseModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n@@ -595,9 +611,20 @@ def forward(\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n+        # This is needed to correctly slice the mask without data-dependent slicing later on if using dynamo tracing\n+        # (retrieving the same value from `cache_position` later on would crash dynamo)\n+        if last_cache_position is None:\n+            last_cache_position = 0\n+            if attention_mask is not None:\n+                # In case a 4d mask is passed directly without using `generate`, we have to rely on cache_position\n+                # It will break dynamo tracing but there are no way around it (and it should never happen in practice)\n+                last_cache_position = (\n+                    attention_mask.shape[-1] if attention_mask.dim() == 2 else cache_position[-1].item()\n+                )\n         causal_mask = self._update_causal_mask(\n             attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n         )\n+\n         hidden_states = inputs_embeds\n \n         # create position embeddings to be shared across the decoder layers\n@@ -621,6 +648,7 @@ def forward(\n                     output_attentions,\n                     use_cache,\n                     cache_position,\n+                    last_cache_position,\n                 )\n             else:\n                 layer_outputs = decoder_layer(\n@@ -631,6 +659,7 @@ def forward(\n                     output_attentions=output_attentions,\n                     use_cache=use_cache,\n                     cache_position=cache_position,\n+                    last_cache_position=last_cache_position,\n                     **flash_attn_kwargs,\n                 )\n \n@@ -917,6 +946,10 @@ def prepare_inputs_for_generation(\n             # The clone here is for the same reason as for `position_ids`.\n             model_inputs = {\"input_ids\": input_ids.clone(memory_format=torch.contiguous_format), \"inputs_embeds\": None}\n \n+        # This is needed to correctly slice the mask without data-dependent slicing later on if using dynamo tracing\n+        # (retrieving the same value from `cache_position` later on would crash dynamo)\n+        model_inputs[\"last_cache_position\"] = attention_mask.shape[-1] if attention_mask is not None else 0\n+\n         if (\n             isinstance(past_key_values, HybridCache)\n             and attention_mask.ndim == 2"
        },
        {
            "sha": "d8dc85c73059fc96d57af730d5a0f1b773cba5b9",
            "filename": "src/transformers/models/cohere2/modular_cohere2.py",
            "status": "modified",
            "additions": 38,
            "deletions": 5,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f860dba553d09a8eb96fded8d940c98a9a86854/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f860dba553d09a8eb96fded8d940c98a9a86854/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py?ref=3f860dba553d09a8eb96fded8d940c98a9a86854",
            "patch": "@@ -305,6 +305,11 @@ def forward(\n             }\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n+            # Here we need to slice as we use a static cache by default, but FA2 does not support it\n+            if attention_mask is not None and self.config._attn_implementation == \"flash_attention_2\":\n+                seq_len = attention_mask.shape[-1]\n+                key_states, value_states = key_states[:, :, :seq_len, :], value_states[:, :, :seq_len, :]\n+\n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n             if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n@@ -349,6 +354,7 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n+        last_cache_position: int = 0,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n@@ -369,21 +375,30 @@ def forward(\n                 (see `past_key_values`).\n             cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n                 Indices depicting the position of the input sequence tokens in the sequence\n+            last_cache_position (`int`): equivalent to `cache_position[-1]` but allow indexing without breaking dynamo tracing\n         \"\"\"\n \n         if self.is_sliding and attention_mask is not None:  # efficient SDPA and no padding\n-            # Flash-attn is a 2D tensor\n+            # In prefill, we may be larger than sliding window\n+            effective_seq_len = max(cache_position.shape[0], self.sliding_window)\n+            # For FA2, the mask is 2D and is of shape [bs, processed_tokens] (not [bs, max_cache_len]),\n+            # thus we must slice from the right (at most `effective_seq_len` elements)\n             if self.config._attn_implementation == \"flash_attention_2\":\n-                if past_key_value is not None:  # when decoding\n-                    attention_mask = attention_mask[:, -self.sliding_window :]\n+                attention_mask = attention_mask[:, -effective_seq_len:]\n+            # Otherwise, the mask is 4D of shape [bs, 1, query_len, max_cache_len] thus we must slice\n+            # from the left, with an offset if we are beyond the sliding window\n             else:\n                 min_dtype = torch.finfo(hidden_states.dtype).min\n                 sliding_window_mask = torch.tril(\n                     torch.ones_like(attention_mask, dtype=torch.bool), diagonal=-self.sliding_window\n                 )\n                 attention_mask = torch.where(sliding_window_mask, min_dtype, attention_mask)\n-                if attention_mask.shape[-1] <= 1:  # when decoding\n-                    attention_mask = attention_mask[:, :, :, -self.sliding_window :]\n+                # In case we are beyond the sliding window, we need to correctly offset the mask slicing\n+                # `last_cache_position` is equivalent to `cache_position[-1]` but without breaking dynamo\n+                offset = last_cache_position - effective_seq_len\n+                # Should only be used when beyond the sliding window (i.e. offset > 0)\n+                offset = max(0, offset)\n+                attention_mask = attention_mask[:, :, :, offset : offset + effective_seq_len]\n \n         residual = hidden_states\n \n@@ -443,6 +458,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        last_cache_position: Optional[int] = None,\n         **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[Tuple, BaseModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n@@ -481,9 +497,20 @@ def forward(\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n+        # This is needed to correctly slice the mask without data-dependent slicing later on if using dynamo tracing\n+        # (retrieving the same value from `cache_position` later on would crash dynamo)\n+        if last_cache_position is None:\n+            last_cache_position = 0\n+            if attention_mask is not None:\n+                # In case a 4d mask is passed directly without using `generate`, we have to rely on cache_position\n+                # It will break dynamo tracing but there are no way around it (and it should never happen in practice)\n+                last_cache_position = (\n+                    attention_mask.shape[-1] if attention_mask.dim() == 2 else cache_position[-1].item()\n+                )\n         causal_mask = self._update_causal_mask(\n             attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n         )\n+\n         hidden_states = inputs_embeds\n \n         # create position embeddings to be shared across the decoder layers\n@@ -507,6 +534,7 @@ def forward(\n                     output_attentions,\n                     use_cache,\n                     cache_position,\n+                    last_cache_position,\n                 )\n             else:\n                 layer_outputs = decoder_layer(\n@@ -517,6 +545,7 @@ def forward(\n                     output_attentions=output_attentions,\n                     use_cache=use_cache,\n                     cache_position=cache_position,\n+                    last_cache_position=last_cache_position,\n                     **flash_attn_kwargs,\n                 )\n \n@@ -586,6 +615,10 @@ def prepare_inputs_for_generation(\n             # The clone here is for the same reason as for `position_ids`.\n             model_inputs = {\"input_ids\": input_ids.clone(memory_format=torch.contiguous_format), \"inputs_embeds\": None}\n \n+        # This is needed to correctly slice the mask without data-dependent slicing later on if using dynamo tracing\n+        # (retrieving the same value from `cache_position` later on would crash dynamo)\n+        model_inputs[\"last_cache_position\"] = attention_mask.shape[-1] if attention_mask is not None else 0\n+\n         if (\n             isinstance(past_key_values, HybridCache)\n             and attention_mask.ndim == 2"
        },
        {
            "sha": "0f585e7bdb209ee3431b1d04eef824f81e3bc817",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 45,
            "deletions": 6,
            "changes": 51,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f860dba553d09a8eb96fded8d940c98a9a86854/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f860dba553d09a8eb96fded8d940c98a9a86854/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=3f860dba553d09a8eb96fded8d940c98a9a86854",
            "patch": "@@ -221,9 +221,19 @@ def forward(\n \n         if past_key_value is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n-            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            cache_kwargs = {\n+                \"sin\": sin,\n+                \"cos\": cos,\n+                \"cache_position\": cache_position,\n+                \"sliding_window\": self.sliding_window,\n+            }\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n+            # Here we need to slice as we use a static cache by default, but FA2 does not support it\n+            if attention_mask is not None and self.config._attn_implementation == \"flash_attention_2\":\n+                seq_len = attention_mask.shape[-1]\n+                key_states, value_states = key_states[:, :, :seq_len, :], value_states[:, :, :seq_len, :]\n+\n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n             if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n@@ -277,20 +287,30 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n+        last_cache_position: int = 0,\n+        **kwargs,\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         if self.is_sliding and attention_mask is not None:  # efficient SDPA and no padding\n-            # Flash-attn is a 2D tensor\n+            # In prefill, we may be larger than sliding window\n+            effective_seq_len = max(cache_position.shape[0], self.sliding_window)\n+            # For FA2, the mask is 2D and is of shape [bs, processed_tokens] (not [bs, max_cache_len]),\n+            # thus we must slice from the right (at most `effective_seq_len` elements)\n             if self.config._attn_implementation == \"flash_attention_2\":\n-                if past_key_value is not None:  # when decoding\n-                    attention_mask = attention_mask[:, -self.sliding_window :]\n+                attention_mask = attention_mask[:, -effective_seq_len:]\n+            # Otherwise, the mask is 4D of shape [bs, 1, query_len, max_cache_len] thus we must slice\n+            # from the left, with an offset if we are beyond the sliding window\n             else:\n                 min_dtype = torch.finfo(hidden_states.dtype).min\n                 sliding_window_mask = torch.tril(\n                     torch.ones_like(attention_mask, dtype=torch.bool), diagonal=-self.sliding_window\n                 )\n                 attention_mask = torch.where(sliding_window_mask, min_dtype, attention_mask)\n-                if attention_mask.shape[-1] <= 1:  # when decoding\n-                    attention_mask = attention_mask[:, :, :, -self.sliding_window :]\n+                # In case we are beyond the sliding window, we need to correctly offset the mask slicing\n+                # `last_cache_position` is equivalent to `cache_position[-1]` but without breaking dynamo\n+                offset = last_cache_position - effective_seq_len\n+                # Should only be used when beyond the sliding window (i.e. offset > 0)\n+                offset = max(0, offset)\n+                attention_mask = attention_mask[:, :, :, offset : offset + effective_seq_len]\n \n         residual = hidden_states\n \n@@ -306,6 +326,7 @@ def forward(\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n         hidden_states = self.post_attention_layernorm(hidden_states)\n         hidden_states = residual + hidden_states\n@@ -554,6 +575,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        last_cache_position: Optional[int] = None,\n         **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[Tuple, BaseModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n@@ -593,6 +615,16 @@ def forward(\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n+        # This is needed to correctly slice the mask without data-dependent slicing later on if using dynamo tracing\n+        # (retrieving the same value from `cache_position` later on would crash dynamo)\n+        if last_cache_position is None:\n+            last_cache_position = 0\n+            if attention_mask is not None:\n+                # In case a 4d mask is passed directly without using `generate`, we have to rely on cache_position\n+                # It will break dynamo tracing but there are no way around it (and it should never happen in practice)\n+                last_cache_position = (\n+                    attention_mask.shape[-1] if attention_mask.dim() == 2 else cache_position[-1].item()\n+                )\n         causal_mask = self._update_causal_mask(\n             attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n         )\n@@ -628,6 +660,7 @@ def forward(\n                     output_attentions,\n                     use_cache,\n                     cache_position,\n+                    last_cache_position,\n                 )\n             else:\n                 layer_outputs = decoder_layer(\n@@ -639,6 +672,7 @@ def forward(\n                     output_attentions=output_attentions,\n                     use_cache=use_cache,\n                     cache_position=cache_position,\n+                    last_cache_position=last_cache_position,\n                     **flash_attn_kwargs,\n                 )\n \n@@ -857,6 +891,7 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n             cache_position=cache_position,\n+            **loss_kwargs,\n         )\n \n         hidden_states = outputs[0]\n@@ -926,6 +961,10 @@ def prepare_inputs_for_generation(\n             # The clone here is for the same reason as for `position_ids`.\n             model_inputs = {\"input_ids\": input_ids.clone(memory_format=torch.contiguous_format), \"inputs_embeds\": None}\n \n+        # This is needed to correctly slice the mask without data-dependent slicing later on if using dynamo tracing\n+        # (retrieving the same value from `cache_position` later on would crash dynamo)\n+        model_inputs[\"last_cache_position\"] = attention_mask.shape[-1] if attention_mask is not None else 0\n+\n         if (\n             isinstance(past_key_values, HybridCache)\n             and attention_mask.ndim == 2"
        },
        {
            "sha": "cce83d204c93b6843c9a2c62911f3bb002a0812c",
            "filename": "src/transformers/models/gemma2/modular_gemma2.py",
            "status": "modified",
            "additions": 45,
            "deletions": 6,
            "changes": 51,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f860dba553d09a8eb96fded8d940c98a9a86854/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f860dba553d09a8eb96fded8d940c98a9a86854/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py?ref=3f860dba553d09a8eb96fded8d940c98a9a86854",
            "patch": "@@ -265,9 +265,19 @@ def forward(\n \n         if past_key_value is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n-            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            cache_kwargs = {\n+                \"sin\": sin,\n+                \"cos\": cos,\n+                \"cache_position\": cache_position,\n+                \"sliding_window\": self.sliding_window,\n+            }\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n+            # Here we need to slice as we use a static cache by default, but FA2 does not support it\n+            if attention_mask is not None and self.config._attn_implementation == \"flash_attention_2\":\n+                seq_len = attention_mask.shape[-1]\n+                key_states, value_states = key_states[:, :, :seq_len, :], value_states[:, :, :seq_len, :]\n+\n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n             if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n@@ -321,20 +331,30 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n+        last_cache_position: int = 0,\n+        **kwargs,\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         if self.is_sliding and attention_mask is not None:  # efficient SDPA and no padding\n-            # Flash-attn is a 2D tensor\n+            # In prefill, we may be larger than sliding window\n+            effective_seq_len = max(cache_position.shape[0], self.sliding_window)\n+            # For FA2, the mask is 2D and is of shape [bs, processed_tokens] (not [bs, max_cache_len]),\n+            # thus we must slice from the right (at most `effective_seq_len` elements)\n             if self.config._attn_implementation == \"flash_attention_2\":\n-                if past_key_value is not None:  # when decoding\n-                    attention_mask = attention_mask[:, -self.sliding_window :]\n+                attention_mask = attention_mask[:, -effective_seq_len:]\n+            # Otherwise, the mask is 4D of shape [bs, 1, query_len, max_cache_len] thus we must slice\n+            # from the left, with an offset if we are beyond the sliding window\n             else:\n                 min_dtype = torch.finfo(hidden_states.dtype).min\n                 sliding_window_mask = torch.tril(\n                     torch.ones_like(attention_mask, dtype=torch.bool), diagonal=-self.sliding_window\n                 )\n                 attention_mask = torch.where(sliding_window_mask, min_dtype, attention_mask)\n-                if attention_mask.shape[-1] <= 1:  # when decoding\n-                    attention_mask = attention_mask[:, :, :, -self.sliding_window :]\n+                # In case we are beyond the sliding window, we need to correctly offset the mask slicing\n+                # `last_cache_position` is equivalent to `cache_position[-1]` but without breaking dynamo\n+                offset = last_cache_position - effective_seq_len\n+                # Should only be used when beyond the sliding window (i.e. offset > 0)\n+                offset = max(0, offset)\n+                attention_mask = attention_mask[:, :, :, offset : offset + effective_seq_len]\n \n         residual = hidden_states\n \n@@ -350,6 +370,7 @@ def forward(\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n         hidden_states = self.post_attention_layernorm(hidden_states)\n         hidden_states = residual + hidden_states\n@@ -387,6 +408,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        last_cache_position: Optional[int] = None,\n         **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[Tuple, BaseModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n@@ -426,6 +448,16 @@ def forward(\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n+        # This is needed to correctly slice the mask without data-dependent slicing later on if using dynamo tracing\n+        # (retrieving the same value from `cache_position` later on would crash dynamo)\n+        if last_cache_position is None:\n+            last_cache_position = 0\n+            if attention_mask is not None:\n+                # In case a 4d mask is passed directly without using `generate`, we have to rely on cache_position\n+                # It will break dynamo tracing but there are no way around it (and it should never happen in practice)\n+                last_cache_position = (\n+                    attention_mask.shape[-1] if attention_mask.dim() == 2 else cache_position[-1].item()\n+                )\n         causal_mask = self._update_causal_mask(\n             attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n         )\n@@ -461,6 +493,7 @@ def forward(\n                     output_attentions,\n                     use_cache,\n                     cache_position,\n+                    last_cache_position,\n                 )\n             else:\n                 layer_outputs = decoder_layer(\n@@ -472,6 +505,7 @@ def forward(\n                     output_attentions=output_attentions,\n                     use_cache=use_cache,\n                     cache_position=cache_position,\n+                    last_cache_position=last_cache_position,\n                     **flash_attn_kwargs,\n                 )\n \n@@ -589,6 +623,7 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n             cache_position=cache_position,\n+            **loss_kwargs,\n         )\n \n         hidden_states = outputs[0]\n@@ -658,6 +693,10 @@ def prepare_inputs_for_generation(\n             # The clone here is for the same reason as for `position_ids`.\n             model_inputs = {\"input_ids\": input_ids.clone(memory_format=torch.contiguous_format), \"inputs_embeds\": None}\n \n+        # This is needed to correctly slice the mask without data-dependent slicing later on if using dynamo tracing\n+        # (retrieving the same value from `cache_position` later on would crash dynamo)\n+        model_inputs[\"last_cache_position\"] = attention_mask.shape[-1] if attention_mask is not None else 0\n+\n         if (\n             isinstance(past_key_values, HybridCache)\n             and attention_mask.ndim == 2"
        },
        {
            "sha": "436f1f965e90587b391bde4bd3fda2c34a242e12",
            "filename": "tests/models/cohere2/test_modeling_cohere2.py",
            "status": "modified",
            "additions": 33,
            "deletions": 0,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f860dba553d09a8eb96fded8d940c98a9a86854/tests%2Fmodels%2Fcohere2%2Ftest_modeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f860dba553d09a8eb96fded8d940c98a9a86854/tests%2Fmodels%2Fcohere2%2Ftest_modeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcohere2%2Ftest_modeling_cohere2.py?ref=3f860dba553d09a8eb96fded8d940c98a9a86854",
            "patch": "@@ -324,3 +324,36 @@ def test_export_static_cache(self):\n         )\n         ep_generated_text = tokenizer.batch_decode(ep_generated_ids, skip_special_tokens=True)\n         self.assertEqual(EXPECTED_TEXT_COMPLETION, ep_generated_text)\n+\n+    @parameterized.expand([(\"flash_attention_2\",), (\"sdpa\",), (\"flex_attention\",), (\"eager\",)])\n+    @require_read_token\n+    def test_generation_beyond_sliding_window(self, attn_implementation: str):\n+        \"\"\"Test that we can correctly generate beyond the sliding window. This is non trivial as\n+        we need to correctly slice the attention mask in all cases (because we use a HybridCache).\n+        Outputs for every attention functions should be coherent and identical.\n+        \"\"\"\n+        model_id = \"CohereForAI/c4ai-command-r7b-12-2024\"\n+        EXPECTED_COMPLETIONS = [\n+            \" the mountains, the lakes, the rivers, the waterfalls, the waterfalls, the waterfalls, the waterfalls\",\n+            \", green, yellow, orange, purple, pink, brown, black, white, grey, silver\",\n+        ]\n+\n+        input_text = [\n+            \"This is a nice place. \" * 800 + \"I really enjoy the scenery,\",  # This is larger than 4096 tokens\n+            \"A list of colors: red, blue\",  # This will almost all be padding tokens\n+        ]\n+        tokenizer = AutoTokenizer.from_pretrained(model_id, padding=\"left\")\n+        inputs = tokenizer(input_text, padding=True, return_tensors=\"pt\").to(torch_device)\n+\n+        model = AutoModelForCausalLM.from_pretrained(\n+            model_id, attn_implementation=attn_implementation, torch_dtype=torch.float16\n+        ).to(torch_device)\n+\n+        # Make sure prefill is larger than sliding window\n+        input_size = inputs.input_ids.shape[-1]\n+        self.assertTrue(input_size > model.config.sliding_window)\n+\n+        out = model.generate(**inputs, max_new_tokens=20)[:, input_size:]\n+        output_text = tokenizer.batch_decode(out)\n+\n+        self.assertEqual(output_text, EXPECTED_COMPLETIONS)"
        },
        {
            "sha": "1fb7bdfa89948749f0bc76f7e444f7d17911d044",
            "filename": "tests/models/gemma2/test_modeling_gemma2.py",
            "status": "modified",
            "additions": 33,
            "deletions": 0,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f860dba553d09a8eb96fded8d940c98a9a86854/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f860dba553d09a8eb96fded8d940c98a9a86854/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py?ref=3f860dba553d09a8eb96fded8d940c98a9a86854",
            "patch": "@@ -394,3 +394,36 @@ def test_model_9b_bf16_flex_attention(self):\n         output_text = tokenizer.batch_decode(output, skip_special_tokens=False)\n \n         self.assertEqual(output_text, EXPECTED_TEXTS)\n+\n+    @parameterized.expand([(\"flash_attention_2\",), (\"sdpa\",), (\"flex_attention\",), (\"eager\",)])\n+    @require_read_token\n+    def test_generation_beyond_sliding_window(self, attn_implementation: str):\n+        \"\"\"Test that we can correctly generate beyond the sliding window. This is non trivial as\n+        we need to correctly slice the attention mask in all cases (because we use a HybridCache).\n+        Outputs for every attention functions should be coherent and identical.\n+        \"\"\"\n+        model_id = \"google/gemma-2-2b\"\n+        EXPECTED_COMPLETIONS = [\n+            \" the people, the food, the culture, the history, the music, the art, the architecture\",\n+            \", green, yellow, orange, purple, pink, brown, black, white, gray, silver\",\n+        ]\n+\n+        input_text = [\n+            \"This is a nice place. \" * 800 + \"I really enjoy the scenery,\",  # This is larger than 4096 tokens\n+            \"A list of colors: red, blue\",  # This will almost all be padding tokens\n+        ]\n+        tokenizer = AutoTokenizer.from_pretrained(model_id, padding=\"left\")\n+        inputs = tokenizer(input_text, padding=True, return_tensors=\"pt\").to(torch_device)\n+\n+        model = AutoModelForCausalLM.from_pretrained(\n+            model_id, attn_implementation=attn_implementation, torch_dtype=torch.float16\n+        ).to(torch_device)\n+\n+        # Make sure prefill is larger than sliding window\n+        input_size = inputs.input_ids.shape[-1]\n+        self.assertTrue(input_size > model.config.sliding_window)\n+\n+        out = model.generate(**inputs, max_new_tokens=20)[:, input_size:]\n+        output_text = tokenizer.batch_decode(out)\n+\n+        self.assertEqual(output_text, EXPECTED_COMPLETIONS)"
        }
    ],
    "stats": {
        "total": 254,
        "additions": 232,
        "deletions": 22
    }
}