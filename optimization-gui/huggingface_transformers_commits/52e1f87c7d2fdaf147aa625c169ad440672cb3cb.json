{
    "author": "zucchini-nlp",
    "message": "[WIP] Emu3: add model (#33770)\n\n* model can convert to HF and be loaded back\r\n\r\n* nit\r\n\r\n* works in single batch generation but hallucinates\r\n\r\n* use the image tokens\r\n\r\n* add image generation\r\n\r\n* now it works\r\n\r\n* add tests\r\n\r\n* update\r\n\r\n* add modulare but it doesn't work for porting docstring :(\r\n\r\n* skip some tests\r\n\r\n* add slow tests\r\n\r\n* modular removed the import?\r\n\r\n* guess this works\r\n\r\n* update\r\n\r\n* update\r\n\r\n* fix copies\r\n\r\n* fix test\r\n\r\n* fix copies\r\n\r\n* update\r\n\r\n* docs\r\n\r\n* fix tests\r\n\r\n* last fix tests?\r\n\r\n* pls\r\n\r\n* repo consistency\r\n\r\n* more style\r\n\r\n* style\r\n\r\n* remove file\r\n\r\n* address comments\r\n\r\n* tiny bits\r\n\r\n* update after the new modular\r\n\r\n* fix tests\r\n\r\n* add one more cond in check attributes\r\n\r\n* decompose down/up/mid blocks\r\n\r\n* allow static cache generation in VLMs\r\n\r\n* nit\r\n\r\n* fix copies\r\n\r\n* Update docs/source/en/model_doc/emu3.md\r\n\r\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\r\n\r\n* Update docs/source/en/model_doc/emu3.md\r\n\r\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\r\n\r\n* Update docs/source/en/model_doc/emu3.md\r\n\r\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\r\n\r\n* Update docs/source/en/model_doc/emu3.md\r\n\r\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\r\n\r\n* Update docs/source/en/model_doc/emu3.md\r\n\r\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\r\n\r\n* Update docs/source/en/model_doc/emu3.md\r\n\r\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\r\n\r\n* Update docs/source/en/model_doc/emu3.md\r\n\r\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\r\n\r\n* Update docs/source/en/model_doc/emu3.md\r\n\r\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\r\n\r\n* fix VAE upsampling\r\n\r\n* Update src/transformers/models/emu3/modular_emu3.py\r\n\r\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\r\n\r\n* address comments\r\n\r\n* state overwritten stuff explicitly\r\n\r\n* fix copies\r\n\r\n* add the flag for flex attn\r\n\r\n---------\r\n\r\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\r\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>",
    "sha": "52e1f87c7d2fdaf147aa625c169ad440672cb3cb",
    "files": [
        {
            "sha": "529b113cf1e57827e024d8dab2788bfdbc304513",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/52e1f87c7d2fdaf147aa625c169ad440672cb3cb/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/52e1f87c7d2fdaf147aa625c169ad440672cb3cb/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=52e1f87c7d2fdaf147aa625c169ad440672cb3cb",
            "patch": "@@ -860,6 +860,8 @@\n         title: DePlot\n       - local: model_doc/donut\n         title: Donut\n+      - local: model_doc/emu3\n+        title: Emu3\n       - local: model_doc/flava\n         title: FLAVA\n       - local: model_doc/git"
        },
        {
            "sha": "d66f4b031ac86bab57f0c356c7e312387ceef1ff",
            "filename": "docs/source/en/index.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/52e1f87c7d2fdaf147aa625c169ad440672cb3cb/docs%2Fsource%2Fen%2Findex.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/52e1f87c7d2fdaf147aa625c169ad440672cb3cb/docs%2Fsource%2Fen%2Findex.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Findex.md?ref=52e1f87c7d2fdaf147aa625c169ad440672cb3cb",
            "patch": "@@ -137,6 +137,7 @@ Flax), PyTorch, and/or TensorFlow.\n |               [EfficientFormer](model_doc/efficientformer)               |       ✅        |         ✅         |      ❌      |\n |                  [EfficientNet](model_doc/efficientnet)                  |       ✅        |         ❌         |      ❌      |\n |                       [ELECTRA](model_doc/electra)                       |       ✅        |         ✅         |      ✅      |\n+|                          [Emu3](model_doc/emu3)                          |       ✅        |         ❌         |      ❌      |\n |                       [EnCodec](model_doc/encodec)                       |       ✅        |         ❌         |      ❌      |\n |               [Encoder decoder](model_doc/encoder-decoder)               |       ✅        |         ✅         |      ✅      |\n |                         [ERNIE](model_doc/ernie)                         |       ✅        |         ❌         |      ❌      |"
        },
        {
            "sha": "0b3220c073fb65e69723129faf35aca237265209",
            "filename": "docs/source/en/model_doc/emu3.md",
            "status": "added",
            "additions": 179,
            "deletions": 0,
            "changes": 179,
            "blob_url": "https://github.com/huggingface/transformers/blob/52e1f87c7d2fdaf147aa625c169ad440672cb3cb/docs%2Fsource%2Fen%2Fmodel_doc%2Femu3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/52e1f87c7d2fdaf147aa625c169ad440672cb3cb/docs%2Fsource%2Fen%2Fmodel_doc%2Femu3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Femu3.md?ref=52e1f87c7d2fdaf147aa625c169ad440672cb3cb",
            "patch": "@@ -0,0 +1,179 @@\n+<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# Emu3\n+\n+## Overview\n+\n+The Emu3 model was proposed in [Emu3: Next-Token Prediction is All You Need](https://arxiv.org/abs/2409.18869) by Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, Yingli Zhao, Yulong Ao, Xuebin Min, Tao Li, Boya Wu, Bo Zhao, Bowen Zhang, Liangdong Wang, Guang Liu, Zheqi He, Xi Yang, Jingjing Liu, Yonghua Lin, Tiejun Huang, Zhongyuan Wang.\n+\n+Emu3 is a multimodal LLM that uses vector quantization to tokenize images into discrete tokens. Discretized image tokens are later fused with text token ids for image and text generation. The model can additionally generate images by predicting image token ids. \n+\n+\n+The abstract from the paper is the following:\n+\n+*While next-token prediction is considered a promising path towards artificial general intelligence, it has struggled to excel in multimodal tasks, which are still dominated by diffusion models (e.g., Stable Diffusion) and compositional approaches (e.g., CLIP combined with LLMs). In this paper, we introduce Emu3, a new suite of state-of-the-art multimodal models trained solely with next-token prediction. By tokenizing images, text, and videos into a discrete space, we train a single transformer from scratch on a mixture of multimodal sequences. Emu3 outperforms several well-established task-specific models in both generation and perception tasks, surpassing flagship models such as SDXL and LLaVA-1.6, while eliminating the need for diffusion or compositional architectures. Emu3 is also capable of generating high-fidelity video via predicting the next token in a video sequence. We simplify complex multimodal model designs by converging on a singular focus: tokens, unlocking great potential for scaling both during training and inference. Our results demonstrate that next-token prediction is a promising path towards building general multimodal intelligence beyond language. We open-source key techniques and models to support further research in this direction.*\n+\n+Tips:\n+\n+- We advise users to set `processor.tokenizer.padding_side = \"left\"` before batched generation as it leads to more accurate results.\n+\n+- Note that the model has been trained with a specific prompt format for chatting. Use `processor.apply_chat_template(my_conversation_dict)` to correctly format your prompts.\n+\n+- Emu3 has two different checkpoints for image-generation and text-generation, make sure to use the correct checkpoint when loading the model. To generate an image, it is advised to use `prefix_constraints` so that the generated tokens are sampled only from possible image tokens. See more below for usage examples.\n+\n+> [!TIP]\n+> Emu3 implementation in Transformers uses a special image token to indicate where to merge image embeddings. The special image token isn't new and uses one of the reserved tokens: `<|extra_0|>`. You have to add `<image>` to your prompt in the place where the image should be embedded for correct generation.\n+\n+\n+This model was contributed by [RaushanTurganbay](https://huggingface.co/RaushanTurganbay).\n+The original code can be found [here](https://github.com/baaivision/Emu3).\n+\n+\n+## Usage example\n+\n+### Text generation inference\n+\n+Here's how to load the model and perform inference in half-precision (`torch.bfloat16`) to generate textual output from text or text and image inputs:\n+\n+```python\n+from transformers import Emu3Processor, Emu3ForConditionalGeneration\n+import torch\n+from PIL import Image\n+import requests\n+\n+processor = Emu3Processor.from_pretrained(\"Emu3-community/Emu3-Chat-hf\")\n+model = Emu3ForConditionalGeneration.from_pretrained(\"Emu3-community/Emu3-Chat-hf\", torch_dtype=torch.bfloat16, device_map=\"cuda\")\n+\n+# prepare image and text prompt\n+url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n+image = Image.open(requests.get(url, stream=True).raw)\n+prompt = \"What do you see in this image?<image>\"\n+\n+inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(model.device, dtype=torch.bfloat16)\n+\n+# autoregressively complete prompt\n+output = model.generate(**inputs, max_new_tokens=50)\n+print(processor.decode(output[0], skip_special_tokens=True))\n+```\n+\n+### Image generation inference\n+\n+Emu3 can also generate images from textual input. Here is how you can do it:\n+\n+```python\n+processor = Emu3Processor.from_pretrained(\"Emu3-community/Emu3-Gen-hf\")\n+model = Emu3ForConditionalGeneration.from_pretrained(\"Emu3-community/Emu3-Gen-hf\", torch_dtype=\"bfloat16\", device_map=\"auto\", attn_implementation=\"flash_attention_2\")\n+\n+\n+inputs = processor(\n+    text=[\"a portrait of young girl. masterpiece, film grained, best quality.\", \"a dog running under the rain\"],\n+    padding=True,\n+    return_tensors=\"pt\",\n+    return_for_image_generation=True,\n+)\n+inputs = inputs.to(device=\"cuda:0\", dtype=torch.bfloat16)\n+\n+neg_prompt = \"lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry.\"\n+neg_inputs = processor(text=[neg_prompt] * 2, return_tensors=\"pt\").to(device=\"cuda:0\")\n+\n+image_sizes = inputs.pop(\"image_sizes\")\n+HEIGHT, WIDTH = image_sizes[0]\n+VISUAL_TOKENS = model.vocabulary_mapping.image_tokens\n+\n+def prefix_allowed_tokens_fn(batch_id, input_ids):\n+    height, width = HEIGHT, WIDTH\n+    visual_tokens = VISUAL_TOKENS\n+    image_wrapper_token_id = torch.tensor([processor.tokenizer.image_wrapper_token_id], device=model.device)\n+    eoi_token_id = torch.tensor([processor.tokenizer.eoi_token_id], device=model.device)\n+    eos_token_id = torch.tensor([processor.tokenizer.eos_token_id], device=model.device)\n+    pad_token_id = torch.tensor([processor.tokenizer.pad_token_id], device=model.device)\n+    eof_token_id = torch.tensor([processor.tokenizer.eof_token_id], device=model.device)\n+    eol_token_id = processor.tokenizer.encode(\"<|extra_200|>\", return_tensors=\"pt\")[0]\n+\n+    position = torch.nonzero(input_ids == image_wrapper_token_id, as_tuple=True)[0][0]\n+    offset = input_ids.shape[0] - position\n+    if offset % (width + 1) == 0:\n+        return (eol_token_id, )\n+    elif offset == (width + 1) * height + 1:\n+        return (eof_token_id, )\n+    elif offset == (width + 1) * height + 2:\n+        return (eoi_token_id, )\n+    elif offset == (width + 1) * height + 3:\n+        return (eos_token_id, )\n+    elif offset > (width + 1) * height + 3:\n+        return (pad_token_id, )\n+    else:\n+        return visual_tokens\n+\n+\n+out = model.generate(\n+    **inputs,\n+    max_new_tokens=50_000, # make sure to have enough tokens for one image\n+    prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n+    return_dict_in_generate=True,\n+    negative_prompt_ids=neg_inputs.input_ids, # indicate for Classifier-Free Guidance\n+    negative_prompt_attention_mask=neg_inputs.attention_mask,\n+)\n+\n+image = model.decode_image_tokens(out.sequences[:, inputs.input_ids.shape[1]: ], height=HEIGHT, width=WIDTH)\n+images = processor.postprocess(list(image.float()), return_tensors=\"PIL.Image.Image\") # internally we convert to np but it's not supported in bf16 precision\n+for i, image in enumerate(images['pixel_values']):\n+    image.save(f\"result{i}.png\")\n+\n+```\n+\n+\n+## Emu3Config\n+\n+[[autodoc]] Emu3Config\n+\n+## Emu3VQVAEConfig\n+\n+[[autodoc]] Emu3VQVAEConfig\n+\n+## Emu3TextConfig\n+\n+[[autodoc]] Emu3TextConfig\n+\n+## Emu3Processor\n+\n+[[autodoc]] Emu3Processor\n+\n+## Emu3ImageProcessor\n+\n+[[autodoc]] Emu3ImageProcessor\n+    - preprocess\n+\n+## Emu3VQVAE\n+\n+[[autodoc]] Emu3VQVAE\n+    - forward\n+\n+## Emu3TextModel\n+\n+[[autodoc]] Emu3TextModel\n+    - forward\n+\n+## Emu3ForCausalLM\n+\n+[[autodoc]] Emu3ForCausalLM\n+    - forward\n+\n+## Emu3ForConditionalGeneration\n+\n+[[autodoc]] Emu3ForConditionalGeneration\n+    - forward"
        },
        {
            "sha": "3e6d764617de2db4be71ab68bb9ff6ae5a668a2e",
            "filename": "docs/source/en/perf_infer_gpu_one.md",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/52e1f87c7d2fdaf147aa625c169ad440672cb3cb/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/52e1f87c7d2fdaf147aa625c169ad440672cb3cb/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md?ref=52e1f87c7d2fdaf147aa625c169ad440672cb3cb",
            "patch": "@@ -49,6 +49,7 @@ FlashAttention-2 is currently supported for the following architectures:\n * [Dbrx](https://huggingface.co/docs/transformers/model_doc/dbrx#transformers.DbrxModel)\n * [DiffLlama](https://huggingface.co/docs/transformers/model_doc/diffllama#transformers.DiffLlamaModel)\n * [DistilBert](https://huggingface.co/docs/transformers/model_doc/distilbert#transformers.DistilBertModel)\n+* [Emu3](https://huggingface.co/docs/transformers/model_doc/emu3)\n * [Gemma](https://huggingface.co/docs/transformers/model_doc/gemma#transformers.GemmaModel)\n * [Gemma2](https://huggingface.co/docs/transformers/model_doc/gemma2#transformers.Gemma2Model)\n * [GPT2](https://huggingface.co/docs/transformers/model_doc/gpt2)\n@@ -245,6 +246,7 @@ For now, Transformers supports SDPA inference and training for the following arc\n * [DistilBert](https://huggingface.co/docs/transformers/model_doc/distilbert#transformers.DistilBertModel)\n * [Dpr](https://huggingface.co/docs/transformers/model_doc/dpr#transformers.DprReader)\n * [EncoderDecoder](https://huggingface.co/docs/transformers/model_doc/encoder_decoder#transformers.EncoderDecoderModel)\n+* [Emu3](https://huggingface.co/docs/transformers/model_doc/emu3)\n * [Falcon](https://huggingface.co/docs/transformers/model_doc/falcon#transformers.FalconModel)\n * [Gemma](https://huggingface.co/docs/transformers/model_doc/gemma#transformers.GemmaModel)\n * [Gemma2](https://huggingface.co/docs/transformers/model_doc/gemma2#transformers.Gemma2Model)"
        },
        {
            "sha": "3cabdf3cee6ade655e516e06c292159ec0e9e2b0",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 30,
            "deletions": 0,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/52e1f87c7d2fdaf147aa625c169ad440672cb3cb/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52e1f87c7d2fdaf147aa625c169ad440672cb3cb/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=52e1f87c7d2fdaf147aa625c169ad440672cb3cb",
            "patch": "@@ -428,6 +428,12 @@\n         \"ElectraConfig\",\n         \"ElectraTokenizer\",\n     ],\n+    \"models.emu3\": [\n+        \"Emu3Config\",\n+        \"Emu3Processor\",\n+        \"Emu3TextConfig\",\n+        \"Emu3VQVAEConfig\",\n+    ],\n     \"models.encodec\": [\n         \"EncodecConfig\",\n         \"EncodecFeatureExtractor\",\n@@ -1222,6 +1228,7 @@\n     _import_structure[\"models.donut\"].extend([\"DonutFeatureExtractor\", \"DonutImageProcessor\"])\n     _import_structure[\"models.dpt\"].extend([\"DPTFeatureExtractor\", \"DPTImageProcessor\"])\n     _import_structure[\"models.efficientnet\"].append(\"EfficientNetImageProcessor\")\n+    _import_structure[\"models.emu3\"].append(\"Emu3ImageProcessor\")\n     _import_structure[\"models.flava\"].extend([\"FlavaFeatureExtractor\", \"FlavaImageProcessor\", \"FlavaProcessor\"])\n     _import_structure[\"models.fuyu\"].extend([\"FuyuImageProcessor\", \"FuyuProcessor\"])\n     _import_structure[\"models.glpn\"].extend([\"GLPNFeatureExtractor\", \"GLPNImageProcessor\"])\n@@ -2243,6 +2250,15 @@\n             \"load_tf_weights_in_electra\",\n         ]\n     )\n+    _import_structure[\"models.emu3\"].extend(\n+        [\n+            \"Emu3ForCausalLM\",\n+            \"Emu3ForConditionalGeneration\",\n+            \"Emu3PreTrainedModel\",\n+            \"Emu3TextModel\",\n+            \"Emu3VQVAE\",\n+        ]\n+    )\n     _import_structure[\"models.encodec\"].extend(\n         [\n             \"EncodecModel\",\n@@ -5440,6 +5456,12 @@\n         ElectraConfig,\n         ElectraTokenizer,\n     )\n+    from .models.emu3 import (\n+        Emu3Config,\n+        Emu3Processor,\n+        Emu3TextConfig,\n+        Emu3VQVAEConfig,\n+    )\n     from .models.encodec import (\n         EncodecConfig,\n         EncodecFeatureExtractor,\n@@ -6270,6 +6292,7 @@\n         from .models.donut import DonutFeatureExtractor, DonutImageProcessor\n         from .models.dpt import DPTFeatureExtractor, DPTImageProcessor\n         from .models.efficientnet import EfficientNetImageProcessor\n+        from .models.emu3 import Emu3ImageProcessor\n         from .models.flava import (\n             FlavaFeatureExtractor,\n             FlavaImageProcessor,\n@@ -7139,6 +7162,13 @@\n             ElectraPreTrainedModel,\n             load_tf_weights_in_electra,\n         )\n+        from .models.emu3 import (\n+            Emu3ForCausalLM,\n+            Emu3ForConditionalGeneration,\n+            Emu3PreTrainedModel,\n+            Emu3TextModel,\n+            Emu3VQVAE,\n+        )\n         from .models.encodec import (\n             EncodecModel,\n             EncodecPreTrainedModel,"
        },
        {
            "sha": "18cbab6005762156bc2716cd20179fe439c73608",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/52e1f87c7d2fdaf147aa625c169ad440672cb3cb/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52e1f87c7d2fdaf147aa625c169ad440672cb3cb/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=52e1f87c7d2fdaf147aa625c169ad440672cb3cb",
            "patch": "@@ -1634,17 +1634,18 @@ def _get_cache(\n                     cache_dtype = self.get_output_embeddings().weight.dtype\n \n             def get_layer_device_map(execution_device_map: Optional[dict] = None):\n+                num_hidden_layers = self.config.get_text_config().num_hidden_layers\n                 if execution_device_map is None:\n                     return None\n                 elif len(execution_device_map) == 1 and \"\" in execution_device_map:\n-                    return {idx: execution_device_map[\"\"] for idx in range(self.config.num_hidden_layers)}\n+                    return {idx: execution_device_map[\"\"] for idx in range(num_hidden_layers)}\n                 layer_device_map = {}\n                 for layer in execution_device_map:\n-                    for idx in range(self.config.num_hidden_layers):\n+                    for idx in range(num_hidden_layers):\n                         if f\".{idx}.\" in f\"{layer}.\":\n                             layer_device_map[idx] = execution_device_map[layer]\n                             break\n-                for idx in range(self.config.num_hidden_layers):\n+                for idx in range(num_hidden_layers):\n                     if idx not in layer_device_map:\n                         raise RuntimeError(f\"layer {idx} has not been mapped to a device.\")\n                 return layer_device_map"
        },
        {
            "sha": "7db328f87af1fbd1bdc11519d769a41f7a34528e",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/52e1f87c7d2fdaf147aa625c169ad440672cb3cb/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52e1f87c7d2fdaf147aa625c169ad440672cb3cb/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=52e1f87c7d2fdaf147aa625c169ad440672cb3cb",
            "patch": "@@ -86,6 +86,7 @@\n     dpt,\n     efficientnet,\n     electra,\n+    emu3,\n     encodec,\n     encoder_decoder,\n     ernie,"
        },
        {
            "sha": "985fe59582d8753bfffffdbb4a903b9485fe49f2",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/52e1f87c7d2fdaf147aa625c169ad440672cb3cb/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52e1f87c7d2fdaf147aa625c169ad440672cb3cb/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=52e1f87c7d2fdaf147aa625c169ad440672cb3cb",
            "patch": "@@ -103,6 +103,7 @@\n         (\"efficientformer\", \"EfficientFormerConfig\"),\n         (\"efficientnet\", \"EfficientNetConfig\"),\n         (\"electra\", \"ElectraConfig\"),\n+        (\"emu3\", \"Emu3Config\"),\n         (\"encodec\", \"EncodecConfig\"),\n         (\"encoder-decoder\", \"EncoderDecoderConfig\"),\n         (\"ernie\", \"ErnieConfig\"),\n@@ -420,6 +421,7 @@\n         (\"efficientformer\", \"EfficientFormer\"),\n         (\"efficientnet\", \"EfficientNet\"),\n         (\"electra\", \"ELECTRA\"),\n+        (\"emu3\", \"Emu3\"),\n         (\"encodec\", \"EnCodec\"),\n         (\"encoder-decoder\", \"Encoder decoder\"),\n         (\"ernie\", \"ERNIE\"),"
        },
        {
            "sha": "bf54a6ce97857bb29a6b459b715a7a7896c8f1d7",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/52e1f87c7d2fdaf147aa625c169ad440672cb3cb/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52e1f87c7d2fdaf147aa625c169ad440672cb3cb/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=52e1f87c7d2fdaf147aa625c169ad440672cb3cb",
            "patch": "@@ -499,6 +499,7 @@\n         (\"dbrx\", \"DbrxForCausalLM\"),\n         (\"diffllama\", \"DiffLlamaForCausalLM\"),\n         (\"electra\", \"ElectraForCausalLM\"),\n+        (\"emu3\", \"Emu3ForCausalLM\"),\n         (\"ernie\", \"ErnieForCausalLM\"),\n         (\"falcon\", \"FalconForCausalLM\"),\n         (\"falcon_mamba\", \"FalconMambaForCausalLM\"),\n@@ -800,6 +801,7 @@\n         (\"blip\", \"BlipForConditionalGeneration\"),\n         (\"blip-2\", \"Blip2ForConditionalGeneration\"),\n         (\"chameleon\", \"ChameleonForConditionalGeneration\"),\n+        (\"emu3\", \"Emu3ForConditionalGeneration\"),\n         (\"fuyu\", \"FuyuForCausalLM\"),\n         (\"git\", \"GitForCausalLM\"),\n         (\"idefics\", \"IdeficsForVisionText2Text\"),\n@@ -1428,6 +1430,7 @@\n         (\"deberta-v2\", \"DebertaV2Model\"),\n         (\"distilbert\", \"DistilBertModel\"),\n         (\"electra\", \"ElectraModel\"),\n+        (\"emu3\", \"Emu3TextModel\"),\n         (\"flaubert\", \"FlaubertModel\"),\n         (\"ibert\", \"IBertModel\"),\n         (\"longformer\", \"LongformerModel\"),"
        },
        {
            "sha": "cf52e73f568aba5306ad6a5e8b650b15e61c272c",
            "filename": "src/transformers/models/auto/processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/52e1f87c7d2fdaf147aa625c169ad440672cb3cb/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52e1f87c7d2fdaf147aa625c169ad440672cb3cb/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py?ref=52e1f87c7d2fdaf147aa625c169ad440672cb3cb",
            "patch": "@@ -59,6 +59,7 @@\n         (\"clipseg\", \"CLIPSegProcessor\"),\n         (\"clvp\", \"ClvpProcessor\"),\n         (\"colpali\", \"ColPaliProcessor\"),\n+        (\"emu3\", \"Emu3Processor\"),\n         (\"flava\", \"FlavaProcessor\"),\n         (\"fuyu\", \"FuyuProcessor\"),\n         (\"git\", \"GitProcessor\"),"
        },
        {
            "sha": "2e26cea97139d48d1dca189a7a1c6184d040e017",
            "filename": "src/transformers/models/auto/tokenization_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/52e1f87c7d2fdaf147aa625c169ad440672cb3cb/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52e1f87c7d2fdaf147aa625c169ad440672cb3cb/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py?ref=52e1f87c7d2fdaf147aa625c169ad440672cb3cb",
            "patch": "@@ -186,6 +186,7 @@\n                 ),\n             ),\n             (\"electra\", (\"ElectraTokenizer\", \"ElectraTokenizerFast\" if is_tokenizers_available() else None)),\n+            (\"emu3\", (\"GPT2Tokenizer\", \"GPT2TokenizerFast\" if is_tokenizers_available() else None)),\n             (\"ernie\", (\"BertTokenizer\", \"BertTokenizerFast\" if is_tokenizers_available() else None)),\n             (\"ernie_m\", (\"ErnieMTokenizer\" if is_sentencepiece_available() else None, None)),\n             (\"esm\", (\"EsmTokenizer\", None)),"
        },
        {
            "sha": "99da53c6c612ff0f4bbbe59ae34222661e8b1756",
            "filename": "src/transformers/models/chameleon/processing_chameleon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/52e1f87c7d2fdaf147aa625c169ad440672cb3cb/src%2Ftransformers%2Fmodels%2Fchameleon%2Fprocessing_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52e1f87c7d2fdaf147aa625c169ad440672cb3cb/src%2Ftransformers%2Fmodels%2Fchameleon%2Fprocessing_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fprocessing_chameleon.py?ref=52e1f87c7d2fdaf147aa625c169ad440672cb3cb",
            "patch": "@@ -62,6 +62,7 @@ class ChameleonProcessor(ProcessorMixin):\n \n     attributes = [\"image_processor\", \"tokenizer\"]\n     tokenizer_class = (\"LlamaTokenizer\", \"LlamaTokenizerFast\")\n+    valid_kwargs = [\"image_seq_length\", \"image_token\"]\n     image_processor_class = \"ChameleonImageProcessor\"\n \n     def __init__(self, image_processor, tokenizer, image_seq_length: int = 1024, image_token: str = \"<image>\"):"
        },
        {
            "sha": "d8555f58d1866451c38abb5559ef5bef9545f0b0",
            "filename": "src/transformers/models/emu3/__init__.py",
            "status": "added",
            "additions": 29,
            "deletions": 0,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/52e1f87c7d2fdaf147aa625c169ad440672cb3cb/src%2Ftransformers%2Fmodels%2Femu3%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52e1f87c7d2fdaf147aa625c169ad440672cb3cb/src%2Ftransformers%2Fmodels%2Femu3%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2F__init__.py?ref=52e1f87c7d2fdaf147aa625c169ad440672cb3cb",
            "patch": "@@ -0,0 +1,29 @@\n+# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_emu3 import *\n+    from .image_processing_emu3 import *\n+    from .modeling_emu3 import *\n+    from .processing_emu3 import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "5b5abedf4016d5959c8eeea9a3d955470c8b1f13",
            "filename": "src/transformers/models/emu3/configuration_emu3.py",
            "status": "added",
            "additions": 327,
            "deletions": 0,
            "changes": 327,
            "blob_url": "https://github.com/huggingface/transformers/blob/52e1f87c7d2fdaf147aa625c169ad440672cb3cb/src%2Ftransformers%2Fmodels%2Femu3%2Fconfiguration_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52e1f87c7d2fdaf147aa625c169ad440672cb3cb/src%2Ftransformers%2Fmodels%2Femu3%2Fconfiguration_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fconfiguration_emu3.py?ref=52e1f87c7d2fdaf147aa625c169ad440672cb3cb",
            "patch": "@@ -0,0 +1,327 @@\n+# coding=utf-8\n+# Copyright 2024 HuggingFace Inc. team. All rights reserved.\n+#\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import Dict, List, Optional, Union\n+\n+from ...configuration_utils import PretrainedConfig\n+from ...modeling_rope_utils import rope_config_validation\n+\n+\n+class Emu3VQVAEConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`Emu3VQVAE`]. It is used to instantiate an VQ-VAE\n+    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n+    defaults will yield a configuration to the VQ model presented in Emu3 paper.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+    Args:\n+        codebook_size (`int`, *optional*, defaults to 32768):\n+            Codebook size of the VQ model.\n+        embed_dim (`int`, *optional*, defaults to 4):\n+            Dimension of the quantized vector in codebook.\n+        latent_channels (`int`, *optional*, defaults to 4):\n+            Dimension of the output channel of encoder and the input channel of decoder\n+        double_latent (`bool`, *optional*, defaults to `False`):\n+            Whether double the output dim of the encoder.\n+        in_channels (`int`, *optional*, defaults to 3):\n+            Input channel of encoder.\n+        out_channels (`int`, *optional*, defaults to 3):\n+            Output channel of decoder.\n+        temporal_downsample_factor (`int`, *optional*, defaults to 4):\n+            Temporal downsample factor.\n+        base_channels (`int`, *optional*, defaults to 256):\n+            Basic channel number of the intermediate blocks.\n+        channel_multiplier (`List[int]`, *optional*, defaults to `[1, 2, 2, 4]`):\n+            Channel scaling factor of the intermediate blocks.\n+        num_res_blocks (`int`, *optional*, defaults to 2):\n+            Residual block number in each stage.\n+        attn_resolutions (`List[int]`, *optional*, defaults to `[3]`):\n+            Stage indices to apply attention.\n+        hidden_size (`int`, *optional*, defaults to 1024):\n+            Dimension of the hidden representations in the attention layer.\n+        num_attention_heads (`int`, *optional*, defaults to 1):\n+            Number of attention heads for each attention layer.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+\n+    ```python\n+    >>> from transformers import Emu3VQVAE, Emu3VQVAEConfig\n+\n+    >>> # Initializing a video VQ model of Emu3 configuration\n+    >>> configuration = Emu3VQVAEConfig()\n+\n+    >>> # Initializing a model from the Emu3 VQ model style configuration\n+    >>> model = Emu3VQVAE(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"emu3_vqgan\"\n+    base_config_key = \"vq_config\"\n+\n+    def __init__(\n+        self,\n+        codebook_size: int = 32768,\n+        embed_dim: int = 4,\n+        latent_channels: int = 4,\n+        double_latent: bool = False,\n+        in_channels: int = 3,\n+        out_channels: int = 3,\n+        temporal_downsample_factor: int = 4,\n+        base_channels: int = 256,\n+        channel_multiplier: List[int] = [1, 2, 2, 4],\n+        num_res_blocks: int = 2,\n+        attn_resolutions: List[int] = [3],\n+        hidden_size: int = 1024,\n+        num_attention_heads: int = 1,\n+        attention_dropout: float = 0.0,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+\n+        self.codebook_size = codebook_size\n+        self.embed_dim = embed_dim\n+        self.latent_channels = latent_channels\n+        self.double_latent = double_latent\n+        self.in_channels = in_channels\n+        self.out_channels = out_channels\n+        self.temporal_downsample_factor = temporal_downsample_factor\n+        self.base_channels = base_channels\n+        self.channel_multiplier = channel_multiplier\n+        self.num_res_blocks = num_res_blocks\n+        self.attn_resolutions = attn_resolutions\n+        self.hidden_size = hidden_size\n+        self.num_attention_heads = num_attention_heads\n+        self.attention_dropout = attention_dropout\n+\n+\n+class Emu3TextConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`Emu3TextModel`]. It is used to instantiate a\n+    emu3 model according to the specified arguments, defining the model architecture. Instantiating a\n+    configuration with the defaults will yield a similar configuration to that of the\n+    [Emu3-community/Emu3-Chat-hf](https://huggingface.co/Emu3-community/Emu3-Chat-hf).\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+\n+    Args:\n+        vocab_size (`int`, *optional*, defaults to 184622):\n+            Vocabulary size of the Emu3 model. Defines the number of different tokens that can be represented by the\n+            `inputs_ids` passed when calling [`Emu3Model`]\n+        hidden_size (`int`, *optional*, defaults to 4096):\n+            Dimension of the hidden representations.\n+        intermediate_size (`int`, *optional*, defaults to 14336):\n+            Dimension of the MLP representations.\n+        num_hidden_layers (`int`, *optional*, defaults to 32):\n+            Number of hidden layers in the Transformer decoder.\n+        num_attention_heads (`int`, *optional*, defaults to 32):\n+            Number of attention heads for each attention layer in the Transformer decoder.\n+        num_key_value_heads (`int`, *optional*, defaults to 8):\n+            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n+            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n+            `num_key_value_heads=1 the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n+            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n+            by meanpooling all the original heads within that group. For more details checkout [this\n+            paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n+            `num_attention_heads`.\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n+            The non-linear activation function (function or string) in the decoder.\n+        max_position_embeddings (`int`, *optional*, defaults to 9216):\n+            The maximum sequence length that this model might ever be used with. Emu supports up to 9216 tokens,\n+        rms_norm_eps (`float`, *optional*, defaults to 1e-05):\n+            The epsilon used by the rms normalization layers.\n+        use_cache (`bool`, *optional*, defaults to `True`):\n+            Whether or not the model should return the last key/values attentions (not used by all models). Only\n+            relevant if `config.is_decoder=True`.\n+        pad_token_id (`int`, *optional*, defaults to 151643):\n+            Padding token id.\n+        bos_token_id (`int`, *optional*, defaults to 151849):\n+            Beginning of stream token id.\n+        eos_token_id (`int`, *optional*, defaults to 151850):\n+            End of stream token id.\n+        tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n+            Whether to tie weight embeddings\n+        rope_theta (`float`, *optional*, defaults to 1000000.0):\n+            The base period of the RoPE embeddings.\n+        rope_scaling (`Dict`, *optional*):\n+            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n+            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n+            accordingly.\n+            Expected contents:\n+                `rope_type` (`str`):\n+                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n+                    'llama3'], with 'default' being the original RoPE implementation.\n+                `factor` (`float`, *optional*):\n+                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n+                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n+                    original maximum pre-trained length.\n+                `original_max_position_embeddings` (`int`, *optional*):\n+                    Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n+                    pretraining.\n+                `attention_factor` (`float`, *optional*):\n+                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n+                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n+                    `factor` field to infer the suggested value.\n+                `beta_fast` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 32.\n+                `beta_slow` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 1.\n+                `short_factor` (`List[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `long_factor` (`List[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `low_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n+                `high_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n+        mlp_bias (`bool`, *optional*, defaults to `False`):\n+            Whether to use a bias in up_proj, down_proj and gate_proj layers in the MLP layers.\n+        attention_bias (`bool`, *optional*, defaults to `False`):\n+            Whether to use a bias in the query, key, value and output projection layers during self-attention.\n+        attention_dropout (`float`, *optional*, defaults to 0.1):\n+            The dropout ratio for the attention probabilities.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+\n+\n+    ```python\n+    >>> from transformers import Emu3Model, Emu3Config\n+\n+    >>> # Initializing a Emu3-community/Emu3-Chat-hf style configuration\n+    >>> configuration = Emu3Config()\n+\n+    >>> # Initializing a model from the Emu3-community/Emu3-Chat-hf style configuration\n+    >>> model = Emu3Model(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"emu3_text_model\"\n+    base_config_key = \"text_config\"\n+    keys_to_ignore_at_inference = [\"past_key_values\"]\n+\n+    def __init__(\n+        self,\n+        vocab_size: int = 184622,\n+        hidden_size: int = 4096,\n+        intermediate_size: int = 14336,\n+        num_hidden_layers: int = 32,\n+        num_attention_heads: int = 32,\n+        num_key_value_heads: Optional[int] = 8,\n+        hidden_act: str = \"silu\",\n+        max_position_embeddings: int = 9216,\n+        rms_norm_eps: float = 1e-5,\n+        use_cache: bool = True,\n+        pad_token_id: int = 151643,\n+        bos_token_id: int = 151849,\n+        eos_token_id: int = 151850,\n+        tie_word_embeddings: bool = False,\n+        rope_theta: float = 1000000.0,\n+        rope_scaling: Optional = None,\n+        mlp_bias=False,\n+        attention_bias=False,\n+        attention_dropout: float = 0.1,\n+        initializer_range: float = 0.02,\n+        **kwargs,\n+    ):\n+        self.vocab_size = vocab_size\n+        self.max_position_embeddings = max_position_embeddings\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.num_key_value_heads = num_key_value_heads\n+        self.hidden_act = hidden_act\n+        self.rms_norm_eps = rms_norm_eps\n+        self.use_cache = use_cache\n+        self.rope_theta = rope_theta\n+        self.rope_scaling = rope_scaling\n+        self.mlp_bias = mlp_bias\n+        self.attention_bias = attention_bias\n+        self.initializer_range = initializer_range\n+        rope_config_validation(self)\n+\n+        self.attention_dropout = attention_dropout\n+\n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n+\n+\n+class Emu3Config(PretrainedConfig):\n+    \"\"\"\n+    This is the configuration class to store the configuration of a [`Emu3Model`]. It is used to instantiate a\n+    emu3 model according to the specified arguments, defining the model architecture. Instantiating a\n+    configuration with the defaults will yield a similar configuration to that of the\n+    [Emu3-community/Emu3-Chat-hf](https://huggingface.co/Emu3-community/Emu3-Chat-hf).\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+\n+    Args:\n+        vq_config (`Union[Dict, Emu3VQVAEConfig]`, *optional*):\n+            Emu3VQVAEConfig instance containing the configuration for the VQ-VAE model.\n+        text_config (`Union[Dict, Emu3TextConfig]``, *optional*):\n+            Emu3TextConfig instance containing the configuration for the language model.\n+        vocabulary_map (`dict`, *optional*):\n+            A dictionary containing the vocabulary map from the tokenizer. Used to obtain tokens from the image inputs.\n+    \"\"\"\n+\n+    model_type = \"emu3\"\n+    keys_to_ignore_at_inference = [\"past_key_values\"]\n+    sub_configs = {\"text_config\": Emu3TextConfig, \"vq_config\": Emu3VQVAEConfig}\n+\n+    def __init__(\n+        self,\n+        vq_config: Union[Dict, Emu3VQVAEConfig] = None,\n+        text_config: Union[Dict, Emu3TextConfig] = None,\n+        vocabulary_map: Dict[int, int] = None,\n+        **kwargs,\n+    ):\n+        if vq_config is None:\n+            vq_config = Emu3VQVAEConfig()\n+        elif isinstance(vq_config, dict):\n+            vq_config = Emu3VQVAEConfig(**vq_config)\n+\n+        if text_config is None:\n+            text_config = Emu3TextConfig()\n+        elif isinstance(text_config, dict):\n+            text_config = Emu3TextConfig(**text_config)\n+\n+        self.vq_config = vq_config\n+        self.text_config = text_config\n+        self.vocabulary_map = vocabulary_map\n+\n+        super().__init__(**kwargs)\n+\n+\n+__all__ = [\"Emu3Config\", \"Emu3TextConfig\", \"Emu3VQVAEConfig\"]"
        },
        {
            "sha": "8ac8db7e429031ee5157532bb8f4fb044844d281",
            "filename": "src/transformers/models/emu3/convert_emu3_weights_to_hf.py",
            "status": "added",
            "additions": 448,
            "deletions": 0,
            "changes": 448,
            "blob_url": "https://github.com/huggingface/transformers/blob/52e1f87c7d2fdaf147aa625c169ad440672cb3cb/src%2Ftransformers%2Fmodels%2Femu3%2Fconvert_emu3_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52e1f87c7d2fdaf147aa625c169ad440672cb3cb/src%2Ftransformers%2Fmodels%2Femu3%2Fconvert_emu3_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fconvert_emu3_weights_to_hf.py?ref=52e1f87c7d2fdaf147aa625c169ad440672cb3cb",
            "patch": "@@ -0,0 +1,448 @@\n+# Copyright 2024 The Emu team, BAAI and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import argparse\n+import json\n+import os\n+import re\n+from typing import Dict, Optional\n+\n+import requests\n+import torch\n+from accelerate import init_empty_weights\n+from PIL import Image\n+\n+from transformers import (\n+    AutoModel,\n+    AutoModelForCausalLM,\n+    AutoTokenizer,\n+    Emu3Config,\n+    Emu3ForConditionalGeneration,\n+    Emu3ImageProcessor,\n+    Emu3Processor,\n+    Emu3TextConfig,\n+    GenerationConfig,\n+)\n+from transformers.models.gpt2.tokenization_gpt2 import bytes_to_unicode\n+\n+\n+\"\"\"\n+Sample usage:\n+\n+```\n+python src/transformers/models/emu3/convert_emu3_weights_to_hf.py \\\n+    --vq_model_id BAAI/Emu3-VisionTokenizer --llm_model_id BAAI/Emu3-Chat --output_dir /output/path\n+```\n+\n+Thereafter, models can be loaded via:\n+\n+```py\n+from transformers import Emu3ForConditionalGeneration, Emu3Processor\n+\n+model = Emu3ForConditionalGeneration.from_pretrained(\"/output/path\")\n+processor = Emu3Processor.from_pretrained(\"/output/path\")\n+```\n+\n+\"\"\"\n+\n+\n+byte_encoder = bytes_to_unicode()\n+CHAT_TEMPLATE = \"{% for message in messages %}{% if message['role'] != 'system' %}{{ message['role'].upper() + ': '}}{% endif %}{# Render all images first #}{% for content in message['content'] | selectattr('type', 'equalto', 'image') %}{{ '<image>' }}{% endfor %}{# Render all text next #}{% if message['role'] != 'assistant' %}{% for content in message['content'] | selectattr('type', 'equalto', 'text') %}{{ content['text'] + ' '}}{% endfor %}{% else %}{% for content in message['content'] | selectattr('type', 'equalto', 'text') %}{% generation %}{{ content['text'] + ' '}}{% endgeneration %}{% endfor %}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ 'ASSISTANT:' }}{% endif %}\"\n+\n+\n+# Tiktoken to HF conversion, thanks for Xenova\n+def token_bytes_to_string(b):\n+    return \"\".join([byte_encoder[ord(char)] for char in b.decode(\"latin-1\")])\n+\n+\n+# Adapted from https://github.com/openai/tiktoken/issues/60#issuecomment-1499977960\n+def bpe(mergeable_ranks: Dict[bytes, int], token: bytes, max_rank: Optional[int] = None):\n+    parts = [bytes([b]) for b in token]\n+    while True:\n+        min_idx = None\n+        min_rank = None\n+        for i, pair in enumerate(zip(parts[:-1], parts[1:])):\n+            rank = mergeable_ranks.get(pair[0] + pair[1])\n+            if rank is not None and (min_rank is None or rank < min_rank):\n+                min_idx = i\n+                min_rank = rank\n+        if min_rank is None or (max_rank is not None and min_rank >= max_rank):\n+            break\n+        assert min_idx is not None\n+        parts = parts[:min_idx] + [parts[min_idx] + parts[min_idx + 1]] + parts[min_idx + 2 :]\n+    return parts\n+\n+\n+def generate_vocab_and_merges(encoder):\n+    mergeable_ranks = encoder._mergeable_ranks\n+\n+    merges = []\n+    vocab = {}\n+    for token, rank in mergeable_ranks.items():\n+        vocab[token_bytes_to_string(token)] = rank\n+\n+        if len(token) == 1:\n+            continue\n+        merged = tuple(bpe(mergeable_ranks, token, max_rank=rank))\n+        assert len(merged) == 2\n+        merges.append(\" \".join(map(token_bytes_to_string, merged)))\n+\n+    # Also add special tokens\n+    vocab.update(encoder._special_tokens)\n+    return vocab, merges\n+\n+\n+def convert_tiktoken(tokenizer, output_dir):\n+    encoder = tokenizer.tokenizer\n+    vocab, merges = generate_vocab_and_merges(encoder)\n+    added_tokens = [\n+        {\n+            \"id\": id,\n+            \"content\": content,\n+            \"single_word\": False,\n+            \"lstrip\": False,\n+            \"rstrip\": False,\n+            \"normalized\": False,\n+            \"special\": True,\n+        }\n+        for content, id in encoder._special_tokens.items()\n+        if content != \"<|extra_0|>\"\n+    ]\n+\n+    # https://huggingface.co/Xenova/gpt2/raw/main/tokenizer_config.json\n+    tokenizer_config_template = {\n+        \"add_prefix_space\": False,\n+        \"bos_token\": \"<|extra_203|>\",\n+        \"clean_up_tokenization_spaces\": False,\n+        \"eos_token\": \"<|extra_204|>\",\n+        \"pad_token\": \"<|endoftext|>\",\n+    }\n+    tokenizer_config_template.update({\"tokenizer_class\": \"GPT2Tokenizer\"})\n+    tokenizer_config_template = dict(sorted(tokenizer_config_template.items(), key=lambda x: x[0]))\n+\n+    # add placeholder image token by taking one of the reserved tokens\n+    reserved_token_id = vocab[\"<|extra_0|>\"]\n+    vocab[\"<image>\"] = reserved_token_id\n+    del vocab[\"<|extra_0|>\"]\n+    added_tokens.append(\n+        {\n+            \"id\": reserved_token_id,\n+            \"content\": \"<image>\",\n+            \"single_word\": False,\n+            \"lstrip\": False,\n+            \"rstrip\": False,\n+            \"normalized\": False,\n+            \"special\": True,\n+        }\n+    )\n+\n+    os.makedirs(output_dir, exist_ok=True)\n+\n+    pre_tokenizer = {\n+        \"type\": \"ByteLevel\",\n+        \"add_prefix_space\": False,\n+        \"trim_offsets\": True,\n+        \"use_regex\": True,\n+    }\n+\n+    # https://huggingface.co/Xenova/gpt2/raw/main/tokenizer.json\n+    tokenizer_template = {\n+        \"version\": \"1.0\",\n+        \"truncation\": None,\n+        \"padding\": None,\n+        \"added_tokens\": added_tokens,\n+        \"normalizer\": None,\n+        \"pre_tokenizer\": pre_tokenizer,\n+        \"post_processor\": None,\n+        \"decoder\": {\n+            \"type\": \"ByteLevel\",\n+            \"add_prefix_space\": True,\n+            \"trim_offsets\": True,\n+            \"use_regex\": True,\n+        },\n+        \"model\": {\n+            \"type\": \"BPE\",\n+            \"dropout\": None,\n+            \"unk_token\": None,\n+            \"continuing_subword_prefix\": \"\",\n+            \"end_of_word_suffix\": \"\",\n+            \"fuse_unk\": False,\n+            \"byte_fallback\": False,\n+            \"vocab\": vocab,\n+            \"merges\": merges,\n+        },\n+    }\n+\n+    # Save to files\n+    with open(os.path.join(output_dir, \"vocab.json\"), \"w\", encoding=\"utf-8\") as fp:\n+        json.dump(vocab, fp, indent=2, ensure_ascii=False)\n+\n+    with open(os.path.join(output_dir, \"tokenizer.json\"), \"w\", encoding=\"utf-8\") as fp:\n+        json.dump(tokenizer_template, fp, indent=2, ensure_ascii=False)\n+\n+    with open(os.path.join(output_dir, \"tokenizer_config.json\"), \"w\", encoding=\"utf-8\") as fp:\n+        json.dump(tokenizer_config_template, fp, indent=2, ensure_ascii=False)\n+\n+    with open(os.path.join(output_dir, \"special_tokens_map.json\"), \"w\", encoding=\"utf-8\") as fp:\n+        json.dump(\n+            {\n+                \"bos_token\": \"<|extra_203|>\",\n+                \"eos_token\": \"<|extra_204|>\",\n+                \"pad_token\": \"<|endoftext|>\",\n+            },\n+            fp,\n+            indent=2,\n+            ensure_ascii=False,\n+        )\n+\n+    with open(os.path.join(output_dir, \"merges.txt\"), \"w\", encoding=\"utf-8\") as fp:\n+        fp.write(\"#version: 0.2\\n\")\n+        fp.write(\"\\n\".join(merges))\n+\n+\n+KEYS_TO_MODIFY_MAPPING = {\n+    \"^encoder\": \"model.vqmodel.encoder\",\n+    \"^decoder\": \"model.vqmodel.decoder\",\n+    \"^post_quant_conv\": \"model.vqmodel.post_quant_conv\",\n+    \"^quant_conv\": \"model.vqmodel.quant_conv\",\n+    \"^quantize\": \"model.vqmodel.quantize\",\n+    \"^model\": \"text_model.model\",\n+    r\"lm_head\\.weight\": \"text_model.lm_head.weight\",\n+    r\"^text_model\\.model\\.vqmodel\": \"vqmodel\",\n+    # rename QKV proj for the VQ-VAE model because we use SiglipAttention\n+    r\"\\.q\\.\": \".q_proj.\",\n+    r\"\\.k\\.\": \".k_proj.\",\n+    r\"\\.v\\.\": \".v_proj.\",\n+    r\"\\.proj_out\\.\": \".out_proj.\",\n+    # move the attention norms outside of attention modules\n+    r\"mid\\.attn_1\\.norm\\.\": \"mid.attn_norm.\",\n+    r\"attn\\.0\\.norm\\.\": \"attn_norms.0.\",\n+    r\"attn\\.1\\.norm\\.\": \"attn_norms.1.\",\n+    r\"attn\\.2\\.norm\\.\": \"attn_norms.2.\",\n+    r\"attn\\.3\\.norm\\.\": \"attn_norms.3.\",\n+    # isolate down/mid/up into separate classes for readability\n+    r\"\\.down\\.\": \".down_block.down.\",\n+    r\"\\.up\\.\": \".up_block.up.\",\n+    r\"\\.mid\\.\": \".middle_block.\",\n+}\n+\n+\n+def convert_state_dict_to_hf(old_state_dict, new_state_dict):\n+    for key, value in old_state_dict.items():\n+        # convert conv layers in attn to linear\n+        if (\n+            any(key.endswith(name) for name in [\"q.weight\", \"k.weight\", \"v.weight\", \"proj_out.weight\"])\n+            and value.ndim == 4\n+        ):\n+            value = value.squeeze()\n+\n+        for old_pattern, new_pattern in KEYS_TO_MODIFY_MAPPING.items():\n+            key = re.sub(old_pattern, new_pattern, key)\n+\n+        new_state_dict[key] = value\n+    return new_state_dict\n+\n+\n+def convert_model(vq_model_id, llm_model_id, output_dir, hub_model_id=None, test_inference=False):\n+    os.makedirs(output_dir, exist_ok=True)\n+\n+    # Convert and save processor\n+    tokenizer_tiktoken = AutoTokenizer.from_pretrained(llm_model_id, trust_remote_code=True)\n+    convert_tiktoken(tokenizer_tiktoken, output_dir)\n+    extra_special_tokens = extra_special_tokens = {\n+        \"image_token\": \"<image>\",\n+        \"boi_token\": \"<|image start|>\",\n+        \"eoi_token\": \"<|image end|>\",\n+        \"image_wrapper_token\": \"<|image token|>\",\n+        \"eof_token\": \"<|extra_201|>\",\n+    }\n+    tokenizer_converted = AutoTokenizer.from_pretrained(output_dir, extra_special_tokens=extra_special_tokens)\n+    tokenizer_converted.padding_side = \"left\"\n+\n+    image_processor = Emu3ImageProcessor.from_pretrained(vq_model_id)\n+    processor = Emu3Processor(image_processor, tokenizer_converted, chat_template=CHAT_TEMPLATE)\n+    processor.save_pretrained(output_dir)\n+\n+    # load models\n+    model_llm = AutoModelForCausalLM.from_pretrained(\n+        llm_model_id,\n+        trust_remote_code=True,\n+    )\n+    model_vqgan = AutoModel.from_pretrained(vq_model_id, trust_remote_code=True)\n+    with open(f\"{output_dir}/tokenizer.json\", \"r\") as file:\n+        tokenizer_config = json.load(file)\n+    vocabulary_map = tokenizer_config[\"model\"][\"vocab\"]\n+\n+    text_config = Emu3TextConfig(\n+        max_position_embeddings=model_llm.config.max_position_embeddings,\n+        rope_scaling={\"rope_type\": \"default\"},\n+    )\n+    config = Emu3Config(text_config=text_config, vocabulary_map=vocabulary_map)\n+\n+    with init_empty_weights():\n+        model = Emu3ForConditionalGeneration(config=config)\n+        model.generation_config = GenerationConfig(\n+            do_sample=True,\n+            top_k=2048,\n+            max_new_tokens=50_000,\n+            pad_token_id=processor.tokenizer.pad_token_id,\n+            eos_token_id=processor.tokenizer.eos_token_id,\n+        )\n+\n+    state_dict = {}\n+    state_dict = convert_state_dict_to_hf(model_llm.state_dict(), state_dict)\n+    state_dict = convert_state_dict_to_hf(model_vqgan.state_dict(), state_dict)\n+\n+    model.load_state_dict(state_dict, assign=True, strict=True)\n+    model.save_pretrained(output_dir, safe_serialization=True)\n+\n+    if hub_model_id is not None:\n+        model.push_to_hub(hub_model_id)\n+        processor.push_to_hub(hub_model_id)\n+\n+    if test_inference and llm_model_id.endswith(\"Chat\"):\n+        # Short inference on a few examples to check if generation makes sense\n+        print(\"Loading the checkpoint in a Emu3 model...\")\n+        print(\"*\" * 100)\n+        model = Emu3ForConditionalGeneration.from_pretrained(output_dir, torch_dtype=torch.bfloat16, device_map=\"auto\")\n+        processor = Emu3Processor.from_pretrained(output_dir)\n+\n+        conversation = [\n+            {\n+                \"role\": \"system\",\n+                \"content\": [\n+                    {\"type\": \"text\", \"text\": \"You are a helpful assistant.\"},\n+                ],\n+            },\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\"type\": \"text\", \"text\": \"Please tell me about this art work and its artist.\"},\n+                    {\"type\": \"image\"},\n+                ],\n+            },\n+        ]\n+        prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n+\n+        image = Image.open(\n+            requests.get(\n+                \"https://uploads4.wikiart.org/images/paul-klee/death-for-the-idea-1915.jpg!Large.jpg\", stream=True\n+            ).raw\n+        )\n+        inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(model.device, torch.bfloat16)\n+        length = inputs.input_ids.shape[1]\n+\n+        out = model.generate(**inputs, max_new_tokens=40, do_sample=False)\n+        generated_text = processor.batch_decode(out[:, length:], skip_special_tokens=True)[0]\n+\n+        print(f\"Generation for single-image: {generated_text}\")\n+        print(\"*\" * 100)\n+    elif test_inference and llm_model_id.endswith(\"Gen\"):\n+        processor = Emu3Processor.from_pretrained(output_dir)\n+        model = Emu3ForConditionalGeneration.from_pretrained(output_dir, torch_dtype=torch.bfloat16, device_map=\"auto\")\n+\n+        inputs = processor(\n+            text=[\n+                \"a portrait of young girl. masterpiece, film grained, best quality.\",\n+                \"a dog running under the rain\",\n+            ],\n+            padding=True,\n+            return_tensors=\"pt\",\n+            return_for_image_generation=True,\n+        )\n+        inputs = inputs.to(device=\"cuda:0\", dtype=torch.bfloat16)\n+\n+        neg_prompt = \"lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry.\"\n+        neg_inputs = processor(text=[neg_prompt] * 2, return_tensors=\"pt\").to(device=\"cuda:0\")\n+\n+        image_sizes = inputs.pop(\"image_sizes\")\n+        HEIGHT, WIDTH = image_sizes[0]\n+        VISUAL_TOKENS = model.vocabulary_mapping.image_tokens\n+\n+        def prefix_allowed_tokens_fn(batch_id, input_ids):\n+            height, width = HEIGHT, WIDTH\n+            visual_tokens = VISUAL_TOKENS\n+            image_token_id = processor.tokenizer.encode(\"<|image token|>\", return_tensors=\"pt\")[0].to(model.device)\n+            eoi_token_id = processor.tokenizer.encode(\"<|image end|>\", return_tensors=\"pt\")[0]\n+            eos_token_id = processor.tokenizer.encode(\"<|extra_204|>\", return_tensors=\"pt\")[0]\n+            pad_token_id = processor.tokenizer.encode(\"<|endoftext|>\", return_tensors=\"pt\")[0]\n+            eol_token_id = processor.tokenizer.encode(\"<|extra_200|>\", return_tensors=\"pt\")[0]\n+            eof_token_id = processor.tokenizer.encode(\"<|extra_201|>\", return_tensors=\"pt\")[0]\n+\n+            position = torch.nonzero(input_ids == image_token_id, as_tuple=True)[0][0]\n+            offset = input_ids.shape[0] - position\n+            if offset % (width + 1) == 0:\n+                return (eol_token_id,)\n+            elif offset == (width + 1) * height + 1:\n+                return (eof_token_id,)\n+            elif offset == (width + 1) * height + 2:\n+                return (eoi_token_id,)\n+            elif offset == (width + 1) * height + 3:\n+                return (eos_token_id,)\n+            elif offset > (width + 1) * height + 3:\n+                return (pad_token_id,)\n+            else:\n+                return visual_tokens\n+\n+        out = model.generate(\n+            **inputs,\n+            prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n+            negative_prompt_ids=neg_inputs.input_ids,\n+            negative_prompt_attention_mask=neg_inputs.attention_mask,\n+        )\n+\n+        image = model.decode_image_tokens(out[:, inputs.input_ids.shape[1] :], height=HEIGHT, width=WIDTH)\n+        images = processor.postprocess(\n+            list(image.float()), return_tensors=\"PIL.Image.Image\"\n+        )  # internally we convert to np but it's not supported in bf16 precision\n+        for i, image in enumerate(images[\"pixel_values\"]):\n+            image.save(f\"result_{i}.png\")\n+\n+\n+def main():\n+    parser = argparse.ArgumentParser()\n+    parser.add_argument(\n+        \"--vq_model_id\",\n+        help=\"Model ID of Emu3 VQ-VAE on the hub\",\n+        default=\"BAAI/Emu3-VisionTokenizer\",\n+    )\n+    parser.add_argument(\n+        \"--llm_model_id\",\n+        help=\"Model ID of Emu3 bacbone LLM on the hub\",\n+        default=\"BAAI/Emu3-Chat\",\n+    )\n+    parser.add_argument(\n+        \"--output_dir\",\n+        help=\"Location to write HF model\",\n+    )\n+    parser.add_argument(\n+        \"--hub_model_id\",\n+        help=\"Model ID in the hub where to push the model.\",\n+    )\n+    parser.add_argument(\n+        \"--test_inference\",\n+        action=\"store_true\",\n+        help=\"Whether to load the model for generation to test it's converted correctly.\",\n+    )\n+    args = parser.parse_args()\n+    convert_model(\n+        vq_model_id=args.vq_model_id,\n+        llm_model_id=args.llm_model_id,\n+        output_dir=args.output_dir,\n+        hub_model_id=args.hub_model_id,\n+        test_inference=args.test_inference,\n+    )\n+\n+\n+if __name__ == \"__main__\":\n+    main()"
        },
        {
            "sha": "f28bc501ba169c640251fc29991e2e523e98c574",
            "filename": "src/transformers/models/emu3/image_processing_emu3.py",
            "status": "added",
            "additions": 552,
            "deletions": 0,
            "changes": 552,
            "blob_url": "https://github.com/huggingface/transformers/blob/52e1f87c7d2fdaf147aa625c169ad440672cb3cb/src%2Ftransformers%2Fmodels%2Femu3%2Fimage_processing_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52e1f87c7d2fdaf147aa625c169ad440672cb3cb/src%2Ftransformers%2Fmodels%2Femu3%2Fimage_processing_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fimage_processing_emu3.py?ref=52e1f87c7d2fdaf147aa625c169ad440672cb3cb",
            "patch": "@@ -0,0 +1,552 @@\n+# coding=utf-8\n+# Copyright 2024 HuggingFace Inc. team. All rights reserved.\n+#\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import math\n+from typing import Dict, Iterable, List, Optional, Union\n+\n+import numpy as np\n+\n+from ...image_processing_utils import BaseImageProcessor, BatchFeature\n+from ...image_transforms import convert_to_rgb, pad, resize, to_channel_dimension_format\n+from ...image_utils import (\n+    OPENAI_CLIP_MEAN,\n+    OPENAI_CLIP_STD,\n+    ChannelDimension,\n+    ImageInput,\n+    PILImageResampling,\n+    VideoInput,\n+    get_image_size,\n+    infer_channel_dimension_format,\n+    is_scaled_image,\n+    is_valid_image,\n+    make_list_of_images,\n+    to_numpy_array,\n+    valid_images,\n+    validate_preprocess_arguments,\n+)\n+from ...utils import TensorType, is_vision_available, logging\n+\n+\n+if is_vision_available():\n+    from PIL import Image\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+def make_batched_images(images) -> List[List[ImageInput]]:\n+    \"\"\"\n+    Accepts images in list or nested list format, and makes a list of images for preprocessing.\n+\n+    Args:\n+        images (`Union[List[List[ImageInput]], List[ImageInput], ImageInput]`):\n+            The input image.\n+\n+    Returns:\n+        list: A list of images.\n+    \"\"\"\n+    if isinstance(images, (list, tuple)) and isinstance(images[0], (list, tuple)) and is_valid_image(images[0][0]):\n+        return [img for img_list in images for img in img_list]\n+\n+    elif isinstance(images, (list, tuple)) and is_valid_image(images[0]):\n+        return images\n+\n+    elif is_valid_image(images):\n+        return [images]\n+\n+    raise ValueError(f\"Could not make batched images from {images}\")\n+\n+\n+def smart_resize(\n+    height: int, width: int, factor: int = 28, min_pixels: int = 56 * 56, max_pixels: int = 14 * 14 * 4 * 1280\n+):\n+    \"\"\"Rescales the image so that the following conditions are met:\n+\n+    1. Both dimensions (height and width) are divisible by 'factor'.\n+\n+    2. The total number of pixels is within the range ['min_pixels', 'max_pixels'].\n+\n+    3. The aspect ratio of the image is maintained as closely as possible.\n+\n+    \"\"\"\n+    if height < factor or width < factor:\n+        raise ValueError(f\"height:{height} or width:{width} must be larger than factor:{factor}\")\n+    elif max(height, width) / min(height, width) > 200:\n+        raise ValueError(\n+            f\"absolute aspect ratio must be smaller than 200, got {max(height, width) / min(height, width)}\"\n+        )\n+    h_bar = round(height / factor) * factor\n+    w_bar = round(width / factor) * factor\n+    if h_bar * w_bar > max_pixels:\n+        beta = math.sqrt((height * width) / max_pixels)\n+        h_bar = math.floor(height / beta / factor) * factor\n+        w_bar = math.floor(width / beta / factor) * factor\n+    elif h_bar * w_bar < min_pixels:\n+        beta = math.sqrt(min_pixels / (height * width))\n+        h_bar = math.ceil(height * beta / factor) * factor\n+        w_bar = math.ceil(width * beta / factor) * factor\n+    return h_bar, w_bar\n+\n+\n+class Emu3ImageProcessor(BaseImageProcessor):\n+    r\"\"\"\n+    Constructs a Emu3 image processor that dynamically resizes images based on the original images.\n+\n+    Args:\n+        do_resize (`bool`, *optional*, defaults to `True`):\n+            Whether to resize the image's (height, width) dimensions.\n+        resample (`PILImageResampling`, *optional*, defaults to `Resampling.BICUBIC`):\n+            Resampling filter to use when resizing the image.\n+        do_rescale (`bool`, *optional*, defaults to `True`):\n+            Whether to rescale the image by the specified scale `rescale_factor`.\n+        rescale_factor (`int` or `float`, *optional*, defaults to `1/255`):\n+            Scale factor to use if rescaling the image.\n+        do_normalize (`bool`, *optional*, defaults to `True`):\n+            Whether to normalize the image.\n+        image_mean (`float` or `List[float]`, *optional*, defaults to `[0.48145466, 0.4578275, 0.40821073]`):\n+            Mean to use if normalizing the image. This is a float or list of floats for each channel in the image.\n+        image_std (`float` or `List[float]`, *optional*, defaults to `[0.26862954, 0.26130258, 0.27577711]`):\n+            Standard deviation to use if normalizing the image. This is a float or list of floats for each channel in the image.\n+        do_convert_rgb (`bool`, *optional*, defaults to `True`):\n+            Whether to convert the image to RGB.\n+        do_pad (`bool`, *optional*, defaults to `True`):\n+                Whether to pad the image. If `True`, will pad the patch dimension of the images in the batch to the largest\n+                number of patches in the batch. Padding will be applied to the bottom and right with zeros.\n+        min_pixels (`int`, *optional*, defaults to `512 * 512`):\n+            The min pixels of the image to resize the image.\n+        max_pixels (`int`, *optional*, defaults to `1024 * 1024`):\n+            The max pixels of the image to resize the image.\n+        spatial_factor (`int`, *optional*, defaults to 8):\n+            The spatial downsample factor the image will be downsampled in feature extracting phase\n+    \"\"\"\n+\n+    model_input_names = [\"pixel_values\"]\n+\n+    def __init__(\n+        self,\n+        do_resize: bool = True,\n+        resample: PILImageResampling = PILImageResampling.BICUBIC,\n+        do_rescale: bool = True,\n+        rescale_factor: Union[int, float] = 1 / 255,\n+        do_normalize: bool = True,\n+        image_mean: Optional[Union[float, List[float]]] = None,\n+        image_std: Optional[Union[float, List[float]]] = None,\n+        do_convert_rgb: bool = True,\n+        do_pad: bool = True,\n+        min_pixels: int = 512 * 512,\n+        max_pixels: int = 1024 * 1024,\n+        spatial_factor: int = 8,\n+        **kwargs,\n+    ) -> None:\n+        super().__init__(**kwargs)\n+        self.do_resize = do_resize\n+        self.resample = resample\n+        self.do_rescale = do_rescale\n+        self.rescale_factor = rescale_factor\n+        self.do_normalize = do_normalize\n+        self.image_mean = image_mean if image_mean is not None else OPENAI_CLIP_MEAN\n+        self.image_std = image_std if image_std is not None else OPENAI_CLIP_STD\n+        self.min_pixels = min_pixels\n+        self.max_pixels = max_pixels\n+        self.spatial_factor = spatial_factor\n+        self.size = {\"min_pixels\": min_pixels, \"max_pixels\": max_pixels}\n+        self.do_convert_rgb = do_convert_rgb\n+\n+    def _preprocess(\n+        self,\n+        images: Union[ImageInput, VideoInput],\n+        do_resize: bool = None,\n+        resample: PILImageResampling = None,\n+        do_rescale: bool = None,\n+        rescale_factor: float = None,\n+        do_normalize: bool = None,\n+        image_mean: Optional[Union[float, List[float]]] = None,\n+        image_std: Optional[Union[float, List[float]]] = None,\n+        do_convert_rgb: bool = None,\n+        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    ):\n+        \"\"\"\n+        Preprocess an image or batch of images.\n+\n+        Args:\n+            images (`ImageInput`):\n+                Image or batch of images to preprocess. Expects pixel values ranging from 0 to 255. If pixel values range from 0 to 1, set `do_rescale=False`.\n+            vision_info (`List[Dict]`, *optional*):\n+                Optional list of dictionaries containing additional information about vision inputs.\n+            do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n+                Whether to resize the image.\n+            resample (`PILImageResampling`, *optional*, defaults to `self.resample`):\n+                Resampling filter to use if resizing the image. This can be one of the `PILImageResampling` enums.\n+            do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):\n+                Whether to rescale the image.\n+            rescale_factor (`float`, *optional*, defaults to `self.rescale_factor`):\n+                Scale factor to use if rescaling the image.\n+            do_normalize (`bool`, *optional*, defaults to `self.do_normalize`):\n+                Whether to normalize the image.\n+            image_mean (`float` or `List[float]`, *optional*, defaults to `self.image_mean`):\n+                Mean to use if normalizing the image. Can be a float or a list of floats corresponding to the number of channels in the image.\n+            image_std (`float` or `List[float]`, *optional*, defaults to `self.image_std`):\n+                Standard deviation to use if normalizing the image. Can be a float or a list of floats corresponding to the number of channels in the image.\n+            do_convert_rgb (`bool`, *optional*, defaults to `self.do_convert_rgb`):\n+                Whether to convert the image to RGB.\n+            data_format (`ChannelDimension`, *optional*, defaults to `ChannelDimension.FIRST`):\n+                The channel dimension format for the output image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - Unset: Use the channel dimension format of the input image.\n+            input_data_format (`ChannelDimension` or `str`, *optional*):\n+                The channel dimension format for the input image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.   - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n+        \"\"\"\n+        images = make_list_of_images(images)\n+\n+        if do_convert_rgb:\n+            images = [convert_to_rgb(image) for image in images]\n+\n+        # All transformations expect numpy arrays.\n+        images = [to_numpy_array(image) for image in images]\n+\n+        if is_scaled_image(images[0]) and do_rescale:\n+            logger.warning_once(\n+                \"It looks like you are trying to rescale already rescaled images. If the input\"\n+                \" images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\"\n+            )\n+        if input_data_format is None:\n+            # We assume that all images have the same channel dimension format.\n+            input_data_format = infer_channel_dimension_format(images[0])\n+\n+        height, width = get_image_size(images[0], channel_dim=input_data_format)\n+        resized_height, resized_width = height, width\n+        processed_images = []\n+        for image in images:\n+            if do_resize:\n+                resized_height, resized_width = smart_resize(\n+                    height,\n+                    width,\n+                    factor=self.spatial_factor,\n+                    min_pixels=self.min_pixels,\n+                    max_pixels=self.max_pixels,\n+                )\n+                image = resize(\n+                    image, size=(resized_height, resized_width), resample=resample, input_data_format=input_data_format\n+                )\n+\n+            if do_rescale:\n+                image = self.rescale(image, scale=rescale_factor, input_data_format=input_data_format)\n+\n+            if do_normalize:\n+                image = self.normalize(\n+                    image=image, mean=image_mean, std=image_std, input_data_format=input_data_format\n+                )\n+\n+            image = to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format)\n+            processed_images.append(image)\n+\n+        images = np.array(processed_images)\n+        return images\n+\n+    def _pad_for_batching(\n+        self,\n+        pixel_values: List[np.ndarray],\n+        image_sizes: List[List[int]],\n+        data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    ):\n+        \"\"\"\n+        Pads images on the `num_of_patches` dimension with zeros to form a batch of same number of patches.\n+\n+        Args:\n+            pixel_values (`List[np.ndarray]`):\n+                An array of pixel values of each images of shape (`batch_size`, `num_patches`, `image_in_3D`)\n+            image_sizes (`List[List[int]]`):\n+                A list of sizes for each image in `pixel_values` in (height, width) format.\n+            data_format (`str` or `ChannelDimension`, *optional*):\n+                The channel dimension format for the output image. Can be one of:\n+                    - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                    - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                If unset, will use same as the input image.\n+            input_data_format (`str` or `ChannelDimension`, *optional*):\n+                The channel dimension format for the input image. Can be one of:\n+                    - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                    - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                If unset, will use the inferred format of the input image.\n+\n+        Returns:\n+            List[`np.ndarray`]: The padded images.\n+        \"\"\"\n+\n+        max_shape = (\n+            max([size[0] for size in image_sizes]),\n+            max([size[1] for size in image_sizes]),\n+        )\n+        pixel_values = [\n+            pad(\n+                image,\n+                padding=((0, max_shape[0] - size[0]), (0, max_shape[1] - size[1])),\n+                data_format=data_format,\n+                input_data_format=input_data_format,\n+            )\n+            for image, size in zip(pixel_values, image_sizes)\n+        ]\n+        return pixel_values\n+\n+    def preprocess(\n+        self,\n+        images: ImageInput,\n+        do_resize: bool = None,\n+        size: Dict[str, int] = None,\n+        resample: PILImageResampling = None,\n+        do_rescale: bool = None,\n+        rescale_factor: float = None,\n+        do_normalize: bool = None,\n+        image_mean: Optional[Union[float, List[float]]] = None,\n+        image_std: Optional[Union[float, List[float]]] = None,\n+        do_convert_rgb: bool = None,\n+        do_pad: bool = True,\n+        return_tensors: Optional[Union[str, TensorType]] = None,\n+        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    ):\n+        \"\"\"\n+        Args:\n+            images (`ImageInput`):\n+                Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If\n+                passing in images with pixel values between 0 and 1, set `do_rescale=False`.\n+            do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n+                Whether to resize the image.\n+            size (`Dict[str, int]`, *optional*, defaults to `self.size`):\n+                Size of the image after resizing. Shortest edge of the image is resized to size[\"shortest_edge\"], with\n+                the longest edge resized to keep the input aspect ratio.\n+            resample (`int`, *optional*, defaults to `self.resample`):\n+                Resampling filter to use if resizing the image. This can be one of the enum `PILImageResampling`. Only\n+                has an effect if `do_resize` is set to `True`.\n+            do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):\n+                Whether to rescale the image.\n+            rescale_factor (`float`, *optional*, defaults to `self.rescale_factor`):\n+                Rescale factor to rescale the image by if `do_rescale` is set to `True`.\n+            do_normalize (`bool`, *optional*, defaults to `self.do_normalize`):\n+                Whether to normalize the image.\n+            image_mean (`float` or `List[float]`, *optional*, defaults to `self.image_mean`):\n+                Image mean to use for normalization. Only has an effect if `do_normalize` is set to `True`.\n+            image_std (`float` or `List[float]`, *optional*, defaults to `self.image_std`):\n+                Image standard deviation to use for normalization. Only has an effect if `do_normalize` is set to\n+                `True`.\n+            do_convert_rgb (`bool`, *optional*, defaults to `self.do_convert_rgb`):\n+                Whether to convert the image to RGB.\n+            do_pad (`bool`, *optional*, defaults to `True`):\n+                Whether to pad the image. If `True`, will pad the patch dimension of the images in the batch to the largest\n+                number of patches in the batch. Padding will be applied to the bottom and right with zeros.\n+            return_tensors (`str` or `TensorType`, *optional*):\n+                The type of tensors to return. Can be one of:\n+                - Unset: Return a list of `np.ndarray`.\n+                - `TensorType.TENSORFLOW` or `'tf'`: Return a batch of type `tf.Tensor`.\n+                - `TensorType.PYTORCH` or `'pt'`: Return a batch of type `torch.Tensor`.\n+                - `TensorType.NUMPY` or `'np'`: Return a batch of type `np.ndarray`.\n+                - `TensorType.JAX` or `'jax'`: Return a batch of type `jax.numpy.ndarray`.\n+            data_format (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`):\n+                The channel dimension format for the output image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - Unset: Use the channel dimension format of the input image.\n+            input_data_format (`ChannelDimension` or `str`, *optional*):\n+                The channel dimension format for the input image. If unset, the channel dimension format is inferred\n+                from the input image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n+\n+        \"\"\"\n+        do_resize = do_resize if do_resize is not None else self.do_resize\n+        size = size if size is not None else self.size\n+        resample = resample if resample is not None else self.resample\n+        do_rescale = do_rescale if do_rescale is not None else self.do_rescale\n+        rescale_factor = rescale_factor if rescale_factor is not None else self.rescale_factor\n+        do_normalize = do_normalize if do_normalize is not None else self.do_normalize\n+        image_mean = image_mean if image_mean is not None else self.image_mean\n+        image_std = image_std if image_std is not None else self.image_std\n+        do_convert_rgb = do_convert_rgb if do_convert_rgb is not None else self.do_convert_rgb\n+        do_pad = do_pad if do_pad is not None else self.do_pad\n+\n+        if images is not None:\n+            images = make_batched_images(images)\n+\n+        if images is not None and not valid_images(images):\n+            raise ValueError(\n+                \"Invalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \"\n+                \"torch.Tensor, tf.Tensor or jax.ndarray.\"\n+            )\n+\n+        validate_preprocess_arguments(\n+            rescale_factor=rescale_factor,\n+            do_normalize=do_normalize,\n+            image_mean=image_mean,\n+            image_std=image_std,\n+            do_resize=do_resize,\n+            size=size,\n+            resample=resample,\n+        )\n+\n+        pixel_values = []\n+        for image in images:\n+            image = self._preprocess(\n+                image,\n+                do_resize=do_resize,\n+                resample=resample,\n+                do_rescale=do_rescale,\n+                rescale_factor=rescale_factor,\n+                do_normalize=do_normalize,\n+                image_mean=image_mean,\n+                image_std=image_std,\n+                data_format=data_format,\n+                do_convert_rgb=do_convert_rgb,\n+                input_data_format=input_data_format,\n+            )\n+            pixel_values.extend(image)\n+\n+        image_sizes = [image.shape[-2:] for image in pixel_values]\n+        if do_pad:\n+            pixel_values = self._pad_for_batching(pixel_values, image_sizes)\n+            pixel_values = np.array(pixel_values)\n+\n+        return BatchFeature(\n+            data={\"pixel_values\": pixel_values, \"image_sizes\": image_sizes}, tensor_type=return_tensors\n+        )\n+\n+    def postprocess(\n+        self,\n+        images: ImageInput,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        do_normalize: Optional[bool] = None,\n+        image_mean: Optional[Union[float, List[float]]] = None,\n+        image_std: Optional[Union[float, List[float]]] = None,\n+        return_tensors: Union[str, TensorType] = \"PIL.Image.Image\",\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    ):\n+        \"\"\"\n+        Postprocess an image or batch of images tensor. Postprocess is the reverse process of preprocess.\n+        The parameters should be same as in preprocess.\n+        Args:\n+            images (`ImageInput`):\n+                Image to postprocess. Expects a single or batch of images with pixel values ranging from -1 to 1.\n+            do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):\n+                Whether to rescale the image.\n+            rescale_factor (`float`, *optional*, defaults to `self.rescale_factor`):\n+                Rescale factor to rescale the image by if `do_rescale` is set to `True`.\n+            do_normalize (`bool`, *optional*, defaults to `self.do_normalize`):\n+                Whether to normalize the image.\n+            image_mean (`float` or `List[float]`, *optional*, defaults to `self.image_mean`):\n+                Image mean to use for normalization. Only has an effect if `do_normalize` is set to `True`.\n+            image_std (`float` or `List[float]`, *optional*, defaults to `self.image_std`):\n+                Image standard deviation to use for normalization. Only has an effect if `do_normalize` is set to `True`.\n+            return_tensors (`str` or `TensorType`, *optional*):\n+                The type of tensors to return. Can be one of:\n+                - Unset: Return a list of `np.ndarray`.\n+                - `TensorType.PYTORCH` or `'pt'`: Return a batch of type `torch.Tensor`.\n+                - `TensorType.NUMPY` or `'np'`: Return a batch of type `np.ndarray`.\n+            input_data_format (`ChannelDimension` or `str`, *optional*):\n+                The channel dimension format for the input image. If unset, the channel dimension format is inferred\n+                from the input image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n+        \"\"\"\n+        do_rescale = do_rescale if do_rescale is not None else self.do_rescale\n+        rescale_factor = 1.0 / self.rescale_factor if rescale_factor is None else rescale_factor\n+        do_normalize = do_normalize if do_normalize is not None else self.do_normalize\n+        image_mean = image_mean if image_mean is not None else self.image_mean\n+        image_std = image_std if image_std is not None else self.image_std\n+\n+        images = make_list_of_images(images)\n+        if isinstance(images[0], Image.Image):\n+            return images if len(images) > 1 else images[0]\n+\n+        if input_data_format is None:\n+            # We assume that all images have the same channel dimension format.\n+            input_data_format = infer_channel_dimension_format(images[0])\n+\n+        pixel_values = []\n+        for image in images:\n+            image = to_numpy_array(image)\n+            if do_normalize:\n+                image = self.unnormalize(\n+                    image=image, image_mean=image_mean, image_std=image_std, input_data_format=input_data_format\n+                )\n+\n+            if do_rescale:\n+                image = self.rescale(image, scale=rescale_factor, input_data_format=input_data_format)\n+                image = image.clip(0, 255).astype(np.uint8)\n+\n+            if do_normalize and do_rescale and return_tensors == \"PIL.Image.Image\":\n+                image = to_channel_dimension_format(image, ChannelDimension.LAST, input_channel_dim=input_data_format)\n+                pixel_values.append(Image.fromarray(image))\n+            else:\n+                pixel_values.extend(image)\n+\n+        data = {\"pixel_values\": pixel_values}\n+        return_tensors = return_tensors if return_tensors != \"PIL.Image.Image\" else None\n+\n+        return BatchFeature(data=data, tensor_type=return_tensors)\n+\n+    def unnormalize(\n+        self,\n+        image: np.array,\n+        image_mean: Union[float, Iterable[float]],\n+        image_std: Union[float, Iterable[float]],\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    ) -> np.array:\n+        \"\"\"\n+        Unnormalizes `image` using the mean and standard deviation specified by `mean` and `std`.\n+        image = (image * image_std) + image_mean\n+        Args:\n+            image (`torch.Tensor` of shape `(batch_size, num_channels, image_size, image_size)` or `(num_channels, image_size, image_size)`):\n+                Batch of pixel values to postprocess.\n+            image_mean (`float` or `Iterable[float]`):\n+                The mean to use for unnormalization.\n+            image_std (`float` or `Iterable[float]`):\n+                The standard deviation to use for unnormalization.\n+            input_data_format (`ChannelDimension` or `str`, *optional*):\n+                The channel dimension format for the input image. If unset, the channel dimension format is inferred\n+                from the input image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n+        \"\"\"\n+        num_channels = 3\n+\n+        if isinstance(image_mean, Iterable):\n+            if len(image_mean) != num_channels:\n+                raise ValueError(f\"mean must have {num_channels} elements if it is an iterable, got {len(image_mean)}\")\n+        else:\n+            image_mean = [image_mean] * num_channels\n+\n+        if isinstance(image_std, Iterable):\n+            if len(image_std) != num_channels:\n+                raise ValueError(f\"std must have {num_channels} elements if it is an iterable, got {len(image_std)}\")\n+        else:\n+            image_std = [image_std] * num_channels\n+\n+        rev_image_mean = tuple(-mean / std for mean, std in zip(image_mean, image_std))\n+        rev_image_std = tuple(1 / std for std in image_std)\n+        image = self.normalize(\n+            image=image, mean=rev_image_mean, std=rev_image_std, input_data_format=input_data_format\n+        )\n+        return image\n+\n+\n+__all__ = [\"Emu3ImageProcessor\"]"
        },
        {
            "sha": "1ee883aa406d648369eb881da1889d03b2691bcd",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "added",
            "additions": 1949,
            "deletions": 0,
            "changes": 1949,
            "blob_url": "https://github.com/huggingface/transformers/blob/52e1f87c7d2fdaf147aa625c169ad440672cb3cb/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52e1f87c7d2fdaf147aa625c169ad440672cb3cb/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=52e1f87c7d2fdaf147aa625c169ad440672cb3cb",
            "patch": "@@ -0,0 +1,1949 @@\n+#                🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨\n+#           This file was automatically generated from src/transformers/models/emu3/modular_emu3.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_emu3.py file directly. One of our CI enforces this.\n+#                🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨\n+# coding=utf-8\n+# Copyright 2024 HuggingFace Inc. team. All rights reserved.\n+#\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import math\n+from functools import cached_property\n+from typing import Callable, List, Optional, Tuple, Union\n+\n+import torch\n+import torch.nn as nn\n+import torch.nn.functional as F\n+\n+from ...activations import ACT2FN\n+from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...generation import GenerationMixin\n+from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import (\n+    LossKwargs,\n+    add_start_docstrings,\n+    add_start_docstrings_to_model_forward,\n+    logging,\n+    replace_return_docstrings,\n+)\n+from .configuration_emu3 import Emu3Config, Emu3TextConfig, Emu3VQVAEConfig\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+_CONFIG_FOR_DOC = \"Emu3Config\"\n+\n+\n+class Emu3RMSNorm(nn.Module):\n+    def __init__(self, hidden_size, eps=1e-6):\n+        \"\"\"\n+        Emu3RMSNorm is equivalent to T5LayerNorm\n+        \"\"\"\n+        super().__init__()\n+        self.weight = nn.Parameter(torch.ones(hidden_size))\n+        self.variance_epsilon = eps\n+\n+    def forward(self, hidden_states):\n+        input_dtype = hidden_states.dtype\n+        hidden_states = hidden_states.to(torch.float32)\n+        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n+        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n+        return self.weight * hidden_states.to(input_dtype)\n+\n+    def extra_repr(self):\n+        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n+\n+\n+class Emu3MLP(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.hidden_size = config.hidden_size\n+        self.intermediate_size = config.intermediate_size\n+        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n+        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n+        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=config.mlp_bias)\n+        self.act_fn = ACT2FN[config.hidden_act]\n+\n+    def forward(self, x):\n+        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n+        return down_proj\n+\n+\n+def rotate_half(x):\n+    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n+    x1 = x[..., : x.shape[-1] // 2]\n+    x2 = x[..., x.shape[-1] // 2 :]\n+    return torch.cat((-x2, x1), dim=-1)\n+\n+\n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n+    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n+\n+    Args:\n+        q (`torch.Tensor`): The query tensor.\n+        k (`torch.Tensor`): The key tensor.\n+        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n+        sin (`torch.Tensor`): The sine part of the rotary embedding.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n+        unsqueeze_dim (`int`, *optional*, defaults to 1):\n+            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n+            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n+            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n+            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n+            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n+            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n+    Returns:\n+        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n+    \"\"\"\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n+    q_embed = (q * cos) + (rotate_half(q) * sin)\n+    k_embed = (k * cos) + (rotate_half(k) * sin)\n+    return q_embed, k_embed\n+\n+\n+def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n+    \"\"\"\n+    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n+    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n+    \"\"\"\n+    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n+    if n_rep == 1:\n+        return hidden_states\n+    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n+    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n+\n+\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n+class Emu3Attention(nn.Module):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+\n+    def __init__(self, config: Emu3Config, layer_idx: int):\n+        super().__init__()\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n+        self.scaling = self.head_dim**-0.5\n+        self.attention_dropout = config.attention_dropout\n+        self.is_causal = True\n+\n+        self.q_proj = nn.Linear(\n+            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.k_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.v_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.o_proj = nn.Linear(\n+            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n+        )\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n+        past_key_value: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+\n+        cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        if past_key_value is not None:\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n+            else:\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n+\n+\n+class Emu3DecoderLayer(nn.Module):\n+    def __init__(self, config: Emu3Config, layer_idx: int):\n+        super().__init__()\n+        self.hidden_size = config.hidden_size\n+\n+        self.self_attn = Emu3Attention(config=config, layer_idx=layer_idx)\n+\n+        self.mlp = Emu3MLP(config)\n+        self.input_layernorm = Emu3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.post_attention_layernorm = Emu3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.dropout = nn.Dropout(config.attention_dropout)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_value: Optional[Cache] = None,\n+        output_attentions: Optional[bool] = False,\n+        use_cache: Optional[bool] = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n+        **kwargs,\n+    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+        \"\"\"\n+        Args:\n+            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n+            attention_mask (`torch.FloatTensor`, *optional*):\n+                attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,\n+                query_sequence_length, key_sequence_length)` if default attention is used.\n+            output_attentions (`bool`, *optional*):\n+                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n+                returned tensors for more detail.\n+            use_cache (`bool`, *optional*):\n+                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n+                (see `past_key_values`).\n+            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+                Indices depicting the position of the input sequence tokens in the sequence\n+            kwargs (`dict`, *optional*):\n+                Arbitrary kwargs to be ignored, used for FSDP and other methods that injects code\n+                into the model\n+        \"\"\"\n+        residual = hidden_states\n+\n+        hidden_states = self.input_layernorm(hidden_states)\n+\n+        # Self Attention\n+        hidden_states, self_attn_weights = self.self_attn(\n+            hidden_states=hidden_states,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_value=past_key_value,\n+            output_attentions=output_attentions,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            position_embeddings=position_embeddings,\n+            **kwargs,\n+        )\n+        hidden_states = residual + self.dropout(hidden_states)\n+\n+        # Fully Connected\n+        residual = hidden_states\n+        hidden_states = self.post_attention_layernorm(hidden_states)\n+        hidden_states = self.mlp(hidden_states)\n+        hidden_states = residual + self.dropout(hidden_states)\n+\n+        outputs = (hidden_states,)\n+\n+        if output_attentions:\n+            outputs += (self_attn_weights,)\n+\n+        return outputs\n+\n+\n+class Emu3VQVAEVectorQuantizer(nn.Module):\n+    \"\"\"\n+    A module for vector quantization using learned embedding vectors.\n+\n+    This module implements the quantization process similar to te one described in\n+    the VQ-VAE (Vector Quantized Variational AutoEncoder) paper. It quantizes continuous\n+    input vectors into discrete codebook vectors, which are learned during training.\n+    Current implementation improves over previous ones by avoiding costly matrix multiplications\n+    and allowing for post-hoc remapping of indices.\n+    \"\"\"\n+\n+    def __init__(self, config: Emu3VQVAEConfig):\n+        super().__init__()\n+        self.embedding = nn.Embedding(config.codebook_size, config.embed_dim)\n+        self.embedding.weight.data.uniform_(-1.0 / config.codebook_size, 1.0 / config.codebook_size)\n+\n+    def forward(self, hidden_state: torch.Tensor):\n+        batch_size, temporal, channels, height, width = hidden_state.shape\n+        hidden_state = hidden_state.permute(0, 1, 3, 4, 2).contiguous()\n+        hidden_state_flattened = hidden_state.view(-1, channels)\n+\n+        # distances from z to embeddings e_j (z - e)^2 = z^2 + e^2 - 2 e * z\n+        hidden_state_sum = torch.sum(hidden_state_flattened**2, dim=1, keepdim=True)\n+        embedding_sum = torch.sum(self.embedding.weight**2, dim=1)\n+\n+        # \"bd,dn->bn\",\n+        distances = 2 * torch.matmul(hidden_state_flattened, self.embedding.weight.transpose(0, 1))\n+        distances = hidden_state_sum + embedding_sum - distances\n+\n+        min_encoding_indices = torch.argmin(distances, dim=1)\n+        min_encoding_indices = min_encoding_indices.view(batch_size, temporal, height, width)\n+        return min_encoding_indices\n+\n+\n+class Emu3VQVAEEncoderConvDownsample(nn.Module):\n+    def __init__(self, in_channels):\n+        super().__init__()\n+        self.conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=2, padding=0)\n+\n+    def forward(self, hidden_states):\n+        # no asymmetric padding in torch conv, must do it ourselves\n+        hidden_states = F.pad(hidden_states, pad=(0, 1, 0, 1), mode=\"constant\", value=0)\n+        hidden_states = self.conv(hidden_states)\n+        return hidden_states\n+\n+\n+class Emu3VQVAEEncoderConvUpsample(nn.Module):\n+    def __init__(self, in_channels):\n+        super().__init__()\n+        self.conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1)\n+\n+    def forward(self, hidden_states):\n+        hidden_states = F.interpolate(hidden_states, scale_factor=2.0, mode=\"nearest\")\n+        hidden_states = self.conv(hidden_states)\n+        return hidden_states\n+\n+\n+class Emu3VQVAEConv3d(nn.Module):\n+    def __init__(\n+        self,\n+        in_channel: int,\n+        out_channel: int,\n+        kernel_size: Tuple[int],\n+        stride: Tuple[int],\n+    ):\n+        super().__init__()\n+\n+        padding_sizes = [one_kernel - one_stride for one_kernel, one_stride in zip(kernel_size[1:], stride[1:])]\n+        self.padding = ()\n+        for pad_size in padding_sizes[::-1]:\n+            self.padding += (pad_size // 2 + pad_size % 2, pad_size // 2)\n+        self.padding += (2, 0)\n+\n+        self.conv = nn.Conv3d(\n+            in_channel,\n+            out_channel,\n+            kernel_size,\n+            stride=stride,\n+        )\n+\n+    def forward(self, hidden_states: torch.Tensor):\n+        hidden_states = F.pad(hidden_states, self.padding)\n+        hidden_states = self.conv(hidden_states)\n+        return hidden_states\n+\n+\n+class Emu3VQVAESpatialNorm(nn.Module):\n+    def __init__(\n+        self,\n+        in_channels: int,\n+        out_channels: int,\n+    ):\n+        super().__init__()\n+        self.norm_layer = nn.GroupNorm(\n+            num_channels=out_channels,\n+            num_groups=32,\n+            eps=1e-6,\n+            affine=True,\n+        )\n+\n+        self.conv_y = nn.Conv2d(\n+            in_channels,\n+            out_channels,\n+            kernel_size=1,\n+            stride=1,\n+            padding=0,\n+        )\n+        self.conv_b = nn.Conv2d(\n+            in_channels,\n+            out_channels,\n+            kernel_size=1,\n+            stride=1,\n+            padding=0,\n+        )\n+\n+    def forward(self, hidden_states: torch.Tensor, quant_states: torch.Tensor):\n+        quant_states = F.interpolate(quant_states, size=hidden_states.shape[-2:], mode=\"nearest\")\n+        hidden_states = self.norm_layer(hidden_states)\n+        hidden_states = hidden_states * self.conv_y(quant_states) + self.conv_b(quant_states)\n+        return hidden_states\n+\n+\n+class Emu3VQVAETemporalUpsample(nn.Module):\n+    def __init__(\n+        self,\n+        in_channel: int,\n+        out_channel: int,\n+    ):\n+        super().__init__()\n+        self.conv = Emu3VQVAEConv3d(\n+            in_channel,\n+            out_channel,\n+            kernel_size=(3, 3, 3),\n+            stride=(1, 1, 1),\n+        )\n+\n+    def forward(self, hidden_states: torch.Tensor):\n+        batch_size, channels, temporal, height, width = hidden_states.shape\n+        hidden_states = hidden_states.permute(0, 1, 3, 4, 2).contiguous().view(batch_size, -1, temporal)\n+        hidden_states = F.interpolate(hidden_states, scale_factor=2.0, mode=\"nearest\")\n+        hidden_states = hidden_states.view(batch_size, channels, height, width, -1).permute(0, 1, 4, 2, 3).contiguous()\n+        hidden_states = self.conv(hidden_states)\n+        return hidden_states\n+\n+\n+class Emu3VQVAETemporalDownsample(nn.Module):\n+    def __init__(\n+        self,\n+        in_channel: int,\n+        out_channel: int,\n+    ):\n+        super().__init__()\n+        self.conv = Emu3VQVAEConv3d(\n+            in_channel,\n+            out_channel,\n+            kernel_size=(4, 3, 3),\n+            stride=(2, 1, 1),\n+        )\n+\n+    def forward(self, hidden_states: torch.Tensor):\n+        hidden_states = self.conv(hidden_states)\n+        return hidden_states\n+\n+\n+class Emu3VQVAETemporalResnetBlock(nn.Module):\n+    def __init__(\n+        self,\n+        in_channels,\n+        out_channels=None,\n+    ):\n+        super().__init__()\n+        self.in_channels = in_channels\n+        self.out_channels = in_channels if out_channels is None else out_channels\n+\n+        self.norm1 = nn.BatchNorm3d(in_channels)\n+        self.conv1 = Emu3VQVAEConv3d(\n+            in_channels,\n+            out_channels,\n+            kernel_size=(3, 3, 3),\n+            stride=(1, 1, 1),\n+        )\n+        self.norm2 = nn.BatchNorm3d(out_channels)\n+        self.conv2 = Emu3VQVAEConv3d(\n+            out_channels,\n+            out_channels,\n+            kernel_size=(3, 3, 3),\n+            stride=(1, 1, 1),\n+        )\n+        if self.in_channels != self.out_channels:\n+            self.nin_shortcut = nn.Conv3d(\n+                in_channels,\n+                out_channels,\n+                kernel_size=1,\n+                stride=1,\n+                padding=0,\n+            )\n+\n+    def forward(self, hidden_states):\n+        residual = hidden_states\n+        hidden_states = self.norm1(hidden_states)\n+        hidden_states *= torch.sigmoid(hidden_states)\n+        hidden_states = self.conv1(hidden_states)\n+\n+        hidden_states = self.norm2(hidden_states)\n+        hidden_states *= torch.sigmoid(hidden_states)\n+        hidden_states = self.conv2(hidden_states)\n+\n+        if self.in_channels != self.out_channels:\n+            residual = self.nin_shortcut(residual)\n+\n+        return residual + hidden_states\n+\n+\n+class Emu3VQVAEResnetBlock(nn.Module):\n+    def __init__(\n+        self,\n+        in_channels: int,\n+        out_channels: Optional[int] = None,\n+        quant_channels: Optional[int] = None,\n+    ):\n+        super().__init__()\n+        self.in_channels = in_channels\n+        out_channels = in_channels if out_channels is None else out_channels\n+        self.out_channels = out_channels\n+        self.quant_channels = quant_channels\n+\n+        if quant_channels is None:\n+            self.norm1 = nn.GroupNorm(num_channels=in_channels, num_groups=32, eps=1e-6, affine=True)\n+            self.norm2 = nn.GroupNorm(num_channels=out_channels, num_groups=32, eps=1e-6, affine=True)\n+        else:\n+            self.norm1 = Emu3VQVAESpatialNorm(quant_channels, in_channels)\n+            self.norm2 = Emu3VQVAESpatialNorm(quant_channels, out_channels)\n+\n+        self.conv1 = nn.Conv2d(\n+            in_channels,\n+            out_channels,\n+            kernel_size=3,\n+            stride=1,\n+            padding=1,\n+        )\n+\n+        self.conv2 = nn.Conv2d(\n+            out_channels,\n+            out_channels,\n+            kernel_size=3,\n+            stride=1,\n+            padding=1,\n+        )\n+\n+        if self.in_channels != self.out_channels:\n+            self.nin_shortcut = nn.Conv2d(\n+                in_channels,\n+                out_channels,\n+                kernel_size=1,\n+                stride=1,\n+                padding=0,\n+            )\n+\n+    def forward(self, hidden_states: torch.Tensor, quant_channels: Optional[torch.Tensor] = None):\n+        norm_args = () if self.quant_channels is None else (quant_channels,)\n+\n+        residual = hidden_states\n+        hidden_states = self.norm1(hidden_states, *norm_args)\n+        hidden_states *= torch.sigmoid(hidden_states)\n+        hidden_states = self.conv1(hidden_states)\n+\n+        hidden_states = self.norm2(hidden_states, *norm_args)\n+        hidden_states *= torch.sigmoid(hidden_states)\n+        hidden_states = self.conv2(hidden_states)\n+\n+        if self.in_channels != self.out_channels:\n+            residual = self.nin_shortcut(residual)\n+\n+        return residual + hidden_states\n+\n+\n+class Emu3VQVAEAttentionBlock(nn.Module):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.embed_dim = config.hidden_size\n+        self.num_heads = config.num_attention_heads\n+        self.head_dim = self.embed_dim // self.num_heads\n+        if self.head_dim * self.num_heads != self.embed_dim:\n+            raise ValueError(\n+                f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim} and `num_heads`:\"\n+                f\" {self.num_heads}).\"\n+            )\n+        self.scale = self.head_dim**-0.5\n+        self.dropout = config.attention_dropout\n+\n+        self.k_proj = nn.Linear(self.embed_dim, self.embed_dim)\n+        self.v_proj = nn.Linear(self.embed_dim, self.embed_dim)\n+        self.q_proj = nn.Linear(self.embed_dim, self.embed_dim)\n+        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = False,\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n+        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n+\n+        batch_size, q_len, _ = hidden_states.size()\n+\n+        query_states = self.q_proj(hidden_states)\n+        key_states = self.k_proj(hidden_states)\n+        value_states = self.v_proj(hidden_states)\n+\n+        query_states = query_states.view(batch_size, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n+        key_states = key_states.view(batch_size, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n+        value_states = value_states.view(batch_size, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n+\n+        k_v_seq_len = key_states.shape[-2]\n+        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) * self.scale\n+\n+        if attn_weights.size() != (batch_size, self.num_heads, q_len, k_v_seq_len):\n+            raise ValueError(\n+                f\"Attention weights should be of size {(batch_size, self.num_heads, q_len, k_v_seq_len)}, but is\"\n+                f\" {attn_weights.size()}\"\n+            )\n+\n+        if attention_mask is not None:\n+            if attention_mask.size() != (batch_size, 1, q_len, k_v_seq_len):\n+                raise ValueError(\n+                    f\"Attention mask should be of size {(batch_size, 1, q_len, k_v_seq_len)}, but is {attention_mask.size()}\"\n+                )\n+            attn_weights = attn_weights + attention_mask\n+\n+        # upcast attention to fp32\n+        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n+        attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n+        attn_output = torch.matmul(attn_weights, value_states)\n+\n+        if attn_output.size() != (batch_size, self.num_heads, q_len, self.head_dim):\n+            raise ValueError(\n+                f\"`attn_output` should be of size {(batch_size, self.num_heads, q_len, self.head_dim)}, but is\"\n+                f\" {attn_output.size()}\"\n+            )\n+\n+        attn_output = attn_output.transpose(1, 2).contiguous()\n+        attn_output = attn_output.reshape(batch_size, q_len, self.embed_dim)\n+\n+        attn_output = self.out_proj(attn_output)\n+\n+        return attn_output, attn_weights\n+\n+\n+class Emu3VQVAEGroupNorm(nn.GroupNorm):\n+    \"\"\"\n+    Same as the torch GroupNorm with the only difference that this ones accepts\n+    an optional kwarg `quant_states` which is not used. This class makes it easier to\n+    use SpatialNorm or GroupNorm without conditionals\n+    \"\"\"\n+\n+    def __init__(self, **kwargs):\n+        super().__init__(**kwargs)\n+\n+    def forward(self, input, quant_states=None):\n+        return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)\n+\n+\n+class Emu3VQVAEMiddleBlock(nn.Module):\n+    def __init__(self, config, in_channels, quant_channels=None):\n+        super().__init__()\n+\n+        self.block_1 = Emu3VQVAEResnetBlock(\n+            in_channels=in_channels,\n+            out_channels=in_channels,\n+            quant_channels=quant_channels,\n+        )\n+        self.attn_1 = Emu3VQVAEAttentionBlock(config)\n+        if quant_channels is None:\n+            self.attn_norm = Emu3VQVAEGroupNorm(num_channels=in_channels, num_groups=32, eps=1e-6, affine=True)\n+        else:\n+            self.attn_norm = Emu3VQVAESpatialNorm(quant_channels, in_channels)\n+\n+        self.block_2 = Emu3VQVAEResnetBlock(\n+            in_channels=in_channels,\n+            out_channels=in_channels,\n+            quant_channels=quant_channels,\n+        )\n+\n+    def forward(self, hidden_states: torch.FloatTensor, quant_states: torch.FloatTensor = None):\n+        hidden_states = self.block_1(hidden_states, quant_states)\n+        residual = hidden_states\n+        hidden_states = self.attn_norm(hidden_states, quant_states)\n+        batch_size, channels, height, width = hidden_states.shape\n+        hidden_states = hidden_states.view(batch_size, channels, height * width).transpose(1, 2)\n+        hidden_states = self.attn_1(hidden_states)[0]\n+        hidden_states = hidden_states.reshape(batch_size, height, width, channels).permute(0, 3, 1, 2)\n+        hidden_states = residual + hidden_states\n+        hidden_states = self.block_2(hidden_states, quant_states)\n+        return hidden_states\n+\n+\n+class Emu3VQVAEDownBlock(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+\n+        self.num_resolutions = len(config.channel_multiplier)\n+        self.num_res_blocks = config.num_res_blocks\n+        base_channels = config.base_channels\n+        channel_multiplier = config.channel_multiplier\n+\n+        in_channel_multiplier = (1,) + tuple(channel_multiplier)\n+        self.in_channel_multiplier = in_channel_multiplier\n+        self.down = nn.ModuleList()\n+        for i_level in range(self.num_resolutions):\n+            block = nn.ModuleList()\n+            attn = nn.ModuleList()\n+            attn_norms = nn.ModuleList()\n+            block_in = base_channels * in_channel_multiplier[i_level]\n+            block_out = base_channels * channel_multiplier[i_level]\n+            for i_block in range(self.num_res_blocks):\n+                block.append(\n+                    Emu3VQVAEResnetBlock(\n+                        in_channels=block_in,\n+                        out_channels=block_out,\n+                    )\n+                )\n+                block_in = block_out\n+                if config.attn_resolutions is not None and i_level in config.attn_resolutions:\n+                    attn.append(Emu3VQVAEAttentionBlock(config))\n+                    attn_norms.append(nn.GroupNorm(num_channels=block_in, num_groups=32, eps=1e-6, affine=True))\n+\n+            down = nn.Module()\n+            down.block = block\n+            down.attn = attn\n+            down.attn_norms = attn_norms\n+            if i_level != self.num_resolutions - 1:\n+                down.downsample = Emu3VQVAEEncoderConvDownsample(block_in)\n+            self.down.append(down)\n+\n+    def forward(self, hidden_states: torch.FloatTensor):\n+        for i_level, blocks in enumerate(self.down):\n+            for i_block in range(self.num_res_blocks):\n+                hidden_states = blocks.block[i_block](hidden_states)\n+                if len(blocks.attn) > 0:\n+                    residual = hidden_states\n+                    hidden_states = blocks.attn_norms[i_block](hidden_states)\n+\n+                    batch_size, channels, height, width = hidden_states.shape\n+                    hidden_states = hidden_states.view(batch_size, channels, height * width).transpose(1, 2)\n+                    hidden_states = blocks.attn[i_block](hidden_states)[0]\n+\n+                    hidden_states = hidden_states.reshape(batch_size, height, width, channels).permute(0, 3, 1, 2)\n+                    hidden_states = residual + hidden_states\n+\n+            if i_level != self.num_resolutions - 1:\n+                hidden_states = blocks.downsample(hidden_states)\n+\n+        return hidden_states\n+\n+\n+class Emu3VQVAEUpBlock(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+\n+        self.num_resolutions = len(config.channel_multiplier)\n+        self.num_res_blocks = config.num_res_blocks\n+\n+        quant_channels = config.embed_dim\n+        block_in = config.base_channels * config.channel_multiplier[-1]\n+\n+        self.up = nn.ModuleList()\n+        for i_level in reversed(range(self.num_resolutions)):\n+            block = nn.ModuleList()\n+            attn = nn.ModuleList()\n+            attn_norms = nn.ModuleList()\n+            block_out = config.base_channels * config.channel_multiplier[i_level]\n+            for i_block in range(self.num_res_blocks + 1):\n+                block.append(\n+                    Emu3VQVAEResnetBlock(\n+                        in_channels=block_in,\n+                        out_channels=block_out,\n+                        quant_channels=quant_channels,\n+                    )\n+                )\n+                block_in = block_out\n+                if i_level in config.attn_resolutions:\n+                    attn.append(Emu3VQVAEAttentionBlock(config))\n+                    attn_norms.append(Emu3VQVAESpatialNorm(quant_channels, block_in))\n+\n+            up = nn.Module()\n+            up.block = block\n+            up.attn = attn\n+            up.attn_norms = attn_norms\n+            if i_level != 0:\n+                up.upsample = Emu3VQVAEEncoderConvUpsample(block_in)\n+\n+            self.up.insert(0, up)\n+\n+    def forward(self, hidden_states: torch.FloatTensor, quant_states: torch.FloatTensor):\n+        for i_level, blocks in enumerate(self.up[::-1]):\n+            for i_block in range(self.num_res_blocks + 1):\n+                hidden_states = blocks.block[i_block](hidden_states, quant_states)\n+                if len(blocks.attn) > 0:\n+                    residual = hidden_states\n+                    hidden_states = blocks.attn_norms[i_block](hidden_states, quant_states)\n+\n+                    batch_size, channels, height, width = hidden_states.shape\n+                    hidden_states = hidden_states.view(batch_size, channels, height * width).transpose(1, 2)\n+                    hidden_states = blocks.attn[i_block](hidden_states)[0]\n+\n+                    hidden_states = hidden_states.reshape(batch_size, height, width, channels).permute(0, 3, 1, 2)\n+                    hidden_states = residual + hidden_states\n+            if i_level != len(self.up) - 1:\n+                hidden_states = blocks.upsample(hidden_states)\n+\n+        return hidden_states\n+\n+\n+class Emu3VQVAEEncoder(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+\n+        base_channels = config.base_channels\n+        in_channels = config.in_channels\n+        double_latent = config.double_latent\n+        latent_channels = config.latent_channels\n+        channel_multiplier = config.channel_multiplier\n+        out_channels = 2 * latent_channels if double_latent else latent_channels\n+        block_in = base_channels * channel_multiplier[-1]\n+\n+        self.conv_in = torch.nn.Conv2d(in_channels, base_channels, kernel_size=3, stride=1, padding=1)\n+        self.down_block = Emu3VQVAEDownBlock(config)\n+        self.middle_block = Emu3VQVAEMiddleBlock(config, block_in)\n+\n+        self.norm_out = torch.nn.GroupNorm(num_groups=32, num_channels=block_in, eps=1e-6, affine=True)\n+        self.conv_out = torch.nn.Conv2d(\n+            block_in,\n+            out_channels,\n+            kernel_size=3,\n+            stride=1,\n+            padding=1,\n+        )\n+\n+        temporal_down_blocks = int(math.log2(config.temporal_downsample_factor))\n+        self.time_conv = nn.ModuleList()\n+        self.time_res_stack = nn.ModuleList()\n+\n+        for i in range(temporal_down_blocks):\n+            conv = Emu3VQVAETemporalDownsample(out_channels, out_channels)\n+            self.time_conv.append(conv)\n+\n+        for _ in range(config.num_res_blocks):\n+            time_res_conv = Emu3VQVAETemporalResnetBlock(\n+                in_channels=out_channels,\n+                out_channels=out_channels,\n+            )\n+            self.time_res_stack.append(time_res_conv)\n+\n+    def forward(self, pixel_values: torch.LongTensor):\n+        temporal_dim = pixel_values.shape[1]\n+        pixel_values = pixel_values.reshape(-1, *pixel_values.shape[2:])\n+\n+        # downsampling & middle\n+        hidden_states = self.conv_in(pixel_values)\n+        hidden_states = self.down_block(hidden_states)\n+        hidden_states = self.middle_block(hidden_states)\n+\n+        # end\n+        hidden_states = self.norm_out(hidden_states)\n+        hidden_states *= torch.sigmoid(hidden_states)\n+        hidden_states = self.conv_out(hidden_states)\n+\n+        hidden_states = hidden_states.reshape(-1, temporal_dim, *hidden_states.shape[1:])\n+        hidden_states = hidden_states.permute(0, 2, 1, 3, 4)\n+\n+        # temporal convs\n+        for conv in self.time_conv:\n+            hidden_states = conv(hidden_states)\n+            hidden_states *= torch.sigmoid(hidden_states)\n+\n+        for layer in self.time_res_stack:\n+            hidden_states = layer(hidden_states)\n+\n+        hidden_states = hidden_states.permute(0, 2, 1, 3, 4)\n+\n+        return hidden_states\n+\n+\n+class Emu3VQVAEDecoder(nn.Module):\n+    def __init__(self, config: Emu3VQVAEConfig):\n+        super().__init__()\n+\n+        quant_channels = config.embed_dim\n+        block_in = config.base_channels * config.channel_multiplier[-1]\n+        self.time_res_stack = nn.ModuleList()\n+        for _ in range(config.num_res_blocks):\n+            time_res_conv = Emu3VQVAETemporalResnetBlock(\n+                in_channels=config.latent_channels, out_channels=config.latent_channels\n+            )\n+            self.time_res_stack.append(time_res_conv)\n+\n+        temp_upsample_block_num = int(math.log2(config.temporal_downsample_factor))\n+        self.time_conv = nn.ModuleList()\n+        for i in range(temp_upsample_block_num):\n+            conv = Emu3VQVAETemporalUpsample(config.latent_channels, config.latent_channels)\n+            self.time_conv.append(conv)\n+\n+        self.conv_in = nn.Conv2d(\n+            config.latent_channels,\n+            block_in,\n+            kernel_size=3,\n+            stride=1,\n+            padding=1,\n+        )\n+\n+        self.middle_block = Emu3VQVAEMiddleBlock(config, block_in, quant_channels=quant_channels)\n+        self.up_block = Emu3VQVAEUpBlock(config)\n+\n+        block_in = config.base_channels * config.channel_multiplier[0]\n+        self.norm_out = Emu3VQVAESpatialNorm(quant_channels, block_in)\n+        self.conv_out = nn.Conv2d(\n+            block_in,\n+            config.out_channels,\n+            kernel_size=3,\n+            stride=1,\n+            padding=1,\n+        )\n+\n+    def forward(self, hidden_states: torch.Tensor, quant_states: torch.Tensor):\n+        hidden_quant_states = torch.cat((hidden_states, quant_states), dim=0)\n+        hidden_quant_states = hidden_quant_states.permute(0, 2, 1, 3, 4)\n+\n+        # temporal convs\n+        for layer in self.time_res_stack:\n+            hidden_quant_states = layer(hidden_quant_states)\n+\n+        for layer in self.time_conv:\n+            hidden_quant_states = layer(hidden_quant_states)\n+            hidden_quant_states *= torch.sigmoid(hidden_quant_states)\n+\n+        hidden_quant_states = hidden_quant_states.permute(0, 2, 1, 3, 4)\n+        hidden_states, quant_states = torch.chunk(hidden_quant_states, 2, dim=0)\n+        hidden_states = hidden_states.reshape(-1, *hidden_states.shape[2:])\n+        quant_states = quant_states.reshape(-1, *quant_states.shape[2:])\n+\n+        hidden_states = self.conv_in(hidden_states)\n+\n+        # middle & upsampling\n+        hidden_states = self.middle_block(hidden_states, quant_states)\n+        hidden_states = self.up_block(hidden_states, quant_states)\n+\n+        hidden_states = self.norm_out(hidden_states, quant_states)\n+        hidden_states *= torch.sigmoid(hidden_states)\n+        hidden_states = self.conv_out(hidden_states)\n+\n+        return hidden_states\n+\n+\n+EMU3_VQ_START_DOCSTRING = r\"\"\"\n+    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n+    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n+    etc.)\n+\n+    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n+    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n+    and behavior.\n+\n+    Parameters:\n+        config ([`Emu3VQVAEConfig`]):\n+            Model configuration class with all the parameters of the model. Initializing with a config file does not\n+            load the weights associated with the model, only the configuration. Check out the\n+            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"\"\"The VQ-VAE model used in Emu3 for encoding/decoding images into discrete tokens.\n+    This model follows the \"Make-a-scene: Scene-based text-to-image generation with human priors\" paper from\n+    [ Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman](https://arxiv.org/abs/2203.13131).\n+    \"\"\",\n+    EMU3_VQ_START_DOCSTRING,\n+)\n+class Emu3VQVAE(PreTrainedModel):\n+    config_class = Emu3VQVAEConfig\n+    base_model_prefix = \"emuvideovq\"\n+    main_input_name = \"pixel_values\"\n+    _no_split_modules = [\n+        \"Emu3VQVAETemporalResnetBlock\",\n+        \"Emu3VQVAEAttentionBlock\",\n+        \"Emu3VQVAEResnetBlock\",\n+        \"Emu3VQVAEVectorQuantizer\",\n+    ]\n+\n+    def _init_weights(self, module):\n+        if isinstance(module, (nn.Conv2d, nn.Conv3d)):\n+            nn.init.kaiming_normal_(module.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n+        elif isinstance(module, nn.Linear):\n+            nn.init.kaiming_uniform_(module.weight, a=math.sqrt(5))\n+            if module.bias is not None:\n+                fan_in, _ = nn.init._calculate_fan_in_and_fan_out(module.weight)\n+                bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n+                nn.init.uniform_(module.bias, -bound, bound)\n+        elif isinstance(module, (nn.BatchNorm2d, nn.BatchNorm3d, nn.GroupNorm)):\n+            nn.init.constant_(module.weight, 1)\n+            nn.init.constant_(module.bias, 0)\n+\n+    def __init__(self, config: Emu3VQVAEConfig):\n+        super().__init__(config)\n+\n+        self.config = config\n+\n+        self.encoder = Emu3VQVAEEncoder(config)\n+        self.decoder = Emu3VQVAEDecoder(config)\n+        self.quantize = Emu3VQVAEVectorQuantizer(config)\n+        self.vision_spatial_factor = 2 ** (len(config.channel_multiplier) - 1)\n+\n+        self.quant_conv = Emu3VQVAEConv3d(\n+            config.latent_channels, config.embed_dim, kernel_size=(3, 1, 1), stride=(1, 1, 1)\n+        )\n+        self.post_quant_conv = Emu3VQVAEConv3d(\n+            config.embed_dim, config.latent_channels, kernel_size=(3, 1, 1), stride=(1, 1, 1)\n+        )\n+        self.spatial_scale_factor = 2 ** (len(config.channel_multiplier) - 1)\n+        self.eval()  # Emu3's VQ model is frozen\n+\n+        self.post_init()\n+\n+    def encode(self, pixel_values: torch.Tensor, image_sizes: torch.Tensor):\n+        is_image = pixel_values.ndim == 4\n+        if is_image:\n+            temporal = self.config.temporal_downsample_factor\n+            batch_size, channels, height, width = pixel_values.shape\n+            pixel_values = pixel_values.unsqueeze(1).repeat(1, temporal, 1, 1, 1)\n+        else:\n+            batch_size, temporal, channels, height, width = pixel_values.shape\n+\n+        hidden_states = self.encoder(pixel_values)\n+\n+        # b t c h w -> b c t h w\n+        hidden_states = hidden_states.permute(0, 2, 1, 3, 4)\n+        hidden_states = self.quant_conv(hidden_states)\n+\n+        # b c t h w -> b t c h w\n+        hidden_states = hidden_states.permute(0, 2, 1, 3, 4)\n+        codes = self.quantize(hidden_states)\n+\n+        image_tokens = codes.squeeze(1) if is_image else codes\n+\n+        image_tokens = [\n+            single_image[: int(size[0] / self.vision_spatial_factor), : int(size[1] / self.vision_spatial_factor)]\n+            for single_image, size in zip(image_tokens, image_sizes)\n+        ]\n+\n+        return image_tokens\n+\n+    def decode(self, hidden_states: torch.Tensor):\n+        is_image = hidden_states.ndim == 3\n+        if is_image:\n+            hidden_states = hidden_states.unsqueeze(1)\n+\n+        batch_size, temporal, height, width = hidden_states.shape\n+        quant = self.quantize.embedding(hidden_states.flatten())\n+\n+        channels = quant.shape[-1]\n+        quant = quant.view(batch_size, temporal, height, width, channels).permute(0, 4, 1, 2, 3).contiguous()\n+        post_quant = self.post_quant_conv(quant)\n+\n+        quant = quant.permute(0, 2, 1, 3, 4)\n+        post_quant = post_quant.permute(0, 2, 1, 3, 4)\n+\n+        video = self.decoder(post_quant, quant)\n+        video = video.reshape(\n+            batch_size,\n+            temporal * self.config.temporal_downsample_factor,\n+            self.config.out_channels,\n+            height * self.spatial_scale_factor,\n+            width * self.spatial_scale_factor,\n+        )\n+        return video[:, 0] if is_image else video\n+\n+\n+class Emu3ImageVocabularyMapping:\n+    \"\"\"\n+    A class for mapping discrete image tokens from VQGAN to BPE tokens.\n+    \"\"\"\n+\n+    def __init__(self, vocab_map):\n+        self.vocab_map = vocab_map\n+        self.eol_token_id = vocab_map.get(\"<|extra_200|>\")\n+        self.image_token_id = vocab_map.get(\"<image>\")\n+\n+    @cached_property\n+    def image_tokens(self):\n+        return sorted([val for name, val in self.vocab_map.items() if name.startswith(\"<|visual token\")])\n+\n+    @cached_property\n+    def image_tokens_str(self):\n+        return sorted([name for name, val in self.vocab_map.items() if name.startswith(\"<|visual token\")])\n+\n+    @cached_property\n+    def img2bpe(self):\n+        return {int(token[-8:-2]): self.vocab_map[token] for token in self.image_tokens_str}\n+\n+    @cached_property\n+    def bpe2img(self):\n+        return {v: k for k, v in self.img2bpe.items()}\n+\n+    @cached_property\n+    def bpe2img_mapping_tensor(self):\n+        mapping = torch.zeros(max(self.bpe2img.keys()) + 1, dtype=torch.int)\n+        for k, v in self.bpe2img.items():\n+            mapping[k] = v\n+        return mapping\n+\n+    @cached_property\n+    def img2bpe_mapping_tensor(self):\n+        mapping = torch.zeros(max(self.img2bpe.keys()) + 1, dtype=torch.int)\n+        for k, v in self.img2bpe.items():\n+            mapping[k] = v\n+        return mapping\n+\n+    def convert_img2bpe(self, img_batch: List[torch.Tensor]) -> torch.Tensor:\n+        device = img_batch.device\n+        eol_row = torch.ones((img_batch.shape[0], 1), dtype=torch.int) * self.eol_token_id\n+        img_tokens = self.img2bpe_mapping_tensor[img_batch.to(\"cpu\")]\n+        img_tokens = torch.cat([img_tokens, eol_row], dim=-1)\n+        return img_tokens.to(device)\n+\n+    def convert_bpe2img(self, img_batch: torch.Tensor) -> torch.Tensor:\n+        device = img_batch.device\n+        img_batch = img_batch[..., :-1]  # remove last row of EOL tokens\n+        img_tokens = self.bpe2img_mapping_tensor[img_batch.to(\"cpu\")]\n+        return img_tokens.to(device)\n+\n+\n+EMU3_START_DOCSTRING = r\"\"\"\n+    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n+    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n+    etc.)\n+\n+    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n+    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n+    and behavior.\n+\n+    Parameters:\n+        config ([`Emu3Config`]):\n+            Model configuration class with all the parameters of the model. Initializing with a config file does not\n+            load the weights associated with the model, only the configuration. Check out the\n+            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"The bare emu3 Model outputting raw hidden-states without any specific head on top.\",\n+    EMU3_START_DOCSTRING,\n+)\n+class Emu3PreTrainedModel(PreTrainedModel):\n+    config_class = Emu3Config\n+    base_model_prefix = \"model\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\n+        \"Emu3DecoderLayer\",\n+    ]\n+    _skip_keys_device_placement = [\"past_key_values\", \"causal_mask\"]\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n+    _supports_quantized_cache = True\n+    _supports_cache_class = True\n+    _supports_static_cache = True\n+    _supports_param_buffer_assignment = False\n+    _supports_flex_attn = True\n+\n+    def _init_weights(self, module):\n+        std = self.config.get_text_config().initializer_range\n+        if isinstance(module, Emu3VQVAE):\n+            module.apply(module._init_weights)\n+        elif isinstance(module, (nn.Linear, nn.Conv2d)):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+\n+\n+class Emu3RotaryEmbedding(nn.Module):\n+    def __init__(self, config: Emu3Config, device=None):\n+        super().__init__()\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+        else:\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+    def _dynamic_frequency_update(self, position_ids, device):\n+        \"\"\"\n+        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n+        1 - growing beyond the cached sequence length (allow scaling)\n+        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n+        \"\"\"\n+        seq_len = torch.max(position_ids) + 1\n+        if seq_len > self.max_seq_len_cached:  # growth\n+            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n+            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n+            self.max_seq_len_cached = seq_len\n+\n+        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n+            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n+            self.max_seq_len_cached = self.original_max_seq_len\n+\n+    @torch.no_grad()\n+    def forward(self, x, position_ids):\n+        if \"dynamic\" in self.rope_type:\n+            self._dynamic_frequency_update(position_ids, device=x.device)\n+\n+        # Core RoPE block\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n+        device_type = x.device.type\n+        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos()\n+            sin = emb.sin()\n+\n+        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n+        cos = cos * self.attention_scaling\n+        sin = sin * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n+EMU3_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n+            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n+            it.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n+\n+            - 1 for tokens that are **not masked**,\n+            - 0 for tokens that are **masked**.\n+\n+            [What are attention masks?](../glossary#attention-mask)\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see\n+            `past_key_values`).\n+\n+            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n+            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n+            information on the default strategy.\n+\n+            - 1 indicates the head is **not masked**,\n+            - 0 indicates the head is **masked**.\n+        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n+            config.n_positions - 1]`.\n+\n+            [What are position IDs?](../glossary#position-ids)\n+        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):\n+            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n+            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n+            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n+\n+            Two formats are allowed:\n+            - a [`~cache_utils.Cache`] instance, see our\n+            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n+            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n+            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n+            cache format.\n+\n+            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the\n+            legacy cache format will be returned.\n+\n+            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n+            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n+            of shape `(batch_size, sequence_length)`.\n+        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n+            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n+            model's internal embedding lookup matrix.\n+        use_cache (`bool`, *optional*):\n+            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n+            `past_key_values`).\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n+            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n+            the complete sequence length.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"The bare Emu3Text Model outputting raw hidden-states without any specific head on top.\",\n+    EMU3_START_DOCSTRING,\n+)\n+class Emu3TextModel(Emu3PreTrainedModel):\n+    \"\"\"\n+    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`Emu3TextDecoderLayer`]\n+\n+    Args:\n+        config: Emu3TextConfig\n+    \"\"\"\n+\n+    def __init__(self, config: Emu3Config):\n+        super().__init__(config)\n+        self.padding_idx = config.pad_token_id\n+        self.vocab_size = config.vocab_size\n+\n+        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n+        self.layers = nn.ModuleList(\n+            [Emu3DecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+        self.norm = Emu3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.rotary_emb = Emu3RotaryEmbedding(config=config)\n+        self.gradient_checkpointing = False\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.embed_tokens\n+\n+    def set_input_embeddings(self, value):\n+        self.embed_tokens = value\n+\n+    @add_start_docstrings_to_model_forward(EMU3_INPUTS_DOCSTRING)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> Union[Tuple, BaseModelOutputWithPast]:\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        use_cache = use_cache if use_cache is not None else self.config.use_cache\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if self.gradient_checkpointing and self.training and use_cache:\n+            logger.warning_once(\n+                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n+            )\n+            use_cache = False\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.embed_tokens(input_ids)\n+\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n+        causal_mask = self._update_causal_mask(\n+            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+        )\n+\n+        hidden_states = inputs_embeds\n+\n+        # create position embeddings to be shared across the decoder layers\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n+        # decoder layers\n+        all_hidden_states = () if output_hidden_states else None\n+        all_self_attns = () if output_attentions else None\n+\n+        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n+            if output_hidden_states:\n+                all_hidden_states += (hidden_states,)\n+\n+            if self.gradient_checkpointing and self.training:\n+                layer_outputs = self._gradient_checkpointing_func(\n+                    decoder_layer.__call__,\n+                    hidden_states,\n+                    causal_mask,\n+                    position_ids,\n+                    past_key_values,\n+                    output_attentions,\n+                    use_cache,\n+                    cache_position,\n+                    position_embeddings,\n+                )\n+            else:\n+                layer_outputs = decoder_layer(\n+                    hidden_states,\n+                    attention_mask=causal_mask,\n+                    position_ids=position_ids,\n+                    past_key_value=past_key_values,\n+                    output_attentions=output_attentions,\n+                    use_cache=use_cache,\n+                    cache_position=cache_position,\n+                    position_embeddings=position_embeddings,\n+                    **flash_attn_kwargs,\n+                )\n+\n+            hidden_states = layer_outputs[0]\n+\n+            if output_attentions:\n+                all_self_attns += (layer_outputs[1],)\n+\n+        hidden_states = self.norm(hidden_states)\n+\n+        # add hidden states from the last decoder layer\n+        if output_hidden_states:\n+            all_hidden_states += (hidden_states,)\n+\n+        output = BaseModelOutputWithPast(\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values if use_cache else None,\n+            hidden_states=all_hidden_states,\n+            attentions=all_self_attns,\n+        )\n+        return output if return_dict else output.to_tuple()\n+\n+    def _update_causal_mask(\n+        self,\n+        attention_mask: torch.Tensor,\n+        input_tensor: torch.Tensor,\n+        cache_position: torch.Tensor,\n+        past_key_values: Cache,\n+        output_attentions: bool,\n+    ):\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            if attention_mask is not None and (attention_mask == 0.0).any():\n+                return attention_mask\n+            return None\n+\n+        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n+        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n+        # to infer the attention mask.\n+        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        using_static_cache = isinstance(past_key_values, StaticCache)\n+\n+        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n+        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n+            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n+                attention_mask,\n+                inputs_embeds=input_tensor,\n+                past_key_values_length=past_seen_tokens,\n+                is_training=self.training,\n+            ):\n+                return None\n+\n+        dtype, device = input_tensor.dtype, input_tensor.device\n+        sequence_length = input_tensor.shape[1]\n+        if using_static_cache:\n+            target_length = past_key_values.get_max_cache_shape()\n+        else:\n+            target_length = (\n+                attention_mask.shape[-1]\n+                if isinstance(attention_mask, torch.Tensor)\n+                else past_seen_tokens + sequence_length + 1\n+            )\n+\n+        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n+        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n+            attention_mask,\n+            sequence_length=sequence_length,\n+            target_length=target_length,\n+            dtype=dtype,\n+            device=device,\n+            cache_position=cache_position,\n+            batch_size=input_tensor.shape[0],\n+        )\n+\n+        if (\n+            self.config._attn_implementation == \"sdpa\"\n+            and attention_mask is not None\n+            and attention_mask.device.type == \"cuda\"\n+            and not output_attentions\n+        ):\n+            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n+            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n+            # Details: https://github.com/pytorch/pytorch/issues/110213\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n+\n+        return causal_mask\n+\n+    @staticmethod\n+    def _prepare_4d_causal_attention_mask_with_cache_position(\n+        attention_mask: torch.Tensor,\n+        sequence_length: int,\n+        target_length: int,\n+        dtype: torch.dtype,\n+        device: torch.device,\n+        cache_position: torch.Tensor,\n+        batch_size: int,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+\n+        Args:\n+            attention_mask (`torch.Tensor`):\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n+                `(batch_size, 1, query_length, key_value_length)`.\n+            sequence_length (`int`):\n+                The sequence length being processed.\n+            target_length (`int`):\n+                The target length: when generating with static cache, the mask should be as long as the static cache,\n+                to account for the 0 padding, the part of the cache that is not filled yet.\n+            dtype (`torch.dtype`):\n+                The dtype to use for the 4D attention mask.\n+            device (`torch.device`):\n+                The device to plcae the 4D attention mask on.\n+            cache_position (`torch.Tensor`):\n+                Indices depicting the position of the input sequence tokens in the sequence.\n+            batch_size (`torch.Tensor`):\n+                Batch size.\n+        \"\"\"\n+        if attention_mask is not None and attention_mask.dim() == 4:\n+            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n+            causal_mask = attention_mask\n+        else:\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = torch.full(\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+            )\n+            if sequence_length != 1:\n+                causal_mask = torch.triu(causal_mask, diagonal=1)\n+            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n+            if attention_mask is not None:\n+                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+                mask_length = attention_mask.shape[-1]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = padding_mask == 0\n+                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                    padding_mask, min_dtype\n+                )\n+\n+        return causal_mask\n+\n+\n+class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n+\n+\n+EMU3_TEXT_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n+            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n+            it.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n+\n+            - 1 for tokens that are **not masked**,\n+            - 0 for tokens that are **masked**.\n+\n+            [What are attention masks?](../glossary#attention-mask)\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see\n+            `past_key_values`).\n+\n+            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n+            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n+            information on the default strategy.\n+\n+            - 1 indicates the head is **not masked**,\n+            - 0 indicates the head is **masked**.\n+        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n+            config.n_positions - 1]`.\n+\n+            [What are position IDs?](../glossary#position-ids)\n+        past_key_values (`Cache`, *optional*):\n+            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n+            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n+            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n+\n+            Has to be an instance of [`~cache_utils.Cache`] instance, see our\n+            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n+\n+            The model will output the same cache type that is fed as input. If no `past_key_values` are passed, the\n+            legacy cache format will be returned.\n+\n+            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n+            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n+            of shape `(batch_size, sequence_length)`.\n+        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n+            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n+            model's internal embedding lookup matrix.\n+        use_cache (`bool`, *optional*):\n+            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n+            `past_key_values`).\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n+            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n+            the complete sequence length.\n+\"\"\"\n+\n+\n+class Emu3ForCausalLM(Emu3PreTrainedModel, GenerationMixin):\n+    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tp_plan = {\"lm_head\": \"colwise_rep\"}\n+    config_class = Emu3TextConfig\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.model = Emu3TextModel(config)\n+        self.vocab_size = config.vocab_size\n+        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.model.embed_tokens\n+\n+    def set_input_embeddings(self, value):\n+        self.model.embed_tokens = value\n+\n+    def get_output_embeddings(self):\n+        return self.lm_head\n+\n+    def set_output_embeddings(self, new_embeddings):\n+        self.lm_head = new_embeddings\n+\n+    def set_decoder(self, decoder):\n+        self.model = decoder\n+\n+    def get_decoder(self):\n+        return self.model\n+\n+    @add_start_docstrings_to_model_forward(EMU3_TEXT_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=\"Emu3TextConfig\")\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        num_logits_to_keep: int = 0,\n+        **kwargs: Unpack[KwargsForCausalLM],\n+    ) -> Union[Tuple, CausalLMOutputWithPast]:\n+        r\"\"\"\n+        Args:\n+            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+            num_logits_to_keep (`int`, *optional*):\n+                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n+                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+\n+        Returns:\n+\n+        Example:\n+\n+        ```python\n+        >>> from transformers import Emu3Processor, Emu3ForConditionalGeneration\n+        >>> import torch\n+        >>> import requests\n+        >>> from PIL import Image\n+\n+        >>> model = Emu3ForCausalLM.from_pretrained(\"Emu3-community/Emu3-Chat-hf\", torch_dtype=torch.bfloat16)\n+        >>> processor = Emu3Processor.from_pretrained(\"Emu3-community/Emu3-Chat-hf\")\n+\n+        >>> inputs = processor(text=[\"Can you write me a poem about winter.\"], return_tensors=\"pt\").to(model.device)\n+\n+        >>> generated_ids = model.generate(**inputs, max_new_tokens=100, do_sample=False)\n+        >>> processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n+        ```\"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n+        outputs = self.model(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+        hidden_states = outputs[0]\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n+\n+        if not return_dict:\n+            output = (logits,) + outputs[1:]\n+            return (loss,) + output if loss is not None else output\n+\n+        return CausalLMOutputWithPast(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+\n+class Emu3ForConditionalGeneration(Emu3PreTrainedModel, GenerationMixin):\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.text_model = Emu3ForCausalLM._from_config(config.text_config)\n+        self.vqmodel = Emu3VQVAE(config.vq_config)\n+        self.vocabulary_mapping = Emu3ImageVocabularyMapping(config.vocabulary_map)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.text_model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.text_model.set_input_embeddings(value)\n+\n+    def get_image_tokens(self, pixel_values: torch.FloatTensor, image_sizes: torch.LongTensor):\n+        \"\"\"\n+        Tokenizes images into discrete tokens with VQGAN module. Converts\n+        obtained image tokens into BPE tokens and wraps with \"boi\" and \"eoi\"\n+        special tokens.\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)`):\n+                The tensors corresponding to the input images.\n+            image_sizes (`torch.LongTensor` of shape `(batch_size, 2)`):\n+                The sizes of the images in the batch, being (height, width) for each image.\n+        \"\"\"\n+        image_tokens_list = self.vqmodel.encode(pixel_values, image_sizes)\n+        bpe_tokens_list = [self.vocabulary_mapping.convert_img2bpe(tokens).flatten() for tokens in image_tokens_list]\n+        bpe_tokens = torch.cat(bpe_tokens_list)\n+        return bpe_tokens\n+\n+    @torch.no_grad\n+    def decode_image_tokens(self, image_tokens: torch.LongTensor, height: int, width: int):\n+        \"\"\"\n+        Decodes generated image tokens from language model to continuous pixel values\n+        with VQGAN module via upsampling.\n+\n+        Args:\n+            image_tokens (`torch.LongTensor` of shape `(batch_size, num_of_tokens)`):\n+                The tensors corresponding to the input images.\n+            height (`int`):\n+                Height of the generated image before upsampling.\n+            width (`int`):\n+                Width of the generated image before upsampling.\n+        \"\"\"\n+        sequences = image_tokens[:, :-3].view(-1, height, width + 1)\n+        image_tokens = self.vocabulary_mapping.convert_bpe2img(sequences)\n+        image = self.vqmodel.decode(image_tokens)\n+        return image\n+\n+    @add_start_docstrings_to_model_forward(EMU3_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n+        image_sizes: torch.Tensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        num_logits_to_keep: int = 0,\n+    ) -> Union[Tuple, CausalLMOutputWithPast]:\n+        r\"\"\"\n+        Args:\n+            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+            num_logits_to_keep (`int`, *optional*):\n+                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n+                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+\n+        Returns:\n+\n+        Example:\n+\n+        ```python\n+        >>> from transformers import Emu3Processor, Emu3ForConditionalGeneration\n+        >>> import torch\n+        >>> import requests\n+        >>> from PIL import Image\n+\n+        >>> model = Emu3ForConditionalGeneration.from_pretrained(\"Emu3-community/Emu3-Chat-hf\", torch_dtype=torch.bfloat16)\n+        >>> processor = Emu3Processor.from_pretrained(\"Emu3-community/Emu3-Chat-hf\")\n+\n+        >>> conversation = [\n+        ...     {\n+        ...     \"role\": \"system\",\n+        ...     \"content\": [\n+        ...         {\"type\": \"text\", \"text\": \"You are a helpful assistant.\"},\n+        ...         ],\n+        ...     },\n+        ...     {\n+        ...     \"role\": \"user\",\n+        ...     \"content\": [\n+        ...         {\"type\": \"image\"},\n+        ...         {\"type\": \"text\", \"text\": \"Please describe the image.\"},\n+        ...         ],\n+        ...     },\n+        ... ]\n+\n+        >>> prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n+        >>> image = Image.open(requests.get(\"https://www.ilankelman.org/stopsigns/australia.jpg\", stream=True).raw)\n+\n+        >>> inputs = processor(images=[image], text=[prompt], return_tensors=\"pt\").to(model.device, torch.bfloat16)\n+\n+        >>> generated_ids = model.generate(**inputs, max_new_tokens=100, do_sample=False)\n+        >>> processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n+        ```\"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\n+                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n+            )\n+\n+        if pixel_values is not None and inputs_embeds is not None:\n+            raise ValueError(\n+                \"You cannot specify both pixel_values and inputs_embeds at the same time, and must specify either one\"\n+            )\n+\n+        if pixel_values is not None:\n+            image_tokens = self.get_image_tokens(pixel_values, image_sizes)\n+            special_image_mask = input_ids == self.vocabulary_mapping.image_token_id\n+            image_tokens = image_tokens.to(input_ids.device, input_ids.dtype)\n+            input_ids = input_ids.masked_scatter(special_image_mask, image_tokens)\n+\n+        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n+        outputs = self.text_model(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+            cache_position=cache_position,\n+            num_logits_to_keep=num_logits_to_keep,\n+        )\n+\n+        return outputs\n+\n+\n+__all__ = [\"Emu3ForConditionalGeneration\", \"Emu3ForCausalLM\", \"Emu3TextModel\", \"Emu3PreTrainedModel\", \"Emu3VQVAE\"]"
        },
        {
            "sha": "e9b80d5cbb4deb4bb0d646951966a820fa676d4c",
            "filename": "src/transformers/models/emu3/modular_emu3.py",
            "status": "added",
            "additions": 1270,
            "deletions": 0,
            "changes": 1270,
            "blob_url": "https://github.com/huggingface/transformers/blob/52e1f87c7d2fdaf147aa625c169ad440672cb3cb/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52e1f87c7d2fdaf147aa625c169ad440672cb3cb/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py?ref=52e1f87c7d2fdaf147aa625c169ad440672cb3cb",
            "patch": "@@ -0,0 +1,1270 @@\n+# coding=utf-8\n+# Copyright 2024 HuggingFace Inc. team. All rights reserved.\n+#\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import math\n+from functools import cached_property\n+from typing import List, Optional, Tuple, Union\n+\n+import torch\n+import torch.nn as nn\n+import torch.nn.functional as F\n+import torch.utils.checkpoint\n+\n+from ...cache_utils import Cache\n+from ...generation import GenerationMixin\n+from ...modeling_outputs import (\n+    CausalLMOutputWithPast,\n+)\n+from ...modeling_utils import PreTrainedModel\n+from ...utils import (\n+    add_start_docstrings,\n+    add_start_docstrings_to_model_forward,\n+    is_flash_attn_2_available,\n+    logging,\n+    replace_return_docstrings,\n+)\n+from ..chameleon.modeling_chameleon import (\n+    ChameleonPreTrainedModel,\n+    ChameleonVQVAEEncoderConvDownsample,\n+)\n+from ..llama.modeling_llama import (\n+    LlamaDecoderLayer,\n+    LlamaForCausalLM,\n+    LlamaModel,\n+)\n+from ..siglip.modeling_siglip import SiglipAttention\n+from .configuration_emu3 import Emu3Config, Emu3TextConfig, Emu3VQVAEConfig\n+\n+\n+if is_flash_attn_2_available():\n+    from flash_attn.bert_padding import index_first_axis, pad_input, unpad_input  # noqa\n+\n+\n+_CONFIG_FOR_DOC = \"Emu3Config\"\n+_CHECKPOINT_FOR_DOC = \"Emu3-community/Emu3-Chat-hf\"\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+# Has extra dropout which no other model in the library has\n+class Emu3DecoderLayer(LlamaDecoderLayer):\n+    def __init__(self, config: Emu3Config, layer_idx: int):\n+        super().__init__(config, layer_idx)\n+        self.dropout = nn.Dropout(config.attention_dropout)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_value: Optional[Cache] = None,\n+        output_attentions: Optional[bool] = False,\n+        use_cache: Optional[bool] = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n+        **kwargs,\n+    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+        \"\"\"\n+        Args:\n+            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n+            attention_mask (`torch.FloatTensor`, *optional*):\n+                attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,\n+                query_sequence_length, key_sequence_length)` if default attention is used.\n+            output_attentions (`bool`, *optional*):\n+                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n+                returned tensors for more detail.\n+            use_cache (`bool`, *optional*):\n+                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n+                (see `past_key_values`).\n+            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+                Indices depicting the position of the input sequence tokens in the sequence\n+            kwargs (`dict`, *optional*):\n+                Arbitrary kwargs to be ignored, used for FSDP and other methods that injects code\n+                into the model\n+        \"\"\"\n+        residual = hidden_states\n+\n+        hidden_states = self.input_layernorm(hidden_states)\n+\n+        # Self Attention\n+        hidden_states, self_attn_weights = self.self_attn(\n+            hidden_states=hidden_states,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_value=past_key_value,\n+            output_attentions=output_attentions,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            position_embeddings=position_embeddings,\n+            **kwargs,\n+        )\n+        hidden_states = residual + self.dropout(hidden_states)\n+\n+        # Fully Connected\n+        residual = hidden_states\n+        hidden_states = self.post_attention_layernorm(hidden_states)\n+        hidden_states = self.mlp(hidden_states)\n+        hidden_states = residual + self.dropout(hidden_states)\n+\n+        outputs = (hidden_states,)\n+\n+        if output_attentions:\n+            outputs += (self_attn_weights,)\n+\n+        return outputs\n+\n+\n+class Emu3VQVAEVectorQuantizer(nn.Module):\n+    \"\"\"\n+    A module for vector quantization using learned embedding vectors.\n+\n+    This module implements the quantization process similar to te one described in\n+    the VQ-VAE (Vector Quantized Variational AutoEncoder) paper. It quantizes continuous\n+    input vectors into discrete codebook vectors, which are learned during training.\n+    Current implementation improves over previous ones by avoiding costly matrix multiplications\n+    and allowing for post-hoc remapping of indices.\n+    \"\"\"\n+\n+    def __init__(self, config: Emu3VQVAEConfig):\n+        super().__init__()\n+        self.embedding = nn.Embedding(config.codebook_size, config.embed_dim)\n+        self.embedding.weight.data.uniform_(-1.0 / config.codebook_size, 1.0 / config.codebook_size)\n+\n+    def forward(self, hidden_state: torch.Tensor):\n+        batch_size, temporal, channels, height, width = hidden_state.shape\n+        hidden_state = hidden_state.permute(0, 1, 3, 4, 2).contiguous()\n+        hidden_state_flattened = hidden_state.view(-1, channels)\n+\n+        # distances from z to embeddings e_j (z - e)^2 = z^2 + e^2 - 2 e * z\n+        hidden_state_sum = torch.sum(hidden_state_flattened**2, dim=1, keepdim=True)\n+        embedding_sum = torch.sum(self.embedding.weight**2, dim=1)\n+\n+        # \"bd,dn->bn\",\n+        distances = 2 * torch.matmul(hidden_state_flattened, self.embedding.weight.transpose(0, 1))\n+        distances = hidden_state_sum + embedding_sum - distances\n+\n+        min_encoding_indices = torch.argmin(distances, dim=1)\n+        min_encoding_indices = min_encoding_indices.view(batch_size, temporal, height, width)\n+        return min_encoding_indices\n+\n+\n+class Emu3VQVAEEncoderConvDownsample(ChameleonVQVAEEncoderConvDownsample):\n+    pass\n+\n+\n+class Emu3VQVAEEncoderConvUpsample(nn.Module):\n+    def __init__(self, in_channels):\n+        super().__init__()\n+        self.conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1)\n+\n+    def forward(self, hidden_states):\n+        hidden_states = F.interpolate(hidden_states, scale_factor=2.0, mode=\"nearest\")\n+        hidden_states = self.conv(hidden_states)\n+        return hidden_states\n+\n+\n+class Emu3VQVAEConv3d(nn.Module):\n+    def __init__(\n+        self,\n+        in_channel: int,\n+        out_channel: int,\n+        kernel_size: Tuple[int],\n+        stride: Tuple[int],\n+    ):\n+        super().__init__()\n+\n+        padding_sizes = [one_kernel - one_stride for one_kernel, one_stride in zip(kernel_size[1:], stride[1:])]\n+        self.padding = ()\n+        for pad_size in padding_sizes[::-1]:\n+            self.padding += (pad_size // 2 + pad_size % 2, pad_size // 2)\n+        self.padding += (2, 0)\n+\n+        self.conv = nn.Conv3d(\n+            in_channel,\n+            out_channel,\n+            kernel_size,\n+            stride=stride,\n+        )\n+\n+    def forward(self, hidden_states: torch.Tensor):\n+        hidden_states = F.pad(hidden_states, self.padding)\n+        hidden_states = self.conv(hidden_states)\n+        return hidden_states\n+\n+\n+class Emu3VQVAESpatialNorm(nn.Module):\n+    def __init__(\n+        self,\n+        in_channels: int,\n+        out_channels: int,\n+    ):\n+        super().__init__()\n+        self.norm_layer = nn.GroupNorm(\n+            num_channels=out_channels,\n+            num_groups=32,\n+            eps=1e-6,\n+            affine=True,\n+        )\n+\n+        self.conv_y = nn.Conv2d(\n+            in_channels,\n+            out_channels,\n+            kernel_size=1,\n+            stride=1,\n+            padding=0,\n+        )\n+        self.conv_b = nn.Conv2d(\n+            in_channels,\n+            out_channels,\n+            kernel_size=1,\n+            stride=1,\n+            padding=0,\n+        )\n+\n+    def forward(self, hidden_states: torch.Tensor, quant_states: torch.Tensor):\n+        quant_states = F.interpolate(quant_states, size=hidden_states.shape[-2:], mode=\"nearest\")\n+        hidden_states = self.norm_layer(hidden_states)\n+        hidden_states = hidden_states * self.conv_y(quant_states) + self.conv_b(quant_states)\n+        return hidden_states\n+\n+\n+class Emu3VQVAETemporalUpsample(nn.Module):\n+    def __init__(\n+        self,\n+        in_channel: int,\n+        out_channel: int,\n+    ):\n+        super().__init__()\n+        self.conv = Emu3VQVAEConv3d(\n+            in_channel,\n+            out_channel,\n+            kernel_size=(3, 3, 3),\n+            stride=(1, 1, 1),\n+        )\n+\n+    def forward(self, hidden_states: torch.Tensor):\n+        batch_size, channels, temporal, height, width = hidden_states.shape\n+        hidden_states = hidden_states.permute(0, 1, 3, 4, 2).contiguous().view(batch_size, -1, temporal)\n+        hidden_states = F.interpolate(hidden_states, scale_factor=2.0, mode=\"nearest\")\n+        hidden_states = hidden_states.view(batch_size, channels, height, width, -1).permute(0, 1, 4, 2, 3).contiguous()\n+        hidden_states = self.conv(hidden_states)\n+        return hidden_states\n+\n+\n+class Emu3VQVAETemporalDownsample(nn.Module):\n+    def __init__(\n+        self,\n+        in_channel: int,\n+        out_channel: int,\n+    ):\n+        super().__init__()\n+        self.conv = Emu3VQVAEConv3d(\n+            in_channel,\n+            out_channel,\n+            kernel_size=(4, 3, 3),\n+            stride=(2, 1, 1),\n+        )\n+\n+    def forward(self, hidden_states: torch.Tensor):\n+        hidden_states = self.conv(hidden_states)\n+        return hidden_states\n+\n+\n+class Emu3VQVAETemporalResnetBlock(nn.Module):\n+    def __init__(\n+        self,\n+        in_channels,\n+        out_channels=None,\n+    ):\n+        super().__init__()\n+        self.in_channels = in_channels\n+        self.out_channels = in_channels if out_channels is None else out_channels\n+\n+        self.norm1 = nn.BatchNorm3d(in_channels)\n+        self.conv1 = Emu3VQVAEConv3d(\n+            in_channels,\n+            out_channels,\n+            kernel_size=(3, 3, 3),\n+            stride=(1, 1, 1),\n+        )\n+        self.norm2 = nn.BatchNorm3d(out_channels)\n+        self.conv2 = Emu3VQVAEConv3d(\n+            out_channels,\n+            out_channels,\n+            kernel_size=(3, 3, 3),\n+            stride=(1, 1, 1),\n+        )\n+        if self.in_channels != self.out_channels:\n+            self.nin_shortcut = nn.Conv3d(\n+                in_channels,\n+                out_channels,\n+                kernel_size=1,\n+                stride=1,\n+                padding=0,\n+            )\n+\n+    def forward(self, hidden_states):\n+        residual = hidden_states\n+        hidden_states = self.norm1(hidden_states)\n+        hidden_states *= torch.sigmoid(hidden_states)\n+        hidden_states = self.conv1(hidden_states)\n+\n+        hidden_states = self.norm2(hidden_states)\n+        hidden_states *= torch.sigmoid(hidden_states)\n+        hidden_states = self.conv2(hidden_states)\n+\n+        if self.in_channels != self.out_channels:\n+            residual = self.nin_shortcut(residual)\n+\n+        return residual + hidden_states\n+\n+\n+class Emu3VQVAEResnetBlock(nn.Module):\n+    def __init__(\n+        self,\n+        in_channels: int,\n+        out_channels: Optional[int] = None,\n+        quant_channels: Optional[int] = None,\n+    ):\n+        super().__init__()\n+        self.in_channels = in_channels\n+        out_channels = in_channels if out_channels is None else out_channels\n+        self.out_channels = out_channels\n+        self.quant_channels = quant_channels\n+\n+        if quant_channels is None:\n+            self.norm1 = nn.GroupNorm(num_channels=in_channels, num_groups=32, eps=1e-6, affine=True)\n+            self.norm2 = nn.GroupNorm(num_channels=out_channels, num_groups=32, eps=1e-6, affine=True)\n+        else:\n+            self.norm1 = Emu3VQVAESpatialNorm(quant_channels, in_channels)\n+            self.norm2 = Emu3VQVAESpatialNorm(quant_channels, out_channels)\n+\n+        self.conv1 = nn.Conv2d(\n+            in_channels,\n+            out_channels,\n+            kernel_size=3,\n+            stride=1,\n+            padding=1,\n+        )\n+\n+        self.conv2 = nn.Conv2d(\n+            out_channels,\n+            out_channels,\n+            kernel_size=3,\n+            stride=1,\n+            padding=1,\n+        )\n+\n+        if self.in_channels != self.out_channels:\n+            self.nin_shortcut = nn.Conv2d(\n+                in_channels,\n+                out_channels,\n+                kernel_size=1,\n+                stride=1,\n+                padding=0,\n+            )\n+\n+    def forward(self, hidden_states: torch.Tensor, quant_channels: Optional[torch.Tensor] = None):\n+        norm_args = () if self.quant_channels is None else (quant_channels,)\n+\n+        residual = hidden_states\n+        hidden_states = self.norm1(hidden_states, *norm_args)\n+        hidden_states *= torch.sigmoid(hidden_states)\n+        hidden_states = self.conv1(hidden_states)\n+\n+        hidden_states = self.norm2(hidden_states, *norm_args)\n+        hidden_states *= torch.sigmoid(hidden_states)\n+        hidden_states = self.conv2(hidden_states)\n+\n+        if self.in_channels != self.out_channels:\n+            residual = self.nin_shortcut(residual)\n+\n+        return residual + hidden_states\n+\n+\n+class Emu3VQVAEAttentionBlock(SiglipAttention):\n+    pass\n+\n+\n+class Emu3VQVAEGroupNorm(nn.GroupNorm):\n+    \"\"\"\n+    Same as the torch GroupNorm with the only difference that this ones accepts\n+    an optional kwarg `quant_states` which is not used. This class makes it easier to\n+    use SpatialNorm or GroupNorm without conditionals\n+    \"\"\"\n+\n+    def __init__(self, **kwargs):\n+        super().__init__(**kwargs)\n+\n+    def forward(self, input, quant_states=None):\n+        return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)\n+\n+\n+class Emu3VQVAEMiddleBlock(nn.Module):\n+    def __init__(self, config, in_channels, quant_channels=None):\n+        super().__init__()\n+\n+        self.block_1 = Emu3VQVAEResnetBlock(\n+            in_channels=in_channels,\n+            out_channels=in_channels,\n+            quant_channels=quant_channels,\n+        )\n+        self.attn_1 = Emu3VQVAEAttentionBlock(config)\n+        if quant_channels is None:\n+            self.attn_norm = Emu3VQVAEGroupNorm(num_channels=in_channels, num_groups=32, eps=1e-6, affine=True)\n+        else:\n+            self.attn_norm = Emu3VQVAESpatialNorm(quant_channels, in_channels)\n+\n+        self.block_2 = Emu3VQVAEResnetBlock(\n+            in_channels=in_channels,\n+            out_channels=in_channels,\n+            quant_channels=quant_channels,\n+        )\n+\n+    def forward(self, hidden_states: torch.FloatTensor, quant_states: torch.FloatTensor = None):\n+        hidden_states = self.block_1(hidden_states, quant_states)\n+        residual = hidden_states\n+        hidden_states = self.attn_norm(hidden_states, quant_states)\n+        batch_size, channels, height, width = hidden_states.shape\n+        hidden_states = hidden_states.view(batch_size, channels, height * width).transpose(1, 2)\n+        hidden_states = self.attn_1(hidden_states)[0]\n+        hidden_states = hidden_states.reshape(batch_size, height, width, channels).permute(0, 3, 1, 2)\n+        hidden_states = residual + hidden_states\n+        hidden_states = self.block_2(hidden_states, quant_states)\n+        return hidden_states\n+\n+\n+class Emu3VQVAEDownBlock(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+\n+        self.num_resolutions = len(config.channel_multiplier)\n+        self.num_res_blocks = config.num_res_blocks\n+        base_channels = config.base_channels\n+        channel_multiplier = config.channel_multiplier\n+\n+        in_channel_multiplier = (1,) + tuple(channel_multiplier)\n+        self.in_channel_multiplier = in_channel_multiplier\n+        self.down = nn.ModuleList()\n+        for i_level in range(self.num_resolutions):\n+            block = nn.ModuleList()\n+            attn = nn.ModuleList()\n+            attn_norms = nn.ModuleList()\n+            block_in = base_channels * in_channel_multiplier[i_level]\n+            block_out = base_channels * channel_multiplier[i_level]\n+            for i_block in range(self.num_res_blocks):\n+                block.append(\n+                    Emu3VQVAEResnetBlock(\n+                        in_channels=block_in,\n+                        out_channels=block_out,\n+                    )\n+                )\n+                block_in = block_out\n+                if config.attn_resolutions is not None and i_level in config.attn_resolutions:\n+                    attn.append(Emu3VQVAEAttentionBlock(config))\n+                    attn_norms.append(nn.GroupNorm(num_channels=block_in, num_groups=32, eps=1e-6, affine=True))\n+\n+            down = nn.Module()\n+            down.block = block\n+            down.attn = attn\n+            down.attn_norms = attn_norms\n+            if i_level != self.num_resolutions - 1:\n+                down.downsample = Emu3VQVAEEncoderConvDownsample(block_in)\n+            self.down.append(down)\n+\n+    def forward(self, hidden_states: torch.FloatTensor):\n+        for i_level, blocks in enumerate(self.down):\n+            for i_block in range(self.num_res_blocks):\n+                hidden_states = blocks.block[i_block](hidden_states)\n+                if len(blocks.attn) > 0:\n+                    residual = hidden_states\n+                    hidden_states = blocks.attn_norms[i_block](hidden_states)\n+\n+                    batch_size, channels, height, width = hidden_states.shape\n+                    hidden_states = hidden_states.view(batch_size, channels, height * width).transpose(1, 2)\n+                    hidden_states = blocks.attn[i_block](hidden_states)[0]\n+\n+                    hidden_states = hidden_states.reshape(batch_size, height, width, channels).permute(0, 3, 1, 2)\n+                    hidden_states = residual + hidden_states\n+\n+            if i_level != self.num_resolutions - 1:\n+                hidden_states = blocks.downsample(hidden_states)\n+\n+        return hidden_states\n+\n+\n+class Emu3VQVAEUpBlock(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+\n+        self.num_resolutions = len(config.channel_multiplier)\n+        self.num_res_blocks = config.num_res_blocks\n+\n+        quant_channels = config.embed_dim\n+        block_in = config.base_channels * config.channel_multiplier[-1]\n+\n+        self.up = nn.ModuleList()\n+        for i_level in reversed(range(self.num_resolutions)):\n+            block = nn.ModuleList()\n+            attn = nn.ModuleList()\n+            attn_norms = nn.ModuleList()\n+            block_out = config.base_channels * config.channel_multiplier[i_level]\n+            for i_block in range(self.num_res_blocks + 1):\n+                block.append(\n+                    Emu3VQVAEResnetBlock(\n+                        in_channels=block_in,\n+                        out_channels=block_out,\n+                        quant_channels=quant_channels,\n+                    )\n+                )\n+                block_in = block_out\n+                if i_level in config.attn_resolutions:\n+                    attn.append(Emu3VQVAEAttentionBlock(config))\n+                    attn_norms.append(Emu3VQVAESpatialNorm(quant_channels, block_in))\n+\n+            up = nn.Module()\n+            up.block = block\n+            up.attn = attn\n+            up.attn_norms = attn_norms\n+            if i_level != 0:\n+                up.upsample = Emu3VQVAEEncoderConvUpsample(block_in)\n+\n+            self.up.insert(0, up)\n+\n+    def forward(self, hidden_states: torch.FloatTensor, quant_states: torch.FloatTensor):\n+        for i_level, blocks in enumerate(self.up[::-1]):\n+            for i_block in range(self.num_res_blocks + 1):\n+                hidden_states = blocks.block[i_block](hidden_states, quant_states)\n+                if len(blocks.attn) > 0:\n+                    residual = hidden_states\n+                    hidden_states = blocks.attn_norms[i_block](hidden_states, quant_states)\n+\n+                    batch_size, channels, height, width = hidden_states.shape\n+                    hidden_states = hidden_states.view(batch_size, channels, height * width).transpose(1, 2)\n+                    hidden_states = blocks.attn[i_block](hidden_states)[0]\n+\n+                    hidden_states = hidden_states.reshape(batch_size, height, width, channels).permute(0, 3, 1, 2)\n+                    hidden_states = residual + hidden_states\n+            if i_level != len(self.up) - 1:\n+                hidden_states = blocks.upsample(hidden_states)\n+\n+        return hidden_states\n+\n+\n+class Emu3VQVAEEncoder(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+\n+        base_channels = config.base_channels\n+        in_channels = config.in_channels\n+        double_latent = config.double_latent\n+        latent_channels = config.latent_channels\n+        channel_multiplier = config.channel_multiplier\n+        out_channels = 2 * latent_channels if double_latent else latent_channels\n+        block_in = base_channels * channel_multiplier[-1]\n+\n+        self.conv_in = torch.nn.Conv2d(in_channels, base_channels, kernel_size=3, stride=1, padding=1)\n+        self.down_block = Emu3VQVAEDownBlock(config)\n+        self.middle_block = Emu3VQVAEMiddleBlock(config, block_in)\n+\n+        self.norm_out = torch.nn.GroupNorm(num_groups=32, num_channels=block_in, eps=1e-6, affine=True)\n+        self.conv_out = torch.nn.Conv2d(\n+            block_in,\n+            out_channels,\n+            kernel_size=3,\n+            stride=1,\n+            padding=1,\n+        )\n+\n+        temporal_down_blocks = int(math.log2(config.temporal_downsample_factor))\n+        self.time_conv = nn.ModuleList()\n+        self.time_res_stack = nn.ModuleList()\n+\n+        for i in range(temporal_down_blocks):\n+            conv = Emu3VQVAETemporalDownsample(out_channels, out_channels)\n+            self.time_conv.append(conv)\n+\n+        for _ in range(config.num_res_blocks):\n+            time_res_conv = Emu3VQVAETemporalResnetBlock(\n+                in_channels=out_channels,\n+                out_channels=out_channels,\n+            )\n+            self.time_res_stack.append(time_res_conv)\n+\n+    def forward(self, pixel_values: torch.LongTensor):\n+        temporal_dim = pixel_values.shape[1]\n+        pixel_values = pixel_values.reshape(-1, *pixel_values.shape[2:])\n+\n+        # downsampling & middle\n+        hidden_states = self.conv_in(pixel_values)\n+        hidden_states = self.down_block(hidden_states)\n+        hidden_states = self.middle_block(hidden_states)\n+\n+        # end\n+        hidden_states = self.norm_out(hidden_states)\n+        hidden_states *= torch.sigmoid(hidden_states)\n+        hidden_states = self.conv_out(hidden_states)\n+\n+        hidden_states = hidden_states.reshape(-1, temporal_dim, *hidden_states.shape[1:])\n+        hidden_states = hidden_states.permute(0, 2, 1, 3, 4)\n+\n+        # temporal convs\n+        for conv in self.time_conv:\n+            hidden_states = conv(hidden_states)\n+            hidden_states *= torch.sigmoid(hidden_states)\n+\n+        for layer in self.time_res_stack:\n+            hidden_states = layer(hidden_states)\n+\n+        hidden_states = hidden_states.permute(0, 2, 1, 3, 4)\n+\n+        return hidden_states\n+\n+\n+class Emu3VQVAEDecoder(nn.Module):\n+    def __init__(self, config: Emu3VQVAEConfig):\n+        super().__init__()\n+\n+        quant_channels = config.embed_dim\n+        block_in = config.base_channels * config.channel_multiplier[-1]\n+        self.time_res_stack = nn.ModuleList()\n+        for _ in range(config.num_res_blocks):\n+            time_res_conv = Emu3VQVAETemporalResnetBlock(\n+                in_channels=config.latent_channels, out_channels=config.latent_channels\n+            )\n+            self.time_res_stack.append(time_res_conv)\n+\n+        temp_upsample_block_num = int(math.log2(config.temporal_downsample_factor))\n+        self.time_conv = nn.ModuleList()\n+        for i in range(temp_upsample_block_num):\n+            conv = Emu3VQVAETemporalUpsample(config.latent_channels, config.latent_channels)\n+            self.time_conv.append(conv)\n+\n+        self.conv_in = nn.Conv2d(\n+            config.latent_channels,\n+            block_in,\n+            kernel_size=3,\n+            stride=1,\n+            padding=1,\n+        )\n+\n+        self.middle_block = Emu3VQVAEMiddleBlock(config, block_in, quant_channels=quant_channels)\n+        self.up_block = Emu3VQVAEUpBlock(config)\n+\n+        block_in = config.base_channels * config.channel_multiplier[0]\n+        self.norm_out = Emu3VQVAESpatialNorm(quant_channels, block_in)\n+        self.conv_out = nn.Conv2d(\n+            block_in,\n+            config.out_channels,\n+            kernel_size=3,\n+            stride=1,\n+            padding=1,\n+        )\n+\n+    def forward(self, hidden_states: torch.Tensor, quant_states: torch.Tensor):\n+        hidden_quant_states = torch.cat((hidden_states, quant_states), dim=0)\n+        hidden_quant_states = hidden_quant_states.permute(0, 2, 1, 3, 4)\n+\n+        # temporal convs\n+        for layer in self.time_res_stack:\n+            hidden_quant_states = layer(hidden_quant_states)\n+\n+        for layer in self.time_conv:\n+            hidden_quant_states = layer(hidden_quant_states)\n+            hidden_quant_states *= torch.sigmoid(hidden_quant_states)\n+\n+        hidden_quant_states = hidden_quant_states.permute(0, 2, 1, 3, 4)\n+        hidden_states, quant_states = torch.chunk(hidden_quant_states, 2, dim=0)\n+        hidden_states = hidden_states.reshape(-1, *hidden_states.shape[2:])\n+        quant_states = quant_states.reshape(-1, *quant_states.shape[2:])\n+\n+        hidden_states = self.conv_in(hidden_states)\n+\n+        # middle & upsampling\n+        hidden_states = self.middle_block(hidden_states, quant_states)\n+        hidden_states = self.up_block(hidden_states, quant_states)\n+\n+        hidden_states = self.norm_out(hidden_states, quant_states)\n+        hidden_states *= torch.sigmoid(hidden_states)\n+        hidden_states = self.conv_out(hidden_states)\n+\n+        return hidden_states\n+\n+\n+EMU3_VQ_START_DOCSTRING = r\"\"\"\n+    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n+    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n+    etc.)\n+\n+    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n+    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n+    and behavior.\n+\n+    Parameters:\n+        config ([`Emu3VQVAEConfig`]):\n+            Model configuration class with all the parameters of the model. Initializing with a config file does not\n+            load the weights associated with the model, only the configuration. Check out the\n+            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"\"\"The VQ-VAE model used in Emu3 for encoding/decoding images into discrete tokens.\n+    This model follows the \"Make-a-scene: Scene-based text-to-image generation with human priors\" paper from\n+    [ Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman](https://arxiv.org/abs/2203.13131).\n+    \"\"\",\n+    EMU3_VQ_START_DOCSTRING,\n+)\n+class Emu3VQVAE(PreTrainedModel):\n+    config_class = Emu3VQVAEConfig\n+    base_model_prefix = \"emuvideovq\"\n+    main_input_name = \"pixel_values\"\n+    _no_split_modules = [\n+        \"Emu3VQVAETemporalResnetBlock\",\n+        \"Emu3VQVAEAttentionBlock\",\n+        \"Emu3VQVAEResnetBlock\",\n+        \"Emu3VQVAEVectorQuantizer\",\n+    ]\n+\n+    def _init_weights(self, module):\n+        if isinstance(module, (nn.Conv2d, nn.Conv3d)):\n+            nn.init.kaiming_normal_(module.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n+        elif isinstance(module, nn.Linear):\n+            nn.init.kaiming_uniform_(module.weight, a=math.sqrt(5))\n+            if module.bias is not None:\n+                fan_in, _ = nn.init._calculate_fan_in_and_fan_out(module.weight)\n+                bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n+                nn.init.uniform_(module.bias, -bound, bound)\n+        elif isinstance(module, (nn.BatchNorm2d, nn.BatchNorm3d, nn.GroupNorm)):\n+            nn.init.constant_(module.weight, 1)\n+            nn.init.constant_(module.bias, 0)\n+\n+    def __init__(self, config: Emu3VQVAEConfig):\n+        super().__init__(config)\n+\n+        self.config = config\n+\n+        self.encoder = Emu3VQVAEEncoder(config)\n+        self.decoder = Emu3VQVAEDecoder(config)\n+        self.quantize = Emu3VQVAEVectorQuantizer(config)\n+        self.vision_spatial_factor = 2 ** (len(config.channel_multiplier) - 1)\n+\n+        self.quant_conv = Emu3VQVAEConv3d(\n+            config.latent_channels, config.embed_dim, kernel_size=(3, 1, 1), stride=(1, 1, 1)\n+        )\n+        self.post_quant_conv = Emu3VQVAEConv3d(\n+            config.embed_dim, config.latent_channels, kernel_size=(3, 1, 1), stride=(1, 1, 1)\n+        )\n+        self.spatial_scale_factor = 2 ** (len(config.channel_multiplier) - 1)\n+        self.eval()  # Emu3's VQ model is frozen\n+\n+        self.post_init()\n+\n+    def encode(self, pixel_values: torch.Tensor, image_sizes: torch.Tensor):\n+        is_image = pixel_values.ndim == 4\n+        if is_image:\n+            temporal = self.config.temporal_downsample_factor\n+            batch_size, channels, height, width = pixel_values.shape\n+            pixel_values = pixel_values.unsqueeze(1).repeat(1, temporal, 1, 1, 1)\n+        else:\n+            batch_size, temporal, channels, height, width = pixel_values.shape\n+\n+        hidden_states = self.encoder(pixel_values)\n+\n+        # b t c h w -> b c t h w\n+        hidden_states = hidden_states.permute(0, 2, 1, 3, 4)\n+        hidden_states = self.quant_conv(hidden_states)\n+\n+        # b c t h w -> b t c h w\n+        hidden_states = hidden_states.permute(0, 2, 1, 3, 4)\n+        codes = self.quantize(hidden_states)\n+\n+        image_tokens = codes.squeeze(1) if is_image else codes\n+\n+        image_tokens = [\n+            single_image[: int(size[0] / self.vision_spatial_factor), : int(size[1] / self.vision_spatial_factor)]\n+            for single_image, size in zip(image_tokens, image_sizes)\n+        ]\n+\n+        return image_tokens\n+\n+    def decode(self, hidden_states: torch.Tensor):\n+        is_image = hidden_states.ndim == 3\n+        if is_image:\n+            hidden_states = hidden_states.unsqueeze(1)\n+\n+        batch_size, temporal, height, width = hidden_states.shape\n+        quant = self.quantize.embedding(hidden_states.flatten())\n+\n+        channels = quant.shape[-1]\n+        quant = quant.view(batch_size, temporal, height, width, channels).permute(0, 4, 1, 2, 3).contiguous()\n+        post_quant = self.post_quant_conv(quant)\n+\n+        quant = quant.permute(0, 2, 1, 3, 4)\n+        post_quant = post_quant.permute(0, 2, 1, 3, 4)\n+\n+        video = self.decoder(post_quant, quant)\n+        video = video.reshape(\n+            batch_size,\n+            temporal * self.config.temporal_downsample_factor,\n+            self.config.out_channels,\n+            height * self.spatial_scale_factor,\n+            width * self.spatial_scale_factor,\n+        )\n+        return video[:, 0] if is_image else video\n+\n+\n+class Emu3ImageVocabularyMapping:\n+    \"\"\"\n+    A class for mapping discrete image tokens from VQGAN to BPE tokens.\n+    \"\"\"\n+\n+    def __init__(self, vocab_map):\n+        self.vocab_map = vocab_map\n+        self.eol_token_id = vocab_map.get(\"<|extra_200|>\")\n+        self.image_token_id = vocab_map.get(\"<image>\")\n+\n+    @cached_property\n+    def image_tokens(self):\n+        return sorted([val for name, val in self.vocab_map.items() if name.startswith(\"<|visual token\")])\n+\n+    @cached_property\n+    def image_tokens_str(self):\n+        return sorted([name for name, val in self.vocab_map.items() if name.startswith(\"<|visual token\")])\n+\n+    @cached_property\n+    def img2bpe(self):\n+        return {int(token[-8:-2]): self.vocab_map[token] for token in self.image_tokens_str}\n+\n+    @cached_property\n+    def bpe2img(self):\n+        return {v: k for k, v in self.img2bpe.items()}\n+\n+    @cached_property\n+    def bpe2img_mapping_tensor(self):\n+        mapping = torch.zeros(max(self.bpe2img.keys()) + 1, dtype=torch.int)\n+        for k, v in self.bpe2img.items():\n+            mapping[k] = v\n+        return mapping\n+\n+    @cached_property\n+    def img2bpe_mapping_tensor(self):\n+        mapping = torch.zeros(max(self.img2bpe.keys()) + 1, dtype=torch.int)\n+        for k, v in self.img2bpe.items():\n+            mapping[k] = v\n+        return mapping\n+\n+    def convert_img2bpe(self, img_batch: List[torch.Tensor]) -> torch.Tensor:\n+        device = img_batch.device\n+        eol_row = torch.ones((img_batch.shape[0], 1), dtype=torch.int) * self.eol_token_id\n+        img_tokens = self.img2bpe_mapping_tensor[img_batch.to(\"cpu\")]\n+        img_tokens = torch.cat([img_tokens, eol_row], dim=-1)\n+        return img_tokens.to(device)\n+\n+    def convert_bpe2img(self, img_batch: torch.Tensor) -> torch.Tensor:\n+        device = img_batch.device\n+        img_batch = img_batch[..., :-1]  # remove last row of EOL tokens\n+        img_tokens = self.bpe2img_mapping_tensor[img_batch.to(\"cpu\")]\n+        return img_tokens.to(device)\n+\n+\n+class Emu3PreTrainedModel(ChameleonPreTrainedModel, Emu3VQVAE):\n+    _no_split_modules = [\n+        \"Emu3DecoderLayer\",\n+    ]\n+    _supports_flex_attn = True\n+\n+    def _init_weights(self, module):\n+        std = self.config.get_text_config().initializer_range\n+        if isinstance(module, Emu3VQVAE):\n+            module.apply(module._init_weights)\n+        elif isinstance(module, (nn.Linear, nn.Conv2d)):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+\n+\n+EMU3_TEXT_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n+            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n+            it.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n+\n+            - 1 for tokens that are **not masked**,\n+            - 0 for tokens that are **masked**.\n+\n+            [What are attention masks?](../glossary#attention-mask)\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see\n+            `past_key_values`).\n+\n+            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n+            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n+            information on the default strategy.\n+\n+            - 1 indicates the head is **not masked**,\n+            - 0 indicates the head is **masked**.\n+        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n+            config.n_positions - 1]`.\n+\n+            [What are position IDs?](../glossary#position-ids)\n+        past_key_values (`Cache`, *optional*):\n+            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n+            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n+            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n+\n+            Has to be an instance of [`~cache_utils.Cache`] instance, see our\n+            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n+\n+            The model will output the same cache type that is fed as input. If no `past_key_values` are passed, the\n+            legacy cache format will be returned.\n+\n+            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n+            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n+            of shape `(batch_size, sequence_length)`.\n+        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n+            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n+            model's internal embedding lookup matrix.\n+        use_cache (`bool`, *optional*):\n+            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n+            `past_key_values`).\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n+            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n+            the complete sequence length.\n+\"\"\"\n+\n+\n+EMU3_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n+            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n+            it.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n+        pixel_values (`torch.FloatTensor` of shape `(batch_size, max_num_images, max_num_tiles, channels, image_size, image_size)):\n+            The tensors corresponding to the input images. Pixel values can be obtained using\n+            [`AutoImageProcessor`]. See [`Emu3ImageProcessor.__call__`] for details ([]`Emu3Processor`] uses\n+            [`Emu3ImageProcessor`] for processing images).\n+        image_sizes (`torch.LongTensor` of shape `(batch_size, 2)`):\n+                The sizes of the images in the batch, being (height, width) for each image. Image sizes can be obtained using\n+            [`AutoImageProcessor`]. See [`Emu3ImageProcessor.__call__`] for details ([]`Emu3Processor`] uses\n+            [`Emu3ImageProcessor`] for processing images).\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n+\n+            - 1 for tokens that are **not masked**,\n+            - 0 for tokens that are **masked**.\n+\n+            [What are attention masks?](../glossary#attention-mask)\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see\n+            `past_key_values`).\n+\n+            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n+            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n+            information on the default strategy.\n+\n+            - 1 indicates the head is **not masked**,\n+            - 0 indicates the head is **masked**.\n+        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n+            config.n_positions - 1]`.\n+\n+            [What are position IDs?](../glossary#position-ids)\n+        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):\n+            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n+            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n+            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n+\n+            Has to be an instance of [`~cache_utils.Cache`] instance, see our\n+            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n+\n+            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the\n+            legacy cache format will be returned.\n+\n+            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n+            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n+            of shape `(batch_size, sequence_length)`.\n+        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n+            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n+            model's internal embedding lookup matrix.\n+        use_cache (`bool`, *optional*):\n+            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n+            `past_key_values`).\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n+            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n+            the complete sequence length.\n+\"\"\"\n+\n+\n+class Emu3TextModel(LlamaModel, Emu3PreTrainedModel):\n+    def __init__(self, config: Emu3Config):\n+        super().__init__(config)\n+        self.layers = nn.ModuleList(\n+            [Emu3DecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+\n+\n+class Emu3ForCausalLM(LlamaForCausalLM, Emu3PreTrainedModel, GenerationMixin):\n+    config_class = Emu3TextConfig\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.model = Emu3TextModel(config)\n+\n+    @add_start_docstrings_to_model_forward(EMU3_TEXT_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=\"Emu3TextConfig\")\n+    def forward(**super_kwargs):\n+        r\"\"\"\n+        Args:\n+            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+            num_logits_to_keep (`int`, *optional*):\n+                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n+                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+\n+        Returns:\n+\n+        Example:\n+\n+        ```python\n+        >>> from transformers import Emu3Processor, Emu3ForConditionalGeneration\n+        >>> import torch\n+        >>> import requests\n+        >>> from PIL import Image\n+\n+        >>> model = Emu3ForCausalLM.from_pretrained(\"Emu3-community/Emu3-Chat-hf\", torch_dtype=torch.bfloat16)\n+        >>> processor = Emu3Processor.from_pretrained(\"Emu3-community/Emu3-Chat-hf\")\n+\n+        >>> inputs = processor(text=[\"Can you write me a poem about winter.\"], return_tensors=\"pt\").to(model.device)\n+\n+        >>> generated_ids = model.generate(**inputs, max_new_tokens=100, do_sample=False)\n+        >>> processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n+        ```\"\"\"\n+        super().forward()\n+\n+\n+class Emu3ForConditionalGeneration(Emu3PreTrainedModel, GenerationMixin):\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.text_model = Emu3ForCausalLM._from_config(config.text_config)\n+        self.vqmodel = Emu3VQVAE(config.vq_config)\n+        self.vocabulary_mapping = Emu3ImageVocabularyMapping(config.vocabulary_map)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.text_model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.text_model.set_input_embeddings(value)\n+\n+    def get_image_tokens(self, pixel_values: torch.FloatTensor, image_sizes: torch.LongTensor):\n+        \"\"\"\n+        Tokenizes images into discrete tokens with VQGAN module. Converts\n+        obtained image tokens into BPE tokens and wraps with \"boi\" and \"eoi\"\n+        special tokens.\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)`):\n+                The tensors corresponding to the input images.\n+            image_sizes (`torch.LongTensor` of shape `(batch_size, 2)`):\n+                The sizes of the images in the batch, being (height, width) for each image.\n+        \"\"\"\n+        image_tokens_list = self.vqmodel.encode(pixel_values, image_sizes)\n+        bpe_tokens_list = [self.vocabulary_mapping.convert_img2bpe(tokens).flatten() for tokens in image_tokens_list]\n+        bpe_tokens = torch.cat(bpe_tokens_list)\n+        return bpe_tokens\n+\n+    @torch.no_grad\n+    def decode_image_tokens(self, image_tokens: torch.LongTensor, height: int, width: int):\n+        \"\"\"\n+        Decodes generated image tokens from language model to continuous pixel values\n+        with VQGAN module via upsampling.\n+\n+        Args:\n+            image_tokens (`torch.LongTensor` of shape `(batch_size, num_of_tokens)`):\n+                The tensors corresponding to the input images.\n+            height (`int`):\n+                Height of the generated image before upsampling.\n+            width (`int`):\n+                Width of the generated image before upsampling.\n+        \"\"\"\n+        sequences = image_tokens[:, :-3].view(-1, height, width + 1)\n+        image_tokens = self.vocabulary_mapping.convert_bpe2img(sequences)\n+        image = self.vqmodel.decode(image_tokens)\n+        return image\n+\n+    @add_start_docstrings_to_model_forward(EMU3_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n+        image_sizes: torch.Tensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        num_logits_to_keep: int = 0,\n+    ) -> Union[Tuple, CausalLMOutputWithPast]:\n+        r\"\"\"\n+        Args:\n+            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+            num_logits_to_keep (`int`, *optional*):\n+                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n+                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+\n+        Returns:\n+\n+        Example:\n+\n+        ```python\n+        >>> from transformers import Emu3Processor, Emu3ForConditionalGeneration\n+        >>> import torch\n+        >>> import requests\n+        >>> from PIL import Image\n+\n+        >>> model = Emu3ForConditionalGeneration.from_pretrained(\"Emu3-community/Emu3-Chat-hf\", torch_dtype=torch.bfloat16)\n+        >>> processor = Emu3Processor.from_pretrained(\"Emu3-community/Emu3-Chat-hf\")\n+\n+        >>> conversation = [\n+        ...     {\n+        ...     \"role\": \"system\",\n+        ...     \"content\": [\n+        ...         {\"type\": \"text\", \"text\": \"You are a helpful assistant.\"},\n+        ...         ],\n+        ...     },\n+        ...     {\n+        ...     \"role\": \"user\",\n+        ...     \"content\": [\n+        ...         {\"type\": \"image\"},\n+        ...         {\"type\": \"text\", \"text\": \"Please describe the image.\"},\n+        ...         ],\n+        ...     },\n+        ... ]\n+\n+        >>> prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n+        >>> image = Image.open(requests.get(\"https://www.ilankelman.org/stopsigns/australia.jpg\", stream=True).raw)\n+\n+        >>> inputs = processor(images=[image], text=[prompt], return_tensors=\"pt\").to(model.device, torch.bfloat16)\n+\n+        >>> generated_ids = model.generate(**inputs, max_new_tokens=100, do_sample=False)\n+        >>> processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n+        ```\"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\n+                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n+            )\n+\n+        if pixel_values is not None and inputs_embeds is not None:\n+            raise ValueError(\n+                \"You cannot specify both pixel_values and inputs_embeds at the same time, and must specify either one\"\n+            )\n+\n+        if pixel_values is not None:\n+            image_tokens = self.get_image_tokens(pixel_values, image_sizes)\n+            special_image_mask = input_ids == self.vocabulary_mapping.image_token_id\n+            image_tokens = image_tokens.to(input_ids.device, input_ids.dtype)\n+            input_ids = input_ids.masked_scatter(special_image_mask, image_tokens)\n+\n+        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n+        outputs = self.text_model(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+            cache_position=cache_position,\n+            num_logits_to_keep=num_logits_to_keep,\n+        )\n+\n+        return outputs\n+\n+\n+__all__ = [\n+    \"Emu3ForConditionalGeneration\",\n+    \"Emu3ForCausalLM\",\n+    \"Emu3TextModel\",\n+    \"Emu3PreTrainedModel\",\n+    \"Emu3VQVAE\",\n+]"
        },
        {
            "sha": "2c536f5f24636f6fbb2447a5fb9b21d1d547f2af",
            "filename": "src/transformers/models/emu3/processing_emu3.py",
            "status": "added",
            "additions": 217,
            "deletions": 0,
            "changes": 217,
            "blob_url": "https://github.com/huggingface/transformers/blob/52e1f87c7d2fdaf147aa625c169ad440672cb3cb/src%2Ftransformers%2Fmodels%2Femu3%2Fprocessing_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52e1f87c7d2fdaf147aa625c169ad440672cb3cb/src%2Ftransformers%2Fmodels%2Femu3%2Fprocessing_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fprocessing_emu3.py?ref=52e1f87c7d2fdaf147aa625c169ad440672cb3cb",
            "patch": "@@ -0,0 +1,217 @@\n+# coding=utf-8\n+# Copyright 2024 HuggingFace Inc. team. All rights reserved.\n+#\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import List, Optional, Union\n+\n+from ...image_processing_utils import BatchFeature\n+from ...image_utils import ImageInput\n+from ...processing_utils import ImagesKwargs, ProcessingKwargs, ProcessorMixin, TextKwargs, Unpack\n+from ...tokenization_utils_base import PreTokenizedInput, TextInput\n+\n+\n+class Emu3TextKwargs(TextKwargs, total=False):\n+    return_for_image_generation: bool\n+\n+\n+class Emu3ImagesKwargs(ImagesKwargs, total=False):\n+    ratio: str\n+    image_area: int\n+\n+\n+class Emu3ProcessorKwargs(ProcessingKwargs, total=False):\n+    text_kwargs: Emu3TextKwargs\n+    images_kwargs: Emu3ImagesKwargs\n+    _defaults = {\n+        \"text_kwargs\": {\n+            \"return_for_image_generation\": False,\n+        },\n+        \"images_kwargs\": {\n+            \"ratio\": \"1:1\",\n+            \"image_area\": 518400,\n+        },\n+    }\n+\n+\n+class Emu3Processor(ProcessorMixin):\n+    r\"\"\"\n+    Constructs a Emu3 processor which wraps a Emu3 image processor and a GPT2 tokenizer into a single\n+    processor.\n+\n+    [`Emu3Processor`] offers all the functionalities of [`Emu3ImageProcessor`] and [`GPT2TokenizerFast`].\n+    See the [`~Emu3Processor.__call__`] and [`~Emu3Processor.decode`] for more information.\n+\n+    Args:\n+        image_processor ([`Emu3ImageProcessor`]):\n+            The image processor is a required input.\n+        tokenizer ([`Emu3TokenizerFast`]):\n+            The tokenizer is a required input.\n+        chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n+            in a chat into a tokenizable string.\n+    \"\"\"\n+\n+    attributes = [\"image_processor\", \"tokenizer\"]\n+    tokenizer_class = (\"GPT2Tokenizer\", \"GPT2TokenizerFast\")\n+    image_processor_class = \"Emu3ImageProcessor\"\n+\n+    def __init__(\n+        self,\n+        image_processor,\n+        tokenizer,\n+        chat_template=None,\n+        **kwargs,\n+    ):\n+        self.image_token = tokenizer.image_token  # image_token as placeholder to be replaced by vq-vae tokens\n+        self.image_start_token = tokenizer.boi_token  # \"<|image start|>\" fixed tokens for start and end of image\n+        self.image_end_token = tokenizer.eoi_token  # \"<|image end|>\"\n+        self.fake_token_around_image = tokenizer.image_wrapper_token  # \"<|image token|>\"  every image starts with it\n+        self.eof_token = tokenizer.eof_token  # \"<|extra_201|>\"\n+        self.bos_token = tokenizer.bos_token\n+        self.downsample_ratio = 8\n+        super().__init__(image_processor, tokenizer, chat_template=chat_template)\n+\n+    def __call__(\n+        self,\n+        images: Optional[ImageInput] = None,\n+        text: Optional[Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]]] = None,\n+        audio=None,\n+        videos=None,\n+        **kwargs: Unpack[Emu3ProcessorKwargs],\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n+        and `kwargs` arguments to Emu3TokenizerFast's [`~Emu3TokenizerFast.__call__`] if `text` is not `None` to encode\n+        the text. To prepare the image(s), this method forwards the `images` and `kwrags` arguments to\n+        CLIPImageProcessor's [`~CLIPImageProcessor.__call__`] if `images` is not `None`. Please refer to the doctsring\n+        of the above two methods for more information.\n+\n+        Args:\n+            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):\n+                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n+                tensor. Both channels-first and channels-last formats are supported.\n+            text (`str`, `List[str]`, `List[List[str]]`):\n+                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n+                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n+                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n+            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n+                If set, will return tensors of a particular framework. Acceptable values are:\n+\n+                - `'tf'`: Return TensorFlow `tf.constant` objects.\n+                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n+                - `'np'`: Return NumPy `np.ndarray` objects.\n+                - `'jax'`: Return JAX `jnp.ndarray` objects.\n+\n+        Returns:\n+            [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n+\n+            - **input_ids** -- List of token ids to be fed to a model. Returned when `text` is not `None`.\n+            - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n+              `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names` and if `text` is not\n+              `None`).\n+            - **pixel_values** -- Pixel values to be fed to a model. Returned when `images` is not `None`.\n+        \"\"\"\n+        # check if images and text inputs are reversed for BC\n+\n+        if isinstance(text, str):\n+            text = [text]\n+        elif not isinstance(text, list) and not isinstance(text[0], str):\n+            raise TypeError(\"Invalid input text. Please provide a string, or a list of strings\")\n+\n+        output_kwargs = self._merge_kwargs(\n+            Emu3ProcessorKwargs,\n+            tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n+            **kwargs,\n+        )\n+        return_for_image_generation = output_kwargs[\"text_kwargs\"].pop(\"return_for_image_generation\", False)\n+        ratio = output_kwargs[\"images_kwargs\"].pop(\"ratio\", None)\n+        image_area = output_kwargs[\"images_kwargs\"].pop(\"image_area\", None)\n+\n+        if return_for_image_generation and images is not None:\n+            raise ValueError(\"You should not provide `images` when `return_for_image_generation=True`\")\n+\n+        if not return_for_image_generation and text is None and images is None:\n+            raise ValueError(\"You must provide either text or images when `return_for_image_generation=False`\")\n+\n+        image_features = {}\n+        image_start_tokens = f\"{self.image_start_token}\"\n+        image_end_tokens = f\"{self.eof_token}{self.image_end_token}\"\n+\n+        # generate text from image + text input, so we add placeholders for image tokens\n+        if not return_for_image_generation and images is not None:\n+            image_features = self.image_processor(images, **output_kwargs[\"images_kwargs\"])\n+            image_sizes = iter(image_features.image_sizes)\n+\n+            prompt_strings = []\n+            for sample in text:\n+                while self.image_token in sample:\n+                    image_size = next(image_sizes)\n+                    height, width = image_size\n+                    height = height // self.downsample_ratio\n+                    width = width // self.downsample_ratio\n+                    image_seq_length = height * (width + 1)  # +1 for extra row when converting to BPE in modeling code\n+\n+                    image_placeholder = f\"{image_start_tokens}{height}*{width}{self.fake_token_around_image}{'<placeholder>' * image_seq_length}{image_end_tokens}\"\n+                    sample = sample.replace(self.image_token, image_placeholder, 1)\n+                    sample = f\"{self.bos_token}{sample}\"  # add BOS because PT tokenizer doesn't add it\n+                prompt_strings.append(sample)\n+            text = [sample.replace(\"<placeholder>\", self.image_token) for sample in prompt_strings]\n+\n+        # generate image from text input, so we add begin-of-image tokens from where image generation starts\n+        elif return_for_image_generation:\n+            height, width = self.calculate_generate_size(ratio, image_area, self.downsample_ratio)\n+            image_prompt = f\"{image_start_tokens}{height}*{width}{self.fake_token_around_image}\"\n+            text = [f\"{self.bos_token}{sample}{image_prompt}\" for sample in text]\n+            image_features[\"image_sizes\"] = [[height, width]] * len(text)\n+\n+        # else just generate from text-only input, and we do no special treatment for text\n+        data = self.tokenizer(text, **output_kwargs[\"text_kwargs\"])\n+        data.update(**image_features)\n+\n+        return BatchFeature(data=data, tensor_type=output_kwargs[\"common_kwargs\"][\"return_tensors\"])\n+\n+    def calculate_generate_size(self, ratio, image_area, spatial_factor):\n+        width, height = map(int, ratio.split(\":\"))\n+        current_area = width * height\n+        target_ratio = (image_area / current_area) ** 0.5\n+\n+        token_height = int(round(height * target_ratio / spatial_factor))\n+        token_width = int(round(width * target_ratio / spatial_factor))\n+        return token_height, token_width\n+\n+    def postprocess(self, images: ImageInput, **kwargs):\n+        return self.image_processor.postprocess(images, **kwargs)\n+\n+    def batch_decode(self, *args, **kwargs):\n+        \"\"\"\n+        This method forwards all its arguments to Emu3TokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n+        refer to the docstring of this method for more information.\n+        \"\"\"\n+        return self.tokenizer.batch_decode(*args, **kwargs)\n+\n+    def decode(self, *args, **kwargs):\n+        \"\"\"\n+        This method forwards all its arguments to Emu3TokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n+        the docstring of this method for more information.\n+        \"\"\"\n+        return self.tokenizer.decode(*args, **kwargs)\n+\n+    @property\n+    def model_input_names(self):\n+        tokenizer_input_names = self.tokenizer.model_input_names\n+        image_processor_input_names = self.image_processor.model_input_names\n+        return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n+\n+\n+__all__ = [\"Emu3Processor\"]"
        },
        {
            "sha": "6a9cd232eb35dd0658722bf292f0edad491380cb",
            "filename": "src/transformers/utils/dummy_pt_objects.py",
            "status": "modified",
            "additions": 35,
            "deletions": 0,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/52e1f87c7d2fdaf147aa625c169ad440672cb3cb/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52e1f87c7d2fdaf147aa625c169ad440672cb3cb/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py?ref=52e1f87c7d2fdaf147aa625c169ad440672cb3cb",
            "patch": "@@ -3933,6 +3933,41 @@ def load_tf_weights_in_electra(*args, **kwargs):\n     requires_backends(load_tf_weights_in_electra, [\"torch\"])\n \n \n+class Emu3ForCausalLM(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n+class Emu3ForConditionalGeneration(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n+class Emu3PreTrainedModel(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n+class Emu3TextModel(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n+class Emu3VQVAE(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n class EncodecModel(metaclass=DummyObject):\n     _backends = [\"torch\"]\n "
        },
        {
            "sha": "2fcc7f172054f9d9905199381dd64963222957a6",
            "filename": "src/transformers/utils/dummy_vision_objects.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/52e1f87c7d2fdaf147aa625c169ad440672cb3cb/src%2Ftransformers%2Futils%2Fdummy_vision_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52e1f87c7d2fdaf147aa625c169ad440672cb3cb/src%2Ftransformers%2Futils%2Fdummy_vision_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_vision_objects.py?ref=52e1f87c7d2fdaf147aa625c169ad440672cb3cb",
            "patch": "@@ -233,6 +233,13 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"vision\"])\n \n \n+class Emu3ImageProcessor(metaclass=DummyObject):\n+    _backends = [\"vision\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"vision\"])\n+\n+\n class FlavaFeatureExtractor(metaclass=DummyObject):\n     _backends = [\"vision\"]\n "
        },
        {
            "sha": "ad2aeacbfebd1580d23125ef308aaed8e3ab4b51",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 17,
            "deletions": 1,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/52e1f87c7d2fdaf147aa625c169ad440672cb3cb/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52e1f87c7d2fdaf147aa625c169ad440672cb3cb/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=52e1f87c7d2fdaf147aa625c169ad440672cb3cb",
            "patch": "@@ -1626,7 +1626,7 @@ def test_generate_from_inputs_embeds(self, _, num_beams):\n             #   checks without adding test complexity. Ditto for `pixel_values_videos` and `pixel_values_images`\n             pixel_values_is_mutually_exclusive = any(\n                 model_name in model_class.__name__.lower()\n-                for model_name in [\"llava\", \"idefics2\", \"idefics3\", \"mllama\", \"paligemma\"]\n+                for model_name in [\"llava\", \"idefics2\", \"idefics3\", \"mllama\", \"paligemma\", \"emu3\"]\n             )\n             if pixel_values_is_mutually_exclusive:\n                 inputs_dict.pop(\"pixel_values\", None)\n@@ -1700,6 +1700,18 @@ def test_generate_from_inputs_embeds_with_static_cache(self):\n             if \"inputs_embeds\" not in inspect.signature(model.prepare_inputs_for_generation).parameters.keys():\n                 self.skipTest(reason=\"This model does not support `inputs_embeds` in generation\")\n \n+            #   Some VLMs assume `inputs_embeds` and `pixel_values` are mutually exclusive AND fall in the\n+            #   exception above (complex `inputs_embeds` computation). Popping `pixel_values` allow us to run the\n+            #   checks without adding test complexity. Ditto for `pixel_values_videos` and `pixel_values_images`\n+            pixel_values_is_mutually_exclusive = any(\n+                model_name in model_class.__name__.lower()\n+                for model_name in [\"llava\", \"idefics2\", \"idefics3\", \"mllama\", \"paligemma\", \"emu3\"]\n+            )\n+            if pixel_values_is_mutually_exclusive:\n+                inputs_dict.pop(\"pixel_values\", None)\n+                inputs_dict.pop(\"pixel_values_videos\", None)\n+                inputs_dict.pop(\"pixel_values_images\", None)\n+\n             input_ids = inputs_dict.pop(\"input_ids\")\n \n             model.config.use_cache = True\n@@ -1941,6 +1953,10 @@ def test_generate_with_static_cache(self):\n \n             for dtype in (torch.float32, torch.float16):\n                 model = model_class(config).to(torch_device).to(dtype).eval()\n+                inputs_dict = {\n+                    k: v.to(dtype) if isinstance(v, torch.Tensor) and torch.is_floating_point(v) else v\n+                    for k, v in inputs_dict.items()\n+                }\n                 set_model_for_less_flaky_test(model)\n \n                 generation_kwargs = {"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/emu3/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/52e1f87c7d2fdaf147aa625c169ad440672cb3cb/tests%2Fmodels%2Femu3%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52e1f87c7d2fdaf147aa625c169ad440672cb3cb/tests%2Fmodels%2Femu3%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Femu3%2F__init__.py?ref=52e1f87c7d2fdaf147aa625c169ad440672cb3cb"
        },
        {
            "sha": "d1c4501c5e8bd074d0eb55548c5bbb400e365e3a",
            "filename": "tests/models/emu3/test_modeling_emu3.py",
            "status": "added",
            "additions": 550,
            "deletions": 0,
            "changes": 550,
            "blob_url": "https://github.com/huggingface/transformers/blob/52e1f87c7d2fdaf147aa625c169ad440672cb3cb/tests%2Fmodels%2Femu3%2Ftest_modeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52e1f87c7d2fdaf147aa625c169ad440672cb3cb/tests%2Fmodels%2Femu3%2Ftest_modeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Femu3%2Ftest_modeling_emu3.py?ref=52e1f87c7d2fdaf147aa625c169ad440672cb3cb",
            "patch": "@@ -0,0 +1,550 @@\n+# coding=utf-8\n+# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch emu3 model.\"\"\"\n+\n+import unittest\n+\n+import numpy as np\n+import requests\n+from huggingface_hub import hf_hub_download\n+from parameterized import parameterized\n+\n+from transformers import Emu3Config, Emu3TextConfig, is_torch_available, is_vision_available, set_seed\n+from transformers.testing_utils import (\n+    require_bitsandbytes,\n+    require_torch,\n+    slow,\n+    torch_device,\n+)\n+\n+from ...generation.test_utils import GenerationTesterMixin\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor\n+from ...test_pipeline_mixin import PipelineTesterMixin\n+\n+\n+if is_vision_available():\n+    from PIL import Image\n+\n+if is_torch_available():\n+    import torch\n+\n+    from transformers import (\n+        Emu3ForCausalLM,\n+        Emu3ForConditionalGeneration,\n+        Emu3Processor,\n+        Emu3TextModel,\n+    )\n+\n+\n+class Emu3Text2TextModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=13,\n+        seq_length=7,\n+        is_training=False,\n+        vocab_size=99,\n+        hidden_size=32,\n+        num_hidden_layers=2,\n+        num_attention_heads=2,\n+        num_key_value_heads=2,\n+        intermediate_size=37,\n+        max_position_embeddings=512,\n+        initializer_range=0.02,\n+        pad_token_id=0,\n+        bos_token_id=1,\n+        eos_token_id=2,\n+    ):\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.seq_length = seq_length\n+        self.is_training = is_training\n+        self.vocab_size = vocab_size\n+        self.hidden_size = hidden_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.num_key_value_heads = num_key_value_heads\n+        self.intermediate_size = intermediate_size\n+        self.max_position_embeddings = max_position_embeddings\n+        self.initializer_range = initializer_range\n+        self.pad_token_id = pad_token_id\n+        self.bos_token_id = bos_token_id\n+        self.eos_token_id = eos_token_id\n+\n+    def prepare_config_and_inputs(self):\n+        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n+        attention_mask = input_ids.ne(1).to(torch_device)\n+\n+        config = self.get_config()\n+\n+        return config, input_ids, attention_mask\n+\n+    def get_config(self):\n+        return Emu3TextConfig(\n+            vocab_size=self.vocab_size,\n+            hidden_size=self.hidden_size,\n+            num_hidden_layers=self.num_hidden_layers,\n+            num_attention_heads=self.num_attention_heads,\n+            num_key_value_heads=self.num_key_value_heads,\n+            intermediate_size=self.intermediate_size,\n+            max_position_embeddings=self.max_position_embeddings,\n+            is_decoder=False,\n+            initializer_range=self.initializer_range,\n+            pad_token_id=self.pad_token_id,\n+            bos_token_id=self.bos_token_id,\n+            eos_token_id=self.eos_token_id,\n+        )\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        (\n+            config,\n+            input_ids,\n+            attention_mask,\n+        ) = config_and_inputs\n+        inputs_dict = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n+        return config, inputs_dict\n+\n+\n+@require_torch\n+class Emu3Text2TextModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+    all_model_classes = (Emu3ForCausalLM,) if is_torch_available() else ()\n+    all_generative_model_classes = (Emu3ForCausalLM,) if is_torch_available() else ()\n+    pipeline_model_mapping = (\n+        {\n+            \"text-generation\": Emu3ForCausalLM,\n+        }\n+        if is_torch_available()\n+        else {}\n+    )\n+    test_headmasking = False\n+    test_pruning = False\n+    fx_compatible = False\n+\n+    def setUp(self):\n+        self.model_tester = Emu3Text2TextModelTester(self)\n+        self.config_tester = ConfigTester(self, config_class=Emu3TextConfig, hidden_size=37)\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n+    @parameterized.expand([(\"linear\",), (\"dynamic\",)])\n+    def test_model_rope_scaling(self, scaling_type):\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+        short_input = ids_tensor([1, 10], config.vocab_size)\n+        long_input = ids_tensor([1, int(config.max_position_embeddings * 1.5)], config.vocab_size)\n+\n+        set_seed(42)  # Fixed seed at init time so the two models get the same random weights\n+        original_model = Emu3TextModel(config)\n+        original_model.to(torch_device)\n+        original_model.eval()\n+        original_short_output = original_model(short_input).last_hidden_state\n+        original_long_output = original_model(long_input).last_hidden_state\n+\n+        set_seed(42)  # Fixed seed at init time so the two models get the same random weights\n+        config.rope_scaling = {\"type\": scaling_type, \"factor\": 10.0}\n+        scaled_model = Emu3TextModel(config)\n+        scaled_model.to(torch_device)\n+        scaled_model.eval()\n+        scaled_short_output = scaled_model(short_input).last_hidden_state\n+        scaled_long_output = scaled_model(long_input).last_hidden_state\n+\n+        # Dynamic scaling does not change the RoPE embeddings until it receives an input longer than the original\n+        # maximum sequence length, so the outputs for the short input should match.\n+        if scaling_type == \"dynamic\":\n+            self.assertTrue(torch.allclose(original_short_output, scaled_short_output, atol=1e-5))\n+        else:\n+            self.assertFalse(torch.allclose(original_short_output, scaled_short_output, atol=1e-5))\n+\n+        # The output should be different for long inputs\n+        self.assertFalse(torch.allclose(original_long_output, scaled_long_output, atol=1e-5))\n+\n+    @unittest.skip(\"Doesn't work, tensors are not almost same\")  # TODO raushan fixme\n+    def test_custom_4d_attention_mask(self):\n+        pass\n+\n+    @unittest.skip(\"Fails with unknown error only on end-to-end compile\")  # TODO raushan fixme\n+    def test_generate_compile_1_end_to_end(self):\n+        pass\n+\n+\n+class Emu3Vision2TextModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=13,\n+        seq_length=7,\n+        is_training=False,\n+        vocab_size=99,\n+        hidden_size=32,\n+        num_hidden_layers=2,\n+        num_attention_heads=2,\n+        num_key_value_heads=2,\n+        intermediate_size=37,\n+        max_position_embeddings=512,\n+        initializer_range=0.02,\n+        pad_token_id=0,\n+        bos_token_id=1,\n+        eos_token_id=2,\n+        image_token_id=3,\n+        image_size=30,\n+        codebook_size=20,\n+        temporal_downsample_factor=1,\n+        base_channels=32,\n+        vq_channel_multiplier=[1, 1],\n+        image_seq_length=100,\n+        vq_img_token_start_id=3,\n+    ):\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.is_training = is_training\n+        self.vocab_size = vocab_size\n+        self.hidden_size = hidden_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.num_key_value_heads = num_key_value_heads\n+        self.intermediate_size = intermediate_size\n+        self.max_position_embeddings = max_position_embeddings\n+        self.initializer_range = initializer_range\n+        self.pad_token_id = pad_token_id\n+        self.bos_token_id = bos_token_id\n+        self.eos_token_id = eos_token_id\n+        self.image_token_id = image_token_id\n+        self.image_size = image_size\n+        self.codebook_size = codebook_size\n+        self.temporal_downsample_factor = temporal_downsample_factor\n+        self.vq_channel_multiplier = vq_channel_multiplier\n+        self.vq_img_token_start_id = vq_img_token_start_id\n+        self.base_channels = base_channels\n+        self.seq_length = seq_length + image_seq_length\n+        self.image_seq_length = image_seq_length\n+\n+    def prepare_config_and_inputs(self):\n+        config = self.get_config()\n+\n+        input_ids = ids_tensor([self.batch_size, self.seq_length], config.text_config.vocab_size)\n+        attention_mask = input_ids.ne(1).to(torch_device)\n+        input_ids[input_ids == self.image_token_id] = self.pad_token_id\n+        input_ids[:, : self.image_seq_length] = self.image_token_id\n+\n+        pixel_values = floats_tensor(\n+            [\n+                self.batch_size,\n+                3,\n+                self.image_size,\n+                self.image_size,\n+            ]\n+        )\n+        image_sizes = [[self.image_size, self.image_size]] * self.batch_size\n+        image_sizes = torch.tensor(image_sizes, device=torch_device, dtype=torch.int64)\n+\n+        return config, input_ids, attention_mask, pixel_values, image_sizes\n+\n+    def get_config(self):\n+        # create dummy vocab map for image2bpe mapping if it needs remapping\n+        # we assume that vocab size is big enough to account for `codebook_size` amount of\n+        # image tokens somewhere at the beginning of total vocab size\n+\n+        vocab_map = {i: chr(i) for i in range(self.vocab_size)}\n+        start = self.vq_img_token_start_id\n+        end = self.vq_img_token_start_id + self.codebook_size\n+        for i in range(start, end):\n+            # dummy str for each token, anything that fits pattern \"<|visual token XXXXXX|>\"\n+            vocab_map[i] = f\"<|visual token{i:06d}|>\"\n+\n+        # add tokens that have to be in the vocab, we'll retrieve their ids later in modeling code\n+        vocab_map[self.image_token_id] = \"<image>\"\n+        vocab_map[self.image_token_id + 1] = \"<|extra_200|>\"\n+        vocab_map = {v: k for k, v in vocab_map.items()}\n+\n+        text_config = Emu3TextConfig(\n+            vocab_size=self.vocab_size,\n+            hidden_size=self.hidden_size,\n+            num_hidden_layers=self.num_hidden_layers,\n+            num_attention_heads=self.num_attention_heads,\n+            num_key_value_heads=self.num_key_value_heads,\n+            intermediate_size=self.intermediate_size,\n+            max_position_embeddings=self.max_position_embeddings,\n+            initializer_range=self.initializer_range,\n+            pad_token_id=self.pad_token_id,\n+            bos_token_id=self.bos_token_id,\n+            eos_token_id=self.eos_token_id,\n+        )\n+\n+        vq_config = {\n+            \"codebook_size\": self.codebook_size,\n+            \"temporal_downsample_factor\": self.temporal_downsample_factor,\n+            \"base_channels\": self.base_channels,\n+            \"channel_multiplier\": self.vq_channel_multiplier,\n+            \"hidden_size\": self.base_channels,\n+        }\n+        return Emu3Config(text_config=text_config, vq_config=vq_config, vocabulary_map=vocab_map)\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        (\n+            config,\n+            input_ids,\n+            attention_mask,\n+            pixel_values,\n+            image_sizes,\n+        ) = config_and_inputs\n+        inputs_dict = {\n+            \"input_ids\": input_ids,\n+            \"attention_mask\": attention_mask,\n+            \"pixel_values\": pixel_values,\n+            \"image_sizes\": image_sizes,\n+        }\n+        return config, inputs_dict\n+\n+\n+@require_torch\n+class Emu3Vision2TextModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+    all_model_classes = (Emu3ForConditionalGeneration,) if is_torch_available() else ()\n+    all_generative_model_classes = (Emu3ForConditionalGeneration,) if is_torch_available() else ()\n+    pipeline_model_mapping = {}\n+    test_headmasking = False\n+    test_pruning = False\n+    fx_compatible = False\n+\n+    def setUp(self):\n+        self.model_tester = Emu3Vision2TextModelTester(self)\n+        self.config_tester = ConfigTester(\n+            self, config_class=Emu3Config, has_text_modality=False, common_properties=[\"vocabulary_map\"]\n+        )\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n+    # overwrite inputs_embeds tests because we need to delete \"pixel values\" for LVLMs\n+    def test_inputs_embeds(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            inputs = self._prepare_for_class(inputs_dict, model_class)\n+\n+            input_ids = inputs[\"input_ids\"]\n+            del inputs[\"input_ids\"]\n+            del inputs[\"pixel_values\"]\n+\n+            wte = model.get_input_embeddings()\n+            inputs[\"inputs_embeds\"] = wte(input_ids)\n+\n+            with torch.no_grad():\n+                model(**inputs)\n+\n+    # overwrite inputs_embeds tests because we need to delete \"pixel values\" for LVLMs\n+    # while some other models require pixel_values to be present\n+    def test_inputs_embeds_matches_input_ids(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            inputs = self._prepare_for_class(inputs_dict, model_class)\n+            input_ids = inputs[\"input_ids\"]\n+            del inputs[\"input_ids\"]\n+            del inputs[\"pixel_values\"]\n+\n+            inputs_embeds = model.get_input_embeddings()(input_ids)\n+\n+            with torch.no_grad():\n+                out_ids = model(input_ids=input_ids, **inputs)[0]\n+                out_embeds = model(inputs_embeds=inputs_embeds, **inputs)[0]\n+            self.assertTrue(torch.allclose(out_embeds, out_ids))\n+\n+    @unittest.skip(\n+        \"Emu3 has a VQ module that uses `weight.data` directly in forward which prevent offloding on that module\"\n+    )\n+    def test_disk_offload_safetensors(self):\n+        pass\n+\n+    @unittest.skip(\n+        \"Emu3 has a VQ module that uses `weight.data` directly in forward which prevent offloding on that module\"\n+    )\n+    def test_disk_offload_bin(self):\n+        pass\n+\n+    @unittest.skip(\n+        \"Emu3 has a VQ module that uses `weight.data` directly in forward which prevent offloding on that module\"\n+    )\n+    def test_cpu_offload(self):\n+        pass\n+\n+    @unittest.skip(\"Doesn't work, tensors are not almost same\")  # TODO raushan fixme\n+    def test_custom_4d_attention_mask(self):\n+        pass\n+\n+    @unittest.skip(\"VQ-VAE module doesn't initialize weights properly\")\n+    def test_initialization(self):\n+        pass\n+\n+    @unittest.skip(\"End-to-end compilation is not supported due to dynamic control in `prepare_inputs_for_generation`\")\n+    def test_generate_compile_1_end_to_end(self):\n+        pass\n+\n+\n+@require_torch\n+class Emu3IntegrationTest(unittest.TestCase):\n+    @slow\n+    @require_bitsandbytes\n+    def test_model_generation(self):\n+        model = Emu3ForConditionalGeneration.from_pretrained(\n+            \"Emu3-community/Emu3-Chat-hf\", load_in_4bit=True, device_map=\"auto\"\n+        )\n+        processor = Emu3Processor.from_pretrained(\"Emu3-community/Emu3-Chat-hf\")\n+\n+        image = Image.open(\n+            requests.get(\"https://nineplanets.org/wp-content/uploads/2020/12/the-big-dipper-1.jpg\", stream=True).raw\n+        )\n+        prompt = \"USER: <image>Describe what do you see here and tell me about the history behind it? ASSISTANT:\"\n+\n+        inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(model.device, torch.float16)\n+\n+        # greedy generation outputs\n+        EXPECTED_TEXT_COMPLETION = ['USER: 114*143Describe what do you see here and tell me about the history behind it? ASSISTANT: The image depicts the constellation of Ursa Minor, also known as the Little Bear. This constellation was one of the 24 modern constellations introduced by Charles Messier in 178']  # fmt: skip\n+        generated_ids = model.generate(**inputs, max_new_tokens=40, do_sample=False)\n+        text = processor.batch_decode(generated_ids, skip_special_tokens=True)\n+        self.assertEqual(EXPECTED_TEXT_COMPLETION, text)\n+\n+    @slow\n+    @require_bitsandbytes\n+    def test_model_generation_batched(self):\n+        model = Emu3ForConditionalGeneration.from_pretrained(\n+            \"Emu3-community/Emu3-Chat-hf\", load_in_4bit=True, device_map=\"auto\"\n+        )\n+        processor = Emu3Processor.from_pretrained(\"Emu3-community/Emu3-Chat-hf\")\n+        processor.tokenizer.padding_side = \"left\"\n+\n+        image = Image.open(\n+            requests.get(\"https://nineplanets.org/wp-content/uploads/2020/12/the-big-dipper-1.jpg\", stream=True).raw\n+        )\n+        image_2 = Image.open(\n+            requests.get(\"https://www.kxan.com/wp-content/uploads/sites/40/2020/10/ORION.jpg\", stream=True).raw\n+        )\n+        prompts = [\n+            \"USER: <image>Describe what do you see here and tell me about the history behind it? ASSISTANT:\",\n+            \"USER: <image>What do you know about the constellation in this image? ASSISTANT:\",\n+        ]\n+\n+        inputs = processor(images=[image, image_2], text=prompts, padding=True, return_tensors=\"pt\").to(\n+            model.device, torch.float16\n+        )\n+\n+        # greedy generation outputs\n+        EXPECTED_TEXT_COMPLETION = [\n+            'USER: 114*143Describe what do you see here and tell me about the history behind it? ASSISTANT: The image depicts the constellation of Ursa Minor, also known as the Little Bear. This constellation was one of the 24 modern constellations introduced by Charles Messier in 178',\n+            'USER: 75*125What do you know about the constellation in this image? ASSISTANT: The image shows a segment of a wire rope, characterized by its consistent pattern and regular twists, indicative of a high-quality, well-made rope. This type of detail suggests careful manufacturing processes and attention to'\n+            ]  # fmt: skip\n+        generated_ids = model.generate(**inputs, max_new_tokens=40, do_sample=False)\n+        text = processor.batch_decode(generated_ids, skip_special_tokens=True)\n+        self.assertEqual(EXPECTED_TEXT_COMPLETION, text)\n+\n+    @slow\n+    @require_bitsandbytes\n+    def test_model_generation_multi_image(self):\n+        model = Emu3ForConditionalGeneration.from_pretrained(\n+            \"Emu3-community/Emu3-Chat-hf\", load_in_4bit=True, device_map=\"auto\"\n+        )\n+        processor = Emu3Processor.from_pretrained(\"Emu3-community/Emu3-Chat-hf\")\n+\n+        image = Image.open(\n+            requests.get(\"https://nineplanets.org/wp-content/uploads/2020/12/the-big-dipper-1.jpg\", stream=True).raw\n+        )\n+        image_2 = Image.open(\n+            requests.get(\"https://www.kxan.com/wp-content/uploads/sites/40/2020/10/ORION.jpg\", stream=True).raw\n+        )\n+        prompt = \"USER: <image><image>What do these two images have in common? ASSISTANT:\"\n+\n+        inputs = processor(images=[image, image_2], text=prompt, return_tensors=\"pt\").to(model.device, torch.float16)\n+\n+        # greedy generation outputs\n+        EXPECTED_TEXT_COMPLETION = ['USER: 114*14375*125What do these two images have in common? ASSISTANT: The two images both depict a geometric shape - a triangle in the larger image and a line segment in the smaller image. They share a common feature of being created with a series of connected dots, which']  # fmt: skip\n+        generated_ids = model.generate(**inputs, max_new_tokens=40, do_sample=False)\n+        text = processor.batch_decode(generated_ids, skip_special_tokens=True)\n+        self.assertEqual(EXPECTED_TEXT_COMPLETION, text)\n+\n+    @slow\n+    @require_bitsandbytes\n+    def test_model_generate_images(self):\n+        model = Emu3ForConditionalGeneration.from_pretrained(\n+            \"Emu3-community/Emu3-Gen-hf\", load_in_4bit=True, device_map=\"auto\"\n+        )\n+        processor = Emu3Processor.from_pretrained(\"Emu3-community/Emu3-Chat-hf\")\n+\n+        inputs = processor(\n+            text=[\"a portrait of young girl. masterpiece, film grained, best quality.\"],\n+            padding=True,\n+            return_tensors=\"pt\",\n+            return_for_image_generation=True,\n+        ).to(model.device)\n+        self.assertTrue(inputs.input_ids.shape[1] == 23)\n+\n+        image_sizes = inputs.pop(\"image_sizes\")\n+        HEIGHT, WIDTH = image_sizes[0]\n+        VISUAL_TOKENS = model.vocabulary_mapping.image_tokens\n+\n+        def prefix_allowed_tokens_fn(batch_id, input_ids):\n+            height, width = HEIGHT, WIDTH\n+            visual_tokens = VISUAL_TOKENS\n+            image_wrapper_token_id = torch.tensor([processor.tokenizer.image_wrapper_token_id], device=model.device)\n+            eoi_token_id = torch.tensor([processor.tokenizer.eoi_token_id], device=model.device)\n+            eos_token_id = torch.tensor([processor.tokenizer.eos_token_id], device=model.device)\n+            pad_token_id = torch.tensor([processor.tokenizer.pad_token_id], device=model.device)\n+            eof_token_id = torch.tensor([processor.tokenizer.eof_token_id], device=model.device)\n+            eol_token_id = processor.tokenizer.encode(\"<|extra_200|>\", return_tensors=\"pt\")[0]\n+\n+            position = torch.nonzero(input_ids == image_wrapper_token_id, as_tuple=True)[0][0]\n+            offset = input_ids.shape[0] - position\n+            if offset % (width + 1) == 0:\n+                return (eol_token_id,)\n+            elif offset == (width + 1) * height + 1:\n+                return (eof_token_id,)\n+            elif offset == (width + 1) * height + 2:\n+                return (eoi_token_id,)\n+            elif offset == (width + 1) * height + 3:\n+                return (eos_token_id,)\n+            elif offset > (width + 1) * height + 3:\n+                return (pad_token_id,)\n+            else:\n+                return visual_tokens\n+\n+        out = model.generate(\n+            **inputs,\n+            max_new_tokens=50_000,\n+            prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n+            do_sample=False,\n+        )\n+        self.assertTrue(out.shape[1] == 8216)\n+\n+        image = model.decode_image_tokens(out[:, inputs.input_ids.shape[1] :], height=HEIGHT, width=WIDTH)\n+        images = processor.postprocess(list(image.float()), return_tensors=\"np\")\n+        self.assertTrue(images[\"pixel_values\"].shape == (3, 720, 720))\n+        self.assertTrue(isinstance(images[\"pixel_values\"], np.ndarray))\n+\n+        filepath = hf_hub_download(\n+            repo_id=\"raushan-testing-hf/images_test\",\n+            filename=\"emu3_generated_pixels.npy\",\n+            repo_type=\"dataset\",\n+        )\n+        original_pixels = np.load(filepath)\n+        self.assertTrue(np.allclose(original_pixels, images[\"pixel_values\"]))"
        },
        {
            "sha": "7bc77075b1a69b04fa329cf6905860e52d35f2ab",
            "filename": "tests/models/emu3/test_processor_emu3.py",
            "status": "added",
            "additions": 85,
            "deletions": 0,
            "changes": 85,
            "blob_url": "https://github.com/huggingface/transformers/blob/52e1f87c7d2fdaf147aa625c169ad440672cb3cb/tests%2Fmodels%2Femu3%2Ftest_processor_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52e1f87c7d2fdaf147aa625c169ad440672cb3cb/tests%2Fmodels%2Femu3%2Ftest_processor_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Femu3%2Ftest_processor_emu3.py?ref=52e1f87c7d2fdaf147aa625c169ad440672cb3cb",
            "patch": "@@ -0,0 +1,85 @@\n+# coding=utf-8\n+# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch emu3 model.\"\"\"\n+\n+import tempfile\n+import unittest\n+\n+import numpy as np\n+\n+from transformers import Emu3Processor, GPT2TokenizerFast\n+from transformers.utils import is_vision_available\n+\n+from ...test_processing_common import ProcessorTesterMixin\n+\n+\n+if is_vision_available():\n+    from transformers import Emu3ImageProcessor\n+\n+\n+class Emu3ProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n+    processor_class = Emu3Processor\n+\n+    def setUp(self):\n+        self.tmpdirname = tempfile.mkdtemp()\n+        image_processor = Emu3ImageProcessor()\n+        extra_special_tokens = extra_special_tokens = {\n+            \"image_token\": \"<image>\",\n+            \"boi_token\": \"<|image start|>\",\n+            \"eoi_token\": \"<|image end|>\",\n+            \"image_wrapper_token\": \"<|image token|>\",\n+            \"eof_token\": \"<|extra_201|>\",\n+        }\n+        tokenizer = GPT2TokenizerFast.from_pretrained(\n+            \"openai-community/gpt2\", extra_special_tokens=extra_special_tokens\n+        )\n+        tokenizer.pad_token_id = 0\n+        tokenizer.sep_token_id = 1\n+        processor = self.processor_class(\n+            image_processor=image_processor, tokenizer=tokenizer, chat_template=\"dummy_template\"\n+        )\n+        processor.save_pretrained(self.tmpdirname)\n+\n+    def test_processor_for_generation(self):\n+        processor_components = self.prepare_components()\n+        processor = self.processor_class(**processor_components)\n+\n+        # we don't need an image as input because the model will generate one\n+        input_str = \"lower newer\"\n+        image_input = self.prepare_image_inputs()\n+        inputs = processor(text=input_str, return_for_image_generation=True, return_tensors=\"pt\")\n+        self.assertListEqual(list(inputs.keys()), [\"input_ids\", \"attention_mask\", \"image_sizes\"])\n+        self.assertEqual(inputs[self.text_input_name].shape[-1], 8)\n+\n+        # when `return_for_image_generation` is set, we raise an error that image should not be provided\n+        with self.assertRaises(ValueError):\n+            inputs = processor(\n+                text=input_str, images=image_input, return_for_image_generation=True, return_tensors=\"pt\"\n+            )\n+\n+    def test_processor_postprocess(self):\n+        processor_components = self.prepare_components()\n+        processor = self.processor_class(**processor_components)\n+\n+        input_str = \"lower newer\"\n+        orig_image_input = self.prepare_image_inputs()\n+        orig_image = np.array(orig_image_input).transpose(2, 0, 1)\n+\n+        inputs = processor(text=input_str, images=orig_image, do_resize=False, return_tensors=\"np\")\n+        normalized_image_input = inputs.pixel_values\n+        unnormalized_images = processor.postprocess(normalized_image_input, return_tensors=\"np\")[\"pixel_values\"]\n+\n+        # For an image where pixels go from 0 to 255 the diff can be 1 due to some numerical precision errors when scaling and unscaling\n+        self.assertTrue(np.abs(orig_image - unnormalized_images).max() >= 1)"
        },
        {
            "sha": "3c3c6684c24cc922b55621403bf6a8cf0a61cde3",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/52e1f87c7d2fdaf147aa625c169ad440672cb3cb/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52e1f87c7d2fdaf147aa625c169ad440672cb3cb/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=52e1f87c7d2fdaf147aa625c169ad440672cb3cb",
            "patch": "@@ -3894,7 +3894,7 @@ def test_sdpa_can_dispatch_non_composite_models(self):\n                 for name, submodule in model_eager.named_modules():\n                     class_name = submodule.__class__.__name__\n                     if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n-                        raise ValueError(\"The eager model should not have SDPA attention layers\")\n+                        raise ValueError(f\"The eager model should not have SDPA attention layers but got {class_name}\")\n \n     @require_torch_sdpa\n     def test_sdpa_can_dispatch_composite_models(self):"
        },
        {
            "sha": "6262b1902aabc3959787cd8c2fa7853f8d2f69a7",
            "filename": "utils/check_config_attributes.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/52e1f87c7d2fdaf147aa625c169ad440672cb3cb/utils%2Fcheck_config_attributes.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52e1f87c7d2fdaf147aa625c169ad440672cb3cb/utils%2Fcheck_config_attributes.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_config_attributes.py?ref=52e1f87c7d2fdaf147aa625c169ad440672cb3cb",
            "patch": "@@ -266,6 +266,10 @@ def check_attribute_being_used(config_class, attributes, default_value, source_s\n                 f\"config.{attribute}\" in modeling_source\n                 or f'getattr(config, \"{attribute}\"' in modeling_source\n                 or f'getattr(self.config, \"{attribute}\"' in modeling_source\n+                or (\n+                    \"TextConfig\" in config_class.__name__\n+                    and f\"config.get_text_config().{attribute}\" in modeling_source\n+                )\n             ):\n                 attribute_used = True\n             # Deal with multi-line cases"
        },
        {
            "sha": "7f3e0c66d55ed0c18a884b2be0f96fbbe6c34237",
            "filename": "utils/check_repo.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/52e1f87c7d2fdaf147aa625c169ad440672cb3cb/utils%2Fcheck_repo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52e1f87c7d2fdaf147aa625c169ad440672cb3cb/utils%2Fcheck_repo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_repo.py?ref=52e1f87c7d2fdaf147aa625c169ad440672cb3cb",
            "patch": "@@ -139,6 +139,8 @@\n         \"Qwen2VLModel\",  # Building part of bigger (tested) model. Tested implicitly through Qwen2VLForConditionalGeneration.\n         \"MllamaTextModel\",  # Building part of bigger (tested) model. # TODO: add tests\n         \"MllamaVisionModel\",  # Building part of bigger (tested) model. # TODO: add tests\n+        \"Emu3VQVAE\",  # Building part of bigger (tested) model\n+        \"Emu3TextModel\",  # Building part of bigger (tested) model\n     ]\n )\n \n@@ -333,6 +335,8 @@\n     \"VitPoseForPoseEstimation\",\n     \"CLIPTextModel\",\n     \"MoshiForConditionalGeneration\",  # no auto class for speech-to-speech\n+    \"Emu3VQVAE\",  # no autoclass for VQ-VAE models\n+    \"Emu3TextModel\",  # Building part of bigger (tested) model\n ]\n \n # DO NOT edit this list!"
        }
    ],
    "stats": {
        "total": 5727,
        "additions": 5722,
        "deletions": 5
    }
}