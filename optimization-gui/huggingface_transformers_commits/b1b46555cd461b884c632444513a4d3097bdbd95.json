{
    "author": "Cyrilvallez",
    "message": "Re-apply make style (#40106)\n\nmake style",
    "sha": "b1b46555cd461b884c632444513a4d3097bdbd95",
    "files": [
        {
            "sha": "67cb89ba48122633ae330bf0ce20787c54a072c5",
            "filename": "src/transformers/integrations/mxfp4.py",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/b1b46555cd461b884c632444513a4d3097bdbd95/src%2Ftransformers%2Fintegrations%2Fmxfp4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b1b46555cd461b884c632444513a4d3097bdbd95/src%2Ftransformers%2Fintegrations%2Fmxfp4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fmxfp4.py?ref=b1b46555cd461b884c632444513a4d3097bdbd95",
            "patch": "@@ -314,12 +314,12 @@ def should_convert_module(current_key_name, patterns):\n def dequantize(module, param_name, param_value, target_device, dq_param_name, **kwargs):\n     from ..integrations.tensor_parallel import shard_and_distribute_module\n \n-    model = kwargs.get(\"model\", None)\n-    empty_param = kwargs.get(\"empty_param\", None)\n-    casting_dtype = kwargs.get(\"casting_dtype\", None)\n-    to_contiguous = kwargs.get(\"to_contiguous\", None)\n-    rank = kwargs.get(\"rank\", None)\n-    device_mesh = kwargs.get(\"device_mesh\", None)\n+    model = kwargs.get(\"model\")\n+    empty_param = kwargs.get(\"empty_param\")\n+    casting_dtype = kwargs.get(\"casting_dtype\")\n+    to_contiguous = kwargs.get(\"to_contiguous\")\n+    rank = kwargs.get(\"rank\")\n+    device_mesh = kwargs.get(\"device_mesh\")\n \n     for proj in [\"gate_up_proj\", \"down_proj\"]:\n         if proj in param_name:\n@@ -357,12 +357,12 @@ def load_and_swizzle_mxfp4(module, param_name, param_value, target_device, **kwa\n     )\n     from ..integrations.tensor_parallel import shard_and_distribute_module\n \n-    model = kwargs.get(\"model\", None)\n-    empty_param = kwargs.get(\"empty_param\", None)\n-    casting_dtype = kwargs.get(\"casting_dtype\", None)\n-    to_contiguous = kwargs.get(\"to_contiguous\", None)\n-    rank = kwargs.get(\"rank\", None)\n-    device_mesh = kwargs.get(\"device_mesh\", None)\n+    model = kwargs.get(\"model\")\n+    empty_param = kwargs.get(\"empty_param\")\n+    casting_dtype = kwargs.get(\"casting_dtype\")\n+    to_contiguous = kwargs.get(\"to_contiguous\")\n+    rank = kwargs.get(\"rank\")\n+    device_mesh = kwargs.get(\"device_mesh\")\n \n     for proj in [\"gate_up_proj\", \"down_proj\"]:\n         if proj in param_name:"
        },
        {
            "sha": "4fb4168a8bd96fdee0c645cb1509464e46605bdc",
            "filename": "src/transformers/quantizers/quantizer_mxfp4.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/b1b46555cd461b884c632444513a4d3097bdbd95/src%2Ftransformers%2Fquantizers%2Fquantizer_mxfp4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b1b46555cd461b884c632444513a4d3097bdbd95/src%2Ftransformers%2Fquantizers%2Fquantizer_mxfp4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_mxfp4.py?ref=b1b46555cd461b884c632444513a4d3097bdbd95",
            "patch": "@@ -101,7 +101,7 @@ def validate_environment(self, *args, **kwargs):\n             global triton_kernels_hub\n             triton_kernels_hub = get_kernel(\"kernels-community/triton_kernels\")\n \n-        device_map = kwargs.get(\"device_map\", None)\n+        device_map = kwargs.get(\"device_map\")\n         if device_map is None:\n             logger.warning_once(\n                 \"You have loaded an FP4 model on CPU and have a CUDA device available, make sure to set \"\n@@ -210,11 +210,11 @@ def create_quantized_param(\n         # we take this path if already quantized but not in a compatible way\n         # The params going here are either gate_up_proj_blocks, or down_proj_blocks, or gate_up_proj_scales, or down_proj_scales\n         else:\n-            empty_param = kwargs.get(\"empty_param\", None)\n-            casting_dtype = kwargs.get(\"casting_dtype\", None)\n-            to_contiguous = kwargs.get(\"to_contiguous\", None)\n-            rank = kwargs.get(\"rank\", None)\n-            device_mesh = kwargs.get(\"device_mesh\", None)\n+            empty_param = kwargs.get(\"empty_param\")\n+            casting_dtype = kwargs.get(\"casting_dtype\")\n+            to_contiguous = kwargs.get(\"to_contiguous\")\n+            rank = kwargs.get(\"rank\")\n+            device_mesh = kwargs.get(\"device_mesh\")\n             if (\"blocks\" in param_name or \"scales\" in param_name) and self.quantization_config.dequantize:\n                 # blocks and scales have the same length that's this works for both\n                 module, _ = get_module_from_name(model, param_name[: -len(\"_blocks\")])"
        }
    ],
    "stats": {
        "total": 36,
        "additions": 18,
        "deletions": 18
    }
}