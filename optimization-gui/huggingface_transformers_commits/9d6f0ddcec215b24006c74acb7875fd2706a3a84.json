{
    "author": "mgoin",
    "message": "Add optimized `PixtralImageProcessorFast` (#34836)\n\n* Add optimized PixtralImageProcessorFast\n\n* make style\n\n* Add dummy_vision_object\n\n* Review comments\n\n* Format\n\n* Fix dummy\n\n* Format\n\n* np.ceil for math.ceil",
    "sha": "9d6f0ddcec215b24006c74acb7875fd2706a3a84",
    "files": [
        {
            "sha": "f49e4e4731965a504b8da443c2cd979638cd22bb",
            "filename": "docs/source/en/_config.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9d6f0ddcec215b24006c74acb7875fd2706a3a84/docs%2Fsource%2Fen%2F_config.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9d6f0ddcec215b24006c74acb7875fd2706a3a84/docs%2Fsource%2Fen%2F_config.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_config.py?ref=9d6f0ddcec215b24006c74acb7875fd2706a3a84",
            "patch": "@@ -11,4 +11,4 @@\n     \"{processor_class}\": \"FakeProcessorClass\",\n     \"{model_class}\": \"FakeModelClass\",\n     \"{object_class}\": \"FakeObjectClass\",\n-}\n\\ No newline at end of file\n+}"
        },
        {
            "sha": "62bdc004c5171853c180002b8b4bb0423f34b4b6",
            "filename": "docs/source/en/model_doc/pixtral.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/9d6f0ddcec215b24006c74acb7875fd2706a3a84/docs%2Fsource%2Fen%2Fmodel_doc%2Fpixtral.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/9d6f0ddcec215b24006c74acb7875fd2706a3a84/docs%2Fsource%2Fen%2Fmodel_doc%2Fpixtral.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fpixtral.md?ref=9d6f0ddcec215b24006c74acb7875fd2706a3a84",
            "patch": "@@ -88,6 +88,11 @@ output = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up\n [[autodoc]] PixtralImageProcessor\n     - preprocess\n \n+## PixtralImageProcessorFast\n+\n+[[autodoc]] PixtralImageProcessorFast\n+    - preprocess\n+\n ## PixtralProcessor\n \n [[autodoc]] PixtralProcessor"
        },
        {
            "sha": "9db2e2c51f6c9ccd9c762259b952d976dc6b152e",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9d6f0ddcec215b24006c74acb7875fd2706a3a84/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9d6f0ddcec215b24006c74acb7875fd2706a3a84/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=9d6f0ddcec215b24006c74acb7875fd2706a3a84",
            "patch": "@@ -1260,6 +1260,7 @@\n     _import_structure[\"image_processing_utils_fast\"] = [\"BaseImageProcessorFast\"]\n     _import_structure[\"models.deformable_detr\"].append(\"DeformableDetrImageProcessorFast\")\n     _import_structure[\"models.detr\"].append(\"DetrImageProcessorFast\")\n+    _import_structure[\"models.pixtral\"].append(\"PixtralImageProcessorFast\")\n     _import_structure[\"models.rt_detr\"].append(\"RTDetrImageProcessorFast\")\n     _import_structure[\"models.vit\"].append(\"ViTImageProcessorFast\")\n \n@@ -6189,6 +6190,7 @@\n         from .image_processing_utils_fast import BaseImageProcessorFast\n         from .models.deformable_detr import DeformableDetrImageProcessorFast\n         from .models.detr import DetrImageProcessorFast\n+        from .models.pixtral import PixtralImageProcessorFast\n         from .models.rt_detr import RTDetrImageProcessorFast\n         from .models.vit import ViTImageProcessorFast\n "
        },
        {
            "sha": "51199d9f3698fc6212b5f8b3c90144fbf147ad41",
            "filename": "src/transformers/image_utils.py",
            "status": "modified",
            "additions": 39,
            "deletions": 0,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/9d6f0ddcec215b24006c74acb7875fd2706a3a84/src%2Ftransformers%2Fimage_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9d6f0ddcec215b24006c74acb7875fd2706a3a84/src%2Ftransformers%2Fimage_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_utils.py?ref=9d6f0ddcec215b24006c74acb7875fd2706a3a84",
            "patch": "@@ -24,6 +24,7 @@\n \n from .utils import (\n     ExplicitEnum,\n+    TensorType,\n     is_jax_tensor,\n     is_numpy_array,\n     is_tf_tensor,\n@@ -447,6 +448,44 @@ def validate_preprocess_arguments(\n         raise ValueError(\"`size` and `resample` must be specified if `do_resize` is `True`.\")\n \n \n+def validate_fast_preprocess_arguments(\n+    do_rescale: Optional[bool] = None,\n+    rescale_factor: Optional[float] = None,\n+    do_normalize: Optional[bool] = None,\n+    image_mean: Optional[Union[float, List[float]]] = None,\n+    image_std: Optional[Union[float, List[float]]] = None,\n+    do_pad: Optional[bool] = None,\n+    size_divisibility: Optional[int] = None,\n+    do_center_crop: Optional[bool] = None,\n+    crop_size: Optional[Dict[str, int]] = None,\n+    do_resize: Optional[bool] = None,\n+    size: Optional[Dict[str, int]] = None,\n+    resample: Optional[\"PILImageResampling\"] = None,\n+    return_tensors: Optional[Union[str, TensorType]] = None,\n+    data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n+):\n+    \"\"\"\n+    Checks validity of typically used arguments in an `ImageProcessorFast` `preprocess` method.\n+    Raises `ValueError` if arguments incompatibility is caught.\n+    \"\"\"\n+    validate_preprocess_arguments(\n+        do_rescale=do_rescale,\n+        rescale_factor=rescale_factor,\n+        do_normalize=do_normalize,\n+        image_mean=image_mean,\n+        image_std=image_std,\n+        do_resize=do_resize,\n+        size=size,\n+        resample=resample,\n+    )\n+    # Extra checks for ImageProcessorFast\n+    if return_tensors != \"pt\":\n+        raise ValueError(\"Only returning PyTorch tensors is currently supported.\")\n+\n+    if data_format != ChannelDimension.FIRST:\n+        raise ValueError(\"Only channel first data format is currently supported.\")\n+\n+\n # In the future we can add a TF implementation here when we have TF models.\n class ImageFeatureExtractionMixin:\n     \"\"\""
        },
        {
            "sha": "11ae15ca461e7950498e39e8af86bfbbc4240601",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9d6f0ddcec215b24006c74acb7875fd2706a3a84/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9d6f0ddcec215b24006c74acb7875fd2706a3a84/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=9d6f0ddcec215b24006c74acb7875fd2706a3a84",
            "patch": "@@ -117,7 +117,7 @@\n             (\"paligemma\", (\"SiglipImageProcessor\",)),\n             (\"perceiver\", (\"PerceiverImageProcessor\",)),\n             (\"pix2struct\", (\"Pix2StructImageProcessor\",)),\n-            (\"pixtral\", (\"PixtralImageProcessor\",)),\n+            (\"pixtral\", (\"PixtralImageProcessor\", \"PixtralImageProcessorFast\")),\n             (\"poolformer\", (\"PoolFormerImageProcessor\",)),\n             (\"pvt\", (\"PvtImageProcessor\",)),\n             (\"pvt_v2\", (\"PvtImageProcessor\",)),"
        },
        {
            "sha": "400a52a8adf2a1a3d95db76f120e550dcc4d0d3d",
            "filename": "src/transformers/models/pixtral/__init__.py",
            "status": "modified",
            "additions": 23,
            "deletions": 1,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/9d6f0ddcec215b24006c74acb7875fd2706a3a84/src%2Ftransformers%2Fmodels%2Fpixtral%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9d6f0ddcec215b24006c74acb7875fd2706a3a84/src%2Ftransformers%2Fmodels%2Fpixtral%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2F__init__.py?ref=9d6f0ddcec215b24006c74acb7875fd2706a3a84",
            "patch": "@@ -13,7 +13,13 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-from ...utils import OptionalDependencyNotAvailable, _LazyModule, is_torch_available, is_vision_available\n+from ...utils import (\n+    OptionalDependencyNotAvailable,\n+    _LazyModule,\n+    is_torch_available,\n+    is_torchvision_available,\n+    is_vision_available,\n+)\n \n \n _import_structure = {\n@@ -41,6 +47,14 @@\n else:\n     _import_structure[\"image_processing_pixtral\"] = [\"PixtralImageProcessor\"]\n \n+try:\n+    if not is_torchvision_available():\n+        raise OptionalDependencyNotAvailable()\n+except OptionalDependencyNotAvailable:\n+    pass\n+else:\n+    _import_structure[\"image_processing_pixtral_fast\"] = [\"PixtralImageProcessorFast\"]\n+\n \n if TYPE_CHECKING:\n     from .configuration_pixtral import PixtralVisionConfig\n@@ -65,6 +79,14 @@\n     else:\n         from .image_processing_pixtral import PixtralImageProcessor\n \n+    try:\n+        if not is_torchvision_available():\n+            raise OptionalDependencyNotAvailable()\n+    except OptionalDependencyNotAvailable:\n+        pass\n+    else:\n+        from .image_processing_pixtral_fast import PixtralImageProcessorFast\n+\n else:\n     import sys\n "
        },
        {
            "sha": "3f3978e1934f5d107b509d49edd0a47b45fe7287",
            "filename": "src/transformers/models/pixtral/image_processing_pixtral.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/9d6f0ddcec215b24006c74acb7875fd2706a3a84/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9d6f0ddcec215b24006c74acb7875fd2706a3a84/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral.py?ref=9d6f0ddcec215b24006c74acb7875fd2706a3a84",
            "patch": "@@ -14,6 +14,7 @@\n # limitations under the License.\n \"\"\"Image processor class for Pixtral.\"\"\"\n \n+import math\n from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n \n import numpy as np\n@@ -179,7 +180,7 @@ def _num_image_tokens(image_size: Tuple[int, int], patch_size: Tuple[int, int])\n \n \n def get_resize_output_image_size(\n-    input_image: np.ndarray,\n+    input_image: ImageInput,\n     size: Union[int, Tuple[int, int], List[int], Tuple[int]],\n     patch_size: Union[int, Tuple[int, int], List[int], Tuple[int]],\n     input_data_format: Optional[Union[str, ChannelDimension]] = None,\n@@ -189,7 +190,7 @@ def get_resize_output_image_size(\n     size.\n \n     Args:\n-        input_image (`np.ndarray`):\n+        input_image (`ImageInput`):\n             The image to resize.\n         size (`int` or `Tuple[int, int]`):\n             Max image size an input image can be. Must be a dictionary with the key \"longest_edge\".\n@@ -210,8 +211,8 @@ def get_resize_output_image_size(\n \n     if ratio > 1:\n         # Orgiginal implementation uses `round` which utilises bankers rounding, which can lead to surprising results\n-        height = int(np.ceil(height / ratio))\n-        width = int(np.ceil(width / ratio))\n+        height = int(math.ceil(height / ratio))\n+        width = int(math.ceil(width / ratio))\n \n     num_height_tokens, num_width_tokens = _num_image_tokens((height, width), (patch_height, patch_width))\n     return num_height_tokens * patch_height, num_width_tokens * patch_width"
        },
        {
            "sha": "82fbf3b2c094a69cf3e042d10c7def95398df8f1",
            "filename": "src/transformers/models/pixtral/image_processing_pixtral_fast.py",
            "status": "added",
            "additions": 349,
            "deletions": 0,
            "changes": 349,
            "blob_url": "https://github.com/huggingface/transformers/blob/9d6f0ddcec215b24006c74acb7875fd2706a3a84/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9d6f0ddcec215b24006c74acb7875fd2706a3a84/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral_fast.py?ref=9d6f0ddcec215b24006c74acb7875fd2706a3a84",
            "patch": "@@ -0,0 +1,349 @@\n+# coding=utf-8\n+# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Image processor class for Pixtral.\"\"\"\n+\n+from typing import Dict, List, Optional, Union\n+\n+from ...image_processing_utils import get_size_dict\n+from ...image_processing_utils_fast import BaseImageProcessorFast\n+from ...image_utils import (\n+    ChannelDimension,\n+    ImageInput,\n+    ImageType,\n+    PILImageResampling,\n+    get_image_size,\n+    get_image_type,\n+    infer_channel_dimension_format,\n+    validate_fast_preprocess_arguments,\n+    validate_kwargs,\n+)\n+from ...utils import (\n+    TensorType,\n+    is_torch_available,\n+    is_torchvision_available,\n+    is_torchvision_v2_available,\n+    is_vision_available,\n+    logging,\n+)\n+from .image_processing_pixtral import (\n+    BatchMixFeature,\n+    convert_to_rgb,\n+    get_resize_output_image_size,\n+    make_list_of_images,\n+)\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+if is_torch_available():\n+    import torch\n+\n+if is_torchvision_available():\n+    if is_vision_available():\n+        from ...image_utils import pil_torch_interpolation_mapping\n+\n+    if is_torchvision_v2_available():\n+        from torchvision.transforms.v2 import functional as F\n+    else:\n+        from torchvision.transforms import functional as F\n+\n+\n+class PixtralImageProcessorFast(BaseImageProcessorFast):\n+    r\"\"\"\n+    Constructs a fast Pixtral image processor that leverages torchvision.\n+\n+    Args:\n+        do_resize (`bool`, *optional*, defaults to `True`):\n+            Whether to resize the image's (height, width) dimensions to the specified `size`. Can be overridden by\n+            `do_resize` in the `preprocess` method.\n+        size (`Dict[str, int]` *optional*, defaults to `{\"longest_edge\": 1024}`):\n+            Size of the maximum dimension of either the height or width dimension of the image. Used to control how\n+            images are resized. If either the height or width are greater than `size[\"longest_edge\"]` then both the height and width are rescaled by `height / ratio`, `width /ratio` where `ratio = max(height / longest_edge, width / longest_edge)`\n+        patch_size (`Dict[str, int]` *optional*, defaults to `{\"height\": 16, \"width\": 16}`):\n+            Size of the patches in the model, used to calculate the output image size. Can be overridden by `patch_size` in the `preprocess` method.\n+        resample (`PILImageResampling`, *optional*, defaults to `Resampling.BICUBIC`):\n+            Resampling filter to use if resizing the image. Can be overridden by `resample` in the `preprocess` method.\n+        do_rescale (`bool`, *optional*, defaults to `True`):\n+            Whether to rescale the image by the specified scale `rescale_factor`. Can be overridden by `do_rescale` in\n+            the `preprocess` method.\n+        rescale_factor (`int` or `float`, *optional*, defaults to `1/255`):\n+            Scale factor to use if rescaling the image. Can be overridden by `rescale_factor` in the `preprocess`\n+            method.\n+        do_normalize (`bool`, *optional*, defaults to `True`):\n+            Whether to normalize the image. Can be overridden by `do_normalize` in the `preprocess` method.\n+        image_mean (`float` or `List[float]`, *optional*, defaults to `[0.48145466, 0.4578275, 0.40821073]`):\n+            Mean to use if normalizing the image. This is a float or list of floats the length of the number of\n+            channels in the image. Can be overridden by the `image_mean` parameter in the `preprocess` method.\n+        image_std (`float` or `List[float]`, *optional*, defaults to `[0.26862954, 0.26130258, 0.27577711]`):\n+            Standard deviation to use if normalizing the image. This is a float or list of floats the length of the\n+            number of channels in the image. Can be overridden by the `image_std` parameter in the `preprocess` method.\n+            Can be overridden by the `image_std` parameter in the `preprocess` method.\n+        do_convert_rgb (`bool`, *optional*, defaults to `True`):\n+            Whether to convert the image to RGB.\n+    \"\"\"\n+\n+    model_input_names = [\"pixel_values\"]\n+\n+    def __init__(\n+        self,\n+        do_resize: bool = True,\n+        size: Dict[str, int] = None,\n+        patch_size: Dict[str, int] = None,\n+        resample: Union[PILImageResampling, \"F.InterpolationMode\"] = PILImageResampling.BICUBIC,\n+        do_rescale: bool = True,\n+        rescale_factor: Union[int, float] = 1 / 255,\n+        do_normalize: bool = True,\n+        image_mean: Optional[Union[float, List[float]]] = None,\n+        image_std: Optional[Union[float, List[float]]] = None,\n+        do_convert_rgb: bool = True,\n+        **kwargs,\n+    ) -> None:\n+        super().__init__(**kwargs)\n+        size = size if size is not None else {\"longest_edge\": 1024}\n+        patch_size = patch_size if patch_size is not None else {\"height\": 16, \"width\": 16}\n+        patch_size = get_size_dict(patch_size, default_to_square=True)\n+\n+        self.do_resize = do_resize\n+        self.size = size\n+        self.patch_size = patch_size\n+        self.resample = resample\n+        self.do_rescale = do_rescale\n+        self.rescale_factor = rescale_factor\n+        self.do_normalize = do_normalize\n+        self.image_mean = image_mean if image_mean is not None else [0.48145466, 0.4578275, 0.40821073]\n+        self.image_std = image_std if image_std is not None else [0.26862954, 0.26130258, 0.27577711]\n+        self.do_convert_rgb = do_convert_rgb\n+        self._valid_processor_keys = [\n+            \"images\",\n+            \"do_resize\",\n+            \"size\",\n+            \"patch_size\",\n+            \"resample\",\n+            \"do_rescale\",\n+            \"rescale_factor\",\n+            \"do_normalize\",\n+            \"image_mean\",\n+            \"image_std\",\n+            \"do_convert_rgb\",\n+            \"return_tensors\",\n+            \"data_format\",\n+            \"input_data_format\",\n+        ]\n+\n+    def resize(\n+        self,\n+        image: torch.Tensor,\n+        size: Dict[str, int],\n+        patch_size: Dict[str, int],\n+        interpolation: \"F.InterpolationMode\" = None,\n+        **kwargs,\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        Resize an image. The shortest edge of the image is resized to size[\"shortest_edge\"], with the longest edge\n+        resized to keep the input aspect ratio.\n+\n+        Args:\n+            image (`torch.Tensor`):\n+                Image to resize.\n+            size (`Dict[str, int]`):\n+                Dict containing the longest possible edge of the image.\n+            patch_size (`Dict[str, int]`):\n+                Patch size used to calculate the size of the output image.\n+            interpolation (`InterpolationMode`, *optional*, defaults to `InterpolationMode.BILINEAR`):\n+                Resampling filter to use when resiizing the image.\n+        \"\"\"\n+        interpolation = interpolation if interpolation is not None else F.InterpolationMode.BILINEAR\n+        if \"longest_edge\" in size:\n+            size = (size[\"longest_edge\"], size[\"longest_edge\"])\n+        elif \"height\" in size and \"width\" in size:\n+            size = (size[\"height\"], size[\"width\"])\n+        else:\n+            raise ValueError(\"size must contain either 'longest_edge' or 'height' and 'width'.\")\n+\n+        if \"height\" in patch_size and \"width\" in patch_size:\n+            patch_size = (patch_size[\"height\"], patch_size[\"width\"])\n+        else:\n+            raise ValueError(\"patch_size must contain either 'shortest_edge' or 'height' and 'width'.\")\n+\n+        output_size = get_resize_output_image_size(\n+            image,\n+            size=size,\n+            patch_size=patch_size,\n+        )\n+        return F.resize(\n+            image,\n+            size=output_size,\n+            interpolation=interpolation,\n+            **kwargs,\n+        )\n+\n+    def preprocess(\n+        self,\n+        images: ImageInput,\n+        do_resize: bool = None,\n+        size: Dict[str, int] = None,\n+        patch_size: Dict[str, int] = None,\n+        resample: Optional[Union[PILImageResampling, \"F.InterpolationMode\"]] = None,\n+        do_rescale: bool = None,\n+        rescale_factor: float = None,\n+        do_normalize: bool = None,\n+        image_mean: Optional[Union[float, List[float]]] = None,\n+        image_std: Optional[Union[float, List[float]]] = None,\n+        do_convert_rgb: bool = None,\n+        return_tensors: Optional[Union[str, TensorType]] = None,\n+        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        **kwargs,\n+    ) -> BatchMixFeature:\n+        \"\"\"\n+        Preprocess an image or batch of images.\n+\n+        Args:\n+            images (`ImageInput`):\n+                Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If\n+                passing in images with pixel values between 0 and 1, set `do_rescale=False`.\n+            do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n+                Whether to resize the image.\n+            size (`Dict[str, int]`, *optional*, defaults to `self.size`):\n+                Describes the maximum input dimensions to the model.\n+            patch_size (`Dict[str, int]`, *optional*, defaults to `self.patch_size`):\n+                Patch size in the model. Used to calculate the image after resizing.\n+            resample (`PILImageResampling` or `InterpolationMode`, *optional*, defaults to self.resample):\n+                Resampling filter to use if resizing the image. This can be one of the enum `PILImageResampling`. Only\n+                has an effect if `do_resize` is set to `True`.\n+            do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):\n+                Whether to rescale the image.\n+            rescale_factor (`float`, *optional*, defaults to `self.rescale_factor`):\n+                Rescale factor to rescale the image by if `do_rescale` is set to `True`.\n+            do_normalize (`bool`, *optional*, defaults to `self.do_normalize`):\n+                Whether to normalize the image.\n+            image_mean (`float` or `List[float]`, *optional*, defaults to `self.image_mean`):\n+                Image mean to use for normalization. Only has an effect if `do_normalize` is set to `True`.\n+            image_std (`float` or `List[float]`, *optional*, defaults to `self.image_std`):\n+                Image standard deviation to use for normalization. Only has an effect if `do_normalize` is set to\n+                `True`.\n+            do_convert_rgb (`bool`, *optional*, defaults to `self.do_convert_rgb`):\n+                Whether to convert the image to RGB.\n+            return_tensors (`str` or `TensorType`, *optional*):\n+                The type of tensors to return. Can be one of:\n+                - Unset: Return a list of `np.ndarray`.\n+                - `TensorType.TENSORFLOW` or `'tf'`: Return a batch of type `tf.Tensor`.\n+                - `TensorType.PYTORCH` or `'pt'`: Return a batch of type `torch.Tensor`.\n+                - `TensorType.NUMPY` or `'np'`: Return a batch of type `np.ndarray`.\n+                - `TensorType.JAX` or `'jax'`: Return a batch of type `jax.numpy.ndarray`.\n+            data_format (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`):\n+                The channel dimension format for the output image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - Unset: Use the channel dimension format of the input image.\n+            input_data_format (`ChannelDimension` or `str`, *optional*):\n+                The channel dimension format for the input image. If unset, the channel dimension format is inferred\n+                from the input image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n+        \"\"\"\n+        patch_size = patch_size if patch_size is not None else self.patch_size\n+        patch_size = get_size_dict(patch_size, default_to_square=True)\n+\n+        do_resize = do_resize if do_resize is not None else self.do_resize\n+        size = size if size is not None else self.size\n+        resample = resample if resample is not None else self.resample\n+        do_rescale = do_rescale if do_rescale is not None else self.do_rescale\n+        rescale_factor = rescale_factor if rescale_factor is not None else self.rescale_factor\n+        do_normalize = do_normalize if do_normalize is not None else self.do_normalize\n+        image_mean = image_mean if image_mean is not None else self.image_mean\n+        image_std = image_std if image_std is not None else self.image_std\n+        do_convert_rgb = do_convert_rgb if do_convert_rgb is not None else self.do_convert_rgb\n+        device = kwargs.pop(\"device\", None)\n+\n+        validate_kwargs(captured_kwargs=kwargs.keys(), valid_processor_keys=self._valid_processor_keys)\n+\n+        images_list = make_list_of_images(images)\n+        image_type = get_image_type(images_list[0][0])\n+\n+        if image_type not in [ImageType.PIL, ImageType.TORCH, ImageType.NUMPY]:\n+            raise ValueError(f\"Unsupported input image type {image_type}\")\n+\n+        validate_fast_preprocess_arguments(\n+            do_rescale=do_rescale,\n+            rescale_factor=rescale_factor,\n+            do_normalize=do_normalize,\n+            image_mean=image_mean,\n+            image_std=image_std,\n+            do_resize=do_resize,\n+            size=size,\n+            resample=resample,\n+            return_tensors=return_tensors,\n+            data_format=data_format,\n+        )\n+\n+        if do_convert_rgb:\n+            images_list = [[convert_to_rgb(image) for image in images] for images in images_list]\n+\n+        if image_type == ImageType.PIL:\n+            images_list = [[F.pil_to_tensor(image) for image in images] for images in images_list]\n+        elif image_type == ImageType.NUMPY:\n+            # not using F.to_tensor as it doesn't handle (C, H, W) numpy arrays\n+            images_list = [[torch.from_numpy(image).contiguous() for image in images] for images in images_list]\n+\n+        if device is not None:\n+            images_list = [[image.to(device) for image in images] for images in images_list]\n+\n+        # We assume that all images have the same channel dimension format.\n+        if input_data_format is None:\n+            input_data_format = infer_channel_dimension_format(images_list[0][0])\n+        if input_data_format == ChannelDimension.LAST:\n+            images_list = [[image.permute(2, 0, 1).contiguous() for image in images] for images in images_list]\n+            input_data_format = ChannelDimension.FIRST\n+\n+        if do_rescale and do_normalize:\n+            # fused rescale and normalize\n+            new_mean = torch.tensor(image_mean, device=images_list[0][0].device) * (1.0 / rescale_factor)\n+            new_std = torch.tensor(image_std, device=images_list[0][0].device) * (1.0 / rescale_factor)\n+\n+        batch_images = []\n+        batch_image_sizes = []\n+        for sample_images in images_list:\n+            images = []\n+            image_sizes = []\n+            for image in sample_images:\n+                if do_resize:\n+                    interpolation = (\n+                        pil_torch_interpolation_mapping[resample]\n+                        if isinstance(resample, (PILImageResampling, int))\n+                        else resample\n+                    )\n+                    image = self.resize(\n+                        image=image,\n+                        size=size,\n+                        patch_size=patch_size,\n+                        interpolation=interpolation,\n+                    )\n+\n+                if do_rescale and do_normalize:\n+                    # fused rescale and normalize\n+                    image = F.normalize(image.to(dtype=torch.float32), new_mean, new_std)\n+                elif do_rescale:\n+                    image = image * rescale_factor\n+                elif do_normalize:\n+                    image = F.normalize(image, image_mean, image_std)\n+\n+                images.append(image)\n+                image_sizes.append(get_image_size(image, input_data_format))\n+            batch_images.append(images)\n+            batch_image_sizes.append(image_sizes)\n+\n+        return BatchMixFeature(data={\"pixel_values\": batch_images, \"image_sizes\": batch_image_sizes}, tensor_type=None)"
        },
        {
            "sha": "747f75386490fcb335376ac3a388dc730a679372",
            "filename": "src/transformers/utils/dummy_torchvision_objects.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/9d6f0ddcec215b24006c74acb7875fd2706a3a84/src%2Ftransformers%2Futils%2Fdummy_torchvision_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9d6f0ddcec215b24006c74acb7875fd2706a3a84/src%2Ftransformers%2Futils%2Fdummy_torchvision_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_torchvision_objects.py?ref=9d6f0ddcec215b24006c74acb7875fd2706a3a84",
            "patch": "@@ -23,6 +23,13 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torchvision\"])\n \n \n+class PixtralImageProcessorFast(metaclass=DummyObject):\n+    _backends = [\"torchvision\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torchvision\"])\n+\n+\n class RTDetrImageProcessorFast(metaclass=DummyObject):\n     _backends = [\"torchvision\"]\n "
        },
        {
            "sha": "8b49b5aa60b99a93c59d3dc13cada3bd7fb2d5f2",
            "filename": "tests/models/pixtral/test_image_processing_pixtral.py",
            "status": "modified",
            "additions": 128,
            "deletions": 67,
            "changes": 195,
            "blob_url": "https://github.com/huggingface/transformers/blob/9d6f0ddcec215b24006c74acb7875fd2706a3a84/tests%2Fmodels%2Fpixtral%2Ftest_image_processing_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9d6f0ddcec215b24006c74acb7875fd2706a3a84/tests%2Fmodels%2Fpixtral%2Ftest_image_processing_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpixtral%2Ftest_image_processing_pixtral.py?ref=9d6f0ddcec215b24006c74acb7875fd2706a3a84",
            "patch": "@@ -14,12 +14,14 @@\n # limitations under the License.\n \n import random\n+import time\n import unittest\n \n import numpy as np\n+import requests\n \n from transformers.testing_utils import require_torch, require_vision\n-from transformers.utils import is_torch_available, is_vision_available\n+from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n \n@@ -32,6 +34,9 @@\n \n     from transformers import PixtralImageProcessor\n \n+    if is_torchvision_available():\n+        from transformers import PixtralImageProcessorFast\n+\n \n class PixtralImageProcessingTester(unittest.TestCase):\n     def __init__(\n@@ -51,6 +56,7 @@ def __init__(\n         image_std=[0.26862954, 0.26130258, 0.27577711],\n         do_convert_rgb=True,\n     ):\n+        super().__init__()\n         size = size if size is not None else {\"longest_edge\": 24}\n         patch_size = patch_size if patch_size is not None else {\"height\": 8, \"width\": 8}\n         self.parent = parent\n@@ -128,6 +134,7 @@ def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=F\n @require_vision\n class PixtralImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n     image_processing_class = PixtralImageProcessor if is_vision_available() else None\n+    fast_image_processing_class = PixtralImageProcessorFast if is_torchvision_available() else None\n \n     def setUp(self):\n         super().setUp()\n@@ -138,79 +145,133 @@ def image_processor_dict(self):\n         return self.image_processor_tester.prepare_image_processor_dict()\n \n     def test_image_processor_properties(self):\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        self.assertTrue(hasattr(image_processing, \"do_resize\"))\n-        self.assertTrue(hasattr(image_processing, \"size\"))\n-        self.assertTrue(hasattr(image_processing, \"patch_size\"))\n-        self.assertTrue(hasattr(image_processing, \"do_rescale\"))\n-        self.assertTrue(hasattr(image_processing, \"rescale_factor\"))\n-        self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n-        self.assertTrue(hasattr(image_processing, \"image_mean\"))\n-        self.assertTrue(hasattr(image_processing, \"image_std\"))\n-        self.assertTrue(hasattr(image_processing, \"do_convert_rgb\"))\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            self.assertTrue(hasattr(image_processing, \"do_resize\"))\n+            self.assertTrue(hasattr(image_processing, \"size\"))\n+            self.assertTrue(hasattr(image_processing, \"patch_size\"))\n+            self.assertTrue(hasattr(image_processing, \"do_rescale\"))\n+            self.assertTrue(hasattr(image_processing, \"rescale_factor\"))\n+            self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n+            self.assertTrue(hasattr(image_processing, \"image_mean\"))\n+            self.assertTrue(hasattr(image_processing, \"image_std\"))\n+            self.assertTrue(hasattr(image_processing, \"do_convert_rgb\"))\n \n     def test_call_pil(self):\n-        # Initialize image_processing\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        # create random PIL images\n-        image_inputs_list = self.image_processor_tester.prepare_image_inputs()\n-        for image_inputs in image_inputs_list:\n-            for image in image_inputs:\n-                self.assertIsInstance(image, Image.Image)\n-\n-        # Test not batched input\n-        encoded_images = image_processing(image_inputs_list[0][0], return_tensors=\"pt\").pixel_values\n-        expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs_list[0][0])\n-        self.assertEqual(tuple(encoded_images[0][0].shape), expected_output_image_shape)\n-\n-        # Test batched\n-        batch_encoded_images = image_processing(image_inputs_list, return_tensors=\"pt\").pixel_values\n-        for encoded_images, images in zip(batch_encoded_images, image_inputs_list):\n-            for encoded_image, image in zip(encoded_images, images):\n-                expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image)\n-                self.assertEqual(tuple(encoded_image.shape), expected_output_image_shape)\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processing\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            # create random PIL images\n+            image_inputs_list = self.image_processor_tester.prepare_image_inputs()\n+            for image_inputs in image_inputs_list:\n+                for image in image_inputs:\n+                    self.assertIsInstance(image, Image.Image)\n+\n+            # Test not batched input\n+            encoded_images = image_processing(image_inputs_list[0][0], return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(\n+                image_inputs_list[0][0]\n+            )\n+            self.assertEqual(tuple(encoded_images[0][0].shape), expected_output_image_shape)\n+\n+            # Test batched\n+            batch_encoded_images = image_processing(image_inputs_list, return_tensors=\"pt\").pixel_values\n+            for encoded_images, images in zip(batch_encoded_images, image_inputs_list):\n+                for encoded_image, image in zip(encoded_images, images):\n+                    expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image)\n+                    self.assertEqual(tuple(encoded_image.shape), expected_output_image_shape)\n \n     def test_call_numpy(self):\n-        # Initialize image_processing\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        # create random numpy tensors\n-        image_inputs_list = self.image_processor_tester.prepare_image_inputs(numpify=True)\n-        for image_inputs in image_inputs_list:\n-            for image in image_inputs:\n-                self.assertIsInstance(image, np.ndarray)\n-\n-        # Test not batched input\n-        encoded_images = image_processing(image_inputs_list[0][0], return_tensors=\"pt\").pixel_values\n-        expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs_list[0][0])\n-        self.assertEqual(tuple(encoded_images[0][0].shape), expected_output_image_shape)\n-\n-        # Test batched\n-        batch_encoded_images = image_processing(image_inputs_list, return_tensors=\"pt\").pixel_values\n-        for encoded_images, images in zip(batch_encoded_images, image_inputs_list):\n-            for encoded_image, image in zip(encoded_images, images):\n-                expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image)\n-                self.assertEqual(tuple(encoded_image.shape), expected_output_image_shape)\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processing\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            # create random numpy tensors\n+            image_inputs_list = self.image_processor_tester.prepare_image_inputs(numpify=True)\n+            for image_inputs in image_inputs_list:\n+                for image in image_inputs:\n+                    self.assertIsInstance(image, np.ndarray)\n+\n+            # Test not batched input\n+            encoded_images = image_processing(image_inputs_list[0][0], return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(\n+                image_inputs_list[0][0]\n+            )\n+            self.assertEqual(tuple(encoded_images[0][0].shape), expected_output_image_shape)\n+\n+            # Test batched\n+            batch_encoded_images = image_processing(image_inputs_list, return_tensors=\"pt\").pixel_values\n+            for encoded_images, images in zip(batch_encoded_images, image_inputs_list):\n+                for encoded_image, image in zip(encoded_images, images):\n+                    expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image)\n+                    self.assertEqual(tuple(encoded_image.shape), expected_output_image_shape)\n \n     def test_call_pytorch(self):\n-        # Initialize image_processing\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        # create random PyTorch tensors\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processing\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            # create random PyTorch tensors\n+            image_inputs_list = self.image_processor_tester.prepare_image_inputs(torchify=True)\n+            for image_inputs in image_inputs_list:\n+                for image in image_inputs:\n+                    self.assertIsInstance(image, torch.Tensor)\n+\n+            # Test not batched input\n+            encoded_images = image_processing(image_inputs_list[0][0], return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(\n+                image_inputs_list[0][0]\n+            )\n+            self.assertEqual(tuple(encoded_images[0][0].shape), expected_output_image_shape)\n+\n+            # Test batched\n+            batch_encoded_images = image_processing(image_inputs_list, return_tensors=\"pt\").pixel_values\n+            for encoded_images, images in zip(batch_encoded_images, image_inputs_list):\n+                for encoded_image, image in zip(encoded_images, images):\n+                    expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image)\n+                    self.assertEqual(tuple(encoded_image.shape), expected_output_image_shape)\n+\n+    @require_vision\n+    @require_torch\n+    def test_fast_is_faster_than_slow(self):\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping speed test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping speed test as one of the image processors is not defined\")\n+\n+        def measure_time(image_processor, image):\n+            start = time.time()\n+            _ = image_processor(image, return_tensors=\"pt\")\n+            return time.time() - start\n+\n         image_inputs_list = self.image_processor_tester.prepare_image_inputs(torchify=True)\n-        for image_inputs in image_inputs_list:\n-            for image in image_inputs:\n-                self.assertIsInstance(image, torch.Tensor)\n-\n-        # Test not batched input\n-        encoded_images = image_processing(image_inputs_list[0][0], return_tensors=\"pt\").pixel_values\n-        expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs_list[0][0])\n-        self.assertEqual(tuple(encoded_images[0][0].shape), expected_output_image_shape)\n-\n-        # Test batched\n-        batch_encoded_images = image_processing(image_inputs_list, return_tensors=\"pt\").pixel_values\n-        for encoded_images, images in zip(batch_encoded_images, image_inputs_list):\n-            for encoded_image, image in zip(encoded_images, images):\n-                expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image)\n-                self.assertEqual(tuple(encoded_image.shape), expected_output_image_shape)\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        fast_time = measure_time(image_processor_fast, image_inputs_list)\n+        slow_time = measure_time(image_processor_slow, image_inputs_list)\n+\n+        self.assertLessEqual(fast_time, slow_time)\n+\n+    @require_vision\n+    @require_torch\n+    def test_slow_fast_equivalence(self):\n+        dummy_image = Image.open(\n+            requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw\n+        )\n+\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n+\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        encoding_slow = image_processor_slow(dummy_image, return_tensors=\"pt\")\n+        encoding_fast = image_processor_fast(dummy_image, return_tensors=\"pt\")\n+\n+        self.assertTrue(torch.allclose(encoding_slow.pixel_values[0][0], encoding_fast.pixel_values[0][0], atol=1e-2))\n \n     @unittest.skip(reason=\"PixtralImageProcessor doesn't treat 4 channel PIL and numpy consistently yet\")  # FIXME Amy\n     def test_call_numpy_4_channels(self):"
        }
    ],
    "stats": {
        "total": 634,
        "additions": 560,
        "deletions": 74
    }
}