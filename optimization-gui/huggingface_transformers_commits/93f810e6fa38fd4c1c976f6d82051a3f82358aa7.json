{
    "author": "gante",
    "message": "[docstrings / type hints] Update outdated annotations for `past_key_values`  (#40803)\n\n* some fixes\n\n* nits\n\n* indentation\n\n* indentation\n\n* a bunch of type hints\n\n* bulk changes",
    "sha": "93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
    "files": [
        {
            "sha": "b073df9e73ab2e12f999856bf8e724d2f4b72b1d",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -166,7 +166,7 @@ class GenerateDecoderOnlyOutput(ModelOutput):\n         hidden_states (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_hidden_states=True`):\n             Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n             `torch.FloatTensor` of shape `(batch_size, generated_length, hidden_size)`.\n-        past_key_values (`tuple(tuple(torch.FloatTensor)))`, *optional*, returned when `use_cache=True`):\n+        past_key_values (`Cache`, *optional*, returned when `use_cache=True`):\n             Returns the model cache, used to speed up decoding. Different models have a different cache format, check\n             the model's documentation. Usually, a [`~cache_utils.Cache`] instance.\n     \"\"\"\n@@ -176,7 +176,7 @@ class GenerateDecoderOnlyOutput(ModelOutput):\n     logits: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[tuple[torch.FloatTensor]]] = None\n     hidden_states: Optional[tuple[tuple[torch.FloatTensor]]] = None\n-    past_key_values: Optional[tuple[tuple[tuple[torch.FloatTensor]]]] = None\n+    past_key_values: Optional[Cache] = None\n \n \n @dataclass\n@@ -211,7 +211,7 @@ class GenerateEncoderDecoderOutput(ModelOutput):\n         decoder_hidden_states (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_hidden_states=True`):\n             Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n             `torch.FloatTensor` of shape `(batch_size, generated_length, hidden_size)`.\n-        past_key_values (`tuple(tuple(torch.FloatTensor)))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        past_key_values (`Cache`, *optional*, returned when `use_cache=True`):\n             Returns the model cache, used to speed up decoding. Different models have a different cache format, check\n             the model's documentation. Usually, a [`~cache_utils.Cache`] instance.\n     \"\"\"\n@@ -224,7 +224,7 @@ class GenerateEncoderDecoderOutput(ModelOutput):\n     decoder_attentions: Optional[tuple[tuple[torch.FloatTensor]]] = None\n     cross_attentions: Optional[tuple[tuple[torch.FloatTensor]]] = None\n     decoder_hidden_states: Optional[tuple[tuple[torch.FloatTensor]]] = None\n-    past_key_values: Optional[tuple[tuple[tuple[torch.FloatTensor]]]] = None\n+    past_key_values: Optional[Cache] = None\n \n \n @dataclass\n@@ -256,7 +256,7 @@ class GenerateBeamDecoderOnlyOutput(ModelOutput):\n         hidden_states (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_hidden_states=True`):\n             Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n             `torch.FloatTensor` of shape `(batch_size*num_beams*num_return_sequences, generated_length, hidden_size)`.\n-        past_key_values (`tuple(tuple(torch.FloatTensor)))`, *optional*, returned when `use_cache=True`):\n+        past_key_values (`Cache`, *optional*, returned when `use_cache=True`):\n             Returns the model cache, used to speed up decoding. Different models have a different cache format, check\n             the model's documentation. Usually, a [`~cache_utils.Cache`] instance.\n     \"\"\"\n@@ -268,7 +268,7 @@ class GenerateBeamDecoderOnlyOutput(ModelOutput):\n     beam_indices: Optional[torch.LongTensor] = None\n     attentions: Optional[tuple[tuple[torch.FloatTensor]]] = None\n     hidden_states: Optional[tuple[tuple[torch.FloatTensor]]] = None\n-    past_key_values: Optional[tuple[tuple[tuple[torch.FloatTensor]]]] = None\n+    past_key_values: Optional[Cache] = None\n \n \n @dataclass\n@@ -310,7 +310,7 @@ class GenerateBeamEncoderDecoderOutput(ModelOutput):\n         decoder_hidden_states (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_hidden_states=True`):\n             Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n             `torch.FloatTensor` of shape `(batch_size*num_beams*num_return_sequences, generated_length, hidden_size)`.\n-        past_key_values (`tuple(tuple(torch.FloatTensor)))`, *optional*, returned when `use_cache=True`):\n+        past_key_values (`Cache`, *optional*, returned when `use_cache=True`):\n             Returns the model cache, used to speed up decoding. Different models have a different cache format, check\n             the model's documentation. Usually, a [`~cache_utils.Cache`] instance.\n     \"\"\"\n@@ -325,7 +325,7 @@ class GenerateBeamEncoderDecoderOutput(ModelOutput):\n     decoder_attentions: Optional[tuple[tuple[torch.FloatTensor]]] = None\n     cross_attentions: Optional[tuple[tuple[torch.FloatTensor]]] = None\n     decoder_hidden_states: Optional[tuple[tuple[torch.FloatTensor]]] = None\n-    past_key_values: Optional[tuple[tuple[tuple[torch.FloatTensor]]]] = None\n+    past_key_values: Optional[Cache] = None\n \n \n # TODO (joao): remove the equivalent classes and typing shortcuts below in v5"
        },
        {
            "sha": "f3261909dd031112765a2fa758d7696bd1beeed3",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -873,8 +873,7 @@ class AriaCausalLMOutputWithPast(ModelOutput):\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n         `past_key_values` input) to speed up sequential decoding.\n@@ -885,7 +884,7 @@ class AriaCausalLMOutputWithPast(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[list[torch.FloatTensor]] = None\n+    past_key_values: Optional[Cache] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n     image_hidden_states: Optional[torch.FloatTensor] = None\n@@ -900,8 +899,7 @@ class AriaCausalLMOutputWithPast(ModelOutput):\n class AriaModelOutputWithPast(BaseModelOutputWithPast):\n     r\"\"\"\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n         `past_key_values` input) to speed up sequential decoding."
        },
        {
            "sha": "efa952a5a28ba463e794c2a5cabf3d2e0d7e5662",
            "filename": "src/transformers/models/autoformer/modeling_autoformer.py",
            "status": "modified",
            "additions": 12,
            "deletions": 19,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -62,11 +62,8 @@ class AutoFormerDecoderOutput(ModelOutput):\n         hidden_size)` is output.\n     trend (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n         Trend tensor for each time series.\n-    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if\n-        `config.is_encoder_decoder=True` 2 additional tensors of shape `(batch_size, num_heads,\n-        encoder_sequence_length, embed_size_per_head)`.\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if\n         `config.is_encoder_decoder=True` in the cross-attention blocks) that can be used (see `past_key_values`\n@@ -81,7 +78,7 @@ class AutoFormerDecoderOutput(ModelOutput):\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n     trend: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None\n+    past_key_values: Optional[Cache] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n     cross_attentions: Optional[tuple[torch.FloatTensor]] = None\n@@ -102,10 +99,8 @@ class AutoformerModelOutput(ModelOutput):\n         hidden_size)` is output.\n     trend (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n         Trend tensor for each time series.\n-    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n-        `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n         blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n@@ -121,7 +116,7 @@ class AutoformerModelOutput(ModelOutput):\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n     trend: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None\n+    past_key_values: Optional[Cache] = None\n     decoder_hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     decoder_attentions: Optional[tuple[torch.FloatTensor]] = None\n     cross_attentions: Optional[tuple[torch.FloatTensor]] = None\n@@ -781,7 +776,7 @@ def forward(\n                 `(encoder_attention_heads,)`.\n             cross_attn_layer_head_mask (`torch.FloatTensor`): mask for cross-attention heads in a given layer of\n                 size `(decoder_attention_heads,)`.\n-            past_key_values (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n+            past_key_values (`Cache`): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -1064,7 +1059,7 @@ def forward(\n         encoder_attention_mask: Optional[torch.LongTensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -1107,10 +1102,8 @@ def forward(\n                 - 1 indicates the head is **not masked**,\n                 - 0 indicates the head is **masked**.\n \n-            past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-                Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n-                shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n-                shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n+            past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+                It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n                 Contains pre-computed hidden-states (key and values in the self-attention blocks and in the\n                 cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n@@ -1440,7 +1433,7 @@ def forward(\n         decoder_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[list[torch.FloatTensor]] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_hidden_states: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         use_cache: Optional[bool] = None,\n@@ -1708,7 +1701,7 @@ def forward(\n         decoder_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[list[torch.FloatTensor]] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_hidden_states: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "fe9c2f72b05b980b6ad187afcdc2f591ce5361ef",
            "filename": "src/transformers/models/aya_vision/modeling_aya_vision.py",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -118,8 +118,7 @@ class AyaVisionCausalLMOutputWithPast(ModelOutput):\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n         `past_key_values` input) to speed up sequential decoding.\n@@ -130,7 +129,7 @@ class AyaVisionCausalLMOutputWithPast(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[list[torch.FloatTensor]] = None\n+    past_key_values: Optional[Cache] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n     image_hidden_states: Optional[torch.FloatTensor] = None\n@@ -145,8 +144,7 @@ class AyaVisionCausalLMOutputWithPast(ModelOutput):\n class AyaVisionModelOutputWithPast(BaseModelOutputWithPast):\n     r\"\"\"\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n         `past_key_values` input) to speed up sequential decoding."
        },
        {
            "sha": "8770e3e0691b00abf0db014db1d566f5fe6fc168",
            "filename": "src/transformers/models/bark/modeling_bark.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -23,7 +23,7 @@\n from torch import nn\n from torch.nn import functional as F\n \n-from ...cache_utils import DynamicCache\n+from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...generation.logits_process import (\n     AlternatingCodebooksLogitsProcessor,\n@@ -437,7 +437,7 @@ def prepare_inputs_for_generation(\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[tuple[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n         head_mask: Optional[torch.Tensor] = None,"
        },
        {
            "sha": "55c7654fe2e54103391a881cc857479260b45534",
            "filename": "src/transformers/models/bart/modeling_bart.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -405,7 +405,7 @@ def forward(\n                 `(encoder_attention_heads,)`.\n             cross_attn_layer_head_mask (`torch.FloatTensor`): mask for cross-attention heads in a given layer of\n                 size `(decoder_attention_heads,)`.\n-            past_key_values (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n+            past_key_values (`Cache`): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -988,9 +988,7 @@ def forward(\n                 - 0 indicates the head is **masked**.\n \n             past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-                Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n-                shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n-                shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n+                It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n                 Contains pre-computed hidden-states (key and values in the self-attention blocks and in the\n                 cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding."
        },
        {
            "sha": "20edbf6383c5cbdfa06e66aa1ec404b957f8e700",
            "filename": "src/transformers/models/bert/modeling_bert.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -613,7 +613,7 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,\n@@ -887,7 +887,7 @@ def forward(\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -1171,7 +1171,7 @@ def forward(\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[list[torch.Tensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,"
        },
        {
            "sha": "f29d22d06f83dba4ef24487a8e15afe17895db15",
            "filename": "src/transformers/models/bert_generation/modeling_bert_generation.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -363,7 +363,7 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,\n@@ -641,7 +641,7 @@ def forward(\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -796,7 +796,7 @@ def forward(\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,"
        },
        {
            "sha": "20a5a08c246a4b8f601eebe5efe9397d41781e28",
            "filename": "src/transformers/models/big_bird/modeling_big_bird.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -1852,7 +1852,7 @@ def forward(\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -2386,7 +2386,7 @@ def forward(\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,"
        },
        {
            "sha": "90f3c886ad93e5ba4b605e142a09fbb3b674c6e4",
            "filename": "src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -1480,7 +1480,7 @@ def forward(\n                 `(encoder_attention_heads,)`.\n             cross_attn_layer_head_mask (`torch.FloatTensor`): mask for cross-attention heads in a given layer of\n                 size `(decoder_attention_heads,)`.\n-            past_key_values (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n+            past_key_values (`Cache`): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -2148,9 +2148,7 @@ def forward(\n                 - 0 indicates the head is **masked**.\n \n             past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-                Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n-                shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n-                shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n+                It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n                 Contains pre-computed hidden-states (key and values in the self-attention blocks and in the\n                 cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n@@ -2945,7 +2943,7 @@ def forward(\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.Tensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "8690082625a7b8ec9a15ae5376c85f09633a43b4",
            "filename": "src/transformers/models/biogpt/modeling_biogpt.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -295,7 +295,7 @@ def forward(\n                 `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n             layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size\n                 `(encoder_attention_heads,)`.\n-            past_key_values (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n+            past_key_values (`Cache`): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -517,7 +517,7 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.Tensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n@@ -688,7 +688,7 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.Tensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n@@ -770,7 +770,7 @@ def forward(\n         token_type_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.Tensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -862,7 +862,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.Tensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "5753e066913f1464c0b7969a0dab761edade78ea",
            "filename": "src/transformers/models/biogpt/modular_biogpt.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -118,7 +118,7 @@ def forward(\n                 `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n             layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size\n                 `(encoder_attention_heads,)`.\n-            past_key_values (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n+            past_key_values (`Cache`): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -340,7 +340,7 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.Tensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n@@ -511,7 +511,7 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.Tensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n@@ -593,7 +593,7 @@ def forward(\n         token_type_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.Tensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -685,7 +685,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.Tensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "56561612bac20aea3a2ba5066070bca00e7cab0e",
            "filename": "src/transformers/models/blenderbot/modeling_blenderbot.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -396,7 +396,7 @@ def forward(\n                 `(encoder_attention_heads,)`.\n             cross_attn_layer_head_mask (`torch.FloatTensor`): mask for cross-attention heads in a given layer of\n                 size `(decoder_attention_heads,)`.\n-            past_key_values (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n+            past_key_values (`Cache`): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -942,9 +942,7 @@ def forward(\n                 - 0 indicates the head is **masked**.\n \n             past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-                Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n-                shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n-                shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n+                It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n                 Contains pre-computed hidden-states (key and values in the self-attention blocks and in the\n                 cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding."
        },
        {
            "sha": "556fbeb4d0cbf7b0c272e6573b04dd0ee2692325",
            "filename": "src/transformers/models/blenderbot_small/modeling_blenderbot_small.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -388,7 +388,7 @@ def forward(\n                 `(encoder_attention_heads,)`.\n             cross_attn_layer_head_mask (`torch.FloatTensor`): mask for cross-attention heads in a given layer of\n                 size `(decoder_attention_heads,)`.\n-            past_key_values (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n+            past_key_values (`Cache`): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -928,9 +928,7 @@ def forward(\n                 - 0 indicates the head is **masked**.\n \n             past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-                Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n-                shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n-                shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n+                It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n                 Contains pre-computed hidden-states (key and values in the self-attention blocks and in the\n                 cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding."
        },
        {
            "sha": "0eb140685fdaac243c1a7f80741d2e1900986638",
            "filename": "src/transformers/models/blip/modeling_blip_text.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -416,7 +416,7 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,\n@@ -699,7 +699,7 @@ def forward(\n         encoder_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -716,7 +716,7 @@ def forward(\n             the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n             - 1 for tokens that are **not masked**,\n             - 0 for tokens that are **masked**.\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*):\n+        past_key_values (`Cache`, *optional*):\n             Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n             If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n             don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n@@ -870,7 +870,7 @@ def forward(\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[list[torch.Tensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -893,7 +893,7 @@ def forward(\n             Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in\n             `[-100, 0, ..., config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are\n             ignored (masked), the loss is only computed for the tokens with labels n `[0, ..., config.vocab_size]`\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*):\n+        past_key_values (`Cache`, *optional*):\n             Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n             If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n             don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all"
        },
        {
            "sha": "2c798fcf4772de9e3e3b1a9296ebf97b39b22c13",
            "filename": "src/transformers/models/bridgetower/modeling_bridgetower.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -750,7 +750,7 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,\n@@ -1041,7 +1041,7 @@ def forward(\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,"
        },
        {
            "sha": "f566bab0b8ed6fefd350cb3b7ec35464153df3a2",
            "filename": "src/transformers/models/camembert/modeling_camembert.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -570,7 +570,7 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,\n@@ -809,7 +809,7 @@ def forward(\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -1462,7 +1462,7 @@ def forward(\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,"
        },
        {
            "sha": "6b2ab15bbb9ebc74401019c624a6dbd9ff85c97c",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -405,7 +405,7 @@ def forward(\n             use_cache (`bool`, *optional*):\n                 If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                 (see `past_key_values`).\n-            past_key_values (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            past_key_values (`Cache`, *optional*): cached past key and value projection states\n             cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n                 Indices depicting the position of the input sequence tokens in the sequence\n             kwargs (`dict`, *optional*):\n@@ -475,7 +475,7 @@ def forward(\n                 query_sequence_length, key_sequence_length)` if default attention is used.\n             position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Indices of positions of each input sequence tokens in the position embeddings\n-            past_key_values (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            past_key_values (`Cache`, *optional*): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail."
        },
        {
            "sha": "c10d0c855ce13efb911ddeedebb23725e11b38e4",
            "filename": "src/transformers/models/chinese_clip/modeling_chinese_clip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -22,6 +22,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n+from ...cache_utils import Cache\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutput,\n@@ -874,7 +875,7 @@ def forward(\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,"
        },
        {
            "sha": "acef62d5da21ade31d7a5b1a70553af1bcde0348",
            "filename": "src/transformers/models/clvp/modeling_clvp.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -1029,7 +1029,7 @@ def forward(\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.Tensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -1195,7 +1195,7 @@ def forward(\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.Tensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -1361,7 +1361,7 @@ def prepare_inputs_for_generation(\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.Tensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "1dfa0ce0be33a6f3383e76d81de4f3895ab70bea",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -307,7 +307,7 @@ def forward(\n             attention_mask (`torch.FloatTensor`, *optional*):\n                 attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,\n                 query_sequence_length, key_sequence_length)` if default attention is used.\n-            past_key_values (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            past_key_values (`Cache`, *optional*): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail."
        },
        {
            "sha": "62b6b1988daa4c926409568cb3ac0b4a5e1e1bad",
            "filename": "src/transformers/models/cohere/modular_cohere.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodular_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodular_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodular_cohere.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -225,7 +225,7 @@ def forward(\n             attention_mask (`torch.FloatTensor`, *optional*):\n                 attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,\n                 query_sequence_length, key_sequence_length)` if default attention is used.\n-            past_key_values (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            past_key_values (`Cache`, *optional*): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail."
        },
        {
            "sha": "bab804aab67ec2fa1006cb70ac83766e8f2e71aa",
            "filename": "src/transformers/models/cohere2/modeling_cohere2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -284,7 +284,7 @@ def forward(\n             attention_mask (`torch.FloatTensor`, *optional*):\n                 attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,\n                 query_sequence_length, key_sequence_length)` if default attention is used.\n-            past_key_values (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            past_key_values (`Cache`, *optional*): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail."
        },
        {
            "sha": "1dc993967b5c767fe93f63fccb954b42a52a1c28",
            "filename": "src/transformers/models/cohere2_vision/modeling_cohere2_vision.py",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fmodeling_cohere2_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fmodeling_cohere2_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fmodeling_cohere2_vision.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -84,8 +84,7 @@ def forward(self, image_features):\n class Cohere2VisionModelOutputWithPast(BaseModelOutputWithPast):\n     r\"\"\"\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n         `past_key_values` input) to speed up sequential decoding.\n@@ -110,8 +109,7 @@ class Cohere2VisionCausalLMOutputWithPast(ModelOutput):\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n         `past_key_values` input) to speed up sequential decoding.\n@@ -122,7 +120,7 @@ class Cohere2VisionCausalLMOutputWithPast(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[list[torch.FloatTensor]] = None\n+    past_key_values: Optional[Cache] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n     image_hidden_states: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "a59224e20456d0c2aef70be34713cd60089ddef7",
            "filename": "src/transformers/models/colpali/modeling_colpali.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodeling_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodeling_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodeling_colpali.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -15,7 +15,7 @@\n \"\"\"PyTorch ColPali model\"\"\"\n \n from dataclasses import dataclass\n-from typing import Optional, Union\n+from typing import Optional\n \n import torch\n from torch import nn\n@@ -67,8 +67,7 @@ class ColPaliForRetrievalOutput(ModelOutput):\n     embeddings (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n         The embeddings of the model.\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n         `past_key_values` input) to speed up sequential decoding.\n@@ -79,7 +78,7 @@ class ColPaliForRetrievalOutput(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     embeddings: Optional[torch.Tensor] = None\n-    past_key_values: Optional[Union[list[torch.FloatTensor], Cache]] = None\n+    past_key_values: Optional[Cache] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n     image_hidden_states: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "d448962f4e97f9c7b2880add53d644306565742f",
            "filename": "src/transformers/models/colqwen2/modeling_colqwen2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodeling_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodeling_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodeling_colqwen2.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -20,7 +20,7 @@\n # limitations under the License.\n \n from dataclasses import dataclass\n-from typing import Optional, Union\n+from typing import Optional\n \n from torch import nn\n \n@@ -75,16 +75,15 @@ class ColQwen2ForRetrievalOutput(ModelOutput):\n     embeddings (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n         The embeddings of the model.\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n         `past_key_values` input) to speed up sequential decoding.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n     embeddings: Optional[torch.Tensor] = None\n-    past_key_values: Optional[Union[list[torch.FloatTensor], Cache]] = None\n+    past_key_values: Optional[Cache] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n "
        },
        {
            "sha": "72469fef7a21a91f5a34e6ad1febaf4620fa2f40",
            "filename": "src/transformers/models/colqwen2/modular_colqwen2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -279,16 +279,15 @@ class ColQwen2ForRetrievalOutput(ModelOutput):\n     embeddings (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n         The embeddings of the model.\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n         `past_key_values` input) to speed up sequential decoding.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n     embeddings: Optional[torch.Tensor] = None\n-    past_key_values: Optional[Union[list[torch.FloatTensor], Cache]] = None\n+    past_key_values: Optional[Cache] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n "
        },
        {
            "sha": "8eb5bc4d1968c34590dff92722cb6ce0c293803f",
            "filename": "src/transformers/models/cpmant/modeling_cpmant.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fcpmant%2Fmodeling_cpmant.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fcpmant%2Fmodeling_cpmant.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcpmant%2Fmodeling_cpmant.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -104,7 +104,7 @@ def forward(\n                 Provide positional information to self-attention block.\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers.\n-            past_key_values (`tuple[torch.Tensor, torch.Tensor]`, *optional*):\n+            past_key_values (`Cache`, *optional*):\n                 Cached past key and value projection states.\n             use_cache (`bool`, *optional*):\n                 If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n@@ -191,7 +191,7 @@ def forward(\n                 Provide positional information to self-attention block.\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers.\n-            past_key_values (`Tuple(torch.FloatTensor)`, *optional*):\n+            past_key_values (`Cache`, *optional*):\n                 Cached past key and value projection states.\n             use_cache (`bool`, *optional*):\n                 If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n@@ -315,7 +315,7 @@ def forward(\n                 Provides position information to attention mechanism of shape `(num_heads, seq_len, seq_len)`\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers.\n-            past_key_values (`tuple[torch.Tensor, torch.Tensor])`, *optional*):\n+            past_key_values (`Cache`, *optional*):\n                 Cached past key and value projection states\n             use_cache (`bool`, *optional*):\n                 If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n@@ -366,7 +366,7 @@ def forward(\n                 Whether or not to return the attentions tensors of all attention layers.\n             output_hidden_states (`bool`, *optional*):\n                 Whether or not to return the hidden states of all layers.\n-            past_key_values (`tuple[torch.Tensor, torch.Tensor])`, *optional*):\n+            past_key_values (`Cache`, *optional*):\n                 Cached past key and value projection states\n             use_cache (`bool`, *optional*):\n                 If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n@@ -590,7 +590,7 @@ def forward(\n         input_ids: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        past_key_values: Optional[tuple[tuple[torch.Tensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n@@ -722,7 +722,7 @@ def __init__(self, config: CpmAntConfig):\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[list[tuple[torch.Tensor, torch.Tensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,"
        },
        {
            "sha": "400c023e02849140eaed44fd4e5ad811337a79e8",
            "filename": "src/transformers/models/csm/generation_csm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fcsm%2Fgeneration_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fcsm%2Fgeneration_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fgeneration_csm.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -62,7 +62,7 @@ class CsmGenerateOutput(GenerateDecoderOnlyOutput):\n         hidden_states (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_hidden_states=True`):\n             Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n             `torch.FloatTensor` of shape `(batch_size, generated_length, hidden_size)`.\n-        past_key_values (`tuple(tuple(torch.FloatTensor)))`, *optional*, returned when `use_cache=True`):\n+        past_key_values (`Cache`, *optional*, returned when `use_cache=True`):\n             Returns the model cache, used to speed up decoding. Different models have a different cache format, check\n         audio (`list(torch.FloatTensor)` of length `batch_size`):\n             The generated audio."
        },
        {
            "sha": "80157e2aa93a89287d35784e6f97aa53d6c0e4d4",
            "filename": "src/transformers/models/csm/modeling_csm.py",
            "status": "modified",
            "additions": 5,
            "deletions": 7,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -60,18 +60,16 @@ class CsmOutputWithPast(ModelOutput):\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n         `past_key_values` input) to speed up sequential decoding.\n     depth_decoder_loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n         Language modeling loss (for next-token prediction) of the depth decoder model.\n     depth_decoder_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n         Prediction scores of the depth decoder (scores for each vocabulary token before SoftMax).\n-    depth_decoder_past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+    depth_decoder_past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n     depth_decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n         Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n         one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n@@ -86,12 +84,12 @@ class CsmOutputWithPast(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None\n+    past_key_values: Optional[Cache] = None\n     hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n     depth_decoder_loss: Optional[torch.FloatTensor] = None\n     depth_decoder_logits: Optional[torch.FloatTensor] = None\n-    depth_decoder_past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None\n+    depth_decoder_past_key_values: Optional[Cache] = None\n     depth_decoder_hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n     depth_decoder_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n     backbone_loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "cb7d4a6c209c37d13bed221eb040d95ceb7d7f80",
            "filename": "src/transformers/models/csm/modular_csm.py",
            "status": "modified",
            "additions": 5,
            "deletions": 7,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -59,18 +59,16 @@ class CsmOutputWithPast(ModelOutput):\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n         `past_key_values` input) to speed up sequential decoding.\n     depth_decoder_loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n         Language modeling loss (for next-token prediction) of the depth decoder model.\n     depth_decoder_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n         Prediction scores of the depth decoder (scores for each vocabulary token before SoftMax).\n-    depth_decoder_past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+    depth_decoder_past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n     depth_decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n         Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n         one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n@@ -85,12 +83,12 @@ class CsmOutputWithPast(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None\n+    past_key_values: Optional[Cache] = None\n     hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n     depth_decoder_loss: Optional[torch.FloatTensor] = None\n     depth_decoder_logits: Optional[torch.FloatTensor] = None\n-    depth_decoder_past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None\n+    depth_decoder_past_key_values: Optional[Cache] = None\n     depth_decoder_hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n     depth_decoder_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n     backbone_loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "e20fffede948d2aa951d9fb4c1008273cefd8e05",
            "filename": "src/transformers/models/ctrl/modeling_ctrl.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -22,7 +22,7 @@\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n-from ...cache_utils import DynamicCache\n+from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast, SequenceClassifierOutput\n from ...modeling_utils import PreTrainedModel\n@@ -271,7 +271,7 @@ def _prune_heads(self, heads_to_prune):\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n@@ -456,7 +456,7 @@ def __init__(self, config):\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n@@ -598,7 +598,7 @@ def __init__(self, config):\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "9d52c995330732e8debf98220beef5f922cdcc63",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_text.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -467,7 +467,7 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,\n@@ -643,7 +643,7 @@ def forward(\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -789,7 +789,7 @@ def forward(\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,"
        },
        {
            "sha": "ab3ddce5131035b0099811068e79c1541bb06a4f",
            "filename": "src/transformers/models/dbrx/modeling_dbrx.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -771,7 +771,7 @@ def forward(\n             attention_mask (`torch.Tensor`, *optional*): attention mask of size (batch_size, sequence_length)\n                 if flash attention is used or (batch_size, 1, query_sequence_length, key_sequence_length)\n                 if default attention is used.\n-            past_key_values (`Tuple(torch.Tensor)`, *optional*): cached past key and value projection states\n+            past_key_values (`Cache`, *optional*): cached past key and value projection states\n             output_attentions (`bool`, *optional*): Whether or not to return the attentions tensors of all\n                 attention layers. See `attentions` under returned tensors for more detail.\n             output_router_logits (`bool`, *optional*): Whether or not to return the router logits."
        },
        {
            "sha": "3ac0ab0382195337abe75c2dce9d17c2b6a203d5",
            "filename": "src/transformers/models/decision_transformer/modeling_decision_transformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -527,7 +527,7 @@ def set_input_embeddings(self, new_embeddings):\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.Tensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "039602a159f2c7c7d59054190976667d1e908c8b",
            "filename": "src/transformers/models/deepseek_vl/modeling_deepseek_vl.py",
            "status": "modified",
            "additions": 4,
            "deletions": 8,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodeling_deepseek_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodeling_deepseek_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodeling_deepseek_vl.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -55,10 +55,7 @@ class DeepseekVLBaseModelOutputWithPast(ModelOutput):\n         If `past_key_values` is used only the last hidden-state of the sequences of shape `(batch_size, 1,\n         hidden_size)` is output.\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if\n-        `config.is_encoder_decoder=True` 2 additional tensors of shape `(batch_size, num_heads,\n-        encoder_sequence_length, embed_size_per_head)`.\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if\n         `config.is_encoder_decoder=True` in the cross-attention blocks) that can be used (see `past_key_values`\n@@ -71,7 +68,7 @@ class DeepseekVLBaseModelOutputWithPast(ModelOutput):\n     \"\"\"\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None\n+    past_key_values: Optional[Cache] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n     image_hidden_states: Optional[tuple[torch.FloatTensor]] = None\n@@ -90,8 +87,7 @@ class DeepseekVLCausalLMOutputWithPast(ModelOutput):\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n         `past_key_values` input) to speed up sequential decoding.\n@@ -104,7 +100,7 @@ class DeepseekVLCausalLMOutputWithPast(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[list[torch.FloatTensor]] = None\n+    past_key_values: Optional[Cache] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n     image_hidden_states: Optional[tuple[torch.FloatTensor]] = None"
        },
        {
            "sha": "cae509e14d64b4142ecb0516e5f654430cb320c0",
            "filename": "src/transformers/models/deepseek_vl_hybrid/modeling_deepseek_vl_hybrid.py",
            "status": "modified",
            "additions": 4,
            "deletions": 8,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodeling_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodeling_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodeling_deepseek_vl_hybrid.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -52,10 +52,7 @@ class DeepseekVLHybridBaseModelOutputWithPast(ModelOutput):\n         If `past_key_values` is used only the last hidden-state of the sequences of shape `(batch_size, 1,\n         hidden_size)` is output.\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if\n-        `config.is_encoder_decoder=True` 2 additional tensors of shape `(batch_size, num_heads,\n-        encoder_sequence_length, embed_size_per_head)`.\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if\n         `config.is_encoder_decoder=True` in the cross-attention blocks) that can be used (see `past_key_values`\n@@ -68,7 +65,7 @@ class DeepseekVLHybridBaseModelOutputWithPast(ModelOutput):\n     \"\"\"\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None\n+    past_key_values: Optional[Cache] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n     image_hidden_states: Optional[tuple[torch.FloatTensor]] = None\n@@ -87,8 +84,7 @@ class DeepseekVLHybridCausalLMOutputWithPast(ModelOutput):\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n         `past_key_values` input) to speed up sequential decoding.\n@@ -101,7 +97,7 @@ class DeepseekVLHybridCausalLMOutputWithPast(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[list[torch.FloatTensor]] = None\n+    past_key_values: Optional[Cache] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n     image_hidden_states: Optional[tuple[torch.FloatTensor]] = None"
        },
        {
            "sha": "90f215157b7eabfc07ad42f6d1cebf386d6c6cac",
            "filename": "src/transformers/models/deprecated/ernie_m/modeling_ernie_m.py",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fernie_m%2Fmodeling_ernie_m.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fernie_m%2Fmodeling_ernie_m.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fernie_m%2Fmodeling_ernie_m.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -23,6 +23,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ....activations import ACT2FN\n+from ....cache_utils import Cache\n from ....modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n     BaseModelOutputWithPoolingAndCrossAttentions,\n@@ -127,7 +128,7 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n     ) -> tuple[torch.Tensor]:\n         mixed_query_layer = self.q_proj(hidden_states)\n@@ -255,7 +256,7 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n     ) -> tuple[torch.Tensor]:\n         self_outputs = self.self_attn(\n@@ -298,7 +299,7 @@ def forward(\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = True,\n     ):\n         residual = hidden_states\n@@ -347,7 +348,7 @@ def forward(\n         input_embeds: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,\n         return_dict: Optional[bool] = True,\n@@ -629,7 +630,7 @@ def forward(\n         position_ids: Optional[torch.Tensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[list[torch.Tensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -818,7 +819,7 @@ def forward(\n         position_ids: Optional[torch.Tensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[list[torch.Tensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_hidden_states: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         return_dict: Optional[bool] = True,"
        },
        {
            "sha": "0e2d27f03bacbd60ade444fe3221ada9f714cb60",
            "filename": "src/transformers/models/deprecated/gptsan_japanese/modeling_gptsan_japanese.py",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fmodeling_gptsan_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fmodeling_gptsan_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fmodeling_gptsan_japanese.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -20,6 +20,7 @@\n import torch.nn as nn\n \n from ....activations import ACT2FN\n+from ....cache_utils import Cache\n from ....modeling_outputs import MoECausalLMOutputWithPast, MoEModelOutputWithPastAndCrossAttentions\n from ....modeling_utils import PreTrainedModel\n from ....utils import (\n@@ -383,7 +384,7 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         key_value_states: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n@@ -519,7 +520,7 @@ def __init__(self, config, has_relative_attention_bias=False):\n     def forward(\n         self,\n         hidden_states: Optional[tuple[torch.FloatTensor]],\n-        past_key_values: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = False,\n@@ -601,7 +602,7 @@ def __init__(self, config, ext_layer=False):\n     def forward(\n         self,\n         hidden_states: Optional[tuple[torch.FloatTensor]],\n-        past_key_values: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = False,\n@@ -885,7 +886,7 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.FloatTensor] = None,\n         spout: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = False,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n@@ -1111,7 +1112,7 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.FloatTensor] = None,\n         spout: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = False,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n@@ -1268,7 +1269,7 @@ def prepare_inputs_for_generation(\n         attention_mask: torch.FloatTensor,\n         token_type_ids: Optional[torch.FloatTensor] = None,\n         spout: Optional[Union[list, torch.FloatTensor]] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         **kwargs,\n     ):\n         if isinstance(spout, list):"
        },
        {
            "sha": "c6edc57f8cf65510dc390fb733e223adf58de6c0",
            "filename": "src/transformers/models/deprecated/mega/modeling_mega.py",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmega%2Fmodeling_mega.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmega%2Fmodeling_mega.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmega%2Fmodeling_mega.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -24,6 +24,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ....activations import ACT2FN\n+from ....cache_utils import Cache\n from ....modeling_outputs import (\n     BaseModelOutputWithPoolingAndCrossAttentions,\n     CausalLMOutputWithCrossAttentions,\n@@ -625,7 +626,7 @@ def forward(\n         key: Optional[torch.Tensor],\n         value: Optional[torch.Tensor],\n         key_padding_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: bool = False,\n         use_cache: bool = False,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n@@ -910,7 +911,7 @@ def forward(\n         input,\n         padding_mask: Optional[torch.Tensor] = None,\n         causal_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions=False,\n         use_cache=False,\n     ):\n@@ -1182,7 +1183,7 @@ def forward(\n         causal_mask: Optional[torch.LongTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[tuple[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: bool = False,\n     ) -> tuple[torch.Tensor]:\n@@ -1490,7 +1491,7 @@ def forward(\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -1675,7 +1676,7 @@ def forward(\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,"
        },
        {
            "sha": "3dd67c22d72ea8637c447fef768bbd9ece7936c5",
            "filename": "src/transformers/models/deprecated/nezha/modeling_nezha.py",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnezha%2Fmodeling_nezha.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnezha%2Fmodeling_nezha.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnezha%2Fmodeling_nezha.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -26,6 +26,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ....activations import ACT2FN\n+from ....cache_utils import Cache\n from ....modeling_layers import GradientCheckpointingLayer\n from ....modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -251,7 +252,7 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n     ) -> tuple[torch.Tensor]:\n         mixed_query_layer = self.query(hidden_states)\n@@ -396,7 +397,7 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n     ) -> tuple[torch.Tensor]:\n         self_outputs = self.self(\n@@ -465,7 +466,7 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n     ) -> tuple[torch.Tensor]:\n         # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n@@ -543,7 +544,7 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,\n@@ -877,7 +878,7 @@ def forward(\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,"
        },
        {
            "sha": "4ce63feceb7480fa32c5046ecc6a0a10e0901559",
            "filename": "src/transformers/models/deprecated/open_llama/modeling_open_llama.py",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fmodeling_open_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fmodeling_open_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fmodeling_open_llama.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -28,6 +28,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ....activations import ACT2FN\n+from ....cache_utils import Cache\n from ....modeling_attn_mask_utils import _prepare_4d_causal_attention_mask\n from ....modeling_layers import GradientCheckpointingLayer\n from ....modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast, SequenceClassifierOutputWithPast\n@@ -274,7 +275,7 @@ def forward(\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: bool = False,\n         use_cache: bool = False,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n@@ -366,7 +367,7 @@ def forward(\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n     ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n@@ -557,7 +558,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -684,7 +685,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -851,7 +852,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "74bad366e8d562043246cc286fae3ba7c16baafd",
            "filename": "src/transformers/models/deprecated/qdqbert/modeling_qdqbert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fqdqbert%2Fmodeling_qdqbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fqdqbert%2Fmodeling_qdqbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fqdqbert%2Fmodeling_qdqbert.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -26,6 +26,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ....activations import ACT2FN\n+from ....cache_utils import Cache\n from ....modeling_layers import GradientCheckpointingLayer\n from ....modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -859,7 +860,7 @@ def forward(\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,"
        },
        {
            "sha": "9e2de5c9c1c4491390a0a31ce9c0613e9500096b",
            "filename": "src/transformers/models/deprecated/realm/modeling_realm.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Fmodeling_realm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Fmodeling_realm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Fmodeling_realm.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -24,6 +24,7 @@\n from torch.nn import CrossEntropyLoss\n \n from ....activations import ACT2FN\n+from ....cache_utils import Cache\n from ....modeling_layers import GradientCheckpointingLayer\n from ....modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -256,7 +257,7 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n     ) -> tuple[torch.Tensor]:\n         mixed_query_layer = self.query(hidden_states)\n@@ -405,7 +406,7 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n     ) -> tuple[torch.Tensor]:\n         self_outputs = self.self(\n@@ -474,7 +475,7 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n     ) -> tuple[torch.Tensor]:\n         # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n@@ -552,7 +553,7 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,"
        },
        {
            "sha": "854f21c0655046124fb439ec3e4a44395c33abcc",
            "filename": "src/transformers/models/deprecated/speech_to_text_2/modeling_speech_to_text_2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Fmodeling_speech_to_text_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Fmodeling_speech_to_text_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Fmodeling_speech_to_text_2.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -22,6 +22,7 @@\n from torch.nn import CrossEntropyLoss\n \n from ....activations import ACT2FN\n+from ....cache_utils import Cache\n from ....modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_causal_attention_mask\n from ....modeling_layers import GradientCheckpointingLayer\n from ....modeling_outputs import BaseModelOutputWithPastAndCrossAttentions, CausalLMOutputWithCrossAttentions\n@@ -148,7 +149,7 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         key_value_states: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n@@ -304,7 +305,7 @@ def forward(\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_layer_head_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = True,\n     ):\n@@ -707,7 +708,7 @@ def forward(\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "dcacffabc8b942f33617b430eb6f4fdd2000625c",
            "filename": "src/transformers/models/deprecated/trajectory_transformer/modeling_trajectory_transformer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftrajectory_transformer%2Fmodeling_trajectory_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftrajectory_transformer%2Fmodeling_trajectory_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftrajectory_transformer%2Fmodeling_trajectory_transformer.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -25,6 +25,7 @@\n from torch import nn\n from torch.nn import functional as F\n \n+from ....cache_utils import Cache\n from ....modeling_layers import GradientCheckpointingLayer\n from ....modeling_utils import PreTrainedModel\n from ....utils import (\n@@ -142,7 +143,7 @@ class TrajectoryTransformerOutput(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None\n+    past_key_values: Optional[Cache] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n \n@@ -464,7 +465,7 @@ def pad_to_full_observation(self, hidden_states):\n     def forward(\n         self,\n         trajectories: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.Tensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         targets: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "e85660223b582ea8da4b76518e6ff7fc10ab49c8",
            "filename": "src/transformers/models/deprecated/xlm_prophetnet/modeling_xlm_prophetnet.py",
            "status": "modified",
            "additions": 11,
            "deletions": 10,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2Fmodeling_xlm_prophetnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2Fmodeling_xlm_prophetnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2Fmodeling_xlm_prophetnet.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -26,6 +26,7 @@\n from torch.nn import LayerNorm\n \n from ....activations import ACT2FN\n+from ....cache_utils import Cache\n from ....modeling_layers import GradientCheckpointingLayer\n from ....modeling_outputs import BaseModelOutput\n from ....modeling_utils import PreTrainedModel\n@@ -312,7 +313,7 @@ class XLMProphetNetSeq2SeqLMOutput(ModelOutput):\n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n     logits_ngram: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[tuple[torch.FloatTensor]] = None\n+    past_key_values: Optional[Cache] = None\n     decoder_hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     decoder_ngram_hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     decoder_attentions: Optional[tuple[torch.FloatTensor]] = None\n@@ -398,7 +399,7 @@ class XLMProphetNetSeq2SeqModelOutput(ModelOutput):\n \n     last_hidden_state: torch.FloatTensor\n     last_hidden_state_ngram: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[tuple[torch.FloatTensor]] = None\n+    past_key_values: Optional[Cache] = None\n     decoder_hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     decoder_ngram_hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     decoder_attentions: Optional[tuple[torch.FloatTensor]] = None\n@@ -470,7 +471,7 @@ class XLMProphetNetDecoderModelOutput(ModelOutput):\n \n     last_hidden_state: torch.FloatTensor\n     last_hidden_state_ngram: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[tuple[torch.FloatTensor]] = None\n+    past_key_values: Optional[Cache] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     hidden_states_ngram: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n@@ -532,7 +533,7 @@ class XLMProphetNetDecoderLMOutput(ModelOutput):\n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n     logits_ngram: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[tuple[torch.FloatTensor]] = None\n+    past_key_values: Optional[Cache] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     hidden_states_ngram: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n@@ -658,7 +659,7 @@ def forward(\n         key_value_states: Optional[Tensor] = None,\n         attention_mask: Optional[Tensor] = None,\n         layer_head_mask: Optional[Tensor] = None,\n-        past_key_values: Optional[tuple[Tensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: bool = False,\n     ) -> tuple[Tensor, Optional[Tensor]]:\n         batch_size, tgt_len, hidden_size = hidden_states.size()\n@@ -814,7 +815,7 @@ def prepare_for_onnx_export_(self):\n     def forward(\n         self,\n         hidden_states,\n-        past_key_values: Optional[tuple[Tensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask=None,\n         layer_head_mask=None,\n         extended_predict_attention_mask=None,\n@@ -1398,7 +1399,7 @@ def forward(\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.Tensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -1745,7 +1746,7 @@ def forward(\n         decoder_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[tuple] = None,\n-        past_key_values: Optional[tuple[tuple[torch.Tensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         decoder_inputs_embeds: Optional[torch.Tensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -1861,7 +1862,7 @@ def forward(\n         decoder_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.Tensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         decoder_inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n@@ -2087,7 +2088,7 @@ def forward(\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.Tensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "5822cad62017c8e390806afb3bb67101e4568837",
            "filename": "src/transformers/models/doge/modeling_doge.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodeling_doge.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodeling_doge.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodeling_doge.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -450,7 +450,7 @@ def forward(\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -726,7 +726,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "c4c95e627376982b3042465624a618bdf1278b87",
            "filename": "src/transformers/models/doge/modular_doge.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodular_doge.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodular_doge.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodular_doge.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -534,7 +534,7 @@ def forward(\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -708,7 +708,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "9327bc0fdf268fdd1c9c7619b1c4e5b4814cb8a1",
            "filename": "src/transformers/models/electra/modeling_electra.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -522,7 +522,7 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,\n@@ -718,7 +718,7 @@ def forward(\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -1498,7 +1498,7 @@ def forward(\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[list[torch.Tensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,"
        },
        {
            "sha": "30e2370b2240d2f2f00182abea548cd6a72b5626",
            "filename": "src/transformers/models/encoder_decoder/modeling_encoder_decoder.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_encoder_decoder.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -25,6 +25,7 @@\n from torch import nn\n from torch.nn import CrossEntropyLoss\n \n+from ...cache_utils import Cache\n from ...configuration_utils import PretrainedConfig\n from ...generation import GenerationMixin\n from ...modeling_outputs import BaseModelOutput, Seq2SeqLMOutput\n@@ -450,7 +451,7 @@ def forward(\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.BoolTensor] = None,\n         encoder_outputs: Optional[tuple[torch.FloatTensor]] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "a5ed4a3f5328cccf0746c9fa27ed6b05a14d1284",
            "filename": "src/transformers/models/ernie/modeling_ernie.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -451,7 +451,7 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,\n@@ -727,7 +727,7 @@ def forward(\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -999,7 +999,7 @@ def forward(\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[list[torch.Tensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,"
        },
        {
            "sha": "2976beba103320a08ab55170785e4b01d427c35a",
            "filename": "src/transformers/models/ernie4_5_moe/modeling_ernie4_5_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -409,7 +409,7 @@ def forward(\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> torch.FloatTensor:\n@@ -427,7 +427,7 @@ def forward(\n             use_cache (`bool`, *optional*):\n                 If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                 (see `past_key_values`).\n-            past_key_values (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            past_key_values (`Cache`, *optional*): cached past key and value projection states\n             cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             position_embeddings (`tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):"
        },
        {
            "sha": "5ec1792e4581a16014dfd580ccf32b7c7bc6d9c6",
            "filename": "src/transformers/models/falcon/modeling_falcon.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -1110,7 +1110,7 @@ def __init__(self, config: FalconConfig):\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.Tensor, torch.Tensor], ...]] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n@@ -1236,7 +1236,7 @@ def __init__(self, config: FalconConfig):\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.Tensor, torch.Tensor], ...]] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,"
        },
        {
            "sha": "85618847dbf7676400fe190fd1a54bab53c87a6a",
            "filename": "src/transformers/models/fsmt/modeling_fsmt.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Ffsmt%2Fmodeling_fsmt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Ffsmt%2Fmodeling_fsmt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffsmt%2Fmodeling_fsmt.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -586,7 +586,7 @@ def forward(\n         head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,\n@@ -922,7 +922,7 @@ def forward(\n         decoder_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[tuple[torch.FloatTensor]] = None,\n-        past_key_values: Optional[tuple[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -1068,7 +1068,7 @@ def forward(\n         decoder_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[tuple[torch.FloatTensor]] = None,\n-        past_key_values: Optional[tuple[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         decoder_inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "889a374b54933bf2bc23f9a45032a72ef6c71bca",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 4,
            "deletions": 11,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -56,12 +56,6 @@\n )\n class Gemma3ModelOutputWithPast(BaseModelOutputWithPast):\n     r\"\"\"\n-    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n-\n-        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n-        `past_key_values` input) to speed up sequential decoding.\n     image_hidden_states (`torch.FloatTensor`, *optional*):\n         A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n         image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n@@ -83,8 +77,7 @@ class Gemma3CausalLMOutputWithPast(ModelOutput):\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.text_config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n         `past_key_values` input) to speed up sequential decoding.\n@@ -95,7 +88,7 @@ class Gemma3CausalLMOutputWithPast(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[Union[list[torch.FloatTensor], Cache]] = None\n+    past_key_values: Optional[Cache] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n     image_hidden_states: Optional[torch.FloatTensor] = None\n@@ -843,7 +836,7 @@ def forward(\n         pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[list[torch.FloatTensor], Cache]] = None,\n+        past_key_values: Optional[Cache] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n@@ -1028,7 +1021,7 @@ def forward(\n         pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[list[torch.FloatTensor], Cache]] = None,\n+        past_key_values: Optional[Cache] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,"
        },
        {
            "sha": "b0cb272dd04af8b3f6bf081b01e2ae1be87ef6aa",
            "filename": "src/transformers/models/gemma3/modular_gemma3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -781,7 +781,7 @@ def forward(\n         pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[list[torch.FloatTensor], Cache]] = None,\n+        past_key_values: Optional[Cache] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n@@ -893,7 +893,7 @@ def forward(\n         pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[list[torch.FloatTensor], Cache]] = None,\n+        past_key_values: Optional[Cache] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,"
        },
        {
            "sha": "68595ead4371bba4087813566d5c83d5c0155c03",
            "filename": "src/transformers/models/gemma3n/modeling_gemma3n.py",
            "status": "modified",
            "additions": 7,
            "deletions": 9,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -56,9 +56,8 @@\n )\n class Gemma3nModelOutputWithPast(BaseModelOutputWithPast):\n     r\"\"\"\n-    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n         `past_key_values` input) to speed up sequential decoding.\n@@ -87,9 +86,8 @@ class Gemma3nCausalLMOutputWithPast(ModelOutput):\n         Language modeling loss (for next-token prediction).\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.text_config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n         `past_key_values` input) to speed up sequential decoding.\n@@ -103,7 +101,7 @@ class Gemma3nCausalLMOutputWithPast(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[Union[list[torch.FloatTensor], Cache]] = None\n+    past_key_values: Optional[Cache] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n     image_hidden_states: Optional[torch.FloatTensor] = None\n@@ -2012,7 +2010,7 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         input_features_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[list[torch.FloatTensor], Cache]] = None,\n+        past_key_values: Optional[Cache] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n@@ -2217,7 +2215,7 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         input_features_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[list[torch.FloatTensor], Cache]] = None,\n+        past_key_values: Optional[Cache] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,"
        },
        {
            "sha": "d46471dfdab89f9a83ef1b682da2d0ebaa10c3a8",
            "filename": "src/transformers/models/gemma3n/modular_gemma3n.py",
            "status": "modified",
            "additions": 6,
            "deletions": 8,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -647,9 +647,8 @@ def __init__(\n \n class Gemma3nModelOutputWithPast(PaligemmaModelOutputWithPast):\n     r\"\"\"\n-    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n         `past_key_values` input) to speed up sequential decoding.\n@@ -670,9 +669,8 @@ class Gemma3nCausalLMOutputWithPast(PaliGemmaCausalLMOutputWithPast):\n         Language modeling loss (for next-token prediction).\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.text_config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n         `past_key_values` input) to speed up sequential decoding.\n@@ -2324,7 +2322,7 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         input_features_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[list[torch.FloatTensor], Cache]] = None,\n+        past_key_values: Optional[Cache] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n@@ -2505,7 +2503,7 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         input_features_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[list[torch.FloatTensor], Cache]] = None,\n+        past_key_values: Optional[Cache] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,"
        },
        {
            "sha": "fbfeae9130e1e4553ffb572e8fabc0360ff74dad",
            "filename": "src/transformers/models/glm4v/modeling_glm4v.py",
            "status": "modified",
            "additions": 8,
            "deletions": 10,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -597,7 +597,7 @@ def forward(\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -642,8 +642,7 @@ def forward(\n class Glm4vModelOutputWithPast(ModelOutput):\n     r\"\"\"\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n         `past_key_values` input) to speed up sequential decoding.\n@@ -652,7 +651,7 @@ class Glm4vModelOutputWithPast(ModelOutput):\n     \"\"\"\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[list[torch.FloatTensor]] = None\n+    past_key_values: Optional[Cache] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n     rope_deltas: Optional[torch.LongTensor] = None\n@@ -813,7 +812,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -1183,7 +1182,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         pixel_values: Optional[torch.Tensor] = None,\n         pixel_values_videos: Optional[torch.FloatTensor] = None,\n@@ -1297,8 +1296,7 @@ class Glm4vCausalLMOutputWithPast(ModelOutput):\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n         `past_key_values` input) to speed up sequential decoding.\n@@ -1308,7 +1306,7 @@ class Glm4vCausalLMOutputWithPast(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[list[torch.FloatTensor]] = None\n+    past_key_values: Optional[Cache] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n     rope_deltas: Optional[torch.LongTensor] = None\n@@ -1363,7 +1361,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         pixel_values: Optional[torch.Tensor] = None,"
        },
        {
            "sha": "7c400edc51c3f41e8681402369d8073e32af44ab",
            "filename": "src/transformers/models/glm4v/modular_glm4v.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -686,7 +686,7 @@ def forward(\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -862,7 +862,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -1194,7 +1194,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         pixel_values: Optional[torch.Tensor] = None,\n         pixel_values_videos: Optional[torch.FloatTensor] = None,\n@@ -1307,7 +1307,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         pixel_values: Optional[torch.Tensor] = None,"
        },
        {
            "sha": "045e78df5233becd5d44bd38ccb3745db51de407",
            "filename": "src/transformers/models/glm4v_moe/modeling_glm4v_moe.py",
            "status": "modified",
            "additions": 7,
            "deletions": 9,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -776,8 +776,7 @@ def forward(self, x, position_ids):\n class Glm4vMoeModelOutputWithPast(ModelOutput):\n     r\"\"\"\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n         `past_key_values` input) to speed up sequential decoding.\n@@ -786,7 +785,7 @@ class Glm4vMoeModelOutputWithPast(ModelOutput):\n     \"\"\"\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[list[torch.FloatTensor]] = None\n+    past_key_values: Optional[Cache] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n     rope_deltas: Optional[torch.LongTensor] = None\n@@ -929,7 +928,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -1299,7 +1298,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         pixel_values: Optional[torch.Tensor] = None,\n         pixel_values_videos: Optional[torch.FloatTensor] = None,\n@@ -1413,8 +1412,7 @@ class Glm4vMoeCausalLMOutputWithPast(ModelOutput):\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n         `past_key_values` input) to speed up sequential decoding.\n@@ -1424,7 +1422,7 @@ class Glm4vMoeCausalLMOutputWithPast(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[list[torch.FloatTensor]] = None\n+    past_key_values: Optional[Cache] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n     rope_deltas: Optional[torch.LongTensor] = None\n@@ -1479,7 +1477,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         pixel_values: Optional[torch.Tensor] = None,"
        },
        {
            "sha": "35b74335c21f5e1e406431ffbf8fbcef96b53e6f",
            "filename": "src/transformers/models/got_ocr2/modeling_got_ocr2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -485,8 +485,7 @@ class GotOcr2CausalLMOutputWithPast(ModelOutput):\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n         `past_key_values` input) to speed up sequential decoding.\n@@ -497,7 +496,7 @@ class GotOcr2CausalLMOutputWithPast(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[list[torch.FloatTensor]] = None\n+    past_key_values: Optional[Cache] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n     image_hidden_states: Optional[torch.FloatTensor] = None\n@@ -512,8 +511,7 @@ class GotOcr2CausalLMOutputWithPast(ModelOutput):\n class GotOcr2ModelOutputWithPast(BaseModelOutputWithPast):\n     r\"\"\"\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n         `past_key_values` input) to speed up sequential decoding."
        },
        {
            "sha": "c34755b8b44090a220771f610040dc87a4ddc88d",
            "filename": "src/transformers/models/gpt2/modeling_gpt2.py",
            "status": "modified",
            "additions": 8,
            "deletions": 9,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -622,9 +622,8 @@ class GPT2DoubleHeadsModelOutput(ModelOutput):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n     mc_logits (`torch.FloatTensor` of shape `(batch_size, num_choices)`):\n         Prediction scores of the multiple choice classification head (scores for each choice before SoftMax).\n-    past_key_values (`tuple[tuple[torch.Tensor]]`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of length `config.n_layers`, containing tuples of tensors of shape `(batch_size, num_heads,\n-        sequence_length, embed_size_per_head)`).\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the attention blocks) that can be used (see\n         `past_key_values` input) to speed up sequential decoding.\n@@ -634,7 +633,7 @@ class GPT2DoubleHeadsModelOutput(ModelOutput):\n     mc_loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n     mc_logits: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None\n+    past_key_values: Optional[Cache] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n \n@@ -777,7 +776,7 @@ def _prune_heads(self, heads_to_prune):\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[tuple[tuple[torch.Tensor]], Cache]] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n@@ -1030,7 +1029,7 @@ def deparallelize(self):\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.Tensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n@@ -1179,7 +1178,7 @@ def deparallelize(self):\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.Tensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n@@ -1332,7 +1331,7 @@ def __init__(self, config):\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.Tensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n@@ -1465,7 +1464,7 @@ def __init__(self, config):\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.Tensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "96fb40b1d69fd2bcc913ddc0c68e2d22ff989f82",
            "filename": "src/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -433,7 +433,7 @@ def set_input_embeddings(self, new_embeddings):\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[list[torch.Tensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n@@ -616,7 +616,7 @@ def __init__(self, config):\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.Tensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n@@ -723,7 +723,7 @@ def __init__(self, config):\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.Tensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n@@ -856,7 +856,7 @@ def __init__(self, config):\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.Tensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,"
        },
        {
            "sha": "0d5c936e8adca36cf69054ec7d7952ccb0125f75",
            "filename": "src/transformers/models/gpt_oss/modeling_gpt_oss.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -463,7 +463,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "aba879af9336d882e742e4bbbf963bee24cef89b",
            "filename": "src/transformers/models/gpt_oss/modular_gpt_oss.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodular_gpt_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodular_gpt_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodular_gpt_oss.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -393,7 +393,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "56256df7d582dcffb293e15ac38e4ab9430acb56",
            "filename": "src/transformers/models/gptj/modeling_gptj.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -1037,7 +1037,7 @@ def __init__(self, config):\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.Tensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "846865c55508e223bf6b512ba03b2b64bd0e2434",
            "filename": "src/transformers/models/granite/modeling_granite.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -259,7 +259,7 @@ def forward(\n             use_cache (`bool`, *optional*):\n                 If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                 (see `past_key_values`).\n-            past_key_values (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            past_key_values (`Cache`, *optional*): cached past key and value projection states\n             cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n                 Indices depicting the position of the input sequence tokens in the sequence\n             position_embeddings (`tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):"
        },
        {
            "sha": "1b90609e54607c3d266fb8dd71bbfbfd02320efd",
            "filename": "src/transformers/models/granite/modular_granite.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodular_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodular_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodular_granite.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -77,7 +77,7 @@ def forward(\n             use_cache (`bool`, *optional*):\n                 If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                 (see `past_key_values`).\n-            past_key_values (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            past_key_values (`Cache`, *optional*): cached past key and value projection states\n             cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n                 Indices depicting the position of the input sequence tokens in the sequence\n             position_embeddings (`tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):"
        },
        {
            "sha": "1e44c9781dec683cf4b12bcb9d55d32b3635fb53",
            "filename": "src/transformers/models/granite_speech/modeling_granite_speech.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fmodeling_granite_speech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fmodeling_granite_speech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fmodeling_granite_speech.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -46,16 +46,15 @@ class GraniteSpeechCausalLMOutputWithPast(ModelOutput):\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n         `past_key_values` input) to speed up sequential decoding.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[list[torch.FloatTensor]] = None\n+    past_key_values: Optional[Cache] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n "
        },
        {
            "sha": "29c23a356509d828ed98160a3bc0f9ebf8118b21",
            "filename": "src/transformers/models/granitemoe/modeling_granitemoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -541,7 +541,7 @@ def forward(\n             use_cache (`bool`, *optional*):\n                 If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                 (see `past_key_values`).\n-            past_key_values (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            past_key_values (`Cache`, *optional*): cached past key and value projection states\n             cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n                 Indices depicting the position of the input sequence tokens in the sequence\n             output_router_logits (`bool`, *optional*):"
        },
        {
            "sha": "f35211558bf79835deb6c66a1df8d27a4610ab20",
            "filename": "src/transformers/models/granitemoehybrid/modeling_granitemoehybrid.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -1150,7 +1150,7 @@ def forward(\n             attention_mask (`torch.FloatTensor`, *optional*):\n                 attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,\n                 query_sequence_length, key_sequence_length)` if default attention is used.\n-            past_key_values (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            past_key_values (`Cache`, *optional*): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail."
        },
        {
            "sha": "2ebddb88e3161d1cc6d25bc9461abc1c0eb7289b",
            "filename": "src/transformers/models/granitemoehybrid/modular_granitemoehybrid.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodular_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodular_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodular_granitemoehybrid.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -96,7 +96,7 @@ def forward(\n             attention_mask (`torch.FloatTensor`, *optional*):\n                 attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,\n                 query_sequence_length, key_sequence_length)` if default attention is used.\n-            past_key_values (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            past_key_values (`Cache`, *optional*): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail."
        },
        {
            "sha": "d9ff21d3ebba535bba58892ba9742985a36ece80",
            "filename": "src/transformers/models/granitemoeshared/modeling_granitemoeshared.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -474,7 +474,7 @@ def forward(\n             use_cache (`bool`, *optional*):\n                 If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                 (see `past_key_values`).\n-            past_key_values (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            past_key_values (`Cache`, *optional*): cached past key and value projection states\n             cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n                 Indices depicting the position of the input sequence tokens in the sequence\n             output_router_logits (`bool`, *optional*):"
        },
        {
            "sha": "529a07f0317a0be3ffaea3582c71b687d762a4bd",
            "filename": "src/transformers/models/granitemoeshared/modular_granitemoeshared.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodular_granitemoeshared.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodular_granitemoeshared.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodular_granitemoeshared.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -117,7 +117,7 @@ def forward(\n             use_cache (`bool`, *optional*):\n                 If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                 (see `past_key_values`).\n-            past_key_values (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            past_key_values (`Cache`, *optional*): cached past key and value projection states\n             cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n                 Indices depicting the position of the input sequence tokens in the sequence\n             output_router_logits (`bool`, *optional*):"
        },
        {
            "sha": "c80cb2e88bdc9e2bd6a64b3fe46bc75d74750c14",
            "filename": "src/transformers/models/idefics/modeling_idefics.py",
            "status": "modified",
            "additions": 7,
            "deletions": 11,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -60,10 +60,7 @@ class IdeficsBaseModelOutputWithPast(ModelOutput):\n         If `past_key_values` is used only the last hidden-state of the sequences of shape `(batch_size, 1,\n         hidden_size)` is output.\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if\n-        `config.is_encoder_decoder=True` 2 additional tensors of shape `(batch_size, num_heads,\n-        encoder_sequence_length, embed_size_per_head)`.\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if\n         `config.is_encoder_decoder=True` in the cross-attention blocks) that can be used (see `past_key_values`\n@@ -76,7 +73,7 @@ class IdeficsBaseModelOutputWithPast(ModelOutput):\n     \"\"\"\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None\n+    past_key_values: Optional[Cache] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n     image_hidden_states: Optional[tuple[torch.FloatTensor]] = None\n@@ -95,8 +92,7 @@ class IdeficsCausalLMOutputWithPast(ModelOutput):\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n         `past_key_values` input) to speed up sequential decoding.\n@@ -109,7 +105,7 @@ class IdeficsCausalLMOutputWithPast(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[list[torch.FloatTensor]] = None\n+    past_key_values: Optional[Cache] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n     image_hidden_states: Optional[tuple[torch.FloatTensor]] = None\n@@ -575,7 +571,7 @@ def forward(\n         key_value_states: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n@@ -663,7 +659,7 @@ def forward(\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> torch.FloatTensor:\n@@ -769,7 +765,7 @@ def forward(\n         image_hidden_states: Optional[torch.Tensor] = None,\n         image_attention_mask: Optional[torch.Tensor] = None,\n         cross_attention_gate: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> torch.FloatTensor:\n         r\"\"\""
        },
        {
            "sha": "264d3fc831ab802683e6f3781a83a60b2731f706",
            "filename": "src/transformers/models/idefics2/modeling_idefics2.py",
            "status": "modified",
            "additions": 9,
            "deletions": 10,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -53,10 +53,8 @@ class Idefics2BaseModelOutputWithPast(ModelOutput):\n         If `past_key_values` is used only the last hidden-state of the sequences of shape `(batch_size, 1,\n         hidden_size)` is output.\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if\n-        `config.is_encoder_decoder=True` 2 additional tensors of shape `(batch_size, num_heads,\n-        encoder_sequence_length, embed_size_per_head)`.\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n+\n         Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if\n         `config.is_encoder_decoder=True` in the cross-attention blocks) that can be used (see `past_key_values`\n         input) to speed up sequential decoding.\n@@ -67,7 +65,7 @@ class Idefics2BaseModelOutputWithPast(ModelOutput):\n     \"\"\"\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None\n+    past_key_values: Optional[Cache] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n     image_hidden_states: Optional[tuple[torch.FloatTensor]] = None\n@@ -87,19 +85,20 @@ class Idefics2CausalLMOutputWithPast(ModelOutput):\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n+\n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n         `past_key_values` input) to speed up sequential decoding.\n     image_hidden_states (`tuple(torch.FloatTensor)`, *optional*):\n         Tuple of `torch.FloatTensor` (one for the output of the image embeddings, `(batch_size, num_images,\n         sequence_length, hidden_size)`.\n+\n         image_hidden_states of the model produced by the vision encoder, and optionally by the perceiver\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[list[torch.FloatTensor]] = None\n+    past_key_values: Optional[Cache] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n     image_hidden_states: Optional[tuple[torch.FloatTensor]] = None\n@@ -584,7 +583,7 @@ def forward(\n             context (`torch.Tensor`): Tensor of shape [bsz, seq, embed_dim] representing long-form context to resample.\n             attention_mask (`torch.Tensor`, *optional*): Tensor of shape [bsz, 1, seq, n_latents] representing attention mask.\n             position_ids (`torch.LongTensor`, *optional*): Tensor of shape [bsz, seq] representing position indices of each input token.\n-            past_key_values (`tuple[torch.Tensor]`, *optional*): Tuple of tensors containing cached key and value states.\n+            past_key_values (`Cache`, *optional*): Tuple of tensors containing cached key and value states.\n             output_attentions (`bool`, *optional*, defaults to `False`): Whether to return attention weights.\n             use_cache (`bool`, *optional*, defaults to `False`): Whether to use past_key_values for caching.\n         \"\"\"\n@@ -669,7 +668,7 @@ def forward(\n             use_cache (`bool`, *optional*):\n                 If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                 (see `past_key_values`).\n-            past_key_values (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            past_key_values (`Cache`, *optional*): cached past key and value projection states\n         \"\"\"\n         residual = latents\n "
        },
        {
            "sha": "9d726f814465ee41863f699f71e9a5eb5e93d74e",
            "filename": "src/transformers/models/idefics3/modeling_idefics3.py",
            "status": "modified",
            "additions": 6,
            "deletions": 8,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -52,10 +52,8 @@ class Idefics3BaseModelOutputWithPast(ModelOutput):\n         If `past_key_values` is used only the last hidden-state of the sequences of shape `(batch_size, 1,\n         hidden_size)` is output.\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if\n-        `config.is_encoder_decoder=True` 2 additional tensors of shape `(batch_size, num_heads,\n-        encoder_sequence_length, embed_size_per_head)`.\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n+\n         Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if\n         `config.is_encoder_decoder=True` in the cross-attention blocks) that can be used (see `past_key_values`\n         input) to speed up sequential decoding.\n@@ -66,7 +64,7 @@ class Idefics3BaseModelOutputWithPast(ModelOutput):\n     \"\"\"\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None\n+    past_key_values: Optional[Cache] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n     image_hidden_states: Optional[tuple[torch.FloatTensor]] = None\n@@ -85,8 +83,8 @@ class Idefics3CausalLMOutputWithPast(ModelOutput):\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n+\n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n         `past_key_values` input) to speed up sequential decoding.\n     image_hidden_states (`tuple(torch.FloatTensor)`, *optional*):\n@@ -97,7 +95,7 @@ class Idefics3CausalLMOutputWithPast(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[list[torch.FloatTensor]] = None\n+    past_key_values: Optional[Cache] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n     image_hidden_states: Optional[tuple[torch.FloatTensor]] = None"
        },
        {
            "sha": "6a424727d8e551e3496069e09906b700fad566b3",
            "filename": "src/transformers/models/imagegpt/modeling_imagegpt.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -573,7 +573,7 @@ def _prune_heads(self, heads_to_prune):\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.Tensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n@@ -800,7 +800,7 @@ def __init__(self, config: ImageGPTConfig):\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.Tensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n@@ -932,7 +932,7 @@ def __init__(self, config: ImageGPTConfig):\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.Tensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,"
        },
        {
            "sha": "b0fc4964a9aea30e23019dbfcfb602c6b4946558",
            "filename": "src/transformers/models/informer/modeling_informer.py",
            "status": "modified",
            "additions": 7,
            "deletions": 9,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -564,7 +564,7 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         key_value_states: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n@@ -911,7 +911,7 @@ def forward(\n                 `(encoder_attention_heads,)`.\n             cross_attn_layer_head_mask (`torch.FloatTensor`): mask for cross-attention heads in a given layer of\n                 size `(decoder_attention_heads,)`.\n-            past_key_values (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n+            past_key_values (`Cache`): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -1141,7 +1141,7 @@ def forward(\n         encoder_attention_mask: Optional[torch.LongTensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -1182,10 +1182,8 @@ def forward(\n                 - 1 indicates the head is **not masked**,\n                 - 0 indicates the head is **masked**.\n \n-            past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-                Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n-                shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n-                shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n+            past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+                It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n                 Contains pre-computed hidden-states (key and values in the self-attention blocks and in the\n                 cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n@@ -1478,7 +1476,7 @@ def forward(\n         decoder_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[list[torch.FloatTensor]] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_hidden_states: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         use_cache: Optional[bool] = None,\n@@ -1772,7 +1770,7 @@ def forward(\n         decoder_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[list[torch.FloatTensor]] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_hidden_states: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "aa6e2ad30a9f858393bff04dd5b3f797574bfacf",
            "filename": "src/transformers/models/informer/modular_informer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Finformer%2Fmodular_informer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Finformer%2Fmodular_informer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finformer%2Fmodular_informer.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -20,7 +20,7 @@\n import torch\n from torch import nn\n \n-from ...cache_utils import EncoderDecoderCache\n+from ...cache_utils import Cache, EncoderDecoderCache\n from ...modeling_attn_mask_utils import (\n     _prepare_4d_attention_mask,\n     _prepare_4d_attention_mask_for_sdpa,\n@@ -251,7 +251,7 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         key_value_states: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,"
        },
        {
            "sha": "2b43dd2998649e3170fc965556c5d9a9a0a6bf11",
            "filename": "src/transformers/models/internvl/modeling_internvl.py",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -509,8 +509,7 @@ def forward(self, image_features):\n class InternVLModelOutputWithPast(BaseModelOutputWithPast):\n     r\"\"\"\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n         `past_key_values` input) to speed up sequential decoding.\n@@ -736,8 +735,7 @@ class InternVLCausalLMOutputWithPast(ModelOutput):\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n         `past_key_values` input) to speed up sequential decoding.\n@@ -748,7 +746,7 @@ class InternVLCausalLMOutputWithPast(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[list[torch.FloatTensor]] = None\n+    past_key_values: Optional[Cache] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n     image_hidden_states: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "eee387664832cbb8bd654f1dbbfd87423de9d7af",
            "filename": "src/transformers/models/janus/modeling_janus.py",
            "status": "modified",
            "additions": 4,
            "deletions": 8,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -100,10 +100,7 @@ class JanusBaseModelOutputWithPast(ModelOutput):\n         If `past_key_values` is used only the last hidden-state of the sequences of shape `(batch_size, 1,\n         hidden_size)` is output.\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if\n-        `config.is_encoder_decoder=True` 2 additional tensors of shape `(batch_size, num_heads,\n-        encoder_sequence_length, embed_size_per_head)`.\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if\n         `config.is_encoder_decoder=True` in the cross-attention blocks) that can be used (see `past_key_values`\n@@ -116,7 +113,7 @@ class JanusBaseModelOutputWithPast(ModelOutput):\n     \"\"\"\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None\n+    past_key_values: Optional[Cache] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n     image_hidden_states: Optional[tuple[torch.FloatTensor]] = None\n@@ -135,8 +132,7 @@ class JanusCausalLMOutputWithPast(ModelOutput):\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n         `past_key_values` input) to speed up sequential decoding.\n@@ -149,7 +145,7 @@ class JanusCausalLMOutputWithPast(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[list[torch.FloatTensor]] = None\n+    past_key_values: Optional[Cache] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n     image_hidden_states: Optional[tuple[torch.FloatTensor]] = None"
        },
        {
            "sha": "896426a5e3208728afb0177e0f166716a5ef89f5",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -798,7 +798,7 @@ def forward(\n         self,\n         hidden_states: Optional[torch.FloatTensor],\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = False,\n         output_router_logits: Optional[bool] = False,"
        },
        {
            "sha": "0372ec92a6ee039dd3da9a896ba12c0ffd5bae8e",
            "filename": "src/transformers/models/kosmos2/modeling_kosmos2.py",
            "status": "modified",
            "additions": 11,
            "deletions": 17,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -99,11 +99,8 @@ def create_position_ids_from_input_ids(input_ids, padding_idx, past_key_values_l\n )\n class Kosmos2ModelOutput(ModelOutput):\n     r\"\"\"\n-    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if\n-        `config.is_encoder_decoder=True` 2 additional tensors of shape `(batch_size, num_heads,\n-        encoder_sequence_length, embed_size_per_head)`.\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if\n         `config.is_encoder_decoder=True` in the cross-attention blocks) that can be used (see `past_key_values`\n@@ -121,7 +118,7 @@ class Kosmos2ModelOutput(ModelOutput):\n     \"\"\"\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None\n+    past_key_values: Optional[Cache] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n     image_embeds: Optional[torch.FloatTensor] = None\n@@ -147,11 +144,8 @@ class Kosmos2ForConditionalGenerationModelOutput(ModelOutput):\n         Language modeling loss (for next-token prediction).\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if\n-        `config.is_encoder_decoder=True` 2 additional tensors of shape `(batch_size, num_heads,\n-        encoder_sequence_length, embed_size_per_head)`.\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if\n         `config.is_encoder_decoder=True` in the cross-attention blocks) that can be used (see `past_key_values`\n@@ -170,7 +164,7 @@ class Kosmos2ForConditionalGenerationModelOutput(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None\n+    past_key_values: Optional[Cache] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n     image_embeds: Optional[torch.FloatTensor] = None\n@@ -1006,7 +1000,7 @@ def forward(\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -1259,7 +1253,7 @@ def forward(\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -1342,7 +1336,7 @@ def forward(\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n@@ -1561,7 +1555,7 @@ def forward(\n         image_embeds_position_mask: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         image_embeds: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n@@ -1697,7 +1691,7 @@ def forward(\n         image_embeds_position_mask: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         image_embeds: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,"
        },
        {
            "sha": "51357a57726c195d791df3235a35ab99a16b4757",
            "filename": "src/transformers/models/kosmos2_5/modeling_kosmos2_5.py",
            "status": "modified",
            "additions": 7,
            "deletions": 13,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fmodeling_kosmos2_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fmodeling_kosmos2_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fmodeling_kosmos2_5.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -150,7 +150,7 @@ def create_position_ids_from_input_ids(input_ids, padding_idx, past_key_values_l\n             - 1 for places where to put the image features,\n             - 0 for places that are not for image features (i.e. for text tokens).\n \n-        past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n+        past_key_values (`Cache` of length `config.n_layers` with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n             Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n \n             If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n@@ -210,7 +210,7 @@ def create_position_ids_from_input_ids(input_ids, padding_idx, past_key_values_l\n \n             [What are attention masks?](../glossary#attention-mask)\n \n-        past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n+        past_key_values (`Cache` of length `config.n_layers` with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n             Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n \n             If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n@@ -272,19 +272,16 @@ class Kosmos2_5ModelOutput(ModelOutput):\n             the weighted average in the self-attention heads.\n         vision_model_output(`BaseModelOutputWithPooling`, *optional*):\n             The output of the [`Kosmos2VisionModel`].\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if\n-            `config.is_encoder_decoder=True` 2 additional tensors of shape `(batch_size, num_heads,\n-            encoder_sequence_length, embed_size_per_head)`.\n+        past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+            It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n             Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if\n             `config.is_encoder_decoder=True` in the cross-attention blocks) that can be used (see `past_key_values`\n             input) to speed up sequential decoding.\n     \"\"\"\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None\n+    past_key_values: Optional[Cache] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n     width: Optional[torch.FloatTensor] = None\n@@ -334,11 +331,8 @@ class Kosmos2_5ForConditionalGenerationModelOutput(ModelOutput):\n             the weighted average in the self-attention heads.\n         vision_model_output(`BaseModelOutputWithPooling`, *optional*):\n             The output of the [`Kosmos2VisionModel`].\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if\n-            `config.is_encoder_decoder=True` 2 additional tensors of shape `(batch_size, num_heads,\n-            encoder_sequence_length, embed_size_per_head)`.\n+        past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+            It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n             Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if\n             `config.is_encoder_decoder=True` in the cross-attention blocks) that can be used (see `past_key_values`"
        },
        {
            "sha": "9eba7e163670c554ff1c096ba9cd595fa973f256",
            "filename": "src/transformers/models/kyutai_speech_to_text/modeling_kyutai_speech_to_text.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -761,7 +761,7 @@ def forward(\n             use_cache (`bool`, *optional*):\n                 If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                 (see `past_key_values`).\n-            past_key_values (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            past_key_values (`Cache`, *optional*): cached past key and value projection states\n             cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n                 Indices depicting the position of the input sequence tokens in the sequence\n             kwargs (`dict`, *optional*):"
        },
        {
            "sha": "e34a261df55293e3cfdf9e8978ea0c6e5268fe47",
            "filename": "src/transformers/models/led/modeling_led.py",
            "status": "modified",
            "additions": 17,
            "deletions": 23,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -1027,7 +1027,7 @@ def forward(\n                 *(decoder_attention_heads,)*.\n             cross_attn_layer_head_mask (`torch.FloatTensor`): mask for encoder attention heads in a given layer of\n                 size *(decoder_attention_heads,)*.\n-            past_key_values (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n+            past_key_values (`Cache`): cached past key and value projection states\n             output_attentions (`bool`): Whether the base model outputs attentions.\n                 This requires the attentions tensor to be reshaped in this function.\n         \"\"\"\n@@ -1190,9 +1190,8 @@ class LEDSeq2SeqModelOutput(ModelOutput):\n \n         If `past_key_values` is used only the last hidden-state of the sequences of shape `(batch_size, 1,\n         hidden_size)` is output.\n-    past_key_values (`list[torch.FloatTensor]`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        List of `torch.FloatTensor` of length `config.n_layers`, with each tensor of shape `(2, batch_size,\n-        num_heads, sequence_length, embed_size_per_head)`).\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be\n         used (see `past_key_values` input) to speed up sequential decoding.\n@@ -1206,7 +1205,7 @@ class LEDSeq2SeqModelOutput(ModelOutput):\n     \"\"\"\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[list[torch.FloatTensor]] = None\n+    past_key_values: Optional[Cache] = None\n     decoder_hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n     decoder_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n     cross_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n@@ -1228,9 +1227,8 @@ class LEDSeq2SeqLMOutput(ModelOutput):\n         Language modeling loss.\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-    past_key_values (`list[torch.FloatTensor]`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        List of `torch.FloatTensor` of length `config.n_layers`, with each tensor of shape `(2, batch_size,\n-        num_heads, sequence_length, embed_size_per_head)`).\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be\n         used (see `past_key_values` input) to speed up sequential decoding.\n@@ -1245,7 +1243,7 @@ class LEDSeq2SeqLMOutput(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[list[torch.FloatTensor]] = None\n+    past_key_values: Optional[Cache] = None\n     decoder_hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n     decoder_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n     cross_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n@@ -1267,9 +1265,8 @@ class LEDSeq2SeqSequenceClassifierOutput(ModelOutput):\n         Classification (or regression if config.num_labels==1) loss.\n     logits (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`):\n         Classification (or regression if config.num_labels==1) scores (before SoftMax).\n-    past_key_values (`list[torch.FloatTensor]`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        List of `torch.FloatTensor` of length `config.n_layers`, with each tensor of shape `(2, batch_size,\n-        num_heads, sequence_length, embed_size_per_head)`).\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be\n         used (see `past_key_values` input) to speed up sequential decoding.\n@@ -1284,7 +1281,7 @@ class LEDSeq2SeqSequenceClassifierOutput(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[list[torch.FloatTensor]] = None\n+    past_key_values: Optional[Cache] = None\n     decoder_hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n     decoder_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n     cross_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n@@ -1304,9 +1301,8 @@ class LEDSeq2SeqQuestionAnsweringModelOutput(ModelOutput):\n     r\"\"\"\n     loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n         Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.\n-    past_key_values (`list[torch.FloatTensor]`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        List of `torch.FloatTensor` of length `config.n_layers`, with each tensor of shape `(2, batch_size,\n-        num_heads, sequence_length, embed_size_per_head)`).\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be\n         used (see `past_key_values` input) to speed up sequential decoding.\n@@ -1322,7 +1318,7 @@ class LEDSeq2SeqQuestionAnsweringModelOutput(ModelOutput):\n     loss: Optional[torch.FloatTensor] = None\n     start_logits: Optional[torch.FloatTensor] = None\n     end_logits: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[list[torch.FloatTensor]] = None\n+    past_key_values: Optional[Cache] = None\n     decoder_hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n     decoder_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n     cross_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n@@ -1709,10 +1705,8 @@ def forward(\n                 - 1 indicates the head is **not masked**,\n                 - 0 indicates the head is **masked**.\n \n-            past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-                Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n-                shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n-                shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n+            past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+                It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n                 Contains pre-computed hidden-states (key and values in the self-attention blocks and in the\n                 cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n@@ -1899,7 +1893,7 @@ def forward(\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         global_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -2064,7 +2058,7 @@ def forward(\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         global_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "5ea4314968e23d399f136a6848ed0c70899f9937",
            "filename": "src/transformers/models/lfm2/modeling_lfm2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -544,7 +544,7 @@ def forward(\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[Lfm2HybridConvCache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs,\n     ) -> torch.Tensor:"
        },
        {
            "sha": "5832a4d457a07e8508df4069f683ecbf2a1b8622",
            "filename": "src/transformers/models/lfm2/modular_lfm2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodular_lfm2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodular_lfm2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodular_lfm2.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -409,7 +409,7 @@ def forward(\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[Lfm2HybridConvCache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs,\n     ) -> torch.Tensor:"
        },
        {
            "sha": "a53443004d4963e6295fc7c170be7e5ff6a6eacc",
            "filename": "src/transformers/models/llama4/modeling_llama4.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -392,7 +392,7 @@ def forward(\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n@@ -657,8 +657,7 @@ class Llama4CausalLMOutputWithPast(ModelOutput):\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n         `past_key_values` input) to speed up sequential decoding.\n@@ -669,7 +668,7 @@ class Llama4CausalLMOutputWithPast(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[list[torch.FloatTensor]] = None\n+    past_key_values: Optional[Cache] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n     image_hidden_states: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "9a116dac4d23448892f10d516f6356be4699ae0d",
            "filename": "src/transformers/models/llava/modeling_llava.py",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -44,8 +44,7 @@\n class LlavaModelOutputWithPast(BaseModelOutputWithPast):\n     r\"\"\"\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n         `past_key_values` input) to speed up sequential decoding.\n@@ -70,8 +69,7 @@ class LlavaCausalLMOutputWithPast(ModelOutput):\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n         `past_key_values` input) to speed up sequential decoding.\n@@ -82,7 +80,7 @@ class LlavaCausalLMOutputWithPast(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[list[torch.FloatTensor]] = None\n+    past_key_values: Optional[Cache] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n     image_hidden_states: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "8cca63f4a66c1c2642fa8c9e4c3a5588ae9ac837",
            "filename": "src/transformers/models/llava_next/modeling_llava_next.py",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -154,8 +154,7 @@ def unpad_image(tensor, original_size):\n class LlavaNextModelOutputWithPast(BaseModelOutputWithPast):\n     r\"\"\"\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n         `past_key_values` input) to speed up sequential decoding.\n@@ -180,8 +179,7 @@ class LlavaNextCausalLMOutputWithPast(ModelOutput):\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n         `past_key_values` input) to speed up sequential decoding.\n@@ -192,7 +190,7 @@ class LlavaNextCausalLMOutputWithPast(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[list[torch.FloatTensor]] = None\n+    past_key_values: Optional[Cache] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n     image_hidden_states: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "3ef172962c2c608cfad7403a68bc2aa7e539c9a5",
            "filename": "src/transformers/models/llava_next_video/modeling_llava_next_video.py",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -52,8 +52,7 @@\n class LlavaNextVideoModelOutputWithPast(BaseModelOutputWithPast):\n     r\"\"\"\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n         `past_key_values` input) to speed up sequential decoding.\n@@ -83,8 +82,7 @@ class LlavaNextVideoCausalLMOutputWithPast(ModelOutput):\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n         `past_key_values` input) to speed up sequential decoding.\n@@ -98,7 +96,7 @@ class LlavaNextVideoCausalLMOutputWithPast(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[list[torch.FloatTensor]] = None\n+    past_key_values: Optional[Cache] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n     image_hidden_states: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "73745f435b7da8dd48eba48f93fa3478a68982dd",
            "filename": "src/transformers/models/llava_next_video/modular_llava_next_video.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -183,8 +183,7 @@ def __init__(\n class LlavaNextVideoModelOutputWithPast(LlavaNextModelOutputWithPast):\n     r\"\"\"\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n         `past_key_values` input) to speed up sequential decoding.\n@@ -206,8 +205,7 @@ class LlavaNextVideoCausalLMOutputWithPast(LlavaNextCausalLMOutputWithPast):\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n         `past_key_values` input) to speed up sequential decoding."
        },
        {
            "sha": "e4cb0c9aeafd386d6f79b01c4853cdf29cc15ec1",
            "filename": "src/transformers/models/llava_onevision/modeling_llava_onevision.py",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -53,8 +53,7 @@\n class LlavaOnevisionModelOutputWithPast(BaseModelOutputWithPast):\n     r\"\"\"\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n         `past_key_values` input) to speed up sequential decoding.\n@@ -84,8 +83,7 @@ class LlavaOnevisionCausalLMOutputWithPast(ModelOutput):\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n         `past_key_values` input) to speed up sequential decoding.\n@@ -99,7 +97,7 @@ class LlavaOnevisionCausalLMOutputWithPast(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[list[torch.FloatTensor]] = None\n+    past_key_values: Optional[Cache] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n     image_hidden_states: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "6015aa54d76b1ef6f7ccb326867ada6c287429a4",
            "filename": "src/transformers/models/m2m_100/modeling_m2m_100.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -462,7 +462,7 @@ def forward(\n                 `(encoder_attention_heads,)`.\n             cross_attn_layer_head_mask (`torch.FloatTensor`): mask for cross-attention heads in a given layer of\n                 size `(decoder_attention_heads,)`.\n-            past_key_values (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n+            past_key_values (`Cache`): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -1001,9 +1001,7 @@ def forward(\n                 - 0 indicates the head is **masked**.\n \n             past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-                Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n-                shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n-                shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n+                It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n                 Contains pre-computed hidden-states (key and values in the self-attention blocks and in the\n                 cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding."
        },
        {
            "sha": "342e622321a85a0bfd9d4b6f58dceeea4c1642c8",
            "filename": "src/transformers/models/marian/modeling_marian.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -405,7 +405,7 @@ def forward(\n                 `(encoder_attention_heads,)`.\n             cross_attn_layer_head_mask (`torch.FloatTensor`): mask for cross-attention heads in a given layer of\n                 size `(decoder_attention_heads,)`.\n-            past_key_values (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n+            past_key_values (`Cache`): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -938,9 +938,7 @@ def forward(\n                 - 0 indicates the head is **masked**.\n \n             past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-                Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n-                shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n-                shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n+                It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n                 Contains pre-computed hidden-states (key and values in the self-attention blocks and in the\n                 cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding."
        },
        {
            "sha": "21c54b6de60e68caaf2fff08d72ec92a5eb4c413",
            "filename": "src/transformers/models/mbart/modeling_mbart.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -406,7 +406,7 @@ def forward(\n                 `(encoder_attention_heads,)`.\n             cross_attn_layer_head_mask (`torch.FloatTensor`): mask for cross-attention heads in a given layer of\n                 size `(decoder_attention_heads,)`.\n-            past_key_values (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n+            past_key_values (`Cache`): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -984,9 +984,7 @@ def forward(\n                 - 0 indicates the head is **masked**.\n \n             past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-                Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n-                shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n-                shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n+                It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n                 Contains pre-computed hidden-states (key and values in the self-attention blocks and in the\n                 cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding."
        },
        {
            "sha": "4987bd15dffd9241f8380e074d3f5c2ae17b7a6d",
            "filename": "src/transformers/models/megatron_bert/modeling_megatron_bert.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -497,7 +497,7 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,\n@@ -773,7 +773,7 @@ def forward(\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.Tensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -1022,7 +1022,7 @@ def forward(\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.Tensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,"
        },
        {
            "sha": "64537d5fcd9465cc1fe8956e8f16c7cf2dcb348f",
            "filename": "src/transformers/models/mimi/modeling_mimi.py",
            "status": "modified",
            "additions": 3,
            "deletions": 14,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -957,7 +957,7 @@ def forward(\n             use_cache (`bool`, *optional*):\n                 If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                 (see `past_key_values`).\n-            past_key_values (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            past_key_values (`Cache`, *optional*): cached past key and value projection states\n             cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n                 Indices depicting the position of the input sequence tokens in the sequence\n             kwargs (`dict`, *optional*):\n@@ -1055,19 +1055,8 @@ def forward(\n                 config.n_positions - 1]`.\n \n                 [What are position IDs?](../glossary#position-ids)\n-            past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):\n-                Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n-                blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n-                returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n-\n-                Two formats are allowed:\n-                - a [`~cache_utils.Cache`] instance;\n-                - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n-                shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n-                cache format.\n-\n-                The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the\n-                legacy cache format will be returned.\n+            past_key_values (`Cache`, *optional*):\n+                It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n                 If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n                 have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`"
        },
        {
            "sha": "633e053e2d546c28e58c1ec50e650c5ac6aab26e",
            "filename": "src/transformers/models/minimax/modeling_minimax.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -517,7 +517,7 @@ def forward(\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         output_router_logits: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n@@ -532,7 +532,7 @@ def forward(\n                 with `head_dim` being the embedding dimension of each attention head.\n             attention_mask (`torch.Tensor`, *optional*): attention mask of size\n                 `(batch, sequence_length)` where padding elements are indicated by 0.\n-            past_key_values (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            past_key_values (`Cache`, *optional*): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail."
        },
        {
            "sha": "76f9df0d5304874e387e62ea82a330be741e1780",
            "filename": "src/transformers/models/minimax/modular_minimax.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodular_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodular_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodular_minimax.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -412,7 +412,7 @@ def forward(\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         output_router_logits: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n@@ -427,7 +427,7 @@ def forward(\n                 with `head_dim` being the embedding dimension of each attention head.\n             attention_mask (`torch.Tensor`, *optional*): attention mask of size\n                 `(batch, sequence_length)` where padding elements are indicated by 0.\n-            past_key_values (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            past_key_values (`Cache`, *optional*): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail."
        },
        {
            "sha": "2c2a53a54352bdb543318d20e094efdc5e2559b8",
            "filename": "src/transformers/models/mistral3/modeling_mistral3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -135,8 +135,7 @@ class Mistral3CausalLMOutputWithPast(ModelOutput):\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n         `past_key_values` input) to speed up sequential decoding.\n@@ -147,7 +146,7 @@ class Mistral3CausalLMOutputWithPast(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[list[torch.FloatTensor]] = None\n+    past_key_values: Optional[Cache] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n     image_hidden_states: Optional[torch.FloatTensor] = None\n@@ -162,8 +161,7 @@ class Mistral3CausalLMOutputWithPast(ModelOutput):\n class Mistral3ModelOutputWithPast(BaseModelOutputWithPast):\n     r\"\"\"\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n         `past_key_values` input) to speed up sequential decoding."
        },
        {
            "sha": "2412092aeb8621b79d1ca605fb9f717102f83a62",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -313,7 +313,7 @@ def forward(\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> torch.FloatTensor:"
        },
        {
            "sha": "d897824c4cff3197dfd33b80167cd2fb91b07d59",
            "filename": "src/transformers/models/mixtral/modular_mixtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -245,7 +245,7 @@ def forward(\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> torch.FloatTensor:"
        },
        {
            "sha": "e30ac5a8fa64526b0dc44a852f78265c314154bf",
            "filename": "src/transformers/models/mllama/modeling_mllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -635,7 +635,7 @@ def forward(\n             use_cache (`bool`, *optional*):\n                 If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                 (see `past_key_values`).\n-            past_key_values (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            past_key_values (`Cache`, *optional*): cached past key and value projection states\n             cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n                 Indices depicting the position of the input sequence tokens in the sequence\n             position_embeddings (`tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):"
        },
        {
            "sha": "868d050db5c9c47f057da4190ca4f82d5a3f37f9",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 11,
            "deletions": 24,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -83,7 +83,7 @@ class MoshiConditionalGenerationGenerateOutput(ModelOutput):\n     hidden_states (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_hidden_states=True`):\n         Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n         `torch.FloatTensor` of shape `(batch_size*num_beams*num_return_sequences, generated_length, hidden_size)`.\n-    past_key_values (`tuple(tuple(torch.FloatTensor)))`, *optional*, returned when `use_cache=True`):\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True`):\n         Contains the model cache, used to speed up decoding. Different models have a different cache format, check\n         the model's documentation. Usually, a [`~cache_utils.Cache`] instance.\n     audio_codes (`torch.LongTensor` of shape `(batch_size*num_return_sequences, num_codeooks, sequence_length)`, *optional*):\n@@ -98,7 +98,7 @@ class MoshiConditionalGenerationGenerateOutput(ModelOutput):\n     beam_indices: Optional[torch.LongTensor] = None\n     attentions: Optional[tuple[tuple[torch.FloatTensor]]] = None\n     hidden_states: Optional[tuple[tuple[torch.FloatTensor]]] = None\n-    past_key_values: Optional[tuple[tuple[tuple[torch.FloatTensor]]]] = None\n+    past_key_values: Optional[Cache] = None\n     audio_codes: Optional[torch.LongTensor] = None\n \n \n@@ -115,8 +115,7 @@ class MoshiCausalLMOutputWithPast(ModelOutput):\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n         `past_key_values` input) to speed up sequential decoding.\n@@ -125,7 +124,7 @@ class MoshiCausalLMOutputWithPast(ModelOutput):\n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n     last_hidden_state: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None\n+    past_key_values: Optional[Cache] = None\n     hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n \n@@ -143,16 +142,15 @@ class MoshiConditionalGenerationOutputWithPast(ModelOutput):\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n         Prediction scores of the text language modeling head (scores for each vocabulary token before SoftMax).\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n         `past_key_values` input) to speed up sequential decoding.\n     depth_loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `audio_labels` is provided):\n         Audio language modeling loss (for next-token prediction).\n     audio_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n         Prediction scores of the audio language modeling heads.\n-    depth_past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+    depth_past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n         Past key-values of the depth decoder.\n     depth_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n         Hidden states of the depth decoder\n@@ -164,12 +162,12 @@ class MoshiConditionalGenerationOutputWithPast(ModelOutput):\n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n     last_hidden_state: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None\n+    past_key_values: Optional[Cache] = None\n     hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n     depth_loss: Optional[torch.FloatTensor] = None\n     audio_logits: Optional[torch.FloatTensor] = None\n-    depth_past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None\n+    depth_past_key_values: Optional[Cache] = None\n     depth_hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n     depth_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n \n@@ -764,7 +762,7 @@ def forward(\n             use_cache (`bool`, *optional*):\n                 If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                 (see `past_key_values`).\n-            past_key_values (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            past_key_values (`Cache`, *optional*): cached past key and value projection states\n             cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n                 Indices depicting the position of the input sequence tokens in the sequence\n             kwargs (`dict`, *optional*):\n@@ -908,19 +906,8 @@ def forward(\n \n                 - 1 indicates the head is **not masked**,\n                 - 0 indicates the head is **masked**.\n-            past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):\n-                Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n-                blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n-                returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n-\n-                Two formats are allowed:\n-                - a [`~cache_utils.Cache`] instance;\n-                - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n-                shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n-                cache format.\n-\n-                The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the\n-                legacy cache format will be returned.\n+            past_key_values (`Cache`, *optional*):\n+                It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n                 If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n                 have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`"
        },
        {
            "sha": "57b875432758014bd8f5908fe42394ddeabbd891",
            "filename": "src/transformers/models/mpt/modeling_mpt.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fmpt%2Fmodeling_mpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fmpt%2Fmodeling_mpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmpt%2Fmodeling_mpt.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -305,7 +305,7 @@ def set_input_embeddings(self, new_embeddings: torch.Tensor):\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[tuple[tuple[torch.Tensor, torch.Tensor], ...], Cache]] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -445,7 +445,7 @@ def set_output_embeddings(self, new_embeddings: torch.Tensor):\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.Tensor, torch.Tensor], ...]] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n@@ -543,7 +543,7 @@ def __init__(self, config: MptConfig):\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.Tensor, torch.Tensor], ...]] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n@@ -666,7 +666,7 @@ def __init__(self, config: MptConfig):\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.Tensor, torch.Tensor], ...]] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,"
        },
        {
            "sha": "fec5fabc54703d69246d1e98659104a03187ae77",
            "filename": "src/transformers/models/musicgen/modeling_musicgen.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -365,7 +365,7 @@ def forward(\n                 `(encoder_attention_heads,)`.\n             cross_attn_layer_head_mask (`torch.FloatTensor`): mask for cross-attention heads in a given layer of\n                 size `(decoder_attention_heads,)`.\n-            past_key_values (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n+            past_key_values (`Cache`): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -487,7 +487,7 @@ def forward(\n         encoder_attention_mask: Optional[torch.LongTensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -751,7 +751,7 @@ def forward(\n         encoder_attention_mask: Optional[torch.LongTensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -878,7 +878,7 @@ def forward(\n         encoder_attention_mask: Optional[torch.LongTensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -1699,7 +1699,7 @@ def forward(\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.BoolTensor] = None,\n         encoder_outputs: Optional[tuple[torch.FloatTensor]] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "e7237157e156e14fb366c3a2f9b714aab8b45d6e",
            "filename": "src/transformers/models/musicgen_melody/modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 8,
            "deletions": 9,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -75,9 +75,8 @@ class MusicgenMelodyOutputWithPast(ModelOutput):\n         Language modeling loss (for next-token prediction).\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n         `past_key_values` input) to speed up sequential decoding.\n@@ -88,7 +87,7 @@ class MusicgenMelodyOutputWithPast(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None\n+    past_key_values: Optional[Cache] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n     encoder_hidden_states: Optional[torch.FloatTensor] = None\n@@ -354,7 +353,7 @@ def forward(\n             attention_mask (`torch.FloatTensor`): attention mask of size\n                 `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n             layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size `(attention_heads,)`.\n-            past_key_values (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n+            past_key_values (`Cache`): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -455,7 +454,7 @@ def forward(\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.LongTensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -689,7 +688,7 @@ def forward(\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.LongTensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -809,7 +808,7 @@ def forward(\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.LongTensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -1585,7 +1584,7 @@ def forward(\n         input_features: Optional[torch.FloatTensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.BoolTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,"
        },
        {
            "sha": "22af2b5a74de932d4b0cd5700d52752e50dd4b73",
            "filename": "src/transformers/models/mvp/modeling_mvp.py",
            "status": "modified",
            "additions": 7,
            "deletions": 9,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -383,7 +383,7 @@ def forward(\n                 `(2, decoder_attention_heads, pro_len, head_dim)`.\n             cross_attn_prompt (`torch.FloatTensor`): prompt of cross attention of shape\n                 `(2, decoder_attention_heads, pro_len, head_dim)`.\n-            past_key_values (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n+            past_key_values (`Cache`): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -755,7 +755,7 @@ def forward(\n         encoder_attention_mask: Optional[torch.LongTensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -804,10 +804,8 @@ def forward(\n                 - 1 indicates the head is **not masked**,\n                 - 0 indicates the head is **masked**.\n \n-            past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-                Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n-                shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n-                shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n+            past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+                It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n                 Contains pre-computed hidden-states (key and values in the self-attention blocks and in the\n                 cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n@@ -1009,7 +1007,7 @@ def forward(\n         decoder_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[list[torch.FloatTensor]] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -1172,7 +1170,7 @@ def forward(\n         decoder_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[list[torch.FloatTensor]] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n@@ -1700,7 +1698,7 @@ def forward(\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "35b1aedb71f8474f6771cdcea8380979d67f089c",
            "filename": "src/transformers/models/nemotron/modeling_nemotron.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -542,7 +542,7 @@ def forward(\n             use_cache (`bool`, *optional*):\n                 If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                 (see `past_key_values`).\n-            past_key_values (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            past_key_values (`Cache`, *optional*): cached past key and value projection states\n             cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n                 Indices depicting the position of the input sequence tokens in the sequence\n             position_embeddings (`tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):"
        },
        {
            "sha": "f0131b6b999b9ab75a557de1a4a5a73147ed0724",
            "filename": "src/transformers/models/nllb_moe/modeling_nllb_moe.py",
            "status": "modified",
            "additions": 6,
            "deletions": 8,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -773,7 +773,7 @@ def forward(\n                 mask for attention heads in a given layer of size `(encoder_attention_heads,)`.\n             cross_attn_layer_head_mask (`torch.FloatTensor`):\n                 mask for cross-attention heads in a given layer of size `(decoder_attention_heads,)`.\n-            past_key_values (`Tuple(torch.FloatTensor)`):\n+            past_key_values (`Cache`):\n                 cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n@@ -1128,7 +1128,7 @@ def forward(\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -1178,10 +1178,8 @@ def forward(\n                 - 1 indicates the head is **not masked**,\n                 - 0 indicates the head is **masked**.\n \n-            past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-                Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n-                shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n-                shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n+            past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+                It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n                 Contains pre-computed hidden-states (key and values in the self-attention blocks and in the\n                 cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n@@ -1470,7 +1468,7 @@ def forward(\n         decoder_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -1614,7 +1612,7 @@ def forward(\n         decoder_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "9d7d0727da9b982050fd935b6d41477b3d5a6d18",
            "filename": "src/transformers/models/olmoe/modeling_olmoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -662,7 +662,7 @@ def forward(\n             use_cache (`bool`, *optional*):\n                 If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                 (see `past_key_values`).\n-            past_key_values (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            past_key_values (`Cache`, *optional*): cached past key and value projection states\n             cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n                 Indices depicting the position of the input sequence tokens in the sequence\n             position_embeddings (`tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):"
        },
        {
            "sha": "a2c4bb500a65b55841597ccedebfe228a144b72e",
            "filename": "src/transformers/models/opt/modeling_opt.py",
            "status": "modified",
            "additions": 8,
            "deletions": 9,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -143,7 +143,7 @@ def __init__(\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        past_key_values: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n@@ -221,7 +221,7 @@ def forward(\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         position_ids: Optional[torch.LongTensor] = None,\n@@ -241,7 +241,7 @@ def forward(\n             use_cache (`bool`, *optional*):\n                 If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                 (see `past_key_values`).\n-            past_key_values (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            past_key_values (`Cache`, *optional*): cached past key and value projection states\n             cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n                 Indices depicting the position of the input sequence tokens in the sequence..\n         \"\"\"\n@@ -537,8 +537,7 @@ def forward(\n                 - 0 indicates the head is **masked**.\n \n             past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-                Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n-                shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n+                It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n                 Contains pre-computed hidden-states (key and values in the self-attention blocks and in the\n                 cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n@@ -703,7 +702,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Union[list[torch.FloatTensor], Cache]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -776,7 +775,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Union[list[torch.FloatTensor], Cache]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -884,7 +883,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Union[list[torch.FloatTensor], Cache]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -996,7 +995,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Union[list[torch.FloatTensor], Cache]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         start_positions: Optional[torch.LongTensor] = None,\n         end_positions: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "75ff19ab9d14fc34b69a0759947373531a4f91f7",
            "filename": "src/transformers/models/ovis2/modeling_ovis2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 7,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fovis2%2Fmodeling_ovis2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fovis2%2Fmodeling_ovis2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fovis2%2Fmodeling_ovis2.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -27,6 +27,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n+from ...cache_utils import Cache\n from ...generation import GenerationMixin\n from ...integrations import use_kernel_forward_from_hub\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -47,8 +48,7 @@\n class Ovis2ModelOutputWithPast(BaseModelOutputWithPast):\n     r\"\"\"\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n         `past_key_values` input) to speed up sequential decoding.\n@@ -73,8 +73,7 @@ class Ovis2CausalLMOutputWithPast(ModelOutput):\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n         `past_key_values` input) to speed up sequential decoding.\n@@ -85,7 +84,7 @@ class Ovis2CausalLMOutputWithPast(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[list[torch.FloatTensor]] = None\n+    past_key_values: Optional[Cache] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n     image_hidden_states: Optional[torch.FloatTensor] = None\n@@ -596,7 +595,7 @@ def forward(\n         pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -717,7 +716,7 @@ def forward(\n         pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "09ce53703a15851f85a3dac6a10ce00f9522cf75",
            "filename": "src/transformers/models/ovis2/modular_ovis2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fovis2%2Fmodular_ovis2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fovis2%2Fmodular_ovis2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fovis2%2Fmodular_ovis2.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -19,6 +19,7 @@\n import torch\n from torch import nn\n \n+from ...cache_utils import Cache\n from ...generation import GenerationMixin\n from ...modeling_outputs import BaseModelOutput\n from ...modeling_utils import PreTrainedModel\n@@ -257,7 +258,7 @@ def forward(\n         pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -351,7 +352,7 @@ def forward(\n         pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "5600af24344f9d2c9e0883024488acffa7abb376",
            "filename": "src/transformers/models/paligemma/modeling_paligemma.py",
            "status": "modified",
            "additions": 4,
            "deletions": 11,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -49,12 +49,6 @@\n )\n class PaligemmaModelOutputWithPast(BaseModelOutputWithPast):\n     r\"\"\"\n-    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n-\n-        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n-        `past_key_values` input) to speed up sequential decoding.\n     image_hidden_states (`torch.FloatTensor`, *optional*):\n         A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n         image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n@@ -76,8 +70,7 @@ class PaliGemmaCausalLMOutputWithPast(ModelOutput):\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.text_config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n         `past_key_values` input) to speed up sequential decoding.\n@@ -88,7 +81,7 @@ class PaliGemmaCausalLMOutputWithPast(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[Union[list[torch.FloatTensor], Cache]] = None\n+    past_key_values: Optional[Cache] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n     image_hidden_states: Optional[torch.FloatTensor] = None\n@@ -280,7 +273,7 @@ def forward(\n         pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[list[torch.FloatTensor], Cache]] = None,\n+        past_key_values: Optional[Cache] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n@@ -440,7 +433,7 @@ def forward(\n         pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[list[torch.FloatTensor], Cache]] = None,\n+        past_key_values: Optional[Cache] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,"
        },
        {
            "sha": "58eedc77bc3c6319ed1b502f0dff9b75265e36ce",
            "filename": "src/transformers/models/pegasus/modeling_pegasus.py",
            "status": "modified",
            "additions": 4,
            "deletions": 6,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -396,7 +396,7 @@ def forward(\n                 `(encoder_attention_heads,)`.\n             cross_attn_layer_head_mask (`torch.FloatTensor`): mask for cross-attention heads in a given layer of\n                 size `(decoder_attention_heads,)`.\n-            past_key_values (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n+            past_key_values (`Cache`): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -988,9 +988,7 @@ def forward(\n                 - 0 indicates the head is **masked**.\n \n             past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-                Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n-                shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n-                shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n+                It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n                 Contains pre-computed hidden-states (key and values in the self-attention blocks and in the\n                 cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n@@ -1222,7 +1220,7 @@ def forward(\n         decoder_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[tuple[torch.FloatTensor]] = None,\n-        past_key_values: Optional[tuple[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         decoder_inputs_embeds: Optional[torch.Tensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -1402,7 +1400,7 @@ def forward(\n         decoder_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[tuple[torch.FloatTensor]] = None,\n-        past_key_values: Optional[tuple[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         decoder_inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,"
        },
        {
            "sha": "0279688c00e8022ea2bb8836ee152163b392722b",
            "filename": "src/transformers/models/pegasus_x/modeling_pegasus_x.py",
            "status": "modified",
            "additions": 4,
            "deletions": 6,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -696,7 +696,7 @@ def forward(\n                 cross attention input to the layer of shape *(seq_len, batch, embed_dim)*\n             encoder_attention_mask (`torch.FloatTensor`): encoder attention mask of size\n                 *(batch, 1, tgt_len, src_len)* where padding elements are indicated by very large negative values.\n-            past_key_values (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n+            past_key_values (`Cache`): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -1248,9 +1248,7 @@ def forward(\n                 [What are attention masks?](../glossary#attention-mask)\n \n             past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-                Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n-                shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n-                shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n+                It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n                 Contains pre-computed hidden-states (key and values in the self-attention blocks and in the\n                 cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n@@ -1469,7 +1467,7 @@ def forward(\n         decoder_input_ids: Optional[torch.Tensor] = None,\n         decoder_attention_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[tuple[torch.FloatTensor]] = None,\n-        past_key_values: Optional[tuple[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         decoder_inputs_embeds: Optional[torch.Tensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -1619,7 +1617,7 @@ def forward(\n         decoder_input_ids: Optional[torch.Tensor] = None,\n         decoder_attention_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[tuple[torch.FloatTensor]] = None,\n-        past_key_values: Optional[tuple[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         decoder_inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,"
        },
        {
            "sha": "074e91e14e88d29296e40f8a3f5c94663221997d",
            "filename": "src/transformers/models/perception_lm/modeling_perception_lm.py",
            "status": "modified",
            "additions": 6,
            "deletions": 7,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodeling_perception_lm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodeling_perception_lm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodeling_perception_lm.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -26,6 +26,7 @@\n import torch.nn.functional as F\n from torch import nn\n \n+from ...cache_utils import Cache\n from ...generation import GenerationMixin\n from ...modeling_outputs import BaseModelOutputWithPast, ModelOutput\n from ...modeling_utils import PreTrainedModel\n@@ -109,8 +110,7 @@ class PerceptionLMPreTrainedModel(PreTrainedModel):\n class PerceptionLMModelOutputWithPast(BaseModelOutputWithPast):\n     r\"\"\"\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n         `past_key_values` input) to speed up sequential decoding.\n@@ -140,8 +140,7 @@ class PerceptionLMCausalLMOutputWithPast(ModelOutput):\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n         `past_key_values` input) to speed up sequential decoding.\n@@ -155,7 +154,7 @@ class PerceptionLMCausalLMOutputWithPast(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[list[torch.FloatTensor]] = None\n+    past_key_values: Optional[Cache] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n     image_hidden_states: Optional[torch.FloatTensor] = None\n@@ -256,7 +255,7 @@ def forward(\n         pixel_values_videos: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -355,7 +354,7 @@ def forward(\n         pixel_values_videos: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "2e748c82a7bc77276e93678fe5ff6196d36bdef7",
            "filename": "src/transformers/models/perception_lm/modular_perception_lm.py",
            "status": "modified",
            "additions": 5,
            "deletions": 6,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodular_perception_lm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodular_perception_lm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodular_perception_lm.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -21,6 +21,7 @@\n import torch.utils.checkpoint\n from torch import nn\n \n+from ...cache_utils import Cache\n from ...utils import (\n     auto_docstring,\n     can_return_tuple,\n@@ -98,8 +99,7 @@ class PerceptionLMPreTrainedModel(LlavaPreTrainedModel):\n class PerceptionLMModelOutputWithPast(LlavaModelOutputWithPast):\n     r\"\"\"\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n         `past_key_values` input) to speed up sequential decoding.\n@@ -121,8 +121,7 @@ class PerceptionLMCausalLMOutputWithPast(LlavaCausalLMOutputWithPast):\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n         `past_key_values` input) to speed up sequential decoding.\n@@ -217,7 +216,7 @@ def forward(\n         pixel_values_videos: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -325,7 +324,7 @@ def forward(\n         pixel_values_videos: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "02ec819315b781d1e67478352d48b207a2add2fe",
            "filename": "src/transformers/models/persimmon/modeling_persimmon.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -322,7 +322,7 @@ def forward(\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -338,7 +338,7 @@ def forward(\n                 Indices of positions of each input sequence tokens in the position embeddings. Selected in the range\n                 `[0, config.n_positions - 1]`.\n                 [What are position IDs?](../glossary#position-ids)\n-            past_key_values (`Tuple(torch.FloatTensor)`, *optional*):\n+            past_key_values (`Cache`, *optional*):\n                 cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under"
        },
        {
            "sha": "165a2b887423d8eba9ff78376eb4a6e11108c095",
            "filename": "src/transformers/models/phi/modeling_phi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -221,7 +221,7 @@ def forward(\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "b7c9b9c926edc211267cdb119e18c3d8540e8b03",
            "filename": "src/transformers/models/phi/modular_phi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fphi%2Fmodular_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fphi%2Fmodular_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodular_phi.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -132,7 +132,7 @@ def forward(\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "d3d79847b073ff4733593d17516b9bb5ca91c001",
            "filename": "src/transformers/models/phimoe/modeling_phimoe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -822,7 +822,7 @@ def forward(\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         output_router_logits: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n@@ -835,7 +835,7 @@ def forward(\n             hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n             attention_mask (`torch.FloatTensor`, *optional*): attention mask of size\n                 `(batch, sequence_length)` where padding elements are indicated by 0.\n-            past_key_values (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            past_key_values (`Cache`, *optional*): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail."
        },
        {
            "sha": "60239bf9ac541d2446a90a87e3a90b800b7ce5bd",
            "filename": "src/transformers/models/plbart/modeling_plbart.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -752,7 +752,7 @@ def forward(\n                 `(encoder_attention_heads,)`.\n             cross_attn_layer_head_mask (`torch.FloatTensor`): mask for cross-attention heads in a given layer of\n                 size `(decoder_attention_heads,)`.\n-            past_key_values (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n+            past_key_values (`Cache`): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -904,9 +904,7 @@ def forward(\n                 - 0 indicates the head is **masked**.\n \n             past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-                Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n-                shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n-                shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n+                It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n                 Contains pre-computed hidden-states (key and values in the self-attention blocks and in the\n                 cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding."
        },
        {
            "sha": "d69bb7d9c802a0fa49ccb737e852e07e1fff431c",
            "filename": "src/transformers/models/prophetnet/modeling_prophetnet.py",
            "status": "modified",
            "additions": 17,
            "deletions": 21,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -129,9 +129,8 @@ class ProphetNetSeq2SeqLMOutput(ModelOutput):\n     logits_ngram (`torch.FloatTensor` of shape `(batch_size, ngram * decoder_sequence_length, config.vocab_size)`):\n         Prediction scores of the predict stream language modeling head (scores for each vocabulary token before\n         SoftMax).\n-    past_key_values (`list[torch.FloatTensor]`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        List of `torch.FloatTensor` of length `config.n_layers`, with each tensor of shape `(2, batch_size,\n-        num_attn_heads, decoder_sequence_length, embed_size_per_head)`).\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be\n         used (see `past_key_values` input) to speed up sequential decoding.\n@@ -154,7 +153,7 @@ class ProphetNetSeq2SeqLMOutput(ModelOutput):\n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n     logits_ngram: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[tuple[torch.FloatTensor]] = None\n+    past_key_values: Optional[Cache] = None\n     decoder_hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     decoder_ngram_hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     decoder_attentions: Optional[tuple[torch.FloatTensor]] = None\n@@ -190,9 +189,8 @@ class ProphetNetSeq2SeqModelOutput(ModelOutput):\n         hidden_size)` is output.\n     last_hidden_state_ngram (`torch.FloatTensor` of shape `(batch_size,ngram * decoder_sequence_length, config.vocab_size)`, *optional*):\n         Sequence of predict stream hidden-states at the output of the last layer of the decoder of the model.\n-    past_key_values (`list[torch.FloatTensor]`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        List of `torch.FloatTensor` of length `config.n_layers`, with each tensor of shape `(2, batch_size,\n-        num_attn_heads, decoder_sequence_length, embed_size_per_head)`).\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be\n         used (see `past_key_values` input) to speed up sequential decoding.\n@@ -214,7 +212,7 @@ class ProphetNetSeq2SeqModelOutput(ModelOutput):\n \n     last_hidden_state: torch.FloatTensor\n     last_hidden_state_ngram: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[tuple[torch.FloatTensor]] = None\n+    past_key_values: Optional[Cache] = None\n     decoder_hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     decoder_ngram_hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     decoder_attentions: Optional[tuple[torch.FloatTensor]] = None\n@@ -249,9 +247,8 @@ class ProphetNetDecoderModelOutput(ModelOutput):\n         hidden_size)` is output.\n     last_hidden_state_ngram (`torch.FloatTensor` of shape `(batch_size, ngram * decoder_sequence_length, config.vocab_size)`):\n         Sequence of predict stream hidden-states at the output of the last layer of the decoder of the model.\n-    past_key_values (`list[torch.FloatTensor]`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        List of `torch.FloatTensor` of length `config.n_layers`, with each tensor of shape `(2, batch_size,\n-        num_attn_heads, decoder_sequence_length, embed_size_per_head)`).\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be\n         used (see `past_key_values` input) to speed up sequential decoding.\n@@ -271,7 +268,7 @@ class ProphetNetDecoderModelOutput(ModelOutput):\n \n     last_hidden_state: torch.FloatTensor\n     last_hidden_state_ngram: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[tuple[torch.FloatTensor]] = None\n+    past_key_values: Optional[Cache] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     hidden_states_ngram: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n@@ -301,9 +298,8 @@ class ProphetNetDecoderLMOutput(ModelOutput):\n     logits_ngram (`torch.FloatTensor` of shape `(batch_size, ngram * decoder_sequence_length, config.vocab_size)`):\n         Prediction scores of the predict stream language modeling head (scores for each vocabulary token before\n         SoftMax).\n-    past_key_values (`list[torch.FloatTensor]`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        List of `torch.FloatTensor` of length `config.n_layers`, with each tensor of shape `(2, batch_size,\n-        num_attn_heads, decoder_sequence_length, embed_size_per_head)`).\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be\n         used (see `past_key_values` input) to speed up sequential decoding.\n@@ -324,7 +320,7 @@ class ProphetNetDecoderLMOutput(ModelOutput):\n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n     logits_ngram: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[tuple[torch.FloatTensor]] = None\n+    past_key_values: Optional[Cache] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     hidden_states_ngram: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n@@ -613,7 +609,7 @@ def prepare_for_onnx_export_(self):\n     def forward(\n         self,\n         hidden_states,\n-        past_key_values: Optional[tuple[Tensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask=None,\n         layer_head_mask=None,\n         extended_predict_attention_mask=None,\n@@ -1188,7 +1184,7 @@ def forward(\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.Tensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -1522,7 +1518,7 @@ def forward(\n         decoder_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[tuple] = None,\n-        past_key_values: Optional[tuple[tuple[torch.Tensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         decoder_inputs_embeds: Optional[torch.Tensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -1658,7 +1654,7 @@ def forward(\n         decoder_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.Tensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         decoder_inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n@@ -1862,7 +1858,7 @@ def forward(\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.Tensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "6b69ced26591e41af0f3ad99f6fd2b167f88e3c8",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 6,
            "deletions": 8,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -494,8 +494,7 @@ class Qwen2_5OmniThinkerCausalLMOutputWithPast(ModelOutput):\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`, *optional*):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n         `past_key_values` input) to speed up sequential decoding.\n@@ -505,7 +504,7 @@ class Qwen2_5OmniThinkerCausalLMOutputWithPast(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[list[torch.FloatTensor]] = None\n+    past_key_values: Optional[Cache] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n     rope_deltas: Optional[torch.LongTensor] = None\n@@ -1439,7 +1438,7 @@ def forward(\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -1457,7 +1456,7 @@ def forward(\n             use_cache (`bool`, *optional*):\n                 If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                 (see `past_key_values`).\n-            past_key_values (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            past_key_values (`Cache`, *optional*): cached past key and value projection states\n             cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             position_embeddings (`tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):\n@@ -2056,8 +2055,7 @@ class Qwen2_5OmniTalkerCausalLMOutputWithPast(ModelOutput):\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n         `past_key_values` input) to speed up sequential decoding.\n@@ -2070,7 +2068,7 @@ class Qwen2_5OmniTalkerCausalLMOutputWithPast(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[list[torch.FloatTensor]] = None\n+    past_key_values: Optional[Cache] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n     rope_deltas: Optional[torch.LongTensor] = None"
        },
        {
            "sha": "007f9834598869e3609dde33ed162f35a8d9a8a2",
            "filename": "src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py",
            "status": "modified",
            "additions": 4,
            "deletions": 6,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -1549,8 +1549,7 @@ class Qwen2_5OmniThinkerCausalLMOutputWithPast(ModelOutput):\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`, *optional*):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n         `past_key_values` input) to speed up sequential decoding.\n@@ -1560,7 +1559,7 @@ class Qwen2_5OmniThinkerCausalLMOutputWithPast(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[list[torch.FloatTensor]] = None\n+    past_key_values: Optional[Cache] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n     rope_deltas: Optional[torch.LongTensor] = None\n@@ -2504,8 +2503,7 @@ class Qwen2_5OmniTalkerCausalLMOutputWithPast(ModelOutput):\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n         `past_key_values` input) to speed up sequential decoding.\n@@ -2518,7 +2516,7 @@ class Qwen2_5OmniTalkerCausalLMOutputWithPast(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[list[torch.FloatTensor]] = None\n+    past_key_values: Optional[Cache] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n     rope_deltas: Optional[torch.LongTensor] = None"
        },
        {
            "sha": "6d05cc32f4a89b45c480f07c68234c28cdd072bb",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 6,
            "deletions": 8,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -473,8 +473,7 @@ def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor, **kwargs)\n class Qwen2_5_VLModelOutputWithPast(ModelOutput):\n     r\"\"\"\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n         `past_key_values` input) to speed up sequential decoding.\n@@ -483,7 +482,7 @@ class Qwen2_5_VLModelOutputWithPast(ModelOutput):\n     \"\"\"\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[list[torch.FloatTensor]] = None\n+    past_key_values: Optional[Cache] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n     rope_deltas: Optional[torch.LongTensor] = None\n@@ -705,7 +704,7 @@ def forward(\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -723,7 +722,7 @@ def forward(\n             use_cache (`bool`, *optional*):\n                 If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                 (see `past_key_values`).\n-            past_key_values (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            past_key_values (`Cache`, *optional*): cached past key and value projection states\n             cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             position_embeddings (`tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):\n@@ -1340,8 +1339,7 @@ class Qwen2_5_VLCausalLMOutputWithPast(ModelOutput):\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n         `past_key_values` input) to speed up sequential decoding.\n@@ -1351,7 +1349,7 @@ class Qwen2_5_VLCausalLMOutputWithPast(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[list[torch.FloatTensor]] = None\n+    past_key_values: Optional[Cache] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n     rope_deltas: Optional[torch.LongTensor] = None"
        },
        {
            "sha": "070eb6e89fd5a2a4ebacf2e4c8459a086b43d825",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -673,7 +673,7 @@ def forward(\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         output_router_logits: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n@@ -695,7 +695,7 @@ def forward(\n             use_cache (`bool`, *optional*):\n                 If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                 (see `past_key_values`).\n-            past_key_values (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            past_key_values (`Cache`, *optional*): cached past key and value projection states\n             cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             position_embeddings (`tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):"
        },
        {
            "sha": "0509ef9e085e57c2f67440478281a14afcb0ee14",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 6,
            "deletions": 8,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -64,8 +64,7 @@\n class Qwen2VLModelOutputWithPast(ModelOutput):\n     r\"\"\"\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n         `past_key_values` input) to speed up sequential decoding.\n@@ -74,7 +73,7 @@ class Qwen2VLModelOutputWithPast(ModelOutput):\n     \"\"\"\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[list[torch.FloatTensor]] = None\n+    past_key_values: Optional[Cache] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n     rope_deltas: Optional[torch.LongTensor] = None\n@@ -93,8 +92,7 @@ class Qwen2VLCausalLMOutputWithPast(ModelOutput):\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n         `past_key_values` input) to speed up sequential decoding.\n@@ -104,7 +102,7 @@ class Qwen2VLCausalLMOutputWithPast(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[list[torch.FloatTensor]] = None\n+    past_key_values: Optional[Cache] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n     rope_deltas: Optional[torch.LongTensor] = None\n@@ -567,7 +565,7 @@ def forward(\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -585,7 +583,7 @@ def forward(\n             use_cache (`bool`, *optional*):\n                 If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                 (see `past_key_values`).\n-            past_key_values (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            past_key_values (`Cache`, *optional*): cached past key and value projection states\n             cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             position_embeddings (`tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):"
        },
        {
            "sha": "2056e7c76a3a443caf2ad788bdc8f7ca0e4ac7f8",
            "filename": "src/transformers/models/qwen3_moe/modeling_qwen3_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -309,7 +309,7 @@ def forward(\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> torch.FloatTensor:\n@@ -327,7 +327,7 @@ def forward(\n             use_cache (`bool`, *optional*):\n                 If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                 (see `past_key_values`).\n-            past_key_values (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            past_key_values (`Cache`, *optional*): cached past key and value projection states\n             cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             position_embeddings (`tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):"
        },
        {
            "sha": "b9213a5e5bbbe083411a221b355c355690697e46",
            "filename": "src/transformers/models/qwen3_moe/modular_qwen3_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodular_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodular_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodular_qwen3_moe.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -147,7 +147,7 @@ def forward(\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> torch.FloatTensor:"
        },
        {
            "sha": "a05857a247b7fb89b51a99a7be7242b5252a4a96",
            "filename": "src/transformers/models/qwen3_next/modeling_qwen3_next.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodeling_qwen3_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodeling_qwen3_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodeling_qwen3_next.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -883,7 +883,7 @@ def forward(\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> torch.FloatTensor:\n@@ -901,7 +901,7 @@ def forward(\n             use_cache (`bool`, *optional*):\n                 If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                 (see `past_key_values`).\n-            past_key_values (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            past_key_values (`Cache`, *optional*): cached past key and value projection states\n             cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             position_embeddings (`tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):"
        },
        {
            "sha": "bc902b8e1b5ff545f069fbf7f30dec65cfccc0fb",
            "filename": "src/transformers/models/qwen3_next/modular_qwen3_next.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodular_qwen3_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodular_qwen3_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodular_qwen3_next.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -647,7 +647,7 @@ def forward(\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> torch.FloatTensor:"
        },
        {
            "sha": "13389107a2cb1f1c42c62da83facf08a38d795fb",
            "filename": "src/transformers/models/rag/modeling_rag.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -51,8 +51,7 @@ class RetrievAugLMMarginOutput(ModelOutput):\n         Score between each retrieved document embeddings (see `retrieved_doc_embeds`) and\n         `question_encoder_last_hidden_state`.\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        List of `torch.FloatTensor` of length `config.n_layers`, with each tensor of shape `(2, batch_size,\n-        num_heads, sequence_length, embed_size_per_head)`).\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains precomputed hidden-states (key and values in the attention blocks) of the decoder that can be used\n         (see `past_key_values` input) to speed up sequential decoding.\n@@ -142,8 +141,7 @@ class RetrievAugLMOutput(ModelOutput):\n         Score between each retrieved document embeddings (see `retrieved_doc_embeds`) and\n         `question_encoder_last_hidden_state`.\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        List of `torch.FloatTensor` of length `config.n_layers`, with each tensor of shape `(2, batch_size,\n-        num_heads, sequence_length, embed_size_per_head)`).\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains precomputed hidden-states (key and values in the attention blocks) of the decoder that can be used\n         (see `past_key_values` input) to speed up sequential decoding."
        },
        {
            "sha": "a9ee455116c49cd492562891df518518a4cc17f7",
            "filename": "src/transformers/models/rembert/modeling_rembert.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -492,7 +492,7 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: bool = False,\n         output_hidden_states: bool = False,\n@@ -690,7 +690,7 @@ def forward(\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -934,7 +934,7 @@ def forward(\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,"
        },
        {
            "sha": "6999dddf1b1ac51bf3ca124da102863d0a980c63",
            "filename": "src/transformers/models/roberta/modeling_roberta.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -569,7 +569,7 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,\n@@ -749,7 +749,7 @@ def forward(\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -927,7 +927,7 @@ def forward(\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,"
        },
        {
            "sha": "072466bd8b041bc8a1d1cccd032875c5f085127f",
            "filename": "src/transformers/models/roberta_prelayernorm/modeling_roberta_prelayernorm.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -459,7 +459,7 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,\n@@ -636,7 +636,7 @@ def forward(\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -795,7 +795,7 @@ def forward(\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,"
        },
        {
            "sha": "e0b8b4b434baf26f8824f7d7ace43209f41c3db6",
            "filename": "src/transformers/models/roc_bert/modeling_roc_bert.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -578,7 +578,7 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,\n@@ -822,7 +822,7 @@ def forward(\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -1352,7 +1352,7 @@ def forward(\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[list[torch.Tensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         labels: Optional[torch.Tensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,"
        },
        {
            "sha": "56dce4cb753bc8446e4789be57f1b7aca2477d1c",
            "filename": "src/transformers/models/roformer/modeling_roformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_roformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_roformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_roformer.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -838,7 +838,7 @@ def forward(\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -1071,7 +1071,7 @@ def forward(\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,"
        },
        {
            "sha": "15f3682817750de4543bcdff5b3f73b713d41475",
            "filename": "src/transformers/models/seamless_m4t/modeling_seamless_m4t.py",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -1285,7 +1285,7 @@ def forward(\n             encoder_attention_mask (`torch.FloatTensor`):\n                 encoder attention mask of size `(batch, 1, tgt_len, src_len)` where padding elements are indicated by\n                 very large negative values.\n-            past_key_values (`Tuple(torch.FloatTensor)`):\n+            past_key_values (`Cache`):\n                 cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n@@ -1762,7 +1762,7 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -1912,7 +1912,7 @@ def forward(\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n         encoder_outputs: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -2032,7 +2032,7 @@ def forward(\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n         encoder_outputs: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n@@ -2507,7 +2507,7 @@ def forward(\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n         encoder_outputs: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n@@ -2759,7 +2759,7 @@ def forward(\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n         encoder_outputs: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n@@ -3028,7 +3028,7 @@ def forward(\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n         encoder_outputs: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n@@ -3346,7 +3346,7 @@ def forward(\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n         encoder_outputs: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n@@ -3700,7 +3700,7 @@ def forward(\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n         encoder_outputs: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "ccad4450451d17683939ae50581db3bae0f32f6b",
            "filename": "src/transformers/models/seamless_m4t_v2/modeling_seamless_m4t_v2.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -1119,7 +1119,7 @@ def forward(\n             encoder_attention_mask (`torch.FloatTensor`):\n                 encoder attention mask of size `(batch, 1, tgt_len, src_len)` where padding elements are indicated by\n                 very large negative values.\n-            past_key_values (`Tuple(torch.FloatTensor)`):\n+            past_key_values (`Cache`):\n                 cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n@@ -1805,7 +1805,7 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -2715,7 +2715,7 @@ def forward(\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n         encoder_outputs: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n@@ -2974,7 +2974,7 @@ def forward(\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n         encoder_outputs: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n@@ -3251,7 +3251,7 @@ def forward(\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n         encoder_outputs: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n@@ -3607,7 +3607,7 @@ def forward(\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n         encoder_outputs: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n@@ -3998,7 +3998,7 @@ def forward(\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n         encoder_outputs: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "49261f039a569105cfb0bf0a2f2664658166d4e2",
            "filename": "src/transformers/models/shieldgemma2/modeling_shieldgemma2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fshieldgemma2%2Fmodeling_shieldgemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fshieldgemma2%2Fmodeling_shieldgemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fshieldgemma2%2Fmodeling_shieldgemma2.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -86,7 +86,7 @@ def forward(\n         pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[list[torch.FloatTensor], Cache]] = None,\n+        past_key_values: Optional[Cache] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,"
        },
        {
            "sha": "d536c52e12cf0036d7cfb46c176fb75ab89b1a6d",
            "filename": "src/transformers/models/smolvlm/modeling_smolvlm.py",
            "status": "modified",
            "additions": 6,
            "deletions": 8,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -419,10 +419,8 @@ class SmolVLMBaseModelOutputWithPast(ModelOutput):\n         If `past_key_values` is used only the last hidden-state of the sequences of shape `(batch_size, 1,\n         hidden_size)` is output.\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if\n-        `config.is_encoder_decoder=True` 2 additional tensors of shape `(batch_size, num_heads,\n-        encoder_sequence_length, embed_size_per_head)`.\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n+\n         Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if\n         `config.is_encoder_decoder=True` in the cross-attention blocks) that can be used (see `past_key_values`\n         input) to speed up sequential decoding.\n@@ -433,7 +431,7 @@ class SmolVLMBaseModelOutputWithPast(ModelOutput):\n     \"\"\"\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None\n+    past_key_values: Optional[Cache] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n     image_hidden_states: Optional[tuple[torch.FloatTensor]] = None\n@@ -741,8 +739,8 @@ class SmolVLMCausalLMOutputWithPast(ModelOutput):\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n+\n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n         `past_key_values` input) to speed up sequential decoding.\n     image_hidden_states (`tuple(torch.FloatTensor)`, *optional*):\n@@ -753,7 +751,7 @@ class SmolVLMCausalLMOutputWithPast(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[list[torch.FloatTensor]] = None\n+    past_key_values: Optional[Cache] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n     image_hidden_states: Optional[tuple[torch.FloatTensor]] = None"
        },
        {
            "sha": "272ebdc741bc6bfee509266b8203ccd57a81d960",
            "filename": "src/transformers/models/speech_encoder_decoder/modeling_speech_encoder_decoder.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fmodeling_speech_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fmodeling_speech_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fmodeling_speech_encoder_decoder.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -20,6 +20,7 @@\n from torch import nn\n from torch.nn import CrossEntropyLoss\n \n+from ...cache_utils import Cache\n from ...configuration_utils import PretrainedConfig\n from ...generation import GenerationMixin\n from ...modeling_outputs import BaseModelOutput, Seq2SeqLMOutput\n@@ -322,7 +323,7 @@ def forward(\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.BoolTensor] = None,\n         encoder_outputs: Optional[tuple[torch.FloatTensor]] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "8cfc9926604169a371cfde0cd47beb415089d895",
            "filename": "src/transformers/models/speech_to_text/modeling_speech_to_text.py",
            "status": "modified",
            "additions": 5,
            "deletions": 7,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -454,7 +454,7 @@ def forward(\n                 `(encoder_attention_heads,)`.\n             cross_attn_layer_head_mask (`torch.FloatTensor`): mask for cross-attention heads in a given layer of\n                 size `(decoder_attention_heads,)`.\n-            past_key_values (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n+            past_key_values (`Cache`): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -822,10 +822,8 @@ def forward(\n                 - 1 indicates the head is **not masked**,\n                 - 0 indicates the head is **masked**.\n \n-            past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-                Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n-                shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n-                shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n+            past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+                It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n                 Contains pre-computed hidden-states (key and values in the self-attention blocks and in the\n                 cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n@@ -1072,7 +1070,7 @@ def forward(\n         decoder_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -1223,7 +1221,7 @@ def forward(\n         decoder_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "026585cdd77bf10108cd66f68b3fe54beac65fa6",
            "filename": "src/transformers/models/speecht5/modeling_speecht5.py",
            "status": "modified",
            "additions": 11,
            "deletions": 13,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -1145,7 +1145,7 @@ def forward(\n                 `(encoder_attention_heads,)`.\n             cross_attn_layer_head_mask (`torch.FloatTensor`): mask for cross-attention heads in a given layer of\n                 size `(decoder_attention_heads,)`.\n-            past_key_values (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n+            past_key_values (`Cache`): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -1501,7 +1501,7 @@ def forward(\n         encoder_attention_mask: Optional[torch.LongTensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -1543,10 +1543,8 @@ def forward(\n                 - 1 indicates the head is **not masked**,\n                 - 0 indicates the head is **masked**.\n \n-            past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-                Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n-                shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n-                shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n+            past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+                It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n                 Contains pre-computed hidden-states (key and values in the self-attention blocks and in the\n                 cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n@@ -1695,7 +1693,7 @@ def forward(\n         speaker_embeddings: Optional[torch.Tensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -1749,7 +1747,7 @@ def forward(\n         encoder_attention_mask: Optional[torch.LongTensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -1797,7 +1795,7 @@ def forward(\n         encoder_attention_mask: Optional[torch.LongTensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -2002,7 +2000,7 @@ def forward(\n         decoder_head_mask: Optional[torch.FloatTensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         speaker_embeddings: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n@@ -2160,7 +2158,7 @@ def forward(\n         decoder_head_mask: Optional[torch.FloatTensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -2484,7 +2482,7 @@ def forward(\n         decoder_head_mask: Optional[torch.FloatTensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -2839,7 +2837,7 @@ def forward(\n         decoder_head_mask: Optional[torch.FloatTensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,"
        },
        {
            "sha": "9daefe0a39df14e24442553223e1e316ef0cd467",
            "filename": "src/transformers/models/stablelm/modeling_stablelm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -546,7 +546,7 @@ def forward(\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -562,7 +562,7 @@ def forward(\n                 `[0, config.n_positions - 1]`.\n \n                 [What are position IDs?](../glossary#position-ids)\n-            past_key_values (`Tuple(torch.FloatTensor)`, *optional*):\n+            past_key_values (`Cache`, *optional*):\n                 cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under"
        },
        {
            "sha": "9810eae30d5e57b011d2f561cddb7498eb2c9223",
            "filename": "src/transformers/models/time_series_transformer/modeling_time_series_transformer.py",
            "status": "modified",
            "additions": 6,
            "deletions": 8,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -570,7 +570,7 @@ def forward(\n                 `(encoder_attention_heads,)`.\n             cross_attn_layer_head_mask (`torch.FloatTensor`): mask for cross-attention heads in a given layer of\n                 size `(decoder_attention_heads,)`.\n-            past_key_values (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n+            past_key_values (`Cache`): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -917,7 +917,7 @@ def forward(\n         encoder_attention_mask: Optional[torch.LongTensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -958,10 +958,8 @@ def forward(\n                 - 1 indicates the head is **not masked**,\n                 - 0 indicates the head is **masked**.\n \n-            past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-                Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n-                shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n-                shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n+            past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+                It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n                 Contains pre-computed hidden-states (key and values in the self-attention blocks and in the\n                 cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n@@ -1254,7 +1252,7 @@ def forward(\n         decoder_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[list[torch.FloatTensor]] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_hidden_states: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         use_cache: Optional[bool] = None,\n@@ -1516,7 +1514,7 @@ def forward(\n         decoder_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[list[torch.FloatTensor]] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_hidden_states: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "70cded0a514774472ef80794595fc337a1056067",
            "filename": "src/transformers/models/trocr/modeling_trocr.py",
            "status": "modified",
            "additions": 4,
            "deletions": 6,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Ftrocr%2Fmodeling_trocr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Ftrocr%2Fmodeling_trocr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftrocr%2Fmodeling_trocr.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -362,7 +362,7 @@ def forward(\n                 `(encoder_attention_heads,)`.\n             cross_attn_layer_head_mask (`torch.FloatTensor`): mask for cross-attention heads in a given layer of\n                 size *(decoder_attention_heads,)*.\n-            past_key_values (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n+            past_key_values (`Cache`): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -535,10 +535,8 @@ def forward(\n                 - 1 indicates the head is **not masked**,\n                 - 0 indicates the head is **masked**.\n \n-            past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-                Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n-                shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n-                shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n+            past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+                It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n                 Contains pre-computed hidden-states (key and values in the self-attention blocks and in the\n                 cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n@@ -752,7 +750,7 @@ def forward(\n         encoder_attention_mask: Optional[torch.LongTensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "668ec6bfec3b6f81303774586024a5916b46235e",
            "filename": "src/transformers/models/udop/modeling_udop.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -77,10 +77,9 @@ class BaseModelOutputWithAttentionMask(ModelOutput):\n         - 1 for tokens that are **not masked**,\n         - 0 for tokens that are **masked**.\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if\n-        `config.is_encoder_decoder=True` 2 additional tensors of shape `(batch_size, num_heads,\n-        encoder_sequence_length, embed_size_per_head)`. Contains pre-computed hidden-states (key and values in the\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n+\n+        Contains pre-computed hidden-states (key and values in the\n         self-attention blocks and optionally if `config.is_encoder_decoder=True` in the cross-attention blocks)\n         that can be used (see `past_key_values` input) to speed up sequential decoding.\n     hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):"
        },
        {
            "sha": "a8c592b727c6c79249c58be68524f1bf7d576418",
            "filename": "src/transformers/models/umt5/modeling_umt5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -263,7 +263,7 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         cache_position: Optional[torch.Tensor] = None,"
        },
        {
            "sha": "41a4d0abed1735b89f6e0871e3e41ac4e1759c07",
            "filename": "src/transformers/models/video_llava/modeling_video_llava.py",
            "status": "modified",
            "additions": 4,
            "deletions": 6,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -45,8 +45,7 @@\n class VideoLlavaModelOutputWithPast(ModelOutput):\n     r\"\"\"\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n         `past_key_values` input) to speed up sequential decoding.\n@@ -59,7 +58,7 @@ class VideoLlavaModelOutputWithPast(ModelOutput):\n     \"\"\"\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[list[torch.FloatTensor]] = None\n+    past_key_values: Optional[Cache] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n     image_hidden_states: Optional[torch.FloatTensor] = None\n@@ -79,8 +78,7 @@ class VideoLlavaCausalLMOutputWithPast(ModelOutput):\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n         `past_key_values` input) to speed up sequential decoding.\n@@ -94,7 +92,7 @@ class VideoLlavaCausalLMOutputWithPast(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[list[torch.FloatTensor]] = None\n+    past_key_values: Optional[Cache] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n     image_hidden_states: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "f9a376120d6303dfd84cc5fe11a2286bf98af8a3",
            "filename": "src/transformers/models/vipllava/modeling_vipllava.py",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -44,8 +44,7 @@\n class VipLlavaModelOutputWithPast(BaseModelOutputWithPast):\n     r\"\"\"\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n         `past_key_values` input) to speed up sequential decoding.\n@@ -70,8 +69,7 @@ class VipLlavaCausalLMOutputWithPast(ModelOutput):\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n         `past_key_values` input) to speed up sequential decoding.\n@@ -82,7 +80,7 @@ class VipLlavaCausalLMOutputWithPast(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[list[torch.FloatTensor]] = None\n+    past_key_values: Optional[Cache] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n     image_hidden_states: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "d6bc2dcc0f8e94e0c79c01e9f47ab2e34aedb3cd",
            "filename": "src/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_vision_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_vision_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_vision_encoder_decoder.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -22,6 +22,7 @@\n import torch\n from torch import nn\n \n+from ...cache_utils import Cache\n from ...configuration_utils import PretrainedConfig\n from ...generation import GenerationMixin\n from ...modeling_outputs import BaseModelOutput, Seq2SeqLMOutput\n@@ -440,7 +441,7 @@ def forward(\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.BoolTensor] = None,\n         encoder_outputs: Optional[tuple[torch.FloatTensor]] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "da30a332d7495b17756d077816f3b0080892831a",
            "filename": "src/transformers/models/whisper/modeling_whisper.py",
            "status": "modified",
            "additions": 2,
            "deletions": 11,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -492,7 +492,7 @@ def forward(\n                 `(encoder_attention_heads,)`.\n             cross_attn_layer_head_mask (`torch.FloatTensor`): mask for cross-attention heads in a given layer of\n                 size `(decoder_attention_heads,)`.\n-            past_key_values (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n+            past_key_values (`Cache`): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -813,16 +813,7 @@ def forward(\n                 - 0 indicates the head is **masked**.\n \n             past_key_values (`EncoderDecoderCache` or `tuple(tuple(torch.FloatTensor))`, *optional*):\n-                Pre-computed hidden-states that can be used to speed up auto-regressive (sequential) decoding. There are\n-                four sets of pre-computed hidden-states: key and values states in the self-attention blocks (2) and\n-                in the cross-attention blocks (2). The `past_key_values` are returned when `use_cache=True` is passed or\n-                when `config.use_cache=True`\n-\n-                Two formats are allowed:\n-                - An [`~cache_utils.EncoderDecoderCache`] instance;\n-                - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n-                shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n-                `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n+                It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n                 If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those\n                 that don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of"
        },
        {
            "sha": "cfa42502399b9718b56079f4f2778c2ca9497455",
            "filename": "src/transformers/models/xglm/modeling_xglm.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -322,7 +322,7 @@ def forward(\n                 `(encoder_attention_heads,)`.\n             cross_attn_layer_head_mask (`torch.FloatTensor`): mask for cross-attention heads in a given layer of\n                 size `(decoder_attention_heads,)`.\n-            past_key_values (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n+            past_key_values (`Cache`): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -439,7 +439,7 @@ def forward(\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -626,7 +626,7 @@ def forward(\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "a398ff0b916ce955fdf3977ebbd4ea6109a6529d",
            "filename": "src/transformers/models/xlm_roberta/modeling_xlm_roberta.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -570,7 +570,7 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,\n@@ -740,7 +740,7 @@ def forward(\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -919,7 +919,7 @@ def forward(\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,"
        },
        {
            "sha": "067f58ab93c631fafbcb0df0951929b567eb206f",
            "filename": "src/transformers/models/xlm_roberta_xl/modeling_xlm_roberta_xl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -732,7 +732,7 @@ def forward(\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -910,7 +910,7 @@ def forward(\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,"
        },
        {
            "sha": "bb1ba68d4624130c8889e87caa6753f0d848cbee",
            "filename": "src/transformers/models/xmod/modeling_xmod.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fxmod%2Fmodeling_xmod.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93f810e6fa38fd4c1c976f6d82051a3f82358aa7/src%2Ftransformers%2Fmodels%2Fxmod%2Fmodeling_xmod.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxmod%2Fmodeling_xmod.py?ref=93f810e6fa38fd4c1c976f6d82051a3f82358aa7",
            "patch": "@@ -521,7 +521,7 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,\n@@ -732,7 +732,7 @@ def forward(\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -895,7 +895,7 @@ def forward(\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,"
        }
    ],
    "stats": {
        "total": 1402,
        "additions": 613,
        "deletions": 789
    }
}