{
    "author": "RyanMullins",
    "message": "docs: improved RoPE function Docstrings (#41004)\n\n* docs: improved RoPE functuon docstrings\n\n* Update src/transformers/modeling_rope_utils.py\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\n\n---------\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>",
    "sha": "ae9ef2e1510eff1e9a620cad9648d0cca5c547cb",
    "files": [
        {
            "sha": "c0070df6ee17ae08c4ddcd6294379bf207867e03",
            "filename": "src/transformers/modeling_rope_utils.py",
            "status": "modified",
            "additions": 155,
            "deletions": 16,
            "changes": 171,
            "blob_url": "https://github.com/huggingface/transformers/blob/ae9ef2e1510eff1e9a620cad9648d0cca5c547cb/src%2Ftransformers%2Fmodeling_rope_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ae9ef2e1510eff1e9a620cad9648d0cca5c547cb/src%2Ftransformers%2Fmodeling_rope_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_rope_utils.py?ref=ae9ef2e1510eff1e9a620cad9648d0cca5c547cb",
            "patch": "@@ -98,17 +98,30 @@ def _compute_default_rope_parameters(\n     Computes the inverse frequencies according to the original RoPE implementation\n     Args:\n         config ([`~transformers.PretrainedConfig`]):\n-            The model configuration.\n+            The model configuration. This function assumes that the config will provide at least the following\n+            properties:\n+\n+            *   rope_theta (`float`): The base wavelength from which the inverse frequencies will be derived.\n+            *   hidden_size (`int`): The numerator when deriving a head_dim, if not provided directly.\n+            *   num_attention_heads (`int`): The denominator when deriving a head_dim, if not provided directly.\n+\n+            Additionally, this function will make use of the following properties if they are found in the config:\n+\n+            *   head_dim (`int`, *optional*): The size of the key-value heads in the model. If None, this value will be\n+                derived as hidden_size // num_attention_heads.\n+            *   partial_rotary_factor (`float`, *optional*): If less than 1.0, inverse frequencies will be returned for\n+                the first fraction of the head_dim. Defaults to 1.0.\n         device (`torch.device`):\n             The device to use for initialization of the inverse frequencies.\n         seq_len (`int`, *optional*):\n             The current sequence length. Unused for this type of RoPE.\n+\n     Returns:\n         Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n         post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n     \"\"\"\n     base = config.rope_theta\n-    partial_rotary_factor = config.partial_rotary_factor if hasattr(config, \"partial_rotary_factor\") else 1.0\n+    partial_rotary_factor = getattr(config, \"partial_rotary_factor\", 1.0)\n     head_dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n     dim = int(head_dim * partial_rotary_factor)\n \n@@ -128,11 +141,24 @@ def _compute_linear_scaling_rope_parameters(\n     Computes the inverse frequencies with linear scaling. Credits to the Reddit user /u/kaiokendev\n     Args:\n         config ([`~transformers.PretrainedConfig`]):\n-            The model configuration.\n+            The model configuration. This function assumes that the config will provide at least the following\n+            properties:\n+\n+            *   rope_theta (`float`): The base wavelength from which the inverse frequencies will be derived.\n+            *   hidden_size (`int`): The numerator when deriving a head_dim, if not provided directly.\n+            *   num_attention_heads (`int`): The denominator when deriving a head_dim, if not provided directly.\n+\n+            Additionally, this function will make use of the following properties if they are found in the config:\n+\n+            *   head_dim (`int`, *optional*): The size of the key-value heads in the model. If None, this value will be\n+                derived as hidden_size // num_attention_heads.\n+            *   partial_rotary_factor (`float`, *optional*): If less than 1.0, inverse frequencies will be returned for\n+                the first fraction of the head_dim. Defaults to 1.0.\n         device (`torch.device`):\n             The device to use for initialization of the inverse frequencies.\n         seq_len (`int`, *optional*):\n             The current sequence length. Unused for this type of RoPE.\n+\n     Returns:\n         Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n         post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n@@ -156,20 +182,43 @@ def _compute_dynamic_ntk_parameters(\n ) -> tuple[\"torch.Tensor\", float]:\n     \"\"\"\n     Computes the inverse frequencies with NTK scaling. Credits to the Reddit users /u/bloc97 and /u/emozilla\n+\n     Args:\n         config ([`~transformers.PretrainedConfig`]):\n-            The model configuration.\n+            The model configuration. This function assumes that the config will provide at least the following\n+            properties:\n+\n+            *   rope_theta (`float`): The base wavelength from which the inverse frequencies will be derived.\n+            *   hidden_size (`int`): The numerator when deriving a head_dim, if not provided directly.\n+            *   num_attention_heads (`int`): The denominator when deriving a head_dim, if not provided directly.\n+            *   max_position_embeddings (`int`): The default sequence length used to update the dynamic RoPE at\n+                inference time\n+            *   rope_scaling (`dict[str, float]`): The standard RoPE scaling parameters, from which `factor`\n+                will be accessed. The value of `factor` is used to determine the new base frequency, along with the\n+                current sequence length (seq_len), the maximum positional embeddings (max_position_embeddings), and the\n+                computed dimensionality (dim) of the rotary embeddings. If seq_len <= max_position_embeddings, this\n+                factor has no effect. If seq_len <= max_position_embeddings, this factor effectively stretches the\n+                context window using an exponent derived from `dim`.\n+\n+            Additionally, this function will make use of the following properties if they are found in the config:\n+\n+            *   head_dim (`int`, *optional*): The size of the key-value heads in the model. If None, this value will be\n+                derived as hidden_size // num_attention_heads.\n+            *   partial_rotary_factor (`float`, *optional*): If less than 1.0, inverse frequencies will be returned for\n+                the first fraction of the head_dim. Defaults to 1.0.\n         device (`torch.device`):\n             The device to use for initialization of the inverse frequencies.\n         seq_len (`int`, *optional*):\n-            The current sequence length, used to update the dynamic RoPE at inference time.\n+            The current sequence length, used to update the dynamic RoPE at inference time. If `None` or shorter than\n+            max_position_embeddings, this value will be overridden by max_position_embeddings.\n+\n     Returns:\n         Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n         post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n     \"\"\"\n     # TODO (joao): use the new `original_max_position_embeddings` from rope_scaling\n     base = config.rope_theta\n-    partial_rotary_factor = config.partial_rotary_factor if hasattr(config, \"partial_rotary_factor\") else 1.0\n+    partial_rotary_factor = getattr(config, \"partial_rotary_factor\", 1.0)\n     head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n     dim = int(head_dim * partial_rotary_factor)\n     max_position_embeddings = config.max_position_embeddings\n@@ -200,20 +249,58 @@ def _compute_yarn_parameters(\n     \"\"\"\n     Computes the inverse frequencies with NTK scaling. Please refer to the\n     [original paper](https://huggingface.co/papers/2309.00071)\n+\n     Args:\n         config ([`~transformers.PretrainedConfig`]):\n-            The model configuration.\n+            The model configuration. This function assumes that the config will provide at least the following\n+            properties:\n+\n+            *   rope_theta (`float`): The base wavelength from which the inverse frequencies will be derived.\n+            *   hidden_size (`int`): The numerator when deriving a head_dim, if not provided directly.\n+            *   num_attention_heads (`int`): The denominator when deriving a head_dim, if not provided directly.\n+            *   max_position_embeddings (`int`): The maximum length of the positional embeddings.\n+            *   rope_scaling (`dict[str, float | int]`): The standard RoPE scaling parameters, from which the following\n+                keys will be accessed:\n+                *   `attention_factor` (`float`, *optional*): The scaling factor to be applied to the computed cos/sin.\n+                    If None, the value is inferred from `factor`, `mscale`, and `mscale_all_dim` as avaialble.\n+                *   `beta_fast` (`float`, *optional*, defaults to 32): Parameter to set the boundary for extrapolation\n+                    (only) in the linear ramp function.\n+                *   `beta_slow` (`float`, *optional*, defaults to 1): Parameter to set the boundary for interpolation\n+                    (only) in the linear ramp function.\n+                *   `factor` (`float`, *optional*): The scaling factor applied when interpolating the position IDs to\n+                    extend the possible context length. Additionally, if `attention_factor` is None, the log of this\n+                    value is used to compute a value for `attention_factor`, possibly in conjunciton with `mscale` and\n+                    `mscale_all_dim`, if provided.\n+                *   `mscale` (`float`, *optional*): If `attention_factor` is None and both `mscale` and\n+                    `mscale_all_dim` are provided, `mscale` acts scalar augmenting `log(factor)` when computing the\n+                    numerator for the inferred value of `attention_factor`. If not provided, `attention_factor` will be\n+                    calculated based on `factor` only.\n+                *   `mscale_all_dim` (`float`, *optional*): If `attention_factor` is None and both `mscale` and\n+                    `mscale_all_dim` are provided, `mscale_all_dim` acts scalar augmenting `log(factor)` when computing\n+                    the denominator for the inferred value of `attention_factor`. If not provided, `attention_factor`\n+                    will be calculated based on `factor` only.\n+                *   `original_max_position_embeddings` (`int`, *optional*): The original max position embeddings used\n+                    during pretraining. If not provided, the function falls back to `max_position_embeddings`.\n+                *   `truncate` (`bool`, *optional*): Whether to truncate the correction range.\n+\n+            Additionally, this function will make use of the following properties if they are found in the config:\n+\n+            *   head_dim (`int`, *optional*): The size of the key-value heads in the model. If None, this value will be\n+                derived as hidden_size // num_attention_heads.\n+            *   partial_rotary_factor (`float`, *optional*, defaults to 1.0): If less than 1.0, inverse frequencies\n+                will be returned for the first fraction of the head_dim.\n         device (`torch.device`):\n             The device to use for initialization of the inverse frequencies.\n         seq_len (`int`, *optional*):\n             The current sequence length. Unused for this type of RoPE.\n+\n     Returns:\n         Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n         post-processing scaling factor applied to the computed cos/sin.\n     \"\"\"\n \n     base = config.rope_theta\n-    partial_rotary_factor = config.partial_rotary_factor if hasattr(config, \"partial_rotary_factor\") else 1.0\n+    partial_rotary_factor = getattr(config, \"partial_rotary_factor\", 1.0)\n     head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n     dim = int(head_dim * partial_rotary_factor)\n     factor = config.rope_scaling[\"factor\"]\n@@ -237,7 +324,7 @@ def get_mscale(scale, mscale=1):\n             attention_factor = get_mscale(factor)\n \n     # Optional config options\n-    # beta_fast/beta_slow: as suggested in the paper, default to 32/1 (correspondingly)\n+    # beta_fast/beta_slow: as suggested in the paper, default to 32 and 1 respectively\n     beta_fast = config.rope_scaling.get(\"beta_fast\") or 32\n     beta_slow = config.rope_scaling.get(\"beta_slow\") or 1\n \n@@ -287,20 +374,49 @@ def _compute_longrope_parameters(\n     \"\"\"\n     Computes the inverse frequencies with LongRoPE scaling. Please refer to the\n     [original implementation](https://github.com/microsoft/LongRoPE)\n+\n     Args:\n         config ([`~transformers.PretrainedConfig`]):\n-            The model configuration.\n+            The model configuration. This function assumes that the config will provide at least the following\n+            properties:\n+\n+            *   rope_theta (`float`): The base wavelength from which the inverse frequencies will be derived.\n+            *   hidden_size (`int`): The numerator when deriving a head_dim, if not provided directly.\n+            *   num_attention_heads (`int`): The denominator when deriving a head_dim, if not provided directly.\n+            *   max_position_embeddings (`int`): The maximum length of the positional embeddings.\n+            *   original_max_position_embeddings (`int`, *optional*): The original max position embeddings used during\n+                pretraining. If not provided, defaults to `max_position_embeddings`.\n+            *   rope_scaling (`dict[str, float]`): The standard RoPE scaling parameters, from which the following keys\n+                will be accessed:\n+                *   `attention_factor` (`float`, *optional*): The scaling factor to be applied on the attention\n+                    computation. If unspecified, it defaults to value recommended by the implementation, inferred from\n+                    the value of `factor`.\n+                *   `factor` (`float`, *optional*): The scaling factor to apply to the RoPE embeddings. If both\n+                    `max_position_embeddings` and `original_max_position_embeddings` are provided, this value will be\n+                    overridden s the ratio between those values.\n+                *   `long_factor` (`float`, *optional*): The scale factor applied when computing the inverse\n+                    frequencies if `seq_len` is provided and greater than `original_max_position_embeddings`.\n+                *   `short_factor` (`float`, *optional*): The scale factor applied when computing the inverse\n+                    frequencies if `seq_len` is None or less-than-or-equal-to `original_max_position_embeddings`.\n+\n+            Additionally, this function will make use of the following properties if they are found in the config:\n+\n+            *   head_dim (`int`, *optional*): The size of the key-value heads in the model. If None, this value will be\n+                derived as hidden_size // num_attention_heads.\n+            *   partial_rotary_factor (`float`, *optional*, defaults to 1.0): If less than 1.0, inverse frequencies\n+                will be returned for the first fraction of the head_dim.\n         device (`torch.device`):\n             The device to use for initialization of the inverse frequencies.\n         seq_len (`int`, *optional*):\n             The current sequence length.\n+\n     Returns:\n         Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n         post-processing scaling factor applied to the computed cos/sin.\n     \"\"\"\n     # TODO (joao): use the new `original_max_position_embeddings` from rope_scaling\n     base = config.rope_theta\n-    partial_rotary_factor = config.partial_rotary_factor if hasattr(config, \"partial_rotary_factor\") else 1.0\n+    partial_rotary_factor = getattr(config, \"partial_rotary_factor\", 1.0)\n     head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n     dim = int(head_dim * partial_rotary_factor)\n     long_factor = config.rope_scaling[\"long_factor\"]\n@@ -311,9 +427,8 @@ def _compute_longrope_parameters(\n     # NOTE: Phi3 (and potentially other models) modify `max_position_embeddings` and have a\n     # `original_max_position_embeddings` field containing the pretrained value. They use the ratio between these two\n     # values to compute the default attention scaling factor, instead of using `factor`.\n-    if hasattr(config, \"original_max_position_embeddings\"):\n-        original_max_position_embeddings = config.original_max_position_embeddings\n-        factor = config.max_position_embeddings / config.original_max_position_embeddings\n+    if original_max_position_embeddings := getattr(config, \"original_max_position_embeddings\", None):\n+        factor = config.max_position_embeddings / original_max_position_embeddings\n     else:\n         original_max_position_embeddings = config.max_position_embeddings\n \n@@ -343,7 +458,31 @@ def _compute_llama3_parameters(\n \n     Args:\n         config ([`~transformers.PretrainedConfig`]):\n-            The model configuration.\n+            The model configuration. This function assumes that the config will provide at least the following\n+            properties:\n+\n+            *   rope_theta (`float`): The base wavelength from which the inverse frequencies will be derived.\n+            *   hidden_size (`int`): The numerator when deriving a head_dim, if not provided directly.\n+            *   num_attention_heads (`int`): The denominator when deriving a head_dim, if not provided directly.\n+            *   rope_scaling (`dict[str, float | int]`): The standard RoPE scaling parameters, from which the following\n+                keys will be accessed:\n+                *   `factor` (`float`, *optional*): The scaling factor applied to the inverse frequencies when 1) the\n+                    wavelength is greater than `low_freq_wavelen` prior to smoothing, and 2) to all inverse frequencies\n+                    during smoothing.\n+                *   `high_freq_factor` (`float`): The scale factor used to compute `high_freq_wavelen` and\n+                    the value for the denominator of the smoothing factor prior to the `low_freq_factor` shift.\n+                *   `low_freq_factor` (`float`): The scale factor used to compute `low_freq_wavelen` and\n+                    the shift applied to the numerator and denominator of the smoothing factor.\n+                    frequencies if `seq_len` is None or less-than-or-equal-to `original_max_position_embeddings`.\n+                *   `original_max_position_embeddings` (`int`): The original max position embeddings used\n+                    during pretraining. If not provided, the function falls back to `max_position_embeddings`.\n+\n+            Additionally, this function will make use of the following properties if they are found in the config:\n+\n+            *   head_dim (`int`, *optional*): The size of the key-value heads in the model. If None, this value will be\n+                derived as hidden_size // num_attention_heads.\n+            *   partial_rotary_factor (`float`, *optional*): If less than 1.0, inverse frequencies will be returned for\n+                the first fraction of the head_dim. Defaults to 1.0.\n         device (`torch.device`):\n             The device to use for initialization of the inverse frequencies.\n         seq_len (`int`, *optional*):\n@@ -527,7 +666,7 @@ def _validate_longrope_parameters(config: PretrainedConfig, ignore_keys: Optiona\n     received_keys = set(rope_scaling.keys())\n     _check_received_keys(rope_type, received_keys, required_keys, optional_keys, ignore_keys=ignore_keys)\n \n-    partial_rotary_factor = config.partial_rotary_factor if hasattr(config, \"partial_rotary_factor\") else 1.0\n+    partial_rotary_factor = getattr(config, \"partial_rotary_factor\", 1.0)\n     head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n     dim = int(head_dim * partial_rotary_factor)\n "
        }
    ],
    "stats": {
        "total": 171,
        "additions": 155,
        "deletions": 16
    }
}