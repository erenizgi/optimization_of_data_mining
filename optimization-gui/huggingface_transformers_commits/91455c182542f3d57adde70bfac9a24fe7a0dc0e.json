{
    "author": "yonigozlan",
    "message": "Fix processor kwargs qwen2 vl (#36890)\n\n* Fix qwen2_vl and qwen2_5_vl processors cutom images kwargs\n\n* change version warning",
    "sha": "91455c182542f3d57adde70bfac9a24fe7a0dc0e",
    "files": [
        {
            "sha": "7cd47bc060d4228cab4a52823707c07cf3e9a2b7",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/91455c182542f3d57adde70bfac9a24fe7a0dc0e/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/91455c182542f3d57adde70bfac9a24fe7a0dc0e/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=91455c182542f3d57adde70bfac9a24fe7a0dc0e",
            "patch": "@@ -493,15 +493,15 @@ def from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs):\n                 image_processor_auto_map = config.auto_map[\"AutoImageProcessor\"]\n \n         image_processor_class = None\n-        # TODO: @yoni, change logic in v4.50 (when use_fast set to True by default)\n+        # TODO: @yoni, change logic in v4.52 (when use_fast set to True by default)\n         if image_processor_type is not None:\n             # if use_fast is not set and the processor was saved with a fast processor, we use it, otherwise we use the slow processor.\n             if use_fast is None:\n                 use_fast = image_processor_type.endswith(\"Fast\")\n                 if not use_fast:\n                     logger.warning_once(\n                         \"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. \"\n-                        \"`use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. \"\n+                        \"`use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. \"\n                         \"This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\"\n                     )\n             # Update class name to reflect the use_fast option. If class is not found, we fall back to the slow version."
        },
        {
            "sha": "a77344c976d9339b213c241da4f8e350fd4703e0",
            "filename": "src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/91455c182542f3d57adde70bfac9a24fe7a0dc0e/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/91455c182542f3d57adde70bfac9a24fe7a0dc0e/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py?ref=91455c182542f3d57adde70bfac9a24fe7a0dc0e",
            "patch": "@@ -41,7 +41,7 @@\n     VisionRotaryEmbedding,\n     VisionSdpaAttention,\n )\n-from transformers.models.qwen2_vl.processing_qwen2_vl import Qwen2VLProcessor\n+from transformers.models.qwen2_vl.processing_qwen2_vl import Qwen2VLImagesKwargs, Qwen2VLProcessor\n \n from ...activations import ACT2FN\n from ...configuration_utils import PretrainedConfig\n@@ -816,7 +816,12 @@ class Qwen2_5_VLVideosProcessorKwargs(VideosKwargs, total=False):\n     fps: Union[List[float], float]\n \n \n+class Qwen2_5_VLImagesKwargs(Qwen2VLImagesKwargs):\n+    pass\n+\n+\n class Qwen2_5_VLProcessorKwargs(ProcessingKwargs, total=False):\n+    images_kwargs: Qwen2_5_VLImagesKwargs\n     videos_kwargs: Qwen2_5_VLVideosProcessorKwargs\n     _defaults = {\n         \"text_kwargs\": {"
        },
        {
            "sha": "e07642a1bf9d8242e0de7e517948cc1854727c63",
            "filename": "src/transformers/models/qwen2_5_vl/processing_qwen2_5_vl.py",
            "status": "modified",
            "additions": 11,
            "deletions": 2,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/91455c182542f3d57adde70bfac9a24fe7a0dc0e/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fprocessing_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/91455c182542f3d57adde70bfac9a24fe7a0dc0e/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fprocessing_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fprocessing_qwen2_5_vl.py?ref=91455c182542f3d57adde70bfac9a24fe7a0dc0e",
            "patch": "@@ -23,19 +23,28 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from typing import List, Union\n+from typing import List, Optional, Union\n \n from ...feature_extraction_utils import BatchFeature\n from ...image_utils import ImageInput, VideoInput\n-from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack, VideosKwargs\n+from ...processing_utils import ImagesKwargs, ProcessingKwargs, ProcessorMixin, Unpack, VideosKwargs\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n \n \n class Qwen2_5_VLVideosProcessorKwargs(VideosKwargs, total=False):\n     fps: Union[List[float], float]\n \n \n+class Qwen2_5_VLImagesKwargs(ImagesKwargs):\n+    min_pixels: Optional[int]\n+    max_pixels: Optional[int]\n+    patch_size: Optional[int]\n+    temporal_patch_size: Optional[int]\n+    merge_size: Optional[int]\n+\n+\n class Qwen2_5_VLProcessorKwargs(ProcessingKwargs, total=False):\n+    images_kwargs: Qwen2_5_VLImagesKwargs\n     videos_kwargs: Qwen2_5_VLVideosProcessorKwargs\n     _defaults = {\n         \"text_kwargs\": {"
        },
        {
            "sha": "671cd861707517970534d370fbc69404598cefde",
            "filename": "src/transformers/models/qwen2_vl/image_processing_qwen2_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/91455c182542f3d57adde70bfac9a24fe7a0dc0e/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/91455c182542f3d57adde70bfac9a24fe7a0dc0e/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl.py?ref=91455c182542f3d57adde70bfac9a24fe7a0dc0e",
            "patch": "@@ -384,7 +384,7 @@ def preprocess(\n                 raise ValueError(\"size must contain 'shortest_edge' and 'longest_edge' keys.\")\n             min_pixels = size[\"shortest_edge\"]\n         else:\n-            size = self.size\n+            size = {**self.size}\n         # backward compatibility: override size with min_pixels and max_pixels if they are provided\n         if min_pixels is not None:\n             size[\"shortest_edge\"] = min_pixels"
        },
        {
            "sha": "8d92cb0845b0d96a0d9cda6dbed3ee1119c2f72e",
            "filename": "src/transformers/models/qwen2_vl/image_processing_qwen2_vl_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/91455c182542f3d57adde70bfac9a24fe7a0dc0e/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/91455c182542f3d57adde70bfac9a24fe7a0dc0e/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py?ref=91455c182542f3d57adde70bfac9a24fe7a0dc0e",
            "patch": "@@ -339,7 +339,7 @@ def preprocess(\n                 raise ValueError(\"size must contain 'shortest_edge' and 'longest_edge' keys.\")\n             min_pixels = size[\"shortest_edge\"]\n         else:\n-            size = self.size\n+            size = {**self.size}\n         # backward compatibility: override size with min_pixels and max_pixels if they are provided\n         if min_pixels is not None:\n             size[\"shortest_edge\"] = min_pixels"
        },
        {
            "sha": "06b0adb0fb2000b72e8bf5abfd964b81610f21bf",
            "filename": "src/transformers/models/qwen2_vl/processing_qwen2_vl.py",
            "status": "modified",
            "additions": 11,
            "deletions": 2,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/91455c182542f3d57adde70bfac9a24fe7a0dc0e/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fprocessing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/91455c182542f3d57adde70bfac9a24fe7a0dc0e/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fprocessing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fprocessing_qwen2_vl.py?ref=91455c182542f3d57adde70bfac9a24fe7a0dc0e",
            "patch": "@@ -21,19 +21,28 @@\n Processor class for Qwen2-VL.\n \"\"\"\n \n-from typing import List, Union\n+from typing import List, Optional, Union\n \n from ...feature_extraction_utils import BatchFeature\n from ...image_utils import ImageInput, VideoInput\n-from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n+from ...processing_utils import ImagesKwargs, ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n+class Qwen2VLImagesKwargs(ImagesKwargs):\n+    min_pixels: Optional[int]\n+    max_pixels: Optional[int]\n+    patch_size: Optional[int]\n+    temporal_patch_size: Optional[int]\n+    merge_size: Optional[int]\n+\n+\n class Qwen2VLProcessorKwargs(ProcessingKwargs, total=False):\n+    images_kwargs: Qwen2VLImagesKwargs\n     _defaults = {\n         \"text_kwargs\": {\n             \"padding\": False,"
        },
        {
            "sha": "556ce3d522c1ed08af27d1423cda6b15da55dca2",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/91455c182542f3d57adde70bfac9a24fe7a0dc0e/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/91455c182542f3d57adde70bfac9a24fe7a0dc0e/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=91455c182542f3d57adde70bfac9a24fe7a0dc0e",
            "patch": "@@ -1111,12 +1111,12 @@ def _get_arguments_from_pretrained(cls, pretrained_model_name_or_path, **kwargs)\n             if isinstance(class_name, tuple):\n                 classes = tuple(cls.get_possibly_dynamic_module(n) if n is not None else None for n in class_name)\n                 if attribute_name == \"image_processor\":\n-                    # TODO: @yoni, change logic in v4.50 (when use_fast set to True by default)\n+                    # TODO: @yoni, change logic in v4.52 (when use_fast set to True by default)\n                     use_fast = kwargs.get(\"use_fast\", None)\n                     if use_fast is None:\n                         logger.warning_once(\n                             \"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. \"\n-                            \"`use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. \"\n+                            \"`use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. \"\n                             \"This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\"\n                         )\n                 else:"
        },
        {
            "sha": "65361f016fdd1a685816c579ae32ac2050511dc7",
            "filename": "tests/models/qwen2_5_vl/test_processor_qwen2_5_vl.py",
            "status": "modified",
            "additions": 16,
            "deletions": 0,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/91455c182542f3d57adde70bfac9a24fe7a0dc0e/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_processor_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/91455c182542f3d57adde70bfac9a24fe7a0dc0e/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_processor_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_processor_qwen2_5_vl.py?ref=91455c182542f3d57adde70bfac9a24fe7a0dc0e",
            "patch": "@@ -310,3 +310,19 @@ def test_chat_template_video(self):\n         )\n         self.assertTrue(self.videos_input_name in out_dict_with_video)\n         self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 71280)\n+\n+    def test_kwargs_overrides_custom_image_processor_kwargs(self):\n+        processor_components = self.prepare_components()\n+        processor_components[\"image_processor\"] = self.get_component(\"image_processor\")\n+        processor_components[\"tokenizer\"] = self.get_component(\"tokenizer\")\n+        processor_kwargs = self.prepare_processor_dict()\n+\n+        processor = self.processor_class(**processor_components, **processor_kwargs, use_fast=True)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = self.prepare_text_inputs()\n+        image_input = self.prepare_image_inputs()\n+        inputs = processor(text=input_str, images=image_input, max_pixels=56 * 56 * 4, return_tensors=\"pt\")\n+        self.assertEqual(inputs[self.images_input_name].shape[0], 612)\n+        inputs = processor(text=input_str, images=image_input, return_tensors=\"pt\")\n+        self.assertEqual(inputs[self.images_input_name].shape[0], 800)"
        },
        {
            "sha": "64540eed047e36c9b10f3757a46d7c78e3d70a67",
            "filename": "tests/models/qwen2_vl/test_processor_qwen2_vl.py",
            "status": "modified",
            "additions": 16,
            "deletions": 0,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/91455c182542f3d57adde70bfac9a24fe7a0dc0e/tests%2Fmodels%2Fqwen2_vl%2Ftest_processor_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/91455c182542f3d57adde70bfac9a24fe7a0dc0e/tests%2Fmodels%2Fqwen2_vl%2Ftest_processor_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_processor_qwen2_vl.py?ref=91455c182542f3d57adde70bfac9a24fe7a0dc0e",
            "patch": "@@ -307,3 +307,19 @@ def test_chat_template_video(self):\n         )\n         self.assertTrue(self.videos_input_name in out_dict_with_video)\n         self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 71280)\n+\n+    def test_kwargs_overrides_custom_image_processor_kwargs(self):\n+        processor_components = self.prepare_components()\n+        processor_components[\"image_processor\"] = self.get_component(\"image_processor\")\n+        processor_components[\"tokenizer\"] = self.get_component(\"tokenizer\")\n+        processor_kwargs = self.prepare_processor_dict()\n+\n+        processor = self.processor_class(**processor_components, **processor_kwargs, use_fast=True)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = self.prepare_text_inputs()\n+        image_input = self.prepare_image_inputs()\n+        inputs = processor(text=input_str, images=image_input, return_tensors=\"pt\")\n+        self.assertEqual(inputs[self.images_input_name].shape[0], 800)\n+        inputs = processor(text=input_str, images=image_input, max_pixels=56 * 56 * 4, return_tensors=\"pt\")\n+        self.assertEqual(inputs[self.images_input_name].shape[0], 612)"
        }
    ],
    "stats": {
        "total": 77,
        "additions": 66,
        "deletions": 11
    }
}