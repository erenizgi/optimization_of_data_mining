{
    "author": "Cyrilvallez",
    "message": "General weight initialization scheme  (#39579)\n\n* general + modulars from llama\n\n* all modular models\n\n* style and fix musicgen\n\n* fix\n\n* Update configuration_musicgen.py\n\n* Update modeling_utils.py",
    "sha": "b16688e96a3b3e1e7a701cd5284a850c696c108e",
    "files": [
        {
            "sha": "71eb25f74ad8e3fc24095a7b176d5390bb1d0693",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 34,
            "deletions": 5,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -2967,12 +2967,41 @@ def disable_input_require_grads(self):\n \n     def _init_weights(self, module):\n         \"\"\"\n-        Initialize the weights. This method should be overridden by derived class and is\n-        the only initialization method that will be called when loading a checkpoint\n-        using `from_pretrained`. Any attempt to initialize outside of this function\n-        will be useless as the torch.nn.init function are all replaced with skip.\n+        Initialize the weights. This is quite general on purpose, in the spirit of what we usually do. For more complex\n+        initialization scheme, it should be overriden by the derived `PreTrainedModel` class. In case a model adds an explicit\n+        `nn.Parameter`, this method should also be overriden in order to initialize it correctly.\n         \"\"\"\n-        pass\n+        if hasattr(self.config, \"initializer_range\"):\n+            std = self.config.initializer_range\n+        else:\n+            # 0.02 is the standard default value accross the library\n+            std = getattr(self.config.get_text_config(), \"initializer_range\", 0.02)\n+\n+        if isinstance(module, (nn.Linear, nn.Conv1d, nn.Conv2d, nn.Conv3d, nn.ConvTranspose1d, nn.ConvTranspose2d)):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, nn.MultiheadAttention):\n+            # This uses torch's original init\n+            module._reset_parameters()\n+        # We cannot use `isinstance` on the RMSNorms or LayerNorms, as they usually are custom modules which change names\n+        # between modelings (because they are prefixed with the model name)\n+        elif (\n+            isinstance(\n+                module, (nn.LayerNorm, nn.RMSNorm, nn.GroupNorm, nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)\n+            )\n+            or \"LayerNorm\" in module.__class__.__name__\n+            or \"RMSNorm\" in module.__class__.__name__\n+        ):\n+            # Norms can exist without weights (in which case they are None from torch primitives)\n+            if hasattr(module, \"weight\") and module.weight is not None:\n+                module.weight.data.fill_(1.0)\n+            if hasattr(module, \"bias\") and module.bias is not None:\n+                module.bias.data.zero_()\n \n     def _initialize_weights(self, module):\n         \"\"\""
        },
        {
            "sha": "ff1eb6278174dc5a49709752f9538f1e3448938a",
            "filename": "src/transformers/models/aimv2/modeling_aimv2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 15,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodeling_aimv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodeling_aimv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodeling_aimv2.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -448,24 +448,12 @@ class Aimv2PreTrainedModel(PreTrainedModel):\n     _supports_flex_attn = True\n \n     def _init_weights(self, module):\n-        std = (\n-            self.config.vision_config.initializer_range\n-            if hasattr(self.config, \"vision_config\")\n-            else self.config.initializer_range\n-        )\n-        if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, Aimv2RMSNorm):\n-            module.weight.data.fill_(1.0)\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-        elif hasattr(module, \"logit_scale\"):\n+        super()._init_weights(module)\n+        if hasattr(module, \"logit_scale\"):\n             if isinstance(module.logit_scale, nn.Parameter):\n                 module.logit_scale.data.fill_(math.log(1 / 0.07))\n         elif isinstance(module, Aimv2AttentionPoolingHead):\n-            module.cls_token.data.normal_(mean=0.0, std=std)\n+            module.cls_token.data.normal_(mean=0.0, std=self.config.initializer_range)\n \n \n @auto_docstring("
        },
        {
            "sha": "ad2c3d78aadcb6caef625097fd033e21df450576",
            "filename": "src/transformers/models/aimv2/modular_aimv2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 15,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodular_aimv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodular_aimv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodular_aimv2.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -445,24 +445,12 @@ class Aimv2PreTrainedModel(PreTrainedModel):\n     _supports_flex_attn = True\n \n     def _init_weights(self, module):\n-        std = (\n-            self.config.vision_config.initializer_range\n-            if hasattr(self.config, \"vision_config\")\n-            else self.config.initializer_range\n-        )\n-        if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, Aimv2RMSNorm):\n-            module.weight.data.fill_(1.0)\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-        elif hasattr(module, \"logit_scale\"):\n+        super()._init_weights(module)\n+        if hasattr(module, \"logit_scale\"):\n             if isinstance(module.logit_scale, nn.Parameter):\n                 module.logit_scale.data.fill_(math.log(1 / 0.07))\n         elif isinstance(module, Aimv2AttentionPoolingHead):\n-            module.cls_token.data.normal_(mean=0.0, std=std)\n+            module.cls_token.data.normal_(mean=0.0, std=self.config.initializer_range)\n \n \n @auto_docstring("
        },
        {
            "sha": "9c7f43a84d225fcbb2028fff5e8c508ed8932c2d",
            "filename": "src/transformers/models/arcee/modeling_arcee.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -324,19 +324,6 @@ class ArceePreTrainedModel(PreTrainedModel):\n         \"attentions\": ArceeAttention,\n     }\n \n-    def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, ArceeRMSNorm):\n-            module.weight.data.fill_(1.0)\n-\n \n @auto_docstring\n class ArceeModel(ArceePreTrainedModel):"
        },
        {
            "sha": "29bbfb16fec4e830e5d41139aca62d527a93aa79",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 6,
            "deletions": 27,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -639,19 +639,9 @@ class AriaTextPreTrainedModel(PreTrainedModel):\n     }\n \n     def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, AriaTextRMSNorm):\n-            module.weight.data.fill_(1.0)\n-        elif isinstance(module, AriaGroupedExpertsGemm):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+        super()._init_weights(module)\n+        if isinstance(module, AriaGroupedExpertsGemm):\n+            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n \n \n @auto_docstring\n@@ -672,20 +662,9 @@ class AriaPreTrainedModel(PreTrainedModel):\n     }\n \n     def _init_weights(self, module):\n-        std = self.config.initializer_range\n-\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.MultiheadAttention):\n-            # This uses torch's original init\n-            module._reset_parameters()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.weight.data.fill_(1.0)\n-            module.bias.data.zero_()\n-        elif isinstance(module, AriaProjector):\n-            nn.init.trunc_normal_(module.query, std=std)\n+        super()._init_weights(module)\n+        if isinstance(module, AriaProjector):\n+            nn.init.trunc_normal_(module.query, std=self.config.initializer_range)\n \n \n class AriaTextRotaryEmbedding(nn.Module):"
        },
        {
            "sha": "1fb64f50b7b7d3167dc34be72584e9ca93b2ca8e",
            "filename": "src/transformers/models/aria/modular_aria.py",
            "status": "modified",
            "additions": 6,
            "deletions": 27,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -1294,19 +1294,9 @@ class AriaTextPreTrainedModel(PreTrainedModel):\n     }\n \n     def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, AriaTextRMSNorm):\n-            module.weight.data.fill_(1.0)\n-        elif isinstance(module, AriaGroupedExpertsGemm):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+        super()._init_weights(module)\n+        if isinstance(module, AriaGroupedExpertsGemm):\n+            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n \n \n class AriaPreTrainedModel(LlamaPreTrainedModel):\n@@ -1316,20 +1306,9 @@ class AriaPreTrainedModel(LlamaPreTrainedModel):\n     _supports_attention_backend = True\n \n     def _init_weights(self, module):\n-        std = self.config.initializer_range\n-\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.MultiheadAttention):\n-            # This uses torch's original init\n-            module._reset_parameters()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.weight.data.fill_(1.0)\n-            module.bias.data.zero_()\n-        elif isinstance(module, AriaProjector):\n-            nn.init.trunc_normal_(module.query, std=std)\n+        LlamaPreTrainedModel._init_weights(module)\n+        if isinstance(module, AriaProjector):\n+            nn.init.trunc_normal_(module.query, std=self.config.initializer_range)\n \n \n class AriaTextModel(LlamaModel):"
        },
        {
            "sha": "70c85ab27f58b13ba428a47d9b239df055a8ef02",
            "filename": "src/transformers/models/aya_vision/modeling_aya_vision.py",
            "status": "modified",
            "additions": 0,
            "deletions": 15,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -100,21 +100,6 @@ class AyaVisionPreTrainedModel(PreTrainedModel):\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n \n-    def _init_weights(self, module):\n-        std = (\n-            self.config.initializer_range\n-            if hasattr(self.config, \"initializer_range\")\n-            else self.config.text_config.initializer_range\n-        )\n-\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.weight.data.fill_(1.0)\n-            module.bias.data.zero_()\n-\n \n @dataclass\n @auto_docstring("
        },
        {
            "sha": "9dcf02547ac0015d7f61b557e81e6fca85620d23",
            "filename": "src/transformers/models/aya_vision/modular_aya_vision.py",
            "status": "modified",
            "additions": 0,
            "deletions": 15,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodular_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodular_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodular_aya_vision.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -92,21 +92,6 @@ def pixel_shuffle(self, image_features):  # B, S, D\n class AyaVisionPreTrainedModel(LlavaPreTrainedModel):\n     _supports_static_cache = False\n \n-    def _init_weights(self, module):\n-        std = (\n-            self.config.initializer_range\n-            if hasattr(self.config, \"initializer_range\")\n-            else self.config.text_config.initializer_range\n-        )\n-\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.weight.data.fill_(1.0)\n-            module.bias.data.zero_()\n-\n \n class AyaVisionCausalLMOutputWithPast(LlavaCausalLMOutputWithPast):\n     pass"
        },
        {
            "sha": "7389ac5a8694964cc81529c5b5a9b4971f586fbf",
            "filename": "src/transformers/models/bamba/modeling_bamba.py",
            "status": "modified",
            "additions": 2,
            "deletions": 12,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -1088,18 +1088,8 @@ class BambaPreTrainedModel(PreTrainedModel):\n     _is_stateful = True\n \n     def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, (nn.Linear, nn.Conv1d)):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, (BambaRMSNormGated, BambaRMSNorm)):\n-            module.weight.data.fill_(1.0)\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, BambaMixer):\n+        super()._init_weights(module)\n+        if isinstance(module, BambaMixer):\n             module.dt_bias.data.fill_(1.0)\n             module.A_log.data = torch.log(torch.arange(1, module.num_heads + 1))\n             module.D.data.fill_(1.0)"
        },
        {
            "sha": "f99faa9ed78df2b128eaed8ee3c3f6bfd958177e",
            "filename": "src/transformers/models/bamba/modular_bamba.py",
            "status": "modified",
            "additions": 2,
            "deletions": 12,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -816,18 +816,8 @@ class BambaPreTrainedModel(PreTrainedModel):\n     _is_stateful = True\n \n     def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, (nn.Linear, nn.Conv1d)):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, (BambaRMSNormGated, BambaRMSNorm)):\n-            module.weight.data.fill_(1.0)\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, BambaMixer):\n+        super()._init_weights(module)\n+        if isinstance(module, BambaMixer):\n             module.dt_bias.data.fill_(1.0)\n             module.A_log.data = torch.log(torch.arange(1, module.num_heads + 1))\n             module.D.data.fill_(1.0)"
        },
        {
            "sha": "63ad37033e29f1d910b39847d7128f25045e284e",
            "filename": "src/transformers/models/biogpt/modeling_biogpt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 16,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -349,22 +349,6 @@ class BioGptPreTrainedModel(PreTrainedModel):\n \n     _supports_static_cache = True\n \n-    def _init_weights(self, module):\n-        \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, nn.Linear):\n-            # Slightly different from the TF version which uses truncated_normal for initialization\n-            # cf https://github.com/pytorch/pytorch/pull/5617\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n-\n     # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_causal_mask\n     def _update_causal_mask(\n         self,"
        },
        {
            "sha": "43717ae4474c397e42f6b54164a6a9e256291340",
            "filename": "src/transformers/models/biogpt/modular_biogpt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 16,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -174,22 +174,6 @@ class BioGptPreTrainedModel(PreTrainedModel):\n \n     _supports_static_cache = True\n \n-    def _init_weights(self, module):\n-        \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, nn.Linear):\n-            # Slightly different from the TF version which uses truncated_normal for initialization\n-            # cf https://github.com/pytorch/pytorch/pull/5617\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n-\n     # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_causal_mask\n     def _update_causal_mask(\n         self,"
        },
        {
            "sha": "59bd7b2ef9a924ba732b09470559e6ace9c6dfcc",
            "filename": "src/transformers/models/bitnet/modeling_bitnet.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -319,19 +319,6 @@ class BitNetPreTrainedModel(PreTrainedModel):\n         \"attentions\": BitNetAttention,\n     }\n \n-    def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, BitNetRMSNorm):\n-            module.weight.data.fill_(1.0)\n-\n \n @auto_docstring\n class BitNetModel(BitNetPreTrainedModel):"
        },
        {
            "sha": "1e43bbc6ecb362848b194e73e41cfc2542dc0701",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 0,
            "deletions": 17,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -820,23 +820,6 @@ class ChameleonPreTrainedModel(PreTrainedModel):\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n \n-    def _init_weights(self, module):\n-        std = self.config.initializer_range\n-\n-        if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, (nn.GroupNorm, nn.LayerNorm)):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n-        elif isinstance(module, ChameleonRMSNorm):\n-            module.weight.data.fill_(1.0)\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-\n \n @auto_docstring(\n     custom_intro=\"\"\""
        },
        {
            "sha": "15934195cebb97362e8bf7cfd37328d0c903cb62",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -352,19 +352,6 @@ class CoherePreTrainedModel(PreTrainedModel):\n         \"attentions\": CohereAttention,\n     }\n \n-    def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, CohereLayerNorm):\n-            module.weight.data.fill_(1.0)\n-\n \n @auto_docstring\n class CohereModel(CoherePreTrainedModel):"
        },
        {
            "sha": "0e27f4dfc4a55256e8dd8dc5add03d501e8cd424",
            "filename": "src/transformers/models/cohere/modular_cohere.py",
            "status": "modified",
            "additions": 0,
            "deletions": 16,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodular_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodular_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodular_cohere.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -41,7 +41,6 @@\n     LlamaForCausalLM,\n     LlamaMLP,\n     LlamaModel,\n-    LlamaPreTrainedModel,\n     LlamaRotaryEmbedding,\n     eager_attention_forward,\n )\n@@ -255,21 +254,6 @@ def forward(\n         return hidden_states\n \n \n-class CoherePreTrainedModel(LlamaPreTrainedModel):\n-    def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, CohereLayerNorm):\n-            module.weight.data.fill_(1.0)\n-\n-\n class CohereModel(LlamaModel):\n     def __init__(self, config: CohereConfig):\n         super().__init__(config)"
        },
        {
            "sha": "85faa46d1759a54c86f418a2ca88928795545e49",
            "filename": "src/transformers/models/cohere2/modeling_cohere2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -329,19 +329,6 @@ class Cohere2PreTrainedModel(PreTrainedModel):\n         \"attentions\": Cohere2Attention,\n     }\n \n-    def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, Cohere2LayerNorm):\n-            module.weight.data.fill_(1.0)\n-\n \n @auto_docstring\n class Cohere2Model(Cohere2PreTrainedModel):"
        },
        {
            "sha": "3cc477b6dcfc8edeee9b55262557b28dda6947e9",
            "filename": "src/transformers/models/csm/modeling_csm.py",
            "status": "modified",
            "additions": 3,
            "deletions": 13,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -379,21 +379,11 @@ class CsmPreTrainedModel(PreTrainedModel):\n     }\n \n     def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, CsmCodebooksHead):\n+        super()._init_weights(module)\n+        if isinstance(module, CsmCodebooksHead):\n             num_codebooks = module.num_codebooks\n             for i in range(num_codebooks - 1):\n-                module.weight.data[i].normal_(mean=0.0, std=std)\n-        elif isinstance(module, CsmRMSNorm):\n-            module.weight.data.fill_(1.0)\n+                module.weight.data[i].normal_(mean=0.0, std=self.config.initializer_range)\n \n \n @auto_docstring"
        },
        {
            "sha": "60b261275999f11385271e1c9eb1d9af90bd7ce9",
            "filename": "src/transformers/models/csm/modular_csm.py",
            "status": "modified",
            "additions": 3,
            "deletions": 13,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -142,21 +142,11 @@ class CsmPreTrainedModel(PreTrainedModel):\n     }\n \n     def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, CsmCodebooksHead):\n+        super()._init_weights(module)\n+        if isinstance(module, CsmCodebooksHead):\n             num_codebooks = module.num_codebooks\n             for i in range(num_codebooks - 1):\n-                module.weight.data[i].normal_(mean=0.0, std=std)\n-        elif isinstance(module, CsmRMSNorm):\n-            module.weight.data.fill_(1.0)\n+                module.weight.data[i].normal_(mean=0.0, std=self.config.initializer_range)\n \n \n @auto_docstring"
        },
        {
            "sha": "d7dd466e3df0675bd3ff3fbb16358bc182851fbc",
            "filename": "src/transformers/models/deepseek_v2/modeling_deepseek_v2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 12,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -467,18 +467,8 @@ class DeepseekV2PreTrainedModel(PreTrainedModel):\n     }\n \n     def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, DeepseekV2RMSNorm):\n-            module.weight.data.fill_(1.0)\n-        elif isinstance(module, DeepseekV2MoEGate):\n+        super()._init_weights(module)\n+        if isinstance(module, DeepseekV2MoEGate):\n             module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n \n "
        },
        {
            "sha": "dddb74d1bf2d68349601599703f476ce02ea3d67",
            "filename": "src/transformers/models/deepseek_v2/modular_deepseek_v2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 12,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodular_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodular_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodular_deepseek_v2.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -503,18 +503,8 @@ def __init__(self, config: DeepseekV2Config, layer_idx: int):\n \n class DeepseekV2PreTrainedModel(LlamaPreTrainedModel):\n     def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, DeepseekV2RMSNorm):\n-            module.weight.data.fill_(1.0)\n-        elif isinstance(module, DeepseekV2MoEGate):\n+        LlamaPreTrainedModel._init_weights(module)\n+        if isinstance(module, DeepseekV2MoEGate):\n             module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n \n "
        },
        {
            "sha": "38d39d03ab860294fea08f7aef68cac6bfba6134",
            "filename": "src/transformers/models/deepseek_v3/modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 13,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -506,19 +506,9 @@ class DeepseekV3PreTrainedModel(PreTrainedModel):\n     }\n \n     def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, DeepseekV3RMSNorm):\n-            module.weight.data.fill_(1.0)\n-        elif isinstance(module, DeepseekV3TopkRouter):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+        super()._init_weights(module)\n+        if isinstance(module, DeepseekV3TopkRouter):\n+            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n \n \n @auto_docstring"
        },
        {
            "sha": "ebcd3aed398c302769df6ba9fd48c05091b1184b",
            "filename": "src/transformers/models/deepseek_v3/modular_deepseek_v3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 13,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodular_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodular_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodular_deepseek_v3.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -339,19 +339,9 @@ def __init__(self, config: DeepseekV3Config, layer_idx: int):\n \n class DeepseekV3PreTrainedModel(LlamaPreTrainedModel):\n     def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, DeepseekV3RMSNorm):\n-            module.weight.data.fill_(1.0)\n-        elif isinstance(module, DeepseekV3TopkRouter):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+        LlamaPreTrainedModel._init_weights(module)\n+        if isinstance(module, DeepseekV3TopkRouter):\n+            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n \n \n class DeepseekV3Model(LlamaModel):"
        },
        {
            "sha": "d115f3766f1c3bd563bd0e669396bcfddb141042",
            "filename": "src/transformers/models/dia/modeling_dia.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -71,19 +71,6 @@ class DiaPreTrainedModel(PreTrainedModel):\n     main_input_name = \"input_ids\"\n     _no_split_modules = [\"DiaEncoderLayer\", \"DiaDecoderLayer\"]\n \n-    def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, DiaRMSNorm):\n-            module.weight.data.fill_(1.0)\n-\n \n class DiaMultiChannelEmbedding(nn.Module):\n     \"\"\"In order to efficiently compute the audio embedding from the 9 different channels,"
        },
        {
            "sha": "6934f6e4f72f552645013e2cda7b68b8f56f7584",
            "filename": "src/transformers/models/dia/modular_dia.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fdia%2Fmodular_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fdia%2Fmodular_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fmodular_dia.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -66,19 +66,6 @@ class DiaPreTrainedModel(PreTrainedModel):\n     main_input_name = \"input_ids\"\n     _no_split_modules = [\"DiaEncoderLayer\", \"DiaDecoderLayer\"]\n \n-    def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, DiaRMSNorm):\n-            module.weight.data.fill_(1.0)\n-\n \n class DiaMultiChannelEmbedding(nn.Module):\n     \"\"\"In order to efficiently compute the audio embedding from the 9 different channels,"
        },
        {
            "sha": "df4c9d01592b496f99e1787910a789c71aad3a88",
            "filename": "src/transformers/models/diffllama/modeling_diffllama.py",
            "status": "modified",
            "additions": 2,
            "deletions": 12,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -542,18 +542,8 @@ class DiffLlamaPreTrainedModel(PreTrainedModel):\n     }\n \n     def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, DiffLlamaRMSNorm):  # noqa: F821\n-            module.weight.data.fill_(1.0)\n-        elif isinstance(module, DiffLlamaAttention):\n+        super()._init_weights(module)\n+        if isinstance(module, DiffLlamaAttention):\n             module.lambda_q1.data.normal_(0, self.config.lambda_std_dev)\n             module.lambda_k1.data.normal_(0, self.config.lambda_std_dev)\n             module.lambda_q2.data.normal_(0, self.config.lambda_std_dev)"
        },
        {
            "sha": "b5034f17490b7617c8bbf5e98dc5f8677112a858",
            "filename": "src/transformers/models/diffllama/modular_diffllama.py",
            "status": "modified",
            "additions": 2,
            "deletions": 12,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodular_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodular_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodular_diffllama.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -404,18 +404,8 @@ class DiffLlamaPreTrainedModel(LlamaPreTrainedModel):\n     _supports_attention_backend = False\n \n     def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, DiffLlamaRMSNorm):  # noqa: F821\n-            module.weight.data.fill_(1.0)\n-        elif isinstance(module, DiffLlamaAttention):\n+        LlamaPreTrainedModel._init_weights(module)\n+        if isinstance(module, DiffLlamaAttention):\n             module.lambda_q1.data.normal_(0, self.config.lambda_std_dev)\n             module.lambda_k1.data.normal_(0, self.config.lambda_std_dev)\n             module.lambda_q2.data.normal_(0, self.config.lambda_std_dev)"
        },
        {
            "sha": "fccfe71de27db499d1971911202cb500c66fcf63",
            "filename": "src/transformers/models/doge/modeling_doge.py",
            "status": "modified",
            "additions": 1,
            "deletions": 12,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodeling_doge.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodeling_doge.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodeling_doge.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -504,18 +504,7 @@ class DogePreTrainedModel(PreTrainedModel):\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n-        std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, DogeRMSNorm):\n-            module.weight.data.fill_(1.0)\n-\n+        super()._init_weights(module)\n         if isinstance(module, DogeAttention):\n             if hasattr(module, \"A\"):\n                 module.A.data.zero_()"
        },
        {
            "sha": "49094ec499b3c6702fbd626358f9f19a5be69772",
            "filename": "src/transformers/models/doge/modular_doge.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodular_doge.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodular_doge.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodular_doge.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -573,8 +573,7 @@ class DogePreTrainedModel(LlamaPreTrainedModel):\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n-        super()._init_weights(module)\n-\n+        LlamaPreTrainedModel._init_weights(module)\n         if isinstance(module, DogeAttention):\n             if hasattr(module, \"A\"):\n                 module.A.data.zero_()"
        },
        {
            "sha": "4a90048d7e5ac3f86926f1b0519a077de8f1a37d",
            "filename": "src/transformers/models/dots1/modeling_dots1.py",
            "status": "modified",
            "additions": 3,
            "deletions": 13,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -426,19 +426,9 @@ class Dots1PreTrainedModel(PreTrainedModel):\n     }\n \n     def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, Dots1RMSNorm):\n-            module.weight.data.fill_(1.0)\n-        elif isinstance(module, Dots1TopkRouter):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+        super()._init_weights(module)\n+        if isinstance(module, Dots1TopkRouter):\n+            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n \n \n @auto_docstring"
        },
        {
            "sha": "570c32c2fb3b464e8c25b65926cb483ac8c69af7",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -1103,19 +1103,6 @@ class Emu3PreTrainedModel(PreTrainedModel):\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n \n-    def _init_weights(self, module):\n-        std = self.config.get_text_config().initializer_range\n-        if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, Emu3RMSNorm):  # noqa: F821\n-            module.weight.data.fill_(1.0)\n-\n \n class Emu3RotaryEmbedding(nn.Module):\n     def __init__(self, config: Emu3Config, device=None):"
        },
        {
            "sha": "9aa66fe65e971813e156e8ede744b23f59ff21c6",
            "filename": "src/transformers/models/emu3/modular_emu3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -845,19 +845,6 @@ class Emu3PreTrainedModel(ChameleonPreTrainedModel, Emu3VQVAE):\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n \n-    def _init_weights(self, module):\n-        std = self.config.get_text_config().initializer_range\n-        if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, Emu3RMSNorm):  # noqa: F821\n-            module.weight.data.fill_(1.0)\n-\n \n class Emu3TextModel(LlamaModel, Emu3PreTrainedModel):\n     _can_record_outputs = {"
        },
        {
            "sha": "0a9a17018429b7011171c974dc667be1c3e383f1",
            "filename": "src/transformers/models/ernie4_5/modeling_ernie4_5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fmodeling_ernie4_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fmodeling_ernie4_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fmodeling_ernie4_5.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -318,19 +318,6 @@ class Ernie4_5PreTrainedModel(PreTrainedModel):\n         \"attentions\": Ernie4_5Attention,\n     }\n \n-    def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, Ernie4_5RMSNorm):\n-            module.weight.data.fill_(1.0)\n-\n \n @auto_docstring\n class Ernie4_5Model(Ernie4_5PreTrainedModel):"
        },
        {
            "sha": "9081f66265012d7e2ceaf8b3273726e4c7b01c5a",
            "filename": "src/transformers/models/ernie4_5_moe/modeling_ernie4_5_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 12,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -483,18 +483,8 @@ class Ernie4_5_MoEPreTrainedModel(PreTrainedModel):\n     _keep_in_fp32_modules_strict = [\"gate\", \"moe_statics\"]\n \n     def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, Ernie4_5_MoERMSNorm):\n-            module.weight.data.fill_(1.0)\n-        elif isinstance(module, Ernie4_5_MoEStatics):\n+        super()._init_weights(module)\n+        if isinstance(module, Ernie4_5_MoEStatics):\n             module.e_score_correction_bias.data.zero_()\n \n "
        },
        {
            "sha": "0763e415c5a4062d3f96158b7f208cd66587f934",
            "filename": "src/transformers/models/ernie4_5_moe/modular_ernie4_5_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 12,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodular_ernie4_5_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodular_ernie4_5_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodular_ernie4_5_moe.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -218,18 +218,8 @@ class Ernie4_5_MoEPreTrainedModel(MixtralPreTrainedModel):\n     _keep_in_fp32_modules_strict = [\"gate\", \"moe_statics\"]\n \n     def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, Ernie4_5_MoERMSNorm):\n-            module.weight.data.fill_(1.0)\n-        elif isinstance(module, Ernie4_5_MoEStatics):\n+        MixtralPreTrainedModel._init_weights(module)\n+        if isinstance(module, Ernie4_5_MoEStatics):\n             module.e_score_correction_bias.data.zero_()\n \n "
        },
        {
            "sha": "770a0bbb8c914b008e4bb20b50a1269f68fa27b8",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -321,19 +321,6 @@ class GemmaPreTrainedModel(PreTrainedModel):\n         \"attentions\": GemmaAttention,\n     }\n \n-    def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, GemmaRMSNorm):\n-            module.weight.data.fill_(1.0)\n-\n \n @auto_docstring\n class GemmaModel(GemmaPreTrainedModel):"
        },
        {
            "sha": "b31e94b3d9364f887643038bb50e62e11ba0bf3a",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -351,19 +351,6 @@ class Gemma2PreTrainedModel(PreTrainedModel):\n         \"attentions\": Gemma2Attention,\n     }\n \n-    def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, Gemma2RMSNorm):\n-            module.weight.data.fill_(1.0)\n-\n \n @auto_docstring\n class Gemma2Model(Gemma2PreTrainedModel):"
        },
        {
            "sha": "42ec9410b993f8dfc59e68638bd6f6e838eb2863",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 13,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -442,19 +442,8 @@ class Gemma3PreTrainedModel(PreTrainedModel):\n     }\n \n     def _init_weights(self, module):\n-        std = self.config.initializer_range\n-\n-        if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, Gemma3RMSNorm):\n-            module.weight.data.fill_(1.0)\n-        elif isinstance(module, Gemma3MultiModalProjector):\n+        super()._init_weights(module)\n+        if isinstance(module, Gemma3MultiModalProjector):\n             module.mm_input_projection_weight.data.zero_()\n \n "
        },
        {
            "sha": "5bf158c00f4fa5b619677acc5a2e4d5821a04f62",
            "filename": "src/transformers/models/gemma3/modular_gemma3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 13,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -529,19 +529,8 @@ class Gemma3PreTrainedModel(Gemma2PreTrainedModel):\n     ]\n \n     def _init_weights(self, module):\n-        std = self.config.initializer_range\n-\n-        if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, Gemma3RMSNorm):\n-            module.weight.data.fill_(1.0)\n-        elif isinstance(module, Gemma3MultiModalProjector):\n+        Gemma2PreTrainedModel._init_weights(module)\n+        if isinstance(module, Gemma3MultiModalProjector):\n             module.mm_input_projection_weight.data.zero_()\n \n "
        },
        {
            "sha": "f581102639f70a7fb7599ef251e841d9ab04b177",
            "filename": "src/transformers/models/gemma3n/modeling_gemma3n.py",
            "status": "modified",
            "additions": 2,
            "deletions": 16,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -1498,22 +1498,8 @@ class Gemma3nPreTrainedModel(PreTrainedModel):\n     }\n \n     def _init_weights(self, module):\n-        # important: this ported version of Gemma2 isn't meant for training from scratch - only\n-        # inference and fine-tuning - so the proper init weights code has been removed\n-        std = getattr(self.config, \"initializer_range\", self.config.get_text_config().initializer_range)\n-\n-        if isinstance(module, (nn.Linear, nn.Conv1d, nn.Conv2d)):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, Gemma3nRMSNorm):\n-            if module.with_scale:\n-                module.weight.data.fill_(1.0)\n-        elif isinstance(module, Gemma3nAudioCumulativeGroupNorm):\n+        super()._init_weights(module)\n+        if isinstance(module, Gemma3nAudioCumulativeGroupNorm):\n             module.weight.data.fill_(1.0)\n         elif isinstance(module, Gemma3nAudioAttention):\n             module.per_dim_scale.data.zero_()"
        },
        {
            "sha": "a9c2dabc76eca0eb5f4c757768d9509c187ca47b",
            "filename": "src/transformers/models/gemma3n/modular_gemma3n.py",
            "status": "modified",
            "additions": 2,
            "deletions": 16,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -1917,22 +1917,8 @@ class Gemma3nPreTrainedModel(Gemma2PreTrainedModel):\n     _no_split_modules = [\"Gemma3nTextDecoderLayer\"]\n \n     def _init_weights(self, module):\n-        # important: this ported version of Gemma2 isn't meant for training from scratch - only\n-        # inference and fine-tuning - so the proper init weights code has been removed\n-        std = getattr(self.config, \"initializer_range\", self.config.get_text_config().initializer_range)\n-\n-        if isinstance(module, (nn.Linear, nn.Conv1d, nn.Conv2d)):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, Gemma3nRMSNorm):\n-            if module.with_scale:\n-                module.weight.data.fill_(1.0)\n-        elif isinstance(module, Gemma3nAudioCumulativeGroupNorm):\n+        Gemma2PreTrainedModel._init_weights(module)\n+        if isinstance(module, Gemma3nAudioCumulativeGroupNorm):\n             module.weight.data.fill_(1.0)\n         elif isinstance(module, Gemma3nAudioAttention):\n             module.per_dim_scale.data.zero_()"
        },
        {
            "sha": "dc6cceb052fe5c60bf94e77d5acc65dd5ca12d34",
            "filename": "src/transformers/models/glm/modeling_glm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -338,19 +338,6 @@ class GlmPreTrainedModel(PreTrainedModel):\n         \"attentions\": GlmAttention,\n     }\n \n-    def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, GlmRMSNorm):\n-            module.weight.data.fill_(1.0)\n-\n \n @auto_docstring\n class GlmModel(GlmPreTrainedModel):"
        },
        {
            "sha": "6c7aeaf02c95f2480c6db2cdf4fdc0d46649840f",
            "filename": "src/transformers/models/glm4/modeling_glm4.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -342,19 +342,6 @@ class Glm4PreTrainedModel(PreTrainedModel):\n         \"attentions\": Glm4Attention,\n     }\n \n-    def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, Glm4RMSNorm):\n-            module.weight.data.fill_(1.0)\n-\n \n @auto_docstring\n class Glm4Model(Glm4PreTrainedModel):"
        },
        {
            "sha": "cba4ee896bca29959e4297bbc4a75c1753294cf5",
            "filename": "src/transformers/models/glm4_moe/modeling_glm4_moe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 13,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -411,19 +411,9 @@ class Glm4MoePreTrainedModel(PreTrainedModel):\n     }\n \n     def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, Glm4MoeRMSNorm):\n-            module.weight.data.fill_(1.0)\n-        elif isinstance(module, Glm4MoeTopkRouter):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+        super()._init_weights(module)\n+        if isinstance(module, Glm4MoeTopkRouter):\n+            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n \n \n class Glm4MoeRotaryEmbedding(nn.Module):"
        },
        {
            "sha": "57d99a1f98f1eceaf6660e652e843bbc1718869a",
            "filename": "src/transformers/models/glm4v/modeling_glm4v.py",
            "status": "modified",
            "additions": 0,
            "deletions": 16,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -410,22 +410,6 @@ class Glm4vPreTrainedModel(PreTrainedModel):\n     _supports_static_cache = True\n     _supports_attention_backend = True\n \n-    def _init_weights(self, module):\n-        std = self.config.get_text_config().initializer_range\n-        if isinstance(module, (nn.Linear, nn.Conv2d, nn.Conv3d)):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, Glm4vRMSNorm):\n-            module.weight.data.fill_(1.0)\n-        elif isinstance(module, nn.LayerNorm):\n-            module.weight.data.fill_(1.0)\n-            module.bias.data.zero_()\n-\n \n class Glm4vVisionModel(Glm4vPreTrainedModel):\n     config: Glm4vVisionConfig"
        },
        {
            "sha": "71cd472cdb7fefbfdcfa573b92ba0abf7c41fbb3",
            "filename": "src/transformers/models/glm4v/modular_glm4v.py",
            "status": "modified",
            "additions": 0,
            "deletions": 16,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -525,22 +525,6 @@ def __init__(self, config) -> None:\n class Glm4vPreTrainedModel(Qwen2_5_VLPreTrainedModel):\n     _no_split_modules = [\"Glm4vTextDecoderLayer\", \"Glm4vVisionBlock\"]\n \n-    def _init_weights(self, module):\n-        std = self.config.get_text_config().initializer_range\n-        if isinstance(module, (nn.Linear, nn.Conv2d, nn.Conv3d)):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, Glm4vRMSNorm):\n-            module.weight.data.fill_(1.0)\n-        elif isinstance(module, nn.LayerNorm):\n-            module.weight.data.fill_(1.0)\n-            module.bias.data.zero_()\n-\n \n class Glm4vVisionModel(Glm4vPreTrainedModel):\n     config: Glm4vVisionConfig"
        },
        {
            "sha": "dead80a503ac025fc9d7348bfb8b7ee60ddd95ab",
            "filename": "src/transformers/models/got_ocr2/modeling_got_ocr2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 10,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -288,16 +288,8 @@ class GotOcr2PreTrainedModel(PreTrainedModel):\n     _supports_attention_backend = True\n \n     def _init_weights(self, module):\n-        std = getattr(self.config, \"initializer_range\", self.config.get_text_config().initializer_range)\n-\n-        if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, (nn.LayerNorm, GotOcr2LayerNorm)):  # noqa: F821\n-            module.weight.data.fill_(1.0)\n-            module.bias.data.zero_()\n-        elif isinstance(module, GotOcr2VisionAttention):\n+        super()._init_weights(module)\n+        if isinstance(module, GotOcr2VisionAttention):\n             if module.use_rel_pos:\n                 module.rel_pos_h.data.zero_()\n                 module.rel_pos_w.data.zero_()"
        },
        {
            "sha": "7b381e08cc8ebd007304714b0008f13e6dd40ebd",
            "filename": "src/transformers/models/got_ocr2/modular_got_ocr2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 10,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -291,16 +291,8 @@ class GotOcr2PreTrainedModel(LlavaPreTrainedModel):\n     _supports_flex_attn = False\n \n     def _init_weights(self, module):\n-        std = getattr(self.config, \"initializer_range\", self.config.get_text_config().initializer_range)\n-\n-        if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, (nn.LayerNorm, GotOcr2LayerNorm)):  # noqa: F821\n-            module.weight.data.fill_(1.0)\n-            module.bias.data.zero_()\n-        elif isinstance(module, GotOcr2VisionAttention):\n+        LlavaPreTrainedModel._init_weights(module)\n+        if isinstance(module, GotOcr2VisionAttention):\n             if module.use_rel_pos:\n                 module.rel_pos_h.data.zero_()\n                 module.rel_pos_w.data.zero_()"
        },
        {
            "sha": "ec9efd340cba0938b6fd87baecb39c806e6cc049",
            "filename": "src/transformers/models/gpt_neox/modeling_gpt_neox.py",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -372,20 +372,6 @@ class GPTNeoXPreTrainedModel(PreTrainedModel):\n     }\n     _keys_to_ignore_on_load_unexpected = [r\"attention.bias\", r\"attention.masked_bias\"]\n \n-    def _init_weights(self, module):\n-        \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n-\n \n @auto_docstring\n class GPTNeoXModel(GPTNeoXPreTrainedModel):"
        },
        {
            "sha": "860c1ea99a00999277e7917a841f851d52d68114",
            "filename": "src/transformers/models/gpt_neox/modular_gpt_neox.py",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodular_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodular_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodular_gpt_neox.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -247,20 +247,6 @@ class GPTNeoXPreTrainedModel(LlamaPreTrainedModel):\n     _no_split_modules = [\"GPTNeoXLayer\"]\n     _keys_to_ignore_on_load_unexpected = [r\"attention.bias\", r\"attention.masked_bias\"]\n \n-    def _init_weights(self, module):\n-        \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n-\n \n GPT_NEOX_START_DOCSTRING = None  # Will be picked up by modular\n GPT_NEOX_INPUTS_DOCSTRING = None  # Will be picked up by modular"
        },
        {
            "sha": "ba2163c4350470b040738386852d309c080e0d3f",
            "filename": "src/transformers/models/granite/modeling_granite.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -316,19 +316,6 @@ class GranitePreTrainedModel(PreTrainedModel):\n         \"attentions\": GraniteAttention,\n     }\n \n-    def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, GraniteRMSNorm):\n-            module.weight.data.fill_(1.0)\n-\n \n class GraniteRotaryEmbedding(nn.Module):\n     def __init__(self, config: GraniteConfig, device=None):"
        },
        {
            "sha": "8a51c28053fb8770b9ce958ca67e0618aebdc352",
            "filename": "src/transformers/models/granitemoe/modeling_granitemoe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 11,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -595,17 +595,8 @@ class GraniteMoePreTrainedModel(PreTrainedModel):\n     _supports_static_cache = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n \n     def _init_weights(self, module):\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, GraniteMoeRMSNorm):\n-            module.weight.data.fill_(1.0)\n-        elif isinstance(module, GraniteMoeParallelExperts):\n+        super()._init_weights(module)\n+        if isinstance(module, GraniteMoeParallelExperts):\n             module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n \n "
        },
        {
            "sha": "f569be5ef120ac03ec8b3db144a315f444ce139b",
            "filename": "src/transformers/models/granitemoehybrid/modeling_granitemoehybrid.py",
            "status": "modified",
            "additions": 3,
            "deletions": 17,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -1212,24 +1212,10 @@ class GraniteMoeHybridPreTrainedModel(PreTrainedModel):\n     _is_stateful = True\n \n     def _init_weights(self, module):\n-        if isinstance(module, nn.Linear):\n+        super()._init_weights(module)\n+        if isinstance(module, GraniteMoeHybridParallelExperts):\n             module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, GraniteMoeHybridRMSNorm):\n-            module.weight.data.fill_(1.0)\n-        elif isinstance(module, GraniteMoeHybridParallelExperts):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n-        # Initialize Mamba modules\n-        if isinstance(module, (nn.Conv1d)):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, GraniteMoeHybridMambaLayer):\n+        if isinstance(module, GraniteMoeHybridMambaLayer):\n             module.dt_bias.data.fill_(1.0)\n             module.A_log.data = torch.log(torch.arange(1, module.num_heads + 1))\n             module.D.data.fill_(1.0)"
        },
        {
            "sha": "4274af1b192075980e2bfbefd73f2e13ad061346",
            "filename": "src/transformers/models/granitemoehybrid/modular_granitemoehybrid.py",
            "status": "modified",
            "additions": 2,
            "deletions": 7,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodular_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodular_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodular_granitemoehybrid.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -167,13 +167,8 @@ class GraniteMoeHybridPreTrainedModel(GraniteMoeSharedPreTrainedModel):\n     _is_stateful = True\n \n     def _init_weights(self, module):\n-        super()._init_weights()\n-        # Initialize Mamba modules\n-        if isinstance(module, (nn.Conv1d)):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, GraniteMoeHybridMambaLayer):\n+        super()._init_weights(module)\n+        if isinstance(module, GraniteMoeHybridMambaLayer):\n             module.dt_bias.data.fill_(1.0)\n             module.A_log.data = torch.log(torch.arange(1, module.num_heads + 1))\n             module.D.data.fill_(1.0)"
        },
        {
            "sha": "9afb268a49ac97ebdac7bc32f3a7a7906d01fe57",
            "filename": "src/transformers/models/granitemoeshared/modeling_granitemoeshared.py",
            "status": "modified",
            "additions": 2,
            "deletions": 11,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -513,17 +513,8 @@ class GraniteMoeSharedPreTrainedModel(PreTrainedModel):\n     _supports_static_cache = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n \n     def _init_weights(self, module):\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, GraniteMoeSharedRMSNorm):\n-            module.weight.data.fill_(1.0)\n-        elif isinstance(module, GraniteMoeSharedParallelExperts):\n+        super()._init_weights(module)\n+        if isinstance(module, GraniteMoeSharedParallelExperts):\n             module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n \n "
        },
        {
            "sha": "83fa413e5a878d1e1bebe1b0c89a46c6a122a89d",
            "filename": "src/transformers/models/helium/modeling_helium.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -323,19 +323,6 @@ class HeliumPreTrainedModel(PreTrainedModel):\n         \"attentions\": HeliumAttention,\n     }\n \n-    def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, HeliumRMSNorm):\n-            module.weight.data.fill_(1.0)\n-\n \n @auto_docstring\n class HeliumModel(HeliumPreTrainedModel):"
        },
        {
            "sha": "16dfeb77c209457ef4d2b36033fa124db716f1b5",
            "filename": "src/transformers/models/hgnet_v2/modeling_hgnet_v2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2Fmodeling_hgnet_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2Fmodeling_hgnet_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2Fmodeling_hgnet_v2.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -45,16 +45,6 @@ class HGNetV2PreTrainedModel(PreTrainedModel):\n     main_input_name = \"pixel_values\"\n     _no_split_modules = [\"HGNetV2BasicLayer\"]\n \n-    def _init_weights(self, module):\n-        if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.BatchNorm2d):\n-            module.weight.data.fill_(1.0)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-\n \n class HGNetV2LearnableAffineBlock(nn.Module):\n     def __init__(self, scale_value: float = 1.0, bias_value: float = 0.0):"
        },
        {
            "sha": "f0c90ce0a63e52b7f4d1122f7674eedb0759fa9d",
            "filename": "src/transformers/models/hgnet_v2/modular_hgnet_v2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2Fmodular_hgnet_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2Fmodular_hgnet_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2Fmodular_hgnet_v2.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -170,16 +170,6 @@ class HGNetV2PreTrainedModel(PreTrainedModel):\n     main_input_name = \"pixel_values\"\n     _no_split_modules = [\"HGNetV2BasicLayer\"]\n \n-    def _init_weights(self, module):\n-        if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.BatchNorm2d):\n-            module.weight.data.fill_(1.0)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-\n \n class HGNetV2LearnableAffineBlock(nn.Module):\n     def __init__(self, scale_value: float = 1.0, bias_value: float = 0.0):"
        },
        {
            "sha": "f62417358c82ce0412336f86f2c9d32d1c019de9",
            "filename": "src/transformers/models/informer/configuration_informer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Finformer%2Fconfiguration_informer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Finformer%2Fconfiguration_informer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finformer%2Fconfiguration_informer.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -132,6 +132,7 @@ class InformerConfig(PretrainedConfig):\n         \"hidden_size\": \"d_model\",\n         \"num_attention_heads\": \"encoder_attention_heads\",\n         \"num_hidden_layers\": \"encoder_layers\",\n+        \"initializer_range\": \"init_std\",\n     }\n \n     def __init__("
        },
        {
            "sha": "34db267e7f499a8bb6f2f8db22e426e759dda046",
            "filename": "src/transformers/models/informer/modeling_informer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 13,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -257,20 +257,9 @@ class InformerPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n \n     def _init_weights(self, module: nn.Module):\n-        std = self.config.init_std\n-        if isinstance(module, (nn.Linear, nn.Conv1d)):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, InformerSinusoidalPositionalEmbedding):\n+        super()._init_weights(module)\n+        if isinstance(module, InformerSinusoidalPositionalEmbedding):\n             module._init_weight()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.weight.data.fill_(1.0)\n-            module.bias.data.zero_()\n \n     # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n     def _update_full_mask("
        },
        {
            "sha": "84ddd83caaa329dabe690989df0e30a03486925c",
            "filename": "src/transformers/models/informer/modular_informer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 13,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Finformer%2Fmodular_informer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Finformer%2Fmodular_informer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finformer%2Fmodular_informer.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -98,20 +98,9 @@ class InformerPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n \n     def _init_weights(self, module: nn.Module):\n-        std = self.config.init_std\n-        if isinstance(module, (nn.Linear, nn.Conv1d)):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, InformerSinusoidalPositionalEmbedding):\n+        super()._init_weights(module)\n+        if isinstance(module, InformerSinusoidalPositionalEmbedding):\n             module._init_weight()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.weight.data.fill_(1.0)\n-            module.bias.data.zero_()\n \n     # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n     def _update_full_mask("
        },
        {
            "sha": "55b9b6dcfe4d219d4b7b8b714557529964b78282",
            "filename": "src/transformers/models/internvl/modeling_internvl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 25,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -185,20 +185,8 @@ class InternVLVisionPreTrainedModel(PreTrainedModel):\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, (nn.Linear, nn.Conv2d, nn.ConvTranspose2d)):\n-            # Slightly different from the TF version which uses truncated_normal for initialization\n-            # cf https://github.com/pytorch/pytorch/pull/5617\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n-        elif isinstance(module, InternVLVisionEmbeddings):\n+        super()._init_weights(module)\n+        if isinstance(module, InternVLVisionEmbeddings):\n             module.cls_token.data.zero_()\n             if module.mask_token is not None:\n                 module.mask_token.data.zero_()\n@@ -528,17 +516,6 @@ class InternVLPreTrainedModel(PreTrainedModel):\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n \n-    def _init_weights(self, module):\n-        std = getattr(self.config, \"initializer_range\", self.config.get_text_config().initializer_range)\n-\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n-\n \n class InternVLMultiModalProjector(nn.Module):\n     def __init__(self, config: InternVLConfig):"
        },
        {
            "sha": "67d864eec7c96ad073c536f979ce424919d6bf18",
            "filename": "src/transformers/models/internvl/modular_internvl.py",
            "status": "modified",
            "additions": 3,
            "deletions": 24,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -147,20 +147,8 @@ class InternVLVisionPreTrainedModel(PreTrainedModel):\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, (nn.Linear, nn.Conv2d, nn.ConvTranspose2d)):\n-            # Slightly different from the TF version which uses truncated_normal for initialization\n-            # cf https://github.com/pytorch/pytorch/pull/5617\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n-        elif isinstance(module, InternVLVisionEmbeddings):\n+        super()._init_weights(module)\n+        if isinstance(module, InternVLVisionEmbeddings):\n             module.cls_token.data.zero_()\n             if module.mask_token is not None:\n                 module.mask_token.data.zero_()\n@@ -466,16 +454,7 @@ def forward(\n \n \n class InternVLPreTrainedModel(LlavaPreTrainedModel):\n-    def _init_weights(self, module):\n-        std = getattr(self.config, \"initializer_range\", self.config.get_text_config().initializer_range)\n-\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+    pass\n \n \n INTERNVL_INPUTS_DOCSTRING = None"
        },
        {
            "sha": "a0e758fd9ed471089065e8502f99d084a5a09b57",
            "filename": "src/transformers/models/janus/configuration_janus.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fjanus%2Fconfiguration_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fjanus%2Fconfiguration_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fconfiguration_janus.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -311,6 +311,7 @@ def __init__(\n                 f\" Type found: {type(vq_config)}\"\n             )\n \n+        self.initializer_range = self.vision_config.initializer_range\n         # This dimension is required when decoding discrete image tokens to continuous input.\n         self.vq_config.num_patches = self.vision_config.image_size // self.vision_config.patch_size\n         # The default is only the index for the 1B model, 7B uses a different one"
        },
        {
            "sha": "0553326af172cfaba97ebcdfaaef45ebd9f39278",
            "filename": "src/transformers/models/janus/modeling_janus.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -66,24 +66,6 @@ class JanusPreTrainedModel(PreTrainedModel):\n     _supports_static_cache = True\n     _supports_param_buffer_assignment = False\n \n-    def _init_weights(self, module):\n-        std = (\n-            self.config.vision_config.initializer_range\n-            if hasattr(self.config, \"vision_config\")\n-            else self.config.initializer_range\n-        )\n-        if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, (nn.GroupNorm, nn.LayerNorm)):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-\n \n @dataclass\n @auto_docstring("
        },
        {
            "sha": "69b9293374551c2555fb5c54424f93f2adf0c7a5",
            "filename": "src/transformers/models/janus/modular_janus.py",
            "status": "modified",
            "additions": 1,
            "deletions": 18,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -373,6 +373,7 @@ def __init__(\n                 f\" Type found: {type(vq_config)}\"\n             )\n \n+        self.initializer_range = self.vision_config.initializer_range\n         # This dimension is required when decoding discrete image tokens to continuous input.\n         self.vq_config.num_patches = self.vision_config.image_size // self.vision_config.patch_size\n         # The default is only the index for the 1B model, 7B uses a different one\n@@ -393,24 +394,6 @@ class JanusPreTrainedModel(PreTrainedModel):\n     _supports_static_cache = True\n     _supports_param_buffer_assignment = False\n \n-    def _init_weights(self, module):\n-        std = (\n-            self.config.vision_config.initializer_range\n-            if hasattr(self.config, \"vision_config\")\n-            else self.config.initializer_range\n-        )\n-        if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, (nn.GroupNorm, nn.LayerNorm)):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-\n \n @dataclass\n @auto_docstring("
        },
        {
            "sha": "21c3d417d03230a3438c56c9aa5024bebffb45d0",
            "filename": "src/transformers/models/lfm2/modeling_lfm2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -549,19 +549,6 @@ class Lfm2PreTrainedModel(PreTrainedModel):\n         \"attentions\": Lfm2Attention,\n     }\n \n-    def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, (nn.Linear, nn.Conv1d)):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, Lfm2RMSNorm):\n-            module.weight.data.fill_(1.0)\n-\n \n @auto_docstring\n class Lfm2Model(Lfm2PreTrainedModel):"
        },
        {
            "sha": "1cdb15d370b82efc4aeb9ad9bd82785219d12717",
            "filename": "src/transformers/models/lfm2/modular_lfm2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodular_lfm2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodular_lfm2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodular_lfm2.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -403,19 +403,6 @@ def forward(\n class Lfm2PreTrainedModel(LlamaPreTrainedModel):\n     _supports_static_cache = False\n \n-    def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, (nn.Linear, nn.Conv1d)):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, Lfm2RMSNorm):\n-            module.weight.data.fill_(1.0)\n-\n \n class Lfm2Model(LlamaModel):\n     def __init__(self, config: Lfm2Config):"
        },
        {
            "sha": "9c768e80af38900c6da4b58ec20d18be20203e37",
            "filename": "src/transformers/models/lightglue/modeling_lightglue.py",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodeling_lightglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodeling_lightglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodeling_lightglue.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -426,16 +426,6 @@ class LightGluePreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n \n-    def _init_weights(self, module: nn.Module) -> None:\n-        \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n-\n \n def get_matches_from_scores(scores: torch.Tensor, threshold: float) -> tuple[torch.Tensor, torch.Tensor]:\n     \"\"\"obtain matches from a score matrix [Bx M+1 x N+1]\"\"\""
        },
        {
            "sha": "d90708e6bd71a4a6970360d9a0fc72bc20a500f7",
            "filename": "src/transformers/models/lightglue/modular_lightglue.py",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodular_lightglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodular_lightglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodular_lightglue.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -511,16 +511,6 @@ class LightGluePreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n \n-    def _init_weights(self, module: nn.Module) -> None:\n-        \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n-\n \n def get_matches_from_scores(scores: torch.Tensor, threshold: float) -> tuple[torch.Tensor, torch.Tensor]:\n     \"\"\"obtain matches from a score matrix [Bx M+1 x N+1]\"\"\""
        },
        {
            "sha": "9d194da3caf225319f94f992f1d599f9666e1deb",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -322,19 +322,6 @@ class LlamaPreTrainedModel(PreTrainedModel):\n         \"attentions\": LlamaAttention,\n     }\n \n-    def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, LlamaRMSNorm):\n-            module.weight.data.fill_(1.0)\n-\n \n @auto_docstring\n class LlamaModel(LlamaPreTrainedModel):"
        },
        {
            "sha": "56f056023bcddd41ae62f93980a7c92ba146608d",
            "filename": "src/transformers/models/llava/modeling_llava.py",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -125,20 +125,6 @@ class LlavaPreTrainedModel(PreTrainedModel):\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n \n-    def _init_weights(self, module):\n-        # important: this ported version of Llava isn't meant for training from scratch - only\n-        # inference and fine-tuning - so the proper init weights code has been removed - the original codebase\n-        # https://github.com/haotian-liu/LLaVA/tree/main/llava should serve for that purpose\n-        std = getattr(self.config, \"initializer_range\", self.config.get_text_config().initializer_range)\n-\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.weight.data.fill_(1.0)\n-            module.bias.data.zero_()\n-\n \n @auto_docstring(\n     custom_intro=\"\"\""
        },
        {
            "sha": "da427b8b7ede60a72c2ec2708c1f6f91bb8d0f99",
            "filename": "src/transformers/models/minimax/modeling_minimax.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -597,19 +597,6 @@ class MiniMaxPreTrainedModel(PreTrainedModel):\n         \"attentions\": [MiniMaxAttention, MiniMaxLightningAttention],\n     }\n \n-    def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, MiniMaxRMSNorm):\n-            module.weight.data.fill_(1.0)\n-\n \n class MiniMaxRotaryEmbedding(nn.Module):\n     def __init__(self, config: MiniMaxConfig, device=None):"
        },
        {
            "sha": "4e0d30d89aa82e7308bcf453f76e395ceb8ae9a1",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -267,19 +267,6 @@ class MistralPreTrainedModel(PreTrainedModel):\n         \"attentions\": MistralAttention,\n     }\n \n-    def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, MistralRMSNorm):\n-            module.weight.data.fill_(1.0)\n-\n \n class MistralRotaryEmbedding(nn.Module):\n     def __init__(self, config: MistralConfig, device=None):"
        },
        {
            "sha": "cf466576277d7b67592c3d6683e12457273d4d8a",
            "filename": "src/transformers/models/mistral3/modeling_mistral3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 16,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -190,22 +190,6 @@ class Mistral3PreTrainedModel(PreTrainedModel):\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n \n-    def _init_weights(self, module):\n-        # important: this ported version of Mistral3 isn't meant for training from scratch - only\n-        # inference and fine-tuning - so the proper init weights code has been removed - the original codebase\n-        # https://github.com/haotian-liu/Mistral3/tree/main/mistral3 should serve for that purpose\n-        std = getattr(self.config, \"initializer_range\", self.config.get_text_config().initializer_range)\n-\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.weight.data.fill_(1.0)\n-            module.bias.data.zero_()\n-        elif isinstance(module, Mistral3RMSNorm):\n-            module.weight.data.fill_(1.0)\n-\n \n @auto_docstring(\n     custom_intro=\"\"\""
        },
        {
            "sha": "da507655fa9127dc6da6383a5b5c683db8b504a1",
            "filename": "src/transformers/models/mistral3/modular_mistral3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 15,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodular_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodular_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodular_mistral3.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -115,21 +115,7 @@ class Mistral3ModelOutputWithPast(LlavaModelOutputWithPast):\n \n \n class Mistral3PreTrainedModel(LlavaPreTrainedModel):\n-    def _init_weights(self, module):\n-        # important: this ported version of Mistral3 isn't meant for training from scratch - only\n-        # inference and fine-tuning - so the proper init weights code has been removed - the original codebase\n-        # https://github.com/haotian-liu/Mistral3/tree/main/mistral3 should serve for that purpose\n-        std = getattr(self.config, \"initializer_range\", self.config.get_text_config().initializer_range)\n-\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.weight.data.fill_(1.0)\n-            module.bias.data.zero_()\n-        elif isinstance(module, Mistral3RMSNorm):\n-            module.weight.data.fill_(1.0)\n+    pass\n \n \n class Mistral3Model(LlavaModel):"
        },
        {
            "sha": "2b700467d0fc6a60e33ad29637545b14f9d8bfab",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -396,19 +396,6 @@ class MixtralPreTrainedModel(PreTrainedModel):\n         \"attentions\": MixtralAttention,\n     }\n \n-    def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, MixtralRMSNorm):\n-            module.weight.data.fill_(1.0)\n-\n \n @auto_docstring\n class MixtralModel(MixtralPreTrainedModel):"
        },
        {
            "sha": "1143ee87ec15ebaca313ffeb376d83388bd1f3de",
            "filename": "src/transformers/models/moonshine/modeling_moonshine.py",
            "status": "modified",
            "additions": 0,
            "deletions": 15,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -465,21 +465,6 @@ class MoonshinePreTrainedModel(PreTrainedModel):\n     _supports_static_cache = True\n     # TODO arthur, how do we separate when it cross / self coming from different layer?\n \n-    def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, (nn.Linear, nn.Conv1d)):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, (nn.GroupNorm, nn.LayerNorm)):\n-            module.weight.data.fill_(1.0)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-\n     def _get_feat_extract_output_lengths(self, input_lengths: torch.LongTensor):\n         \"\"\"\n         Computes the output length of the convolutional layers"
        },
        {
            "sha": "46193793526298124fb36b8e41edadb6404dfae5",
            "filename": "src/transformers/models/moonshine/modular_moonshine.py",
            "status": "modified",
            "additions": 0,
            "deletions": 15,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -500,21 +500,6 @@ class MoonshinePreTrainedModel(PreTrainedModel):\n     _supports_static_cache = True\n     # TODO arthur, how do we separate when it cross / self coming from different layer?\n \n-    def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, (nn.Linear, nn.Conv1d)):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, (nn.GroupNorm, nn.LayerNorm)):\n-            module.weight.data.fill_(1.0)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-\n     def _get_feat_extract_output_lengths(self, input_lengths: torch.LongTensor):\n         \"\"\"\n         Computes the output length of the convolutional layers"
        },
        {
            "sha": "878cc122f17d31d7d0bea3d9218663cfc585af9a",
            "filename": "src/transformers/models/musicgen/configuration_musicgen.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fconfiguration_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fconfiguration_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fconfiguration_musicgen.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -214,6 +214,7 @@ def __init__(self, **kwargs):\n         self.audio_encoder = AutoConfig.for_model(audio_encoder_model_type, **audio_encoder_config)\n         self.decoder = MusicgenDecoderConfig(**decoder_config)\n         self.is_encoder_decoder = True\n+        self.initializer_factor = self.decoder.initializer_factor\n \n     @classmethod\n     def from_sub_models_config("
        },
        {
            "sha": "cd76c87162921b9defc319344e0de23ae73e9b8c",
            "filename": "src/transformers/models/musicgen/modeling_musicgen.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -1333,14 +1333,11 @@ def generate(\n     The composite MusicGen model with a text encoder, audio encoder and Musicgen decoder,\n     \"\"\"\n )\n-class MusicgenForConditionalGeneration(PreTrainedModel, GenerationMixin):\n+class MusicgenForConditionalGeneration(MusicgenPreTrainedModel, GenerationMixin):\n     config: MusicgenConfig\n     base_model_prefix = \"encoder_decoder\"\n     main_input_name = \"input_ids\"\n     supports_gradient_checkpointing = True\n-    _supports_flash_attn = True\n-    _supports_sdpa = True\n-    _supports_flex_attn = True\n \n     def __init__(\n         self,"
        },
        {
            "sha": "2a4756e30732a14c082818d0e37544a405aadd96",
            "filename": "src/transformers/models/olmo/modeling_olmo.py",
            "status": "modified",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -301,17 +301,6 @@ class OlmoPreTrainedModel(PreTrainedModel):\n         \"attentions\": OlmoAttention,\n     }\n \n-    def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-\n \n @auto_docstring\n class OlmoModel(OlmoPreTrainedModel):"
        },
        {
            "sha": "6bc439888eb1879ea5db4da896d1bd5069af47c3",
            "filename": "src/transformers/models/olmo/modular_olmo.py",
            "status": "modified",
            "additions": 5,
            "deletions": 15,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Folmo%2Fmodular_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Folmo%2Fmodular_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodular_olmo.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -14,7 +14,6 @@\n     LlamaForCausalLM,\n     LlamaMLP,\n     LlamaModel,\n-    LlamaPreTrainedModel,\n     LlamaRotaryEmbedding,\n     eager_attention_forward,\n     rotate_half,\n@@ -153,19 +152,6 @@ def forward(self, x, position_ids):\n             return cos, sin\n \n \n-class OlmoPreTrainedModel(LlamaPreTrainedModel):\n-    def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-\n-\n class OlmoModel(LlamaModel):\n     def __init__(self, config: OlmoConfig):\n         super().__init__(config)\n@@ -179,4 +165,8 @@ class OlmoForCausalLM(LlamaForCausalLM):\n     pass\n \n \n-__all__ = [\"OlmoForCausalLM\", \"OlmoModel\", \"OlmoPreTrainedModel\"]\n+__all__ = [\n+    \"OlmoForCausalLM\",\n+    \"OlmoModel\",\n+    \"OlmoPreTrainedModel\",  # noqa: F822\n+]"
        },
        {
            "sha": "097b7a8d1650f2207ccafc24d0cafc0a78bcd10c",
            "filename": "src/transformers/models/olmo2/modeling_olmo2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -306,19 +306,6 @@ class Olmo2PreTrainedModel(PreTrainedModel):\n         \"attentions\": Olmo2Attention,\n     }\n \n-    def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, Olmo2RMSNorm):\n-            module.weight.data.fill_(1.0)\n-\n \n @auto_docstring\n class Olmo2Model(Olmo2PreTrainedModel):"
        },
        {
            "sha": "37e1bed147fb667b1d7265d0dfdd3fa5108e8c6a",
            "filename": "src/transformers/models/perception_lm/modeling_perception_lm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodeling_perception_lm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodeling_perception_lm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodeling_perception_lm.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -99,20 +99,6 @@ class PerceptionLMPreTrainedModel(PreTrainedModel):\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n \n-    def _init_weights(self, module):\n-        # important: this ported version of PerceptionLM isn't meant for training from scratch - only\n-        # inference and fine-tuning - so the proper init weights code has been removed - the original codebase\n-        # https://github.com/haotian-liu/PerceptionLM/tree/main/perception_lm should serve for that purpose\n-        std = getattr(self.config, \"initializer_range\", self.config.get_text_config().initializer_range)\n-\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.weight.data.fill_(1.0)\n-            module.bias.data.zero_()\n-\n \n @dataclass\n @auto_docstring("
        },
        {
            "sha": "dcef7cdcf44afa62505b03b777c7c245b7ef6ac1",
            "filename": "src/transformers/models/phi/modeling_phi.py",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -306,20 +306,6 @@ class PhiPreTrainedModel(PreTrainedModel):\n         \"attentions\": PhiAttention,\n     }\n \n-    def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.weight.data.fill_(1.0)\n-            module.bias.data.zero_()\n-\n \n @auto_docstring\n class PhiModel(PhiPreTrainedModel):"
        },
        {
            "sha": "f197abf129897d87bad2f076265ba44ca56a1ba6",
            "filename": "src/transformers/models/phi/modular_phi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 18,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fphi%2Fmodular_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fphi%2Fmodular_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodular_phi.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -19,7 +19,6 @@\n     LlamaForSequenceClassification,\n     LlamaForTokenClassification,\n     LlamaModel,\n-    LlamaPreTrainedModel,\n     LlamaRotaryEmbedding,\n     apply_rotary_pos_emb,\n     eager_attention_forward,  # copied from Llama\n@@ -169,22 +168,6 @@ class PhiRotaryEmbedding(LlamaRotaryEmbedding):\n     pass\n \n \n-class PhiPreTrainedModel(LlamaPreTrainedModel):\n-    def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.weight.data.fill_(1.0)\n-            module.bias.data.zero_()\n-\n-\n class PhiModel(LlamaModel):\n     def __init__(self, config: PhiConfig):\n         super().__init__(config)\n@@ -307,7 +290,7 @@ class PhiForTokenClassification(LlamaForTokenClassification):\n \n \n __all__ = [\n-    \"PhiPreTrainedModel\",\n+    \"PhiPreTrainedModel\",  # noqa: F822\n     \"PhiModel\",\n     \"PhiForCausalLM\",\n     \"PhiForSequenceClassification\","
        },
        {
            "sha": "32e1f2e0aaff94d2d9142afe52eabcfbf2927737",
            "filename": "src/transformers/models/phi3/modeling_phi3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -299,19 +299,6 @@ class Phi3PreTrainedModel(PreTrainedModel):\n     }\n     _version = \"0.0.5\"\n \n-    def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, Phi3RMSNorm):\n-            module.weight.data.fill_(1.0)\n-\n \n class Phi3RotaryEmbedding(nn.Module):\n     def __init__(self, config: Phi3Config, device=None):"
        },
        {
            "sha": "a984a7957caf2f96bc808b5196eb7ac8dbd7b31c",
            "filename": "src/transformers/models/phi4_multimodal/modeling_phi4_multimodal.py",
            "status": "modified",
            "additions": 4,
            "deletions": 25,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -1000,19 +1000,8 @@ class Phi4MultimodalAudioPreTrainedModel(PreTrainedModel):\n     _supports_flex_attn = True\n \n     def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, (nn.Linear, nn.Conv1d, nn.Conv2d)):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n-        elif isinstance(module, Phi4MultimodalAudioGluPointWiseConv):\n+        super()._init_weights(module)\n+        if isinstance(module, Phi4MultimodalAudioGluPointWiseConv):\n             module.b1.data.zero_()\n             module.b2.data.zero_()\n \n@@ -1602,18 +1591,8 @@ class Phi4MultimodalPreTrainedModel(PreTrainedModel):\n     _version = \"0.0.5\"\n \n     def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, Phi4MultimodalRMSNorm):\n-            module.weight.data.fill_(1.0)\n-        elif isinstance(module, Phi4MultimodalImageEmbedding):\n+        super()._init_weights(module)\n+        if isinstance(module, Phi4MultimodalImageEmbedding):\n             module.global_img_feature_extensor.data.zero_()\n             module.sub_img_feature_extensor.data.zero_()\n "
        },
        {
            "sha": "7208cab16f16546145bb542c3ca0d7cd8b5e31e0",
            "filename": "src/transformers/models/phi4_multimodal/modular_phi4_multimodal.py",
            "status": "modified",
            "additions": 4,
            "deletions": 25,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -1125,19 +1125,8 @@ class Phi4MultimodalAudioPreTrainedModel(PreTrainedModel):\n     _supports_flex_attn = True\n \n     def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, (nn.Linear, nn.Conv1d, nn.Conv2d)):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n-        elif isinstance(module, Phi4MultimodalAudioGluPointWiseConv):\n+        super()._init_weights(module)\n+        if isinstance(module, Phi4MultimodalAudioGluPointWiseConv):\n             module.b1.data.zero_()\n             module.b2.data.zero_()\n \n@@ -1460,18 +1449,8 @@ class Phi4MultimodalRotaryEmbedding(Phi3RotaryEmbedding):\n \n class Phi4MultimodalPreTrainedModel(Phi3PreTrainedModel):\n     def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, Phi4MultimodalRMSNorm):\n-            module.weight.data.fill_(1.0)\n-        elif isinstance(module, Phi4MultimodalImageEmbedding):\n+        Phi3PreTrainedModel._init_weights(module)\n+        if isinstance(module, Phi4MultimodalImageEmbedding):\n             module.global_img_feature_extensor.data.zero_()\n             module.sub_img_feature_extensor.data.zero_()\n "
        },
        {
            "sha": "a4aaa3ff3703407bd86abe1af61d621aea326025",
            "filename": "src/transformers/models/plbart/configuration_plbart.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fplbart%2Fconfiguration_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fplbart%2Fconfiguration_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fplbart%2Fconfiguration_plbart.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -101,7 +101,11 @@ class PLBartConfig(PretrainedConfig):\n \n     model_type = \"plbart\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n-    attribute_map = {\"num_attention_heads\": \"encoder_attention_heads\", \"hidden_size\": \"d_model\"}\n+    attribute_map = {\n+        \"num_attention_heads\": \"encoder_attention_heads\",\n+        \"hidden_size\": \"d_model\",\n+        \"initializer_range\": \"init_std\",\n+    }\n \n     def __init__(\n         self,"
        },
        {
            "sha": "541178bd38cde9c1f9deebf9c6dce3c05566f2ed",
            "filename": "src/transformers/models/plbart/modeling_plbart.py",
            "status": "modified",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -80,17 +80,6 @@ class PLBartPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_flex_attn = True\n \n-    def _init_weights(self, module):\n-        std = self.config.init_std\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-\n     # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n     def _update_full_mask(\n         self,"
        },
        {
            "sha": "c0fcdea7afcf20709a56d1171bb2ceb9a54336e6",
            "filename": "src/transformers/models/plbart/modular_plbart.py",
            "status": "modified",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodular_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodular_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodular_plbart.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -66,17 +66,6 @@ class PLBartPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_flex_attn = True\n \n-    def _init_weights(self, module):\n-        std = self.config.init_std\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-\n     # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n     def _update_full_mask(\n         self,"
        },
        {
            "sha": "986e6e7e75179979ddea2cb1662afa6f07b6ae62",
            "filename": "src/transformers/models/prompt_depth_anything/modeling_prompt_depth_anything.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fmodeling_prompt_depth_anything.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fmodeling_prompt_depth_anything.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fmodeling_prompt_depth_anything.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -245,13 +245,6 @@ class PromptDepthAnythingPreTrainedModel(PreTrainedModel):\n     main_input_name = \"pixel_values\"\n     supports_gradient_checkpointing = True\n \n-    def _init_weights(self, module):\n-        \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, (nn.Conv2d, nn.ConvTranspose2d)):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-\n \n class PromptDepthAnythingReassembleLayer(nn.Module):\n     def __init__(self, config: PromptDepthAnythingConfig, channels: int, factor: int):"
        },
        {
            "sha": "988143000ca13c78b8890a976196924c81aaf6e1",
            "filename": "src/transformers/models/prompt_depth_anything/modular_prompt_depth_anything.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fmodular_prompt_depth_anything.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fmodular_prompt_depth_anything.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fmodular_prompt_depth_anything.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -164,13 +164,6 @@ class PromptDepthAnythingPreTrainedModel(PreTrainedModel):\n     main_input_name = \"pixel_values\"\n     supports_gradient_checkpointing = True\n \n-    def _init_weights(self, module):\n-        \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, (nn.Conv2d, nn.ConvTranspose2d)):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-\n \n class PromptDepthAnythingReassembleLayer(nn.Module):\n     def __init__(self, config: PromptDepthAnythingConfig, channels: int, factor: int):"
        },
        {
            "sha": "95b08ce24e45e6774c296767cc8303d54235dede",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -270,19 +270,6 @@ class Qwen2PreTrainedModel(PreTrainedModel):\n         \"attentions\": Qwen2Attention,\n     }\n \n-    def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, Qwen2RMSNorm):\n-            module.weight.data.fill_(1.0)\n-\n \n class Qwen2RotaryEmbedding(nn.Module):\n     def __init__(self, config: Qwen2Config, device=None):"
        },
        {
            "sha": "b8da2d7846ee336b06c30555ff153f2ff6743dca",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 20,
            "deletions": 41,
            "changes": 61,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -58,26 +58,6 @@\n logger = logging.get_logger(__name__)\n \n \n-class Qwen2RMSNorm(nn.Module):\n-    def __init__(self, hidden_size, eps=1e-6):\n-        \"\"\"\n-        Qwen2RMSNorm is equivalent to T5LayerNorm\n-        \"\"\"\n-        super().__init__()\n-        self.weight = nn.Parameter(torch.ones(hidden_size))\n-        self.variance_epsilon = eps\n-\n-    def forward(self, hidden_states):\n-        input_dtype = hidden_states.dtype\n-        hidden_states = hidden_states.to(torch.float32)\n-        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n-        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n-        return self.weight * hidden_states.to(input_dtype)\n-\n-    def extra_repr(self):\n-        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n-\n-\n @auto_docstring\n class Qwen2_5OmniPreTrainedModel(PreTrainedModel):\n     config: Qwen2_5OmniConfig\n@@ -90,27 +70,6 @@ class Qwen2_5OmniPreTrainedModel(PreTrainedModel):\n     _supports_static_cache = False\n     _supports_attention_backend = True\n \n-    def _init_weights(self, module):\n-        # important: this ported version of Qwen2.5OmniThinker isn't meant for training from scratch - only\n-        # inference and fine-tuning - so the proper init weights code has been removed\n-        std = self.config.initializer_range if hasattr(self.config, \"initializer_range\") else 0.02\n-\n-        if isinstance(module, (nn.Linear, nn.Conv1d, nn.Conv3d, nn.ConvTranspose1d)):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            if module.weight is not None:\n-                module.weight.data.fill_(1.0)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, Qwen2RMSNorm):\n-            module.weight.data.fill_(1.0)\n-\n \n class Qwen2_5OmniPreTrainedModelForConditionalGeneration(Qwen2_5OmniPreTrainedModel):\n     def _prepare_4d_causal_attention_mask_with_cache_position(\n@@ -1026,6 +985,26 @@ def forward(self, hidden_state):\n         return self.down_proj(self.act_fn(self.gate_proj(hidden_state)) * self.up_proj(hidden_state))\n \n \n+class Qwen2RMSNorm(nn.Module):\n+    def __init__(self, hidden_size, eps=1e-6):\n+        \"\"\"\n+        Qwen2RMSNorm is equivalent to T5LayerNorm\n+        \"\"\"\n+        super().__init__()\n+        self.weight = nn.Parameter(torch.ones(hidden_size))\n+        self.variance_epsilon = eps\n+\n+    def forward(self, hidden_states):\n+        input_dtype = hidden_states.dtype\n+        hidden_states = hidden_states.to(torch.float32)\n+        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n+        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n+        return self.weight * hidden_states.to(input_dtype)\n+\n+    def extra_repr(self):\n+        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n+\n+\n class Qwen2_5OmniVisionBlock(GradientCheckpointingLayer):\n     def __init__(self, config: Qwen2_5OmniVisionEncoderConfig) -> None:\n         super().__init__()"
        },
        {
            "sha": "f899d0f8c47b0da18fc5a3c6e395cb2bfec87e40",
            "filename": "src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py",
            "status": "modified",
            "additions": 0,
            "deletions": 22,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -35,7 +35,6 @@\n     Qwen2_5_VLPreTrainedModel,\n     Qwen2_5_VLTextModel,\n     Qwen2_5_VLVisionBlock,\n-    Qwen2RMSNorm,\n     eager_attention_forward,\n )\n from transformers.models.qwen2_audio.configuration_qwen2_audio import Qwen2AudioEncoderConfig\n@@ -1136,27 +1135,6 @@ class Qwen2_5OmniPreTrainedModel(Qwen2_5_VLPreTrainedModel):\n     config: Qwen2_5OmniConfig\n     _supports_static_cache = False\n \n-    def _init_weights(self, module):\n-        # important: this ported version of Qwen2.5OmniThinker isn't meant for training from scratch - only\n-        # inference and fine-tuning - so the proper init weights code has been removed\n-        std = self.config.initializer_range if hasattr(self.config, \"initializer_range\") else 0.02\n-\n-        if isinstance(module, (nn.Linear, nn.Conv1d, nn.Conv3d, nn.ConvTranspose1d)):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            if module.weight is not None:\n-                module.weight.data.fill_(1.0)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, Qwen2RMSNorm):\n-            module.weight.data.fill_(1.0)\n-\n \n class Qwen2_5OmniPreTrainedModelForConditionalGeneration(Qwen2_5OmniPreTrainedModel):\n     def _prepare_4d_causal_attention_mask_with_cache_position("
        },
        {
            "sha": "d653b9de87a3aa8c977a629616beb6b84c70260e",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -329,19 +329,6 @@ class Qwen2_5_VLPreTrainedModel(PreTrainedModel):\n     _supports_static_cache = True\n     _supports_attention_backend = True\n \n-    def _init_weights(self, module):\n-        std = self.config.get_text_config().initializer_range\n-        if isinstance(module, (nn.Linear, nn.Conv3d)):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, Qwen2RMSNorm):\n-            module.weight.data.fill_(1.0)\n-\n \n class Qwen2_5_VisionTransformerPretrainedModel(Qwen2_5_VLPreTrainedModel):\n     config: Qwen2_5_VLVisionConfig"
        },
        {
            "sha": "7c4e7117a28f8f55fd72211e5640d9be522f509f",
            "filename": "src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 12,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -174,18 +174,7 @@ def forward(\n \n \n class Qwen2_5_VLPreTrainedModel(Qwen2VLPreTrainedModel):\n-    def _init_weights(self, module):\n-        std = self.config.get_text_config().initializer_range\n-        if isinstance(module, (nn.Linear, nn.Conv3d)):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, Qwen2RMSNorm):\n-            module.weight.data.fill_(1.0)\n+    pass\n \n \n class Qwen2_5_VisionTransformerPretrainedModel(Qwen2_5_VLPreTrainedModel):"
        },
        {
            "sha": "6c17e0c9c47b1427a5bf66493aba0ce546ec3c47",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 16,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -663,22 +663,6 @@ class Qwen2VLPreTrainedModel(PreTrainedModel):\n     _supports_static_cache = True\n     _supports_attention_backend = True\n \n-    def _init_weights(self, module):\n-        std = self.config.get_text_config().initializer_range\n-        if isinstance(module, (nn.Linear, nn.Conv3d)):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.weight.data.fill_(1.0)\n-            module.bias.data.zero_()\n-        elif isinstance(module, Qwen2RMSNorm):\n-            module.weight.data.fill_(1.0)\n-\n \n @auto_docstring\n class Qwen2VisionTransformerPretrainedModel(Qwen2VLPreTrainedModel):"
        },
        {
            "sha": "7cd5eb52b8e35a9ead2a055b59b200cae65537bd",
            "filename": "src/transformers/models/qwen3/modeling_qwen3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -296,19 +296,6 @@ class Qwen3PreTrainedModel(PreTrainedModel):\n         \"attentions\": Qwen3Attention,\n     }\n \n-    def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, Qwen3RMSNorm):\n-            module.weight.data.fill_(1.0)\n-\n \n class Qwen3RotaryEmbedding(nn.Module):\n     def __init__(self, config: Qwen3Config, device=None):"
        },
        {
            "sha": "b7151bba0509e2024c7c62eaa7ea1d704c3dfdb1",
            "filename": "src/transformers/models/qwen3_moe/modeling_qwen3_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -419,19 +419,6 @@ class Qwen3MoePreTrainedModel(PreTrainedModel):\n         \"attentions\": Qwen3MoeAttention,\n     }\n \n-    def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, Qwen3MoeRMSNorm):\n-            module.weight.data.fill_(1.0)\n-\n \n @auto_docstring\n class Qwen3MoeModel(Qwen3MoePreTrainedModel):"
        },
        {
            "sha": "ac030a4e866118781de9d2a9d2b02f7d8b390e1a",
            "filename": "src/transformers/models/sam/modeling_sam.py",
            "status": "modified",
            "additions": 2,
            "deletions": 13,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_sam.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -1017,19 +1017,8 @@ class SamPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n \n     def _init_weights(self, module: nn.Module):\n-        std = self.config.initializer_range\n-        if isinstance(module, (nn.Linear, nn.Conv2d, nn.ConvTranspose2d)):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, (SamLayerNorm, nn.LayerNorm)):\n-            module.weight.data.fill_(1.0)\n-            module.bias.data.zero_()\n-        elif isinstance(module, SamVisionAttention):\n+        super()._init_weights(module)\n+        if isinstance(module, SamVisionAttention):\n             if module.use_rel_pos:\n                 module.rel_pos_h.data.zero_()\n                 module.rel_pos_w.data.zero_()"
        },
        {
            "sha": "5c688e5089f2c47904c3c1be37a7fa2d64458a5e",
            "filename": "src/transformers/models/sam_hq/modeling_sam_hq.py",
            "status": "modified",
            "additions": 3,
            "deletions": 14,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodeling_sam_hq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodeling_sam_hq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodeling_sam_hq.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -482,20 +482,9 @@ class SamHQPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _supports_sdpa = True\n \n-    def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, (nn.Linear, nn.Conv2d, nn.ConvTranspose2d)):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, (SamHQLayerNorm, nn.LayerNorm)):\n-            module.weight.data.fill_(1.0)\n-            module.bias.data.zero_()\n-        elif isinstance(module, SamHQVisionAttention):\n+    def _init_weights(self, module: nn.Module):\n+        super()._init_weights(module)\n+        if isinstance(module, SamHQVisionAttention):\n             if module.use_rel_pos:\n                 module.rel_pos_h.data.zero_()\n                 module.rel_pos_w.data.zero_()"
        },
        {
            "sha": "2ea759664723215df955b70b74d645642090cd03",
            "filename": "src/transformers/models/sam_hq/modular_sam_hq.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodular_sam_hq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodular_sam_hq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodular_sam_hq.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -186,8 +186,7 @@ class SamHQVisionLayer(SamVisionLayer):\n \n \n class SamHQPreTrainedModel(SamPreTrainedModel):\n-    def _init_weights(self, module):\n-        super()._init_weights(module)\n+    pass\n \n \n class SamHQVisionEncoder(SamVisionEncoder, SamHQPreTrainedModel):"
        },
        {
            "sha": "c4287d1d235153e4eaf39ec9a1438303256d25b9",
            "filename": "src/transformers/models/smollm3/modeling_smollm3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodeling_smollm3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodeling_smollm3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodeling_smollm3.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -300,19 +300,6 @@ class SmolLM3PreTrainedModel(PreTrainedModel):\n         \"attentions\": SmolLM3Attention,\n     }\n \n-    def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, SmolLM3RMSNorm):\n-            module.weight.data.fill_(1.0)\n-\n \n class SmolLM3RotaryEmbedding(nn.Module):\n     def __init__(self, config: SmolLM3Config, device=None):"
        },
        {
            "sha": "a9d8f043c4671d3d8c7732fac214e77c5b3a77f0",
            "filename": "src/transformers/models/smolvlm/modeling_smolvlm.py",
            "status": "modified",
            "additions": 22,
            "deletions": 0,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -47,6 +47,26 @@\n logger = logging.get_logger(__name__)\n \n \n+class SmolVLMRMSNorm(nn.Module):\n+    def __init__(self, hidden_size, eps=1e-6):\n+        \"\"\"\n+        SmolVLMRMSNorm is equivalent to T5LayerNorm\n+        \"\"\"\n+        super().__init__()\n+        self.weight = nn.Parameter(torch.ones(hidden_size))\n+        self.variance_epsilon = eps\n+\n+    def forward(self, hidden_states):\n+        input_dtype = hidden_states.dtype\n+        hidden_states = hidden_states.to(torch.float32)\n+        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n+        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n+        return self.weight * hidden_states.to(input_dtype)\n+\n+    def extra_repr(self):\n+        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n+\n+\n @auto_docstring\n class SmolVLMPreTrainedModel(PreTrainedModel):\n     config: SmolVLMConfig\n@@ -74,6 +94,8 @@ def _init_weights(self, module):\n         elif isinstance(module, nn.LayerNorm):\n             module.weight.data.fill_(1.0)\n             module.bias.data.zero_()\n+        elif isinstance(module, SmolVLMRMSNorm):\n+            module.weight.data.fill_(1.0)\n \n \n class SmolVLMVisionEmbeddings(nn.Module):"
        },
        {
            "sha": "72d7c85877762f7f10fcb6e2f549b84a7df28a35",
            "filename": "src/transformers/models/smolvlm/modular_smolvlm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 14,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodular_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodular_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodular_smolvlm.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -95,20 +95,7 @@ class SmolVLMVisionConfig(Idefics3VisionConfig):\n \n \n class SmolVLMPreTrainedModel(Idefics3PreTrainedModel):\n-    def _init_weights(self, module):\n-        std = getattr(self.config, \"initializer_range\", self.config.get_text_config().initializer_range)\n-\n-        if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.weight.data.fill_(1.0)\n-            module.bias.data.zero_()\n+    pass\n \n \n class SmolVLMVisionTransformer(Idefics3VisionTransformer):"
        },
        {
            "sha": "87da8a3f7f2bc6d4314e20a2e11d631a81246b0e",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -304,20 +304,6 @@ class Starcoder2PreTrainedModel(PreTrainedModel):\n         \"attentions\": Starcoder2Attention,\n     }\n \n-    def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.weight.data.fill_(1.0)\n-            module.bias.data.zero_()\n-\n \n @auto_docstring\n class Starcoder2Model(Starcoder2PreTrainedModel):"
        },
        {
            "sha": "349ffb8acbe4720fd6d353456a8a70124a7936e2",
            "filename": "src/transformers/models/starcoder2/modular_starcoder2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 17,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodular_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodular_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodular_starcoder2.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -42,7 +42,6 @@\n     MistralForSequenceClassification,\n     MistralForTokenClassification,\n     MistralModel,\n-    MistralPreTrainedModel,\n     MistralRotaryEmbedding,\n     apply_rotary_pos_emb,\n     eager_attention_forward,\n@@ -141,22 +140,6 @@ class Starcoder2RotaryEmbedding(MistralRotaryEmbedding):\n     pass\n \n \n-class Starcoder2PreTrainedModel(MistralPreTrainedModel):\n-    def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.weight.data.fill_(1.0)\n-            module.bias.data.zero_()\n-\n-\n class Starcoder2Model(MistralModel):\n     def __init__(self, config: Starcoder2Config):\n         super().__init__(config)"
        },
        {
            "sha": "41ce05b2956ce14a1d2effa06ce451e10bd60311",
            "filename": "src/transformers/models/t5gemma/modeling_t5gemma.py",
            "status": "modified",
            "additions": 2,
            "deletions": 11,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -594,18 +594,9 @@ class T5GemmaPreTrainedModel(PreTrainedModel):\n \n     def _init_weights(self, module):\n         # TODO: support intialization for encoders and decoders separately(?)\n+        super()._init_weights(module)\n         std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, T5GemmaRMSNorm):\n-            module.weight.data.fill_(1.0)\n-        elif isinstance(module, T5GemmaClassificationHead):\n+        if isinstance(module, T5GemmaClassificationHead):\n             scale = module.out_proj.weight.shape[0] ** -0.5\n             module.out_proj.weight.data.normal_(mean=0.0, std=std * scale)\n             if hasattr(module.out_proj, \"bias\") and module.out_proj.bias is not None:"
        },
        {
            "sha": "540a796f604ef1af5cc2c9434bcc6fd03a1f21f1",
            "filename": "src/transformers/models/t5gemma/modular_t5gemma.py",
            "status": "modified",
            "additions": 2,
            "deletions": 11,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -485,18 +485,9 @@ class T5GemmaPreTrainedModel(Gemma2PreTrainedModel):\n \n     def _init_weights(self, module):\n         # TODO: support intialization for encoders and decoders separately(?)\n+        Gemma2PreTrainedModel._init_weights(module)\n         std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, T5GemmaRMSNorm):\n-            module.weight.data.fill_(1.0)\n-        elif isinstance(module, T5GemmaClassificationHead):\n+        if isinstance(module, T5GemmaClassificationHead):\n             scale = module.out_proj.weight.shape[0] ** -0.5\n             module.out_proj.weight.data.normal_(mean=0.0, std=std * scale)\n             if hasattr(module.out_proj, \"bias\") and module.out_proj.bias is not None:"
        },
        {
            "sha": "dd6f352376ffe817a1112ef0a60880612f667f1c",
            "filename": "src/transformers/models/timesfm/modeling_timesfm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 16,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Ftimesfm%2Fmodeling_timesfm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Ftimesfm%2Fmodeling_timesfm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftimesfm%2Fmodeling_timesfm.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -306,22 +306,8 @@ class TimesFmPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n \n     def _init_weights(self, module):\n-        if isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0, std=self.config.initializer_range)\n-\n-        elif isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                nn.init.zeros_(module.bias)\n-\n-        elif isinstance(module, nn.LayerNorm):\n-            nn.init.ones_(module.weight)\n-            nn.init.zeros_(module.bias)\n-\n-        elif isinstance(module, TimesFmRMSNorm):\n-            nn.init.zeros_(module.weight)\n-\n-        elif isinstance(module, TimesFmAttention):\n+        super()._init_weights(module)\n+        if isinstance(module, TimesFmAttention):\n             # Initialize scaling parameter\n             nn.init.ones_(module.scaling)\n "
        },
        {
            "sha": "b82816e7c737364bd1d5d256d6d37174a9d9965e",
            "filename": "src/transformers/models/timesfm/modular_timesfm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 16,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Ftimesfm%2Fmodular_timesfm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Ftimesfm%2Fmodular_timesfm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftimesfm%2Fmodular_timesfm.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -262,22 +262,8 @@ class TimesFmPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n \n     def _init_weights(self, module):\n-        if isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0, std=self.config.initializer_range)\n-\n-        elif isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                nn.init.zeros_(module.bias)\n-\n-        elif isinstance(module, nn.LayerNorm):\n-            nn.init.ones_(module.weight)\n-            nn.init.zeros_(module.bias)\n-\n-        elif isinstance(module, TimesFmRMSNorm):\n-            nn.init.zeros_(module.weight)\n-\n-        elif isinstance(module, TimesFmAttention):\n+        super()._init_weights(module)\n+        if isinstance(module, TimesFmAttention):\n             # Initialize scaling parameter\n             nn.init.ones_(module.scaling)\n "
        },
        {
            "sha": "2f3f536f79e625012c33a20fea0728b57f8a4462",
            "filename": "src/transformers/models/vipllava/modeling_vipllava.py",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -126,20 +126,6 @@ class VipLlavaPreTrainedModel(PreTrainedModel):\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n \n-    def _init_weights(self, module):\n-        # important: this ported version of VipLlava isn't meant for training from scratch - only\n-        # inference and fine-tuning - so the proper init weights code has been removed - the original codebase\n-        # https://github.com/haotian-liu/VipLlava/tree/main/vipllava should serve for that purpose\n-        std = getattr(self.config, \"initializer_range\", self.config.get_text_config().initializer_range)\n-\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.weight.data.fill_(1.0)\n-            module.bias.data.zero_()\n-\n \n @auto_docstring(\n     custom_intro=\"\"\""
        },
        {
            "sha": "17a96aaad86bb701bacb9a3436fbd20ef187de97",
            "filename": "src/transformers/models/zamba2/modeling_zamba2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 12,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -1183,18 +1183,8 @@ class Zamba2PreTrainedModel(PreTrainedModel):\n     _is_stateful = True\n \n     def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, (nn.Linear, nn.Conv1d)):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, (Zamba2RMSNorm, Zamba2RMSNormGated)):\n-            module.weight.data.fill_(1.0)\n-        elif isinstance(module, Zamba2MambaMixer):\n+        super()._init_weights(module)\n+        if isinstance(module, Zamba2MambaMixer):\n             dt = torch.exp(\n                 torch.rand(self.config.n_mamba_heads)\n                 * (math.log(self.config.time_step_max) - math.log(self.config.time_step_min))"
        },
        {
            "sha": "2e2c4cd608de954d2b8a166f1cdacb9ca7b8e67c",
            "filename": "src/transformers/models/zamba2/modular_zamba2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 12,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodular_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b16688e96a3b3e1e7a701cd5284a850c696c108e/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodular_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodular_zamba2.py?ref=b16688e96a3b3e1e7a701cd5284a850c696c108e",
            "patch": "@@ -906,18 +906,8 @@ class Zamba2PreTrainedModel(PreTrainedModel):\n     _is_stateful = True\n \n     def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, (nn.Linear, nn.Conv1d)):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, (Zamba2RMSNorm, Zamba2RMSNormGated)):\n-            module.weight.data.fill_(1.0)\n-        elif isinstance(module, Zamba2MambaMixer):\n+        super()._init_weights(module)\n+        if isinstance(module, Zamba2MambaMixer):\n             dt = torch.exp(\n                 torch.rand(self.config.n_mamba_heads)\n                 * (math.log(self.config.time_step_max) - math.log(self.config.time_step_min))"
        }
    ],
    "stats": {
        "total": 1771,
        "additions": 205,
        "deletions": 1566
    }
}