{
    "author": "sebbaur",
    "message": "Make ViTPooler configurable (#36517)\n\n* Make ViT Pooler configurable, so that it is possible to pick the activation function and the number of channels in the output\n\n* Add documentation and allow functions as activations (instead of just string)\n\n* formatting change\n\n* Use ACT2FN\n\n* Formatting change\n\n* Formatting changes\n\n* force pooler_act to be string\n\n* force pooler_act to be string\n\n* Add configs to OBJECTS_TO_IGNORE to make check_docstrings happy\n\n* Making the same change in ijepa to make check_modular_conversion happy\n\n* Add IJepaConfig to make CI happy\n\n* rename pooler_size to pooler_output_size as defined in the config\n\n* typo\n\n* revert change to ignore variable\n\n* Ran utils/check_docstrings.py --fix_and_overwrite\n\n* revert unrelated change\n\n* remove redundant defaults\n\n* rename self.act -> self.activation\n\n* tanh activation function in mapping",
    "sha": "62116c967f3ef07e905a1f3ef0555be022abccb8",
    "files": [
        {
            "sha": "5c439420148545ae08450c212b7a2e9d973af057",
            "filename": "src/transformers/modeling_flax_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62116c967f3ef07e905a1f3ef0555be022abccb8/src%2Ftransformers%2Fmodeling_flax_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62116c967f3ef07e905a1f3ef0555be022abccb8/src%2Ftransformers%2Fmodeling_flax_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_flax_utils.py?ref=62116c967f3ef07e905a1f3ef0555be022abccb8",
            "patch": "@@ -79,6 +79,7 @@ def quick_gelu(x):\n     \"gelu_new\": partial(nn.gelu, approximate=True),\n     \"quick_gelu\": quick_gelu,\n     \"gelu_pytorch_tanh\": partial(nn.gelu, approximate=True),\n+    \"tanh\": nn.tanh,\n }\n \n "
        },
        {
            "sha": "63d772067c5fd3e13ab0051b377751a873c7f03b",
            "filename": "src/transformers/models/deit/configuration_deit.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/62116c967f3ef07e905a1f3ef0555be022abccb8/src%2Ftransformers%2Fmodels%2Fdeit%2Fconfiguration_deit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62116c967f3ef07e905a1f3ef0555be022abccb8/src%2Ftransformers%2Fmodels%2Fdeit%2Fconfiguration_deit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeit%2Fconfiguration_deit.py?ref=62116c967f3ef07e905a1f3ef0555be022abccb8",
            "patch": "@@ -69,6 +69,12 @@ class DeiTConfig(PretrainedConfig):\n             Whether to add a bias to the queries, keys and values.\n         encoder_stride (`int`, *optional*, defaults to 16):\n             Factor to increase the spatial resolution by in the decoder head for masked image modeling.\n+        pooler_output_size (`int`, *optional*):\n+           Dimensionality of the pooler layer. If None, defaults to `hidden_size`.\n+        pooler_act (`str`, *optional*, defaults to `\"tanh\"`):\n+           The activation function to be used by the pooler. Keys of ACT2FN are supported for Flax and\n+           Pytorch, and elements of https://www.tensorflow.org/api_docs/python/tf/keras/activations are\n+           supported for Tensorflow.\n \n     Example:\n \n@@ -103,6 +109,8 @@ def __init__(\n         num_channels=3,\n         qkv_bias=True,\n         encoder_stride=16,\n+        pooler_output_size=None,\n+        pooler_act=\"tanh\",\n         **kwargs,\n     ):\n         super().__init__(**kwargs)\n@@ -121,6 +129,8 @@ def __init__(\n         self.num_channels = num_channels\n         self.qkv_bias = qkv_bias\n         self.encoder_stride = encoder_stride\n+        self.pooler_output_size = pooler_output_size if pooler_output_size else hidden_size\n+        self.pooler_act = pooler_act\n \n \n class DeiTOnnxConfig(OnnxConfig):"
        },
        {
            "sha": "0c41ae1f7fe5049b79897b2d9714c787f87f769f",
            "filename": "src/transformers/models/deit/modeling_deit.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/62116c967f3ef07e905a1f3ef0555be022abccb8/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_deit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62116c967f3ef07e905a1f3ef0555be022abccb8/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_deit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_deit.py?ref=62116c967f3ef07e905a1f3ef0555be022abccb8",
            "patch": "@@ -628,8 +628,8 @@ def forward(\n class DeiTPooler(nn.Module):\n     def __init__(self, config: DeiTConfig):\n         super().__init__()\n-        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n-        self.activation = nn.Tanh()\n+        self.dense = nn.Linear(config.hidden_size, config.pooler_output_size)\n+        self.activation = ACT2FN[config.pooler_act]\n \n     def forward(self, hidden_states):\n         # We \"pool\" the model by simply taking the hidden state corresponding"
        },
        {
            "sha": "48725cd816c75b2af2ad69617267f317a96d19b1",
            "filename": "src/transformers/models/deit/modeling_tf_deit.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/62116c967f3ef07e905a1f3ef0555be022abccb8/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_tf_deit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62116c967f3ef07e905a1f3ef0555be022abccb8/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_tf_deit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_tf_deit.py?ref=62116c967f3ef07e905a1f3ef0555be022abccb8",
            "patch": "@@ -813,9 +813,9 @@ def __init__(self, config: DeiTConfig, **kwargs):\n         super().__init__(**kwargs)\n \n         self.dense = keras.layers.Dense(\n-            units=config.hidden_size,\n+            units=config.pooler_output_size,\n             kernel_initializer=get_initializer(config.initializer_range),\n-            activation=\"tanh\",\n+            activation=config.pooler_act,\n             name=\"dense\",\n         )\n         self.config = config"
        },
        {
            "sha": "22f25e18423f142cd9847a3441db723b3e005025",
            "filename": "src/transformers/models/dpt/configuration_dpt.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/62116c967f3ef07e905a1f3ef0555be022abccb8/src%2Ftransformers%2Fmodels%2Fdpt%2Fconfiguration_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62116c967f3ef07e905a1f3ef0555be022abccb8/src%2Ftransformers%2Fmodels%2Fdpt%2Fconfiguration_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fconfiguration_dpt.py?ref=62116c967f3ef07e905a1f3ef0555be022abccb8",
            "patch": "@@ -119,6 +119,12 @@ class DPTConfig(PretrainedConfig):\n         backbone_kwargs (`dict`, *optional*):\n             Keyword arguments to be passed to AutoBackbone when loading from a checkpoint\n             e.g. `{'out_indices': (0, 1, 2, 3)}`. Cannot be specified if `backbone_config` is set.\n+        pooler_output_size (`int`, *optional*):\n+           Dimensionality of the pooler layer. If None, defaults to `hidden_size`.\n+        pooler_act (`str`, *optional*, defaults to `\"tanh\"`):\n+           The activation function to be used by the pooler. Keys of ACT2FN are supported for Flax and\n+           Pytorch, and elements of https://www.tensorflow.org/api_docs/python/tf/keras/activations are\n+           supported for Tensorflow.\n \n     Example:\n \n@@ -173,6 +179,8 @@ def __init__(\n         use_pretrained_backbone=False,\n         use_timm_backbone=False,\n         backbone_kwargs=None,\n+        pooler_output_size=None,\n+        pooler_act=\"tanh\",\n         **kwargs,\n     ):\n         super().__init__(**kwargs)\n@@ -268,6 +276,8 @@ def __init__(\n         self.auxiliary_loss_weight = auxiliary_loss_weight\n         self.semantic_loss_ignore_index = semantic_loss_ignore_index\n         self.semantic_classifier_dropout = semantic_classifier_dropout\n+        self.pooler_output_size = pooler_output_size if pooler_output_size else hidden_size\n+        self.pooler_act = pooler_act\n \n     def to_dict(self):\n         \"\"\""
        },
        {
            "sha": "98b9782ad33578213f98ebee327d7b8b4349baec",
            "filename": "src/transformers/models/dpt/modeling_dpt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/62116c967f3ef07e905a1f3ef0555be022abccb8/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodeling_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62116c967f3ef07e905a1f3ef0555be022abccb8/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodeling_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodeling_dpt.py?ref=62116c967f3ef07e905a1f3ef0555be022abccb8",
            "patch": "@@ -989,8 +989,8 @@ def forward(\n class DPTViTPooler(nn.Module):\n     def __init__(self, config: DPTConfig):\n         super().__init__()\n-        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n-        self.activation = nn.Tanh()\n+        self.dense = nn.Linear(config.hidden_size, config.pooler_output_size)\n+        self.activation = ACT2FN[config.pooler_act]\n \n     def forward(self, hidden_states):\n         # We \"pool\" the model by simply taking the hidden state corresponding"
        },
        {
            "sha": "5f528adad0d55711f40ea21f58e1e0196822f449",
            "filename": "src/transformers/models/ijepa/configuration_ijepa.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/62116c967f3ef07e905a1f3ef0555be022abccb8/src%2Ftransformers%2Fmodels%2Fijepa%2Fconfiguration_ijepa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62116c967f3ef07e905a1f3ef0555be022abccb8/src%2Ftransformers%2Fmodels%2Fijepa%2Fconfiguration_ijepa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fijepa%2Fconfiguration_ijepa.py?ref=62116c967f3ef07e905a1f3ef0555be022abccb8",
            "patch": "@@ -56,6 +56,12 @@ class IJepaConfig(PretrainedConfig):\n             The number of input channels.\n         qkv_bias (`bool`, *optional*, defaults to `True`):\n             Whether to add a bias to the queries, keys and values.\n+        pooler_output_size (`int`, *optional*):\n+           Dimensionality of the pooler layer. If None, defaults to `hidden_size`.\n+        pooler_act (`str`, *optional*, defaults to `\"tanh\"`):\n+           The activation function to be used by the pooler. Keys of ACT2FN are supported for Flax and\n+           Pytorch, and elements of https://www.tensorflow.org/api_docs/python/tf/keras/activations are\n+           supported for Tensorflow.\n \n     Example:\n \n@@ -89,6 +95,8 @@ def __init__(\n         patch_size=16,\n         num_channels=3,\n         qkv_bias=True,\n+        pooler_output_size=None,\n+        pooler_act=\"tanh\",\n         **kwargs,\n     ):\n         super().__init__(**kwargs)\n@@ -106,6 +114,8 @@ def __init__(\n         self.patch_size = patch_size\n         self.num_channels = num_channels\n         self.qkv_bias = qkv_bias\n+        self.pooler_output_size = pooler_output_size if pooler_output_size else hidden_size\n+        self.pooler_act = pooler_act\n \n \n __all__ = [\"IJepaConfig\"]"
        },
        {
            "sha": "e0ee50a0044a83f60251a8101c99144d75b87980",
            "filename": "src/transformers/models/ijepa/modeling_ijepa.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/62116c967f3ef07e905a1f3ef0555be022abccb8/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodeling_ijepa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62116c967f3ef07e905a1f3ef0555be022abccb8/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodeling_ijepa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodeling_ijepa.py?ref=62116c967f3ef07e905a1f3ef0555be022abccb8",
            "patch": "@@ -465,8 +465,8 @@ def forward(\n class IJepaPooler(nn.Module):\n     def __init__(self, config: IJepaConfig):\n         super().__init__()\n-        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n-        self.activation = nn.Tanh()\n+        self.dense = nn.Linear(config.hidden_size, config.pooler_output_size)\n+        self.activation = ACT2FN[config.pooler_act]\n \n     def forward(self, hidden_states):\n         # We \"pool\" the model by simply taking the hidden state corresponding"
        },
        {
            "sha": "13ad3a7715c5dd411147e33823e0d69e7106005d",
            "filename": "src/transformers/models/vit/configuration_vit.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/62116c967f3ef07e905a1f3ef0555be022abccb8/src%2Ftransformers%2Fmodels%2Fvit%2Fconfiguration_vit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62116c967f3ef07e905a1f3ef0555be022abccb8/src%2Ftransformers%2Fmodels%2Fvit%2Fconfiguration_vit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvit%2Fconfiguration_vit.py?ref=62116c967f3ef07e905a1f3ef0555be022abccb8",
            "patch": "@@ -68,6 +68,12 @@ class ViTConfig(PretrainedConfig):\n             Whether to add a bias to the queries, keys and values.\n         encoder_stride (`int`, *optional*, defaults to 16):\n            Factor to increase the spatial resolution by in the decoder head for masked image modeling.\n+        pooler_output_size (`int`, *optional*):\n+           Dimensionality of the pooler layer. If None, defaults to `hidden_size`.\n+        pooler_act (`str`, *optional*, defaults to `\"tanh\"`):\n+           The activation function to be used by the pooler. Keys of ACT2FN are supported for Flax and\n+           Pytorch, and elements of https://www.tensorflow.org/api_docs/python/tf/keras/activations are\n+           supported for Tensorflow.\n \n     Example:\n \n@@ -102,6 +108,8 @@ def __init__(\n         num_channels=3,\n         qkv_bias=True,\n         encoder_stride=16,\n+        pooler_output_size=None,\n+        pooler_act=\"tanh\",\n         **kwargs,\n     ):\n         super().__init__(**kwargs)\n@@ -120,6 +128,8 @@ def __init__(\n         self.num_channels = num_channels\n         self.qkv_bias = qkv_bias\n         self.encoder_stride = encoder_stride\n+        self.pooler_output_size = pooler_output_size if pooler_output_size else hidden_size\n+        self.pooler_act = pooler_act\n \n \n class ViTOnnxConfig(OnnxConfig):"
        },
        {
            "sha": "5cf3477b5ddff66caac2b84ed6ba2f0e00336780",
            "filename": "src/transformers/models/vit/modeling_flax_vit.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/62116c967f3ef07e905a1f3ef0555be022abccb8/src%2Ftransformers%2Fmodels%2Fvit%2Fmodeling_flax_vit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62116c967f3ef07e905a1f3ef0555be022abccb8/src%2Ftransformers%2Fmodels%2Fvit%2Fmodeling_flax_vit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvit%2Fmodeling_flax_vit.py?ref=62116c967f3ef07e905a1f3ef0555be022abccb8",
            "patch": "@@ -412,17 +412,18 @@ class FlaxViTPooler(nn.Module):\n \n     def setup(self):\n         self.dense = nn.Dense(\n-            self.config.hidden_size,\n+            self.config.pooler_output_size,\n             kernel_init=jax.nn.initializers.variance_scaling(\n                 self.config.initializer_range**2, \"fan_in\", \"truncated_normal\"\n             ),\n             dtype=self.dtype,\n         )\n+        self.activation = ACT2FN[self.config.pooler_act]\n \n     def __call__(self, hidden_states):\n         cls_hidden_state = hidden_states[:, 0]\n         cls_hidden_state = self.dense(cls_hidden_state)\n-        return nn.tanh(cls_hidden_state)\n+        return self.activation(cls_hidden_state)\n \n \n class FlaxViTPreTrainedModel(FlaxPreTrainedModel):"
        },
        {
            "sha": "e18b38e597f37298a75570a90f7c1019bf7c7788",
            "filename": "src/transformers/models/vit/modeling_tf_vit.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/62116c967f3ef07e905a1f3ef0555be022abccb8/src%2Ftransformers%2Fmodels%2Fvit%2Fmodeling_tf_vit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62116c967f3ef07e905a1f3ef0555be022abccb8/src%2Ftransformers%2Fmodels%2Fvit%2Fmodeling_tf_vit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvit%2Fmodeling_tf_vit.py?ref=62116c967f3ef07e905a1f3ef0555be022abccb8",
            "patch": "@@ -788,9 +788,9 @@ def __init__(self, config: ViTConfig, **kwargs):\n         super().__init__(**kwargs)\n \n         self.dense = keras.layers.Dense(\n-            units=config.hidden_size,\n+            units=config.pooler_output_size,\n             kernel_initializer=get_initializer(config.initializer_range),\n-            activation=\"tanh\",\n+            activation=config.pooler_act,\n             name=\"dense\",\n         )\n         self.config = config"
        },
        {
            "sha": "158a3e3e6d519bd9f985adf21b0ab17b881bcd1b",
            "filename": "src/transformers/models/vit/modeling_vit.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/62116c967f3ef07e905a1f3ef0555be022abccb8/src%2Ftransformers%2Fmodels%2Fvit%2Fmodeling_vit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62116c967f3ef07e905a1f3ef0555be022abccb8/src%2Ftransformers%2Fmodels%2Fvit%2Fmodeling_vit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvit%2Fmodeling_vit.py?ref=62116c967f3ef07e905a1f3ef0555be022abccb8",
            "patch": "@@ -636,8 +636,8 @@ def forward(\n class ViTPooler(nn.Module):\n     def __init__(self, config: ViTConfig):\n         super().__init__()\n-        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n-        self.activation = nn.Tanh()\n+        self.dense = nn.Linear(config.hidden_size, config.pooler_output_size)\n+        self.activation = ACT2FN[config.pooler_act]\n \n     def forward(self, hidden_states):\n         # We \"pool\" the model by simply taking the hidden state corresponding"
        }
    ],
    "stats": {
        "total": 70,
        "additions": 56,
        "deletions": 14
    }
}