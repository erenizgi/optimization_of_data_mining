{
    "author": "yonigozlan",
    "message": "[SAM3 Video] Add support for multi prompts  (#42293)\n\n* add support for multi prompts + fix checkpoints in tests\n\n* Make sure to apply heuristics per prompt group\n\n* simplify NMS to probs",
    "sha": "c3fb1b1a6ca1102f62b139c83a088a97e5a55477",
    "files": [
        {
            "sha": "25c233f6b09c8bee04087035e79083f66f62d4d5",
            "filename": "docs/source/en/model_doc/sam3_video.md",
            "status": "modified",
            "additions": 33,
            "deletions": 0,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/c3fb1b1a6ca1102f62b139c83a088a97e5a55477/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam3_video.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c3fb1b1a6ca1102f62b139c83a088a97e5a55477/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam3_video.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam3_video.md?ref=c3fb1b1a6ca1102f62b139c83a088a97e5a55477",
            "patch": "@@ -97,6 +97,39 @@ Processed 51 frames\n >>> print(f\"Masks shape: {frame_0_outputs['masks'].shape}\")\n ```\n \n+You can also track multiple object categories simultaneously by providing multiple prompts. The model efficiently reuses vision features across all prompts:\n+\n+```python\n+>>> # Add multiple text prompts (or use a list in add_text_prompt)\n+>>> multi_prompt_session = processor.init_video_session(\n+...     video=video_frames,\n+...     inference_device=device,\n+...     processing_device=\"cpu\",\n+...     video_storage_device=\"cpu\",\n+...     dtype=torch.bfloat16,\n+... )\n+>>>\n+>>> prompts = [\"person\", \"bed\", \"lamp\"]\n+>>> processor.add_text_prompt(multi_prompt_session, prompts)\n+>>>\n+>>> # Process video - detects objects from ALL prompts in a single pass\n+>>> multi_outputs_per_frame = {}\n+>>> for model_outputs in model.propagate_in_video_iterator(\n+...     inference_session=multi_prompt_session, max_frame_num_to_track=50\n+... ):\n+...     processed_outputs = processor.postprocess_outputs(multi_prompt_session, model_outputs)\n+...     multi_outputs_per_frame[model_outputs.frame_idx] = processed_outputs\n+>>>\n+>>> # Check which objects were detected by each prompt\n+>>> frame_0_outputs = multi_outputs_per_frame[0]\n+>>> prompt_to_obj_ids = frame_0_outputs[\"prompt_to_obj_ids\"]\n+>>> for prompt, obj_ids in prompt_to_obj_ids.items():\n+...     print(f\"{prompt}: {len(obj_ids)} objects\")\n+person: 2 objects\n+bed: 1 objects\n+lamp: 1 objects\n+```\n+\n #### Streaming Video Inference\n \n <div class=\"warning\">"
        },
        {
            "sha": "cd8938c8393b11fdb392ebf284acbf42b3ee15ad",
            "filename": "src/transformers/models/sam3_video/modeling_sam3_video.py",
            "status": "modified",
            "additions": 249,
            "deletions": 64,
            "changes": 313,
            "blob_url": "https://github.com/huggingface/transformers/blob/c3fb1b1a6ca1102f62b139c83a088a97e5a55477/src%2Ftransformers%2Fmodels%2Fsam3_video%2Fmodeling_sam3_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c3fb1b1a6ca1102f62b139c83a088a97e5a55477/src%2Ftransformers%2Fmodels%2Fsam3_video%2Fmodeling_sam3_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam3_video%2Fmodeling_sam3_video.py?ref=c3fb1b1a6ca1102f62b139c83a088a97e5a55477",
            "patch": "@@ -182,13 +182,12 @@ def __init__(\n         self.output_dict_per_obj = {}\n         self.frames_tracked_per_obj = {}\n \n-        # Session state flags\n-        self.has_new_text_input = False\n-\n-        # Detection-specific state\n-        self.text_input_ids = None  # Cached text input ids for the video\n-        self.text_embeddings = None  # Cached text embeddings for the video\n-        self.text_attention_mask = None  # Cached text attention mask for the video\n+        # Multi-prompt support\n+        self.prompts = {}  # prompt_id -> prompt_text\n+        self.prompt_input_ids = {}  # prompt_id -> input_ids\n+        self.prompt_embeddings = {}  # prompt_id -> text embeddings\n+        self.prompt_attention_masks = {}  # prompt_id -> attention_mask\n+        self.obj_id_to_prompt_id = {}  # obj_id -> prompt_id (assigned at detection time)\n \n         # Tracking metadata for detection-tracking fusion\n         self.obj_id_to_score = {}  # Detection scores per object\n@@ -213,6 +212,19 @@ def num_frames(self) -> Optional[int]:\n         \"\"\"Number of frames in the video.\"\"\"\n         return len(self.processed_frames) if self.processed_frames is not None else None\n \n+    def add_prompt(self, prompt_text: str) -> int:\n+        \"\"\"\n+        Add a text prompt to the session and return its unique ID.\n+        If the prompt already exists, returns the existing ID.\n+        \"\"\"\n+        for prompt_id, text in self.prompts.items():\n+            if text == prompt_text:\n+                return prompt_id\n+\n+        prompt_id = len(self.prompts)\n+        self.prompts[prompt_id] = prompt_text\n+        return prompt_id\n+\n     # Object management\n     def obj_id_to_idx(self, obj_id: int) -> int:\n         \"\"\"Map object ID to index, creating new entry if needed.\"\"\"\n@@ -257,6 +269,7 @@ def remove_object(self, obj_id: int, strict: bool = False):\n \n         Args:\n             obj_id (`int`): The object ID to remove.\n+            strict (`bool`, *optional*, defaults to `False`): Whether to raise an error if the object doesn't exist.\n         \"\"\"\n         old_obj_idx_to_rm = self._obj_id_to_idx.get(obj_id, None)\n         # Check whether this object_id to remove actually exists and possibly raise an error.\n@@ -267,6 +280,9 @@ def remove_object(self, obj_id: int, strict: bool = False):\n                 f\"Cannot remove object id {obj_id} as it doesn't exist. All existing object ids: {self.obj_ids}.\"\n             )\n \n+        # Clean up prompt mapping\n+        self.obj_id_to_prompt_id.pop(obj_id, None)\n+\n         # If this is the only remaining object id, we simply reset the state.\n         if len(self._obj_id_to_idx) == 1:\n             self.reset_inference_session()\n@@ -394,6 +410,9 @@ def reset_tracking_data(self):\n         self.frames_tracked_per_obj.clear()\n         # Note: cache and video data are preserved\n \n+        # Reset prompt mappings for objects (but keep prompts themselves)\n+        self.obj_id_to_prompt_id.clear()\n+\n     def reset_inference_session(self):\n         \"\"\"Reset tracking data and cache.\"\"\"\n         self._obj_id_to_idx.clear()\n@@ -403,6 +422,9 @@ def reset_inference_session(self):\n         self.frames_tracked_per_obj.clear()\n         self.cache.clear_all()\n \n+        # Reset prompt mappings for objects (but keep prompts themselves)\n+        self.obj_id_to_prompt_id.clear()\n+\n     def reset_state(self):\n         \"\"\"Reset the inference session state.\"\"\"\n         self._obj_id_to_idx = OrderedDict()\n@@ -412,7 +434,6 @@ def reset_state(self):\n         self.frames_tracked_per_obj = {}\n \n         # Reset detection-tracking fusion state\n-        self.text_embeddings = None\n         self.obj_id_to_score = {}\n         self.obj_id_to_tracker_score_frame_wise = defaultdict(dict)\n         self.obj_id_to_last_occluded = {}\n@@ -425,6 +446,13 @@ def reset_state(self):\n         self.suppressed_obj_ids = defaultdict(set)\n         self.output_buffer = []\n \n+        # Reset multi-prompt state\n+        self.prompts.clear()\n+        self.prompt_input_ids.clear()\n+        self.prompt_embeddings.clear()\n+        self.prompt_attention_masks.clear()\n+        self.obj_id_to_prompt_id.clear()\n+\n         # Clear cache\n         self.cache.clear_all()\n \n@@ -541,53 +569,71 @@ def run_detection(\n         inference_session: Sam3VideoInferenceSession,\n         vision_embeds: torch.Tensor,\n     ):\n-        if inference_session.has_new_text_input:\n-            text_embeds = self.detector_model.get_text_features(\n-                input_ids=inference_session.text_input_ids,\n-                attention_mask=inference_session.text_attention_mask,\n-            )\n-            inference_session.text_embeddings = text_embeds\n-            inference_session.has_new_text_input = False\n-        else:\n-            text_embeds = inference_session.text_embeddings\n-        detector_outputs = self.detector_model(\n-            vision_embeds=vision_embeds,\n-            text_embeds=text_embeds,\n-            attention_mask=inference_session.text_attention_mask,\n-        )\n+        \"\"\"\n+        Run detection for all prompts efficiently by reusing vision embeddings.\n \n-        pred_logits = detector_outputs.pred_logits\n-        presence_logits = detector_outputs.presence_logits\n+        Args:\n+            inference_session: The inference session containing prompts and state\n+            vision_embeds: Pre-computed vision embeddings to reuse across prompts\n \n-        pred_probs = pred_logits.sigmoid()\n-        presence_scores = presence_logits.sigmoid()\n-        pred_probs = pred_probs * presence_scores\n+        Returns:\n+            Dictionary mapping prompt_id to detection outputs\n+        \"\"\"\n+        prompt_ids = list(inference_session.prompts.keys())\n+        if not prompt_ids:\n+            raise ValueError(\"No prompts available for detection. Please add prompts to the session first.\")\n+\n+        all_detections = {}\n+\n+        for prompt_id in prompt_ids:\n+            # Get or compute text embeddings for this prompt\n+            if prompt_id not in inference_session.prompt_embeddings:\n+                text_embeds = self.detector_model.get_text_features(\n+                    input_ids=inference_session.prompt_input_ids[prompt_id],\n+                    attention_mask=inference_session.prompt_attention_masks[prompt_id],\n+                )\n+                inference_session.prompt_embeddings[prompt_id] = text_embeds\n+            else:\n+                text_embeds = inference_session.prompt_embeddings[prompt_id]\n \n-        run_nms = self.det_nms_thresh > 0.0\n-        if run_nms:\n-            keep = nms_masks(\n-                pred_probs=pred_probs[0],\n-                pred_masks=detector_outputs.pred_masks[0],\n-                prob_threshold=self.score_threshold_detection,\n-                iou_threshold=self.det_nms_thresh,\n+            # Run detector with cached vision features (efficient!)\n+            detector_outputs = self.detector_model(\n+                vision_embeds=vision_embeds,\n+                text_embeds=text_embeds,\n+                attention_mask=inference_session.prompt_attention_masks[prompt_id],\n             )\n-            # set suppressed detections' logits to a very low value\n-            detector_outputs.pred_logits[0] -= 1e4 * (~keep).float()\n-            # Recompute pred_probs after NMS suppression\n+\n+            pred_logits = detector_outputs.pred_logits\n+            presence_logits = detector_outputs.presence_logits\n+\n             pred_probs = pred_logits.sigmoid()\n+            presence_scores = presence_logits.sigmoid()\n             pred_probs = pred_probs * presence_scores\n \n-        pred_boxes_xyxy = detector_outputs.pred_boxes\n-        pred_masks = detector_outputs.pred_masks\n-        # get the positive detection outputs above threshold\n-        pos_pred_idx = torch.where(pred_probs > self.score_threshold_detection)\n-        det_out = {\n-            \"bbox\": pred_boxes_xyxy[pos_pred_idx[0], pos_pred_idx[1]],\n-            \"mask\": pred_masks[pos_pred_idx[0], pos_pred_idx[1]],\n-            \"scores\": pred_probs[pos_pred_idx[0], pos_pred_idx[1]],\n-        }\n+            run_nms = self.det_nms_thresh > 0.0\n+            if run_nms:\n+                keep = nms_masks(\n+                    pred_probs=pred_probs[0],\n+                    pred_masks=detector_outputs.pred_masks[0],\n+                    prob_threshold=self.score_threshold_detection,\n+                    iou_threshold=self.det_nms_thresh,\n+                )\n+                # Set suppressed detections' probabilities to 0\n+                pred_probs[0][~keep] = 0.0\n+\n+            pred_boxes_xyxy = detector_outputs.pred_boxes\n+            pred_masks = detector_outputs.pred_masks\n+            # get the positive detection outputs above threshold\n+            pos_pred_idx = torch.where(pred_probs > self.score_threshold_detection)\n+            det_out = {\n+                \"bbox\": pred_boxes_xyxy[pos_pred_idx[0], pos_pred_idx[1]],\n+                \"mask\": pred_masks[pos_pred_idx[0], pos_pred_idx[1]],\n+                \"scores\": pred_probs[pos_pred_idx[0], pos_pred_idx[1]],\n+            }\n+\n+            all_detections[prompt_id] = det_out\n \n-        return det_out\n+        return all_detections\n \n     def run_tracker_propagation(\n         self,\n@@ -638,6 +684,8 @@ def _associate_det_trk(\n         det_scores: Tensor,\n         trk_masks: Tensor,\n         trk_obj_ids: list[int],\n+        det_prompt_ids: torch.Tensor,\n+        trk_prompt_ids: torch.Tensor,\n     ):\n         \"\"\"\n         Match detections on the current frame with the existing masklets.\n@@ -647,6 +695,10 @@ def _associate_det_trk(\n           - det_scores: (N,) tensor of detection scores\n           - trk_masks: (M, H, W) tensor of track masks\n           - trk_obj_ids: (M,) list of object IDs corresponding to trk_masks\n+          - det_prompt_ids: (N,) tensor of prompt IDs for each detection. Prevents cross-prompt\n+            associations by zeroing IoUs between detections and tracks from different prompts.\n+          - trk_prompt_ids: (M,) tensor of prompt IDs for each tracked object. Prevents cross-prompt\n+            associations by zeroing IoUs between detections and tracks from different prompts.\n \n         Returns:\n           - new_det_out_inds: list of new object indices among in FA detection outputs\n@@ -700,6 +752,10 @@ def _associate_det_trk(\n         trk_masks_binary = trk_masks > 0\n         ious = mask_iou(det_masks_binary, trk_masks_binary)  # (N, M) tensor\n \n+        # Prevent cross-prompt associations by zeroing IoUs between different prompt groups.\n+        prompt_match = det_prompt_ids.unsqueeze(1) == trk_prompt_ids.unsqueeze(0)\n+        ious = torch.where(prompt_match, ious, torch.zeros_like(ious))\n+\n         # trk_is_matched: for each track, True if matched to any detection above threshold\n         trk_is_matched = (ious >= iou_threshold_trk).any(dim=0)  # (M,)\n         # Non-empty tracks not matched by Hungarian assignment above threshold are unmatched\n@@ -1009,12 +1065,36 @@ def _suppress_overlapping_based_on_recent_occlusion(\n                 ],\n                 dim=0,\n             )\n-            to_suppress = self._get_objects_to_suppress_based_on_most_recently_occluded(\n-                binary_tracker_low_res_masks_global,\n-                last_occluded_prev,\n-                obj_ids_global,\n-                reverse,\n+\n+            prompt_ids_global = torch.tensor(\n+                [inference_session.obj_id_to_prompt_id[obj_id] for obj_id in obj_ids_global],\n+                device=binary_tracker_low_res_masks_global.device,\n+                dtype=torch.long,\n             )\n+            to_suppress = torch.zeros(\n+                batch_size,\n+                device=binary_tracker_low_res_masks_global.device,\n+                dtype=torch.bool,\n+            )\n+\n+            # Only suppress overlaps within the same prompt group.\n+            unique_prompts = prompt_ids_global.unique(sorted=True)\n+            for prompt_id in unique_prompts:\n+                prompt_mask = prompt_ids_global == prompt_id\n+                prompt_indices = torch.nonzero(prompt_mask, as_tuple=True)[0]\n+                if prompt_indices.numel() <= 1:\n+                    continue\n+\n+                prompt_masks = binary_tracker_low_res_masks_global[prompt_indices]\n+                prompt_last_occ = last_occluded_prev[prompt_indices]\n+                prompt_obj_ids = [obj_ids_global[idx] for idx in prompt_indices.tolist()]\n+                prompt_suppress = self._get_objects_to_suppress_based_on_most_recently_occluded(\n+                    prompt_masks,\n+                    prompt_last_occ,\n+                    prompt_obj_ids,\n+                    reverse,\n+                )\n+                to_suppress[prompt_indices] = prompt_suppress\n \n             # Update metadata with occlusion information\n             is_obj_occluded = ~(binary_tracker_low_res_masks_global.any(dim=(-1, -2)))\n@@ -1062,15 +1142,35 @@ def _suppress_shrinked_masks(self, pred_masks, new_pred_masks, shrink_threshold=\n         pred_masks_after = torch.where(keep_mask, pred_masks, torch.clamp(pred_masks, max=-10.0))\n         return pred_masks_after\n \n-    def _suppress_object_pw_area_shrinkage(self, pred_masks):\n+    def _suppress_object_pw_area_shrinkage(\n+        self,\n+        pred_masks,\n+        prompt_ids: Optional[list[int]] = None,\n+    ):\n         \"\"\"\n-        This function suppresses masks that shrink in area after applying pixelwise non-overlapping constriants.\n-        Note that the final output can still be overlapping.\n+        This function suppresses masks that shrink in area after applying pixelwise non-overlapping constraints.\n+        When `prompt_ids` are provided, constraints are enforced independently per prompt group.\n         \"\"\"\n-        # Apply pixel-wise non-overlapping constraint based on mask scores\n+        if prompt_ids is None:\n+            return self._suppress_object_pw_area_shrinkage_impl(pred_masks)\n+\n+        if len(prompt_ids) != pred_masks.size(0):\n+            raise ValueError(\"prompt_ids must have the same length as pred_masks\")\n+\n+        prompt_ids_tensor = torch.tensor(prompt_ids, device=pred_masks.device, dtype=torch.long)\n+        pred_masks_grouped = pred_masks.clone()\n+        for prompt_id in prompt_ids_tensor.unique(sorted=True):\n+            indices = torch.nonzero(prompt_ids_tensor == prompt_id, as_tuple=True)[0]\n+            if indices.numel() == 0:\n+                continue\n+            pred_masks_grouped[indices] = self._suppress_object_pw_area_shrinkage_impl(pred_masks_grouped[indices])\n+        return pred_masks_grouped\n+\n+    def _suppress_object_pw_area_shrinkage_impl(self, pred_masks):\n+        if pred_masks.size(0) <= 1:\n+            return pred_masks\n+\n         pixel_level_non_overlapping_masks = self._apply_non_overlapping_constraints(pred_masks)\n-        # Fully suppress masks with high shrinkage (probably noisy) based on the pixel wise non-overlapping constraints\n-        # NOTE: The output of this function can be a no op if none of the masks shrinked by a large factor.\n         pred_masks = self._suppress_shrinked_masks(pred_masks, pixel_level_non_overlapping_masks)\n         return pred_masks\n \n@@ -1109,8 +1209,13 @@ def _tracker_update_memories(\n                 current_out = output_dict[\"non_cond_frame_outputs\"].pop(frame_idx)\n                 output_dict[\"cond_frame_outputs\"][frame_idx] = current_out\n \n-        # Apply non-overlapping constraints before memory encoding\n-        high_res_masks = self._suppress_object_pw_area_shrinkage(high_res_masks)\n+        # Apply non-overlapping constraints before memory encoding.\n+        # Constraints are enforced independently per prompt group.\n+        # Every object ID has a prompt_id assigned when it's created.\n+        prompt_ids_for_objects = [\n+            inference_session.obj_id_to_prompt_id[obj_id] for obj_id in inference_session.obj_ids\n+        ]\n+        high_res_masks = self._suppress_object_pw_area_shrinkage(high_res_masks, prompt_ids_for_objects)\n         # Use mask areas as a proxy for object scores\n         object_score_logits = torch.where((high_res_masks > 0).any(dim=(-1, -2)), 10.0, -10.0)\n \n@@ -1148,6 +1253,7 @@ def run_tracker_update_planning_phase(\n         det_out: dict[str, Tensor],\n         tracker_low_res_masks_global: Tensor,\n         tracker_obj_scores_global: Tensor,\n+        det_idx_to_prompt_id: dict[int, int],\n         streaming: bool = False,\n     ):\n         # initialize new metadata from previous metadata (its values will be updated later)\n@@ -1165,6 +1271,28 @@ def run_tracker_update_planning_phase(\n         # Step 1: make the update plan and resolve heuristics\n         det_mask_preds: Tensor = det_out[\"mask\"]  # low-res mask logits\n         det_scores: Tensor = det_out[\"scores\"].float()  # Keep as tensor!\n+        # det_idx_to_prompt_id maps every detection index to its prompt_id (created by _merge_detections_from_prompts).\n+        det_prompt_ids = (\n+            torch.tensor(\n+                [det_idx_to_prompt_id[idx] for idx in range(det_mask_preds.size(0))],\n+                device=det_mask_preds.device,\n+                dtype=torch.long,\n+            )\n+            if det_mask_preds.size(0) > 0\n+            else torch.empty(0, device=det_mask_preds.device, dtype=torch.long)\n+        )\n+        # Get prompt IDs for tracked objects.\n+        trk_prompt_ids = (\n+            torch.tensor(\n+                [inference_session.obj_id_to_prompt_id[obj_id] for obj_id in inference_session.obj_ids],\n+                device=tracker_low_res_masks_global.device\n+                if tracker_low_res_masks_global.numel() > 0\n+                else det_mask_preds.device,\n+                dtype=torch.long,\n+            )\n+            if tracker_low_res_masks_global.numel() > 0\n+            else torch.empty(0, device=det_mask_preds.device, dtype=torch.long)\n+        )\n         # a) match FA and SAM2 masks and find new objects\n         (\n             new_det_out_inds,\n@@ -1177,6 +1305,8 @@ def run_tracker_update_planning_phase(\n             det_scores=det_scores,\n             trk_masks=tracker_low_res_masks_global,\n             trk_obj_ids=inference_session.obj_ids,\n+            det_prompt_ids=det_prompt_ids,\n+            trk_prompt_ids=trk_prompt_ids,\n         )\n \n         # check whether we've hit the maximum number of objects we can track (and if so, drop some detections)\n@@ -1198,6 +1328,11 @@ def run_tracker_update_planning_phase(\n         new_det_start_obj_id = inference_session.max_obj_id + 1\n         new_det_obj_ids = list(range(new_det_start_obj_id, new_det_start_obj_id + new_det_num))\n \n+        # Assign prompt IDs to new objects based on which prompt detected them.\n+        for obj_id, det_idx in zip(new_det_obj_ids, new_det_out_inds):\n+            prompt_id = det_idx_to_prompt_id[det_idx]\n+            inference_session.obj_id_to_prompt_id[obj_id] = prompt_id\n+\n         # b) handle hotstart heuristics to remove objects\n         extra_metadata_new = deepcopy(\n             {\n@@ -1412,6 +1547,53 @@ def build_outputs(\n \n         return obj_id_to_mask\n \n+    def _merge_detections_from_prompts(\n+        self,\n+        all_detections: dict[int, dict[str, Tensor]],\n+        inference_session: Sam3VideoInferenceSession,\n+    ) -> tuple[dict[str, Tensor], dict[int, int]]:\n+        \"\"\"\n+        Merge detections from multiple prompts into a single detection output.\n+        Assigns unique object IDs and tracks which prompt detected each object.\n+\n+        Args:\n+            all_detections: Dictionary mapping prompt_id to detection outputs\n+            inference_session: Session to track obj_id -> prompt_id mapping\n+\n+        Returns:\n+            Tuple of (merged_det_out, det_idx_to_prompt_id) where det_idx_to_prompt_id\n+            maps detection index in the merged output to the prompt that produced it.\n+        \"\"\"\n+        merged_bboxes, merged_masks, merged_scores = [], [], []\n+        det_idx_to_prompt_id = {}\n+        det_idx = 0\n+\n+        for prompt_id, det_out in all_detections.items():\n+            num_dets = len(det_out[\"bbox\"])\n+            if num_dets > 0:\n+                merged_bboxes.append(det_out[\"bbox\"])\n+                merged_masks.append(det_out[\"mask\"])\n+                merged_scores.append(det_out[\"scores\"])\n+                for i in range(num_dets):\n+                    det_idx_to_prompt_id[det_idx + i] = prompt_id\n+                det_idx += num_dets\n+\n+        if merged_bboxes:\n+            merged_det_out = {\n+                \"bbox\": torch.cat(merged_bboxes),\n+                \"mask\": torch.cat(merged_masks),\n+                \"scores\": torch.cat(merged_scores),\n+            }\n+        else:\n+            device = inference_session.inference_device\n+            merged_det_out = {\n+                \"bbox\": torch.zeros(0, 4, device=device),\n+                \"mask\": torch.zeros(0, self.low_res_mask_size, self.low_res_mask_size, device=device),\n+                \"scores\": torch.zeros(0, device=device),\n+            }\n+\n+        return merged_det_out, det_idx_to_prompt_id\n+\n     def _det_track_one_frame(\n         self,\n         inference_session: Sam3VideoInferenceSession,\n@@ -1430,14 +1612,16 @@ def _det_track_one_frame(\n         pixel_values = inference_session.get_frame(frame_idx).unsqueeze(0)\n         vision_embeds = self.detector_model.get_vision_features(pixel_values=pixel_values)\n \n-        # Step 1: run detection\n-        # It returns a \"det_out\" dict for `frame_idx`\n-        # into `feature_cache`.\n-        det_out = self.run_detection(\n+        # Step 1: run detection for all prompts (efficiently reusing vision embeddings)\n+        # Returns dict mapping prompt_id to detection outputs\n+        all_detections = self.run_detection(\n             inference_session=inference_session,\n             vision_embeds=vision_embeds,\n         )\n \n+        # Merge detections from all prompts into single output for tracking\n+        det_out, det_idx_to_prompt_id = self._merge_detections_from_prompts(all_detections, inference_session)\n+\n         # share the vision encoder outputs from the detector to the tracker\n         vision_feats, vision_pos_embeds = self.get_vision_features_for_tracker(\n             vision_embeds=vision_embeds,\n@@ -1466,6 +1650,7 @@ def _det_track_one_frame(\n             det_out=det_out,\n             tracker_low_res_masks_global=tracker_low_res_masks_global,\n             tracker_obj_scores_global=tracker_obj_scores_global,\n+            det_idx_to_prompt_id=det_idx_to_prompt_id,\n             streaming=streaming,\n         )\n "
        },
        {
            "sha": "311ef6b5fab01552af751bf4da61738b6c4e997d",
            "filename": "src/transformers/models/sam3_video/processing_sam3_video.py",
            "status": "modified",
            "additions": 78,
            "deletions": 17,
            "changes": 95,
            "blob_url": "https://github.com/huggingface/transformers/blob/c3fb1b1a6ca1102f62b139c83a088a97e5a55477/src%2Ftransformers%2Fmodels%2Fsam3_video%2Fprocessing_sam3_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c3fb1b1a6ca1102f62b139c83a088a97e5a55477/src%2Ftransformers%2Fmodels%2Fsam3_video%2Fprocessing_sam3_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam3_video%2Fprocessing_sam3_video.py?ref=c3fb1b1a6ca1102f62b139c83a088a97e5a55477",
            "patch": "@@ -36,11 +36,11 @@ class Sam3VideoProcessor(ProcessorMixin):\n     [`~Sam3ImageProcessor.__call__`] and [`~Sam3VideoProcessor.__call__`] for more information.\n \n     Args:\n-        image_processor (`Sam2ImageProcessorFast`):\n-            An instance of [`Sam2ImageProcessorFast`].\n+        image_processor (`Sam3ImageProcessorFast`):\n+            An instance of [`Sam3ImageProcessorFast`].\n         video_processor (`Sam2VideoVideoProcessor`):\n             An instance of [`Sam2VideoVideoProcessor`].\n-        tokenizer ([`PreTrainedTokenizer`, `PreTrainedTokenizerFast`]):\n+        tokenizer ([`CLIPTokenizer`, `CLIPTokenizerFast`]):\n             An instance of [`PreTrainedTokenizer`, `PreTrainedTokenizerFast`]. The tokenizer is a required input.\n         target_size (`int`, *optional*):\n             The target size (target_size, target_size) to which the image will be resized.\n@@ -109,16 +109,36 @@ def __call__(\n \n         return encoding_image_processor\n \n-    def add_text_prompt(self, inference_session, text):\n+    def add_text_prompt(self, inference_session: Sam3VideoInferenceSession, text: Union[str, list[str]]):\n         \"\"\"\n-        Add text prompt to the inference session.\n+        Add text prompt(s) to the inference session.\n+\n+        Args:\n+            inference_session (`Sam3VideoInferenceSession`): The inference session.\n+            text (`str` or `list[str]`): The text prompt(s) to add.\n+\n+        Returns:\n+            `Sam3VideoInferenceSession`: The inference session with the added text prompt(s).\n         \"\"\"\n-        encoded_text = self.tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", max_length=32).to(\n-            inference_session.inference_device\n-        )\n-        inference_session.text_attention_mask = encoded_text.attention_mask\n-        inference_session.text_input_ids = encoded_text.input_ids\n-        inference_session.has_new_text_input = True\n+        if isinstance(text, str):\n+            text = [text]\n+\n+        prompt_ids = []\n+        for prompt_text in text:\n+            # Add prompt and get its ID (reuses existing if duplicate)\n+            prompt_id = inference_session.add_prompt(prompt_text)\n+\n+            # Only encode if this is a new prompt (not already in prompt_input_ids)\n+            if prompt_id not in inference_session.prompt_input_ids:\n+                encoded_text = self.tokenizer(\n+                    prompt_text, return_tensors=\"pt\", padding=\"max_length\", max_length=32\n+                ).to(inference_session.inference_device)\n+\n+                inference_session.prompt_input_ids[prompt_id] = encoded_text.input_ids\n+                inference_session.prompt_attention_masks[prompt_id] = encoded_text.attention_mask\n+\n+            prompt_ids.append(prompt_id)\n+\n         return inference_session\n \n     def init_video_session(\n@@ -194,20 +214,46 @@ def _apply_non_overlapping_constraints(self, pred_masks):\n         pred_masks = torch.where(keep, pred_masks, torch.clamp(pred_masks, max=-10.0))\n         return pred_masks\n \n-    def _apply_object_wise_non_overlapping_constraints(self, pred_masks, obj_scores, background_value=-10.0):\n+    def _apply_object_wise_non_overlapping_constraints(\n+        self,\n+        pred_masks,\n+        obj_scores,\n+        background_value=-10.0,\n+        prompt_ids=None,\n+    ):\n         \"\"\"\n-        Applies non-overlapping constraints object wise (i.e. only one object can claim the overlapping region)\n+        Applies non-overlapping constraints object wise (i.e. only one object can claim the overlapping region).\n+        Constraints are enforced independently for each prompt group when `prompt_ids` are provided.\n         \"\"\"\n+        if prompt_ids is None:\n+            return self._apply_object_wise_non_overlapping_constraints_impl(pred_masks, obj_scores, background_value)\n+\n+        if len(prompt_ids) != pred_masks.size(0):\n+            raise ValueError(\"prompt_ids must have the same length as pred_masks\")\n+\n+        pred_masks_grouped = pred_masks.clone()\n+        prompt_ids_tensor = torch.tensor(prompt_ids, device=pred_masks.device, dtype=torch.long)\n+        for prompt_id in prompt_ids_tensor.unique(sorted=True):\n+            indices = torch.nonzero(prompt_ids_tensor == prompt_id, as_tuple=True)[0]\n+            if indices.numel() == 0:\n+                continue\n+            prompt_masks = self._apply_object_wise_non_overlapping_constraints_impl(\n+                pred_masks_grouped[indices],\n+                obj_scores[indices],\n+                background_value,\n+            )\n+            pred_masks_grouped[indices] = prompt_masks.to(pred_masks_grouped.dtype)\n+        return pred_masks_grouped\n+\n+    def _apply_object_wise_non_overlapping_constraints_impl(self, pred_masks, obj_scores, background_value=-10.0):\n         pred_masks_single_score = torch.where(pred_masks > 0, obj_scores[..., None, None], background_value)\n-        # Apply pixel-wise non-overlapping constraint based on mask scores\n         pixel_level_non_overlapping_masks = self._apply_non_overlapping_constraints(pred_masks_single_score)\n-        # Replace object scores with pixel scores. Note, that now only one object can claim the overlapping region\n         pred_masks = torch.where(\n             pixel_level_non_overlapping_masks > 0,\n             pred_masks,\n             torch.clamp(pred_masks, max=background_value),\n         )\n-        return pred_masks\n+        return pred_masks.to(pred_masks_single_score.dtype)\n \n     def postprocess_outputs(\n         self,\n@@ -235,6 +281,8 @@ def postprocess_outputs(\n                   (top_left_x, top_left_y, bottom_right_x, bottom_right_y).\n                 - **masks** (`torch.Tensor` of shape `(num_objects, height, width)`): Binary segmentation masks\n                   for each object at the original video resolution.\n+                - **prompt_to_obj_ids** (`dict[str, list[int]]`): Mapping from prompt text to list of\n+                  object IDs detected by that prompt.\n         \"\"\"\n         obj_id_to_mask = model_outputs[\"obj_id_to_mask\"]  # low res masks (1, H_low, W_low)\n         curr_obj_ids = sorted(obj_id_to_mask.keys())\n@@ -301,22 +349,35 @@ def postprocess_outputs(\n \n             out_boxes_xyxy = masks_to_boxes(out_binary_masks)\n \n-        # apply non-overlapping constraints on the existing masklets\n+        # Apply non-overlapping constraints on the existing masklets.\n+        # Constraints are enforced independently per prompt group.\n         if out_binary_masks.shape[0] > 1:\n             assert len(out_binary_masks) == len(out_tracker_probs)\n+            prompt_ids_filtered = [\n+                inference_session.obj_id_to_prompt_id[int(obj_id)] for obj_id in out_obj_ids.tolist()\n+            ]\n             out_binary_masks = (\n                 self._apply_object_wise_non_overlapping_constraints(\n                     out_binary_masks.unsqueeze(1),\n                     out_tracker_probs.unsqueeze(1).to(out_binary_masks.device),\n                     background_value=0,\n+                    prompt_ids=prompt_ids_filtered,\n                 ).squeeze(1)\n             ) > 0\n \n+        # Build prompt_to_obj_ids mapping: group object IDs by their associated prompt text.\n+        prompt_to_obj_ids = {}\n+        for obj_id in out_obj_ids.tolist():\n+            prompt_id = inference_session.obj_id_to_prompt_id[obj_id]\n+            prompt_text = inference_session.prompts[prompt_id]\n+            prompt_to_obj_ids.setdefault(prompt_text, []).append(obj_id)\n+\n         outputs = {\n             \"object_ids\": out_obj_ids,\n             \"scores\": out_probs,\n             \"boxes\": out_boxes_xyxy,\n             \"masks\": out_binary_masks,\n+            \"prompt_to_obj_ids\": prompt_to_obj_ids,\n         }\n         return outputs\n "
        },
        {
            "sha": "7e0b335fa7a4bf90637f09537547cf2dc31e9fb3",
            "filename": "tests/models/sam3/test_modeling_sam3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c3fb1b1a6ca1102f62b139c83a088a97e5a55477/tests%2Fmodels%2Fsam3%2Ftest_modeling_sam3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c3fb1b1a6ca1102f62b139c83a088a97e5a55477/tests%2Fmodels%2Fsam3%2Ftest_modeling_sam3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam3%2Ftest_modeling_sam3.py?ref=c3fb1b1a6ca1102f62b139c83a088a97e5a55477",
            "patch": "@@ -987,7 +987,7 @@ class Sam3ModelIntegrationTest(unittest.TestCase):\n \n     def setUp(self):\n         super().setUp()\n-        model_name = \"../sam3-hf-v4-video-full\"\n+        model_name = \"facebook/sam3\"\n         self.model = Sam3Model.from_pretrained(model_name).to(torch.float32)\n         self.processor = Sam3Processor.from_pretrained(model_name)\n         self.model.to(torch_device)"
        },
        {
            "sha": "919116a4ac19e23a31d852174b2929b2530f3e49",
            "filename": "tests/models/sam3_tracker/test_modeling_sam3_tracker.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c3fb1b1a6ca1102f62b139c83a088a97e5a55477/tests%2Fmodels%2Fsam3_tracker%2Ftest_modeling_sam3_tracker.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c3fb1b1a6ca1102f62b139c83a088a97e5a55477/tests%2Fmodels%2Fsam3_tracker%2Ftest_modeling_sam3_tracker.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam3_tracker%2Ftest_modeling_sam3_tracker.py?ref=c3fb1b1a6ca1102f62b139c83a088a97e5a55477",
            "patch": "@@ -510,7 +510,7 @@ def prepare_video():\n class Sam3TrackerModelIntegrationTest(unittest.TestCase):\n     def setUp(self):\n         super().setUp()\n-        checkpoint_path = \"../sam3-hf-v4-video-full\"\n+        checkpoint_path = \"facebook/sam3\"\n         self.model = Sam3TrackerModel.from_pretrained(checkpoint_path).to(torch.float32)\n         self.processor = Sam3TrackerProcessor.from_pretrained(checkpoint_path)\n         self.model.to(torch_device)\n@@ -817,7 +817,7 @@ def test_inference_mask_generation_from_existing_points_and_mask(self):\n         )\n \n     def test_dummy_pipeline_generation(self):\n-        generator = pipeline(\"mask-generation\", model=\"../sam3-hf-v4-video-full\", device=torch_device)\n+        generator = pipeline(\"mask-generation\", model=\"facebook/sam3\", device=torch_device)\n         raw_image = prepare_image()\n \n         _ = generator(raw_image, points_per_batch=64)"
        },
        {
            "sha": "37660645dfc5dbf12dea7136eb4a9d3b43b60d1e",
            "filename": "tests/models/sam3_tracker_video/test_modeling_sam3_tracker_video.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c3fb1b1a6ca1102f62b139c83a088a97e5a55477/tests%2Fmodels%2Fsam3_tracker_video%2Ftest_modeling_sam3_tracker_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c3fb1b1a6ca1102f62b139c83a088a97e5a55477/tests%2Fmodels%2Fsam3_tracker_video%2Ftest_modeling_sam3_tracker_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam3_tracker_video%2Ftest_modeling_sam3_tracker_video.py?ref=c3fb1b1a6ca1102f62b139c83a088a97e5a55477",
            "patch": "@@ -66,8 +66,8 @@ def prepare_video():\n class Sam3TrackerVideoModelIntegrationTest(unittest.TestCase):\n     def setUp(self):\n         super().setUp()\n-        self.video_model = Sam3TrackerVideoModel.from_pretrained(\"../sam3-hf-v4-video-full\").to(torch.float32)\n-        self.processor = Sam3TrackerVideoProcessor.from_pretrained(\"../sam3-hf-v4-video-full\")\n+        self.video_model = Sam3TrackerVideoModel.from_pretrained(\"facebook/sam3\").to(torch.float32)\n+        self.processor = Sam3TrackerVideoProcessor.from_pretrained(\"facebook/sam3\")\n         self.video_model.to(torch_device)\n         self.video_model.eval()\n "
        },
        {
            "sha": "87a98d2029c5ceafe24efa2584c13874fd739220",
            "filename": "tests/models/sam3_video/test_modeling_sam3_video.py",
            "status": "modified",
            "additions": 60,
            "deletions": 1,
            "changes": 61,
            "blob_url": "https://github.com/huggingface/transformers/blob/c3fb1b1a6ca1102f62b139c83a088a97e5a55477/tests%2Fmodels%2Fsam3_video%2Ftest_modeling_sam3_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c3fb1b1a6ca1102f62b139c83a088a97e5a55477/tests%2Fmodels%2Fsam3_video%2Ftest_modeling_sam3_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam3_video%2Ftest_modeling_sam3_video.py?ref=c3fb1b1a6ca1102f62b139c83a088a97e5a55477",
            "patch": "@@ -42,7 +42,7 @@ def prepare_video():\n class Sam3VideoModelIntegrationTest(unittest.TestCase):\n     def setUp(self):\n         super().setUp()\n-        checkpoint_path = \"../sam3-hf-v4-video-full\"\n+        checkpoint_path = \"facebook/sam3\"\n         self.video_model = Sam3VideoModel.from_pretrained(checkpoint_path).to(torch.float32)\n         self.processor = Sam3VideoProcessor.from_pretrained(checkpoint_path)\n         self.video_model.to(torch_device)\n@@ -473,3 +473,62 @@ def test_inference_video_streaming_with_text_prompt(self):\n                     atol=5e-3,  # Higher tolerance for raw logits\n                     rtol=5e-3,\n                 )\n+\n+    def test_inference_video_multi_prompt(self):\n+        \"\"\"Test multi-prompt tracking - detecting multiple object categories in one pass.\"\"\"\n+        raw_video = prepare_video()\n+        inference_session = self.processor.init_video_session(\n+            video=raw_video,\n+            inference_device=torch_device,\n+            processing_device=\"cpu\",\n+            video_storage_device=\"cpu\",\n+        )\n+\n+        # Add multiple text prompts\n+        prompts = [\"person\", \"bed\"]\n+        self.processor.add_text_prompt(\n+            inference_session=inference_session,\n+            text=prompts,\n+        )\n+\n+        # Propagate through video frames\n+        outputs_per_frame = {}\n+        for model_outputs in self.video_model.propagate_in_video_iterator(\n+            inference_session=inference_session,\n+            max_frame_num_to_track=3,\n+        ):\n+            processed_outputs = self.processor.postprocess_outputs(inference_session, model_outputs)\n+            outputs_per_frame[model_outputs.frame_idx] = processed_outputs\n+\n+        # Check we processed the expected number of frames\n+        self.assertGreaterEqual(len(outputs_per_frame), 1)\n+        self.assertLessEqual(len(outputs_per_frame), 4)\n+\n+        # Check output structure for each frame\n+        for processed_outputs in outputs_per_frame.values():\n+            self.assertIn(\"object_ids\", processed_outputs)\n+            self.assertIn(\"scores\", processed_outputs)\n+            self.assertIn(\"boxes\", processed_outputs)\n+            self.assertIn(\"masks\", processed_outputs)\n+            self.assertIn(\"prompt_to_obj_ids\", processed_outputs)  # Multi-prompt specific\n+\n+            # Check prompt_to_obj_ids structure\n+            prompt_to_obj_ids = processed_outputs[\"prompt_to_obj_ids\"]\n+            self.assertIsInstance(prompt_to_obj_ids, dict)\n+            for prompt, obj_ids in prompt_to_obj_ids.items():\n+                self.assertIsInstance(prompt, str)\n+                self.assertIsInstance(obj_ids, list)\n+                # Each object ID should be in the main object_ids list\n+                for obj_id in obj_ids:\n+                    self.assertIn(obj_id, processed_outputs[\"object_ids\"].tolist())\n+\n+        # Check that we detected objects from multiple prompts\n+        first_frame_outputs = outputs_per_frame[min(outputs_per_frame.keys())]\n+        prompt_to_obj_ids = first_frame_outputs[\"prompt_to_obj_ids\"]\n+\n+        # Should have at least one prompt with detections\n+        self.assertGreater(len(prompt_to_obj_ids), 0)\n+\n+        # All prompts in prompt_to_obj_ids should be from our original prompts\n+        for prompt in prompt_to_obj_ids.keys():\n+            self.assertIn(prompt, prompts)"
        }
    ],
    "stats": {
        "total": 512,
        "additions": 425,
        "deletions": 87
    }
}