{
    "author": "pogpog",
    "message": "Fix link in gguf.md (#33768)\n\nChange hyphen to underscore for URL in link to convert_hf_to_gguf.py",
    "sha": "b77846a6e624b28952bfce8c54b5a20e0b44d2f6",
    "files": [
        {
            "sha": "0c6700544ae5c2b91ec337a9bf2229966b0fdca6",
            "filename": "docs/source/en/gguf.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b77846a6e624b28952bfce8c54b5a20e0b44d2f6/docs%2Fsource%2Fen%2Fgguf.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b77846a6e624b28952bfce8c54b5a20e0b44d2f6/docs%2Fsource%2Fen%2Fgguf.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fgguf.md?ref=b77846a6e624b28952bfce8c54b5a20e0b44d2f6",
            "patch": "@@ -102,7 +102,7 @@ Now you have access to the full, unquantized version of the model in the PyTorch\n with a plethora of other tools.\n \n In order to convert back to a `gguf` file, we recommend using the \n-[`convert-hf-to-gguf.py` file](https://github.com/ggerganov/llama.cpp/blob/master/convert-hf-to-gguf.py) from llama.cpp.\n+[`convert-hf-to-gguf.py` file](https://github.com/ggerganov/llama.cpp/blob/master/convert_hf_to_gguf.py) from llama.cpp.\n \n Here's how you would complete the script above to save the model and export it back to `gguf`:\n "
        }
    ],
    "stats": {
        "total": 2,
        "additions": 1,
        "deletions": 1
    }
}