{
    "author": "Bpriya42",
    "message": "docs: update LLaVA-NeXT model card (#38894)\n\n* docs: update LLaVA-NeXT model card\n\n* Update docs/source/en/model_doc/llava_next.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/llava_next.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/llava_next.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/llava_next.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/llava_next.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/llava_next.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/llava_next.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/llava_next.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* [docs] Updated llava_next model card\n\n* Update docs/source/en/model_doc/llava_next.md remove image sources\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* [fix] Change Flash Attention to SDPA badge\n\n* [doc] fixed quantization example\n\n* docs: updated contribution details and badges\n\n* Update llava_next.md\n\n---------\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "2781ad092dad77ff554cb70ec130b97e44cfba78",
    "files": [
        {
            "sha": "9d3f66a2090bbd11eae6d8aab6d87756ae8afa33",
            "filename": "docs/source/en/model_doc/llava_next.md",
            "status": "modified",
            "additions": 125,
            "deletions": 234,
            "changes": 359,
            "blob_url": "https://github.com/huggingface/transformers/blob/2781ad092dad77ff554cb70ec130b97e44cfba78/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2781ad092dad77ff554cb70ec130b97e44cfba78/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next.md?ref=2781ad092dad77ff554cb70ec130b97e44cfba78",
            "patch": "@@ -14,287 +14,178 @@ rendered properly in your Markdown viewer.\n \n -->\n \n-# LLaVA-NeXT\n-\n-<div class=\"flex flex-wrap space-x-1\">\n-<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n-<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<div style=\"float: right;\">\n+  <div class=\"flex flex-wrap space-x-1\">\n+        <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+        <img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+        <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+  </div>\n </div>\n \n-## Overview\n-\n-The LLaVA-NeXT model was proposed in [LLaVA-NeXT: Improved reasoning, OCR, and world knowledge](https://llava-vl.github.io/blog/2024-01-30-llava-next/) by Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, Yong Jae Lee. LLaVa-NeXT (also called LLaVa-1.6) improves upon [LLaVa](llava) by increasing the input image resolution and training on an improved visual instruction tuning dataset to improve OCR and common sense reasoning.\n-\n-The introduction from the blog is the following:\n-\n-*In October 2023, we released LLaVA-1.5 with a simple and efficient design along with great performance on a benchmark suite of 12 datasets. It has since served as the foundation of many comprehensive studies of data, model, and capabilities of large multimodal models (LMM), and has enabled various new applications.\n-\n-Today, we are thrilled to present LLaVA-NeXT, with improved reasoning, OCR, and world knowledge. LLaVA-NeXT even exceeds Gemini Pro on several benchmarks.\n-\n-Compared with LLaVA-1.5, LLaVA-NeXT has several improvements:\n-\n-Increasing the input image resolution to 4x more pixels. This allows it to grasp more visual details. It supports three aspect ratios, up to 672x672, 336x1344, 1344x336 resolution.\n-Better visual reasoning and OCR capability with an improved visual instruction tuning data mixture.\n-Better visual conversation for more scenarios, covering different applications. Better world knowledge and logical reasoning.\n-Efficient deployment and inference with SGLang.\n-Along with performance improvements, LLaVA-NeXT maintains the minimalist design and data efficiency of LLaVA-1.5. It re-uses the pretrained connector of LLaVA-1.5, and still uses less than 1M visual instruction tuning samples. The largest 34B variant finishes training in ~1 day with 32 A100s.*\n-\n-<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/llava_next_overview.png\"\n-alt=\"drawing\" width=\"600\"/>\n-\n-<small> LLaVa-NeXT incorporates a higher input resolution by encoding various patches of the input image. Taken from the <a href=\"https://huggingface.co/papers/2310.03744\">original paper.</a> </small>\n-\n-This model was contributed by [nielsr](https://huggingface.co/nielsr).\n-The original code can be found [here](https://github.com/haotian-liu/LLaVA/tree/main).\n-\n-## Usage tips\n-\n-- We advise users to use `padding_side=\"left\"` when computing batched generation as it leads to more accurate results. Simply make sure to call `processor.tokenizer.padding_side = \"left\"` before generating.\n-\n-<Tip warning={true}>\n-\n-- Llava-Next uses different number of patches for images and thus has to pad the inputs inside modeling code, aside from the padding done when processing the inputs. The default setting is \"left-padding\" if model is in `eval()` mode, otherwise \"right-padding\".\n+# LLaVA-NeXT\n \n-</Tip>\n+[LLaVAâ€‘NeXT](https://llava-vl.github.io/blog/2024-05-10-llava-next-stronger-llms/) improves on [Llava](./llava) by increasing the input image resolution by 4x more pixels and supporting 3 aspect ratios (up to 672x672, 336x1344, 1344x336) to better grasp visual details. It is also trained on an improved visual instruction tuning dataset covering more scenarios and applications to improve OCR and common sense reasoning.\n \n+You can find all the original LLaVAâ€‘NeXT checkpoints under the [LLaVA-NeXT](https://huggingface.co/collections/llava-hf/llava-next-65f75c4afac77fd37dbbe6cf) collection.\n \n-> [!NOTE]\n-> LLaVA models after release v4.46 will raise warnings about adding `processor.patch_size = {{patch_size}}`, `processor.num_additional_image_tokens = {{num_additional_image_tokens}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. It is strongly recommended to add the attributes to the processor if you own the model checkpoint, or open a PR if it is not owned by you.\n-Adding these attributes means that LLaVA will try to infer the number of image tokens required per image and expand the text with as many `<image>` placeholders as there will be tokens. Usually it is around 500 tokens per image, so make sure that the text is not truncated as otherwise there will be failure when merging the embeddings.\n-The attributes can be obtained from model config, as `model.config.vision_config.patch_size` or `model.config.vision_feature_select_strategy`. The `num_additional_image_tokens` should be `1` if the vision backbone adds a CLS token or `0` if nothing extra is added to the vision patches.\n+> [!TIP]\n+> This model was contributed by [nielsr](https://huggingface.co/nielsr).\n+>\n+> Click on the LLaVAâ€‘NeXT models in the right sidebar for more examples of how to apply Llava-NeXT to different multimodal tasks.\n \n+The example below demonstrates how to generate text based on an image with [`Pipeline`] or the [`AutoModel`] class.\n \n-### Formatting Prompts with Chat Templates  \n+<hfoptions id=\"usage\">\n \n-Each **checkpoint** is trained with a specific prompt format, depending on the underlying large language model backbone. To ensure correct formatting, use the processorâ€™s `apply_chat_template` method.  \n+<hfoption id=\"Pipeline\">\n \n-**Important:**  \n-- You must construct a conversation history â€” passing a plain string won't work.  \n-- Each message should be a dictionary with `\"role\"` and `\"content\"` keys.  \n-- The `\"content\"` should be a list of dictionaries for different modalities like `\"text\"` and `\"image\"`.  \n+```python\n+import torch  \n+from transformers import pipeline  \n+\n+pipeline = pipeline(  \n+    task=\"image-text-to-text\",  \n+    model=\"llava-hf/llava-v1.6-mistral-7b-hf\",  \n+    device=0,  \n+    torch_dtype=torch.bfloat16  \n+)  \n+messages = [  \n+    {  \n+        \"role\": \"user\",  \n+        \"content\": [  \n+            {  \n+                \"type\": \"image\",  \n+                \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\",  \n+            },  \n+            { \"type\": \"text\", \"text\": \"Describe this image.\"},  \n+        ]  \n+    }  \n+]  \n+pipeline(text=messages, max_new_tokens=20, return_full_text=False)\n+```\n \n+</hfoption>\n \n-Hereâ€™s an example of how to structure your input. We will use [llava-v1.6-mistral-7b-hf](https://huggingface.co/llava-hf/llava-v1.6-mistral-7b-hf) and a conversation history of text and image.\n+<hfoption id=\"AutoModel\">\n \n ```python\n-from transformers import LlavaNextProcessor\n+import torch  \n+import requests  \n+from PIL import Image  \n+from transformers import AutoProcessor, LlavaNextForConditionalGeneration  \n+\n+processor = AutoProcessor.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\")  \n+model = LlavaNextForConditionalGeneration.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\", torch_dtype=torch.float16).to(\"cuda\")  \n+\n+url = \"https://github.com/haotian-liu/LLaVA/blob/1a91fc274d7c35a9b50b3cb29c4247ae5837ce39/images/llava_v1_5_radar.jpg?raw=true\"  \n+image = Image.open(requests.get(url, stream=True).raw)  \n+\n+conversation = [  \n+    {  \n+        \"role\": \"user\",  \n+        \"content\": [  \n+            {\"type\": \"image\"},  \n+            {\"type\": \"text\", \"text\": \"What is shown in this image?\"},  \n+        ],  \n+    },  \n+]  \n+prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)  \n+inputs = processor(image, prompt, return_tensors=\"pt\").to(\"cuda\")  \n+output = model.generate(**inputs, max_new_tokens=100)  \n+print(processor.decode(output[0], skip_special_tokens=True))  \n+```\n \n-processor = LlavaNextProcessor.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\")\n+</hfoption>\n \n-conversation = [\n-    {\n-        \"role\": \"user\",\n-        \"content\": [\n-            {\"type\": \"image\"},\n-            {\"type\": \"text\", \"text\": \"Whatâ€™s shown in this image?\"},\n-        ],\n-    },\n-    {\n-        \"role\": \"assistant\",\n-        \"content\": [{\"type\": \"text\", \"text\": \"This image shows a red stop sign.\"},]\n-    },\n-    {\n-\n-        \"role\": \"user\",\n-        \"content\": [\n-            {\"type\": \"text\", \"text\": \"Describe the image in more details.\"},\n-        ],\n-    },\n-]\n+</hfoptions>\n \n-text_prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n+Quantization reduces the memory burden of large models by representing the weights in a lower precision. Refer to the [Quantization](../quantization/overview) overview for more available quantization backends.\n \n-# Note that the template simply formats your prompt, you still have to tokenize it and obtain pixel values for your images\n-print(text_prompt)\n->>> \"[INST] <image>\\nWhat's shown in this image? [/INST] This image shows a red stop sign. [INST] Describe the image in more details. [/INST]\"\n-```\n+The example below uses [bitsandbytes](../quantization/bitsandbytes) to only quantize the weights to int4.\n \n-- If you want to construct a chat prompt yourself, below is a list of possible formats\n-.\n-[llava-v1.6-mistral-7b-hf](https://huggingface.co/llava-hf/llava-v1.6-mistral-7b-hf) requires the following format:\n-```bash\n-\"[INST] <image>\\nWhat is shown in this image? [/INST]\"\n+```python\n+import torch  \n+import requests  \n+from PIL import Image  \n+from transformers import AutoModelForImageTextToText, AutoProcessor, BitsAndBytesConfig  \n+\n+quant_config = BitsAndBytesConfig(  \n+    load_in_4bit=True,  \n+    bnb_4bit_compute_dtype=torch.float16,  \n+    bnb_4bit_quant_type=\"nf4\"  \n+)  \n+\n+processor = AutoProcessor.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\")  \n+model = AutoModelForImageTextToText.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\", quantization_config=quant_config, device_map=\"auto\")  \n+\n+url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/llava_next_ocr.png\"  \n+image = Image.open(requests.get(url, stream=True).raw)  \n+\n+conversation = [  \n+    {  \n+        \"role\": \"user\",  \n+        \"content\": [  \n+            {\"type\": \"image\"},  \n+            {\"type\": \"text\", \"text\": \"What does this chart show?\"},  \n+        ],  \n+    },  \n+]  \n+prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)  \n+inputs = processor(image, prompt, return_tensors=\"pt\").to(\"cuda\")  \n+\n+with torch.inference_mode():  \n+    output = model.generate(**inputs, max_new_tokens=100)  \n+print(processor.decode(output[0], skip_special_tokens=True))  \n ```\n \n-[llava-v1.6-vicuna-7b-hf](https://huggingface.co/llava-hf/llava-v1.6-vicuna-7b-hf) and [llava-v1.6-vicuna-13b-hf](https://huggingface.co/llava-hf/llava-v1.6-vicuna-13b-hf) require the following format:\n-```bash\n-\"A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <image>\\nWhat is shown in this image? ASSISTANT:\"\n-```\n \n-[llava-v1.6-34b-hf](https://huggingface.co/llava-hf/llava-v1.6-34b-hf) requires the following format:\n-```bash\n-\"<|im_start|>system\\nAnswer the questions.<|im_end|><|im_start|>user\\n<image>\\nWhat is shown in this image?<|im_end|><|im_start|>assistant\\n\"\n-```\n+## Notes\n \n-[llama3-llava-next-8b-hf](https://huggingface.co/llava-hf/llava-next-8b-hf) requires the following format:\n+* Different checkpoints (Mistral, Vicuna, etc.) require a specific prompt format depending on the underlying LLM. Always use [`~ProcessorMixin.apply_chat_template`] to ensure correct formatting. Refer to the [Templates](../chat_templating) guide for more details.\n \n-```bash\n-\"<|start_header_id|>system<|end_header_id|>\\n\\nYou are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.<|eot_id|><|start_header_id|><|start_header_id|>user<|end_header_id|>\\n\\n<image>\\nWhat is shown in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n-```\n+* Set `padding_side=\"left\"` during batched generation for more accurate results.\n \n-[llava-next-72b-hf](https://huggingface.co/llava-hf/llava-next-72b-hf) and [llava-next-110b-hf](https://huggingface.co/llava-hf/llava-next-110b-hf) require the following format:\n-\n-```bash\n-\"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<image>\\nWhat is shown in this image?<|im_end|>\\n<|im_start|>assistant\\n\"\n+```py\n+processor.tokenizer.padding_side = \"left\"\n ```\n \n-ðŸš€ **Bonus:** If you're using `transformers>=4.49.0`, you can also get a vectorized output from `apply_chat_template`. See the **Usage Examples** below for more details on how to use it.\n-\n+* LLaVA-NeXT uses different numbers of patches for images and pads the inputs inside the modeling code except when padding is done during processing. The default setting is *left-padding* if the model is in `eval()` mode, otherwise it is *right-padding*.\n \n+* LLaVA models after v4.46 raises warnings about adding `processor.patch_size = {{patch_size}}`, `processor.num_additional_image_tokens = {{num_additional_image_tokens}}`, and `processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. It is strongly recommended to add these attributes to the processor if you own the model checkpoint or open a PR if it isn't.\n \n-## Usage example\n+  Adding these attributes means LLaVA will try to infer the number of image tokens required per image and expand the text with the same number of `<image>` token placeholders. There are usually ~500 tokens per image, so make sure the text is not truncated because it will cause a failure when merging the embeddings. The attributes can be found in `model.config.vision_config.patch_size` or `model.config.vision_feature_select_strategy`.\n \n-### Single image inference\n+  The `num_additional_image_tokens` should be `1` if the vision backbone adds a `CLS` token or `0` if nothing extra is added.\n \n-Here's how to load the model and perform inference in half-precision (`torch.float16`):\n+* The example below demonstrates inference with multiple input images.\n \n ```python\n from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\n-import torch\n from PIL import Image\n-import requests\n+import requests, torch\n \n processor = LlavaNextProcessor.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\")\n+model = LlavaNextForConditionalGeneration.from_pretrained(\n+    \"llava-hf/llava-v1.6-mistral-7b-hf\", torch_dtype=torch.float16\n+).to(\"cuda\")\n \n-model = LlavaNextForConditionalGeneration.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\", torch_dtype=torch.float16)\n-model.to(\"cuda:0\")\n+# Load multiple images\n+url1 = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/llava_next_ocr.png\"\n+url2 = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/llava_next_comparison.png\"\n \n-# prepare image and text prompt, using the appropriate prompt template\n-url = \"https://github.com/haotian-liu/LLaVA/blob/1a91fc274d7c35a9b50b3cb29c4247ae5837ce39/images/llava_v1_5_radar.jpg?raw=true\"\n-image = Image.open(requests.get(url, stream=True).raw)\n+image1 = Image.open(requests.get(url1, stream=True).raw)\n+image2 = Image.open(requests.get(url2, stream=True).raw)\n \n conversation = [\n-    {\n-        \"role\": \"user\",\n-        \"content\": [\n-            {\"type\": \"image\"},\n-            {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n-        ],\n-    },\n+    {\"role\": \"user\", \"content\": [{\"type\": \"image\"}, {\"type\": \"image\"}, {\"type\": \"text\", \"text\": \"Compare these two images and describe the differences.\"}]}\n ]\n prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n-inputs = processor(image, prompt, return_tensors=\"pt\").to(\"cuda:0\")\n+inputs = processor([image1, image2], prompt, return_tensors=\"pt\").to(\"cuda\")\n \n-# autoregressively complete prompt\n output = model.generate(**inputs, max_new_tokens=100)\n-\n print(processor.decode(output[0], skip_special_tokens=True))\n ```\n \n-### Multi image inference\n-\n-LLaVa-Next can perform inference with multiple images as input, where images either belong to the same prompt or different prompts (in batched inference). Here is how you can do it:\n-\n-```python\n-import requests\n-from PIL import Image\n-import torch\n-from transformers import AutoProcessor, AutoModelForImageTextToText\n-\n-# Load the model in half-precision\n-model = AutoModelForImageTextToText.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\", torch_dtype=torch.float16, device_map=\"auto\")\n-processor = AutoProcessor.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\")\n-\n-# Get three different images\n-url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n-image_stop = Image.open(requests.get(url, stream=True).raw)\n-\n-url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n-image_cats = Image.open(requests.get(url, stream=True).raw)\n-\n-url = \"https://huggingface.co/microsoft/kosmos-2-patch14-224/resolve/main/snowman.jpg\"\n-image_snowman = Image.open(requests.get(url, stream=True).raw)\n-\n-# Prepare a batch of two prompts, where the first one is a multi-turn conversation and the second is not\n-conversation_1 = [\n-    {\n-        \"role\": \"user\",\n-        \"content\": [\n-            {\"type\": \"image\"},\n-            {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n-            ],\n-    },\n-    {\n-        \"role\": \"assistant\",\n-        \"content\": [\n-            {\"type\": \"text\", \"text\": \"There is a red stop sign in the image.\"},\n-            ],\n-    },\n-    {\n-        \"role\": \"user\",\n-        \"content\": [\n-            {\"type\": \"image\"},\n-            {\"type\": \"text\", \"text\": \"What about this image? How many cats do you see?\"},\n-            ],\n-    },\n-]\n-\n-conversation_2 = [\n-    {\n-        \"role\": \"user\",\n-        \"content\": [\n-            {\"type\": \"image\"},\n-            {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n-            ],\n-    },\n-]\n-\n-prompt_1 = processor.apply_chat_template(conversation_1, add_generation_prompt=True)\n-prompt_2 = processor.apply_chat_template(conversation_2, add_generation_prompt=True)\n-prompts = [prompt_1, prompt_2]\n-\n-# We can simply feed images in the order they have to be used in the text prompt\n-# Each \"<image>\" token uses one image leaving the next for the subsequent \"<image>\" tokens\n-inputs = processor(images=[image_stop, image_cats, image_snowman], text=prompts, padding=True, return_tensors=\"pt\").to(model.device)\n-\n-# Generate\n-generate_ids = model.generate(**inputs, max_new_tokens=30)\n-processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n-```\n-\n-## Model optimization\n-\n-### Quantization using Bitsandbytes\n-\n-The model can be loaded in 8 or 4 bits, greatly reducing the memory requirements while maintaining the performance of the original model. First make sure to install bitsandbytes, `pip install bitsandbytes`, and to have access to a GPU/accelerator that is supported by the library.\n-\n-<Tip>\n-\n-bitsandbytes is being refactored to support multiple backends beyond CUDA. Currently, ROCm (AMD GPU) and Intel CPU implementations are mature, with Intel XPU in progress and Apple Silicon support expected by Q4/Q1. For installation instructions and the latest backend updates, visit [this link](https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend).\n-\n-We value your feedback to help identify bugs before the full release! Check out [these docs](https://huggingface.co/docs/bitsandbytes/main/en/non_cuda_backends) for more details and feedback links.\n-\n-</Tip>\n-\n-Simply change the snippet above with:\n-\n-```python\n-from transformers import AutoModelForImageTextToText, BitsAndBytesConfig\n-\n-# specify how to quantize the model\n-quantization_config = BitsAndBytesConfig(\n-    load_in_4bit=True,\n-    bnb_4bit_quant_type=\"nf4\",\n-    bnb_4bit_compute_dtype=torch.float16,\n-)\n-\n-model = AutoModelForImageTextToText.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\", quantization_config=quantization_config, device_map=\"auto\")\n-```\n-\n-### Use Flash-Attention 2 to further speed-up generation\n-\n-First make sure to install flash-attn. Refer to the [original repository of Flash Attention](https://github.com/Dao-AILab/flash-attention) regarding that package installation. Simply change the snippet above with:\n-\n-```python\n-from transformers import AutoModelForImageTextToText\n-\n-model = AutoModelForImageTextToText.from_pretrained(\n-    model_id,\n-    torch_dtype=torch.float16,\n-    use_flash_attention_2=True\n-).to(0)\n-```\n \n ## LlavaNextConfig\n "
        }
    ],
    "stats": {
        "total": 359,
        "additions": 125,
        "deletions": 234
    }
}