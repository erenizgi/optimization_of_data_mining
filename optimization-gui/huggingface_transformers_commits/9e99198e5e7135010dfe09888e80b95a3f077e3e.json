{
    "author": "cyyever",
    "message": "Use | for Optional and Union typing (#41646)\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>",
    "sha": "9e99198e5e7135010dfe09888e80b95a3f077e3e",
    "files": [
        {
            "sha": "1396caa1b0c3577bb855275276aa13e82ed4642e",
            "filename": "benchmark/benches/llama.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e99198e5e7135010dfe09888e80b95a3f077e3e/benchmark%2Fbenches%2Fllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e99198e5e7135010dfe09888e80b95a3f077e3e/benchmark%2Fbenches%2Fllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark%2Fbenches%2Fllama.py?ref=9e99198e5e7135010dfe09888e80b95a3f077e3e",
            "patch": "@@ -16,7 +16,6 @@\n from logging import Logger\n from threading import Event, Thread\n from time import perf_counter, sleep\n-from typing import Optional\n \n \n # Add the parent directory to Python path to import benchmarks_entrypoint\n@@ -145,7 +144,7 @@ def multinomial_sample_one_no_sync(probs_sort):  # Does multinomial sampling wit\n             q = torch.empty_like(probs_sort).exponential_(1)\n             return torch.argmax(probs_sort / q, dim=-1, keepdim=True).to(dtype=torch.int)\n \n-        def logits_to_probs(logits, temperature: float = 1.0, top_k: Optional[int] = None):\n+        def logits_to_probs(logits, temperature: float = 1.0, top_k: int | None = None):\n             logits = logits / max(temperature, 1e-5)\n \n             if top_k is not None:\n@@ -155,7 +154,7 @@ def logits_to_probs(logits, temperature: float = 1.0, top_k: Optional[int] = Non\n             probs = torch.nn.functional.softmax(logits, dim=-1)\n             return probs\n \n-        def sample(logits, temperature: float = 1.0, top_k: Optional[int] = None):\n+        def sample(logits, temperature: float = 1.0, top_k: int | None = None):\n             probs = logits_to_probs(logits[0, -1], temperature, top_k)\n             idx_next = multinomial_sample_one_no_sync(probs)\n             return idx_next, probs"
        },
        {
            "sha": "3e3e37171ae82c442af3dd270dfa4f2e2ad28c28",
            "filename": "benchmark_v2/framework/benchmark_config.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e99198e5e7135010dfe09888e80b95a3f077e3e/benchmark_v2%2Fframework%2Fbenchmark_config.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e99198e5e7135010dfe09888e80b95a3f077e3e/benchmark_v2%2Fframework%2Fbenchmark_config.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark_v2%2Fframework%2Fbenchmark_config.py?ref=9e99198e5e7135010dfe09888e80b95a3f077e3e",
            "patch": "@@ -1,7 +1,7 @@\n import hashlib\n import json\n import logging\n-from typing import Any, Optional\n+from typing import Any\n \n \n KERNELIZATION_AVAILABLE = False\n@@ -27,11 +27,11 @@ def __init__(\n         sequence_length: int = 128,\n         num_tokens_to_generate: int = 128,\n         attn_implementation: str = \"eager\",\n-        sdpa_backend: Optional[str] = None,\n-        compile_mode: Optional[str] = None,\n-        compile_options: Optional[dict[str, Any]] = None,\n+        sdpa_backend: str | None = None,\n+        compile_mode: str | None = None,\n+        compile_options: dict[str, Any] | None = None,\n         kernelize: bool = False,\n-        name: Optional[str] = None,\n+        name: str | None = None,\n         skip_validity_check: bool = False,\n     ) -> None:\n         # Benchmark parameters\n@@ -128,8 +128,8 @@ def from_dict(cls, data: dict[str, Any], skip_validity_check: bool = False) -> \"\n \n \n def cross_generate_configs(\n-    attn_impl_and_sdpa_backend: list[tuple[str, Optional[str]]],\n-    compiled_mode: list[Optional[str]],\n+    attn_impl_and_sdpa_backend: list[tuple[str, str | None]],\n+    compiled_mode: list[str | None],\n     kernelized: list[bool],\n     warmup_iterations: int = 5,\n     measurement_iterations: int = 20,"
        },
        {
            "sha": "a6ec16aecb61f16dffe4e60826b84bde1d0f5f58",
            "filename": "benchmark_v2/framework/benchmark_runner.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e99198e5e7135010dfe09888e80b95a3f077e3e/benchmark_v2%2Fframework%2Fbenchmark_runner.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e99198e5e7135010dfe09888e80b95a3f077e3e/benchmark_v2%2Fframework%2Fbenchmark_runner.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark_v2%2Fframework%2Fbenchmark_runner.py?ref=9e99198e5e7135010dfe09888e80b95a3f077e3e",
            "patch": "@@ -8,7 +8,7 @@\n from contextlib import nullcontext\n from datetime import datetime\n from queue import Queue\n-from typing import Any, Optional\n+from typing import Any\n \n import torch\n from tqdm import trange\n@@ -74,7 +74,7 @@ def get_git_revision() -> str:\n         return git_hash.readline().strip()\n \n \n-def get_sdpa_backend(backend_name: Optional[str]) -> Optional[torch.nn.attention.SDPBackend]:\n+def get_sdpa_backend(backend_name: str | None) -> torch.nn.attention.SDPBackend | None:\n     \"\"\"Get the SDPA backend enum from string name.\"\"\"\n     if backend_name is None:\n         return None\n@@ -145,7 +145,7 @@ class BenchmarkRunner:\n     \"\"\"Main benchmark runner that coordinates benchmark execution.\"\"\"\n \n     def __init__(\n-        self, logger: logging.Logger, output_dir: str = \"benchmark_results\", commit_id: Optional[str] = None\n+        self, logger: logging.Logger, output_dir: str = \"benchmark_results\", commit_id: str | None = None\n     ) -> None:\n         # Those stay constant for the whole run\n         self.logger = logger\n@@ -156,7 +156,7 @@ def __init__(\n         # Attributes that are reset for each model\n         self._setup_for = \"\"\n         # Attributes that are reset for each run\n-        self.model: Optional[GenerationMixin] = None\n+        self.model: GenerationMixin | None = None\n \n     def cleanup(self) -> None:\n         del self.model\n@@ -251,8 +251,8 @@ def run_one_benchmark(self, model_id: str, config: BenchmarkConfig, num_tokens_t\n     def time_generate(\n         self,\n         max_new_tokens: int,\n-        gpu_monitor: Optional[GPUMonitor] = None,\n-    ) -> tuple[float, list[float], str, Optional[GPURawMetrics]]:\n+        gpu_monitor: GPUMonitor | None = None,\n+    ) -> tuple[float, list[float], str, GPURawMetrics | None]:\n         \"\"\"Time the latency of a call to model.generate() with the given (inputs) and (max_new_tokens).\"\"\"\n         # Prepare gpu monitoring if needed\n         if gpu_monitor is not None:"
        },
        {
            "sha": "f5e740d97b5575086ca3db7f02f9f365e1924dcd",
            "filename": "benchmark_v2/framework/data_classes.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e99198e5e7135010dfe09888e80b95a3f077e3e/benchmark_v2%2Fframework%2Fdata_classes.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e99198e5e7135010dfe09888e80b95a3f077e3e/benchmark_v2%2Fframework%2Fdata_classes.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark_v2%2Fframework%2Fdata_classes.py?ref=9e99198e5e7135010dfe09888e80b95a3f077e3e",
            "patch": "@@ -1,6 +1,6 @@\n from dataclasses import dataclass\n from datetime import datetime\n-from typing import Any, Optional, Union\n+from typing import Any\n \n import numpy as np\n \n@@ -90,14 +90,14 @@ def accumulate(\n         e2e_latency: float,\n         token_generation_times: list[float],\n         decoded_output: str,\n-        gpu_metrics: Optional[GPURawMetrics],\n+        gpu_metrics: GPURawMetrics | None,\n     ) -> None:\n         self.e2e_latency.append(e2e_latency)\n         self.token_generation_times.append(token_generation_times)\n         self.decoded_outputs.append(decoded_output)\n         self.gpu_metrics.append(gpu_metrics)\n \n-    def to_dict(self) -> dict[str, Union[None, int, float]]:\n+    def to_dict(self) -> dict[str, None | int | float]:\n         # Save GPU metrics as None if it contains only None values\n         if all(gm is None for gm in self.gpu_metrics):\n             gpu_metrics = None\n@@ -111,7 +111,7 @@ def to_dict(self) -> dict[str, Union[None, int, float]]:\n         }\n \n     @classmethod\n-    def from_dict(cls, data: dict[str, Union[None, int, float]]) -> \"BenchmarkResult\":\n+    def from_dict(cls, data: dict[str, None | int | float]) -> \"BenchmarkResult\":\n         # Handle GPU metrics, which is saved as None if it contains only None values\n         if data[\"gpu_metrics\"] is None:\n             gpu_metrics = [None for _ in range(len(data[\"e2e_latency\"]))]"
        },
        {
            "sha": "4657227fb934b5b083f937f6efd48ef5dac0bae1",
            "filename": "benchmark_v2/framework/hardware_metrics.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e99198e5e7135010dfe09888e80b95a3f077e3e/benchmark_v2%2Fframework%2Fhardware_metrics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e99198e5e7135010dfe09888e80b95a3f077e3e/benchmark_v2%2Fframework%2Fhardware_metrics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark_v2%2Fframework%2Fhardware_metrics.py?ref=9e99198e5e7135010dfe09888e80b95a3f077e3e",
            "patch": "@@ -7,7 +7,6 @@\n from dataclasses import dataclass\n from enum import Enum\n from logging import Logger\n-from typing import Optional, Union\n \n import gpustat\n import psutil\n@@ -42,7 +41,7 @@ def __init__(self) -> None:\n         self.cpu_count = psutil.cpu_count()\n         self.memory_total_mb = int(psutil.virtual_memory().total / (1024 * 1024))\n \n-    def to_dict(self) -> dict[str, Union[None, int, float, str]]:\n+    def to_dict(self) -> dict[str, None | int | float | str]:\n         return {\n             \"gpu_name\": self.gpu_name,\n             \"gpu_memory_total_gb\": self.gpu_memory_total_gb,\n@@ -109,7 +108,7 @@ class GPURawMetrics:\n     timestamp_0: float  # in seconds\n     monitoring_status: GPUMonitoringStatus\n \n-    def to_dict(self) -> dict[str, Union[None, int, float, str]]:\n+    def to_dict(self) -> dict[str, None | int | float | str]:\n         return {\n             \"utilization\": self.utilization,\n             \"memory_used\": self.memory_used,\n@@ -123,7 +122,7 @@ def to_dict(self) -> dict[str, Union[None, int, float, str]]:\n class GPUMonitor:\n     \"\"\"Monitor GPU utilization during benchmark execution.\"\"\"\n \n-    def __init__(self, sample_interval_sec: float = 0.1, logger: Optional[Logger] = None):\n+    def __init__(self, sample_interval_sec: float = 0.1, logger: Logger | None = None):\n         self.sample_interval_sec = sample_interval_sec\n         self.logger = logger if logger is not None else logging.getLogger(__name__)\n "
        },
        {
            "sha": "f4b2528c981d0f904ddd9b34d9d3b50024583feb",
            "filename": "tests/deepspeed/test_deepspeed.py",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e99198e5e7135010dfe09888e80b95a3f077e3e/tests%2Fdeepspeed%2Ftest_deepspeed.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e99198e5e7135010dfe09888e80b95a3f077e3e/tests%2Fdeepspeed%2Ftest_deepspeed.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fdeepspeed%2Ftest_deepspeed.py?ref=9e99198e5e7135010dfe09888e80b95a3f077e3e",
            "patch": "@@ -19,7 +19,6 @@\n import unittest\n from copy import deepcopy\n from functools import partial\n-from typing import Optional\n \n import datasets\n from parameterized import parameterized\n@@ -1254,8 +1253,8 @@ def run_and_check(\n         do_eval: bool = True,\n         quality_checks: bool = True,\n         fp32: bool = False,\n-        extra_args_str: Optional[str] = None,\n-        remove_args_str: Optional[str] = None,\n+        extra_args_str: str | None = None,\n+        remove_args_str: str | None = None,\n     ):\n         # we are doing quality testing so using a small real model\n         output_dir = self.run_trainer(\n@@ -1287,8 +1286,8 @@ def run_trainer(\n         do_eval: bool = True,\n         distributed: bool = True,\n         fp32: bool = False,\n-        extra_args_str: Optional[str] = None,\n-        remove_args_str: Optional[str] = None,\n+        extra_args_str: str | None = None,\n+        remove_args_str: str | None = None,\n     ):\n         max_len = 32\n         data_dir = self.test_file_dir / \"../fixtures/tests_samples/wmt_en_ro\""
        },
        {
            "sha": "71db2022bbf895c1beb229ab141c73b334b21b95",
            "filename": "tests/extended/test_trainer_ext.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e99198e5e7135010dfe09888e80b95a3f077e3e/tests%2Fextended%2Ftest_trainer_ext.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e99198e5e7135010dfe09888e80b95a3f077e3e/tests%2Fextended%2Ftest_trainer_ext.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fextended%2Ftest_trainer_ext.py?ref=9e99198e5e7135010dfe09888e80b95a3f077e3e",
            "patch": "@@ -17,7 +17,6 @@\n import re\n import sys\n from pathlib import Path\n-from typing import Optional\n from unittest.mock import patch\n \n from parameterized import parameterized\n@@ -251,13 +250,13 @@ def run_trainer(\n         learning_rate: float = 3e-3,\n         optim: str = \"adafactor\",\n         distributed: bool = False,\n-        extra_args_str: Optional[str] = None,\n+        extra_args_str: str | None = None,\n         eval_steps: int = 0,\n         predict_with_generate: bool = True,\n         do_train: bool = True,\n         do_eval: bool = True,\n         do_predict: bool = True,\n-        n_gpus_to_use: Optional[int] = None,\n+        n_gpus_to_use: int | None = None,\n     ):\n         data_dir = self.test_file_dir / \"../fixtures/tests_samples/wmt_en_ro\"\n         output_dir = self.get_auto_remove_tmp_dir()"
        },
        {
            "sha": "80da7886dccf5265b6414caf5de2d8ecfd8405c0",
            "filename": "tests/generation/test_continuous_batching.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e99198e5e7135010dfe09888e80b95a3f077e3e/tests%2Fgeneration%2Ftest_continuous_batching.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e99198e5e7135010dfe09888e80b95a3f077e3e/tests%2Fgeneration%2Ftest_continuous_batching.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_continuous_batching.py?ref=9e99198e5e7135010dfe09888e80b95a3f077e3e",
            "patch": "@@ -13,7 +13,6 @@\n # limitations under the License.\n \n import unittest\n-from typing import Optional\n \n import torch\n from parameterized import parameterized\n@@ -43,8 +42,8 @@ class ContinuousBatchingTest(unittest.TestCase):\n     )\n     def test_group_layers(\n         self,\n-        layer_types_str: Optional[str],\n-        sliding_window: Optional[int],\n+        layer_types_str: str | None,\n+        sliding_window: int | None,\n         expected_groups: str,\n     ) -> None:\n         # Take a config and change the layer_types attribute to the mix we want"
        },
        {
            "sha": "71efb438be933ef064ce340f9a2a19f891419189",
            "filename": "tests/generation/test_logits_process.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e99198e5e7135010dfe09888e80b95a3f077e3e/tests%2Fgeneration%2Ftest_logits_process.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e99198e5e7135010dfe09888e80b95a3f077e3e/tests%2Fgeneration%2Ftest_logits_process.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_logits_process.py?ref=9e99198e5e7135010dfe09888e80b95a3f077e3e",
            "patch": "@@ -13,7 +13,6 @@\n # limitations under the License.\n \n import unittest\n-from typing import Union\n \n import numpy as np\n from parameterized import parameterized\n@@ -90,7 +89,7 @@ def test_min_length_dist_processor(self):\n         self.assertFalse(torch.isinf(scores_before_min_length).any())\n \n     @parameterized.expand([(0,), ([0, 18],)])\n-    def test_new_min_length_dist_processor(self, eos_token_id: Union[int, list[int]]):\n+    def test_new_min_length_dist_processor(self, eos_token_id: int | list[int]):\n         vocab_size = 20\n         batch_size = 4\n "
        },
        {
            "sha": "4120f0926f0fb137ad286fb147cc436c95c422c5",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e99198e5e7135010dfe09888e80b95a3f077e3e/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e99198e5e7135010dfe09888e80b95a3f077e3e/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=9e99198e5e7135010dfe09888e80b95a3f077e3e",
            "patch": "@@ -23,7 +23,6 @@\n import unittest\n import warnings\n from pathlib import Path\n-from typing import Optional\n \n import numpy as np\n import pytest\n@@ -908,7 +907,7 @@ def test_prompt_lookup_decoding_stops_at_eos(self):\n \n     @pytest.mark.generate\n     def test_left_padding_compatibility(\n-        self, unpadded_custom_inputs: Optional[dict] = None, padded_custom_inputs: Optional[dict] = None\n+        self, unpadded_custom_inputs: dict | None = None, padded_custom_inputs: dict | None = None\n     ):\n         \"\"\"\n         Tests that adding left-padding yields the same logits as the original input. Exposes arguments for custom"
        },
        {
            "sha": "a59aab3a1c1e6ce0d224d58bbd82d8066d9b8e6e",
            "filename": "tests/models/bridgetower/test_image_processing_bridgetower.py",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e99198e5e7135010dfe09888e80b95a3f077e3e/tests%2Fmodels%2Fbridgetower%2Ftest_image_processing_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e99198e5e7135010dfe09888e80b95a3f077e3e/tests%2Fmodels%2Fbridgetower%2Ftest_image_processing_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbridgetower%2Ftest_image_processing_bridgetower.py?ref=9e99198e5e7135010dfe09888e80b95a3f077e3e",
            "patch": "@@ -14,7 +14,6 @@\n \n \n import unittest\n-from typing import Optional, Union\n \n from transformers.image_utils import load_image\n from transformers.testing_utils import require_torch, require_vision\n@@ -36,14 +35,14 @@ def __init__(\n         self,\n         parent,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n         size_divisor: int = 32,\n         do_rescale: bool = True,\n-        rescale_factor: Union[int, float] = 1 / 255,\n+        rescale_factor: int | float = 1 / 255,\n         do_normalize: bool = True,\n         do_center_crop: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = [0.48145466, 0.4578275, 0.40821073],\n-        image_std: Optional[Union[float, list[float]]] = [0.26862954, 0.26130258, 0.27577711],\n+        image_mean: float | list[float] | None = [0.48145466, 0.4578275, 0.40821073],\n+        image_std: float | list[float] | None = [0.26862954, 0.26130258, 0.27577711],\n         do_pad: bool = True,\n         batch_size=7,\n         min_resolution=30,"
        },
        {
            "sha": "9e773de46d1b91f83be8feedd66d53e89fe4e1d3",
            "filename": "tests/models/gemma3/test_processing_gemma3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e99198e5e7135010dfe09888e80b95a3f077e3e/tests%2Fmodels%2Fgemma3%2Ftest_processing_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e99198e5e7135010dfe09888e80b95a3f077e3e/tests%2Fmodels%2Fgemma3%2Ftest_processing_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3%2Ftest_processing_gemma3.py?ref=9e99198e5e7135010dfe09888e80b95a3f077e3e",
            "patch": "@@ -15,7 +15,6 @@\n import shutil\n import tempfile\n import unittest\n-from typing import Optional\n \n from transformers import Gemma3Processor, GemmaTokenizer\n from transformers.testing_utils import get_tests_dir, require_vision\n@@ -83,7 +82,7 @@ def prepare_processor_dict():\n         }  # fmt: skip\n \n     # Override as Gemma3 needs images to be an explicitly nested batch\n-    def prepare_image_inputs(self, batch_size: Optional[int] = None):\n+    def prepare_image_inputs(self, batch_size: int | None = None):\n         \"\"\"This function prepares a list of PIL images for testing\"\"\"\n         images = super().prepare_image_inputs(batch_size)\n         if isinstance(images, (list, tuple)):"
        },
        {
            "sha": "92359040c9d302b94ac071b0b6cb861d625830ce",
            "filename": "tests/models/gemma3n/test_feature_extraction_gemma3n.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e99198e5e7135010dfe09888e80b95a3f077e3e/tests%2Fmodels%2Fgemma3n%2Ftest_feature_extraction_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e99198e5e7135010dfe09888e80b95a3f077e3e/tests%2Fmodels%2Fgemma3n%2Ftest_feature_extraction_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3n%2Ftest_feature_extraction_gemma3n.py?ref=9e99198e5e7135010dfe09888e80b95a3f077e3e",
            "patch": "@@ -19,7 +19,6 @@\n import tempfile\n import unittest\n from collections.abc import Sequence\n-from typing import Optional\n \n import numpy as np\n from parameterized import parameterized\n@@ -78,8 +77,8 @@ def __init__(\n         dither: float = 0.0,\n         input_scale_factor: float = 1.0,\n         mel_floor: float = 1e-5,\n-        per_bin_mean: Optional[Sequence[float]] = None,\n-        per_bin_stddev: Optional[Sequence[float]] = None,\n+        per_bin_mean: Sequence[float] | None = None,\n+        per_bin_stddev: Sequence[float] | None = None,\n     ):\n         self.parent = parent\n         self.batch_size = batch_size"
        },
        {
            "sha": "9ce0b841992eb2a8fa89d709c495edc5e0d89876",
            "filename": "tests/models/grounding_dino/test_processing_grounding_dino.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e99198e5e7135010dfe09888e80b95a3f077e3e/tests%2Fmodels%2Fgrounding_dino%2Ftest_processing_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e99198e5e7135010dfe09888e80b95a3f077e3e/tests%2Fmodels%2Fgrounding_dino%2Ftest_processing_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgrounding_dino%2Ftest_processing_grounding_dino.py?ref=9e99198e5e7135010dfe09888e80b95a3f077e3e",
            "patch": "@@ -17,7 +17,6 @@\n import shutil\n import tempfile\n import unittest\n-from typing import Optional\n \n import pytest\n \n@@ -79,7 +78,7 @@ def setUpClass(cls):\n         cls.embed_dim = 5\n         cls.seq_length = 5\n \n-    def prepare_text_inputs(self, batch_size: Optional[int] = None, **kwargs):\n+    def prepare_text_inputs(self, batch_size: int | None = None, **kwargs):\n         labels = [\"a cat\", \"remote control\"]\n         labels_longer = [\"a person\", \"a car\", \"a dog\", \"a cat\"]\n "
        },
        {
            "sha": "4e99940c052e107c23a7c6cbc8a00bbf7c564fc0",
            "filename": "tests/models/llava/test_image_processing_llava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e99198e5e7135010dfe09888e80b95a3f077e3e/tests%2Fmodels%2Fllava%2Ftest_image_processing_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e99198e5e7135010dfe09888e80b95a3f077e3e/tests%2Fmodels%2Fllava%2Ftest_image_processing_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava%2Ftest_image_processing_llava.py?ref=9e99198e5e7135010dfe09888e80b95a3f077e3e",
            "patch": "@@ -14,7 +14,6 @@\n \n \n import unittest\n-from typing import Union\n \n import numpy as np\n \n@@ -151,7 +150,7 @@ def test_padding(self):\n \n         # taken from original implementation: https://github.com/haotian-liu/LLaVA/blob/c121f0432da27facab705978f83c4ada465e46fd/llava/mm_utils.py#L152\n         def pad_to_square_original(\n-            image: Image.Image, background_color: Union[int, tuple[int, int, int]] = 0\n+            image: Image.Image, background_color: int | tuple[int, int, int] = 0\n         ) -> Image.Image:\n             width, height = image.size\n             if width == height:"
        },
        {
            "sha": "1d6f89587fef902273b2dbd427b9452f0733c65b",
            "filename": "tests/models/mllama/test_processing_mllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e99198e5e7135010dfe09888e80b95a3f077e3e/tests%2Fmodels%2Fmllama%2Ftest_processing_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e99198e5e7135010dfe09888e80b95a3f077e3e/tests%2Fmodels%2Fmllama%2Ftest_processing_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmllama%2Ftest_processing_mllama.py?ref=9e99198e5e7135010dfe09888e80b95a3f077e3e",
            "patch": "@@ -16,7 +16,6 @@\n import shutil\n import tempfile\n import unittest\n-from typing import Optional\n \n import numpy as np\n \n@@ -60,7 +59,7 @@ def prepare_processor_dict(self):\n         return {\"chat_template\": \"{% for message in messages %}{% if loop.index0 == 0 %}{{ bos_token }}{% endif %}{{ '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' }}{% if message['content'] is string %}{{ message['content'] }}{% else %}{% for content in message['content'] %}{% if content['type'] == 'image' %}{{ '<|image|>' }}{% elif content['type'] == 'text' %}{{ content['text'] }}{% endif %}{% endfor %}{% endif %}{{ '<|eot_id|>' }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\"}  # fmt: skip\n \n     # Override as Mllama needs images to be an explicitly nested batch\n-    def prepare_image_inputs(self, batch_size: Optional[int] = None):\n+    def prepare_image_inputs(self, batch_size: int | None = None):\n         \"\"\"This function prepares a list of PIL images for testing\"\"\"\n         images = super().prepare_image_inputs(batch_size)\n         if isinstance(images, (list, tuple)):"
        },
        {
            "sha": "20d394e8992dd745161051df0cca3b71c842eeb4",
            "filename": "tests/models/patchtsmixer/test_modeling_patchtsmixer.py",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e99198e5e7135010dfe09888e80b95a3f077e3e/tests%2Fmodels%2Fpatchtsmixer%2Ftest_modeling_patchtsmixer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e99198e5e7135010dfe09888e80b95a3f077e3e/tests%2Fmodels%2Fpatchtsmixer%2Ftest_modeling_patchtsmixer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpatchtsmixer%2Ftest_modeling_patchtsmixer.py?ref=9e99198e5e7135010dfe09888e80b95a3f077e3e",
            "patch": "@@ -18,7 +18,6 @@\n import random\n import tempfile\n import unittest\n-from typing import Optional, Union\n \n import numpy as np\n from huggingface_hub import hf_hub_download\n@@ -87,17 +86,17 @@ def __init__(\n         masked_loss: bool = False,\n         mask_mode: str = \"mask_before_encoder\",\n         channel_consistent_masking: bool = True,\n-        scaling: Optional[Union[str, bool]] = \"std\",\n+        scaling: str | bool | None = \"std\",\n         # Head related\n         head_dropout: float = 0.2,\n         # forecast related\n         prediction_length: int = 16,\n-        out_channels: Optional[int] = None,\n+        out_channels: int | None = None,\n         # Classification/regression related\n         # num_labels: int = 3,\n         num_targets: int = 3,\n-        output_range: Optional[list] = None,\n-        head_aggregation: Optional[str] = None,\n+        output_range: list | None = None,\n+        head_aggregation: str | None = None,\n         # Trainer related\n         batch_size=13,\n         is_training=True,"
        },
        {
            "sha": "e1563ddcad9e8b44491deab76b3628cb60390372",
            "filename": "tests/models/smolvlm/test_processing_smolvlm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e99198e5e7135010dfe09888e80b95a3f077e3e/tests%2Fmodels%2Fsmolvlm%2Ftest_processing_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e99198e5e7135010dfe09888e80b95a3f077e3e/tests%2Fmodels%2Fsmolvlm%2Ftest_processing_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsmolvlm%2Ftest_processing_smolvlm.py?ref=9e99198e5e7135010dfe09888e80b95a3f077e3e",
            "patch": "@@ -15,7 +15,6 @@\n import shutil\n import tempfile\n import unittest\n-from typing import Optional\n \n import numpy as np\n \n@@ -94,14 +93,14 @@ def prepare_processor_dict():\n         }\n \n     # Override as SmolVLM needs images/video to be an explicitly nested batch\n-    def prepare_image_inputs(self, batch_size: Optional[int] = None):\n+    def prepare_image_inputs(self, batch_size: int | None = None):\n         \"\"\"This function prepares a list of PIL images for testing\"\"\"\n         images = super().prepare_image_inputs(batch_size)\n         if isinstance(images, (list, tuple)):\n             images = [[image] for image in images]\n         return images\n \n-    def prepare_video_inputs(self, batch_size: Optional[int] = None):\n+    def prepare_video_inputs(self, batch_size: int | None = None):\n         \"\"\"This function prepares a list of numpy videos.\"\"\"\n         video_input = [np.random.randint(255, size=(3, 30, 400), dtype=np.uint8)] * 8\n         if batch_size is None:"
        },
        {
            "sha": "aa4d46abaab3bd082f2f503baeeec6fb423d2a1c",
            "filename": "tests/models/tvp/test_image_processing_tvp.py",
            "status": "modified",
            "additions": 6,
            "deletions": 7,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e99198e5e7135010dfe09888e80b95a3f077e3e/tests%2Fmodels%2Ftvp%2Ftest_image_processing_tvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e99198e5e7135010dfe09888e80b95a3f077e3e/tests%2Fmodels%2Ftvp%2Ftest_image_processing_tvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftvp%2Ftest_image_processing_tvp.py?ref=9e99198e5e7135010dfe09888e80b95a3f077e3e",
            "patch": "@@ -14,7 +14,6 @@\n \n \n import unittest\n-from typing import Optional, Union\n \n import numpy as np\n \n@@ -41,16 +40,16 @@ def __init__(\n         do_resize: bool = True,\n         size: dict[str, int] = {\"longest_edge\": 40},\n         do_center_crop: bool = False,\n-        crop_size: Optional[dict[str, int]] = None,\n+        crop_size: dict[str, int] | None = None,\n         do_rescale: bool = False,\n-        rescale_factor: Union[int, float] = 1 / 255,\n+        rescale_factor: int | float = 1 / 255,\n         do_pad: bool = True,\n         pad_size: dict[str, int] = {\"height\": 80, \"width\": 80},\n-        fill: Optional[int] = None,\n-        pad_mode: Optional[PaddingMode] = None,\n+        fill: int | None = None,\n+        pad_mode: PaddingMode | None = None,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = [0.48145466, 0.4578275, 0.40821073],\n-        image_std: Optional[Union[float, list[float]]] = [0.26862954, 0.26130258, 0.27577711],\n+        image_mean: float | list[float] | None = [0.48145466, 0.4578275, 0.40821073],\n+        image_std: float | list[float] | None = [0.26862954, 0.26130258, 0.27577711],\n         batch_size=2,\n         min_resolution=40,\n         max_resolution=80,"
        },
        {
            "sha": "74c0b8cd65b79eff1ba62bc07f08274f3216b1d9",
            "filename": "tests/models/video_llama_3/test_processing_video_llama_3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e99198e5e7135010dfe09888e80b95a3f077e3e/tests%2Fmodels%2Fvideo_llama_3%2Ftest_processing_video_llama_3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e99198e5e7135010dfe09888e80b95a3f077e3e/tests%2Fmodels%2Fvideo_llama_3%2Ftest_processing_video_llama_3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvideo_llama_3%2Ftest_processing_video_llama_3.py?ref=9e99198e5e7135010dfe09888e80b95a3f077e3e",
            "patch": "@@ -17,7 +17,6 @@\n import shutil\n import tempfile\n import unittest\n-from typing import Optional\n \n import numpy as np\n import pytest\n@@ -79,7 +78,7 @@ def tearDownClass(cls):\n         shutil.rmtree(cls.tmpdirname, ignore_errors=True)\n \n     @require_vision\n-    def prepare_image_inputs(self, batch_size: Optional[int] = None):\n+    def prepare_image_inputs(self, batch_size: int | None = None):\n         \"\"\"This function prepares a list of PIL images for testing\"\"\"\n         if batch_size is None:\n             return prepare_image_inputs()[0]"
        },
        {
            "sha": "1b0ec6fa2d3f58767bf1792eb315290e086ed93f",
            "filename": "tests/sagemaker/scripts/pytorch/run_glue_model_parallelism.py",
            "status": "modified",
            "additions": 10,
            "deletions": 11,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e99198e5e7135010dfe09888e80b95a3f077e3e/tests%2Fsagemaker%2Fscripts%2Fpytorch%2Frun_glue_model_parallelism.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e99198e5e7135010dfe09888e80b95a3f077e3e/tests%2Fsagemaker%2Fscripts%2Fpytorch%2Frun_glue_model_parallelism.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fsagemaker%2Fscripts%2Fpytorch%2Frun_glue_model_parallelism.py?ref=9e99198e5e7135010dfe09888e80b95a3f077e3e",
            "patch": "@@ -20,7 +20,6 @@\n import random\n import sys\n from dataclasses import dataclass, field\n-from typing import Optional\n \n import numpy as np\n from datasets import load_dataset, load_metric\n@@ -70,7 +69,7 @@ class DataTrainingArguments:\n     the command line.\n     \"\"\"\n \n-    task_name: Optional[str] = field(\n+    task_name: str | None = field(\n         default=None,\n         metadata={\"help\": \"The name of the task to train on: \" + \", \".join(task_to_keys.keys())},\n     )\n@@ -95,7 +94,7 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    max_train_samples: Optional[int] = field(\n+    max_train_samples: int | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -104,7 +103,7 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    max_val_samples: Optional[int] = field(\n+    max_val_samples: int | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -113,7 +112,7 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    max_test_samples: Optional[int] = field(\n+    max_test_samples: int | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -122,13 +121,13 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    train_file: Optional[str] = field(\n+    train_file: str | None = field(\n         default=None, metadata={\"help\": \"A csv or a json file containing the training data.\"}\n     )\n-    validation_file: Optional[str] = field(\n+    validation_file: str | None = field(\n         default=None, metadata={\"help\": \"A csv or a json file containing the validation data.\"}\n     )\n-    test_file: Optional[str] = field(default=None, metadata={\"help\": \"A csv or a json file containing the test data.\"})\n+    test_file: str | None = field(default=None, metadata={\"help\": \"A csv or a json file containing the test data.\"})\n \n     def __post_init__(self):\n         if self.task_name is not None:\n@@ -155,13 +154,13 @@ class ModelArguments:\n     model_name_or_path: str = field(\n         metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n     )\n-    config_name: Optional[str] = field(\n+    config_name: str | None = field(\n         default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n     )\n-    tokenizer_name: Optional[str] = field(\n+    tokenizer_name: str | None = field(\n         default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n     )\n-    cache_dir: Optional[str] = field(\n+    cache_dir: str | None = field(\n         default=None,\n         metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n     )"
        },
        {
            "sha": "4cb6a712342ffeb8cefb3078598caf5534a35dd7",
            "filename": "tests/test_processing_common.py",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e99198e5e7135010dfe09888e80b95a3f077e3e/tests%2Ftest_processing_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e99198e5e7135010dfe09888e80b95a3f077e3e/tests%2Ftest_processing_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_processing_common.py?ref=9e99198e5e7135010dfe09888e80b95a3f077e3e",
            "patch": "@@ -19,7 +19,6 @@\n import sys\n import tempfile\n from pathlib import Path\n-from typing import Optional, Union\n \n import numpy as np\n from huggingface_hub import hf_hub_download\n@@ -136,7 +135,7 @@ def get_processor(self):\n         processor = self.processor_class(**components, **self.prepare_processor_dict())\n         return processor\n \n-    def prepare_text_inputs(self, batch_size: Optional[int] = None, modalities: Optional[Union[str, list]] = None):\n+    def prepare_text_inputs(self, batch_size: int | None = None, modalities: str | list | None = None):\n         if isinstance(modalities, str):\n             modalities = [modalities]\n \n@@ -158,7 +157,7 @@ def prepare_text_inputs(self, batch_size: Optional[int] = None, modalities: Opti\n         ] * (batch_size - 2)\n \n     @require_vision\n-    def prepare_image_inputs(self, batch_size: Optional[int] = None):\n+    def prepare_image_inputs(self, batch_size: int | None = None):\n         \"\"\"This function prepares a list of PIL images for testing\"\"\"\n         if batch_size is None:\n             return prepare_image_inputs()[0]\n@@ -167,15 +166,15 @@ def prepare_image_inputs(self, batch_size: Optional[int] = None):\n         return prepare_image_inputs() * batch_size\n \n     @require_vision\n-    def prepare_video_inputs(self, batch_size: Optional[int] = None):\n+    def prepare_video_inputs(self, batch_size: int | None = None):\n         \"\"\"This function prepares a list of numpy videos.\"\"\"\n         video_input = [np.random.randint(255, size=(3, 30, 400), dtype=np.uint8)] * 8\n         video_input = np.array(video_input)\n         if batch_size is None:\n             return video_input\n         return [video_input] * batch_size\n \n-    def prepare_audio_inputs(self, batch_size: Optional[int] = None):\n+    def prepare_audio_inputs(self, batch_size: int | None = None):\n         \"\"\"This function prepares a list of numpy audio.\"\"\"\n         raw_speech = floats_list((1, 1000))\n         raw_speech = [np.asarray(audio) for audio in raw_speech]"
        },
        {
            "sha": "2a8fe003efa69b1914ee7e8d35a906cfe342ba70",
            "filename": "tests/test_tokenization_common.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e99198e5e7135010dfe09888e80b95a3f077e3e/tests%2Ftest_tokenization_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e99198e5e7135010dfe09888e80b95a3f077e3e/tests%2Ftest_tokenization_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_tokenization_common.py?ref=9e99198e5e7135010dfe09888e80b95a3f077e3e",
            "patch": "@@ -26,7 +26,7 @@\n from collections import OrderedDict\n from itertools import takewhile\n from pathlib import Path\n-from typing import TYPE_CHECKING, Any, Optional, Union\n+from typing import TYPE_CHECKING, Any, Union\n \n from parameterized import parameterized\n \n@@ -169,7 +169,7 @@ def _test_subword_regularization_tokenizer(in_queue, out_queue, timeout):\n \n def check_subword_sampling(\n     tokenizer: PreTrainedTokenizer,\n-    text: Optional[str] = None,\n+    text: str | None = None,\n     test_sentencepiece_ignore_case: bool = True,\n ) -> None:\n     \"\"\"\n@@ -313,9 +313,9 @@ def tokenizer_integration_test_util(\n         self,\n         expected_encoding: dict,\n         model_name: str,\n-        revision: Optional[str] = None,\n-        sequences: Optional[list[str]] = None,\n-        decode_kwargs: Optional[dict[str, Any]] = None,\n+        revision: str | None = None,\n+        sequences: list[str] | None = None,\n+        decode_kwargs: dict[str, Any] | None = None,\n         padding: bool = True,\n     ):\n         \"\"\""
        },
        {
            "sha": "b58bac948f53a01ba3f796d026d083ce8b976e1e",
            "filename": "tests/utils/test_doc_samples.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e99198e5e7135010dfe09888e80b95a3f077e3e/tests%2Futils%2Ftest_doc_samples.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e99198e5e7135010dfe09888e80b95a3f077e3e/tests%2Futils%2Ftest_doc_samples.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_doc_samples.py?ref=9e99198e5e7135010dfe09888e80b95a3f077e3e",
            "patch": "@@ -16,7 +16,6 @@\n import os\n import unittest\n from pathlib import Path\n-from typing import Union\n \n import transformers\n from transformers.testing_utils import require_torch, slow\n@@ -32,9 +31,9 @@ class TestCodeExamples(unittest.TestCase):\n     def analyze_directory(\n         self,\n         directory: Path,\n-        identifier: Union[str, None] = None,\n-        ignore_files: Union[list[str], None] = None,\n-        n_identifier: Union[str, list[str], None] = None,\n+        identifier: str | None = None,\n+        ignore_files: list[str] | None = None,\n+        n_identifier: str | list[str] | None = None,\n         only_modules: bool = True,\n     ):\n         \"\"\""
        },
        {
            "sha": "873b87299449b434d200c4c8cb9664c033e88e76",
            "filename": "tests/utils/test_hf_argparser.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e99198e5e7135010dfe09888e80b95a3f077e3e/tests%2Futils%2Ftest_hf_argparser.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e99198e5e7135010dfe09888e80b95a3f077e3e/tests%2Futils%2Ftest_hf_argparser.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_hf_argparser.py?ref=9e99198e5e7135010dfe09888e80b95a3f077e3e",
            "patch": "@@ -22,7 +22,7 @@\n from dataclasses import dataclass, field\n from enum import Enum\n from pathlib import Path\n-from typing import Literal, Optional, Union, get_args, get_origin\n+from typing import Literal, Union, get_args, get_origin\n from unittest.mock import patch\n \n import yaml\n@@ -59,7 +59,7 @@ class WithDefaultExample:\n class WithDefaultBoolExample:\n     foo: bool = False\n     baz: bool = True\n-    opt: Optional[bool] = None\n+    opt: bool | None = None\n \n \n class BasicEnum(Enum):\n@@ -91,11 +91,11 @@ def __post_init__(self):\n \n @dataclass\n class OptionalExample:\n-    foo: Optional[int] = None\n-    bar: Optional[float] = field(default=None, metadata={\"help\": \"help message\"})\n-    baz: Optional[str] = None\n-    ces: Optional[list[str]] = list_field(default=[])\n-    des: Optional[list[int]] = list_field(default=[])\n+    foo: int | None = None\n+    bar: float | None = field(default=None, metadata={\"help\": \"help message\"})\n+    baz: str | None = None\n+    ces: list[str] | None = list_field(default=[])\n+    des: list[int] | None = list_field(default=[])\n \n \n @dataclass\n@@ -120,7 +120,7 @@ def __post_init__(self):\n class StringLiteralAnnotationExample:\n     foo: int\n     required_enum: \"BasicEnum\" = field()\n-    opt: \"Optional[bool]\" = None\n+    opt: \"bool | None\" = None\n     baz: \"str\" = field(default=\"toto\", metadata={\"help\": \"help message\"})\n     foo_str: \"list[str]\" = list_field(default=[\"Hallo\", \"Bonjour\", \"Hello\"])\n "
        },
        {
            "sha": "124bc73d7357d837781ff51263fd2986ba09236e",
            "filename": "tests/utils/test_image_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e99198e5e7135010dfe09888e80b95a3f077e3e/tests%2Futils%2Ftest_image_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e99198e5e7135010dfe09888e80b95a3f077e3e/tests%2Futils%2Ftest_image_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_image_utils.py?ref=9e99198e5e7135010dfe09888e80b95a3f077e3e",
            "patch": "@@ -17,7 +17,6 @@\n import tempfile\n import unittest\n from io import BytesIO\n-from typing import Optional\n \n import httpx\n import numpy as np\n@@ -46,7 +45,7 @@\n     from transformers.image_utils import get_image_size, infer_channel_dimension_format, load_image\n \n \n-def get_image_from_hub_dataset(dataset_id: str, filename: str, revision: Optional[str] = None) -> \"PIL.Image.Image\":\n+def get_image_from_hub_dataset(dataset_id: str, filename: str, revision: str | None = None) -> \"PIL.Image.Image\":\n     url = hf_hub_url(dataset_id, filename, repo_type=\"dataset\", revision=revision)\n     return PIL.Image.open(BytesIO(httpx.get(url, follow_redirects=True).content))\n "
        },
        {
            "sha": "f259888f9251561c5e6b046fa99e075d819a8f07",
            "filename": "tests/utils/test_model_output.py",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e99198e5e7135010dfe09888e80b95a3f077e3e/tests%2Futils%2Ftest_model_output.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e99198e5e7135010dfe09888e80b95a3f077e3e/tests%2Futils%2Ftest_model_output.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_model_output.py?ref=9e99198e5e7135010dfe09888e80b95a3f077e3e",
            "patch": "@@ -15,7 +15,6 @@\n import io\n import unittest\n from dataclasses import dataclass\n-from typing import Optional\n \n import pytest\n \n@@ -31,8 +30,8 @@\n @dataclass\n class ModelOutputTest(ModelOutput):\n     a: float\n-    b: Optional[float] = None\n-    c: Optional[float] = None\n+    b: float | None = None\n+    c: float | None = None\n \n \n class ModelOutputTester(unittest.TestCase):\n@@ -182,8 +181,8 @@ class ModelOutputTestNoDataclass(ModelOutput):\n     \"\"\"Invalid test subclass of ModelOutput where @dataclass decorator is not used\"\"\"\n \n     a: float\n-    b: Optional[float] = None\n-    c: Optional[float] = None\n+    b: float | None = None\n+    c: float | None = None\n \n \n class ModelOutputSubclassTester(unittest.TestCase):"
        },
        {
            "sha": "6719beae4b63f9f460f0339df3f9c6e4b867eac2",
            "filename": "utils/add_dates.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e99198e5e7135010dfe09888e80b95a3f077e3e/utils%2Fadd_dates.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e99198e5e7135010dfe09888e80b95a3f077e3e/utils%2Fadd_dates.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fadd_dates.py?ref=9e99198e5e7135010dfe09888e80b95a3f077e3e",
            "patch": "@@ -3,7 +3,6 @@\n import re\n import subprocess\n from datetime import date\n-from typing import Optional\n \n from huggingface_hub import paper_info\n \n@@ -51,7 +50,7 @@ def get_modified_cards() -> list[str]:\n     return model_names\n \n \n-def get_paper_link(model_card: Optional[str], path: Optional[str]) -> str:\n+def get_paper_link(model_card: str | None, path: str | None) -> str:\n     \"\"\"Get the first paper link from the model card content.\"\"\"\n \n     if model_card is not None and not model_card.endswith(\".md\"):\n@@ -91,7 +90,7 @@ def get_paper_link(model_card: Optional[str], path: Optional[str]) -> str:\n     return paper_ids[0]\n \n \n-def get_first_commit_date(model_name: Optional[str]) -> str:\n+def get_first_commit_date(model_name: str | None) -> str:\n     \"\"\"Get the first commit date of the model's init file or model.md. This date is considered as the date the model was added to HF transformers\"\"\"\n \n     if model_name.endswith(\".md\"):"
        },
        {
            "sha": "d1deab2fbd2b8e390e2798ca1dca48af61334c74",
            "filename": "utils/check_copies.py",
            "status": "modified",
            "additions": 7,
            "deletions": 8,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e99198e5e7135010dfe09888e80b95a3f077e3e/utils%2Fcheck_copies.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e99198e5e7135010dfe09888e80b95a3f077e3e/utils%2Fcheck_copies.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_copies.py?ref=9e99198e5e7135010dfe09888e80b95a3f077e3e",
            "patch": "@@ -42,7 +42,6 @@\n import re\n import subprocess\n from collections import OrderedDict\n-from typing import Optional, Union\n \n from transformers.utils import direct_transformers_import\n \n@@ -384,8 +383,8 @@ def split_code_into_blocks(\n \n \n def find_code_in_transformers(\n-    object_name: str, base_path: Optional[str] = None, return_indices: bool = False\n-) -> Union[str, tuple[list[str], int, int]]:\n+    object_name: str, base_path: str | None = None, return_indices: bool = False\n+) -> str | tuple[list[str], int, int]:\n     \"\"\"\n     Find and return the source code of an object.\n \n@@ -485,7 +484,7 @@ def replace_code(code: str, replace_pattern: str) -> str:\n     return code\n \n \n-def find_code_and_splits(object_name: str, base_path: str, buffer: Optional[dict] = None):\n+def find_code_and_splits(object_name: str, base_path: str, buffer: dict | None = None):\n     \"\"\"Find the code of an object (specified by `object_name`) and split it into blocks.\n \n     Args:\n@@ -581,7 +580,7 @@ def stylify(code: str) -> str:\n     return formatted_code[len(\"class Bla:\\n\") :] if has_indent else formatted_code\n \n \n-def check_codes_match(observed_code: str, theoretical_code: str) -> Optional[int]:\n+def check_codes_match(observed_code: str, theoretical_code: str) -> int | None:\n     \"\"\"\n     Checks if two version of a code match with the exception of the class/function name.\n \n@@ -633,8 +632,8 @@ def check_codes_match(observed_code: str, theoretical_code: str) -> Optional[int\n \n \n def is_copy_consistent(\n-    filename: str, overwrite: bool = False, buffer: Optional[dict] = None\n-) -> Optional[list[tuple[str, int]]]:\n+    filename: str, overwrite: bool = False, buffer: dict | None = None\n+) -> list[tuple[str, int]] | None:\n     \"\"\"\n     Check if the code commented as a copy in a file matches the original.\n \n@@ -826,7 +825,7 @@ def is_copy_consistent(\n     return diffs\n \n \n-def check_copies(overwrite: bool = False, file: Optional[str] = None):\n+def check_copies(overwrite: bool = False, file: str | None = None):\n     \"\"\"\n     Check every file is copy-consistent with the original. Also check the model list in the main README and other\n     READMEs are consistent."
        },
        {
            "sha": "0e7ec68a02e5b8c39c59f60958509d1cb98441be",
            "filename": "utils/check_docstrings.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e99198e5e7135010dfe09888e80b95a3f077e3e/utils%2Fcheck_docstrings.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e99198e5e7135010dfe09888e80b95a3f077e3e/utils%2Fcheck_docstrings.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_docstrings.py?ref=9e99198e5e7135010dfe09888e80b95a3f077e3e",
            "patch": "@@ -43,7 +43,7 @@\n import re\n from collections import OrderedDict\n from pathlib import Path\n-from typing import Any, Optional, Union\n+from typing import Any\n \n from check_repo import ignore_undocumented\n from git import Repo\n@@ -525,7 +525,7 @@ def stringify_default(default: Any) -> str:\n         return f\"`{default}`\"\n \n \n-def eval_math_expression(expression: str) -> Optional[Union[float, int]]:\n+def eval_math_expression(expression: str) -> float | int | None:\n     # Mainly taken from the excellent https://stackoverflow.com/a/9558001\n     \"\"\"\n     Evaluate (safely) a mathematial expression and returns its value.\n@@ -673,7 +673,7 @@ def find_source_file(obj: Any) -> Path:\n     return obj_file.with_suffix(\".py\")\n \n \n-def match_docstring_with_signature(obj: Any) -> Optional[tuple[str, str]]:\n+def match_docstring_with_signature(obj: Any) -> tuple[str, str] | None:\n     \"\"\"\n     Matches the docstring of an object with its signature.\n "
        },
        {
            "sha": "0ee4ccea5dc8b20085ec7e333172fec2348d90e4",
            "filename": "utils/check_dummies.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e99198e5e7135010dfe09888e80b95a3f077e3e/utils%2Fcheck_dummies.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e99198e5e7135010dfe09888e80b95a3f077e3e/utils%2Fcheck_dummies.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_dummies.py?ref=9e99198e5e7135010dfe09888e80b95a3f077e3e",
            "patch": "@@ -37,7 +37,6 @@\n import argparse\n import os\n import re\n-from typing import Optional\n \n \n # All paths are set with the intent you should run this script from the root of the repo with the command\n@@ -73,7 +72,7 @@ def {0}(*args, **kwargs):\n \"\"\"\n \n \n-def find_backend(line: str) -> Optional[str]:\n+def find_backend(line: str) -> str | None:\n     \"\"\"\n     Find one (or multiple) backend in a code line of the init.\n \n@@ -156,7 +155,7 @@ def create_dummy_object(name: str, backend_name: str) -> str:\n         return DUMMY_CLASS.format(name, backend_name)\n \n \n-def create_dummy_files(backend_specific_objects: Optional[dict[str, list[str]]] = None) -> dict[str, str]:\n+def create_dummy_files(backend_specific_objects: dict[str, list[str]] | None = None) -> dict[str, str]:\n     \"\"\"\n     Create the content of the dummy files.\n "
        },
        {
            "sha": "06f560ffcb2d18f767764ba38c617e81542388cd",
            "filename": "utils/check_inits.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e99198e5e7135010dfe09888e80b95a3f077e3e/utils%2Fcheck_inits.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e99198e5e7135010dfe09888e80b95a3f077e3e/utils%2Fcheck_inits.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_inits.py?ref=9e99198e5e7135010dfe09888e80b95a3f077e3e",
            "patch": "@@ -39,7 +39,6 @@\n import os\n import re\n from pathlib import Path\n-from typing import Optional\n \n \n # Path is set with the intent you should run this script from the root of the repo.\n@@ -70,7 +69,7 @@\n _re_else = re.compile(r\"^\\s*else:\")\n \n \n-def find_backend(line: str) -> Optional[str]:\n+def find_backend(line: str) -> str | None:\n     \"\"\"\n     Find one (or multiple) backend in a code line of the init.\n \n@@ -89,7 +88,7 @@ def find_backend(line: str) -> Optional[str]:\n     return \"_and_\".join(backends)\n \n \n-def parse_init(init_file) -> Optional[tuple[dict[str, list[str]], dict[str, list[str]]]]:\n+def parse_init(init_file) -> tuple[dict[str, list[str]], dict[str, list[str]]] | None:\n     \"\"\"\n     Read an init_file and parse (per backend) the `_import_structure` objects defined and the `TYPE_CHECKING` objects\n     defined."
        },
        {
            "sha": "560ef8cbab82794fef5d0bcafcdef8df06b09a12",
            "filename": "utils/custom_init_isort.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e99198e5e7135010dfe09888e80b95a3f077e3e/utils%2Fcustom_init_isort.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e99198e5e7135010dfe09888e80b95a3f077e3e/utils%2Fcustom_init_isort.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcustom_init_isort.py?ref=9e99198e5e7135010dfe09888e80b95a3f077e3e",
            "patch": "@@ -39,7 +39,7 @@\n import os\n import re\n from collections.abc import Callable\n-from typing import Any, Optional\n+from typing import Any\n \n \n # Path is defined with the intent you should run this script from the root of the repo.\n@@ -64,7 +64,7 @@ def get_indent(line: str) -> str:\n \n \n def split_code_in_indented_blocks(\n-    code: str, indent_level: str = \"\", start_prompt: Optional[str] = None, end_prompt: Optional[str] = None\n+    code: str, indent_level: str = \"\", start_prompt: str | None = None, end_prompt: str | None = None\n ) -> list[str]:\n     \"\"\"\n     Split some code into its indented blocks, starting at a given level.\n@@ -141,7 +141,7 @@ def _inner(x):\n     return _inner\n \n \n-def sort_objects(objects: list[Any], key: Optional[Callable[[Any], str]] = None) -> list[Any]:\n+def sort_objects(objects: list[Any], key: Callable[[Any], str] | None = None) -> list[Any]:\n     \"\"\"\n     Sort a list of objects following the rules of isort (all uppercased first, camel-cased second and lower-cased\n     last)."
        },
        {
            "sha": "13a7641e6608559ee36531e4c63ba776b978a87b",
            "filename": "utils/deprecate_models.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e99198e5e7135010dfe09888e80b95a3f077e3e/utils%2Fdeprecate_models.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e99198e5e7135010dfe09888e80b95a3f077e3e/utils%2Fdeprecate_models.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fdeprecate_models.py?ref=9e99198e5e7135010dfe09888e80b95a3f077e3e",
            "patch": "@@ -9,7 +9,6 @@\n import os\n from collections import defaultdict\n from pathlib import Path\n-from typing import Optional\n \n import requests\n from custom_init_isort import sort_imports_in_all_inits\n@@ -77,7 +76,7 @@ def insert_tip_to_model_doc(model_doc_path, tip_message):\n         f.write(\"\\n\".join(new_model_lines))\n \n \n-def get_model_doc_path(model: str) -> tuple[Optional[str], Optional[str]]:\n+def get_model_doc_path(model: str) -> tuple[str | None, str | None]:\n     # Possible variants of the model name in the model doc path\n     model_names = [model, model.replace(\"_\", \"-\"), model.replace(\"_\", \"\")]\n "
        },
        {
            "sha": "88f3d5f869a8babe70c46a0360480ca9884132c0",
            "filename": "utils/get_test_reports.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e99198e5e7135010dfe09888e80b95a3f077e3e/utils%2Fget_test_reports.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e99198e5e7135010dfe09888e80b95a3f077e3e/utils%2Fget_test_reports.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fget_test_reports.py?ref=9e99198e5e7135010dfe09888e80b95a3f077e3e",
            "patch": "@@ -33,7 +33,6 @@\n import subprocess\n import tempfile\n from pathlib import Path\n-from typing import Optional\n \n import torch\n \n@@ -85,8 +84,8 @@ def handle_suite(\n     machine_type: str,\n     dry_run: bool,\n     tmp_cache: str = \"\",\n-    resume_at: Optional[str] = None,\n-    only_in: Optional[list[str]] = None,\n+    resume_at: str | None = None,\n+    only_in: list[str] | None = None,\n     cpu_tests: bool = False,\n     process_id: int = 1,\n     total_processes: int = 1,"
        },
        {
            "sha": "e88c522557e08ccb660368723f6a8234b692cb77",
            "filename": "utils/modular_integrations.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e99198e5e7135010dfe09888e80b95a3f077e3e/utils%2Fmodular_integrations.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e99198e5e7135010dfe09888e80b95a3f077e3e/utils%2Fmodular_integrations.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fmodular_integrations.py?ref=9e99198e5e7135010dfe09888e80b95a3f077e3e",
            "patch": "@@ -1,5 +1,4 @@\n import os\n-from typing import Optional\n \n import libcst as cst\n \n@@ -14,7 +13,7 @@\n def convert_relative_import_to_absolute(\n     import_node: cst.ImportFrom,\n     file_path: str,\n-    package_name: Optional[str] = \"transformers\",\n+    package_name: str | None = \"transformers\",\n ) -> cst.ImportFrom:\n     \"\"\"\n     Convert a relative libcst.ImportFrom node into an absolute one,\n@@ -51,7 +50,7 @@ def convert_relative_import_to_absolute(\n     base_parts = module_parts[:-rel_level]\n \n     # Flatten the module being imported (if any)\n-    def flatten_module(module: Optional[cst.BaseExpression]) -> list[str]:\n+    def flatten_module(module: cst.BaseExpression | None) -> list[str]:\n         if not module:\n             return []\n         if isinstance(module, cst.Name):\n@@ -76,7 +75,7 @@ def flatten_module(module: Optional[cst.BaseExpression]) -> list[str]:\n         full_parts = [file_parts[pkg_index - 1]] + full_parts\n \n     # Build the dotted module path\n-    dotted_module: Optional[cst.BaseExpression] = None\n+    dotted_module: cst.BaseExpression | None = None\n     for part in full_parts:\n         name = cst.Name(part)\n         dotted_module = name if dotted_module is None else cst.Attribute(value=dotted_module, attr=name)"
        },
        {
            "sha": "3216495133b05f714334c1daf3c47669dbd8998f",
            "filename": "utils/modular_model_converter.py",
            "status": "modified",
            "additions": 11,
            "deletions": 12,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e99198e5e7135010dfe09888e80b95a3f077e3e/utils%2Fmodular_model_converter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e99198e5e7135010dfe09888e80b95a3f077e3e/utils%2Fmodular_model_converter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fmodular_model_converter.py?ref=9e99198e5e7135010dfe09888e80b95a3f077e3e",
            "patch": "@@ -22,7 +22,6 @@\n from abc import ABC, abstractmethod\n from collections import Counter, defaultdict, deque\n from functools import partial\n-from typing import Optional, Union\n \n import libcst as cst\n from create_dependency_mapping import find_priority_list\n@@ -177,7 +176,7 @@ def leave_ImportFrom(self, original_node, updated_node):\n )\n \n \n-def get_full_attribute_name(node: Union[cst.Attribute, cst.Name]) -> Optional[str]:\n+def get_full_attribute_name(node: cst.Attribute | cst.Name) -> str | None:\n     \"\"\"Get the full name of an Attribute or Name node (e.g. `\"nn.Module\"` for an Attribute representing it). If the\n     successive value of an Attribute are not Name nodes, return `None`.\"\"\"\n     if m.matches(node, m.Name()):\n@@ -378,11 +377,11 @@ def leave_FunctionDef(self, original_node: cst.FunctionDef, updated_node: cst.Fu\n \n def find_all_dependencies(\n     dependency_mapping: dict[str, set],\n-    start_entity: Optional[str] = None,\n-    initial_dependencies: Optional[set] = None,\n-    initial_checked_dependencies: Optional[set] = None,\n+    start_entity: str | None = None,\n+    initial_dependencies: set | None = None,\n+    initial_checked_dependencies: set | None = None,\n     return_parent: bool = False,\n-) -> Union[list, set]:\n+) -> list | set:\n     \"\"\"Return all the dependencies of the given `start_entity` or `initial_dependencies`. This is basically some kind of\n     BFS traversal algorithm. It can either start from `start_entity`, or `initial_dependencies`.\n \n@@ -476,7 +475,7 @@ class ClassDependencyMapper(CSTVisitor):\n     \"\"\"\n \n     def __init__(\n-        self, class_name: str, global_names: set[str], objects_imported_from_modeling: Optional[set[str]] = None\n+        self, class_name: str, global_names: set[str], objects_imported_from_modeling: set[str] | None = None\n     ):\n         super().__init__()\n         self.class_name = class_name\n@@ -504,7 +503,7 @@ def dependencies_for_class_node(node: cst.ClassDef, global_names: set[str]) -> s\n \n \n def augmented_dependencies_for_class_node(\n-    node: cst.ClassDef, mapper: \"ModuleMapper\", objects_imported_from_modeling: Optional[set[str]] = None\n+    node: cst.ClassDef, mapper: \"ModuleMapper\", objects_imported_from_modeling: set[str] | None = None\n ) -> set:\n     \"\"\"Create augmented dependencies for a class node based on a `mapper`.\n     Augmented dependencies means immediate dependencies + recursive function and assignments dependencies.\n@@ -1659,8 +1658,8 @@ class node based on the inherited classes if needed. Also returns any new import\n \n def create_modules(\n     modular_mapper: ModularFileMapper,\n-    file_path: Optional[str] = None,\n-    package_name: Optional[str] = \"transformers\",\n+    file_path: str | None = None,\n+    package_name: str | None = \"transformers\",\n ) -> dict[str, cst.Module]:\n     \"\"\"Create all the new modules based on visiting the modular file. It replaces all classes as necessary.\"\"\"\n     files = defaultdict(dict)\n@@ -1747,7 +1746,7 @@ def run_ruff(code, check=False):\n     return stdout.decode()\n \n \n-def convert_modular_file(modular_file: str, source_library: Optional[str] = \"transformers\") -> dict[str, str]:\n+def convert_modular_file(modular_file: str, source_library: str | None = \"transformers\") -> dict[str, str]:\n     \"\"\"Convert a `modular_file` into all the different model-specific files it depicts.\"\"\"\n     pattern = re.search(r\"modular_(.*)(?=\\.py$)\", modular_file)\n     output = {}\n@@ -1818,7 +1817,7 @@ def count_loc(file_path: str) -> int:\n     return len([line for line in comment_less_code.split(\"\\n\") if line.strip()])\n \n \n-def run_converter(modular_file: str, source_library: Optional[str] = \"transformers\"):\n+def run_converter(modular_file: str, source_library: str | None = \"transformers\"):\n     \"\"\"Convert a modular file, and save resulting files.\"\"\"\n     print(f\"Converting {modular_file} to a single model single file format\")\n     converted_files = convert_modular_file(modular_file, source_library=source_library)"
        },
        {
            "sha": "258ccc2772dbf39789a53affb62814f996c2df20",
            "filename": "utils/notification_service.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e99198e5e7135010dfe09888e80b95a3f077e3e/utils%2Fnotification_service.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e99198e5e7135010dfe09888e80b95a3f077e3e/utils%2Fnotification_service.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fnotification_service.py?ref=9e99198e5e7135010dfe09888e80b95a3f077e3e",
            "patch": "@@ -21,7 +21,7 @@\n import re\n import sys\n import time\n-from typing import Any, Optional, Union\n+from typing import Any\n \n import requests\n from compare_test_runs import compare_job_sets\n@@ -120,7 +120,7 @@ def handle_stacktraces(test_results):\n     return stacktraces\n \n \n-def dicts_to_sum(objects: Union[dict[str, dict], list[dict]]):\n+def dicts_to_sum(objects: dict[str, dict] | list[dict]):\n     if isinstance(objects, dict):\n         lists = objects.values()\n     else:\n@@ -139,7 +139,7 @@ def __init__(\n         ci_title: str,\n         model_results: dict,\n         additional_results: dict,\n-        selected_warnings: Optional[list] = None,\n+        selected_warnings: list | None = None,\n         prev_ci_artifacts=None,\n         other_ci_artifacts=None,\n     ):\n@@ -941,7 +941,7 @@ def post_reply(self):\n                     time.sleep(1)\n \n \n-def retrieve_artifact(artifact_path: str, gpu: Optional[str]):\n+def retrieve_artifact(artifact_path: str, gpu: str | None):\n     if gpu not in [None, \"single\", \"multi\"]:\n         raise ValueError(f\"Invalid GPU for artifact. Passed GPU: `{gpu}`.\")\n \n@@ -970,7 +970,7 @@ def __init__(self, name: str, single_gpu: bool = False, multi_gpu: bool = False)\n         def __str__(self):\n             return self.name\n \n-        def add_path(self, path: str, gpu: Optional[str] = None):\n+        def add_path(self, path: str, gpu: str | None = None):\n             self.paths.append({\"name\": self.name, \"path\": path, \"gpu\": gpu})\n \n     _available_artifacts: dict[str, Artifact] = {}"
        },
        {
            "sha": "4f9d5a905a2f5e4365030d9ddce01bab9016c96f",
            "filename": "utils/sort_auto_mappings.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e99198e5e7135010dfe09888e80b95a3f077e3e/utils%2Fsort_auto_mappings.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e99198e5e7135010dfe09888e80b95a3f077e3e/utils%2Fsort_auto_mappings.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fsort_auto_mappings.py?ref=9e99198e5e7135010dfe09888e80b95a3f077e3e",
            "patch": "@@ -33,7 +33,6 @@\n import argparse\n import os\n import re\n-from typing import Optional\n \n \n # Path are set with the intent you should run this script from the root of the repo.\n@@ -47,7 +46,7 @@\n _re_identifier = re.compile(r'\\s*\\(\\s*\"(\\S[^\"]+)\"')\n \n \n-def sort_auto_mapping(fname: str, overwrite: bool = False) -> Optional[bool]:\n+def sort_auto_mapping(fname: str, overwrite: bool = False) -> bool | None:\n     \"\"\"\n     Sort all auto mappings in a file.\n "
        },
        {
            "sha": "4c1d26d89cffaa89684c1d25e453ffdfcc3ecf1e",
            "filename": "utils/tests_fetcher.py",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e99198e5e7135010dfe09888e80b95a3f077e3e/utils%2Ftests_fetcher.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e99198e5e7135010dfe09888e80b95a3f077e3e/utils%2Ftests_fetcher.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Ftests_fetcher.py?ref=9e99198e5e7135010dfe09888e80b95a3f077e3e",
            "patch": "@@ -57,7 +57,6 @@\n import re\n from contextlib import contextmanager\n from pathlib import Path\n-from typing import Optional, Union\n \n from git import Repo\n \n@@ -555,7 +554,7 @@ def get_doctest_files(diff_with_last_commit: bool = False) -> list[str]:\n _re_multi_line_direct_imports = re.compile(r\"(?:^|\\n)\\s*from\\s+transformers(\\S*)\\s+import\\s+\\(([^\\)]+)\\)\")\n \n \n-def extract_imports(module_fname: str, cache: Optional[dict[str, list[str]]] = None) -> list[str]:\n+def extract_imports(module_fname: str, cache: dict[str, list[str]] | None = None) -> list[str]:\n     \"\"\"\n     Get the imports a given module makes.\n \n@@ -637,7 +636,7 @@ def extract_imports(module_fname: str, cache: Optional[dict[str, list[str]]] = N\n     return result\n \n \n-def get_module_dependencies(module_fname: str, cache: Optional[dict[str, list[str]]] = None) -> list[str]:\n+def get_module_dependencies(module_fname: str, cache: dict[str, list[str]] | None = None) -> list[str]:\n     \"\"\"\n     Refines the result of `extract_imports` to remove subfolders and get a proper list of module filenames: if a file\n     as an import `from utils import Foo, Bar`, with `utils` being a subfolder containing many files, this will traverse\n@@ -734,7 +733,7 @@ def create_reverse_dependency_tree() -> list[tuple[str, str]]:\n     return list(set(edges))\n \n \n-def get_tree_starting_at(module: str, edges: list[tuple[str, str]]) -> list[Union[str, list[str]]]:\n+def get_tree_starting_at(module: str, edges: list[tuple[str, str]]) -> list[str | list[str]]:\n     \"\"\"\n     Returns the tree starting at a given module following all edges.\n \n@@ -883,7 +882,7 @@ def create_reverse_dependency_map() -> dict[str, list[str]]:\n \n \n def create_module_to_test_map(\n-    reverse_map: Optional[dict[str, list[str]]] = None, filter_models: bool = False\n+    reverse_map: dict[str, list[str]] | None = None, filter_models: bool = False\n ) -> dict[str, list[str]]:\n     \"\"\"\n     Extract the tests from the reverse_dependency_map and potentially filters the model tests."
        }
    ],
    "stats": {
        "total": 304,
        "additions": 136,
        "deletions": 168
    }
}