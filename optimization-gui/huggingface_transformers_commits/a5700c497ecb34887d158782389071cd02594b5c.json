{
    "author": "Samoed",
    "message": "Better typehints for `apply_chat_template` (#41355)",
    "sha": "a5700c497ecb34887d158782389071cd02594b5c",
    "files": [
        {
            "sha": "5de343759de9620adde45b7b5ebf477ea67b46a9",
            "filename": "src/transformers/models/auto/tokenization_auto.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5700c497ecb34887d158782389071cd02594b5c/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5700c497ecb34887d158782389071cd02594b5c/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py?ref=a5700c497ecb34887d158782389071cd02594b5c",
            "patch": "@@ -940,7 +940,9 @@ def __init__(self):\n \n     @classmethod\n     @replace_list_option_in_docstrings(TOKENIZER_MAPPING_NAMES)\n-    def from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs):\n+    def from_pretrained(\n+        cls, pretrained_model_name_or_path, *inputs, **kwargs\n+    ) -> Union[PreTrainedTokenizer, PreTrainedTokenizerFast]:\n         r\"\"\"\n         Instantiate one of the tokenizer classes of the library from a pretrained model vocabulary.\n "
        },
        {
            "sha": "0533bdfaaeda01e17355a81215ee861903f60828",
            "filename": "src/transformers/tokenization_utils_base.py",
            "status": "modified",
            "additions": 64,
            "deletions": 1,
            "changes": 65,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5700c497ecb34887d158782389071cd02594b5c/src%2Ftransformers%2Ftokenization_utils_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5700c497ecb34887d158782389071cd02594b5c/src%2Ftransformers%2Ftokenization_utils_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_base.py?ref=a5700c497ecb34887d158782389071cd02594b5c",
            "patch": "@@ -28,7 +28,7 @@\n from contextlib import contextmanager\n from dataclasses import dataclass\n from pathlib import Path\n-from typing import TYPE_CHECKING, Any, Callable, NamedTuple, Optional, Union\n+from typing import TYPE_CHECKING, Any, Callable, Literal, NamedTuple, Optional, Union, overload\n \n import numpy as np\n from packaging import version\n@@ -1507,6 +1507,69 @@ def get_vocab(self) -> dict[str, int]:\n         \"\"\"\n         raise NotImplementedError()\n \n+    # Case: tokenize=False → returns rendered string\n+    @overload\n+    def apply_chat_template(\n+        self,\n+        conversation: Union[list[dict[str, str]], list[list[dict[str, str]]]],\n+        tools: Optional[list[Union[dict, Callable]]] = None,\n+        documents: Optional[list[dict[str, str]]] = None,\n+        chat_template: Optional[str] = None,\n+        add_generation_prompt: bool = False,\n+        continue_final_message: bool = False,\n+        tokenize: Literal[False] = False,\n+        padding: Union[bool, str, PaddingStrategy] = False,\n+        truncation: bool = False,\n+        max_length: Optional[int] = None,\n+        return_tensors: Optional[Union[str, TensorType]] = None,\n+        return_dict: bool = False,\n+        return_assistant_tokens_mask: bool = False,\n+        tokenizer_kwargs: Optional[dict[str, Any]] = None,\n+        **kwargs,\n+    ) -> str: ...\n+\n+    # Case: tokenize=True, return_dict=False, return_tensors=None → returns ids\n+    @overload\n+    def apply_chat_template(\n+        self,\n+        conversation: Union[list[dict[str, str]], list[list[dict[str, str]]]],\n+        tools: Optional[list[Union[dict, Callable]]] = None,\n+        documents: Optional[list[dict[str, str]]] = None,\n+        chat_template: Optional[str] = None,\n+        add_generation_prompt: bool = False,\n+        continue_final_message: bool = False,\n+        tokenize: Literal[True] = True,\n+        padding: Union[bool, str, PaddingStrategy] = False,\n+        truncation: bool = False,\n+        max_length: Optional[int] = None,\n+        return_tensors: Literal[None] = None,\n+        return_dict: Literal[False] = False,\n+        return_assistant_tokens_mask: bool = False,\n+        tokenizer_kwargs: Optional[dict[str, Any]] = None,\n+        **kwargs,\n+    ) -> Union[list[int], list[list[int]]]: ...\n+\n+    # Case: tokenize=True, return_dict=True → returns BatchEncoding\n+    @overload\n+    def apply_chat_template(\n+        self,\n+        conversation: Union[list[dict[str, str]], list[list[dict[str, str]]]],\n+        tools: Optional[list[Union[dict, Callable]]] = None,\n+        documents: Optional[list[dict[str, str]]] = None,\n+        chat_template: Optional[str] = None,\n+        add_generation_prompt: bool = False,\n+        continue_final_message: bool = False,\n+        tokenize: Literal[True] = True,\n+        padding: Union[bool, str, PaddingStrategy] = False,\n+        truncation: bool = False,\n+        max_length: Optional[int] = None,\n+        return_tensors: Optional[Union[str, TensorType]] = None,\n+        return_dict: Literal[True] = True,\n+        return_assistant_tokens_mask: bool = False,\n+        tokenizer_kwargs: Optional[dict[str, Any]] = None,\n+        **kwargs,\n+    ) -> BatchEncoding: ...\n+\n     def apply_chat_template(\n         self,\n         conversation: Union[list[dict[str, str]], list[list[dict[str, str]]]],"
        }
    ],
    "stats": {
        "total": 69,
        "additions": 67,
        "deletions": 2
    }
}