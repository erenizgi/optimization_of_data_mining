{
    "author": "AshAnand34",
    "message": "Created model card for xlm-roberta-xl (#38597)\n\n* Created model card for xlm-roberta-xl\n\n* Update XLM-RoBERTa-XL model card with improved descriptions and usage examples\n\n* Minor option labeling fix\n\n* Added MaskedLM version of XLM RoBERTa XL to model card\n\n* Added quantization example for XLM RoBERTa XL model card\n\n* minor fixes to xlm roberta xl model card\n\n* Minor fixes to mask format in xlm roberta xl model card",
    "sha": "b61c47f5a555219bc6e29789263ea39d754109f4",
    "files": [
        {
            "sha": "56306bcb4a6c80d50f9a22ecc14a14f65c9fa86d",
            "filename": "docs/source/en/model_doc/xlm-roberta-xl.md",
            "status": "modified",
            "additions": 96,
            "deletions": 20,
            "changes": 116,
            "blob_url": "https://github.com/huggingface/transformers/blob/b61c47f5a555219bc6e29789263ea39d754109f4/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlm-roberta-xl.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b61c47f5a555219bc6e29789263ea39d754109f4/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlm-roberta-xl.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlm-roberta-xl.md?ref=b61c47f5a555219bc6e29789263ea39d754109f4",
            "patch": "@@ -14,37 +14,113 @@ rendered properly in your Markdown viewer.\n \n -->\n \n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+        <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+        <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+    </div>\n+</div>\n+\n # XLM-RoBERTa-XL\n \n-<div class=\"flex flex-wrap space-x-1\">\n-<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-</div>\n+[XLM-RoBERTa-XL](https://huggingface.co/papers/2105.00572) is a 3.5B parameter multilingual masked language model pretrained on 100 languages. It shows that by scaling model capacity, multilingual models demonstrates strong performance on high-resource languages and can even zero-shot low-resource languages.\n+\n+You can find all the original XLM-RoBERTa-XL checkpoints under the [AI at Meta](https://huggingface.co/facebook?search_models=xlm) organization.\n+\n+> [!TIP]\n+> Click on the XLM-RoBERTa-XL models in the right sidebar for more examples of how to apply XLM-RoBERTa-XL to different cross-lingual tasks like classification, translation, and question answering.\n+\n+The example below demonstrates how to predict the `<mask>` token with [`Pipeline`], [`AutoModel`], and from the command line.\n+\n+<hfoptions id=\"usage\">\n+<hfoption id=\"Pipeline\">\n+\n+```python\n+import torch  \n+from transformers import pipeline  \n+\n+pipeline = pipeline(  \n+    task=\"fill-mask\",  \n+    model=\"facebook/xlm-roberta-xl\",  \n+    torch_dtype=torch.float16,  \n+    device=0  \n+)  \n+pipeline(\"Bonjour, je suis un modèle <mask>.\")  \n+```\n+\n+</hfoption>\n+<hfoption id=\"AutoModel\">\n+\n+```python\n+import torch  \n+from transformers import AutoModelForMaskedLM, AutoTokenizer  \n+\n+tokenizer = AutoTokenizer.from_pretrained(  \n+    \"facebook/xlm-roberta-xl\",  \n+)  \n+model = AutoModelForMaskedLM.from_pretrained(  \n+    \"facebook/xlm-roberta-xl\",  \n+    torch_dtype=torch.float16,  \n+    device_map=\"auto\",  \n+    attn_implementation=\"sdpa\"  \n+)  \n+inputs = tokenizer(\"Bonjour, je suis un modèle <mask>.\", return_tensors=\"pt\").to(\"cuda\")  \n+\n+with torch.no_grad():  \n+    outputs = model(**inputs)  \n+    predictions = outputs.logits  \n+\n+masked_index = torch.where(inputs['input_ids'] == tokenizer.mask_token_id)[1]  \n+predicted_token_id = predictions[0, masked_index].argmax(dim=-1)  \n+predicted_token = tokenizer.decode(predicted_token_id)  \n+\n+print(f\"The predicted token is: {predicted_token}\")\n+```\n+</hfoption>\n+\n+<hfoption id=\"transformers CLI\">\n+\n+```bash\n+echo -e \"Plants create <mask> through a process known as photosynthesis.\" | transformers-cli run --task fill-mask --model facebook/xlm-roberta-xl --device 0\n+```\n+</hfoption>\n+</hfoptions>\n \n-## Overview\n+Quantization reduces the memory burden of large models by representing the weights in a lower precision. Refer to the [Quantization](../quantization/overview) overview for more available quantization backends.\n \n-The XLM-RoBERTa-XL model was proposed in [Larger-Scale Transformers for Multilingual Masked Language Modeling](https://arxiv.org/abs/2105.00572) by Naman Goyal, Jingfei Du, Myle Ott, Giri Anantharaman, Alexis Conneau. \n+The example below uses [torchao](../quantization/torchao) to only quantize the weights to int4.\n \n-The abstract from the paper is the following:\n+```py\n+import torch\n+from transformers import AutoModelForMaskedLM, AutoTokenizer, TorchAoConfig\n \n-*Recent work has demonstrated the effectiveness of cross-lingual language model pretraining for cross-lingual understanding. In this study, we present the results of two larger multilingual masked language models, with 3.5B and 10.7B parameters. Our two new models dubbed XLM-R XL and XLM-R XXL outperform XLM-R by 1.8% and 2.4% average accuracy on XNLI. Our model also outperforms the RoBERTa-Large model on several English tasks of the GLUE benchmark by 0.3% on average while handling 99 more languages. This suggests pretrained models with larger capacity may obtain both strong performance on high-resource languages while greatly improving low-resource languages. We make our code and models publicly available.*\n+quantization_config = TorchAoConfig(\"int4_weight_only\", group_size=128)\n+tokenizer = AutoTokenizer.from_pretrained(\n+    \"facebook/xlm-roberta-xl\",\n+)\n+model = AutoModelForMaskedLM.from_pretrained(\n+    \"facebook/xlm-roberta-xl\",\n+    torch_dtype=torch.float16,\n+    device_map=\"auto\",\n+    attn_implementation=\"sdpa\",\n+    quantization_config=quantization_config\n+)\n+inputs = tokenizer(\"Bonjour, je suis un modèle <mask>.\", return_tensors=\"pt\").to(\"cuda\")\n \n-This model was contributed by [Soonhwan-Kwon](https://github.com/Soonhwan-Kwon) and [stefan-it](https://huggingface.co/stefan-it). The original code can be found [here](https://github.com/pytorch/fairseq/tree/master/examples/xlmr).\n+with torch.no_grad():\n+    outputs = model(**inputs)\n+    predictions = outputs.logits\n \n-## Usage tips\n+masked_index = torch.where(inputs['input_ids'] == tokenizer.mask_token_id)[1]\n+predicted_token_id = predictions[0, masked_index].argmax(dim=-1)\n+predicted_token = tokenizer.decode(predicted_token_id)\n \n-XLM-RoBERTa-XL is a multilingual model trained on 100 different languages. Unlike some XLM multilingual models, it does \n-not require `lang` tensors to understand which language is used, and should be able to determine the correct \n-language from the input ids.\n+print(f\"The predicted token is: {predicted_token}\")\n+```\n \n-## Resources\n+## Notes\n \n-- [Text classification task guide](../tasks/sequence_classification)\n-- [Token classification task guide](../tasks/token_classification)\n-- [Question answering task guide](../tasks/question_answering)\n-- [Causal language modeling task guide](../tasks/language_modeling)\n-- [Masked language modeling task guide](../tasks/masked_language_modeling)\n-- [Multiple choice task guide](../tasks/multiple_choice)\n+- Unlike some XLM models, XLM-RoBERTa-XL doesn't require `lang` tensors to understand which language is used. It automatically determines the language from the input ids.\n \n ## XLMRobertaXLConfig\n "
        }
    ],
    "stats": {
        "total": 116,
        "additions": 96,
        "deletions": 20
    }
}