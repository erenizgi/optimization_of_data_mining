{
    "author": "2015aroras",
    "message": "Make HF implementation match original OLMo 2 models for lower precisions (#38131)\n\n* Make HF implementation match OLMo models for lower precisions\n\n* Add test of 1B logits in bfloat16\n\n* Run make fixup",
    "sha": "aef12349b690541285a8ff6565561e8c45af2afe",
    "files": [
        {
            "sha": "72abff85dd581160a7b91638bb6d6072ba4dec2c",
            "filename": "src/transformers/models/olmo/modeling_olmo.py",
            "status": "modified",
            "additions": 29,
            "deletions": 29,
            "changes": 58,
            "blob_url": "https://github.com/huggingface/transformers/blob/aef12349b690541285a8ff6565561e8c45af2afe/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/aef12349b690541285a8ff6565561e8c45af2afe/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py?ref=aef12349b690541285a8ff6565561e8c45af2afe",
            "patch": "@@ -70,33 +70,6 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n-def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n-    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n-\n-    Args:\n-        q (`torch.Tensor`): The query tensor.\n-        k (`torch.Tensor`): The key tensor.\n-        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n-        sin (`torch.Tensor`): The sine part of the rotary embedding.\n-        position_ids (`torch.Tensor`, *optional*):\n-            Deprecated and unused.\n-        unsqueeze_dim (`int`, *optional*, defaults to 1):\n-            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n-            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n-            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n-            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n-            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n-            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n-    Returns:\n-        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n-    \"\"\"\n-    cos = cos.unsqueeze(unsqueeze_dim)\n-    sin = sin.unsqueeze(unsqueeze_dim)\n-    q_embed = (q * cos) + (rotate_half(q) * sin)\n-    k_embed = (k * cos) + (rotate_half(k) * sin)\n-    return q_embed, k_embed\n-\n-\n def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     \"\"\"\n     This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n@@ -135,6 +108,34 @@ def eager_attention_forward(\n     return attn_output, attn_weights\n \n \n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n+    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n+\n+    Args:\n+        q (`torch.Tensor`): The query tensor.\n+        k (`torch.Tensor`): The key tensor.\n+        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n+        sin (`torch.Tensor`): The sine part of the rotary embedding.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n+        unsqueeze_dim (`int`, *optional*, defaults to 1):\n+            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n+            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n+            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n+            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n+            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n+            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n+    Returns:\n+        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n+    \"\"\"\n+    q_type, k_type = q.dtype, k.dtype\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n+    q_embed = (q * cos) + (rotate_half(q) * sin)\n+    k_embed = (k * cos) + (rotate_half(k) * sin)\n+    return q_embed.to(q_type), k_embed.to(k_type)\n+\n+\n class OlmoAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -302,8 +303,7 @@ def forward(self, x, position_ids):\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling\n             sin = emb.sin() * self.attention_scaling\n-\n-        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+            return cos, sin\n \n \n @auto_docstring"
        },
        {
            "sha": "02ff85ac1007a6a45ab7ae5f0ab08bfe733e315a",
            "filename": "src/transformers/models/olmo/modular_olmo.py",
            "status": "modified",
            "additions": 42,
            "deletions": 2,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/aef12349b690541285a8ff6565561e8c45af2afe/src%2Ftransformers%2Fmodels%2Folmo%2Fmodular_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/aef12349b690541285a8ff6565561e8c45af2afe/src%2Ftransformers%2Fmodels%2Folmo%2Fmodular_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodular_olmo.py?ref=aef12349b690541285a8ff6565561e8c45af2afe",
            "patch": "@@ -16,8 +16,8 @@\n     LlamaModel,\n     LlamaPreTrainedModel,\n     LlamaRotaryEmbedding,\n-    apply_rotary_pos_emb,\n     eager_attention_forward,\n+    rotate_half,\n )\n from .configuration_olmo import OlmoConfig\n \n@@ -47,6 +47,34 @@ def __init__(self, config):\n         self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n \n \n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n+    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n+\n+    Args:\n+        q (`torch.Tensor`): The query tensor.\n+        k (`torch.Tensor`): The key tensor.\n+        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n+        sin (`torch.Tensor`): The sine part of the rotary embedding.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n+        unsqueeze_dim (`int`, *optional*, defaults to 1):\n+            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n+            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n+            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n+            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n+            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n+            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n+    Returns:\n+        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n+    \"\"\"\n+    q_type, k_type = q.dtype, k.dtype\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n+    q_embed = (q * cos) + (rotate_half(q) * sin)\n+    k_embed = (k * cos) + (rotate_half(k) * sin)\n+    return q_embed.to(q_type), k_embed.to(k_type)\n+\n+\n class OlmoAttention(LlamaAttention):\n     def forward(\n         self,\n@@ -115,8 +143,20 @@ def __init__(self, config: OlmoConfig, layer_idx: int):\n         self.self_attn = OlmoAttention(config=config, layer_idx=layer_idx)\n \n \n+# This is identical to LlamaRotaryEmbedding except the output cos and sin are returned\n+# as float32 rather than the input type.\n class OlmoRotaryEmbedding(LlamaRotaryEmbedding):\n-    pass\n+    def forward(self, x, position_ids):\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n+            return cos, sin\n \n \n class OlmoPreTrainedModel(LlamaPreTrainedModel):"
        },
        {
            "sha": "58cad693741d7006bea9783d089d61f2c30a48ac",
            "filename": "src/transformers/models/olmo2/modeling_olmo2.py",
            "status": "modified",
            "additions": 37,
            "deletions": 37,
            "changes": 74,
            "blob_url": "https://github.com/huggingface/transformers/blob/aef12349b690541285a8ff6565561e8c45af2afe/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/aef12349b690541285a8ff6565561e8c45af2afe/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py?ref=aef12349b690541285a8ff6565561e8c45af2afe",
            "patch": "@@ -48,46 +48,12 @@ def forward(self, hidden_states):\n         hidden_states = hidden_states.to(torch.float32)\n         variance = hidden_states.pow(2).mean(-1, keepdim=True)\n         hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n-        return self.weight * hidden_states.to(input_dtype)\n+        return (self.weight * hidden_states).to(input_dtype)\n \n     def extra_repr(self):\n         return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n \n \n-def rotate_half(x):\n-    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n-    x1 = x[..., : x.shape[-1] // 2]\n-    x2 = x[..., x.shape[-1] // 2 :]\n-    return torch.cat((-x2, x1), dim=-1)\n-\n-\n-def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n-    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n-\n-    Args:\n-        q (`torch.Tensor`): The query tensor.\n-        k (`torch.Tensor`): The key tensor.\n-        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n-        sin (`torch.Tensor`): The sine part of the rotary embedding.\n-        position_ids (`torch.Tensor`, *optional*):\n-            Deprecated and unused.\n-        unsqueeze_dim (`int`, *optional*, defaults to 1):\n-            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n-            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n-            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n-            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n-            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n-            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n-    Returns:\n-        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n-    \"\"\"\n-    cos = cos.unsqueeze(unsqueeze_dim)\n-    sin = sin.unsqueeze(unsqueeze_dim)\n-    q_embed = (q * cos) + (rotate_half(q) * sin)\n-    k_embed = (k * cos) + (rotate_half(k) * sin)\n-    return q_embed, k_embed\n-\n-\n def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     \"\"\"\n     This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n@@ -126,6 +92,41 @@ def eager_attention_forward(\n     return attn_output, attn_weights\n \n \n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n+    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n+\n+    Args:\n+        q (`torch.Tensor`): The query tensor.\n+        k (`torch.Tensor`): The key tensor.\n+        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n+        sin (`torch.Tensor`): The sine part of the rotary embedding.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n+        unsqueeze_dim (`int`, *optional*, defaults to 1):\n+            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n+            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n+            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n+            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n+            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n+            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n+    Returns:\n+        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n+    \"\"\"\n+    q_type, k_type = q.dtype, k.dtype\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n+    q_embed = (q * cos) + (rotate_half(q) * sin)\n+    k_embed = (k * cos) + (rotate_half(k) * sin)\n+    return q_embed.to(q_type), k_embed.to(k_type)\n+\n+\n+def rotate_half(x):\n+    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n+    x1 = x[..., : x.shape[-1] // 2]\n+    x2 = x[..., x.shape[-1] // 2 :]\n+    return torch.cat((-x2, x1), dim=-1)\n+\n+\n class Olmo2Attention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -306,8 +307,7 @@ def forward(self, x, position_ids):\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling\n             sin = emb.sin() * self.attention_scaling\n-\n-        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+            return cos, sin\n \n \n @auto_docstring"
        },
        {
            "sha": "a6527098624565023ce80adb894eabb3a84c8f9a",
            "filename": "src/transformers/models/olmo2/modular_olmo2.py",
            "status": "modified",
            "additions": 15,
            "deletions": 1,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/aef12349b690541285a8ff6565561e8c45af2afe/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodular_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/aef12349b690541285a8ff6565561e8c45af2afe/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodular_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodular_olmo2.py?ref=aef12349b690541285a8ff6565561e8c45af2afe",
            "patch": "@@ -165,13 +165,27 @@ def __init__(\n         del self.clip_qkv\n \n \n+# OLMo2 RMS norm is identical to Llama RMS norm except:\n+# - Weight and hidden states are multiplied before converting back to the input dtype, rather than after.\n class Olmo2RMSNorm(LlamaRMSNorm):\n-    pass\n+    def forward(self, hidden_states):\n+        input_dtype = hidden_states.dtype\n+        hidden_states = hidden_states.to(torch.float32)\n+        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n+        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n+        return (self.weight * hidden_states).to(input_dtype)\n \n \n ALL_LAYERNORM_LAYERS.append(Olmo2RMSNorm)\n \n \n+def rotate_half(x):\n+    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n+    x1 = x[..., : x.shape[-1] // 2]\n+    x2 = x[..., x.shape[-1] // 2 :]\n+    return torch.cat((-x2, x1), dim=-1)\n+\n+\n # Olmo2 attention is identical to OLMo attention except:\n # - Norm is applied to attention queries and keys.\n # - No qkv clipping."
        },
        {
            "sha": "dcef3148d2d251c609f083b28457c3dc46c2d448",
            "filename": "tests/models/olmo2/test_modeling_olmo2.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/aef12349b690541285a8ff6565561e8c45af2afe/tests%2Fmodels%2Folmo2%2Ftest_modeling_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/aef12349b690541285a8ff6565561e8c45af2afe/tests%2Fmodels%2Folmo2%2Ftest_modeling_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Folmo2%2Ftest_modeling_olmo2.py?ref=aef12349b690541285a8ff6565561e8c45af2afe",
            "patch": "@@ -232,6 +232,18 @@ def test_model_rope_scaling(self, scaling_type):\n \n @require_torch\n class Olmo2IntegrationTest(unittest.TestCase):\n+    @slow\n+    def test_model_1b_logits_bfloat16(self):\n+        input_ids = [[1, 306, 4658, 278, 6593, 310, 2834, 338]]\n+        model = Olmo2ForCausalLM.from_pretrained(\"allenai/OLMo-2-0425-1B\").to(torch.bfloat16)\n+        out = model(torch.tensor(input_ids)).logits.float()\n+        # Expected mean on dim = -1\n+        EXPECTED_MEAN = torch.tensor([[-5.7094, -6.5548, -3.2527, -2.7847, -5.5092, -4.5223, -4.8427, -4.6867]])\n+        torch.testing.assert_close(out.mean(-1), EXPECTED_MEAN, rtol=1e-2, atol=1e-2)\n+        # slicing logits[0, 0, 0:30]\n+        EXPECTED_SLICE = torch.tensor([2.4531, -5.7188, -5.1562, -4.8750, -6.7812, -4.0625, -4.4375, -4.5938, -7.5938, -5.0938, -3.9375, -3.6875, -5.0938, -3.1875, -5.6875, 0.2266, 1.2578, 1.1016, 0.8945, 0.4785, 0.2256, -0.3613, -0.4258, 0.1377, -0.1104, -7.1875, -5.2188, -6.8125, -0.9062, -2.9062])  # fmt: skip\n+        torch.testing.assert_close(out[0, 0, :30], EXPECTED_SLICE, rtol=1e-2, atol=1e-2)\n+\n     @slow\n     def test_model_7b_logits(self):\n         input_ids = [[1, 306, 4658, 278, 6593, 310, 2834, 338]]"
        }
    ],
    "stats": {
        "total": 204,
        "additions": 135,
        "deletions": 69
    }
}