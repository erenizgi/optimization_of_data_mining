{
    "author": "guangy10",
    "message": "Make StaticCache configurable at model construct time (#32830)\n\n* Make StaticCache configurable at model construct time\r\n\r\n* integrations import structure\r\n\r\n* add new doc file to toc\r\n\r\n---------\r\n\r\nCo-authored-by: Guang Yang <guangyang@fb.com>\r\nCo-authored-by: Joao Gante <joao@huggingface.co>",
    "sha": "f38590dade57c1f8cf8a67e9409dae8935f8c478",
    "files": [
        {
            "sha": "1c7f62ec6ea7b8990915c320385d77527de9cf6d",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f38590dade57c1f8cf8a67e9409dae8935f8c478/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/f38590dade57c1f8cf8a67e9409dae8935f8c478/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=f38590dade57c1f8cf8a67e9409dae8935f8c478",
            "patch": "@@ -296,6 +296,8 @@\n       title: Trainer\n     - local: main_classes/deepspeed\n       title: DeepSpeed\n+    - local: main_classes/executorch\n+      title: ExecuTorch\n     - local: main_classes/feature_extractor\n       title: Feature Extractor\n     - local: main_classes/image_processor"
        },
        {
            "sha": "28e0a445e79f4ba6adbe126663a8839992aa4091",
            "filename": "docs/source/en/main_classes/executorch.md",
            "status": "added",
            "additions": 33,
            "deletions": 0,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/f38590dade57c1f8cf8a67e9409dae8935f8c478/docs%2Fsource%2Fen%2Fmain_classes%2Fexecutorch.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f38590dade57c1f8cf8a67e9409dae8935f8c478/docs%2Fsource%2Fen%2Fmain_classes%2Fexecutorch.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmain_classes%2Fexecutorch.md?ref=f38590dade57c1f8cf8a67e9409dae8935f8c478",
            "patch": "@@ -0,0 +1,33 @@\n+<!--Copyright (c) Meta Platforms, Inc. and affiliates.\n+All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+\n+# ExecuTorch\n+\n+[`ExecuTorch`](https://github.com/pytorch/executorch) is an end-to-end solution for enabling on-device inference capabilities across mobile and edge devices including wearables, embedded devices and microcontrollers. It is part of the PyTorch ecosystem and supports the deployment of PyTorch models with a focus on portability, productivity, and performance.\n+\n+ExecuTorch introduces well defined entry points to perform model, device, and/or use-case specific optimizations such as backend delegation, user-defined compiler transformations, memory planning, and more. The first step in preparing a PyTorch model for execution on an edge device using ExecuTorch is to export the model. This is achieved through the use of a PyTorch API called [`torch.export`](https://pytorch.org/docs/stable/export.html).\n+\n+\n+## ExecuTorch Integration\n+\n+An integration point is being developed to ensure that ðŸ¤— Transformers can be exported using `torch.export`. The goal of this integration is not only to enable export but also to ensure that the exported artifact can be further lowered and optimized to run efficiently in `ExecuTorch`, particularly for mobile and edge use cases.\n+\n+[[autodoc]] integrations.executorch.TorchExportableModuleWithStaticCache\n+    - forward\n+\n+[[autodoc]] integrations.executorch.convert_and_export_with_cache"
        },
        {
            "sha": "00cc67915f3664402e7316aac16b9150ace6b09c",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/f38590dade57c1f8cf8a67e9409dae8935f8c478/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f38590dade57c1f8cf8a67e9409dae8935f8c478/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=f38590dade57c1f8cf8a67e9409dae8935f8c478",
            "patch": "@@ -1323,6 +1323,13 @@\n             \"WhisperTimeStampLogitsProcessor\",\n         ]\n     )\n+\n+    # PyTorch domain libraries integration\n+    _import_structure[\"integrations.executorch\"] = [\n+        \"TorchExportableModuleWithStaticCache\",\n+        \"convert_and_export_with_cache\",\n+    ]\n+\n     _import_structure[\"modeling_flash_attention_utils\"] = []\n     _import_structure[\"modeling_outputs\"] = []\n     _import_structure[\"modeling_rope_utils\"] = [\"ROPE_INIT_FUNCTIONS\"]\n@@ -6121,6 +6128,10 @@\n             WatermarkLogitsProcessor,\n             WhisperTimeStampLogitsProcessor,\n         )\n+        from .integrations.executorch import (\n+            TorchExportableModuleWithStaticCache,\n+            convert_and_export_with_cache,\n+        )\n         from .modeling_rope_utils import ROPE_INIT_FUNCTIONS\n         from .modeling_utils import PreTrainedModel\n         from .models.albert import ("
        },
        {
            "sha": "b3e94da3d7d7bd737b47e49079a752fc5d67c0b7",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 40,
            "deletions": 0,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/f38590dade57c1f8cf8a67e9409dae8935f8c478/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f38590dade57c1f8cf8a67e9409dae8935f8c478/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=f38590dade57c1f8cf8a67e9409dae8935f8c478",
            "patch": "@@ -293,6 +293,46 @@ def validate(self):\n             )\n \n \n+@dataclass\n+class StaticCacheConfig(CacheConfig):\n+    \"\"\"\n+    Configuration class for static cache settings.\n+    \"\"\"\n+\n+    cache_implementation = \"static\"\n+\n+    def __init__(self, batch_size: int, max_cache_len: int, device=\"cpu\"):\n+        self.batch_size = batch_size\n+        self.max_cache_len = max_cache_len\n+        self.device = device\n+\n+    def validate(self):\n+        \"\"\"Validates if the arguments passed are correct\"\"\"\n+\n+        incorrect_arg_msg = (\n+            \"Some of the keys in `cache_config` are defined incorrectly. `{key}` should be {correct_value}` \"\n+            \"but found {found_value}\"\n+        )\n+\n+        if self.batch_size <= 0:\n+            raise ValueError(\n+                incorrect_arg_msg.format(\n+                    key=\"batch_size\",\n+                    correct_value=\"> 0\",\n+                    found_value=self.batch_size,\n+                ),\n+            )\n+\n+        if self.max_cache_len <= 0:\n+            raise ValueError(\n+                incorrect_arg_msg.format(\n+                    key=\"max_cache_len\",\n+                    correct_value=\"> 0\",\n+                    found_value=self.max_cache_len,\n+                ),\n+            )\n+\n+\n class DynamicCache(Cache):\n     \"\"\"\n     A cache that grows dynamically as more tokens are generated. This is the default for generative models."
        },
        {
            "sha": "4eedf2699b55b552168b66bd1be7489eccfd3679",
            "filename": "src/transformers/generation/configuration_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f38590dade57c1f8cf8a67e9409dae8935f8c478/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f38590dade57c1f8cf8a67e9409dae8935f8c478/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py?ref=f38590dade57c1f8cf8a67e9409dae8935f8c478",
            "patch": "@@ -57,9 +57,11 @@\n         QuantoQuantizedCache,\n         SlidingWindowCache,\n         StaticCache,\n+        StaticCacheConfig,\n     )\n \n     NEEDS_CACHE_CONFIG[\"quantized\"] = QuantizedCacheConfig\n+    NEEDS_CACHE_CONFIG[\"static\"] = StaticCacheConfig\n     NEED_SETUP_CACHE_CLASSES_MAPPING = {\n         \"static\": StaticCache,\n         \"offloaded_static\": OffloadedStaticCache,"
        },
        {
            "sha": "0a28ff022a536b04b570881b5344a0ecfca9d10d",
            "filename": "src/transformers/integrations/__init__.py",
            "status": "modified",
            "additions": 21,
            "deletions": 1,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/f38590dade57c1f8cf8a67e9409dae8935f8c478/src%2Ftransformers%2Fintegrations%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f38590dade57c1f8cf8a67e9409dae8935f8c478/src%2Ftransformers%2Fintegrations%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2F__init__.py?ref=f38590dade57c1f8cf8a67e9409dae8935f8c478",
            "patch": "@@ -13,7 +13,7 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-from ..utils import _LazyModule\n+from ..utils import OptionalDependencyNotAvailable, _LazyModule, is_torch_available\n \n \n _import_structure = {\n@@ -98,6 +98,17 @@\n     \"quanto\": [\"replace_with_quanto_layers\"],\n }\n \n+try:\n+    if not is_torch_available():\n+        raise OptionalDependencyNotAvailable()\n+except OptionalDependencyNotAvailable:\n+    pass\n+else:\n+    _import_structure[\"executorch\"] = [\n+        \"TorchExportableModuleWithStaticCache\",\n+        \"convert_and_export_with_cache\",\n+    ]\n+\n if TYPE_CHECKING:\n     from .aqlm import replace_with_aqlm_linear\n     from .awq import (\n@@ -178,6 +189,15 @@\n     )\n     from .peft import PeftAdapterMixin\n     from .quanto import replace_with_quanto_layers\n+\n+    try:\n+        if not is_torch_available():\n+            raise OptionalDependencyNotAvailable()\n+    except OptionalDependencyNotAvailable:\n+        pass\n+    else:\n+        from .executorch import TorchExportableModuleWithStaticCache, convert_and_export_with_cache\n+\n else:\n     import sys\n "
        },
        {
            "sha": "afcba5ebd069297e58f95eabc52df3f13adaea31",
            "filename": "src/transformers/integrations/executorch.py",
            "status": "added",
            "additions": 159,
            "deletions": 0,
            "changes": 159,
            "blob_url": "https://github.com/huggingface/transformers/blob/f38590dade57c1f8cf8a67e9409dae8935f8c478/src%2Ftransformers%2Fintegrations%2Fexecutorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f38590dade57c1f8cf8a67e9409dae8935f8c478/src%2Ftransformers%2Fintegrations%2Fexecutorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fexecutorch.py?ref=f38590dade57c1f8cf8a67e9409dae8935f8c478",
            "patch": "@@ -0,0 +1,159 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+# the License. You may obtain a copy of the License at\n+#\n+# http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+# an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+# specific language governing permissions and limitations under the License.\n+\n+import torch\n+\n+from transformers import (\n+    PreTrainedModel,\n+    StaticCache,\n+)\n+from transformers.pytorch_utils import is_torch_greater_or_equal_than_2_3\n+\n+\n+class TorchExportableModuleWithStaticCache(torch.nn.Module):\n+    \"\"\"\n+    A wrapper module designed to make a `PreTrainedModel` exportable with `torch.export`,\n+    specifically for use with static caching. This module ensures that the exported model\n+    is compatible with further lowering and execution in `ExecuTorch`.\n+\n+    Note:\n+        This class is specifically designed to support export process using `torch.export`\n+        in a way that ensures the model can be further lowered and run efficiently in `ExecuTorch`.\n+    \"\"\"\n+\n+    def __init__(self, model: PreTrainedModel):\n+        \"\"\"\n+        Initializes the wrapper module with the pretrained model.\n+\n+        Args:\n+            model (`PreTrainedModel`): The pretrained model to wrap. The model must have caching\n+            enabled and use a 'static' caching implementation.\n+\n+        Raises:\n+            AssertionError: If the pretrained model does not have caching enabled or if it does\n+            not use a 'static' caching implementation in `model.generation_config`.\n+        \"\"\"\n+        super().__init__()\n+\n+        # Sanity checks\n+        if model.generation_config is None:\n+            raise AssertionError(\n+                \"The model must have a generation config to be exported with static caching. \"\n+                \"Please set `generation_config`.\"\n+            )\n+\n+        if not model.generation_config.use_cache:\n+            raise AssertionError(\n+                \"The model must have caching enabled to be exported with static caching. \"\n+                \"Please set `generation_config.use_cache=True`.\"\n+            )\n+\n+        if model.generation_config.cache_implementation != \"static\":\n+            raise AssertionError(\n+                \"The model must use a 'static' caching implementation to be exported with static caching. \"\n+                \"Please set `generation_config.cache_implementation='static'`.\"\n+            )\n+\n+        self.model = model\n+        self.static_cache = StaticCache(\n+            config=self.model.config,\n+            batch_size=self.model.generation_config.cache_config.batch_size,\n+            max_cache_len=self.model.generation_config.cache_config.max_cache_len,\n+            dtype=self.model.config.torch_dtype,\n+        )\n+        self.is_causal = any(\"CausalLM\" in arch for arch in self.model.config.architectures)\n+        if self.is_causal:\n+            causal_mask = torch.tril(\n+                torch.ones(\n+                    self.static_cache.max_cache_len,\n+                    self.static_cache.max_cache_len,\n+                    dtype=torch.bool,\n+                )\n+            )\n+            self.register_buffer(\"mask\", causal_mask, persistent=False)\n+\n+    def forward(self, input_ids: torch.Tensor, cache_position: torch.Tensor):\n+        \"\"\"\n+        Forward pass of the module, which is compatible with the ExecuTorch runtime.\n+\n+        Args:\n+            input_ids (`torch.Tensor`): Tensor representing current input token id to the module.\n+            cache_position (`torch.Tensor`): Tensor representing current input position in the cache.\n+\n+        Returns:\n+            torch.Tensor: Logits output from the model.\n+\n+        This forward adapter serves two primary purposes:\n+\n+        1. **Making the Model `torch.export`-Compatible**:\n+            The adapter hides unsupported objects, such as the `Cache`, from the graph inputs and outputs,\n+            enabling the model to be exportable using `torch.export` without encountering issues.\n+\n+        2. **Ensuring Compatibility with `ExecuTorch` runtime**:\n+            The adapter matches the model's forward signature with that in `executorch/extension/llm/runner`,\n+            ensuring that the exported model can be executed in `ExecuTorch` out-of-the-box.\n+        \"\"\"\n+        _, seqlen = input_ids.shape\n+        attn_mask = self.mask[cache_position, :seqlen] if self.is_causal else None\n+        outs = self.model(\n+            input_ids=input_ids,\n+            attention_mask=attn_mask,\n+            position_ids=cache_position.unsqueeze(0),\n+            cache_position=cache_position,\n+            past_key_values=self.static_cache,\n+            use_cache=True,\n+        )\n+        return outs.logits\n+\n+\n+def convert_and_export_with_cache(\n+    model: PreTrainedModel,\n+    example_input_ids: torch.Tensor = None,\n+    example_cache_position: torch.Tensor = None,\n+):\n+    \"\"\"\n+    Convert a `PreTrainedModel` into an exportable module and export it using `torch.export`,\n+    ensuring the exported model is compatible with `ExecuTorch`.\n+\n+    Args:\n+        model (`PreTrainedModel`): The pretrained model to be exported.\n+        example_input_ids (`torch.Tensor`): Example input token id used by `torch.export`.\n+        example_cache_position (`torch.Tensor`): Example current cache position used by `torch.export`.\n+\n+    Returns:\n+        Exported program (`torch.export.ExportedProgram`): The exported program generated via `torch.export`.\n+    \"\"\"\n+\n+    if not is_torch_greater_or_equal_than_2_3:\n+        raise ImportError(\"torch >= 2.3 is required.\")\n+\n+    import torch.export._trace\n+\n+    with torch.no_grad():\n+        # TODO: The default inputs only work for text models. We need to add support for vision/audio models.\n+        example_input_ids = (\n+            example_input_ids if example_input_ids is not None else torch.tensor([[1]], dtype=torch.long)\n+        )\n+        example_cache_position = (\n+            example_cache_position if example_cache_position is not None else torch.tensor([0], dtype=torch.long)\n+        )\n+\n+        # Due to issue https://github.com/pytorch/pytorch/issues/128394, we need to switch to use an internal\n+        # export API and pre_dispatch=False. Switch to use the public API once the issue is included in 2.5 release.\n+        exported_program = torch.export._trace._export(\n+            TorchExportableModuleWithStaticCache(model),\n+            args=(example_input_ids,),\n+            kwargs={\"cache_position\": example_cache_position},\n+            pre_dispatch=False,\n+            strict=True,\n+        )\n+        return exported_program"
        },
        {
            "sha": "359509f469a7039ca4655e8e662dc72667880bdf",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/f38590dade57c1f8cf8a67e9409dae8935f8c478/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f38590dade57c1f8cf8a67e9409dae8935f8c478/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=f38590dade57c1f8cf8a67e9409dae8935f8c478",
            "patch": "@@ -3223,6 +3223,7 @@ def from_pretrained(\n         adapter_kwargs = kwargs.pop(\"adapter_kwargs\", {})\n         adapter_name = kwargs.pop(\"adapter_name\", \"default\")\n         use_flash_attention_2 = kwargs.pop(\"use_flash_attention_2\", False)\n+        generation_config = kwargs.pop(\"generation_config\", None)\n \n         gguf_file = kwargs.pop(\"gguf_file\", None)\n         # Cache path to the GGUF file\n@@ -3998,7 +3999,10 @@ def from_pretrained(\n         model.eval()\n \n         # If it is a model with generation capabilities, attempt to load the generation config\n-        if model.can_generate() and pretrained_model_name_or_path is not None:\n+        if model.can_generate() and generation_config is not None:\n+            logger.info(\"The user-defined `generation_config` will be used to override the default generation config.\")\n+            model.generation_config = model.generation_config.from_dict(generation_config.to_dict())\n+        elif model.can_generate() and pretrained_model_name_or_path is not None:\n             try:\n                 model.generation_config = GenerationConfig.from_pretrained(\n                     pretrained_model_name_or_path,"
        },
        {
            "sha": "b9ce0d0f15bbf5a3887707042d96e6a643b411f9",
            "filename": "src/transformers/utils/dummy_pt_objects.py",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/f38590dade57c1f8cf8a67e9409dae8935f8c478/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f38590dade57c1f8cf8a67e9409dae8935f8c478/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py?ref=f38590dade57c1f8cf8a67e9409dae8935f8c478",
            "patch": "@@ -513,6 +513,17 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n+class TorchExportableModuleWithStaticCache(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n+def convert_and_export_with_cache(*args, **kwargs):\n+    requires_backends(convert_and_export_with_cache, [\"torch\"])\n+\n+\n ROPE_INIT_FUNCTIONS = None\n \n "
        },
        {
            "sha": "6ab821231fd5dfb5b7a2635fd746c3ead7395c4c",
            "filename": "tests/utils/test_cache_utils.py",
            "status": "modified",
            "additions": 42,
            "deletions": 49,
            "changes": 91,
            "blob_url": "https://github.com/huggingface/transformers/blob/f38590dade57c1f8cf8a67e9409dae8935f8c478/tests%2Futils%2Ftest_cache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f38590dade57c1f8cf8a67e9409dae8935f8c478/tests%2Futils%2Ftest_cache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_cache_utils.py?ref=f38590dade57c1f8cf8a67e9409dae8935f8c478",
            "patch": "@@ -16,7 +16,6 @@\n import copy\n import unittest\n \n-from packaging import version\n from parameterized import parameterized\n \n from transformers import set_seed\n@@ -35,7 +34,6 @@\n     import torch\n \n     from transformers import (\n-        AutoConfig,\n         AutoModelForCausalLM,\n         AutoTokenizer,\n         DynamicCache,\n@@ -44,7 +42,9 @@\n         LlamaConfig,\n         SinkCache,\n         StaticCache,\n+        convert_and_export_with_cache,\n     )\n+    from transformers.pytorch_utils import is_torch_greater_or_equal_than_2_3\n \n \n @require_torch\n@@ -175,61 +175,54 @@ def test_static_cache_exportability(self):\n         \"\"\"\n         Tests that static cache works with `torch.export()`\n         \"\"\"\n-        import torch\n-\n-        if version.parse(torch.__version__) < version.parse(\"2.3\"):\n+        if not is_torch_greater_or_equal_than_2_3:\n             self.skipTest(reason=\"This test requires torch >= 2.3 to run.\")\n \n+        set_seed(0)\n         device = \"cpu\"\n         dtype = torch.float32\n+        cache_implementation = \"static\"\n+        attn_implementation = \"sdpa\"  # Export and ExecuTorch only works for SdpaAttention\n         batch_size = 1\n-\n-        config = AutoConfig.from_pretrained(\n+        max_cache_len = 1234\n+        model = AutoModelForCausalLM.from_pretrained(\n             \"google/gemma-2b\",\n+            device_map=device,\n             torch_dtype=dtype,\n-            use_cache=True,\n+            attn_implementation=attn_implementation,\n+            generation_config=GenerationConfig(\n+                use_cache=True,\n+                cache_implementation=cache_implementation,\n+                max_length=max_cache_len,\n+                cache_config={\n+                    \"batch_size\": batch_size,\n+                    \"max_cache_len\": max_cache_len,\n+                },\n+            ),\n         )\n-        m = AutoModelForCausalLM.from_pretrained(\n-            \"google/gemma-2b\",\n-            config=config,\n-            torch_dtype=dtype,\n-            attn_implementation=\"sdpa\",  # Export and ExecuTorch only works for SdpaAttention\n-        ).to(device)\n-        tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\n-        inputs = tokenizer([\"The best color is\"], return_tensors=\"pt\").to(device)[\"input_ids\"]\n-\n-        class ExportatibleModelWithStaticCache(torch.nn.Module):\n-            def __init__(self, config, model):\n-                super().__init__()\n-                self.config = config\n-                self.model = model\n-                self.static_cache = StaticCache(\n-                    config=config, batch_size=batch_size, max_cache_len=config.max_length, device=device\n-                )\n-\n-            def forward(self, tokens: torch.Tensor, input_pos: torch.Tensor):\n-                outs = self.model(\n-                    input_ids=tokens,\n-                    attention_mask=None,\n-                    position_ids=input_pos.unsqueeze(0),\n-                    cache_position=input_pos,\n-                    past_key_values=self.static_cache,\n-                    use_cache=True,\n-                )\n-                return outs.logits\n-\n-        set_seed(0)\n-        with torch.no_grad():\n-            import torch.export._trace\n-            from torch.export import ExportedProgram\n-\n-            model = ExportatibleModelWithStaticCache(config, m)\n-            # Due to issue https://github.com/pytorch/pytorch/issues/128394, we need to switch to use an internal\n-            # export API and pre_dispatch=False. Switch to use the public API once the issue is included in 2.4.1+ release.\n-            exported_program = torch.export._trace._export(\n-                model, args=(inputs,), kwargs={\"input_pos\": torch.arange(1)}, pre_dispatch=False, strict=True\n-            )\n-            self.assertTrue(isinstance(exported_program, ExportedProgram))\n+        # Check if cache config is passed through correctly\n+        self.assertEqual(model.generation_config.use_cache, True)\n+        self.assertEqual(model.generation_config.cache_implementation, cache_implementation)\n+        self.assertEqual(model.generation_config.max_length, max_cache_len)\n+        self.assertTrue(model.generation_config.cache_config is not None)\n+        self.assertEqual(model.generation_config.cache_config.batch_size, batch_size)\n+        self.assertEqual(model.generation_config.cache_config.max_cache_len, max_cache_len)\n+\n+        exported_program = convert_and_export_with_cache(model)\n+\n+        # Check if the exported model is configured with the `StaticCache` correctly\n+        n_static_key_caches = n_static_value_caches = 0\n+        for buffer_name, buffer in exported_program.named_buffers():\n+            if buffer_name.startswith(\"static_cache.key_cache\"):\n+                self.assertTrue(buffer.shape[0] == batch_size)\n+                self.assertTrue(buffer.shape[2] == max_cache_len)\n+                n_static_key_caches = n_static_key_caches + 1\n+            if buffer_name.startswith(\"static_cache.value_cache\"):\n+                self.assertTrue(buffer.shape[0] == batch_size)\n+                self.assertTrue(buffer.shape[2] == max_cache_len)\n+                n_static_value_caches = n_static_value_caches + 1\n+        self.assertEqual(n_static_key_caches, model.config.num_hidden_layers)\n+        self.assertEqual(n_static_value_caches, model.config.num_hidden_layers)\n \n \n @require_torch_gpu"
        }
    ],
    "stats": {
        "total": 377,
        "additions": 326,
        "deletions": 51
    }
}