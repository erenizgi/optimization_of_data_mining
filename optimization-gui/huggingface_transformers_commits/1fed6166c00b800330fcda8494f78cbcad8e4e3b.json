{
    "author": "henrikm11",
    "message": "added fast image processor for ZoeDepth and expanded tests accordingly (#38515)\n\n* added fast image processor for ZoeDepth and expanded tests accordingly\n\n* added fast image processor for ZoeDepth and expanded tests accordingly, hopefully fixed repo consistency issue too now\n\n* final edits for zoedept fast image processor\n\n* final minor edit for zoedepth fast imate procesor",
    "sha": "1fed6166c00b800330fcda8494f78cbcad8e4e3b",
    "files": [
        {
            "sha": "d392b34abba84d0679ef5fb6ceba07448f00880c",
            "filename": "docs/source/en/model_doc/zoedepth.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fed6166c00b800330fcda8494f78cbcad8e4e3b/docs%2Fsource%2Fen%2Fmodel_doc%2Fzoedepth.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fed6166c00b800330fcda8494f78cbcad8e4e3b/docs%2Fsource%2Fen%2Fmodel_doc%2Fzoedepth.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fzoedepth.md?ref=1fed6166c00b800330fcda8494f78cbcad8e4e3b",
            "patch": "@@ -119,6 +119,11 @@ Image.fromarray(depth.astype(\"uint8\"))\n [[autodoc]] ZoeDepthImageProcessor\n     - preprocess\n \n+## ZoeDepthImageProcessorFast\n+\n+[[autodoc]] ZoeDepthImageProcessorFast\n+    - preprocess\n+\n ## ZoeDepthForDepthEstimation\n \n [[autodoc]] ZoeDepthForDepthEstimation"
        },
        {
            "sha": "f992369f27c16791939e71d9d03d383239699430",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fed6166c00b800330fcda8494f78cbcad8e4e3b/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fed6166c00b800330fcda8494f78cbcad8e4e3b/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=1fed6166c00b800330fcda8494f78cbcad8e4e3b",
            "patch": "@@ -170,7 +170,7 @@\n             (\"vitmatte\", (\"VitMatteImageProcessor\", \"VitMatteImageProcessorFast\")),\n             (\"xclip\", (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")),\n             (\"yolos\", (\"YolosImageProcessor\", \"YolosImageProcessorFast\")),\n-            (\"zoedepth\", (\"ZoeDepthImageProcessor\",)),\n+            (\"zoedepth\", (\"ZoeDepthImageProcessor\", \"ZoeDepthImageProcessorFast\")),\n         ]\n     )\n "
        },
        {
            "sha": "abc436fa801f8d7f8cf0e6f44504b33f528131ec",
            "filename": "src/transformers/models/zoedepth/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fed6166c00b800330fcda8494f78cbcad8e4e3b/src%2Ftransformers%2Fmodels%2Fzoedepth%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fed6166c00b800330fcda8494f78cbcad8e4e3b/src%2Ftransformers%2Fmodels%2Fzoedepth%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzoedepth%2F__init__.py?ref=1fed6166c00b800330fcda8494f78cbcad8e4e3b",
            "patch": "@@ -20,6 +20,7 @@\n if TYPE_CHECKING:\n     from .configuration_zoedepth import *\n     from .image_processing_zoedepth import *\n+    from .image_processing_zoedepth_fast import *\n     from .modeling_zoedepth import *\n else:\n     import sys"
        },
        {
            "sha": "abc72cd8cd1065fac202c39c715ba5b28800249e",
            "filename": "src/transformers/models/zoedepth/image_processing_zoedepth_fast.py",
            "status": "added",
            "additions": 328,
            "deletions": 0,
            "changes": 328,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fed6166c00b800330fcda8494f78cbcad8e4e3b/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fimage_processing_zoedepth_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fed6166c00b800330fcda8494f78cbcad8e4e3b/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fimage_processing_zoedepth_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fimage_processing_zoedepth_fast.py?ref=1fed6166c00b800330fcda8494f78cbcad8e4e3b",
            "patch": "@@ -0,0 +1,328 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Fast Image processor class for ZoeDepth.\"\"\"\n+\n+from typing import (\n+    Dict,\n+    List,\n+    Optional,\n+    Tuple,\n+    Union,\n+)\n+\n+import numpy as np\n+\n+from ...image_processing_utils import (\n+    BatchFeature,\n+)\n+from ...image_processing_utils_fast import (\n+    BaseImageProcessorFast,\n+    DefaultFastImageProcessorKwargs,\n+    group_images_by_shape,\n+    reorder_images,\n+)\n+from ...image_utils import (\n+    IMAGENET_STANDARD_MEAN,\n+    IMAGENET_STANDARD_STD,\n+    ChannelDimension,\n+    ImageInput,\n+    PILImageResampling,\n+    SizeDict,\n+    get_image_size,\n+)\n+from ...processing_utils import Unpack\n+from ...utils import (\n+    TensorType,\n+    auto_docstring,\n+    is_torch_available,\n+    is_torchvision_available,\n+    is_torchvision_v2_available,\n+    logging,\n+    requires_backends,\n+)\n+from .image_processing_zoedepth import get_resize_output_image_size\n+from .modeling_zoedepth import ZoeDepthDepthEstimatorOutput\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+if is_torchvision_available():\n+    if is_torchvision_v2_available():\n+        from torchvision.transforms.v2 import functional as F\n+    else:\n+        from torchvision.transforms import functional as F\n+\n+    from torchvision.transforms import InterpolationMode\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class ZoeDepthFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n+    \"\"\"\n+    do_pad (`bool`, *optional*, defaults to `True`):\n+        Whether to apply pad the input.\n+    keep_aspect_ratio (`bool`, *optional*, defaults to `True`):\n+        If `True`, the image is resized by choosing the smaller of the height and width scaling factors and using it\n+        for both dimensions. This ensures that the image is scaled down as little as possible while still fitting\n+        within the desired output size. In case `ensure_multiple_of` is also set, the image is further resized to a\n+        size that is a multiple of this value by flooring the height and width to the nearest multiple of this value.\n+        Can be overridden by `keep_aspect_ratio` in `preprocess`.\n+    ensure_multiple_of (`int`, *optional*, defaults to 32):\n+        If `do_resize` is `True`, the image is resized to a size that is a multiple of this value. Works by flooring\n+        the height and width to the nearest multiple of this value.\n+        Works both with and without `keep_aspect_ratio` being set to `True`.\n+        Can be overridden by `ensure_multiple_of` in `preprocess`.\n+    \"\"\"\n+\n+    do_pad: Optional[bool]\n+    keep_aspect_ratio: Optional[bool]\n+    ensure_multiple_of: Optional[int]\n+\n+\n+@auto_docstring\n+class ZoeDepthImageProcessorFast(BaseImageProcessorFast):\n+    do_pad = True\n+    do_rescale = True\n+    do_normalize = True\n+    image_mean = IMAGENET_STANDARD_MEAN\n+    image_std = IMAGENET_STANDARD_STD\n+    do_resize = True\n+    size = {\"height\": 384, \"width\": 512}\n+    resample = PILImageResampling.BILINEAR\n+    keep_aspect_ratio = True\n+    ensure_multiple_of = 1 / 32\n+    valid_kwargs = ZoeDepthFastImageProcessorKwargs\n+\n+    def __init__(self, **kwargs: Unpack[ZoeDepthFastImageProcessorKwargs]) -> None:\n+        super().__init__(**kwargs)\n+\n+    @auto_docstring\n+    def preprocess(\n+        self,\n+        images: ImageInput,\n+        **kwargs: Unpack[ZoeDepthFastImageProcessorKwargs],\n+    ) -> BatchFeature:\n+        return super().preprocess(images, **kwargs)\n+\n+    def resize(\n+        self,\n+        images: \"torch.Tensor\",\n+        size: SizeDict,\n+        keep_aspect_ratio: bool = False,\n+        ensure_multiple_of: int = 1,\n+        interpolation: Optional[\"F.InterpolationMode\"] = None,\n+    ) -> \"torch.Tensor\":\n+        \"\"\"\n+        Resize an image or batchd images to target size `(size[\"height\"], size[\"width\"])`. If `keep_aspect_ratio` is `True`, the image\n+        is resized to the largest possible size such that the aspect ratio is preserved. If `ensure_multiple_of` is\n+        set, the image is resized to a size that is a multiple of this value.\n+\n+        Args:\n+            images (`torch.Tensor`):\n+                Images to resize.\n+            size (`Dict[str, int]`):\n+                Target size of the output image.\n+            keep_aspect_ratio (`bool`, *optional*, defaults to `False`):\n+                If `True`, the image is resized to the largest possible size such that the aspect ratio is preserved.\n+            ensure_multiple_of (`int`, *optional*, defaults to 1):\n+                The image is resized to a size that is a multiple of this value.\n+            interpolation (`F.InterpolationMode`, *optional*, defaults to `InterpolationMode.BILINEAR`):\n+                Defines the resampling filter to use if resizing the image. Otherwise, the image is resized to size\n+                specified in `size`.\n+        \"\"\"\n+        if not size.height or not size.width:\n+            raise ValueError(f\"The size dictionary must contain the keys 'height' and 'width'. Got {size}\")\n+        output_size = get_resize_output_image_size(\n+            images,\n+            output_size=(size.height, size.width),\n+            keep_aspect_ratio=keep_aspect_ratio,\n+            multiple=ensure_multiple_of,\n+            input_data_format=ChannelDimension.FIRST,\n+        )\n+        height, width = output_size\n+\n+        resized_images = torch.nn.functional.interpolate(\n+            images, (int(height), int(width)), mode=interpolation.value, align_corners=True\n+        )\n+\n+        return resized_images\n+\n+    def _pad_images(\n+        self,\n+        images: \"torch.Tensor\",\n+    ):\n+        \"\"\"\n+        Args:\n+            image (`torch.Tensor`):\n+                Image to pad.\n+        \"\"\"\n+        height, width = get_image_size(images, channel_dim=ChannelDimension.FIRST)\n+\n+        pad_height = int(np.sqrt(height / 2) * 3)\n+        pad_width = int(np.sqrt(width / 2) * 3)\n+\n+        return F.pad(images, padding=(pad_width, pad_height), padding_mode=\"reflect\")\n+\n+    def _preprocess(\n+        self,\n+        images: list[\"torch.Tensor\"],\n+        do_resize: bool,\n+        size: SizeDict,\n+        keep_aspect_ratio: Optional[bool],\n+        ensure_multiple_of: Optional[int],\n+        interpolation: Optional[\"F.InterpolationMode\"],\n+        do_pad: bool,\n+        do_rescale: bool,\n+        rescale_factor: Optional[float],\n+        do_normalize: bool,\n+        image_mean: Optional[Union[float, List[float]]],\n+        image_std: Optional[Union[float, List[float]]],\n+        return_tensors: Optional[Union[str, TensorType]] = None,\n+        **kwargs,\n+    ) -> BatchFeature:\n+        # Group images by size for batched resizing\n+        grouped_images, grouped_images_index = group_images_by_shape(images)\n+        resized_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            if do_rescale:\n+                stacked_images = self.rescale(stacked_images, rescale_factor)\n+            if do_pad:\n+                stacked_images = self._pad_images(images=stacked_images)\n+            if do_resize:\n+                stacked_images = self.resize(\n+                    stacked_images, size, keep_aspect_ratio, ensure_multiple_of, interpolation\n+                )\n+            if do_normalize:\n+                stacked_images = self.normalize(stacked_images, image_mean, image_std)\n+            resized_images_grouped[shape] = stacked_images\n+        resized_images = reorder_images(resized_images_grouped, grouped_images_index)\n+\n+        processed_images = torch.stack(resized_images, dim=0) if return_tensors else resized_images\n+\n+        return BatchFeature(data={\"pixel_values\": processed_images}, tensor_type=return_tensors)\n+\n+    def post_process_depth_estimation(\n+        self,\n+        outputs: \"ZoeDepthDepthEstimatorOutput\",\n+        source_sizes: Optional[Union[TensorType, List[Tuple[int, int]], None]] = None,\n+        target_sizes: Optional[Union[TensorType, List[Tuple[int, int]], None]] = None,\n+        outputs_flipped: Optional[Union[\"ZoeDepthDepthEstimatorOutput\", None]] = None,\n+        do_remove_padding: Optional[Union[bool, None]] = None,\n+    ) -> List[Dict[str, TensorType]]:\n+        \"\"\"\n+        Converts the raw output of [`ZoeDepthDepthEstimatorOutput`] into final depth predictions and depth PIL images.\n+        Only supports PyTorch.\n+\n+        Args:\n+            outputs ([`ZoeDepthDepthEstimatorOutput`]):\n+                Raw outputs of the model.\n+            source_sizes (`TensorType` or `List[Tuple[int, int]]`, *optional*):\n+                Tensor of shape `(batch_size, 2)` or list of tuples (`Tuple[int, int]`) containing the source size\n+                (height, width) of each image in the batch before preprocessing. This argument should be dealt as\n+                \"required\" unless the user passes `do_remove_padding=False` as input to this function.\n+            target_sizes (`TensorType` or `List[Tuple[int, int]]`, *optional*):\n+                Tensor of shape `(batch_size, 2)` or list of tuples (`Tuple[int, int]`) containing the target size\n+                (height, width) of each image in the batch. If left to None, predictions will not be resized.\n+            outputs_flipped ([`ZoeDepthDepthEstimatorOutput`], *optional*):\n+                Raw outputs of the model from flipped input (averaged out in the end).\n+            do_remove_padding (`bool`, *optional*):\n+                By default ZoeDepth adds padding equal to `int(âˆš(height / 2) * 3)` (and similarly for width) to fix the\n+                boundary artifacts in the output depth map, so we need remove this padding during post_processing. The\n+                parameter exists here in case the user changed the image preprocessing to not include padding.\n+\n+        Returns:\n+            `List[Dict[str, TensorType]]`: A list of dictionaries of tensors representing the processed depth\n+            predictions.\n+        \"\"\"\n+        requires_backends(self, \"torch\")\n+\n+        predicted_depth = outputs.predicted_depth\n+\n+        if (outputs_flipped is not None) and (predicted_depth.shape != outputs_flipped.predicted_depth.shape):\n+            raise ValueError(\"Make sure that `outputs` and `outputs_flipped` have the same shape\")\n+\n+        if (target_sizes is not None) and (len(predicted_depth) != len(target_sizes)):\n+            raise ValueError(\n+                \"Make sure that you pass in as many target sizes as the batch dimension of the predicted depth\"\n+            )\n+\n+        if do_remove_padding is None:\n+            do_remove_padding = self.do_pad\n+\n+        if source_sizes is None and do_remove_padding:\n+            raise ValueError(\n+                \"Either `source_sizes` should be passed in, or `do_remove_padding` should be set to False\"\n+            )\n+\n+        if (source_sizes is not None) and (len(predicted_depth) != len(source_sizes)):\n+            raise ValueError(\n+                \"Make sure that you pass in as many source image sizes as the batch dimension of the logits\"\n+            )\n+\n+        if outputs_flipped is not None:\n+            predicted_depth = (predicted_depth + torch.flip(outputs_flipped.predicted_depth, dims=[-1])) / 2\n+\n+        predicted_depth = predicted_depth.unsqueeze(1)\n+\n+        # Zoe Depth model adds padding around the images to fix the boundary artifacts in the output depth map\n+        # The padding length is `int(np.sqrt(img_h/2) * fh)` for the height and similar for the width\n+        # fh (and fw respectively) are equal to '3' by default\n+        # Check [here](https://github.com/isl-org/ZoeDepth/blob/edb6daf45458569e24f50250ef1ed08c015f17a7/zoedepth/models/depth_model.py#L57)\n+        # for the original implementation.\n+        # In this section, we remove this padding to get the final depth image and depth prediction\n+        padding_factor_h = padding_factor_w = 3\n+\n+        results = []\n+        target_sizes = [None] * len(predicted_depth) if target_sizes is None else target_sizes\n+        source_sizes = [None] * len(predicted_depth) if source_sizes is None else source_sizes\n+        for depth, target_size, source_size in zip(predicted_depth, target_sizes, source_sizes):\n+            # depth.shape = [1, H, W]\n+            if source_size is not None:\n+                pad_h = pad_w = 0\n+\n+                if do_remove_padding:\n+                    pad_h = int(np.sqrt(source_size[0] / 2) * padding_factor_h)\n+                    pad_w = int(np.sqrt(source_size[1] / 2) * padding_factor_w)\n+\n+                depth = F.resize(\n+                    depth,\n+                    size=[source_size[0] + 2 * pad_h, source_size[1] + 2 * pad_w],\n+                    interpolation=InterpolationMode.BICUBIC,\n+                    antialias=False,\n+                )\n+\n+                if pad_h > 0:\n+                    depth = depth[:, pad_h:-pad_h, :]\n+                if pad_w > 0:\n+                    depth = depth[:, :, pad_w:-pad_w]\n+\n+            if target_size is not None:\n+                target_size = [target_size[0], target_size[1]]\n+                depth = F.resize(\n+                    depth,\n+                    size=target_size,\n+                    interpolation=InterpolationMode.BICUBIC,\n+                    antialias=False,\n+                )\n+            depth = depth.squeeze(0)\n+            # depth.shape = [H, W]\n+            results.append({\"predicted_depth\": depth})\n+\n+        return results\n+\n+\n+__all__ = [\"ZoeDepthImageProcessorFast\"]"
        },
        {
            "sha": "5e5ec4c0425e23f6a73177a8a902cf2ac36fbab7",
            "filename": "tests/models/zoedepth/test_image_processing_zoedepth.py",
            "status": "modified",
            "additions": 101,
            "deletions": 36,
            "changes": 137,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fed6166c00b800330fcda8494f78cbcad8e4e3b/tests%2Fmodels%2Fzoedepth%2Ftest_image_processing_zoedepth.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fed6166c00b800330fcda8494f78cbcad8e4e3b/tests%2Fmodels%2Fzoedepth%2Ftest_image_processing_zoedepth.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fzoedepth%2Ftest_image_processing_zoedepth.py?ref=1fed6166c00b800330fcda8494f78cbcad8e4e3b",
            "patch": "@@ -14,18 +14,30 @@\n \n \n import unittest\n+from dataclasses import dataclass\n \n import numpy as np\n \n-from transformers.file_utils import is_vision_available\n from transformers.testing_utils import require_torch, require_vision\n+from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n \n \n+if is_torch_available():\n+    import torch\n+\n if is_vision_available():\n     from transformers import ZoeDepthImageProcessor\n \n+    if is_torchvision_available():\n+        from transformers import ZoeDepthImageProcessorFast\n+\n+\n+@dataclass\n+class ZoeDepthDepthOutputProxy:\n+    predicted_depth: torch.FloatTensor = None\n+\n \n class ZoeDepthImageProcessingTester:\n     def __init__(\n@@ -43,7 +55,7 @@ def __init__(\n         do_normalize=True,\n         image_mean=[0.5, 0.5, 0.5],\n         image_std=[0.5, 0.5, 0.5],\n-        do_pad=False,\n+        do_pad=True,\n     ):\n         size = size if size is not None else {\"height\": 18, \"width\": 18}\n         self.parent = parent\n@@ -87,11 +99,25 @@ def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=F\n             torchify=torchify,\n         )\n \n+    def prepare_depth_outputs(self):\n+        depth_tensors = prepare_image_inputs(\n+            batch_size=self.batch_size,\n+            num_channels=1,\n+            min_resolution=self.min_resolution,\n+            max_resolution=self.max_resolution,\n+            equal_resolution=True,\n+            torchify=True,\n+        )\n+        depth_tensors = [depth_tensor.squeeze(0) for depth_tensor in depth_tensors]\n+        stacked_depth_tensors = torch.stack(depth_tensors, dim=0)\n+        return ZoeDepthDepthOutputProxy(predicted_depth=stacked_depth_tensors)\n+\n \n @require_torch\n @require_vision\n class ZoeDepthImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n     image_processing_class = ZoeDepthImageProcessor if is_vision_available() else None\n+    fast_image_processing_class = ZoeDepthImageProcessorFast if is_torchvision_available() else None\n \n     def setUp(self):\n         super().setUp()\n@@ -115,72 +141,111 @@ def test_image_processor_properties(self):\n         self.assertTrue(hasattr(image_processing, \"do_pad\"))\n \n     def test_image_processor_from_dict_with_kwargs(self):\n-        image_processor = self.image_processing_class.from_dict(self.image_processor_dict)\n-        self.assertEqual(image_processor.size, {\"height\": 18, \"width\": 18})\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class(**self.image_processor_dict)\n+            self.assertEqual(image_processor.size, {\"height\": 18, \"width\": 18})\n \n-        image_processor = self.image_processing_class.from_dict(self.image_processor_dict, size=42)\n-        self.assertEqual(image_processor.size, {\"height\": 42, \"width\": 42})\n+        for image_processing_class in self.image_processor_list:\n+            modified_dict = self.image_processor_dict\n+            modified_dict[\"size\"] = 42\n+            image_processor = image_processing_class(**modified_dict)\n+            self.assertEqual(image_processor.size, {\"height\": 42, \"width\": 42})\n \n     def test_ensure_multiple_of(self):\n         # Test variable by turning off all other variables which affect the size, size which is not multiple of 32\n         image = np.zeros((489, 640, 3))\n \n         size = {\"height\": 380, \"width\": 513}\n         multiple = 32\n-        image_processor = ZoeDepthImageProcessor(\n-            do_pad=False, ensure_multiple_of=multiple, size=size, keep_aspect_ratio=False\n-        )\n-        pixel_values = image_processor(image, return_tensors=\"pt\").pixel_values\n+        for image_processor_class in self.image_processor_list:\n+            image_processor = image_processor_class(\n+                do_pad=False, ensure_multiple_of=multiple, size=size, keep_aspect_ratio=False\n+            )\n+            pixel_values = image_processor(image, return_tensors=\"pt\").pixel_values\n \n-        self.assertEqual(list(pixel_values.shape), [1, 3, 384, 512])\n-        self.assertTrue(pixel_values.shape[2] % multiple == 0)\n-        self.assertTrue(pixel_values.shape[3] % multiple == 0)\n+            self.assertEqual(list(pixel_values.shape), [1, 3, 384, 512])\n+            self.assertTrue(pixel_values.shape[2] % multiple == 0)\n+            self.assertTrue(pixel_values.shape[3] % multiple == 0)\n \n         # Test variable by turning off all other variables which affect the size, size which is already multiple of 32\n         image = np.zeros((511, 511, 3))\n \n         height, width = 512, 512\n         size = {\"height\": height, \"width\": width}\n         multiple = 32\n-        image_processor = ZoeDepthImageProcessor(\n-            do_pad=False, ensure_multiple_of=multiple, size=size, keep_aspect_ratio=False\n-        )\n-        pixel_values = image_processor(image, return_tensors=\"pt\").pixel_values\n+        for image_processor_class in self.image_processor_list:\n+            image_processor = image_processor_class(\n+                do_pad=False, ensure_multiple_of=multiple, size=size, keep_aspect_ratio=False\n+            )\n+            pixel_values = image_processor(image, return_tensors=\"pt\").pixel_values\n \n-        self.assertEqual(list(pixel_values.shape), [1, 3, height, width])\n-        self.assertTrue(pixel_values.shape[2] % multiple == 0)\n-        self.assertTrue(pixel_values.shape[3] % multiple == 0)\n+            self.assertEqual(list(pixel_values.shape), [1, 3, height, width])\n+            self.assertTrue(pixel_values.shape[2] % multiple == 0)\n+            self.assertTrue(pixel_values.shape[3] % multiple == 0)\n \n     def test_keep_aspect_ratio(self):\n         # Test `keep_aspect_ratio=True` by turning off all other variables which affect the size\n         height, width = 489, 640\n         image = np.zeros((height, width, 3))\n \n         size = {\"height\": 512, \"width\": 512}\n-        image_processor = ZoeDepthImageProcessor(do_pad=False, keep_aspect_ratio=True, size=size, ensure_multiple_of=1)\n-        pixel_values = image_processor(image, return_tensors=\"pt\").pixel_values\n+        for image_processor_class in self.image_processor_list:\n+            image_processor = image_processor_class(\n+                do_pad=False, keep_aspect_ratio=True, size=size, ensure_multiple_of=1\n+            )\n+            pixel_values = image_processor(image, return_tensors=\"pt\").pixel_values\n \n-        # As can be seen, the image is resized to the maximum size that fits in the specified size\n-        self.assertEqual(list(pixel_values.shape), [1, 3, 512, 670])\n+            # As can be seen, the image is resized to the maximum size that fits in the specified size\n+            self.assertEqual(list(pixel_values.shape), [1, 3, 512, 670])\n \n         # Test `keep_aspect_ratio=False` by turning off all other variables which affect the size\n-        image_processor = ZoeDepthImageProcessor(\n-            do_pad=False, keep_aspect_ratio=False, size=size, ensure_multiple_of=1\n-        )\n-        pixel_values = image_processor(image, return_tensors=\"pt\").pixel_values\n+        for image_processor_class in self.image_processor_list:\n+            image_processor = image_processor_class(\n+                do_pad=False, keep_aspect_ratio=False, size=size, ensure_multiple_of=1\n+            )\n+            pixel_values = image_processor(image, return_tensors=\"pt\").pixel_values\n \n-        # As can be seen, the size is respected\n-        self.assertEqual(list(pixel_values.shape), [1, 3, size[\"height\"], size[\"width\"]])\n+            # As can be seen, the size is respected\n+            self.assertEqual(list(pixel_values.shape), [1, 3, size[\"height\"], size[\"width\"]])\n \n         # Test `keep_aspect_ratio=True` with `ensure_multiple_of` set\n         image = np.zeros((489, 640, 3))\n \n         size = {\"height\": 511, \"width\": 511}\n         multiple = 32\n-        image_processor = ZoeDepthImageProcessor(size=size, keep_aspect_ratio=True, ensure_multiple_of=multiple)\n-\n-        pixel_values = image_processor(image, return_tensors=\"pt\").pixel_values\n+        for image_processor_class in self.image_processor_list:\n+            image_processor = image_processor_class(size=size, keep_aspect_ratio=True, ensure_multiple_of=multiple)\n+\n+            pixel_values = image_processor(image, return_tensors=\"pt\").pixel_values\n+\n+            self.assertEqual(list(pixel_values.shape), [1, 3, 512, 672])\n+            self.assertTrue(pixel_values.shape[2] % multiple == 0)\n+            self.assertTrue(pixel_values.shape[3] % multiple == 0)\n+\n+    # extend this test to check if removal of padding works fine!\n+    def test_post_processing_equivalence(self):\n+        outputs = self.image_processor_tester.prepare_depth_outputs()\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+\n+        source_sizes = [outputs.predicted_depth.shape[1:]] * self.image_processor_tester.batch_size\n+        target_sizes = [\n+            torch.Size([outputs.predicted_depth.shape[1] // 2, *(outputs.predicted_depth.shape[2:])])\n+        ] * self.image_processor_tester.batch_size\n+\n+        processed_fast = image_processor_fast.post_process_depth_estimation(\n+            outputs,\n+            source_sizes=source_sizes,\n+            target_sizes=target_sizes,\n+        )\n+        processed_slow = image_processor_slow.post_process_depth_estimation(\n+            outputs,\n+            source_sizes=source_sizes,\n+            target_sizes=target_sizes,\n+        )\n+        for pred_fast, pred_slow in zip(processed_fast, processed_slow):\n+            depth_fast = pred_fast[\"predicted_depth\"]\n+            depth_slow = pred_slow[\"predicted_depth\"]\n \n-        self.assertEqual(list(pixel_values.shape), [1, 3, 512, 672])\n-        self.assertTrue(pixel_values.shape[2] % multiple == 0)\n-        self.assertTrue(pixel_values.shape[3] % multiple == 0)\n+            torch.testing.assert_close(depth_fast, depth_slow, atol=1e-1, rtol=1e-3)\n+            self.assertLessEqual(torch.mean(torch.abs(depth_fast.float() - depth_slow.float())).item(), 5e-3)"
        }
    ],
    "stats": {
        "total": 473,
        "additions": 436,
        "deletions": 37
    }
}