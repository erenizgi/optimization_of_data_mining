{
    "author": "HRezaei",
    "message": "Fix bugs in pytorch example run_clm when streaming is enabled (#39286)",
    "sha": "d8e05951b8efd4880acca9a3f291e8b65841a86d",
    "files": [
        {
            "sha": "dad24cd7ef838336e8447359e78bcb2a68aefa49",
            "filename": "examples/pytorch/language-modeling/run_clm.py",
            "status": "modified",
            "additions": 117,
            "deletions": 41,
            "changes": 158,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8e05951b8efd4880acca9a3f291e8b65841a86d/examples%2Fpytorch%2Flanguage-modeling%2Frun_clm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8e05951b8efd4880acca9a3f291e8b65841a86d/examples%2Fpytorch%2Flanguage-modeling%2Frun_clm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Flanguage-modeling%2Frun_clm.py?ref=d8e05951b8efd4880acca9a3f291e8b65841a86d",
            "patch": "@@ -31,7 +31,7 @@\n import datasets\n import evaluate\n import torch\n-from datasets import load_dataset\n+from datasets import IterableDataset, IterableDatasetDict, load_dataset\n \n import transformers\n from transformers import (\n@@ -225,6 +225,45 @@ def __post_init__(self):\n                 assert extension in [\"csv\", \"json\", \"txt\"], \"`validation_file` should be a csv, a json or a txt file.\"\n \n \n+def split_streaming_dataset(\n+    full_streaming_dataset,\n+    validation_percentage: int = 5,\n+) -> IterableDatasetDict:\n+    \"\"\"\n+    Splits a streaming dataset into\n+    training and validation IterableDatasets, and supports methods like .map(), .filter(),\n+    .take() and properties like .features on the resulting streams.\n+\n+    Args:\n+        full_streaming_dataset (Dataset): The name of the dataset to load (e.g., \"HuggingFaceFW/fineweb\").\n+        validation_percentage (int): The proportion of the dataset to be used for validation split.\n+\n+    Returns:\n+        IterableDatasetDict: An IterableDatasetDict containing two IterableDataset objects: (train_stream, validation_stream).\n+    \"\"\"\n+    if not (0 < validation_percentage < 100):\n+        raise ValueError(\n+            f\"validation_percentage must be between 0 and 100 (exclusive). Passed: {validation_percentage}\"\n+        )\n+\n+    def split_generator(is_train: bool):\n+        for i, example in enumerate(full_streaming_dataset):\n+            if is_train:\n+                if i % 100 > validation_percentage:\n+                    yield example\n+            else:\n+                if i % 100 < validation_percentage:\n+                    yield example\n+\n+    features = full_streaming_dataset.features\n+    train_stream = IterableDataset.from_generator(split_generator, gen_kwargs={\"is_train\": True}, features=features)\n+    validation_stream = IterableDataset.from_generator(\n+        split_generator, gen_kwargs={\"is_train\": False}, features=features\n+    )\n+\n+    return IterableDatasetDict({\"train\": train_stream, \"validation\": validation_stream})\n+\n+\n def main():\n     # See all possible arguments in src/transformers/training_args.py\n     # or by passing the --help flag to this script.\n@@ -305,24 +344,36 @@ def main():\n             trust_remote_code=model_args.trust_remote_code,\n         )\n         if \"validation\" not in raw_datasets.keys():\n-            raw_datasets[\"validation\"] = load_dataset(\n-                data_args.dataset_name,\n-                data_args.dataset_config_name,\n-                split=f\"train[:{data_args.validation_split_percentage}%]\",\n-                cache_dir=model_args.cache_dir,\n-                token=model_args.token,\n-                streaming=data_args.streaming,\n-                trust_remote_code=model_args.trust_remote_code,\n-            )\n-            raw_datasets[\"train\"] = load_dataset(\n-                data_args.dataset_name,\n-                data_args.dataset_config_name,\n-                split=f\"train[{data_args.validation_split_percentage}%:]\",\n-                cache_dir=model_args.cache_dir,\n-                token=model_args.token,\n-                streaming=data_args.streaming,\n-                trust_remote_code=model_args.trust_remote_code,\n-            )\n+            if data_args.streaming:\n+                dataset_stream = load_dataset(\n+                    data_args.dataset_name,\n+                    data_args.dataset_config_name,\n+                    split=\"train\",\n+                    cache_dir=model_args.cache_dir,\n+                    token=model_args.token,\n+                    streaming=data_args.streaming,\n+                    trust_remote_code=model_args.trust_remote_code,\n+                )\n+                raw_datasets = split_streaming_dataset(dataset_stream, data_args.validation_split_percentage)\n+            else:\n+                raw_datasets[\"validation\"] = load_dataset(\n+                    data_args.dataset_name,\n+                    data_args.dataset_config_name,\n+                    split=f\"train[:{data_args.validation_split_percentage}%]\",\n+                    cache_dir=model_args.cache_dir,\n+                    token=model_args.token,\n+                    streaming=data_args.streaming,\n+                    trust_remote_code=model_args.trust_remote_code,\n+                )\n+                raw_datasets[\"train\"] = load_dataset(\n+                    data_args.dataset_name,\n+                    data_args.dataset_config_name,\n+                    split=f\"train[{data_args.validation_split_percentage}%:]\",\n+                    cache_dir=model_args.cache_dir,\n+                    token=model_args.token,\n+                    streaming=data_args.streaming,\n+                    trust_remote_code=model_args.trust_remote_code,\n+                )\n     else:\n         data_files = {}\n         dataset_args = {}\n@@ -347,22 +398,34 @@ def main():\n         )\n         # If no validation data is there, validation_split_percentage will be used to divide the dataset.\n         if \"validation\" not in raw_datasets.keys():\n-            raw_datasets[\"validation\"] = load_dataset(\n-                extension,\n-                data_files=data_files,\n-                split=f\"train[:{data_args.validation_split_percentage}%]\",\n-                cache_dir=model_args.cache_dir,\n-                token=model_args.token,\n-                **dataset_args,\n-            )\n-            raw_datasets[\"train\"] = load_dataset(\n-                extension,\n-                data_files=data_files,\n-                split=f\"train[{data_args.validation_split_percentage}%:]\",\n-                cache_dir=model_args.cache_dir,\n-                token=model_args.token,\n-                **dataset_args,\n-            )\n+            if data_args.streaming:\n+                dataset_stream = load_dataset(\n+                    extension,\n+                    data_files=data_files,\n+                    split=\"train\",\n+                    cache_dir=model_args.cache_dir,\n+                    token=model_args.token,\n+                    **dataset_args,\n+                )\n+                raw_datasets = split_streaming_dataset(dataset_stream, data_args.validation_split_percentage)\n+            else:\n+                raw_datasets[\"validation\"] = load_dataset(\n+                    extension,\n+                    data_files=data_files,\n+                    split=f\"train[:{data_args.validation_split_percentage}%]\",\n+                    cache_dir=model_args.cache_dir,\n+                    token=model_args.token,\n+                    **dataset_args,\n+                )\n+\n+                raw_datasets[\"train\"] = load_dataset(\n+                    extension,\n+                    data_files=data_files,\n+                    split=f\"train[{data_args.validation_split_percentage}%:]\",\n+                    cache_dir=model_args.cache_dir,\n+                    token=model_args.token,\n+                    **dataset_args,\n+                )\n \n     # See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at\n     # https://huggingface.co/docs/datasets/loading_datasets.\n@@ -541,16 +604,22 @@ def group_texts(examples):\n             raise ValueError(\"--do_train requires a train dataset\")\n         train_dataset = lm_datasets[\"train\"]\n         if data_args.max_train_samples is not None:\n-            max_train_samples = min(len(train_dataset), data_args.max_train_samples)\n-            train_dataset = train_dataset.select(range(max_train_samples))\n+            if data_args.streaming:\n+                train_dataset = train_dataset.take(data_args.max_train_samples)\n+            else:\n+                max_train_samples = min(len(train_dataset), data_args.max_train_samples)\n+                train_dataset = train_dataset.select(range(max_train_samples))\n \n     if training_args.do_eval:\n         if \"validation\" not in tokenized_datasets:\n             raise ValueError(\"--do_eval requires a validation dataset\")\n         eval_dataset = lm_datasets[\"validation\"]\n         if data_args.max_eval_samples is not None:\n-            max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)\n-            eval_dataset = eval_dataset.select(range(max_eval_samples))\n+            if data_args.streaming:\n+                eval_dataset = eval_dataset.take(data_args.max_eval_samples)\n+            else:\n+                max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)\n+                eval_dataset = eval_dataset.select(range(max_eval_samples))\n \n         def preprocess_logits_for_metrics(logits, labels):\n             if isinstance(logits, tuple):\n@@ -599,7 +668,10 @@ def compute_metrics(eval_preds):\n         max_train_samples = (\n             data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)\n         )\n-        metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n+        if data_args.streaming:\n+            metrics[\"train_samples\"] = max_train_samples\n+        else:\n+            metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n \n         trainer.log_metrics(\"train\", metrics)\n         trainer.save_metrics(\"train\", metrics)\n@@ -612,7 +684,11 @@ def compute_metrics(eval_preds):\n         metrics = trainer.evaluate()\n \n         max_eval_samples = data_args.max_eval_samples if data_args.max_eval_samples is not None else len(eval_dataset)\n-        metrics[\"eval_samples\"] = min(max_eval_samples, len(eval_dataset))\n+        if data_args.streaming:\n+            metrics[\"eval_samples\"] = max_eval_samples\n+        else:\n+            metrics[\"eval_samples\"] = min(max_eval_samples, len(eval_dataset))\n+\n         try:\n             perplexity = math.exp(metrics[\"eval_loss\"])\n         except OverflowError:"
        }
    ],
    "stats": {
        "total": 158,
        "additions": 117,
        "deletions": 41
    }
}