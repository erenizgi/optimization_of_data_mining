{
    "author": "qubvel",
    "message": "Introduce GradientCheckpointingLayer (#37223)\n\n* GradientCheckpointingLayer\n\n* trigger\n\n* Move GC layer to a separate file\n\n* Update import\n\n* Expose and document GC layer\n\n* Fix dummy\n\n* Apply to llama-based models\n\n* Update modulars\n\n* Update a few more models for consistency\n\n* Update glm4\n\n* Update Janus",
    "sha": "9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24",
    "files": [
        {
            "sha": "1c7d16ad06102fe9f89c930a0f3abd90038f9468",
            "filename": "docs/source/en/internal/modeling_utils.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24/docs%2Fsource%2Fen%2Finternal%2Fmodeling_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24/docs%2Fsource%2Fen%2Finternal%2Fmodeling_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Finternal%2Fmodeling_utils.md?ref=9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24",
            "patch": "@@ -20,6 +20,10 @@ This page lists all the custom layers used by the library, as well as the utilit\n \n Most of those are only useful if you are studying the code of the models in the library.\n \n+## Layers\n+\n+[[autodoc]] GradientCheckpointingLayer\n+\n ## Attention Functions\n \n [[autodoc]] AttentionInterface"
        },
        {
            "sha": "9d051dd2dbf56469ad5dd6ba6ba1750e1271e96a",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24",
            "patch": "@@ -438,6 +438,7 @@\n     ]\n \n     _import_structure[\"modeling_flash_attention_utils\"] = []\n+    _import_structure[\"modeling_layers\"] = [\"GradientCheckpointingLayer\"]\n     _import_structure[\"modeling_outputs\"] = []\n     _import_structure[\"modeling_rope_utils\"] = [\"ROPE_INIT_FUNCTIONS\", \"dynamic_rope_update\"]\n     _import_structure[\"modeling_utils\"] = [\"PreTrainedModel\", \"AttentionInterface\"]\n@@ -911,6 +912,7 @@\n         from .model_debugging_utils import (\n             model_addition_debugger_context,\n         )\n+        from .modeling_layers import GradientCheckpointingLayer\n         from .modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n         from .modeling_utils import AttentionInterface, PreTrainedModel\n "
        },
        {
            "sha": "57be2d8e0d7dc7af50f847f699bf80b707bbb98e",
            "filename": "src/transformers/modeling_layers.py",
            "status": "added",
            "additions": 48,
            "deletions": 0,
            "changes": 48,
            "blob_url": "https://github.com/huggingface/transformers/blob/9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24/src%2Ftransformers%2Fmodeling_layers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24/src%2Ftransformers%2Fmodeling_layers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_layers.py?ref=9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24",
            "patch": "@@ -0,0 +1,48 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from functools import partial\n+\n+import torch.nn as nn\n+\n+\n+class GradientCheckpointingLayer(nn.Module):\n+    \"\"\"Base class for layers with gradient checkpointing.\n+\n+    This class enables gradient checkpointing functionality for a layer. By default, gradient checkpointing is disabled\n+    (`gradient_checkpointing = False`). When `model.set_gradient_checkpointing()` is called, gradient checkpointing is\n+    enabled by setting `gradient_checkpointing = True` and assigning a checkpointing function to `_gradient_checkpointing_func`.\n+\n+    Important:\n+\n+        When using gradient checkpointing with `use_reentrant=True`, inputs that require gradients (e.g. hidden states)\n+        must be passed as positional arguments (`*args`) rather than keyword arguments to properly propagate gradients.\n+\n+        Example:\n+\n+            ```python\n+            >>> # Correct - hidden_states passed as positional arg\n+            >>> out = self.layer(hidden_states, attention_mask=attention_mask)\n+\n+            >>> # Incorrect - hidden_states passed as keyword arg\n+            >>> out = self.layer(hidden_states=hidden_states, attention_mask=attention_mask)\n+            ```\n+    \"\"\"\n+\n+    gradient_checkpointing = False\n+\n+    def __call__(self, *args, **kwargs):\n+        if self.gradient_checkpointing and self.training:\n+            return self._gradient_checkpointing_func(partial(super().__call__, **kwargs), *args)\n+        return super().__call__(*args, **kwargs)"
        },
        {
            "sha": "1b9892c94f85b4b1eed091f02fb9e299fb24e548",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 13,
            "deletions": 26,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24",
            "patch": "@@ -19,7 +19,6 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n from dataclasses import dataclass\n-from functools import partial\n from typing import Callable, List, Optional, Tuple, Union\n \n from ...activations import ACT2FN\n@@ -28,6 +27,7 @@\n from ...integrations import use_kernel_forward_from_hub\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast, ModelOutput\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n@@ -590,7 +590,7 @@ def forward(\n         return attn_output, attn_weights\n \n \n-class AriaTextDecoderLayer(nn.Module):\n+class AriaTextDecoderLayer(GradientCheckpointingLayer):\n     \"\"\"\n     Aria Text Decoder Layer.\n \n@@ -940,30 +940,17 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    partial(decoder_layer.__call__, **flash_attn_kwargs),\n-                    hidden_states,\n-                    causal_mask,\n-                    position_ids,\n-                    past_key_values,\n-                    output_attentions,\n-                    use_cache,\n-                    cache_position,\n-                    position_embeddings,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=causal_mask,\n-                    position_ids=position_ids,\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                    position_embeddings=position_embeddings,\n-                    **flash_attn_kwargs,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+                **flash_attn_kwargs,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "9d13b3048628bb8a559250298e524fa8aef65273",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 13,
            "deletions": 26,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24",
            "patch": "@@ -27,7 +27,6 @@\n # This file is based on the LLama model definition file in transformers\n \n \n-from functools import partial\n from typing import Callable, List, Optional, Tuple, Union\n \n import torch\n@@ -38,6 +37,7 @@\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n@@ -301,7 +301,7 @@ def forward(\n         return attn_output, attn_weights\n \n \n-class CohereDecoderLayer(nn.Module):\n+class CohereDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: CohereConfig, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n@@ -589,30 +589,17 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    partial(decoder_layer.__call__, **flash_attn_kwargs),\n-                    hidden_states,\n-                    causal_mask,\n-                    position_ids,\n-                    past_key_values,\n-                    output_attentions,\n-                    use_cache,\n-                    cache_position,\n-                    position_embeddings,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=causal_mask,\n-                    position_ids=position_ids,\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                    position_embeddings=position_embeddings,\n-                    **flash_attn_kwargs,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+                **flash_attn_kwargs,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "c4acb434948659746bb13dff6c32bcae1a78f3bf",
            "filename": "src/transformers/models/cohere/modular_cohere.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodular_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodular_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodular_cohere.py?ref=9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24",
            "patch": "@@ -30,6 +30,7 @@\n \n from ...cache_utils import Cache\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_rope_utils import dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n@@ -209,7 +210,7 @@ def forward(\n         return attn_output, attn_weights\n \n \n-class CohereDecoderLayer(nn.Module):\n+class CohereDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: CohereConfig, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size"
        },
        {
            "sha": "cc790124ccdeb715083a7377698d758e867f88b6",
            "filename": "src/transformers/models/cohere2/modeling_cohere2.py",
            "status": "modified",
            "additions": 12,
            "deletions": 24,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py?ref=9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24",
            "patch": "@@ -19,7 +19,6 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from functools import partial\n from typing import Callable, List, Optional, Tuple, Union\n \n import torch\n@@ -29,6 +28,7 @@\n from ...cache_utils import Cache, HybridCache, StaticCache\n from ...generation import GenerationMixin\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n@@ -290,7 +290,7 @@ def forward(self, x):\n         return down_proj\n \n \n-class Cohere2DecoderLayer(nn.Module):\n+class Cohere2DecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: Cohere2Config, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n@@ -612,28 +612,16 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    partial(decoder_layer.__call__, **flash_attn_kwargs),\n-                    hidden_states,\n-                    position_embeddings,\n-                    causal_mask,\n-                    past_key_values,\n-                    output_attentions,\n-                    use_cache,\n-                    cache_position,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    position_embeddings=position_embeddings,\n-                    attention_mask=causal_mask,\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                    **flash_attn_kwargs,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                position_embeddings=position_embeddings,\n+                attention_mask=causal_mask,\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                **flash_attn_kwargs,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "e811aabedbde8d43b65f879c66f0f7960714df95",
            "filename": "src/transformers/models/cohere2/modular_cohere2.py",
            "status": "modified",
            "additions": 10,
            "deletions": 23,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py?ref=9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24",
            "patch": "@@ -13,7 +13,6 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from functools import partial\n from typing import Callable, Optional, Tuple\n \n import torch\n@@ -526,28 +525,16 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    partial(decoder_layer.__call__, **flash_attn_kwargs),\n-                    hidden_states,\n-                    position_embeddings,\n-                    causal_mask,\n-                    past_key_values,\n-                    output_attentions,\n-                    use_cache,\n-                    cache_position,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    position_embeddings=position_embeddings,\n-                    attention_mask=causal_mask,\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                    **flash_attn_kwargs,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                position_embeddings=position_embeddings,\n+                attention_mask=causal_mask,\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                **flash_attn_kwargs,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "511c401f7e39a1b6ed44106245e79ad52e7cd8ea",
            "filename": "src/transformers/models/deepseek_v3/modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 13,
            "deletions": 26,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py?ref=9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24",
            "patch": "@@ -5,7 +5,6 @@\n #                          modular_deepseek_v3.py file directly. One of our CI enforces this.\n #                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n import math\n-from functools import partial\n from typing import Callable, Optional, Tuple, Union\n \n import torch\n@@ -18,6 +17,7 @@\n from ...integrations import use_kernel_forward_from_hub\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n@@ -454,7 +454,7 @@ def forward(\n         return attn_output, attn_weights\n \n \n-class DeepseekV3DecoderLayer(nn.Module):\n+class DeepseekV3DecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: DeepseekV3Config, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n@@ -734,30 +734,17 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    partial(decoder_layer.__call__, **flash_attn_kwargs),\n-                    hidden_states,\n-                    causal_mask,\n-                    position_ids,\n-                    past_key_values,\n-                    output_attentions,\n-                    use_cache,\n-                    cache_position,\n-                    position_embeddings,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=causal_mask,\n-                    position_ids=position_ids,\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                    position_embeddings=position_embeddings,\n-                    **flash_attn_kwargs,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+                **flash_attn_kwargs,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "b2d736cb4f99d912ea8493e35f886be6f4702a21",
            "filename": "src/transformers/models/diffllama/modeling_diffllama.py",
            "status": "modified",
            "additions": 13,
            "deletions": 26,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py?ref=9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24",
            "patch": "@@ -22,7 +22,6 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n import math\n-from functools import partial\n from typing import Optional, Tuple, Union\n \n import torch\n@@ -38,6 +37,7 @@\n     _flash_attention_forward,\n     flash_attn_supports_top_left_mask,\n )\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n     CausalLMOutputWithPast,\n@@ -526,7 +526,7 @@ def extra_repr(self):\n }\n \n \n-class DiffLlamaDecoderLayer(nn.Module):\n+class DiffLlamaDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: DiffLlamaConfig, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n@@ -837,30 +837,17 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    partial(decoder_layer.__call__, **flash_attn_kwargs),\n-                    hidden_states,\n-                    causal_mask,\n-                    position_ids,\n-                    past_key_values,\n-                    output_attentions,\n-                    use_cache,\n-                    cache_position,\n-                    position_embeddings,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=causal_mask,\n-                    position_ids=position_ids,\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                    position_embeddings=position_embeddings,\n-                    **flash_attn_kwargs,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+                **flash_attn_kwargs,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "375b1beb230ce89ef388f0f7b521bb1d28acec00",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 14,
            "deletions": 26,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24",
            "patch": "@@ -21,7 +21,7 @@\n # limitations under the License.\n \n import math\n-from functools import cached_property, partial\n+from functools import cached_property\n from typing import Callable, List, Optional, Tuple, Union\n \n import torch\n@@ -34,6 +34,7 @@\n from ...integrations import use_kernel_forward_from_hub\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n@@ -248,7 +249,7 @@ def forward(\n         return attn_output, attn_weights\n \n \n-class Emu3DecoderLayer(nn.Module):\n+class Emu3DecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: Emu3Config, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n@@ -1422,30 +1423,17 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    partial(decoder_layer.__call__, **flash_attn_kwargs),\n-                    hidden_states,\n-                    causal_mask,\n-                    position_ids,\n-                    past_key_values,\n-                    output_attentions,\n-                    use_cache,\n-                    cache_position,\n-                    position_embeddings,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=causal_mask,\n-                    position_ids=position_ids,\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                    position_embeddings=position_embeddings,\n-                    **flash_attn_kwargs,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+                **flash_attn_kwargs,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "481fa70eb2549243c99f7d4058618d868f6cb313",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 12,
            "deletions": 24,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24",
            "patch": "@@ -29,6 +29,7 @@\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n     CausalLMOutputWithPast,\n@@ -282,7 +283,7 @@ def forward(\n         return attn_output, attn_weights\n \n \n-class GemmaDecoderLayer(nn.Module):\n+class GemmaDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: GemmaConfig, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n@@ -557,29 +558,16 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n-                    hidden_states,\n-                    causal_mask,\n-                    position_ids,\n-                    past_key_values,\n-                    output_attentions,\n-                    use_cache,\n-                    cache_position,\n-                    position_embeddings,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=causal_mask,\n-                    position_ids=position_ids,\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                    position_embeddings=position_embeddings,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "ec16f5bba8f1f9e94d3bc3c86ec5c972550dffa7",
            "filename": "src/transformers/models/gemma/modular_gemma.py",
            "status": "modified",
            "additions": 10,
            "deletions": 23,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py?ref=9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24",
            "patch": "@@ -434,29 +434,16 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n-                    hidden_states,\n-                    causal_mask,\n-                    position_ids,\n-                    past_key_values,\n-                    output_attentions,\n-                    use_cache,\n-                    cache_position,\n-                    position_embeddings,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=causal_mask,\n-                    position_ids=position_ids,\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                    position_embeddings=position_embeddings,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "9a688e0b8e6f6b50467a463f6045d3c9ab29e986",
            "filename": "src/transformers/models/glm/modeling_glm.py",
            "status": "modified",
            "additions": 13,
            "deletions": 26,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py?ref=9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24",
            "patch": "@@ -19,7 +19,6 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from functools import partial\n from typing import Callable, Optional, Tuple, Union\n \n import torch\n@@ -31,6 +30,7 @@\n from ...integrations import use_kernel_forward_from_hub\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n     CausalLMOutputWithPast,\n@@ -300,7 +300,7 @@ def forward(self, x, position_ids):\n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n \n \n-class GlmDecoderLayer(nn.Module):\n+class GlmDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: GlmConfig, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n@@ -572,30 +572,17 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    partial(decoder_layer.__call__, **flash_attn_kwargs),\n-                    hidden_states,\n-                    causal_mask,\n-                    position_ids,\n-                    past_key_values,\n-                    output_attentions,\n-                    use_cache,\n-                    cache_position,\n-                    position_embeddings,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=causal_mask,\n-                    position_ids=position_ids,\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                    position_embeddings=position_embeddings,\n-                    **flash_attn_kwargs,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+                **flash_attn_kwargs,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "faf188387dbcb6674c830a8fe79341a8afbd9938",
            "filename": "src/transformers/models/glm4/modeling_glm4.py",
            "status": "modified",
            "additions": 13,
            "deletions": 26,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py?ref=9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24",
            "patch": "@@ -19,7 +19,6 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from functools import partial\n from typing import Callable, Optional, Tuple, Union\n \n import torch\n@@ -31,6 +30,7 @@\n from ...integrations import use_kernel_forward_from_hub\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n     CausalLMOutputWithPast,\n@@ -83,7 +83,7 @@ def forward(self, hidden_states: torch.FloatTensor) -> torch.FloatTensor:\n         return self.down_proj(up_states)\n \n \n-class Glm4DecoderLayer(nn.Module):\n+class Glm4DecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: Glm4Config, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n@@ -580,30 +580,17 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    partial(decoder_layer.__call__, **flash_attn_kwargs),\n-                    hidden_states,\n-                    causal_mask,\n-                    position_ids,\n-                    past_key_values,\n-                    output_attentions,\n-                    use_cache,\n-                    cache_position,\n-                    position_embeddings,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=causal_mask,\n-                    position_ids=position_ids,\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                    position_embeddings=position_embeddings,\n-                    **flash_attn_kwargs,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+                **flash_attn_kwargs,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "2a52e5b6708e17339f42ebb1dde0b41d4f03549a",
            "filename": "src/transformers/models/glm4/modular_glm4.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodular_glm4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodular_glm4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodular_glm4.py?ref=9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24",
            "patch": "@@ -15,11 +15,11 @@\n # limitations under the License.\n from typing import Optional, Tuple, Union\n \n-import torch.nn as nn\n import torch.utils.checkpoint\n \n from ...cache_utils import Cache\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import CausalLMOutputWithPast\n from ...processing_utils import Unpack\n from ...utils import LossKwargs, logging\n@@ -43,7 +43,7 @@ class Glm4MLP(Phi3MLP):\n     pass\n \n \n-class Glm4DecoderLayer(nn.Module):\n+class Glm4DecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: Glm4Config, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size"
        },
        {
            "sha": "4538a323c748edfaa7dbb16ed79f7f612560f4c6",
            "filename": "src/transformers/models/granite/modeling_granite.py",
            "status": "modified",
            "additions": 13,
            "deletions": 26,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py?ref=9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24",
            "patch": "@@ -19,7 +19,6 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from functools import partial\n from typing import Callable, List, Optional, Tuple, Union\n \n import torch\n@@ -31,6 +30,7 @@\n from ...integrations import use_kernel_forward_from_hub\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n@@ -243,7 +243,7 @@ def forward(self, x):\n         return down_proj\n \n \n-class GraniteDecoderLayer(nn.Module):\n+class GraniteDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: GraniteConfig, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n@@ -572,30 +572,17 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    partial(decoder_layer.__call__, **flash_attn_kwargs),\n-                    hidden_states,\n-                    causal_mask,\n-                    position_ids,\n-                    past_key_values,\n-                    output_attentions,\n-                    use_cache,\n-                    cache_position,\n-                    position_embeddings,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=causal_mask,\n-                    position_ids=position_ids,\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                    position_embeddings=position_embeddings,\n-                    **flash_attn_kwargs,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+                **flash_attn_kwargs,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "1d0ff532d8a599cf4b8724728e9419eb4ede3755",
            "filename": "src/transformers/models/granite/modular_granite.py",
            "status": "modified",
            "additions": 11,
            "deletions": 25,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodular_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodular_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodular_granite.py?ref=9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24",
            "patch": "@@ -13,7 +13,6 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from functools import partial\n from typing import List, Optional, Tuple, Union\n \n import torch\n@@ -192,30 +191,17 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    partial(decoder_layer.__call__, **flash_attn_kwargs),\n-                    hidden_states,\n-                    causal_mask,\n-                    position_ids,\n-                    past_key_values,\n-                    output_attentions,\n-                    use_cache,\n-                    cache_position,\n-                    position_embeddings,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=causal_mask,\n-                    position_ids=position_ids,\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                    position_embeddings=position_embeddings,\n-                    **flash_attn_kwargs,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+                **flash_attn_kwargs,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "9f7b4b87933272315ecc044236bf8a3a0efe1b1f",
            "filename": "src/transformers/models/helium/modeling_helium.py",
            "status": "modified",
            "additions": 13,
            "deletions": 26,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py?ref=9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24",
            "patch": "@@ -20,7 +20,6 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n import math\n-from functools import partial\n from typing import Callable, Optional, Tuple, Union\n \n import torch\n@@ -31,6 +30,7 @@\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n     CausalLMOutputWithPast,\n@@ -285,7 +285,7 @@ def forward(\n         return attn_output, attn_weights\n \n \n-class HeliumDecoderLayer(nn.Module):\n+class HeliumDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: HeliumConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n@@ -557,30 +557,17 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    partial(decoder_layer.__call__, **flash_attn_kwargs),\n-                    hidden_states,\n-                    causal_mask,\n-                    position_ids,\n-                    past_key_values,\n-                    output_attentions,\n-                    use_cache,\n-                    cache_position,\n-                    position_embeddings,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=causal_mask,\n-                    position_ids=position_ids,\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                    position_embeddings=position_embeddings,\n-                    **flash_attn_kwargs,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+                **flash_attn_kwargs,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "4fa538ee7c6363e59fad9b2aea502fe07d3389fd",
            "filename": "src/transformers/models/janus/modeling_janus.py",
            "status": "modified",
            "additions": 8,
            "deletions": 14,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py?ref=9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24",
            "patch": "@@ -31,6 +31,7 @@\n from ...generation import ClassifierFreeGuidanceLogitsProcessor, GenerationMixin, GenerationMode, LogitsProcessorList\n from ...generation.utils import GenerateDecoderOnlyOutput\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling, ModelOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n@@ -429,7 +430,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return hidden_states\n \n \n-class JanusVisionEncoderLayer(nn.Module):\n+class JanusVisionEncoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: JanusVisionConfig):\n         super().__init__()\n         self.embed_dim = config.hidden_size\n@@ -536,19 +537,12 @@ def forward(\n         for encoder_layer in self.layers:\n             if output_hidden_states:\n                 encoder_states = encoder_states + (hidden_states,)\n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    encoder_layer.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = encoder_layer(\n-                    hidden_states,\n-                    attention_mask,\n-                    output_attentions=output_attentions,\n-                )\n+\n+            layer_outputs = encoder_layer(\n+                hidden_states,\n+                attention_mask,\n+                output_attentions=output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "f6d5471ce3db2cdced9781dfbfea274d195ec9c8",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 13,
            "deletions": 26,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24",
            "patch": "@@ -17,7 +17,6 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from functools import partial\n from typing import Callable, Optional, Tuple, Union\n \n import torch\n@@ -29,6 +28,7 @@\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n     CausalLMOutputWithPast,\n@@ -290,7 +290,7 @@ def forward(\n         return attn_output, attn_weights\n \n \n-class LlamaDecoderLayer(nn.Module):\n+class LlamaDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: LlamaConfig, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n@@ -562,30 +562,17 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    partial(decoder_layer.__call__, **flash_attn_kwargs),\n-                    hidden_states,\n-                    causal_mask,\n-                    position_ids,\n-                    past_key_values,\n-                    output_attentions,\n-                    use_cache,\n-                    cache_position,\n-                    position_embeddings,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=causal_mask,\n-                    position_ids=position_ids,\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                    position_embeddings=position_embeddings,\n-                    **flash_attn_kwargs,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+                **flash_attn_kwargs,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "ebd0fe6c0189d8552f6727c43d0ddfd27b47c789",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 13,
            "deletions": 26,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24",
            "patch": "@@ -4,7 +4,6 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_mistral.py file directly. One of our CI enforces this.\n #                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n-from functools import partial\n from typing import Callable, List, Optional, Tuple, Union\n \n import torch\n@@ -16,6 +15,7 @@\n from ...integrations import use_kernel_forward_from_hub\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n     CausalLMOutputWithPast,\n@@ -228,7 +228,7 @@ def extra_repr(self):\n         return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n \n \n-class MistralDecoderLayer(nn.Module):\n+class MistralDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: MistralConfig, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n@@ -532,30 +532,17 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    partial(decoder_layer.__call__, **flash_attn_kwargs),\n-                    hidden_states,\n-                    causal_mask,\n-                    position_ids,\n-                    past_key_values,\n-                    output_attentions,\n-                    use_cache,\n-                    cache_position,\n-                    position_embeddings,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=causal_mask,\n-                    position_ids=position_ids,\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                    position_embeddings=position_embeddings,\n-                    **flash_attn_kwargs,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+                **flash_attn_kwargs,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "6d10364a6e9d71d29fa5b985fc2fc91eb44ede95",
            "filename": "src/transformers/models/moonshine/modeling_moonshine.py",
            "status": "modified",
            "additions": 24,
            "deletions": 51,
            "changes": 75,
            "blob_url": "https://github.com/huggingface/transformers/blob/9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py?ref=9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24",
            "patch": "@@ -18,7 +18,6 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from functools import partial\n from typing import Callable, Optional, Tuple, Union\n \n import numpy as np\n@@ -34,6 +33,7 @@\n     _prepare_4d_attention_mask_for_sdpa,\n )\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPast,\n@@ -351,7 +351,7 @@ def forward(self, x, position_ids):\n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n \n \n-class MoonshineEncoderLayer(nn.Module):\n+class MoonshineEncoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: MoonshineConfig, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n@@ -410,7 +410,7 @@ def forward(\n         return outputs\n \n \n-class MoonshineDecoderLayer(nn.Module):\n+class MoonshineDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: MoonshineConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n@@ -668,27 +668,14 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    encoder_layer.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    position_ids,\n-                    None,\n-                    output_attentions,\n-                    False,\n-                    None,\n-                    position_embeddings,\n-                )\n-            else:\n-                layer_outputs = encoder_layer(\n-                    hidden_states,\n-                    attention_mask=attention_mask,\n-                    position_ids=position_ids,\n-                    output_attentions=output_attentions,\n-                    position_embeddings=position_embeddings,\n-                    **flash_attn_kwargs,\n-                )\n+            layer_outputs = encoder_layer(\n+                hidden_states,\n+                attention_mask=attention_mask,\n+                position_ids=position_ids,\n+                output_attentions=output_attentions,\n+                position_embeddings=position_embeddings,\n+                **flash_attn_kwargs,\n+            )\n \n             hidden_states = layer_outputs[0]\n \n@@ -912,33 +899,19 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    partial(decoder_layer.__call__, **flash_attn_kwargs),\n-                    hidden_states,\n-                    causal_mask,\n-                    encoder_hidden_states,\n-                    position_ids,\n-                    past_key_values,\n-                    output_attentions,\n-                    use_cache,\n-                    cache_position,\n-                    position_embeddings,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=causal_mask,\n-                    encoder_attention_mask=encoder_attention_mask,\n-                    encoder_hidden_states=encoder_hidden_states,\n-                    position_ids=position_ids,\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                    position_embeddings=position_embeddings,\n-                    **flash_attn_kwargs,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                encoder_attention_mask=encoder_attention_mask,\n+                encoder_hidden_states=encoder_hidden_states,\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+                **flash_attn_kwargs,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "8864c0b41bd188bc56c3dbbcd1627816e638af78",
            "filename": "src/transformers/models/moonshine/modular_moonshine.py",
            "status": "modified",
            "additions": 23,
            "deletions": 50,
            "changes": 73,
            "blob_url": "https://github.com/huggingface/transformers/blob/9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py?ref=9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24",
            "patch": "@@ -12,7 +12,6 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from functools import partial\n from typing import Callable, Optional, Tuple, Union\n \n import torch\n@@ -27,6 +26,7 @@\n     _prepare_4d_attention_mask_for_sdpa,\n )\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPast,\n@@ -428,7 +428,7 @@ def __init__(self, config: MoonshineConfig, layer_idx: int):\n         self.post_attention_layernorm = nn.LayerNorm(config.hidden_size, bias=False)\n \n \n-class MoonshineDecoderLayer(nn.Module):\n+class MoonshineDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: MoonshineConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n@@ -686,27 +686,14 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    encoder_layer.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    position_ids,\n-                    None,\n-                    output_attentions,\n-                    False,\n-                    None,\n-                    position_embeddings,\n-                )\n-            else:\n-                layer_outputs = encoder_layer(\n-                    hidden_states,\n-                    attention_mask=attention_mask,\n-                    position_ids=position_ids,\n-                    output_attentions=output_attentions,\n-                    position_embeddings=position_embeddings,\n-                    **flash_attn_kwargs,\n-                )\n+            layer_outputs = encoder_layer(\n+                hidden_states,\n+                attention_mask=attention_mask,\n+                position_ids=position_ids,\n+                output_attentions=output_attentions,\n+                position_embeddings=position_embeddings,\n+                **flash_attn_kwargs,\n+            )\n \n             hidden_states = layer_outputs[0]\n \n@@ -832,33 +819,19 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    partial(decoder_layer.__call__, **flash_attn_kwargs),\n-                    hidden_states,\n-                    causal_mask,\n-                    encoder_hidden_states,\n-                    position_ids,\n-                    past_key_values,\n-                    output_attentions,\n-                    use_cache,\n-                    cache_position,\n-                    position_embeddings,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=causal_mask,\n-                    encoder_attention_mask=encoder_attention_mask,\n-                    encoder_hidden_states=encoder_hidden_states,\n-                    position_ids=position_ids,\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                    position_embeddings=position_embeddings,\n-                    **flash_attn_kwargs,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                encoder_attention_mask=encoder_attention_mask,\n+                encoder_hidden_states=encoder_hidden_states,\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+                **flash_attn_kwargs,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "6de200e66285d328fb3c571ca7b6a903c80bb609",
            "filename": "src/transformers/models/olmo/modeling_olmo.py",
            "status": "modified",
            "additions": 13,
            "deletions": 26,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py?ref=9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24",
            "patch": "@@ -4,7 +4,6 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_olmo.py file directly. One of our CI enforces this.\n #                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n-from functools import partial\n from typing import Callable, Optional, Tuple, Union\n \n import torch\n@@ -16,6 +15,7 @@\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n@@ -229,7 +229,7 @@ def forward(\n         return attn_output, attn_weights\n \n \n-class OlmoDecoderLayer(nn.Module):\n+class OlmoDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: OlmoConfig, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n@@ -532,30 +532,17 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    partial(decoder_layer.__call__, **flash_attn_kwargs),\n-                    hidden_states,\n-                    causal_mask,\n-                    position_ids,\n-                    past_key_values,\n-                    output_attentions,\n-                    use_cache,\n-                    cache_position,\n-                    position_embeddings,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=causal_mask,\n-                    position_ids=position_ids,\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                    position_embeddings=position_embeddings,\n-                    **flash_attn_kwargs,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+                **flash_attn_kwargs,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "31e6805cfde8cbb5adb90c79adec9694d9931e44",
            "filename": "src/transformers/models/olmo2/modeling_olmo2.py",
            "status": "modified",
            "additions": 13,
            "deletions": 26,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py?ref=9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24",
            "patch": "@@ -4,7 +4,6 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_olmo2.py file directly. One of our CI enforces this.\n #                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n-from functools import partial\n from typing import Callable, Optional, Tuple, Union\n \n import torch\n@@ -16,6 +15,7 @@\n from ...integrations import use_kernel_forward_from_hub\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n@@ -233,7 +233,7 @@ def forward(self, x):\n         return down_proj\n \n \n-class Olmo2DecoderLayer(nn.Module):\n+class Olmo2DecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: Olmo2Config, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n@@ -538,30 +538,17 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    partial(decoder_layer.__call__, **flash_attn_kwargs),\n-                    hidden_states,\n-                    causal_mask,\n-                    position_ids,\n-                    past_key_values,\n-                    output_attentions,\n-                    use_cache,\n-                    cache_position,\n-                    position_embeddings,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=causal_mask,\n-                    position_ids=position_ids,\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                    position_embeddings=position_embeddings,\n-                    **flash_attn_kwargs,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+                **flash_attn_kwargs,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "aaf5332c7176ac1d4b0de76652f343a7da3b99e2",
            "filename": "src/transformers/models/phi3/modeling_phi3.py",
            "status": "modified",
            "additions": 13,
            "deletions": 26,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py?ref=9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24",
            "patch": "@@ -20,7 +20,6 @@\n # limitations under the License.\n \n \n-from functools import partial\n from typing import Callable, Optional, Tuple, Union\n \n import torch\n@@ -32,6 +31,7 @@\n from ...integrations import use_kernel_forward_from_hub\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n     CausalLMOutputWithPast,\n@@ -257,7 +257,7 @@ def extra_repr(self):\n         return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n \n \n-class Phi3DecoderLayer(nn.Module):\n+class Phi3DecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: Phi3Config, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n@@ -587,30 +587,17 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    partial(decoder_layer.__call__, **flash_attn_kwargs),\n-                    hidden_states,\n-                    causal_mask,\n-                    position_ids,\n-                    past_key_values,\n-                    output_attentions,\n-                    use_cache,\n-                    cache_position,\n-                    position_embeddings,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=causal_mask,\n-                    position_ids=position_ids,\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                    position_embeddings=position_embeddings,\n-                    **flash_attn_kwargs,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+                **flash_attn_kwargs,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "3b9979edf31c1321f5f942f83a26ecbe31706921",
            "filename": "src/transformers/models/phi4_multimodal/modeling_phi4_multimodal.py",
            "status": "modified",
            "additions": 21,
            "deletions": 47,
            "changes": 68,
            "blob_url": "https://github.com/huggingface/transformers/blob/9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py?ref=9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24",
            "patch": "@@ -36,6 +36,7 @@\n from ...integrations import use_kernel_forward_from_hub\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPast,\n@@ -154,7 +155,7 @@ def forward(\n         return attn_output, attn_weights\n \n \n-class Phi4MultimodalVisionEncoderLayer(nn.Module):\n+class Phi4MultimodalVisionEncoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: Phi4MultimodalVisionConfig):\n         super().__init__()\n         self.embed_dim = config.hidden_size\n@@ -262,19 +263,12 @@ def forward(\n         for encoder_layer in self.layers:\n             if output_hidden_states:\n                 encoder_states = encoder_states + (hidden_states,)\n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    encoder_layer.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = encoder_layer(\n-                    hidden_states,\n-                    attention_mask,\n-                    output_attentions=output_attentions,\n-                )\n+\n+            layer_outputs = encoder_layer(\n+                hidden_states,\n+                attention_mask,\n+                output_attentions=output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0]\n \n@@ -1213,14 +1207,7 @@ def forward(self, hidden_states: torch.Tensor, mask: Optional[torch.Tensor]):\n         attention_mask = hs_mask.unsqueeze(1) + relative_attention_bias\n \n         for layer in self.encoders:\n-            if self.gradient_checkpointing and self.training:\n-                hidden_states = self._gradient_checkpointing_func(\n-                    layer.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                )\n-            else:\n-                hidden_states = layer(hidden_states, attention_mask)\n+            hidden_states = layer(hidden_states, attention_mask)\n \n         if unfolded:\n             embed_dim = hidden_states.shape[-1]\n@@ -1483,7 +1470,7 @@ def forward(\n         return attn_output, attn_weights\n \n \n-class Phi4MultimodalDecoderLayer(nn.Module):\n+class Phi4MultimodalDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: Phi4MultimodalConfig, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n@@ -1885,30 +1872,17 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n-                    hidden_states,\n-                    causal_mask,\n-                    position_ids,\n-                    past_key_values,\n-                    output_attentions,\n-                    use_cache,\n-                    cache_position,\n-                    position_embeddings,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=causal_mask,\n-                    position_ids=position_ids,\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                    position_embeddings=position_embeddings,\n-                    **kwargs,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+                **kwargs,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "b9d6bc0daf9434049629a6a1cf7ba48177180b45",
            "filename": "src/transformers/models/phi4_multimodal/modular_phi4_multimodal.py",
            "status": "modified",
            "additions": 12,
            "deletions": 32,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py?ref=9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24",
            "patch": "@@ -1265,14 +1265,7 @@ def forward(self, hidden_states: torch.Tensor, mask: Optional[torch.Tensor]):\n         attention_mask = hs_mask.unsqueeze(1) + relative_attention_bias\n \n         for layer in self.encoders:\n-            if self.gradient_checkpointing and self.training:\n-                hidden_states = self._gradient_checkpointing_func(\n-                    layer.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                )\n-            else:\n-                hidden_states = layer(hidden_states, attention_mask)\n+            hidden_states = layer(hidden_states, attention_mask)\n \n         if unfolded:\n             embed_dim = hidden_states.shape[-1]\n@@ -1655,30 +1648,17 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n-                    hidden_states,\n-                    causal_mask,\n-                    position_ids,\n-                    past_key_values,\n-                    output_attentions,\n-                    use_cache,\n-                    cache_position,\n-                    position_embeddings,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=causal_mask,\n-                    position_ids=position_ids,\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                    position_embeddings=position_embeddings,\n-                    **kwargs,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+                **kwargs,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "46fb6326ada90bd0dac50e0bc57d505cb8f92520",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 13,
            "deletions": 26,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24",
            "patch": "@@ -4,7 +4,6 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_qwen2.py file directly. One of our CI enforces this.\n #                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n-from functools import partial\n from typing import Callable, Optional, Tuple, Union\n \n import torch\n@@ -16,6 +15,7 @@\n from ...integrations import use_kernel_forward_from_hub\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n     CausalLMOutputWithPast,\n@@ -236,7 +236,7 @@ def extra_repr(self):\n         return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n \n \n-class Qwen2DecoderLayer(nn.Module):\n+class Qwen2DecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: Qwen2Config, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n@@ -545,30 +545,17 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    partial(decoder_layer.__call__, **flash_attn_kwargs),\n-                    hidden_states,\n-                    causal_mask,\n-                    position_ids,\n-                    past_key_values,\n-                    output_attentions,\n-                    use_cache,\n-                    cache_position,\n-                    position_embeddings,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=causal_mask,\n-                    position_ids=position_ids,\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                    position_embeddings=position_embeddings,\n-                    **flash_attn_kwargs,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+                **flash_attn_kwargs,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "2a869fe63043553bb38e0667a1f4efddef05bd5f",
            "filename": "src/transformers/models/qwen3/modeling_qwen3.py",
            "status": "modified",
            "additions": 13,
            "deletions": 26,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py?ref=9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24",
            "patch": "@@ -19,7 +19,6 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from functools import partial\n from typing import Callable, Optional, Tuple, Union\n \n import torch\n@@ -31,6 +30,7 @@\n from ...integrations import use_kernel_forward_from_hub\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n     CausalLMOutputWithPast,\n@@ -261,7 +261,7 @@ def forward(\n         return attn_output, attn_weights\n \n \n-class Qwen3DecoderLayer(nn.Module):\n+class Qwen3DecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: Qwen3Config, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n@@ -572,30 +572,17 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    partial(decoder_layer.__call__, **flash_attn_kwargs),\n-                    hidden_states,\n-                    causal_mask,\n-                    position_ids,\n-                    past_key_values,\n-                    output_attentions,\n-                    use_cache,\n-                    cache_position,\n-                    position_embeddings,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=causal_mask,\n-                    position_ids=position_ids,\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                    position_embeddings=position_embeddings,\n-                    **flash_attn_kwargs,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+                **flash_attn_kwargs,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "f77248970c4826ef51b5f3625105fa51cbb04167",
            "filename": "src/transformers/models/siglip/modeling_siglip.py",
            "status": "modified",
            "additions": 8,
            "deletions": 14,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py?ref=9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24",
            "patch": "@@ -28,6 +28,7 @@\n \n from ...activations import ACT2FN\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling, ImageClassifierOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...utils import (\n@@ -465,7 +466,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return hidden_states\n \n \n-class SiglipEncoderLayer(nn.Module):\n+class SiglipEncoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: Union[SiglipVisionConfig, SiglipTextConfig]):\n         super().__init__()\n         self.embed_dim = config.hidden_size\n@@ -743,19 +744,12 @@ def forward(\n         for encoder_layer in self.layers:\n             if output_hidden_states:\n                 encoder_states = encoder_states + (hidden_states,)\n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    encoder_layer.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = encoder_layer(\n-                    hidden_states,\n-                    attention_mask,\n-                    output_attentions=output_attentions,\n-                )\n+\n+            layer_outputs = encoder_layer(\n+                hidden_states,\n+                attention_mask,\n+                output_attentions=output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "0999ff69845b1854b65203dfc8cf77f0ec35831b",
            "filename": "src/transformers/models/siglip2/modeling_siglip2.py",
            "status": "modified",
            "additions": 8,
            "deletions": 14,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py?ref=9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24",
            "patch": "@@ -32,6 +32,7 @@\n \n from ...activations import ACT2FN\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling, ImageClassifierOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...utils import (\n@@ -356,7 +357,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return hidden_states\n \n \n-class Siglip2EncoderLayer(nn.Module):\n+class Siglip2EncoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: Union[Siglip2VisionConfig, Siglip2TextConfig]):\n         super().__init__()\n         self.embed_dim = config.hidden_size\n@@ -462,19 +463,12 @@ def forward(\n         for encoder_layer in self.layers:\n             if output_hidden_states:\n                 encoder_states = encoder_states + (hidden_states,)\n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    encoder_layer.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = encoder_layer(\n-                    hidden_states,\n-                    attention_mask,\n-                    output_attentions=output_attentions,\n-                )\n+\n+            layer_outputs = encoder_layer(\n+                hidden_states,\n+                attention_mask,\n+                output_attentions=output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "84e420607b99ae377fc2d4e92aee257ab608f614",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24",
            "patch": "@@ -34,6 +34,7 @@\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n     CausalLMOutputWithPast,\n@@ -230,7 +231,7 @@ def forward(\n         return attn_output, attn_weights\n \n \n-class Starcoder2DecoderLayer(nn.Module):\n+class Starcoder2DecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: Starcoder2Config, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size"
        },
        {
            "sha": "b7291969ebba2c961050641a17a76e22bd5cdb8b",
            "filename": "src/transformers/utils/dummy_pt_objects.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py?ref=9167fadab9bbe7ecb758e5d2e6b70ac4d0781e24",
            "patch": "@@ -542,6 +542,13 @@ def model_addition_debugger_context(*args, **kwargs):\n     requires_backends(model_addition_debugger_context, [\"torch\"])\n \n \n+class GradientCheckpointingLayer(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n ROPE_INIT_FUNCTIONS = None\n \n "
        }
    ],
    "stats": {
        "total": 1196,
        "additions": 435,
        "deletions": 761
    }
}