{
    "author": "fabxoe",
    "message": "ğŸŒ [i18n-KO] Translated `model_doc/deberta-v2.md` to Korean (#33968)\n\n* docs: ko: model_doc/deberta-v2.md\r\n\r\n* feat: nmt draft\r\n\r\n* fix: resolve suggestions\r\n\r\nCo-authored-by: Chaewon Song <chaewon1019@ewhain.net>\r\n\r\n* fix: resolve suggestions\r\n\r\n* fix: resolve suggestions\r\n\r\n---------\r\n\r\nCo-authored-by: Chaewon Song <chaewon1019@ewhain.net>",
    "sha": "db5f117b8a414ec23e12036baed67d36732f9ee4",
    "files": [
        {
            "sha": "3843c2ad17a4672f9a0fc52b0a83e0b4599c5ab6",
            "filename": "docs/source/ko/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/db5f117b8a414ec23e12036baed67d36732f9ee4/docs%2Fsource%2Fko%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/db5f117b8a414ec23e12036baed67d36732f9ee4/docs%2Fsource%2Fko%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2F_toctree.yml?ref=db5f117b8a414ec23e12036baed67d36732f9ee4",
            "patch": "@@ -366,8 +366,8 @@\n         title: DBRX\n       - local: in_translation\n         title: (ë²ˆì—­ì¤‘) DeBERTa\n-      - local: in_translation\n-        title: (ë²ˆì—­ì¤‘) DeBERTa-v2\n+      - local: model_doc/deberta-v2\n+        title: DeBERTa-v2\n       - local: in_translation\n         title: (ë²ˆì—­ì¤‘) DialoGPT\n       - local: in_translation"
        },
        {
            "sha": "2e590ad8a5abcf90da995dc90b6592c8a205b309",
            "filename": "docs/source/ko/model_doc/deberta-v2.md",
            "status": "added",
            "additions": 147,
            "deletions": 0,
            "changes": 147,
            "blob_url": "https://github.com/huggingface/transformers/blob/db5f117b8a414ec23e12036baed67d36732f9ee4/docs%2Fsource%2Fko%2Fmodel_doc%2Fdeberta-v2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/db5f117b8a414ec23e12036baed67d36732f9ee4/docs%2Fsource%2Fko%2Fmodel_doc%2Fdeberta-v2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fdeberta-v2.md?ref=db5f117b8a414ec23e12036baed67d36732f9ee4",
            "patch": "@@ -0,0 +1,147 @@\n+<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# DeBERTa-v2\n+\n+## ê°œìš”\n+\n+\n+DeBERTa ëª¨ë¸ì€ Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chenì´ ì‘ì„±í•œ [DeBERTa: ë¶„ë¦¬ëœ ì–´í…ì…˜ì„ í™œìš©í•œ ë””ì½”ë”© ê°•í™” BERT](https://arxiv.org/abs/2006.03654)ì´ë¼ëŠ” ë…¼ë¬¸ì—ì„œ ì œì•ˆë˜ì—ˆìŠµë‹ˆë‹¤. ì´ ëª¨ë¸ì€ 2018ë…„ Googleì´ ë°œí‘œí•œ BERT ëª¨ë¸ê³¼ 2019ë…„ Facebookì´ ë°œí‘œí•œ RoBERTa ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤.\n+DeBERTaëŠ” RoBERTaì—ì„œ ì‚¬ìš©ëœ ë°ì´í„°ì˜ ì ˆë°˜ë§Œì„ ì‚¬ìš©í•˜ì—¬ ë¶„ë¦¬ëœ(disentangled) ì–´í…ì…˜ê³¼ í–¥ìƒëœ ë§ˆìŠ¤í¬ ë””ì½”ë” í•™ìŠµì„ í†µí•´ RoBERTaë¥¼ ê°œì„ í–ˆìŠµë‹ˆë‹¤.\n+\n+ë…¼ë¬¸ì˜ ì´ˆë¡ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n+\n+*ì‚¬ì „ í•™ìŠµëœ ì‹ ê²½ë§ ì–¸ì–´ ëª¨ë¸ì˜ ìµœê·¼ ë°œì „ì€ ë§ì€ ìì—°ì–´ ì²˜ë¦¬(NLP) ì‘ì—…ì˜ ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ë‘ ê°€ì§€ ìƒˆë¡œìš´ ê¸°ìˆ ì„ ì‚¬ìš©í•˜ì—¬ BERTì™€ RoBERTa ëª¨ë¸ì„ ê°œì„ í•œ ìƒˆë¡œìš´ ëª¨ë¸ êµ¬ì¡°ì¸ DeBERTaë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì²« ë²ˆì§¸ëŠ” ë¶„ë¦¬ëœ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ìœ¼ë¡œ, ê° ë‹¨ì–´ê°€ ë‚´ìš©ê³¼ ìœ„ì¹˜ë¥¼ ê°ê° ì¸ì½”ë”©í•˜ëŠ” ë‘ ê°œì˜ ë²¡í„°ë¡œ í‘œí˜„ë˜ë©°, ë‹¨ì–´ë“¤ ê°„ì˜ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ëŠ” ë‚´ìš©ê³¼ ìƒëŒ€ì  ìœ„ì¹˜ì— ëŒ€í•œ ë¶„ë¦¬ëœ í–‰ë ¬ì„ ì‚¬ìš©í•˜ì—¬ ê³„ì‚°ë©ë‹ˆë‹¤. ë‘ ë²ˆì§¸ë¡œ, ëª¨ë¸ ì‚¬ì „ í•™ìŠµì„ ìœ„í•´ ë§ˆìŠ¤í‚¹ëœ í† í°ì„ ì˜ˆì¸¡í•˜ëŠ” ì¶œë ¥ ì†Œí”„íŠ¸ë§¥ìŠ¤ ì¸µì„ ëŒ€ì²´í•˜ëŠ” í–¥ìƒëœ ë§ˆìŠ¤í¬ ë””ì½”ë”ê°€ ì‚¬ìš©ë©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì´ ë‘ ê°€ì§€ ê¸°ìˆ ì´ ëª¨ë¸ ì‚¬ì „ í•™ìŠµì˜ íš¨ìœ¨ì„±ê³¼ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—…ì˜ ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œí‚¨ë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. RoBERTa-Largeì™€ ë¹„êµí–ˆì„ ë•Œ, ì ˆë°˜ì˜ í•™ìŠµ ë°ì´í„°ë¡œ í•™ìŠµëœ DeBERTa ëª¨ë¸ì€ ê´‘ë²”ìœ„í•œ NLP ì‘ì—…ì—ì„œ ì¼ê´€ë˜ê²Œ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ë©°, MNLIì—ì„œ +0.9%(90.2% vs 91.1%), SQuAD v2.0ì—ì„œ +2.3%(88.4% vs 90.7%), RACEì—ì„œ +3.6%(83.2% vs 86.8%)ì˜ ì„±ëŠ¥ í–¥ìƒì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. DeBERTa ì½”ë“œì™€ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì€ https://github.com/microsoft/DeBERTa ì—ì„œ ê³µê°œë  ì˜ˆì •ì…ë‹ˆë‹¤.*\n+\n+\n+ë‹¤ìŒ ì •ë³´ë“¤ì€ [ì›ë³¸ êµ¬í˜„ ì €ì¥ì†Œ](https://github.com/microsoft/DeBERTa)ì—ì„œ ë³´ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. DeBERTa v2ëŠ” DeBERTaì˜ ë‘ë²ˆì§¸ ëª¨ë¸ì…ë‹ˆë‹¤. \n+DeBERTa v2ëŠ” SuperGLUE ë‹¨ì¼ ëª¨ë¸ ì œì¶œì— ì‚¬ìš©ëœ 1.5B ëª¨ë¸ì„ í¬í•¨í•˜ë©°, ì¸ê°„ ê¸°ì¤€ì (ë² ì´ìŠ¤ë¼ì¸) 89.8ì  ëŒ€ë¹„ 89.9ì ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. ì €ìì˜ \n+[ë¸”ë¡œê·¸](https://www.microsoft.com/en-us/research/blog/microsoft-deberta-surpasses-human-performance-on-the-superglue-benchmark/)ì—ì„œ ë” ìì„¸í•œ ì •ë³´ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+v2ì˜ ìƒˆë¡œìš´ ì :\n+\n+- **ì–´íœ˜(Vocabulary)** v2ì—ì„œëŠ” í•™ìŠµ ë°ì´í„°ë¡œë¶€í„° êµ¬ì¶•ëœ 128K í¬ê¸°ì˜ ìƒˆë¡œìš´ ì–´íœ˜ë¥¼ ì‚¬ìš©í•˜ë„ë¡ í† í¬ë‚˜ì´ì €ê°€ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤. GPT2 ê¸°ë°˜ í† í¬ë‚˜ì´ì € ëŒ€ì‹ , ì´ì œëŠ” [ì„¼í…ìŠ¤í”¼ìŠ¤ ê¸°ë°˜](https://github.com/google/sentencepiece) í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n+- **nGiE[nê·¸ë¨ ìœ ë„(Induced) ì…ë ¥ ì¸ì½”ë”©]** DeBERTa-v2 ëª¨ë¸ì€ ì…ë ¥ í† í°ë“¤ì˜ ì§€ì—­ì  ì˜ì¡´ì„±ì„ ë” ì˜ í•™ìŠµí•˜ê¸° ìœ„í•´ ì²« ë²ˆì§¸ íŠ¸ëœìŠ¤í¬ë¨¸ ì¸µê³¼ í•¨ê»˜ ì¶”ê°€ì ì¸ í•©ì„±ê³± ì¸µì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n+- **ì–´í…ì…˜ ì¸µì—ì„œ ìœ„ì¹˜ íˆ¬ì˜ í–‰ë ¬ê³¼ ë‚´ìš© íˆ¬ì˜ í–‰ë ¬ ê³µìœ ** ì´ì „ ì‹¤í—˜ë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ, ì´ëŠ” ì„±ëŠ¥ì— ì˜í–¥ì„ ì£¼ì§€ ì•Šìœ¼ë©´ì„œ ë§¤ê°œë³€ìˆ˜ë¥¼ ì ˆì•½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+- **ìƒëŒ€ì  ìœ„ì¹˜ë¥¼ ì¸ì½”ë”©í•˜ê¸° ìœ„í•œ ë²„í‚· ì ìš©** DeBERTa-v2 ëª¨ë¸ì€ T5ì™€ ìœ ì‚¬í•˜ê²Œ ìƒëŒ€ì  ìœ„ì¹˜ë¥¼ ì¸ì½”ë”©í•˜ê¸° ìœ„í•´ ë¡œê·¸ ë²„í‚·ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n+- **900M ëª¨ë¸ & 1.5B ëª¨ë¸** 900Mê³¼ 1.5B, ë‘ ê°€ì§€ ì¶”ê°€ ëª¨ë¸ í¬ê¸°ê°€ ì œê³µë˜ë©°, ì´ëŠ” ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—…ì˜ ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œí‚µë‹ˆë‹¤.\n+\n+[DeBERTa](https://huggingface.co/DeBERTa) ëª¨ë¸ì˜ í…ì„œí”Œë¡œ 2.0 êµ¬í˜„ì€ [kamalkraj](https://huggingface.co/kamalkraj)ê°€ ê¸°ì—¬í–ˆìŠµë‹ˆë‹¤. ì›ë³¸ ì½”ë“œëŠ” [ì´ê³³](https://github.com/microsoft/DeBERTa)ì—ì„œ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+## ìë£Œ\n+\n+- [í…ìŠ¤íŠ¸ ë¶„ë¥˜ ì‘ì—… ê°€ì´ë“œ](../tasks/sequence_classification)\n+- [í† í° ë¶„ë¥˜ ì‘ì—… ê°€ì´ë“œ](../tasks/token_classification)\n+- [ì§ˆì˜ì‘ë‹µ ì‘ì—… ê°€ì´ë“œ](../tasks/question_answering)\n+- [ë§ˆìŠ¤í¬ ì–¸ì–´ ëª¨ë¸ë§ ì‘ì—… ê°€ì´ë“œ](../tasks/masked_language_modeling)\n+- [ë‹¤ì¤‘ ì„ íƒ ì‘ì—… ê°€ì´ë“œ](../tasks/multiple_choice)\n+\n+## DebertaV2Config\n+\n+[[autodoc]] DebertaV2Config\n+\n+## DebertaV2Tokenizer\n+\n+[[autodoc]] DebertaV2Tokenizer\n+    - build_inputs_with_special_tokens\n+    - get_special_tokens_mask\n+    - create_token_type_ids_from_sequences\n+    - save_vocabulary\n+\n+## DebertaV2TokenizerFast\n+\n+[[autodoc]] DebertaV2TokenizerFast\n+    - build_inputs_with_special_tokens\n+    - create_token_type_ids_from_sequences\n+\n+<frameworkcontent>\n+<pt>\n+\n+## DebertaV2Model\n+\n+[[autodoc]] DebertaV2Model\n+    - forward\n+\n+## DebertaV2PreTrainedModel\n+\n+[[autodoc]] DebertaV2PreTrainedModel\n+    - forward\n+\n+## DebertaV2ForMaskedLM\n+\n+[[autodoc]] DebertaV2ForMaskedLM\n+    - forward\n+\n+## DebertaV2ForSequenceClassification\n+\n+[[autodoc]] DebertaV2ForSequenceClassification\n+    - forward\n+\n+## DebertaV2ForTokenClassification\n+\n+[[autodoc]] DebertaV2ForTokenClassification\n+    - forward\n+\n+## DebertaV2ForQuestionAnswering\n+\n+[[autodoc]] DebertaV2ForQuestionAnswering\n+    - forward\n+\n+## DebertaV2ForMultipleChoice\n+\n+[[autodoc]] DebertaV2ForMultipleChoice\n+    - forward\n+\n+</pt>\n+<tf>\n+\n+## TFDebertaV2Model\n+\n+[[autodoc]] TFDebertaV2Model\n+    - call\n+\n+## TFDebertaV2PreTrainedModel\n+\n+[[autodoc]] TFDebertaV2PreTrainedModel\n+    - call\n+\n+## TFDebertaV2ForMaskedLM\n+\n+[[autodoc]] TFDebertaV2ForMaskedLM\n+    - call\n+\n+## TFDebertaV2ForSequenceClassification\n+\n+[[autodoc]] TFDebertaV2ForSequenceClassification\n+    - call\n+\n+## TFDebertaV2ForTokenClassification\n+\n+[[autodoc]] TFDebertaV2ForTokenClassification\n+    - call\n+\n+## TFDebertaV2ForQuestionAnswering\n+\n+[[autodoc]] TFDebertaV2ForQuestionAnswering\n+    - call\n+\n+## TFDebertaV2ForMultipleChoice\n+\n+[[autodoc]] TFDebertaV2ForMultipleChoice\n+    - call\n+\n+</tf>\n+</frameworkcontent>"
        }
    ],
    "stats": {
        "total": 151,
        "additions": 149,
        "deletions": 2
    }
}