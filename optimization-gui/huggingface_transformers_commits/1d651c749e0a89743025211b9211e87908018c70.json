{
    "author": "molbap",
    "message": "Modernize CLIP modeling code  (#41546)\n\n* stranded\n\n* update modular\n\n* modularities\n\n* update\n\n* fx broken\n\n* fx stillb roken\n\n* update\n\n* missed this\n\n* fix metaclip",
    "sha": "1d651c749e0a89743025211b9211e87908018c70",
    "files": [
        {
            "sha": "013e68fab2171eb5295b84cf9e43d9626c88103c",
            "filename": "src/transformers/models/clip/modeling_clip.py",
            "status": "modified",
            "additions": 48,
            "deletions": 132,
            "changes": 180,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d651c749e0a89743025211b9211e87908018c70/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d651c749e0a89743025211b9211e87908018c70/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py?ref=1d651c749e0a89743025211b9211e87908018c70",
            "patch": "@@ -26,7 +26,17 @@\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling, ImageClassifierOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n-from ...utils import ModelOutput, auto_docstring, can_return_tuple, filter_out_non_signature_kwargs, logging, torch_int\n+from ...processing_utils import Unpack\n+from ...utils import (\n+    ModelOutput,\n+    TransformersKwargs,\n+    auto_docstring,\n+    can_return_tuple,\n+    filter_out_non_signature_kwargs,\n+    logging,\n+    torch_int,\n+)\n+from ...utils.generic import check_model_inputs\n from .configuration_clip import CLIPConfig, CLIPTextConfig, CLIPVisionConfig\n \n \n@@ -260,8 +270,7 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: float,\n     dropout: float = 0.0,\n-    output_attentions: bool = True,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n     attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n     if attention_mask is not None:\n@@ -271,8 +280,6 @@ def eager_attention_forward(\n \n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n-    if not output_attentions:\n-        attn_weights = None\n     return attn_output, attn_weights\n \n \n@@ -304,7 +311,7 @@ def forward(\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         causal_attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = False,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n@@ -340,14 +347,12 @@ def forward(\n             is_causal=self.is_causal,\n             scaling=self.scale,\n             dropout=0.0 if not self.training else self.dropout,\n-            output_attentions=output_attentions,\n+            **kwargs,\n         )\n \n         attn_output = attn_output.reshape(batch_size, seq_length, embed_dim).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n-        if not output_attentions:\n-            attn_weights = None\n         return attn_output, attn_weights\n \n \n@@ -380,26 +385,16 @@ def forward(\n         hidden_states: torch.Tensor,\n         attention_mask: torch.Tensor,\n         causal_attention_mask: torch.Tensor,\n-        output_attentions: Optional[bool] = False,\n-    ) -> tuple[torch.FloatTensor]:\n-        \"\"\"\n-        Args:\n-            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n-            attention_mask (`torch.FloatTensor`): attention mask of size\n-                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n-                `(config.encoder_attention_heads,)`.\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-        \"\"\"\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> torch.FloatTensor:\n         residual = hidden_states\n \n         hidden_states = self.layer_norm1(hidden_states)\n         hidden_states, attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             causal_attention_mask=causal_attention_mask,\n-            output_attentions=output_attentions,\n+            **kwargs,\n         )\n         hidden_states = residual + hidden_states\n \n@@ -408,12 +403,7 @@ def forward(\n         hidden_states = self.mlp(hidden_states)\n         hidden_states = residual + hidden_states\n \n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (attn_weights,)\n-\n-        return outputs\n+        return hidden_states\n \n \n @auto_docstring\n@@ -426,6 +416,10 @@ class CLIPPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n+    _can_record_outputs = {\n+        \"hidden_states\": CLIPEncoderLayer,\n+        \"attentions\": CLIPAttention,\n+    }\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n@@ -504,8 +498,7 @@ def forward(\n         inputs_embeds,\n         attention_mask: Optional[torch.Tensor] = None,\n         causal_attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutput:\n         r\"\"\"\n         Args:\n@@ -527,46 +520,18 @@ def forward(\n                 - 0 for tokens that are **masked**.\n \n                 [What are attention masks?](../glossary#attention-mask)\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-            output_hidden_states (`bool`, *optional*):\n-                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n-                for more detail.\n-            return_dict (`bool`, *optional*):\n-                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n         \"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-\n-        encoder_states = () if output_hidden_states else None\n-        all_attentions = () if output_attentions else None\n-\n         hidden_states = inputs_embeds\n-        for idx, encoder_layer in enumerate(self.layers):\n-            if output_hidden_states:\n-                encoder_states = encoder_states + (hidden_states,)\n-            layer_outputs = encoder_layer(\n+        for encoder_layer in self.layers:\n+            hidden_states = encoder_layer(\n                 hidden_states,\n                 attention_mask,\n                 causal_attention_mask,\n-                output_attentions=output_attentions,\n+                **kwargs,\n             )\n \n-            hidden_states = layer_outputs[0]\n-\n-            if output_attentions:\n-                all_attentions = all_attentions + (layer_outputs[1],)\n-\n-        if output_hidden_states:\n-            encoder_states = encoder_states + (hidden_states,)\n-\n         return BaseModelOutput(\n             last_hidden_state=hidden_states,\n-            hidden_states=encoder_states,\n-            attentions=all_attentions,\n         )\n \n \n@@ -588,14 +553,8 @@ def forward(\n         input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutputWithPooling:\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-\n         if input_ids is None:\n             raise ValueError(\"You have to specify input_ids\")\n \n@@ -604,23 +563,18 @@ def forward(\n \n         hidden_states = self.embeddings(input_ids=input_ids, position_ids=position_ids)\n \n-        # CLIP's text model uses causal mask, prepare it here.\n-        # https://github.com/openai/CLIP/blob/cfcffb90e69f37bf2ff1e988237a0fbe41f33c04/clip/model.py#L324\n         causal_attention_mask = _create_4d_causal_attention_mask(\n             input_shape, hidden_states.dtype, device=hidden_states.device\n         )\n \n-        # expand attention_mask\n         if attention_mask is not None and self.config._attn_implementation != \"flash_attention_2\":\n-            # [batch_size, seq_len] -> [batch_size, 1, tgt_seq_len, src_seq_len]\n             attention_mask = _prepare_4d_attention_mask(attention_mask, hidden_states.dtype)\n \n         encoder_outputs: BaseModelOutput = self.encoder(\n             inputs_embeds=hidden_states,\n             attention_mask=attention_mask,\n             causal_attention_mask=causal_attention_mask,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n+            **kwargs,\n         )\n \n         last_hidden_state = encoder_outputs.last_hidden_state\n@@ -651,8 +605,6 @@ def forward(\n         return BaseModelOutputWithPooling(\n             last_hidden_state=last_hidden_state,\n             pooler_output=pooled_output,\n-            hidden_states=encoder_outputs.hidden_states,\n-            attentions=encoder_outputs.attentions,\n         )\n \n \n@@ -680,15 +632,15 @@ def get_input_embeddings(self) -> nn.Module:\n     def set_input_embeddings(self, value):\n         self.text_model.embeddings.token_embedding = value\n \n+    @check_model_inputs()\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutputWithPooling:\n         r\"\"\"\n         Examples:\n@@ -710,8 +662,7 @@ def forward(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n+            **kwargs,\n         )\n \n \n@@ -730,15 +681,9 @@ def __init__(self, config: CLIPVisionConfig):\n     def forward(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: Optional[bool] = False,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutputWithPooling:\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-\n         if pixel_values is None:\n             raise ValueError(\"You have to specify pixel_values\")\n \n@@ -747,8 +692,7 @@ def forward(\n \n         encoder_outputs: BaseModelOutput = self.encoder(\n             inputs_embeds=hidden_states,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n+            **kwargs,\n         )\n \n         last_hidden_state = encoder_outputs.last_hidden_state\n@@ -758,8 +702,6 @@ def forward(\n         return BaseModelOutputWithPooling(\n             last_hidden_state=last_hidden_state,\n             pooler_output=pooled_output,\n-            hidden_states=encoder_outputs.hidden_states,\n-            attentions=encoder_outputs.attentions,\n         )\n \n \n@@ -783,14 +725,14 @@ def __init__(self, config: CLIPVisionConfig):\n     def get_input_embeddings(self) -> nn.Module:\n         return self.vision_model.embeddings.patch_embedding\n \n+    @check_model_inputs(tie_last_hidden_states=False)\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutputWithPooling:\n         r\"\"\"\n         Example:\n@@ -815,9 +757,8 @@ def forward(\n \n         return self.vision_model(\n             pixel_values=pixel_values,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n             interpolate_pos_encoding=interpolate_pos_encoding,\n+            **kwargs,\n         )\n \n \n@@ -947,9 +888,8 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         return_loss: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> CLIPOutput:\n         r\"\"\"\n         return_loss (`bool`, *optional*):\n@@ -977,25 +917,17 @@ def forward(\n         >>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n         >>> probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n         ```\"\"\"\n-        # Use CLIP model's config for some fields (if specified) instead of those of vision & text components.\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-\n         vision_outputs: BaseModelOutputWithPooling = self.vision_model(\n             pixel_values=pixel_values,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n             interpolate_pos_encoding=interpolate_pos_encoding,\n+            **kwargs,\n         )\n \n         text_outputs: BaseModelOutputWithPooling = self.text_model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n+            **kwargs,\n         )\n \n         image_embeds = vision_outputs.pooler_output\n@@ -1054,15 +986,15 @@ def get_input_embeddings(self) -> nn.Module:\n     def set_input_embeddings(self, value):\n         self.text_model.embeddings.token_embedding = value\n \n+    @check_model_inputs()\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> CLIPTextModelOutput:\n         r\"\"\"\n         Examples:\n@@ -1085,17 +1017,14 @@ def forward(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n+            **kwargs,\n         )\n         pooled_output = text_outputs.pooler_output\n         text_embeds = self.text_projection(pooled_output)\n \n         return CLIPTextModelOutput(\n             text_embeds=text_embeds,\n             last_hidden_state=text_outputs.last_hidden_state,\n-            hidden_states=text_outputs.hidden_states,\n-            attentions=text_outputs.attentions,\n         )\n \n \n@@ -1119,14 +1048,14 @@ def __init__(self, config: CLIPVisionConfig):\n     def get_input_embeddings(self) -> nn.Module:\n         return self.vision_model.embeddings.patch_embedding\n \n+    @check_model_inputs(tie_last_hidden_states=False)\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> CLIPVisionModelOutput:\n         r\"\"\"\n         Examples:\n@@ -1151,18 +1080,15 @@ def forward(\n \n         vision_outputs: BaseModelOutputWithPooling = self.vision_model(\n             pixel_values=pixel_values,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n             interpolate_pos_encoding=interpolate_pos_encoding,\n+            **kwargs,\n         )\n         pooled_output = vision_outputs.pooler_output\n         image_embeds = self.visual_projection(pooled_output)\n \n         return CLIPVisionModelOutput(\n             image_embeds=image_embeds,\n             last_hidden_state=vision_outputs.last_hidden_state,\n-            hidden_states=vision_outputs.hidden_states,\n-            attentions=vision_outputs.attentions,\n         )\n \n \n@@ -1191,37 +1117,29 @@ def __init__(self, config: CLIPConfig) -> None:\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @check_model_inputs()\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n         pixel_values: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> ImageClassifierOutput:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n         \"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-\n         outputs: BaseModelOutputWithPooling = self.vision_model(\n             pixel_values,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n+            **kwargs,\n         )\n \n         sequence_output = outputs.last_hidden_state\n \n-        # average pool the patch tokens\n         sequence_output = torch.mean(sequence_output[:, 1:, :], dim=1)\n-        # apply classifier\n         logits = self.classifier(sequence_output)\n \n         loss = None\n@@ -1231,8 +1149,6 @@ def forward(\n         return ImageClassifierOutput(\n             loss=loss,\n             logits=logits,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n         )\n \n "
        },
        {
            "sha": "abf9c1f1874172f541597d807880b33a88f0a358",
            "filename": "src/transformers/models/metaclip_2/modeling_metaclip_2.py",
            "status": "modified",
            "additions": 66,
            "deletions": 148,
            "changes": 214,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d651c749e0a89743025211b9211e87908018c70/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodeling_metaclip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d651c749e0a89743025211b9211e87908018c70/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodeling_metaclip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodeling_metaclip_2.py?ref=1d651c749e0a89743025211b9211e87908018c70",
            "patch": "@@ -160,8 +160,7 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: float,\n     dropout: float = 0.0,\n-    output_attentions: bool = True,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n     attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n     if attention_mask is not None:\n@@ -171,8 +170,6 @@ def eager_attention_forward(\n \n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n-    if not output_attentions:\n-        attn_weights = None\n     return attn_output, attn_weights\n \n \n@@ -204,7 +201,7 @@ def forward(\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         causal_attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = False,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n@@ -240,14 +237,12 @@ def forward(\n             is_causal=self.is_causal,\n             scaling=self.scale,\n             dropout=0.0 if not self.training else self.dropout,\n-            output_attentions=output_attentions,\n+            **kwargs,\n         )\n \n         attn_output = attn_output.reshape(batch_size, seq_length, embed_dim).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n-        if not output_attentions:\n-            attn_weights = None\n         return attn_output, attn_weights\n \n \n@@ -266,6 +261,41 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return hidden_states\n \n \n+class MetaClip2EncoderLayer(GradientCheckpointingLayer):\n+    def __init__(self, config: Union[MetaClip2VisionConfig, MetaClip2TextConfig]):\n+        super().__init__()\n+        self.embed_dim = config.hidden_size\n+        self.self_attn = MetaClip2Attention(config)\n+        self.layer_norm1 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n+        self.mlp = MetaClip2MLP(config)\n+        self.layer_norm2 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: torch.Tensor,\n+        causal_attention_mask: torch.Tensor,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> torch.FloatTensor:\n+        residual = hidden_states\n+\n+        hidden_states = self.layer_norm1(hidden_states)\n+        hidden_states, attn_weights = self.self_attn(\n+            hidden_states=hidden_states,\n+            attention_mask=attention_mask,\n+            causal_attention_mask=causal_attention_mask,\n+            **kwargs,\n+        )\n+        hidden_states = residual + hidden_states\n+\n+        residual = hidden_states\n+        hidden_states = self.layer_norm2(hidden_states)\n+        hidden_states = self.mlp(hidden_states)\n+        hidden_states = residual + hidden_states\n+\n+        return hidden_states\n+\n+\n @auto_docstring\n class MetaClip2PreTrainedModel(PreTrainedModel):\n     config: MetaClip2Config\n@@ -276,6 +306,10 @@ class MetaClip2PreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n+    _can_record_outputs = {\n+        \"hidden_states\": MetaClip2EncoderLayer,\n+        \"attentions\": MetaClip2Attention,\n+    }\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n@@ -334,56 +368,6 @@ def _init_weights(self, module):\n             module.bias.data.zero_()\n \n \n-class MetaClip2EncoderLayer(GradientCheckpointingLayer):\n-    def __init__(self, config: Union[MetaClip2VisionConfig, MetaClip2TextConfig]):\n-        super().__init__()\n-        self.embed_dim = config.hidden_size\n-        self.self_attn = MetaClip2Attention(config)\n-        self.layer_norm1 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n-        self.mlp = MetaClip2MLP(config)\n-        self.layer_norm2 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: torch.Tensor,\n-        causal_attention_mask: torch.Tensor,\n-        output_attentions: Optional[bool] = False,\n-    ) -> tuple[torch.FloatTensor]:\n-        \"\"\"\n-        Args:\n-            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n-            attention_mask (`torch.FloatTensor`): attention mask of size\n-                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n-                `(config.encoder_attention_heads,)`.\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-        \"\"\"\n-        residual = hidden_states\n-\n-        hidden_states = self.layer_norm1(hidden_states)\n-        hidden_states, attn_weights = self.self_attn(\n-            hidden_states=hidden_states,\n-            attention_mask=attention_mask,\n-            causal_attention_mask=causal_attention_mask,\n-            output_attentions=output_attentions,\n-        )\n-        hidden_states = residual + hidden_states\n-\n-        residual = hidden_states\n-        hidden_states = self.layer_norm2(hidden_states)\n-        hidden_states = self.mlp(hidden_states)\n-        hidden_states = residual + hidden_states\n-\n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (attn_weights,)\n-\n-        return outputs\n-\n-\n class MetaClip2Encoder(nn.Module):\n     \"\"\"\n     Transformer encoder consisting of `config.num_hidden_layers` self attention layers. Each layer is a\n@@ -404,8 +388,7 @@ def forward(\n         inputs_embeds,\n         attention_mask: Optional[torch.Tensor] = None,\n         causal_attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutput:\n         r\"\"\"\n         Args:\n@@ -427,46 +410,18 @@ def forward(\n                 - 0 for tokens that are **masked**.\n \n                 [What are attention masks?](../glossary#attention-mask)\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-            output_hidden_states (`bool`, *optional*):\n-                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n-                for more detail.\n-            return_dict (`bool`, *optional*):\n-                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n         \"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-\n-        encoder_states = () if output_hidden_states else None\n-        all_attentions = () if output_attentions else None\n-\n         hidden_states = inputs_embeds\n-        for idx, encoder_layer in enumerate(self.layers):\n-            if output_hidden_states:\n-                encoder_states = encoder_states + (hidden_states,)\n-            layer_outputs = encoder_layer(\n+        for encoder_layer in self.layers:\n+            hidden_states = encoder_layer(\n                 hidden_states,\n                 attention_mask,\n                 causal_attention_mask,\n-                output_attentions=output_attentions,\n+                **kwargs,\n             )\n \n-            hidden_states = layer_outputs[0]\n-\n-            if output_attentions:\n-                all_attentions = all_attentions + (layer_outputs[1],)\n-\n-        if output_hidden_states:\n-            encoder_states = encoder_states + (hidden_states,)\n-\n         return BaseModelOutput(\n             last_hidden_state=hidden_states,\n-            hidden_states=encoder_states,\n-            attentions=all_attentions,\n         )\n \n \n@@ -586,6 +541,7 @@ def get_input_embeddings(self) -> nn.Module:\n     def set_input_embeddings(self, value):\n         self.text_model.embeddings.token_embedding = value\n \n+    @check_model_inputs()\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -595,6 +551,7 @@ def forward(\n         position_ids: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutputWithPooling:\n         r\"\"\"\n         Examples:\n@@ -616,8 +573,7 @@ def forward(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n+            **kwargs,\n         )\n \n \n@@ -694,6 +650,7 @@ def get_input_embeddings(self) -> nn.Module:\n     def set_input_embeddings(self, value):\n         self.text_model.embeddings.token_embedding = value\n \n+    @check_model_inputs()\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -703,6 +660,7 @@ def forward(\n         position_ids: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> MetaClip2TextModelOutput:\n         r\"\"\"\n         Examples:\n@@ -723,17 +681,14 @@ def forward(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n+            **kwargs,\n         )\n         pooled_output = text_outputs.pooler_output\n         text_embeds = self.text_projection(pooled_output)\n \n         return MetaClip2TextModelOutput(\n             text_embeds=text_embeds,\n             last_hidden_state=text_outputs.last_hidden_state,\n-            hidden_states=text_outputs.hidden_states,\n-            attentions=text_outputs.attentions,\n         )\n \n \n@@ -881,8 +836,6 @@ def get_text_features(\n         input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n     ) -> torch.FloatTensor:\n         r\"\"\"\n         Returns:\n@@ -915,8 +868,6 @@ def get_text_features(\n     def get_image_features(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n     ) -> torch.FloatTensor:\n         r\"\"\"\n@@ -959,9 +910,8 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         return_loss: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> MetaClip2Output:\n         r\"\"\"\n         return_loss (`bool`, *optional*):\n@@ -988,25 +938,17 @@ def forward(\n         >>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n         >>> probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n         ```\"\"\"\n-        # Use METACLIP_2 model's config for some fields (if specified) instead of those of vision & text components.\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-\n         vision_outputs: BaseModelOutputWithPooling = self.vision_model(\n             pixel_values=pixel_values,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n             interpolate_pos_encoding=interpolate_pos_encoding,\n+            **kwargs,\n         )\n \n         text_outputs: BaseModelOutputWithPooling = self.text_model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n+            **kwargs,\n         )\n \n         image_embeds = vision_outputs.pooler_output\n@@ -1055,15 +997,9 @@ def __init__(self, config: MetaClip2VisionConfig):\n     def forward(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: Optional[bool] = False,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutputWithPooling:\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-\n         if pixel_values is None:\n             raise ValueError(\"You have to specify pixel_values\")\n \n@@ -1072,8 +1008,7 @@ def forward(\n \n         encoder_outputs: BaseModelOutput = self.encoder(\n             inputs_embeds=hidden_states,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n+            **kwargs,\n         )\n \n         last_hidden_state = encoder_outputs.last_hidden_state\n@@ -1083,8 +1018,6 @@ def forward(\n         return BaseModelOutputWithPooling(\n             last_hidden_state=last_hidden_state,\n             pooler_output=pooled_output,\n-            hidden_states=encoder_outputs.hidden_states,\n-            attentions=encoder_outputs.attentions,\n         )\n \n \n@@ -1144,14 +1077,13 @@ def __init__(self, config: MetaClip2VisionConfig):\n     def get_input_embeddings(self) -> nn.Module:\n         return self.vision_model.embeddings.patch_embedding\n \n+    @check_model_inputs(tie_last_hidden_states=False)\n     @can_return_tuple\n-    @auto_docstring\n     def forward(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutputWithPooling:\n         r\"\"\"\n         Examples:\n@@ -1176,9 +1108,8 @@ def forward(\n \n         return self.vision_model(\n             pixel_values=pixel_values,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n             interpolate_pos_encoding=interpolate_pos_encoding,\n+            **kwargs,\n         )\n \n \n@@ -1255,14 +1186,14 @@ def __init__(self, config: MetaClip2VisionConfig):\n     def get_input_embeddings(self) -> nn.Module:\n         return self.vision_model.embeddings.patch_embedding\n \n+    @check_model_inputs(tie_last_hidden_states=False)\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> MetaClip2VisionModelOutput:\n         r\"\"\"\n         Examples:\n@@ -1286,18 +1217,15 @@ def forward(\n \n         vision_outputs: BaseModelOutputWithPooling = self.vision_model(\n             pixel_values=pixel_values,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n             interpolate_pos_encoding=interpolate_pos_encoding,\n+            **kwargs,\n         )\n         pooled_output = vision_outputs.pooler_output\n         image_embeds = self.visual_projection(pooled_output)\n \n         return MetaClip2VisionModelOutput(\n             image_embeds=image_embeds,\n             last_hidden_state=vision_outputs.last_hidden_state,\n-            hidden_states=vision_outputs.hidden_states,\n-            attentions=vision_outputs.attentions,\n         )\n \n \n@@ -1326,37 +1254,29 @@ def __init__(self, config: MetaClip2Config) -> None:\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @check_model_inputs()\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n         pixel_values: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> ImageClassifierOutput:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n         \"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-\n         outputs: BaseModelOutputWithPooling = self.vision_model(\n             pixel_values,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n+            **kwargs,\n         )\n \n         sequence_output = outputs.last_hidden_state\n \n-        # average pool the patch tokens\n         sequence_output = torch.mean(sequence_output[:, 1:, :], dim=1)\n-        # apply classifier\n         logits = self.classifier(sequence_output)\n \n         loss = None\n@@ -1366,8 +1286,6 @@ def forward(\n         return ImageClassifierOutput(\n             loss=loss,\n             logits=logits,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n         )\n \n "
        },
        {
            "sha": "1f9a0a7d0939daae9352738138c0862763f65fa7",
            "filename": "src/transformers/models/metaclip_2/modular_metaclip_2.py",
            "status": "modified",
            "additions": 25,
            "deletions": 21,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d651c749e0a89743025211b9211e87908018c70/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodular_metaclip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d651c749e0a89743025211b9211e87908018c70/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodular_metaclip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodular_metaclip_2.py?ref=1d651c749e0a89743025211b9211e87908018c70",
            "patch": "@@ -7,12 +7,13 @@\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n from ...modeling_utils import PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import TransformersKwargs, auto_docstring, logging\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n from ...utils.generic import check_model_inputs\n from ..clip.configuration_clip import CLIPConfig, CLIPTextConfig, CLIPVisionConfig\n from ..clip.modeling_clip import (\n     CLIPMLP,\n     CLIPAttention,\n+    CLIPEncoderLayer,\n     CLIPForImageClassification,\n     CLIPModel,\n     CLIPTextEmbeddings,\n@@ -213,6 +214,10 @@ class MetaClip2MLP(CLIPMLP):\n     pass\n \n \n+class MetaClip2EncoderLayer(CLIPEncoderLayer):\n+    pass\n+\n+\n @auto_docstring\n class MetaClip2PreTrainedModel(PreTrainedModel):\n     config: MetaClip2Config\n@@ -223,6 +228,10 @@ class MetaClip2PreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n+    _can_record_outputs = {\n+        \"hidden_states\": MetaClip2EncoderLayer,\n+        \"attentions\": MetaClip2Attention,\n+    }\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n@@ -369,13 +378,17 @@ def __init__(self, config: MetaClip2TextConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @check_model_inputs()\n+    @can_return_tuple\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ):\n         r\"\"\"\n         Examples:\n@@ -398,6 +411,7 @@ def forward(\n             position_ids=position_ids,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n+            **kwargs,\n         )\n \n \n@@ -450,6 +464,7 @@ def forward(\n         position_ids: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ):\n         r\"\"\"\n         Examples:\n@@ -471,6 +486,7 @@ def forward(\n             position_ids=position_ids,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n+            **kwargs,\n         )\n \n \n@@ -541,9 +557,8 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         return_loss: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n+        **kwargs: Unpack[TransformersKwargs],\n     ):\n         r\"\"\"\n         return_loss (`bool`, *optional*):\n@@ -576,18 +591,15 @@ def forward(\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             return_loss=return_loss,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n             interpolate_pos_encoding=interpolate_pos_encoding,\n+            **kwargs,\n         )\n \n     def get_text_features(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n     ):\n         r\"\"\"\n         Returns:\n@@ -609,15 +621,11 @@ def get_text_features(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n         )\n \n     def get_image_features(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n     ):\n         r\"\"\"\n@@ -644,8 +652,6 @@ def get_image_features(\n         ```\"\"\"\n         return super().get_image_features(\n             pixel_values=pixel_values,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n             interpolate_pos_encoding=interpolate_pos_encoding,\n         )\n \n@@ -687,12 +693,13 @@ class MetaClip2VisionModel(CLIPVisionModel):\n     >>> pooled_output = outputs.pooler_output  # pooled CLS states\n     ```\"\"\"\n \n+    @check_model_inputs(tie_last_hidden_states=False)\n+    @can_return_tuple\n     def forward(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n+        **kwargs: Unpack[TransformersKwargs],\n     ):\n         r\"\"\"\n         Examples:\n@@ -716,9 +723,8 @@ def forward(\n         ```\"\"\"\n         return super().forward(\n             pixel_values=pixel_values,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n             interpolate_pos_encoding=interpolate_pos_encoding,\n+            **kwargs,\n         )\n \n \n@@ -761,9 +767,8 @@ class MetaClip2VisionModelWithProjection(CLIPVisionModelWithProjection):\n     def forward(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n+        **kwargs: Unpack[TransformersKwargs],\n     ):\n         r\"\"\"\n         Examples:\n@@ -786,9 +791,8 @@ def forward(\n         ```\"\"\"\n         return super().forward(\n             pixel_values=pixel_values,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n             interpolate_pos_encoding=interpolate_pos_encoding,\n+            **kwargs,\n         )\n \n "
        }
    ],
    "stats": {
        "total": 440,
        "additions": 139,
        "deletions": 301
    }
}