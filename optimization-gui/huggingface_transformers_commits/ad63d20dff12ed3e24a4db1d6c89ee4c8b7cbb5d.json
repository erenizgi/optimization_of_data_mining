{
    "author": "jiqing-feng",
    "message": "fix whisper re-compile (#36712)\n\n* fix whisper re-compile\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* fix copy\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* fix comment\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* fix copies\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* revert useless changes\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n---------\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>",
    "sha": "ad63d20dff12ed3e24a4db1d6c89ee4c8b7cbb5d",
    "files": [
        {
            "sha": "e40208a65b65bf08304e17f0ca9e098af2fe4b89",
            "filename": "src/transformers/models/blenderbot/modeling_blenderbot.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad63d20dff12ed3e24a4db1d6c89ee4c8b7cbb5d/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad63d20dff12ed3e24a4db1d6c89ee4c8b7cbb5d/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py?ref=ad63d20dff12ed3e24a4db1d6c89ee4c8b7cbb5d",
            "patch": "@@ -320,9 +320,7 @@ def forward(\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n         hidden_states = residual + hidden_states\n \n-        if hidden_states.dtype == torch.float16 and (\n-            torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any()\n-        ):\n+        if hidden_states.dtype == torch.float16:\n             clamp_value = torch.finfo(hidden_states.dtype).max - 1000\n             hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n "
        },
        {
            "sha": "e16a299dfcd21bba6a3f5e55757b463e3276d633",
            "filename": "src/transformers/models/m2m_100/modeling_m2m_100.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad63d20dff12ed3e24a4db1d6c89ee4c8b7cbb5d/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad63d20dff12ed3e24a4db1d6c89ee4c8b7cbb5d/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py?ref=ad63d20dff12ed3e24a4db1d6c89ee4c8b7cbb5d",
            "patch": "@@ -631,9 +631,7 @@ def forward(\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n         hidden_states = residual + hidden_states\n \n-        if hidden_states.dtype == torch.float16 and (\n-            torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any()\n-        ):\n+        if hidden_states.dtype == torch.float16:\n             clamp_value = torch.finfo(hidden_states.dtype).max - 1000\n             hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n "
        },
        {
            "sha": "c1cd512f85b6d651f51b87dd60f945894b410468",
            "filename": "src/transformers/models/mbart/modeling_mbart.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad63d20dff12ed3e24a4db1d6c89ee4c8b7cbb5d/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad63d20dff12ed3e24a4db1d6c89ee4c8b7cbb5d/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py?ref=ad63d20dff12ed3e24a4db1d6c89ee4c8b7cbb5d",
            "patch": "@@ -580,9 +580,7 @@ def forward(\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n         hidden_states = residual + hidden_states\n \n-        if hidden_states.dtype == torch.float16 and (\n-            torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any()\n-        ):\n+        if hidden_states.dtype == torch.float16:\n             clamp_value = torch.finfo(hidden_states.dtype).max - 1000\n             hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n "
        },
        {
            "sha": "32e105053bcbf86c77c3cbec034a429222cffd63",
            "filename": "src/transformers/models/pegasus/modeling_pegasus.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad63d20dff12ed3e24a4db1d6c89ee4c8b7cbb5d/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad63d20dff12ed3e24a4db1d6c89ee4c8b7cbb5d/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py?ref=ad63d20dff12ed3e24a4db1d6c89ee4c8b7cbb5d",
            "patch": "@@ -321,9 +321,7 @@ def forward(\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n         hidden_states = residual + hidden_states\n \n-        if hidden_states.dtype == torch.float16 and (\n-            torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any()\n-        ):\n+        if hidden_states.dtype == torch.float16:\n             clamp_value = torch.finfo(hidden_states.dtype).max - 1000\n             hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n "
        },
        {
            "sha": "8a9122c487089b90aa8d3ca9eb4ef941123c10d2",
            "filename": "src/transformers/models/qwen2_audio/modeling_qwen2_audio.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad63d20dff12ed3e24a4db1d6c89ee4c8b7cbb5d/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad63d20dff12ed3e24a4db1d6c89ee4c8b7cbb5d/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py?ref=ad63d20dff12ed3e24a4db1d6c89ee4c8b7cbb5d",
            "patch": "@@ -427,9 +427,7 @@ def forward(\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n         hidden_states = residual + hidden_states\n \n-        if hidden_states.dtype == torch.float16 and (\n-            torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any()\n-        ):\n+        if hidden_states.dtype == torch.float16:\n             clamp_value = torch.finfo(hidden_states.dtype).max - 1000\n             hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n "
        },
        {
            "sha": "fd52e64a1065c54050b55a72c6b9fa6b9491892f",
            "filename": "src/transformers/models/speech_to_text/modeling_speech_to_text.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad63d20dff12ed3e24a4db1d6c89ee4c8b7cbb5d/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad63d20dff12ed3e24a4db1d6c89ee4c8b7cbb5d/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py?ref=ad63d20dff12ed3e24a4db1d6c89ee4c8b7cbb5d",
            "patch": "@@ -386,9 +386,7 @@ def forward(\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n         hidden_states = residual + hidden_states\n \n-        if hidden_states.dtype == torch.float16 and (\n-            torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any()\n-        ):\n+        if hidden_states.dtype == torch.float16:\n             clamp_value = torch.finfo(hidden_states.dtype).max - 1000\n             hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n "
        },
        {
            "sha": "5f7cf59e8e9ffff5e4914cf502ef0a7828d7d691",
            "filename": "src/transformers/models/whisper/modeling_whisper.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad63d20dff12ed3e24a4db1d6c89ee4c8b7cbb5d/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad63d20dff12ed3e24a4db1d6c89ee4c8b7cbb5d/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py?ref=ad63d20dff12ed3e24a4db1d6c89ee4c8b7cbb5d",
            "patch": "@@ -637,9 +637,7 @@ def forward(\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n         hidden_states = residual + hidden_states\n \n-        if hidden_states.dtype == torch.float16 and (\n-            torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any()\n-        ):\n+        if hidden_states.dtype == torch.float16:\n             clamp_value = torch.finfo(hidden_states.dtype).max - 1000\n             hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n "
        }
    ],
    "stats": {
        "total": 28,
        "additions": 7,
        "deletions": 21
    }
}