{
    "author": "ArthurZucker",
    "message": "Refactor weight loading (#41580)\n\n* ah actually we don't discard lm head if missing -> needs to be moved to correct device and etc\n\n* fix some tests\n\n* small fixes\n\n* up\n\n* up\n\n* dik why we tie weights twice but,..,,.\n\n* ups\n\n* removeunused\n\n* fix hunyuan\n\n* small fix\n\n* nits\n\n* ish\n\n* up\n\n* rev\n\n* fix more tie weights keys\n\n* small fixes\n\n* nit\n\n* update\n\n* fix and fix\n\n* fix a test\n\n* glubs\n\n* current shitty changes\n\n* ship validated ones\n\n* more\n\n* more update\n\n* more\n\n* more\n\n* more\n\n* mllama\n\n* more up\n\n* fix ernie\n\n* fix xopies\n\n* up more\n\n* more fixes\n\n* up\n\n* up\n\n* fix-copies\n\n* fix more\n\n* more updates\n\n* AI UPDATE\n\n* up\n\n* hoey\n\n* make it fast\n\n* fix\n\n* lol\n\n* fix asjusting\n\n* more fixes\n\n* _dtype nit\n\n* up\n\n* nit\n\n* update\n\n* update\n\n* remove semaphores\n\n* fix import to avoid jit execution\n\n* try to remove custom tiing logic when its stupid\n\n* fix more individual models\n\n* fix whisper as well\n\n* fix?\n\n* fox umt5\n\n* improve tqdm bar\n\n* cleanup a bit\n\n* oupsi\n\n* some updates\n\n* improve\n\n* remove all buffering -> much faster without it\n\n* remove some tie_weights custome funcs when not needed\n\n* more fixes related to strict matching regex\n\n* remove ALL custom tie weights\n\n* small update\n\n* revert change to init scheme (no need for params)\n\n* mixtral init\n\n* try less strict source check\n\n* tied weight first shot to the fiiiixxxxxx\n\n* does this help?\n\n* :)\n\n* fix some ppolry defined tied_weights_keys for now\n\n* subclass nn.Parameters\n\n* up\n\n* lol\n\n* Ouiiii\n\n* fix led\n\n* fix long cat flash\n\n* fix qwen and long cat flash\n\n* properly fix qwen init\n\n* just push this for now\n\n* propnet is dumb\n\n* update\n\n* push\n\n* remove explict sharing of some tied keys.\n\n* update decoder.bias\n\n* moe case\n\n* more changes to untangle old hardcoded ting\n\n* fixup\n\n* fix big faileurs\n\n* fix prophnet\n\n* fix resize token embeddings\n\n* nits\n\n* fix xcodex\n\n* asyncio?\n\n* fix smart apply\n\n* fix data-2-vec\n\n* [build-ci-image]\n\n* checkout\n\n* uupdate\n\n* fix hunyuan\n\n* update error message\n\n* fix deformable detr\n\n* fixes\n\n* fix init weights for non param gate up projs\n\n* shared todo?\n\n* update some models\n\n* big revert, don't break this behaviour\n\n* ty @SunMarc this fixes the buffers\n\nCo-authored-by: SunMarc <SunMarc@users.noreply.github.com>\n\n* mt5 fuck\n\n* fix lxmbert\n\n* nuke slow test fetcher\n\n* fix zamba and deepcopy for now\n\n* fix zamba tied weight keys! ~\n\n* fix-copies\n\n* update fetch terst\n\n* fix gradient for test modeling common!\n\n* break \"shared\" for now I will fix tomorrow changes are properly isoalted now :)\n\n* does this fix marian? probably not\n\n* fix some vlms\n\n* D fine seems to handle this well\n\n* glob is fine actually\n\n* fix dab detr\n\n* small steps\n\n* opusy\n\n* fix some more models?\n\n* yups\n\n* better erro\n\n* fix?\n\n* fix double escape\n\n* escape wehere it makes sense\n\n* ??\n\n* fix ibert\n\n* fix tvp as well\n\n* more fxes\n\n* try always download ref PR\n\n* ONONONO\n\n* big fixup\n\n* more fixup\n\n* small step\n\n* small nits\n\n* nits\n\n* brut force some stuff\n\n* fix vilt\n\n* make sure special models that always need tie always tie\n\n* cleaning up\n\n* small nits\n\n* fix zamba and bridge tower!\n\n* just fixup\n\n* potential culprits\n\n* revert bark and fix bridgetower\n\n* remove now non existant tie_weights\n\n* ?\n\n* lol reformer actually had nothing tied!\n\n* wow these two fucking models were really not well made\n\n* fix sam family!\n\n* fix bark revision\n\n* fix speech2test ?\n\n* push this for now....\n\n* upsy\n\n* the fuck\n\n* fix rtdetr\n\n* update\n\n* proper\n\n* wow that one 's annoying\n\n* update\n\n* try to find the culprit\n\n* get some help on common\n\n* nit about general init and cls.padding_idx\n\n* revert num workers update\n\n* remove old loading func\n\n* fix glob\n\n* add annotations\n\n* fix re\n\n* small improvements\n\n* clean some stuff\n\n* improvements\n\n* someone did not understannnnnnd what I tried to dooo or does BNB not support that either?\n\n* gluos\n\n* fix case when `.` is just not there\n\n* remove unused arg\n\n* recover orignal parameter/buffer using _original\n\n* fix glob issu\n\n* this?\n\n* deepspeed best-effort\n\n* remove unused stuff\n\n* Update tie weight keys as they were just wroong\n\nCo-authored-by: Benjamin Bossan <benjaminbossan@users.noreply.github.com>\"\n\n* up\n\n* augustuc clauss, a gloubs gloups gloubs\n\n* fixup\n\n* fixup\n\n* there was fucking typo\n\n* mrain\n\n* nits\n\n* fix marian 3 remaining tests\n\n* one more\n\n* fix some of the copies, not all :)\n\n* small cleanup\n\n* one propertest\n\n* fix core model loadig tes\n\n* attempt a new test\n\n* fix some of the annoying tests by supporting reading .bin sometimes\n\n* push\n\n* push more small fixes\n\n* remove 1 useless test\n\n* up\n\n* fix audio flamingo post rebase\n\n* fixup\n\n* some small updatess\n\n* fix sam models\n\n* nits\n\n* up\n\n* updates\n\n* onem ore\n\n* skip this stupid test\n\n* some other fixes\n\n* fixup\n\n* update\n\n* skip more offloaded stuff\n\n* oups\n\n* ups\n\n* update mixtral\n\n* skip this one\n\n* LET\"SGO\n\n* fixup\n\n* rope delta order\n\n* fix csm\n\n* small nit\n\n---------\n\nCo-authored-by: Cyril Vallez <cyril.vallez@gmail.com>\nCo-authored-by: SunMarc <SunMarc@users.noreply.github.com>\nCo-authored-by: Marc Sun <marc@huggingface.co>",
    "sha": "6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
    "files": [
        {
            "sha": "656902b92dd063a55b57d1756d14d520522c29fd",
            "filename": ".circleci/config.yml",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/.circleci%2Fconfig.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/.circleci%2Fconfig.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.circleci%2Fconfig.yml?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -46,8 +46,8 @@ jobs:\n             - run: uv pip install -U -e .\n             - run: echo 'export \"GIT_COMMIT_MESSAGE=$(git show -s --format=%s)\"' >> \"$BASH_ENV\" && source \"$BASH_ENV\"\n             - run: mkdir -p test_preparation\n-            - run: python utils/tests_fetcher.py | tee tests_fetched_summary.txt\n-            - run: python utils/tests_fetcher.py --filter_tests\n+            - run: python utils/tests_fetcher.py | tee tests_fetched_summary.txt || true\n+            - run: python utils/tests_fetcher.py --filter_tests || true\n             - run: export \"GIT_COMMIT_MESSAGE=$(git show -s --format=%s)\" && echo $GIT_COMMIT_MESSAGE && python .circleci/create_circleci_config.py --fetcher_folder test_preparation\n             - run: |\n                 if [ ! -s test_preparation/generated_config.yml ]; then\n@@ -98,8 +98,8 @@ jobs:\n             - run: uv pip install -U -e .\n             - run: echo 'export \"GIT_COMMIT_MESSAGE=$(git show -s --format=%s)\"' >> \"$BASH_ENV\" && source \"$BASH_ENV\"\n             - run: mkdir -p test_preparation\n-            - run: python utils/tests_fetcher.py --fetch_all | tee tests_fetched_summary.txt\n-            - run: python utils/tests_fetcher.py --filter_tests\n+            - run: python utils/tests_fetcher.py --fetch_all | tee tests_fetched_summary.txt || true\n+            - run: python utils/tests_fetcher.py --filter_tests || true\n             - run: export \"GIT_COMMIT_MESSAGE=$(git show -s --format=%s)\" && echo $GIT_COMMIT_MESSAGE && python .circleci/create_circleci_config.py --fetcher_folder test_preparation\n             - run: |\n                 if [ ! -s test_preparation/generated_config.yml ]; then"
        },
        {
            "sha": "591fd5b6387b54d8ce7ea6582b2e0a5bfa143ad9",
            "filename": "Makefile",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/Makefile",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/Makefile",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/Makefile?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -45,6 +45,7 @@ repo-consistency:\n \tpython utils/check_modular_conversion.py\n \tpython utils/check_dummies.py\n \tpython utils/check_repo.py\n+\tpython utils/check_init_weights_data.py\n \tpython utils/check_inits.py\n \tpython utils/check_pipeline_typing.py\n \tpython utils/check_config_docstrings.py"
        },
        {
            "sha": "8f19517819b99d31ed5be7d0dac32d860b09a57e",
            "filename": "docs/source/de/add_new_model.md",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/docs%2Fsource%2Fde%2Fadd_new_model.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/docs%2Fsource%2Fde%2Fadd_new_model.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fde%2Fadd_new_model.md?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -508,16 +508,16 @@ BERT `_init_weights` Methode:\n def _init_weights(self, module):\n     \"\"\"Initialize the weights\"\"\"\n     if isinstance(module, nn.Linear):\n-        module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+        module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n         if module.bias is not None:\n-            module.bias.data.zero_()\n+            module.bias.zero_()\n     elif isinstance(module, nn.Embedding):\n-        module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+        module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n         if module.padding_idx is not None:\n             module.weight.data[module.padding_idx].zero_()\n     elif isinstance(module, nn.LayerNorm):\n-        module.bias.data.zero_()\n-        module.weight.data.fill_(1.0)\n+        module.bias.zero_()\n+        module.weight.fill_(1.0)\n ```\n \n Sie können weitere benutzerdefinierte Schemata verwenden, wenn Sie eine spezielle Initialisierung für einige Module benötigen. Zum Beispiel in\n@@ -533,9 +533,9 @@ def _init_weights(self, module):\n         module.project_hid._is_hf_initialized = True\n         module.project_q._is_hf_initialized = True\n     elif isinstance(module, nn.Linear):\n-        module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+        module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n         if module.bias is not None:\n-            module.bias.data.zero_()\n+            module.bias.zero_()\n ```\n \n Das Flag `_is_hf_initialized` wird intern verwendet, um sicherzustellen, dass wir ein Submodul nur einmal initialisieren. Wenn Sie es auf"
        },
        {
            "sha": "2cd88930fbbc75b18d7c207cfe09f3cb9aa9ba70",
            "filename": "docs/source/en/add_new_model.md",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/docs%2Fsource%2Fen%2Fadd_new_model.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/docs%2Fsource%2Fen%2Fadd_new_model.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fadd_new_model.md?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -314,16 +314,16 @@ Random initialization occurs in the `_init_weights` method of `BrandNewLlamaPreT\n def _init_weights(self, module):\n     \"\"\"Initialize the weights\"\"\"\n     if isinstance(module, nn.Linear):\n-        module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+        module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n         if module.bias is not None:\n-            module.bias.data.zero_()\n+            module.bias.zero_()\n     elif isinstance(module, nn.Embedding):\n-        module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+        module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n         if module.padding_idx is not None:\n             module.weight.data[module.padding_idx].zero_()\n     elif isinstance(module, nn.LayerNorm):\n-        module.bias.data.zero_()\n-        module.weight.data.fill_(1.0)\n+        module.bias.zero_()\n+        module.weight.fill_(1.0)\n ```\n \n The initialization scheme can look different if you need to adapt it to your model. For example, [`Wav2Vec2ForPreTraining`] initializes [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) in its last two linear layers.\n@@ -339,9 +339,9 @@ def _init_weights(self, module):\n         module.project_hid._is_hf_initialized = True\n         module.project_q._is_hf_initialized = True\n     elif isinstance(module, nn.Linear):\n-        module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+        module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n         if module.bias is not None:\n-            module.bias.data.zero_()\n+            module.bias.zero_()\n ```\n \n ### Convert checkpoints to Transformers"
        },
        {
            "sha": "893dd28d7b454162242e2a9237db5510d6799e76",
            "filename": "docs/source/en/perf_infer_gpu_multi.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/docs%2Fsource%2Fen%2Fperf_infer_gpu_multi.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/docs%2Fsource%2Fen%2Fperf_infer_gpu_multi.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_infer_gpu_multi.md?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -149,7 +149,7 @@ The example below packs `up_proj` and `gate_proj` into a single `gate_up_proj` m\n ```python\n class Llama4TextExperts(nn.Module):\n     ...\n-    self.gate_up_proj = nn.Parameter(torch.empty(self.num_experts, self.hidden_size, 2 * self.expert_dim))\n+    self.gate_up_proj = nn.Parameter(torch.zeros(self.num_experts, self.hidden_size, 2 * self.expert_dim))\n ```\n \n Batch matrix multiplication can be used in the `forward` pass to compute the output of the `gate_up_proj` module."
        },
        {
            "sha": "c4a8573af49c5cd6f37752a5e06f8f3d95f856ee",
            "filename": "docs/source/it/migration.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/docs%2Fsource%2Fit%2Fmigration.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/docs%2Fsource%2Fit%2Fmigration.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fit%2Fmigration.md?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -170,7 +170,7 @@ Per quanto riguarda la classe `TrainingArguments`:\n - L'argomento `evaluate_during_training` di `TrainingArguments` è deprecato a favore di `eval_strategy`.\n \n Per quanto riguarda il modello Transfo-XL:\n-- L'attributo di configurazione `tie_weight` di Transfo-XL diventa `tie_words_embeddings`.\n+- L'attributo di configurazione `tie_weight` di Transfo-XL diventa `tie_word_embeddings`.\n - Il metodo di modellazione `reset_length` di Transfo-XL diventa `reset_memory_length`.\n \n Per quanto riguarda le pipeline:"
        },
        {
            "sha": "f768c094a084bb4244a99fd115c39dcb32929bd2",
            "filename": "docs/source/ja/add_new_model.md",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/docs%2Fsource%2Fja%2Fadd_new_model.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/docs%2Fsource%2Fja%2Fadd_new_model.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fadd_new_model.md?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -406,16 +406,16 @@ model = BrandNewBertModel(BrandNewBertConfig())\n def _init_weights(self, module):\n     \"\"\"Initialize the weights\"\"\"\n     if isinstance(module, nn.Linear):\n-        module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+        module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n         if module.bias is not None:\n-            module.bias.data.zero_()\n+            module.bias.zero_()\n     elif isinstance(module, nn.Embedding):\n-        module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+        module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n         if module.padding_idx is not None:\n             module.weight.data[module.padding_idx].zero_()\n     elif isinstance(module, nn.LayerNorm):\n-        module.bias.data.zero_()\n-        module.weight.data.fill_(1.0)\n+        module.bias.zero_()\n+        module.weight.fill_(1.0)\n ```\n \n 特定のモジュールに特別な初期化が必要な場合、カスタムスキームをさらに持つことができます。たとえば、\n@@ -431,9 +431,9 @@ def _init_weights(self, module):\n         module.project_hid._is_hf_initialized = True\n         module.project_q._is_hf_initialized = True\n     elif isinstance(module, nn.Linear):\n-        module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+        module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n         if module.bias is not None:\n-            module.bias.data.zero_()\n+            module.bias.zero_()\n ```\n \n `_is_hf_initialized`フラグは、サブモジュールを一度だけ初期化することを確実にするために内部で使用されます。"
        },
        {
            "sha": "be33c92dc4b039e7015cefb73f4857f1871a7090",
            "filename": "docs/source/ko/add_new_model.md",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/docs%2Fsource%2Fko%2Fadd_new_model.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/docs%2Fsource%2Fko%2Fadd_new_model.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fadd_new_model.md?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -348,16 +348,16 @@ model = BrandNewBertModel(BrandNewBertConfig())\n def _init_weights(self, module):\n     \"\"\"Initialize the weights\"\"\"\n     if isinstance(module, nn.Linear):\n-        module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+        module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n         if module.bias is not None:\n-            module.bias.data.zero_()\n+            module.bias.zero_()\n     elif isinstance(module, nn.Embedding):\n-        module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+        module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n         if module.padding_idx is not None:\n             module.weight.data[module.padding_idx].zero_()\n     elif isinstance(module, nn.LayerNorm):\n-        module.bias.data.zero_()\n-        module.weight.data.fill_(1.0)\n+        module.bias.zero_()\n+        module.weight.fill_(1.0)\n ```\n \n 몇 가지 모듈에 대해 특별한 초기화가 필요한 경우 사용자 정의 방식을 사용할 수도 있습니다. 예를 들어, `Wav2Vec2ForPreTraining`에서 마지막 두 개의 선형 레이어는 일반적인 PyTorch `nn.Linear`의 초기화를 가져야 하지만, 다른 모든 레이어는 위와 같은 초기화를 사용해야 합니다. 이는 다음과 같이 코드화됩니다:\n@@ -371,9 +371,9 @@ def _init_weights(self, module):\n         module.project_hid._is_hf_initialized = True\n         module.project_q._is_hf_initialized = True\n     elif isinstance(module, nn.Linear):\n-        module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+        module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n         if module.bias is not None:\n-            module.bias.data.zero_()\n+            module.bias.zero_()\n ```\n \n `_is_hf_initialized` 플래그는 서브모듈을 한 번만 초기화하도록 내부적으로 사용됩니다. `module.project_q` 및 `module.project_hid`에 대해 `True`로 설정함으로써, 우리가 수행한 사용자 정의 초기화가 이후에 덮어쓰이지 않도록 합니다. 즉, `_init_weights` 함수가 이들에게 적용되지 않습니다."
        },
        {
            "sha": "676ed59800352edd1d0f382cd71bb41fd716dd69",
            "filename": "docs/source/ko/perf_infer_gpu_multi.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/docs%2Fsource%2Fko%2Fperf_infer_gpu_multi.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/docs%2Fsource%2Fko%2Fperf_infer_gpu_multi.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fperf_infer_gpu_multi.md?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -152,7 +152,7 @@ class ParallelInterface(MutableMapping):\n ```python\n class Llama4TextExperts(nn.Module):\n     ...\n-    self.gate_up_proj = nn.Parameter(torch.empty(self.num_experts, self.hidden_size, 2 * self.expert_dim))\n+    self.gate_up_proj = nn.Parameter(torch.zeros(self.num_experts, self.hidden_size, 2 * self.expert_dim))\n ```\n \n 배치 행렬 곱셈을 `forward` 패스에서 사용하여 `gate_up_proj` 모듈의 출력을 계산할 수 있습니다."
        },
        {
            "sha": "15c96bf7bbc8e7cb113eaca27475cd02bb16618b",
            "filename": "examples/modular-transformers/modeling_dummy_bert.py",
            "status": "modified",
            "additions": 7,
            "deletions": 13,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/examples%2Fmodular-transformers%2Fmodeling_dummy_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/examples%2Fmodular-transformers%2Fmodeling_dummy_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_dummy_bert.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -502,16 +502,10 @@ def __init__(self, config):\n \n         # The output weights are the same as the input embeddings, but there is\n         # an output-only bias for each token.\n-        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n+        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=True)\n \n         self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n \n-        # Need a link between the two variables so that the bias is correctly resized with `resize_token_embeddings`\n-        self.decoder.bias = self.bias\n-\n-    def _tie_weights(self):\n-        self.decoder.bias = self.bias\n-\n     def forward(self, hidden_states):\n         hidden_states = self.transform(hidden_states)\n         hidden_states = self.decoder(hidden_states)\n@@ -536,18 +530,18 @@ class DummyBertPreTrainedModel(PreTrainedModel):\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n         elif isinstance(module, DummyBertLMPredictionHead):\n-            module.bias.data.zero_()\n+            module.bias.zero_()\n \n \n @auto_docstring("
        },
        {
            "sha": "440878c3df494ad4c5e71a552a5c84dcc5a10f97",
            "filename": "examples/modular-transformers/modeling_my_new_model2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -265,7 +265,7 @@ def _init_weights(self, module):\n \n         # We initialize with 0s to be 1 centered as the RMSNorm here does (1 + weight)\n         if \"RMSNorm\" in module.__class__.__name__:\n-            module.weight.data.zero_()\n+            module.weight.zero_()\n \n \n class MyNewModel2ForSequenceClassification(GenericForSequenceClassification, MyNewModel2PreTrainedModel):"
        },
        {
            "sha": "041f1d4a0422da480457750518ec2a8212bdec93",
            "filename": "examples/modular-transformers/modeling_new_task_model.py",
            "status": "modified",
            "additions": 12,
            "deletions": 4,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -104,9 +104,9 @@ def _init_weights(self, module):\n         std = getattr(self.config, \"initializer_range\", self.config.get_text_config().initializer_range)\n \n         if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n \n \n def token_type_ids_mask_function(\n@@ -428,7 +428,7 @@ class NewTaskModelForNewTask(NewTaskModelPreTrainedModel, GenerationMixin):\n         \"^multi_modal_projector\": \"model.multi_modal_projector\",\n         \"^language_model.lm_head\": \"lm_head\",\n     }\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.language_model.embed_tokens.weight\"}\n     main_input_name: ClassVar[str] = \"doc_input_ids\"  # transformers-related\n \n     def __init__(self, config):\n@@ -440,7 +440,15 @@ def __init__(self, config):\n         self.custom_text_proj = nn.Linear(self.config.text_config.hidden_size, self.embedding_dim)\n \n         if self.language_model._tied_weights_keys is not None:\n-            self._tied_weights_keys = [f\"model.language_model.{k}\" for k in self.language_model._tied_weights_keys]\n+            prefix = \"model.language_model.\"\n+            prefixed_mapping = {\n+                f\"{prefix}{target}\": f\"{prefix}{source}\"\n+                for target, source in self.language_model._tied_weights_keys.items()\n+            }\n+            if isinstance(self._tied_weights_keys, dict):\n+                self._tied_weights_keys.update(prefixed_mapping)\n+            else:\n+                self._tied_weights_keys = prefixed_mapping\n         self.post_init()\n \n     def get_input_embeddings(self):"
        },
        {
            "sha": "b1f35119580b9ba7fc9609682ab7f37e73ff6c0f",
            "filename": "examples/modular-transformers/modeling_roberta.py",
            "status": "modified",
            "additions": 7,
            "deletions": 13,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/examples%2Fmodular-transformers%2Fmodeling_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/examples%2Fmodular-transformers%2Fmodeling_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_roberta.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -505,16 +505,10 @@ def __init__(self, config):\n \n         # The output weights are the same as the input embeddings, but there is\n         # an output-only bias for each token.\n-        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n+        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=True)\n \n         self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n \n-        # Need a link between the two variables so that the bias is correctly resized with `resize_token_embeddings`\n-        self.decoder.bias = self.bias\n-\n-    def _tie_weights(self):\n-        self.decoder.bias = self.bias\n-\n     def forward(self, hidden_states):\n         hidden_states = self.transform(hidden_states)\n         hidden_states = self.decoder(hidden_states)\n@@ -539,18 +533,18 @@ class RobertaPreTrainedModel(PreTrainedModel):\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n         elif isinstance(module, RobertaLMPredictionHead):\n-            module.bias.data.zero_()\n+            module.bias.zero_()\n \n \n @auto_docstring("
        },
        {
            "sha": "6f88e341a03285efaf43063bf75cfd73a7afb627",
            "filename": "examples/modular-transformers/modeling_test_detr.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/examples%2Fmodular-transformers%2Fmodeling_test_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/examples%2Fmodular-transformers%2Fmodeling_test_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_test_detr.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -846,11 +846,11 @@ def _init_weights(self, module):\n             nn.init.xavier_uniform_(module.output_proj.weight.data)\n             nn.init.constant_(module.output_proj.bias.data, 0.0)\n         elif isinstance(module, (nn.Linear, nn.Conv2d, nn.BatchNorm2d)):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n         if hasattr(module, \"reference_points\") and not self.config.two_stage:"
        },
        {
            "sha": "43830b12c784f28a68e602a0108847ec8aef80ee",
            "filename": "examples/modular-transformers/modular_new_task_model.py",
            "status": "modified",
            "additions": 9,
            "deletions": 1,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/examples%2Fmodular-transformers%2Fmodular_new_task_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/examples%2Fmodular-transformers%2Fmodular_new_task_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodular_new_task_model.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -19,7 +19,15 @@ def __init__(self, config):\n         self.custom_text_proj = nn.Linear(self.config.text_config.hidden_size, self.embedding_dim)\n \n         if self.language_model._tied_weights_keys is not None:\n-            self._tied_weights_keys = [f\"model.language_model.{k}\" for k in self.language_model._tied_weights_keys]\n+            prefix = \"model.language_model.\"\n+            prefixed_mapping = {\n+                f\"{prefix}{target}\": f\"{prefix}{source}\"\n+                for target, source in self.language_model._tied_weights_keys.items()\n+            }\n+            if isinstance(self._tied_weights_keys, dict):\n+                self._tied_weights_keys.update(prefixed_mapping)\n+            else:\n+                self._tied_weights_keys = prefixed_mapping\n \n         self.post_init()\n "
        },
        {
            "sha": "f94b4b0c5aa4a62a70965830ab425a052a1a5f8c",
            "filename": "src/transformers/configuration_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconfiguration_utils.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -876,7 +876,7 @@ def to_diff_dict(self) -> dict[str, Any]:\n         if hasattr(self, \"quantization_config\"):\n             serializable_config_dict[\"quantization_config\"] = (\n                 self.quantization_config.to_dict()\n-                if not isinstance(self.quantization_config, dict)\n+                if not isinstance(self.quantization_config, dict) and self.quantization_config is not None\n                 else self.quantization_config\n             )\n         self.dict_dtype_to_str(serializable_config_dict)\n@@ -910,7 +910,7 @@ def to_dict(self) -> dict[str, Any]:\n         if hasattr(self, \"quantization_config\"):\n             output[\"quantization_config\"] = (\n                 self.quantization_config.to_dict()\n-                if not isinstance(self.quantization_config, dict)\n+                if not isinstance(self.quantization_config, dict) and self.quantization_config is not None\n                 else self.quantization_config\n             )\n         self.dict_dtype_to_str(output)"
        },
        {
            "sha": "636a872487e55a53b4a9d96712432d157c34a3ed",
            "filename": "src/transformers/conversion_mapping.py",
            "status": "added",
            "additions": 136,
            "deletions": 0,
            "changes": 136,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fconversion_mapping.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fconversion_mapping.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconversion_mapping.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -0,0 +1,136 @@\n+# coding=utf-8\n+# Copyright (C) 2025 the HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from copy import deepcopy\n+\n+from .core_model_loading import Concatenate, MergeModulelist, WeightConverter\n+from .utils import is_torch_available\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+\n+def _build_checkpoint_conversion_mapping():\n+    mapping = {\n+        \"mixtral\": [\n+            WeightConverter(\n+                source_keys=[\n+                    \"block_sparse_moe.experts.*.w1.weight\",\n+                    \"block_sparse_moe.experts.*.w3.weight\",\n+                ],  # you give me a list of 2 keys, I collect a list of a list of tensors\n+                target_keys=\"mlp.experts.gate_up_proj\",  # target key gets the list of two tensors\n+                operations=[\n+                    MergeModulelist(\n+                        dim=0\n+                    ),  # each process has two lists of tensors, we cat each list. -> we end up with 2 tensors\n+                    Concatenate(dim=1),  # each process has 2 tensors, gate and up, we concat them into gate_up\n+                ],  # we want the loading to add this shard operation here. Though we can't shard after concats and merge, needs to be first\n+            ),\n+            WeightConverter(\n+                source_keys=[\n+                    \"block_sparse_moe.experts.*.w2.weight\",\n+                ],\n+                target_keys=\"mlp.experts.down_proj\",  # target key gets the list of two tensors\n+                operations=[\n+                    MergeModulelist(\n+                        dim=0\n+                    ),  # each process has two lists of tensors, we cat each list. -> we end up with 2 tensors\n+                ],  # we want the loading to add this shard operation here. Though we can't shard after concats and merge, needs to be first\n+            ),\n+            # WeightConverter(\n+            #     [\"self_attn.q_proj\", \"self_attn.k_proj\", \"self_attn.v_proj\"],\n+            #     \"self_attn.qkv_proj\",\n+            #     operations=[Concatenate(dim=0)],  # more like stack?\n+            # ),\n+            WeightConverter(\"*.block_sparse_moe.\", \"*.mlp.\"),\n+        ],\n+        \"qwen2_moe\": [\n+            WeightConverter(\n+                source_keys=[\n+                    \"mlp.experts.*.gate_proj.weight\",\n+                    \"mlp.experts.*.up_proj.weight\",\n+                ],\n+                target_keys=\"mlp.experts.gate_up_proj\",\n+                operations=[MergeModulelist(dim=0), Concatenate(dim=1)],\n+            ),\n+            WeightConverter(\n+                source_keys=[\"mlp.experts.*.down_proj.weight\"],\n+                target_keys=\"mlp.experts.down_proj\",\n+                operations=[MergeModulelist(dim=0)],\n+            ),\n+        ],\n+        \"legacy\": [\n+            WeightConverter(\n+                source_keys=\"LayerNorm.gamma\",\n+                target_keys=\"LayerNorm.weight\",\n+            ),\n+            WeightConverter(\n+                source_keys=\"LayerNorm.beta\",\n+                target_keys=\"LayerNorm.bias\",\n+            ),\n+        ],\n+    }\n+    if hasattr(torch.nn.utils.parametrizations, \"weight_norm\"):\n+        mapping[\"legacy\"] += [\n+            WeightConverter(\n+                source_keys=\"weight_g\",\n+                target_keys=\"parametrizations.weight.original0\",\n+            ),\n+            WeightConverter(\n+                source_keys=\"weight_v\",\n+                target_keys=\"parametrizations.weight.original1\",\n+            ),\n+        ]\n+    else:\n+        mapping[\"legacy\"] += [\n+            WeightConverter(\n+                source_keys=\"parametrizations.weight.original0\",\n+                target_keys=\"weight_g\",\n+            ),\n+            WeightConverter(\n+                source_keys=\"parametrizations.weight.original1\",\n+                target_keys=\"weight_v\",\n+            ),\n+        ]\n+\n+    mapping[\"phimoe\"] = mapping[\"mixtral\"].copy()\n+    mapping[\"deepseek_v2\"] = mapping[\"qwen2_moe\"].copy()\n+    mapping[\"deepseek_v3\"] = mapping[\"qwen2_moe\"].copy()\n+    mapping[\"dot1\"] = mapping[\"qwen2_moe\"].copy()\n+    mapping[\"ernie_4_5_moe\"] = mapping[\"qwen2_moe\"].copy()\n+    mapping[\"glm4_moe\"] = mapping[\"qwen2_moe\"].copy()\n+    mapping[\"glm4v_moe\"] = mapping[\"qwen2_moe\"].copy()\n+    mapping[\"jamba\"] = mapping[\"qwen2_moe\"].copy()\n+    mapping[\"lfm2_moe\"] = mapping[\"mixtral\"].copy()\n+    mapping[\"long_cat_flash\"] = mapping[\"qwen2_moe\"].copy()\n+    mapping[\"qwen3_moe\"] = mapping[\"qwen2_moe\"].copy()\n+    mapping[\"qwen3_omni_moe\"] = mapping[\"qwen2_moe\"].copy()\n+    mapping[\"qwen3_next\"] = mapping[\"qwen2_moe\"].copy()\n+    mapping[\"qwen3_vl_moe\"] = mapping[\"qwen2_moe\"].copy()\n+    mapping[\"hunyuan_v1_moe\"] = mapping[\"qwen2_moe\"].copy()\n+    mapping[\"minimax\"] = mapping[\"mixtral\"].copy()\n+\n+    return mapping\n+\n+\n+_checkpoint_conversion_mapping_cache = None\n+\n+\n+def get_checkpoint_conversion_mapping(model_type):\n+    global _checkpoint_conversion_mapping_cache\n+    _checkpoint_conversion_mapping_cache = _build_checkpoint_conversion_mapping()\n+    globals()[\"_checkpoint_conversion_mapping\"] = _checkpoint_conversion_mapping_cache\n+    return deepcopy(_checkpoint_conversion_mapping_cache.get(model_type, None))"
        },
        {
            "sha": "72eb613753aa01f96e00fbbf94597355f26eed8c",
            "filename": "src/transformers/core_model_loading.py",
            "status": "added",
            "additions": 732,
            "deletions": 0,
            "changes": 732,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fcore_model_loading.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fcore_model_loading.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcore_model_loading.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -0,0 +1,732 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Core helpers for loading model checkpoints.\"\"\"\n+\n+from __future__ import annotations\n+\n+import itertools\n+import os\n+import re\n+from abc import abstractmethod\n+from collections import defaultdict\n+from collections.abc import MutableMapping, MutableSet, Sequence\n+from concurrent.futures import Future, ThreadPoolExecutor\n+from contextlib import contextmanager\n+from dataclasses import dataclass, field\n+from functools import partial\n+from types import MethodType\n+from typing import TYPE_CHECKING, Any, Optional, Union\n+\n+import torch\n+\n+from .integrations.tensor_parallel import ALL_PARALLEL_STYLES, DTensor, Replicate, TensorParallelLayer\n+from .utils import is_torch_greater_or_equal, logging\n+\n+\n+_torch_distributed_available = torch.distributed.is_available()\n+_is_dtensor_available = _torch_distributed_available and is_torch_greater_or_equal(\"2.5\")\n+if _is_dtensor_available:\n+    from torch.distributed.tensor import DTensor\n+\n+if TYPE_CHECKING:\n+    from .modeling_utils import PreTrainedModel\n+    from .quantizers import HfQuantizer\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+str_to_torch_dtype = {\n+    \"BOOL\": torch.bool,\n+    \"U8\": torch.uint8,\n+    \"I8\": torch.int8,\n+    \"I16\": torch.int16,\n+    \"F16\": torch.float16,\n+    \"BF16\": torch.bfloat16,\n+    \"I32\": torch.int32,\n+    \"F32\": torch.float32,\n+    \"F64\": torch.float64,\n+    \"I64\": torch.int64,\n+    \"F8_E4M3\": torch.float8_e4m3fn,\n+    \"F8_E5M2\": torch.float8_e5m2,\n+}\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+def _glob_to_regex_src(glob: str, *, digits_only: bool = True) -> str:\n+    \"\"\"\n+    Convert a glob with '*' into a regex *source* string. We don't use `glob.translate`\n+    '*' matches (\\\\d+) if digits_only else (.+). Inner groups are non-capturing.\n+    \"\"\"\n+    star = r\"(\\d+)\" if digits_only else r\"(.+)\"\n+    return glob.replace(r\"\\*\", star)\n+\n+\n+def build_glob_alt(\n+    globs: list[str],\n+) -> tuple[re.Pattern, dict[str, str]]:\n+    r\"\"\"\n+    Build one compiled regex alternation with a named group per glob. This allows to run a single\n+    re.match and get the correct group name to finally get which pattern matched.\n+    Returns (compiled_regex, name->glob map).\n+\n+    Example:\n+\n+    ```py\n+    >>> reg, map_ = build_glob_alt([\"mlp.*.w1\", \"mlp.*.w2\"])\n+    >>> print(reg)\n+    (re.compile(r'(?P<g0>.*mlp\\.(\\d+)\\.w1)|(?P<g1>.*mlp\\.(\\d+)\\.w2)', re.UNICODE),\n+    >>> print(map_)\n+    {'g0': 'mlp.*.w1', 'g1': 'mlp.*.w2'})\n+    >>> match_ = reg.match(\"model.layers.0.mlp.0.w1.weight\")\n+    >>> print(match_.lastgroup)\n+    'g0'\n+    >>> print(map_[match_.lastgroup])\n+    mlp.*.w1\n+    ```\n+    \"\"\"\n+    name_map: dict[str, str] = {}\n+    parts: list[str] = []\n+\n+    for i, g in enumerate(globs):\n+        name = f\"g{i}\"\n+        name_map[name] = g\n+        pat_src = _glob_to_regex_src(g)\n+        prefix_src = \"\"\n+        if pat_src.startswith(\"*\"):\n+            prefix_src = \".\"\n+        elif not pat_src.startswith(r\"\\^\") and not pat_src.startswith(r\".*\"):\n+            prefix_src = \".*\"\n+\n+        parts.append(f\"(?P<{name}>{prefix_src}{pat_src}.*)\")\n+\n+    alt_src = \"|\".join(parts).replace(\"\\\\^\", \"^\").replace(\"\\\\.\", r\"\\.\")\n+    try:\n+        reg = re.compile(alt_src)\n+    except re.error as e:\n+        logger.error(f\"Error compiling regex for alternation: {alt_src}\")\n+        raise e\n+\n+    return reg, name_map\n+\n+\n+def match_glob(key: str, alt: re.Pattern, name_map: dict[str, str]) -> Optional[str]:\n+    \"\"\"\n+    Match the key against the alternation; return the original glob string that matched.\n+    \"\"\"\n+    m = alt.match(key)\n+    if not m:\n+        return None\n+    return name_map.get(m.lastgroup)\n+\n+\n+class ConversionOps:\n+    \"\"\"Base class for weight conversion operations.\"\"\"\n+\n+    # The inverse operation class, will be used when saving the checkpoint\n+    reverse_op: type[ConversionOps]\n+\n+    @abstractmethod\n+    def convert(\n+        self, value: Union[dict[str, torch.Tensor], Sequence[torch.Tensor], torch.Tensor], *args, **kwargs\n+    ) -> torch.Tensor:\n+        raise NotImplementedError\n+\n+\n+class Chunk(ConversionOps):\n+    \"\"\"Split a tensor along ``dim`` into equally sized chunks or using explicit ``sizes``.\"\"\"\n+\n+    reverse_op: type[ConversionOps]\n+\n+    def __init__(self, dim: int = 0, chunks: Optional[int] = None, sizes: Optional[Sequence[int]] = None):\n+        if chunks is None and sizes is None:\n+            raise ValueError(\"`chunks` or `sizes` must be provided for Chunk operations.\")\n+        if chunks is not None and chunks <= 0:\n+            raise ValueError(\"`chunks` must be a strictly positive integer.\")\n+        self.dim = dim\n+        self.chunks = chunks\n+        self.sizes = list(sizes) if sizes is not None else None\n+        self.reverse_op = Concatenate\n+\n+    def convert(self, value: torch.Tensor, *args, **kwargs) -> list[torch.Tensor]:\n+        # chunk requires a single tensor input\n+        if len(value) != 1 or len(value[0]) != 1:\n+            raise ValueError(\"Chunk operation requires a single tensor input.\")\n+        return list(torch.chunk(value[0][0], self.chunks, dim=self.dim))\n+\n+\n+class Concatenate(ConversionOps):\n+    \"\"\"Concatenate tensors along `dim` using a reusable buffer.\"\"\"\n+\n+    reverse_op: type[ConversionOps]\n+\n+    def __init__(self, dim: int = 0):\n+        self.dim = dim\n+        self.reverse_op = Chunk\n+\n+    @torch.no_grad\n+    def convert(self, value: Sequence[torch.Tensor], *args, **kwargs) -> torch.Tensor:\n+        if isinstance(value[0], list):\n+            value = [v[0] for v in value]\n+        tensors = value\n+        if not tensors:\n+            raise ValueError(\"Fuse requires at least one tensor to concatenate.\")\n+\n+        return torch.cat(tuple(tensors), dim=self.dim)\n+\n+\n+class MergeModulelist(Concatenate):\n+    \"\"\"\n+    Merge a list of tensors into a single tensor along the first dimension.\n+    We explicitly define this because for EP or TP you want to make sure you know what you are doing!\n+\n+    \"\"\"\n+\n+    def __init__(self, dim: int = 0):\n+        super().__init__(dim=dim)\n+        self.reverse_op = SplitModulelist\n+\n+    @torch.no_grad\n+    def convert(self, value: Sequence[torch.Tensor], *args, **kwargs) -> list[torch.Tensor]:\n+        merged = []\n+        for group in value:\n+            if not isinstance(group, Sequence) or len(group) == 0:\n+                raise ValueError(\"MergeModulelist requires non-empty sub-sequences.\")\n+            group = [k for k in group if k.ndim]\n+            merged.append(torch.stack(group, dim=self.dim))\n+        return merged\n+\n+\n+class SplitModulelist(ConversionOps):\n+    \"\"\"Inverse of :class:`MergeModulelist` using explicit split sizes per group.\"\"\"\n+\n+    def __init__(self, sizes: Sequence[Sequence[int]], dim: int = 0):\n+        if not isinstance(sizes, Sequence) or not all(isinstance(sub, Sequence) and sub for sub in sizes):\n+            raise ValueError(\"`sizes` must be a sequence of non-empty sequences of integers.\")\n+        self.sizes = [list(sub) for sub in sizes]\n+        self.dim = dim\n+        self.reverse_op = MergeModulelist\n+\n+    @torch.no_grad\n+    def convert(self, value: Sequence[torch.Tensor], *, context: dict[str, Any]) -> list[list[torch.Tensor]]:\n+        if not isinstance(value, Sequence):\n+            raise TypeError(\"SplitModulelist expects a sequence of tensors.\")\n+        if len(value) != len(self.sizes):\n+            raise ValueError(\"Number of tensors does not match the provided split specifications.\")\n+\n+        result: list[list[torch.Tensor]] = []\n+        for tensor, split_sizes in zip(value, self.sizes):\n+            if not isinstance(tensor, torch.Tensor):\n+                raise TypeError(\"SplitModulelist can only split torch.Tensor instances.\")\n+            splits = torch.split(tensor, split_sizes, dim=self.dim)\n+            result.append(list(splits))\n+        return result\n+\n+\n+class PermuteForRope(ConversionOps):\n+    \"\"\"\n+    Applies the permutation required to convert complex RoPE weights to the split sin/cos format.\n+    \"\"\"\n+\n+    def __init__(self):\n+        pass\n+\n+    def _apply(self, tensor: torch.Tensor) -> torch.Tensor:\n+        dim1, dim2 = tensor.shape\n+        n_heads = self.config.getattr(\"num_attention_heads\", 1)\n+\n+        tensor = tensor.view(n_heads, dim1 // n_heads // 2, 2, dim2)\n+        tensor = tensor.transpose(1, 2).reshape(dim1, dim2)\n+        return tensor\n+\n+    @torch.no_grad\n+    def convert(\n+        self, value: Union[dict[str, torch.Tensor], Sequence[torch.Tensor], torch.Tensor], config\n+    ) -> Union[dict[str, torch.Tensor], list[torch.Tensor], torch.Tensor]:\n+        self.config = config\n+        out = [[self._apply(x) for x in inner] if isinstance(inner, list) else self._apply(inner) for inner in value]\n+        return out\n+\n+\n+@dataclass(slots=True)\n+class WeightConverter:\n+    r\"\"\"\n+    A weight convert that acts on a pattern of source keys.\n+    The keys need to be collected based on the target keys.\n+\n+    With wild card, glob patterns are matched, so you have to be detailed with what to match. If you match:\n+    `model.layers.*.experts.*` -> it will act on all of them\n+    {\"model.layers.*.experts.*\": []}\n+    but\n+    `experts.*.mlp` will be layer specific.\n+    {\"model.layers.1.experts.*\": [], }\n+    - source_keys: str | list[str] (wildcards '*' match digits)\n+    - target_keys: str | list[str] | None\n+    - distributed_operation / operations / quantization_operations are ALWAYS lists.\n+\n+    TODO: for BNB we need to collect model.weight.quant_state_keys\n+    \"\"\"\n+\n+    source_keys: Union[str, list[str]]\n+    target_keys: Optional[Union[str, list[str]]] = None\n+    operations: list[ConversionOps] = field(default_factory=list, repr=False)\n+\n+    distributed_operation: Optional[TensorParallelLayer] = None\n+    quantization_operation: Optional[ConversionOps] = None\n+\n+    def __post_init__(self):\n+        if not isinstance(self.source_keys, list):\n+            self.source_keys = [self.source_keys]\n+        targets_were_none = False\n+        if not isinstance(self.target_keys, list):\n+            if self.target_keys is None:\n+                self.target_keys = list(self.source_keys)\n+                targets_were_none = True\n+            else:\n+                self.target_keys = [self.target_keys]\n+\n+        if not targets_were_none and bool(len(self.source_keys) - 1) + bool(len(self.target_keys) - 1) >= 2:\n+            raise ValueError(\n+                f\"source keys={self.source_keys}, target_keys={self.target_keys} but you can only have one to many, one to one or many to one.\"\n+            )\n+\n+\n+@dataclass(slots=True)\n+class ConversionEntry:\n+    weight_converter: WeightConverter\n+    collected_tensors: dict = field(default_factory=lambda: defaultdict(dict))\n+\n+\n+GLOBAL_WORKERS = min(16, (os.cpu_count() or 8) * 2)  # NVMe: 8-16; HDD/NFS: 2-4\n+\n+\n+# Factory function to create LoadedParameter subclasses dynamically\n+def get_loaded_parameter_class(base_cls):\n+    \"\"\"\n+    base_cls: an nn.Parameter subclass (or nn.Parameter) or a Tensor\n+    Returns a new class that combines the base_cls with LoadedParameterMixin\n+\n+    \"\"\"\n+\n+    class LoadedParam(base_cls):\n+        _inplace_methods = [\n+            \"add_\",\n+            \"mul_\",\n+            \"clamp_\",\n+            \"zero_\",\n+            \"fill_\",\n+            \"normal_\",\n+            \"uniform_\",\n+            \"copy_\",\n+            \"erfinv_\",\n+            \"log_\",\n+            \"__getitem__\",\n+            \"neg_\",\n+            \"exp_\",\n+            \"sub_\",\n+        ]\n+\n+        def __new__(cls, from_existing, **kwargs):\n+            if isinstance(from_existing, torch.nn.Parameter):\n+                inst = super().__new__(cls, from_existing.data, from_existing.requires_grad, **from_existing.__dict__)\n+            else:\n+                inst = super().__new__(cls, from_existing)\n+            # we store the original object to get it back later on\n+            inst._original = from_existing\n+            # Explicitly override all in-place methods per instance\n+            for method_name in inst._inplace_methods:\n+                setattr(inst, method_name, MethodType(inst._skip, inst))\n+\n+            return inst\n+\n+        def _skip(self, *args, **kwargs):\n+            \"\"\"Helper to skip in-place operations.\"\"\"\n+            return self\n+\n+        def __repr__(self):\n+            return f\"LoadedParameter(data={self.data})\"\n+\n+        @property\n+        def data(self):\n+            return super().data\n+\n+        @data.setter\n+        def data(self, new):\n+            pass\n+\n+    def __lt__(self, other):\n+        return torch.Tensor.__lt__(self, other)\n+\n+    def __le__(self, other):\n+        return torch.Tensor.__le__(self, other)\n+\n+    def __gt__(self, other):\n+        return torch.Tensor.__gt__(self, other)\n+\n+    def __ge__(self, other):\n+        return torch.Tensor.__ge__(self, other)\n+\n+    def __eq__(self, other):\n+        return torch.Tensor.__eq__(self, other)\n+\n+    def __ne__(self, other):\n+        return torch.Tensor.__ne__(self, other)\n+\n+    def __iadd__(self, *args, **kwargs):\n+        return self\n+\n+    def __isub__(self, *args, **kwargs):\n+        return self\n+\n+    def __imul__(self, *args, **kwargs):\n+        return self\n+\n+    def __imatmul__(self, *args, **kwargs):\n+        return self\n+\n+    def __itruediv__(self, *args, **kwargs):\n+        return self\n+\n+    def __ifloordiv__(self, *args, **kwargs):\n+        return self\n+\n+    def __imod__(self, *args, **kwargs):\n+        return self\n+\n+    def __ipow__(self, *args, **kwargs):\n+        return self\n+\n+    def __iand__(self, *args, **kwargs):\n+        return self\n+\n+    def __ior__(self, *args, **kwargs):\n+        return self\n+\n+    def __ixor__(self, *args, **kwargs):\n+        return self\n+\n+    def __ilshift__(self, *args, **kwargs):\n+        return self\n+\n+    def __irshift__(self, *args, **kwargs):\n+        return self\n+\n+    return LoadedParam\n+\n+\n+def _materialize_copy(tensor, dtype=None):\n+    tensor = tensor[...]\n+    if dtype is not None:\n+        tensor = tensor.to(dtype)\n+    return tensor\n+\n+\n+def spawn_materialize(thread_pool, tensor, dtype=None) -> Future:\n+    def _job():\n+        return _materialize_copy(tensor, dtype)\n+\n+    return thread_pool.submit(_job)\n+\n+\n+def spawn_tp_materialize(thread_pool, tensor, sharding_method, tensor_idx, dtype=None) -> Future:\n+    def _job():\n+        return sharding_method.shard_tensor(tensor, param_casting_dtype=dtype, tensor_idx=tensor_idx)[0]\n+\n+    return thread_pool.submit(_job)\n+\n+\n+def dot_natural_key(s: str):\n+    parts = s.split(\".\")\n+    for i, p in enumerate(parts):\n+        # whole-segment digits -> int; otherwise leave as str\n+        if p.isdigit():\n+            parts[i] = int(p)\n+    return parts\n+\n+\n+@contextmanager\n+def log_to_misc(\n+    layer_name: str,\n+    misc: MutableMapping[str, str],\n+    extras: Any = None,\n+    op: Union[list[ConversionOps], ConversionOps, None] = None,\n+):\n+    # A simple helper to handle errors with contextual messages.\n+    try:\n+        yield\n+    except Exception as e:\n+\n+        def _format_op_name(curr_op: Union[list[ConversionOps], ConversionOps, None]) -> Optional[str]:\n+            if curr_op is None:\n+                return None\n+            if isinstance(curr_op, (list, tuple, set)):\n+                names = [o.__class__.__name__ for o in curr_op if o is not None]\n+                if not names:\n+                    return None\n+                return \", \".join(names)\n+            return curr_op.__class__.__name__\n+\n+        op_name = _format_op_name(op)\n+        if isinstance(extras, tuple) and len(extras) == 2:\n+            values, target_keys = extras\n+            descriptor = f\"{op_name} \" if op_name else \"\"\n+            misc[layer_name] = (\n+                f\"{e}\\nError: {descriptor}on tensors destined for {target_keys}. Ckpt contains: {len(values[0])}\"\n+            )\n+        elif isinstance(extras, str):\n+            suffix = f\" via {op_name}\" if op_name else \"\"\n+            misc[layer_name] = f\"{e}\\nError{suffix} when processing parameter {extras}\"\n+        elif extras is None and op_name:\n+            misc[layer_name] = f\"{op_name}: {e}\"\n+        else:\n+            misc[layer_name] = f\"{extras} |Error: {e}\"\n+        raise SkipLayer()\n+\n+\n+def set_param_for_module(\n+    model: PreTrainedModel,\n+    layer_name: str,\n+    param_value: torch.Tensor,\n+    mismatch_keys: MutableSet[tuple[str, torch.Size, torch.Size]],\n+    missing_keys: MutableSet[str],\n+    misc: MutableMapping[str, Any],\n+    distributed_operation: Optional[TensorParallelLayer],\n+):\n+    with log_to_misc(layer_name, misc, layer_name):\n+        module_path, _, param_name = layer_name.rpartition(\".\")\n+        module_obj = model.get_submodule(module_path) if module_path else model\n+        param_value = param_value[0] if isinstance(param_value, list) else param_value[...]\n+        ref = getattr(module_obj, param_name)\n+\n+        use_dtensor = hasattr(distributed_operation, \"use_dtensor\") and distributed_operation.use_dtensor\n+        if not isinstance(param_value, torch.nn.Parameter):\n+            if distributed_operation is not None:\n+                param_value = DTensor.from_local(\n+                    param_value,\n+                    distributed_operation.device_mesh,\n+                    getattr(distributed_operation, \"shard\", Replicate()),\n+                    run_check=False,\n+                    shape=ref.size(),\n+                    stride=ref.stride(),\n+                )\n+                if not use_dtensor:\n+                    # we convert to local\n+                    param_value = param_value.to_local()\n+            if param_name not in module_obj._buffers:\n+                param_value = torch.nn.Parameter(param_value, requires_grad=param_value.is_floating_point())\n+        param_value = get_loaded_parameter_class(param_value.__class__)(from_existing=param_value)\n+\n+        # Remove from missing keys (it's either mismatched, or all good)\n+        missing_keys.discard(layer_name)\n+        if ref is not None and ref.shape != param_value.shape:\n+            mismatch_keys.add((layer_name, param_value.shape, ref.shape))\n+            module_obj.param_name._is_hf_initialized = False  # Needs to be initialized\n+        else:\n+            param_value._is_hf_initialized = True  # super important otherwise _init_weight re-initi if bias is missing\n+            setattr(module_obj, param_name, param_value)\n+\n+\n+class SkipLayer(Exception):\n+    \"\"\"Control-flow sentinel: abort processing of the current layer only.\"\"\"\n+\n+    pass\n+\n+\n+def convert_and_load_state_dict_in_model(\n+    model: PreTrainedModel,\n+    state_dict: dict[str, Any],\n+    weight_mapping: dict[str, WeightConverter] | None,\n+    tp_plan: dict[str, str] | None,\n+    quantizer: HfQuantizer | None,\n+    dtype: torch.dtype | None = None,\n+    device_map: dict | None = None,\n+    dtype_plan: dict | None = None,\n+    device_mesh: torch.distributed.device_mesh.DeviceMesh | None = None,\n+):\n+    \"\"\"\n+    Convert a state dict according to a weight mapping (one WeightConverter per glob pattern),\n+    collecting tensors per *layer instance* (the concrete indices captured from '*').\n+    \"\"\"\n+\n+    prefix = model.base_model_prefix\n+    tp_plan = tp_plan or {}  # {glob_pattern: plan_obj_or_key}\n+    device_map = device_map or {}  # {exact_target_key: device}\n+    dtype_plan = dtype_plan or {}  # {glob_pattern: dtype}\n+    weight_mapping = weight_mapping or {}  # {glob_pattern: WeightConverter}\n+    meta_model_state_dict = model.state_dict()\n+    missing_keys = set(meta_model_state_dict.keys())\n+\n+    misc = {}\n+    mismatch_keys = set()\n+    unexpected_keys = set()\n+    # Global thread_pool\n+    thread_pool = ThreadPoolExecutor(max_workers=GLOBAL_WORKERS)\n+\n+    _patterns = list(itertools.chain.from_iterable([k.source_keys for k in weight_mapping]))\n+    source_to_target = {sk: k for k in weight_mapping for sk in k.source_keys}\n+    weight_pattern_alt, weight_pattern_by_group_name = build_glob_alt(_patterns)\n+    tp_plan_alt, tp_plan_by_group_name = build_glob_alt(list(tp_plan.keys()))\n+    dtype_policy_alt, dtype_policy_by_group_name = build_glob_alt(list(dtype_plan.keys()))\n+\n+    state_dict = sorted(state_dict.items(), key=lambda kv: dot_natural_key(kv[0]))\n+    # 1. Create the conversion entries\n+    by_conversion_pattern: dict[str, ConversionEntry] = {}\n+    for original_key, tensor in state_dict:\n+        matched_pattern = match_glob(original_key, weight_pattern_alt, weight_pattern_by_group_name)\n+        if matched_pattern is not None:\n+            converter = source_to_target[matched_pattern]  # TODO make sure its the ref\n+            sub_with_extractor = partial(re.sub, matched_pattern.replace(\"*\", r\"(\\d+)\"), string=original_key)\n+            entry_key = \"|\".join(converter.target_keys)\n+            target_key = \"|\".join(map(sub_with_extractor, [k.replace(\"*\", \"\\\\1\") for k in converter.target_keys]))\n+            entry: ConversionEntry = by_conversion_pattern.setdefault(entry_key, ConversionEntry(converter))\n+            converter_key = sub_with_extractor(matched_pattern)\n+        else:\n+            converter = WeightConverter(original_key)\n+            converter_key = entry_key = target_key = original_key\n+            entry = by_conversion_pattern.setdefault(converter_key, ConversionEntry(converter))\n+\n+        _dtype = dtype\n+        new_target_key = []  # test_load_with_mismatched_shapes for AutoModel.from_pretrained(AutoForCausal, vocab=10)\n+        for t in target_key.split(\"|\"):\n+            if t.startswith(prefix) and meta_model_state_dict.get(re.sub(f\"^{prefix}.\", \"\", t, count=1)) is not None:\n+                t = re.sub(f\"^{prefix}.\", \"\", t, count=1)\n+            elif meta_model_state_dict.get(f\"{prefix}.{t}\") is not None:\n+                t = f\"{prefix}.{t}\"\n+            new_target_key.append(t)\n+            empty_param = meta_model_state_dict.get(t)\n+            # If it does not exist, it's unexpected\n+            if empty_param is None:\n+                unexpected_keys.add(t)\n+                continue\n+\n+            if quantizer is not None and quantizer.param_needs_quantization(model, t):\n+                if quantizer.__class__.__name__ == \"FineGrainedFP8HfQuantizer\":\n+                    from .integrations.finegrained_fp8 import Fp8Quantize\n+\n+                    converter.quantization_operation = Fp8Quantize()  # TODO support other methods\n+                else:\n+                    raise ValueError(\"This quantization method is gonna be supported SOOOON\")\n+            else:\n+                _dtype = dtype\n+                matched_dtype_pattern = match_glob(t, dtype_policy_alt, dtype_policy_by_group_name)\n+                if matched_dtype_pattern is not None:\n+                    _dtype = dtype_plan[matched_dtype_pattern]\n+                elif empty_param.dtype != _dtype:\n+                    _dtype = empty_param.dtype\n+\n+        first_target_key = new_target_key[0]\n+        target_key = \"|\".join(new_target_key)\n+\n+        future = None\n+        if device_mesh:\n+            if matched_tp_pattern := match_glob(first_target_key, tp_plan_alt, tp_plan_by_group_name):\n+                empty_param = meta_model_state_dict.get(first_target_key)\n+                if getattr(converter, \"distributed_operation\", {}) is None:\n+                    tp_layer = ALL_PARALLEL_STYLES[model.tp_plan[matched_tp_pattern]].__class__\n+                    converter.distributed_operation = tp_layer(\n+                        device_mesh=device_mesh, rank=device_map[\"\"].index, empty_param=empty_param.clone()\n+                    )\n+                    # VERY IMPORTANT: this tells us wether we collected stuffs or not.\n+                shard_index = len(entry.collected_tensors[target_key].get(converter_key, []))\n+                future = spawn_tp_materialize(\n+                    thread_pool,\n+                    tensor,\n+                    _dtype,\n+                    converter.distributed_operation,\n+                    shard_index,\n+                )\n+\n+        if future is None:  # If not TP, async materialize the tensors. TODO handle disk offload?\n+            future = spawn_materialize(thread_pool, tensor, _dtype)\n+        entry.collected_tensors[target_key].setdefault(converter_key, []).append(future)\n+\n+    # 2. Actually convert the ckpt\n+    inverse_converters = {}\n+    keys = list(by_conversion_pattern.keys())\n+\n+    with logging.tqdm(total=len(keys), desc=\"Loading weights\") as pbar:\n+        for key in keys[::-1]:  # revert to process simple keys first\n+            group = by_conversion_pattern.pop(key)\n+            converter = group.weight_converter\n+            operations = converter.operations if isinstance(converter.operations, list) else [converter.operations]\n+            for layer_name, tensors_for_this_layer in group.collected_tensors.items():\n+                pbar.update(1)\n+                pbar.set_postfix({\"Materializing param\": layer_name})\n+                pbar.refresh()\n+                concrete_target_keys = layer_name.split(\"|\")\n+                try:\n+                    if bool(set(concrete_target_keys) - unexpected_keys):\n+                        with log_to_misc(layer_name, misc):\n+                            values = [[k.result() for k in inner] for inner in tensors_for_this_layer.values()]\n+\n+                        for op in operations:\n+                            with log_to_misc(layer_name, misc, (values, concrete_target_keys), operations):\n+                                values = op.convert(values, model.config)\n+\n+                        values = [values] if not isinstance(values, list) else values\n+                        with log_to_misc(layer_name, misc, (values, concrete_target_keys), operations):\n+                            realized_value = {\n+                                k: t for k, t in zip(concrete_target_keys, values) if k not in unexpected_keys\n+                            }\n+\n+                        for k in list(realized_value.keys()).copy():\n+                            if op := converter.quantization_operation:\n+                                with log_to_misc(layer_name, misc, op=op):\n+                                    realized_value.update(\n+                                        op.convert(\n+                                            {k: realized_value.pop(k)}, quant_config=quantizer.quantization_config\n+                                        )\n+                                    )\n+\n+                        for k, output_value in realized_value.items():\n+                            for src in converter.source_keys:  # what should happen to k when we meet k at saving\n+                                inverse_converters[k] = {src: converter}\n+                            set_param_for_module(\n+                                model,\n+                                k,\n+                                output_value,\n+                                mismatch_keys,\n+                                missing_keys,\n+                                misc,\n+                                converter.distributed_operation,\n+                            )\n+\n+                except SkipLayer:\n+                    continue\n+            del group\n+\n+    model.inverse_converters = inverse_converters\n+    thread_pool.shutdown(wait=False)\n+    return missing_keys, unexpected_keys, mismatch_keys, misc\n+\n+\n+# TODO this is not done yet!\n+def revert_weight_conversion(model, state_dict):\n+    mapping = getattr(model, \"_checkpoint_conversion_mapping\", {})  # IDK why but setting this will fail all llava.\n+    reverse_key_mapping = [(v, k) for k, v in mapping.items()]\n+    original_state_dict = {}\n+    for key, value in state_dict.items():\n+        for pattern, inverse_converter in reverse_key_mapping:\n+            # TODO FIXME you name it\n+            replacement = inverse_converter.lstrip(\"^\")  # strip off un-needed chars and patterns\n+            replacement = re.sub(r\"\\(.*\\)\", \"\", replacement)\n+            key, n_replace = re.subn(pattern, replacement, key)\n+            # Early exit of the loop\n+            if n_replace > 0:\n+                break\n+        original_state_dict[key] = value\n+    state_dict = original_state_dict\n+    return state_dict"
        },
        {
            "sha": "b54b46017f869f3ebc6d0784c786c63c2ddf9c06",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 7,
            "deletions": 2,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -411,7 +411,7 @@ def adjust_generation_fn(\n                     \"Generation config file not found, using a generation config created from the model config.\"\n                 )\n             # Load custom generate function if `pretrained_model_name_or_path` defines it (and override `generate`)\n-            if hasattr(self, \"load_custom_generate\"):\n+            if hasattr(self, \"load_custom_generate\") and trust_remote_code:\n                 try:\n                     custom_generate = self.load_custom_generate(\n                         pretrained_model_name_or_path, trust_remote_code=trust_remote_code, **repo_loading_kwargs\n@@ -1635,7 +1635,12 @@ def _validate_model_kwargs(self, model_kwargs: dict[str, Any]):\n \n         # TransformersKwargs are model-agnostic attention and generation arguments such as 'output_attentions'\n         for key, value in model_kwargs.items():\n-            if value is not None and key not in model_args and key not in TransformersKwargs.__optional_keys__:\n+            if (\n+                value is not None\n+                and key not in model_args\n+                and key not in TransformersKwargs.__optional_keys__\n+                and key != \"debug_io\"\n+            ):\n                 unused_model_args.append(key)\n \n         if unused_model_args:"
        },
        {
            "sha": "da978c3c107e963ce82c2fd3db411f945e709fc1",
            "filename": "src/transformers/generation/watermarking.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fgeneration%2Fwatermarking.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fgeneration%2Fwatermarking.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fwatermarking.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -383,10 +383,11 @@ def __init__(self, config):\n         )\n         self.prior = torch.nn.Parameter(torch.tensor([self.base_rate]))\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights.\"\"\"\n         if isinstance(module, nn.Parameter):\n-            module.weight.data.normal_(mean=0.0, std=0.02)\n+            module.weight.normal_(mean=0.0, std=0.02)\n \n     def _compute_posterior(\n         self,"
        },
        {
            "sha": "c9a8ab56d4cb0e9ecacfa7b5bf37bd707130d39e",
            "filename": "src/transformers/integrations/accelerate.py",
            "status": "modified",
            "additions": 1,
            "deletions": 9,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fintegrations%2Faccelerate.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fintegrations%2Faccelerate.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Faccelerate.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -512,10 +512,8 @@ def accelerate_disk_offload(\n     checkpoint_files,\n     device_map,\n     checkpoint_keys,\n-    key_renaming_mapping,\n     sharded_metadata,\n     dtype,\n-    reverse_key_renaming_mapping,\n ):\n     disk_only_shard_files = []\n     if disk_offload_folder is not None:\n@@ -534,19 +532,13 @@ def accelerate_disk_offload(\n             weight_map = dict.fromkeys(checkpoint_keys, checkpoint_files[0])\n         else:\n             folder = os.path.sep.join(checkpoint_files[0].split(os.path.sep)[:-1])\n-            # Fix the weight map keys according to the key mapping\n-            weight_map = {\n-                key_renaming_mapping[k]: v\n-                for k, v in sharded_metadata[\"weight_map\"].items()\n-                if k in key_renaming_mapping\n-            }\n             weight_map = {k: os.path.join(folder, v) for k, v in weight_map.items()}\n             # Find potential checkpoints containing only offloaded weights\n             disk_only_shard_files = get_disk_only_shard_files(device_map, weight_map)\n         disk_offload_index = {\n             name: {\n                 \"safetensors_file\": file,\n-                \"weight_name\": reverse_key_renaming_mapping[name],\n+                \"weight_name\": name,\n                 \"dtype\": str_dtype,\n             }\n             for name, file in weight_map.items()"
        },
        {
            "sha": "931e6a88d963a8bb5dc3d2fec48d35768421edfe",
            "filename": "src/transformers/integrations/bitsandbytes.py",
            "status": "modified",
            "additions": 0,
            "deletions": 48,
            "changes": 48,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fintegrations%2Fbitsandbytes.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fintegrations%2Fbitsandbytes.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fbitsandbytes.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -1,5 +1,4 @@\n import inspect\n-from copy import deepcopy\n from inspect import signature\n \n from ..utils import (\n@@ -24,7 +23,6 @@\n     import accelerate\n     from accelerate import init_empty_weights\n     from accelerate.hooks import add_hook_to_module, remove_hook_from_module\n-    from accelerate.utils import find_tied_parameters\n \n logger = logging.get_logger(__name__)\n \n@@ -151,52 +149,6 @@ def replace_with_bnb_linear(model, modules_to_not_convert=None, current_key_name\n     return model\n \n \n-def get_keys_to_not_convert(model):\n-    r\"\"\"\n-    An utility function to get the key of the module to keep in full precision if any For example for CausalLM modules\n-    we may want to keep the lm_head in full precision for numerical stability reasons. For other architectures, we want\n-    to keep the tied weights of the model. The function will return a list of the keys of the modules to not convert in\n-    int8.\n-\n-    Parameters:\n-    model (`torch.nn.Module`):\n-        Input model\n-    \"\"\"\n-    # Create a copy of the model and tie the weights, then\n-    # check if it contains tied weights\n-    tied_model = deepcopy(model)  # this has 0 cost since it is done inside `init_empty_weights` context manager`\n-    tied_model.tie_weights()\n-\n-    tied_params = find_tied_parameters(tied_model)\n-    tied_keys = sum(tied_params, [])\n-    has_tied_params = len(tied_keys) > 0\n-\n-    # If there is not tied weights, we want to keep the lm_head（output_embedding) in full precision\n-    if not has_tied_params:\n-        output_emb = model.get_output_embeddings()\n-        if output_emb is not None:\n-            list_last_module = [name for name, module in model.named_modules() if id(module) == id(output_emb)]\n-            return list_last_module\n-\n-    # otherwise, no tied weights, no output embedding defined, simply keep the last module in full precision\n-    list_modules = list(model.named_parameters())\n-    list_last_module = [list_modules[-1][0]]\n-    # add last module together with tied weights\n-    intersection = set(list_last_module) - set(tied_keys)\n-    list_untouched = list(set(tied_keys)) + list(intersection)\n-\n-    # remove \".weight\" from the keys\n-    names_to_remove = [\".weight\", \".bias\"]\n-    filtered_module_names = []\n-    for name in list_untouched:\n-        for name_to_remove in names_to_remove:\n-            if name_to_remove in name:\n-                name = name.replace(name_to_remove, \"\")\n-        filtered_module_names.append(name)\n-\n-    return filtered_module_names\n-\n-\n # Copied from PEFT: https://github.com/huggingface/peft/blob/47b3712898539569c02ec5b3ed4a6c36811331a1/src/peft/utils/integrations.py#L41\n def dequantize_bnb_weight(weight: \"torch.nn.Parameter\", dtype: \"torch.dtype\", state=None):\n     \"\"\""
        },
        {
            "sha": "50eefbbd0809012eef6e86c1fd58db2dd7ac2b7f",
            "filename": "src/transformers/integrations/finegrained_fp8.py",
            "status": "modified",
            "additions": 310,
            "deletions": 36,
            "changes": 346,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fintegrations%2Ffinegrained_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fintegrations%2Ffinegrained_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ffinegrained_fp8.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -13,8 +13,11 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from typing import Optional\n+import re\n+from collections.abc import Sequence\n+from typing import Any, Optional, Union\n \n+from ..core_model_loading import ConversionOps\n from ..utils import is_accelerate_available, is_torch_accelerator_available, is_torch_available, logging\n \n \n@@ -30,6 +33,18 @@\n \n \n logger = logging.get_logger(__name__)\n+try:\n+    _FP8_DTYPE = torch.float8_e4m3fn\n+    _FP8_MIN = torch.finfo(_FP8_DTYPE).min\n+    _FP8_MAX = torch.finfo(_FP8_DTYPE).max\n+    _FP8_IS_INT = False\n+except AttributeError:\n+    _FP8_DTYPE = torch.int8\n+    _FP8_MIN, _FP8_MAX = -127, 127\n+    _FP8_IS_INT = True\n+    logger.warning_once(\n+        \"torch.float8_e4m3fn not available; falling back to int8 emulation for Fp8Quantize operations.\"\n+    )\n \n \n # Copied from https://huggingface.co/deepseek-ai/DeepSeek-V3/blob/main/inference/kernel.py\n@@ -332,16 +347,22 @@ def forward(self, input: torch.Tensor) -> torch.Tensor:\n         if self.weight.element_size() > 1:\n             return F.linear(input, self.weight, self.bias)\n         else:\n+            if isinstance(self.weight, torch.distributed.tensor.DTensor):\n+                weight = self.weight._local_tensor.contiguous()\n+                scale_inv = self.weight_scale_inv._local_tensor.contiguous()\n+            else:\n+                weight = self.weight.contiguous()\n+                scale_inv = self.weight_scale_inv.contiguous()\n             # Context manager used to switch among the available accelerators\n             device_type = torch.accelerator.current_accelerator().type if is_torch_accelerator_available() else \"cuda\"\n             torch_accelerator_module = getattr(torch, device_type, torch.cuda)\n             with torch_accelerator_module.device(input.device):\n                 qinput, scale = act_quant(input, self.block_size[1])\n                 output = w8a8_block_fp8_matmul_triton(\n                     qinput,\n-                    self.weight,\n+                    weight,\n                     scale,\n-                    self.weight_scale_inv,\n+                    scale_inv,\n                     self.block_size,\n                     output_dtype=input.dtype,\n                 )\n@@ -350,9 +371,124 @@ def forward(self, input: torch.Tensor) -> torch.Tensor:\n             torch_accelerator_module.synchronize()\n             if self.bias is not None:\n                 output = output + self.bias\n+            output = torch.nan_to_num(output, nan=0.0)\n+            return output.to(dtype=input.dtype)\n+\n+\n+def _ceil_div(a, b):\n+    return (a + b - 1) // b\n+\n+\n+class FP8Expert(nn.Module):\n+    dtype = torch.float8_e4m3fn\n+\n+    def __init__(self, config, block_size, device):\n+        super().__init__()\n+\n+        from ..activations import ACT2FN\n+\n+        self.block_size = block_size\n+        self.num_experts = config.num_local_experts\n+        self.hidden_dim = config.hidden_size\n+        self.intermediate_dim = config.intermediate_size\n+\n+        Wg_out, Wg_in = 2 * self.intermediate_dim, self.hidden_dim\n+        Wd_out, Wd_in = self.hidden_dim, self.intermediate_dim\n+\n+        self.gate_up_proj = nn.Parameter(\n+            torch.zeros(self.num_experts, Wg_out, Wg_in, dtype=FP8Expert.dtype, device=device)\n+        )\n+        self.down_proj = nn.Parameter(\n+            torch.zeros(self.num_experts, Wd_out, Wd_in, dtype=FP8Expert.dtype, device=device)\n+        )\n+\n+        # Create inverse scale tiles only when using 1-byte types (fp8)\n+        if self.gate_up_proj.element_size() == 1:\n+            bo, bi = self.block_size\n+\n+            # gate_up tiles: ceil(Wg_out/bo) x ceil(Wg_in/bi)\n+            gu_scale_o = _ceil_div(Wg_out, bo)\n+            gu_scale_i = _ceil_div(Wg_in, bi)\n+            self.gate_up_proj_scales_inv = nn.Parameter(\n+                torch.zeros(self.num_experts, gu_scale_o, gu_scale_i, dtype=torch.float32, device=device)\n+            )\n+\n+            # down tiles: ceil(Wd_out/bo) x ceil(Wd_in/bi)\n+            dp_scale_o = _ceil_div(Wd_out, bo)\n+            dp_scale_i = _ceil_div(Wd_in, bi)\n+            self.down_proj_scales_inv = nn.Parameter(\n+                torch.zeros(self.num_experts, dp_scale_o, dp_scale_i, dtype=torch.float32, device=device)\n+            )\n+        else:\n+            # Match FP8Linear behavior when not using 1-byte weights\n+            self.register_parameter(\"gate_up_proj_scale_inv\", None)\n+            self.register_parameter(\"down_proj_scale_inv\", None)\n+\n+        # (Optional) bias per projection — many MoEs omit bias; keep None to match your FP8Linear default\n+        self.register_parameter(\"gate_up_bias\", None)\n+        self.register_parameter(\"down_bias\", None)\n+\n+        # Activation used in the MLP (same as your config / ACT2FN)\n+        # Keep a handle here; actual usage happens in forward of your MoE block\n+        self.act_fn = ACT2FN[config.hidden_act]\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        top_k_index: torch.Tensor,\n+        top_k_weights: torch.Tensor,\n+    ) -> torch.Tensor:\n+        final_hidden_states = torch.zeros_like(hidden_states)\n+        num_experts = top_k_weights.shape[1]\n+        with torch.no_grad():\n+            expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=num_experts + 1)\n+            expert_mask = expert_mask.permute(2, 1, 0)\n+            expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n+\n+        for expert_idx in expert_hit:\n+            expert_idx = expert_idx[0]\n+            if expert_idx == num_experts:\n+                continue\n+            _, token_idx = torch.where(expert_mask[expert_idx])\n+            current_state = hidden_states.index_select(0, token_idx)\n+            gate, up = self.linear(\n+                current_state, self.gate_up_proj[expert_idx], self.gate_up_proj_scales_inv[expert_idx]\n+            ).chunk(2, dim=-1)\n+            current_hidden_states = self.act_fn(gate) * up\n+            current_hidden_states = self.linear(\n+                current_hidden_states, self.down_proj[expert_idx], self.down_proj_scales_inv[expert_idx]\n+            )\n+\n+            routing_weights = top_k_weights[token_idx, expert_idx].unsqueeze(-1)\n+            current_hidden_states = current_hidden_states * routing_weights.to(current_hidden_states.dtype)\n+            final_hidden_states.index_add_(0, token_idx, current_hidden_states.to(final_hidden_states.dtype))\n+\n+        return final_hidden_states\n+\n+    def linear(self, input: torch.Tensor, weight: torch.Tensor, weight_scale_inv: torch.Tensor) -> torch.Tensor:\n+        if weight.element_size() > 1:\n+            return F.linear(input, weight, None)\n+        else:\n+            # Context manager used to switch among the available accelerators\n+            device_type = torch.accelerator.current_accelerator().type if is_torch_accelerator_available() else \"cuda\"\n+            torch_accelerator_module = getattr(torch, device_type, torch.cuda)\n+            with torch_accelerator_module.device(input.device):\n+                qinput, scale = act_quant(input, self.block_size[1])\n+                output = w8a8_block_fp8_matmul_triton(\n+                    qinput,\n+                    weight,\n+                    scale,\n+                    weight_scale_inv,\n+                    self.block_size,\n+                    output_dtype=input.dtype,\n+                )\n+            # Blocks the CPU until all accelerator operations on the specified device are complete. It is used to ensure that the results of the\n+            # preceding operations are ready before proceeding\n+            torch_accelerator_module.synchronize()\n             return output.to(dtype=input.dtype)\n \n \n+# TODO: we do need this.... but not recursive...\n def _replace_with_fp8_linear(\n     model,\n     tp_plan=None,\n@@ -361,40 +497,48 @@ def _replace_with_fp8_linear(\n     quantization_config=None,\n     has_been_replaced=False,\n ):\n-    \"\"\"Replace Linear layers with FP8Linear.\"\"\"\n-    if current_key_name is None:\n-        current_key_name = []\n-\n-    for name, module in model.named_children():\n-        current_key_name.append(name)\n-\n-        if isinstance(module, nn.Linear) and name not in (modules_to_not_convert or []):\n-            current_key_name_str = \".\".join(current_key_name)\n-            if not any(key in current_key_name_str for key in (modules_to_not_convert or [])):\n-                with init_empty_weights():\n-                    model._modules[name] = FP8Linear(\n-                        in_features=module.in_features,\n-                        out_features=module.out_features,\n-                        bias=module.bias is not None,\n-                        device=module.weight.device,\n-                        dtype=module.weight.dtype,\n-                        activation_scheme=quantization_config.activation_scheme,\n-                        block_size=quantization_config.weight_block_size,\n+    iterator = list(model.named_parameters()).copy()\n+    for name, empty_tensor in iterator:\n+        current_key_name = name\n+        name = name.rsplit(\".\", 1)[0] if \".\" in name else name\n+        module = model.get_submodule(name)\n+\n+        current_key_name_str = re.sub(r\"\\d+\", \"*\", current_key_name)\n+        if not any(key in current_key_name_str for key in (modules_to_not_convert or [])):\n+            with init_empty_weights():\n+                if (\n+                    \"gate_up_proj\" in current_key_name\n+                    or \"down_proj\" in current_key_name\n+                    and \"experts\" in current_key_name\n+                ):  # Experts!\n+                    in_features = empty_tensor.size(-2)\n+                    out_features = empty_tensor.size(-1)\n+                    model.set_submodule(\n+                        name,\n+                        FP8Expert(\n+                            config=model.config,\n+                            block_size=quantization_config.weight_block_size,\n+                            device=empty_tensor.device,\n+                        ),\n                     )\n-                    has_been_replaced = True\n-            # when changing a layer the TP PLAN for that layer should be updated. TODO\n-\n-        if len(list(module.children())) > 0:\n-            _, has_been_replaced = _replace_with_fp8_linear(\n-                module,\n-                tp_plan,\n-                modules_to_not_convert,\n-                current_key_name,\n-                quantization_config,\n-                has_been_replaced=has_been_replaced,\n-            )\n \n-        current_key_name.pop(-1)\n+                elif isinstance(module, nn.Linear):\n+                    in_features = module.in_features\n+                    out_features = module.out_features\n+                    model.set_submodule(\n+                        name,\n+                        FP8Linear(\n+                            in_features=in_features,\n+                            out_features=out_features,\n+                            bias=module.bias is not None,\n+                            device=module.weight.device,\n+                            dtype=module.weight.dtype,\n+                            activation_scheme=quantization_config.activation_scheme,\n+                            block_size=quantization_config.weight_block_size,\n+                        ),\n+                    )\n+                has_been_replaced = True\n+        # when changing a layer the TP PLAN for that layer should be updated. TODO\n \n     return model, has_been_replaced\n \n@@ -405,7 +549,7 @@ def replace_with_fp8_linear(\n     quantization_config=None,\n ):\n     \"\"\"Helper function to replace model layers with FP8 versions.\"\"\"\n-    modules_to_not_convert = [\"lm_head\"] if modules_to_not_convert is None else modules_to_not_convert\n+    modules_to_not_convert += [\"lm_head\"]\n \n     if quantization_config.modules_to_not_convert is not None:\n         modules_to_not_convert.extend(quantization_config.modules_to_not_convert)\n@@ -424,3 +568,133 @@ def replace_with_fp8_linear(\n         )\n \n     return model\n+\n+\n+class QuantizationOp(ConversionOps):\n+    \"\"\"Base class for quantization operations.\"\"\"\n+\n+    pass\n+\n+\n+class Fp8Quantize(QuantizationOp):\n+    \"\"\"\n+    A quantization operation that creates two tensors, weight and scale out of a weight.\n+    \"\"\"\n+\n+    reverse_op: type[ConversionOps]\n+\n+    def __init__(self, block_size: Optional[tuple[int, int]] = None):\n+        self.block_size = block_size\n+        self.reverse_op = Fp8Dequantize\n+\n+    def convert(self, input_dict: torch.Tensor, *, quant_config: dict[str, Any]) -> dict[str, torch.Tensor]:\n+        # Unpack single key/value (value may be wrapped in a list)\n+        target_keys, value = tuple(input_dict.items())[0]\n+        value = value[0] if isinstance(value, list) else value\n+\n+        # Resolve block size (support dict-like or attr-like quant_config)\n+        block_size = None\n+        if quant_config is not None:\n+            if isinstance(quant_config, dict):\n+                block_size = quant_config.get(\"weight_block_size\")\n+            else:\n+                block_size = getattr(quant_config, \"weight_block_size\", None)\n+        if block_size is None:\n+            block_size = (value.shape[-2], value.shape[-1])\n+\n+        block_m, block_n = block_size\n+        rows, cols = value.shape[-2], value.shape[-1]\n+\n+        # Enforce exact tiling like your original\n+        if rows % block_m != 0 or cols % block_n != 0:\n+            raise ValueError(\n+                f\"Matrix dimensions ({rows}, {cols}) must be divisible by block sizes ({block_m}, {block_n}). for {target_keys}\"\n+            )\n+\n+        # Leading dims can be empty (2D) or include num_experts/... (3D+)\n+        leading_shape = value.shape[:-2]\n+        rows_tiles = rows // block_m\n+        cols_tiles = cols // block_n\n+\n+        original_shape = value.shape\n+        value_fp32 = value.to(torch.float32)\n+\n+        # Reshape to (..., rows_tiles, block_m, cols_tiles, block_n)\n+        reshaped = value_fp32.reshape(*leading_shape, rows_tiles, block_m, cols_tiles, block_n)\n+\n+        # Per-tile max-abs over the block dims\n+        # dims: block_m is at -3, block_n is at -1 after the reshape\n+        max_abs = reshaped.abs().amax(dim=(-3, -1))\n+        safe_max_abs = torch.where(max_abs > 0, max_abs, torch.ones_like(max_abs))\n+\n+        # Tile scale (we store inverse scale like your Linear: weight_scale_inv)\n+        scales = _FP8_MAX / safe_max_abs\n+        scales = torch.where(max_abs > 0, scales, torch.ones_like(scales))  # keep zeros stable\n+\n+        # Broadcast scales back over the block dims and quantize\n+        # max_abs/scales shape: (..., rows_tiles, cols_tiles)\n+        scales_broadcast = scales.unsqueeze(-1).unsqueeze(-3)  # -> (..., rows_tiles, 1, cols_tiles, 1)\n+        scaled = reshaped * scales_broadcast\n+\n+        if _FP8_IS_INT:\n+            quantized = torch.clamp(scaled.round(), min=_FP8_MIN, max=_FP8_MAX).to(_FP8_DTYPE)\n+        else:\n+            quantized = torch.clamp(scaled, min=_FP8_MIN, max=_FP8_MAX).to(_FP8_DTYPE)\n+\n+        quantized = quantized.reshape(original_shape)\n+\n+        inv_scales = (1.0 / scales).to(torch.float32)  # shape: (*leading, rows_tiles, cols_tiles)\n+        if target_keys.endswith(\"weight\"):\n+            scale_key = target_keys.rsplit(\".\", 1)[0] + \".weight_scale_inv\"\n+        else:\n+            scale_key = target_keys + \"_scales_inv\"\n+\n+        # Return both quantized weights and per-tile inverse scales (keeps leading dims, e.g., num_experts)\n+        return {\n+            target_keys: quantized,\n+            scale_key: inv_scales,\n+        }\n+\n+\n+class Fp8Dequantize(QuantizationOp):\n+    \"\"\"Inverse operation of :class:`Fp8Quantize`. Takes a pair (weight, scale) and reconstructs the fp32 tensor.\"\"\"\n+\n+    def __init__(self, block_size: Optional[tuple[int, int]] = None):\n+        self.block_size = block_size\n+        self.reverse_op = Fp8Quantize\n+\n+    def convert(\n+        self,\n+        value: Union[Sequence[torch.Tensor], dict[str, torch.Tensor]],\n+        *,\n+        context: dict[str, Any],\n+    ) -> torch.Tensor:\n+        if isinstance(value, dict):\n+            tensors = list(value.values())\n+        else:\n+            tensors = list(value) if isinstance(value, Sequence) else [value]\n+        if len(tensors) != 2:\n+            raise ValueError(\"Fp8Dequantize expects exactly two tensors: quantized weights and scales.\")\n+        quantized, scales = tensors\n+        if not isinstance(quantized, torch.Tensor) or not isinstance(scales, torch.Tensor):\n+            raise TypeError(\"Fp8Dequantize expects tensors as inputs.\")\n+\n+        quantized_fp32 = quantized.to(torch.float32)\n+        rows, cols = quantized_fp32.shape[-2:]\n+        block_size = self.block_size\n+        if block_size is None:\n+            quant_config = context.get(\"quantization_config\")\n+            block_size = getattr(quant_config, \"weight_block_size\", None)\n+        if block_size is None:\n+            block_size = (rows, cols)\n+        block_m, block_n = block_size\n+        if rows % block_m != 0 or cols % block_n != 0:\n+            raise ValueError(\n+                f\"Matrix dimensions ({rows}, {cols}) must be divisible by block sizes ({block_m}, {block_n}).\"\n+            )\n+\n+        reshaped = quantized_fp32.reshape(-1, rows // block_m, block_m, cols // block_n, block_n)\n+        expanded_scales = scales.to(torch.float32).reshape(-1, rows // block_m, cols // block_n)\n+        expanded_scales = expanded_scales.unsqueeze(-1).unsqueeze(2)\n+        dequantized = reshaped * expanded_scales\n+        return dequantized.reshape(quantized_fp32.shape)"
        },
        {
            "sha": "db3c5df70d919f4eb18ea72590c4a76fb013b167",
            "filename": "src/transformers/integrations/peft.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fintegrations%2Fpeft.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fintegrations%2Fpeft.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fpeft.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -236,7 +236,7 @@ def load_adapter(\n                 **adapter_kwargs,\n             )\n             peft_config.inference_mode = not is_trainable\n-\n+        # TODO: WE NEED TOO APPLY OUR DYNAMIC WEIGHT CONVERSION AT SOME POINT HERE!\n         # Create and add fresh new adapters into the model.\n         inject_adapter_in_model(peft_config, self, adapter_name, **peft_load_kwargs)\n "
        },
        {
            "sha": "2ef2ea5467cd2c42eea1baf19c5ffb28b3dfccb1",
            "filename": "src/transformers/integrations/tensor_parallel.py",
            "status": "modified",
            "additions": 258,
            "deletions": 41,
            "changes": 299,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -18,6 +18,7 @@\n import os\n import re\n from functools import partial, reduce\n+from typing import Optional\n \n import torch\n import torch.distributed as dist\n@@ -316,7 +317,7 @@ def repack_weights(\n     return final_ordered_tensor\n \n \n-def get_tensor_shard(param, empty_param, device_mesh, rank, dim):\n+def get_tensor_shard(param, empty_param, device_mesh, rank, dim, tensor_idx: Optional[int] = None):\n     \"\"\"\n     Generalized tensor sharding across a multi-dimensional device mesh.\n     Extract only the fraction of the parameter owned by the given `rank` when the parameter would have gone sharding at provided `dim`.\n@@ -368,32 +369,57 @@ def get_tensor_shard(param, empty_param, device_mesh, rank, dim):\n         rank (int): Global rank of the current process/device.\n         dim (int): Dimension along which to shard the tensor.\n     \"\"\"\n-    param_dim = empty_param.dim()\n-\n+    param_dim = empty_param.ndim\n+    # Flatten the mesh to get the total number of devices\n+    mesh_shape = device_mesh.shape\n+    world_size = reduce(operator.mul, mesh_shape)\n     if dim < 0:\n         dim = param_dim + dim\n+    if empty_param.dim() == 3 and dim == 1 and len(param.get_shape()) == 2:\n+        dim = 0\n+    elif empty_param.dim() == 3 and dim == 2 and len(param.get_shape()) == 2:\n+        dim = 0\n+\n+    shard_size = math.ceil(empty_param.size(dim) / world_size)\n+    start = rank * shard_size\n+    end = min(start + shard_size, empty_param.size(dim))\n+\n     if dim >= param_dim:\n         raise ValueError(f\"dim {dim} is out of bounds for tensor of dimension {param_dim}\")\n \n-    # Flatten the mesh to get the total number of devices\n-    mesh_shape = device_mesh.shape\n-    world_size = reduce(operator.mul, mesh_shape)\n-\n     if rank >= world_size:\n         raise ValueError(f\"Rank {rank} is out of bounds for mesh size {world_size}\")\n \n-    shard_size = math.ceil(empty_param.shape[dim] / world_size)\n-    start = rank * shard_size\n+    # we have the full tensor not 1 part of it.\n+    # in that case, we just assume that the weight was properly saved\n+    # and thus because we TP if the layer is colwise it should not use this. Layer should be packed_colwise\n+    # to inform that it needs to read form a packed tensor. It will also take care of the module list thingy.\n+    # here we take care of potential chunking / layer split / layer chunking.\n+    # The only \"hard\" case is? if we collect q,k,v -> merge it into qkv. In that case\n+    # actually we still shard dim=0 does not change\n+    # so only case is if the dim of the empty param is 3 and the shard dim is 0 -> we put the\n+    # tensor on a certain device (with the input tensor_index)\n+    dimensions = param.get_shape()\n+\n+    if empty_param.dim() == 3 and dim == 0 and len(param.get_shape()) == 2:\n+        # special case we don't \"shard\" just send this entire tensor to the correct rank.\n+        if start <= tensor_idx < end:\n+            # this tensor does need to be materialized on this device:\n+            return param[:]\n+        else:\n+            return torch.empty([], dtype=torch.int64, device=rank)\n \n-    # Construct slicing index dynamically\n-    end = min(start + shard_size, empty_param.shape[dim])\n-    slice_indices = [slice(None)] * param_dim\n-    if start < empty_param.shape[dim]:\n+    slice_indices = [slice(None)] * len(param.get_shape())\n+\n+    if start < param.get_shape()[dim]:\n         slice_indices[dim] = slice(start, end)\n-        return param[tuple(slice_indices)]\n-    dimensions = list(param.shape)\n+        param = param[tuple(slice_indices)]\n+        if isinstance(param, list):  # TODO handle the modulelist case!\n+            param = [p[:] for p in param]\n+        return param\n+\n     dimensions[dim] = 0\n-    return torch.empty(tuple(dimensions), dtype=torch.int64)\n+    return torch.empty(tuple(dimensions), dtype=torch.int64)  # empty allocates memory....\n \n \n def distribute_module(\n@@ -420,6 +446,19 @@ class TensorParallelLayer:\n     \"\"\"\n \n     use_dtensor = True\n+    device_mesh = None\n+    rank = None\n+\n+    # Used to compare the shape of the original tensor\n+    empty_param = None\n+\n+    # Used to init the corresponding DTensor\n+    shard = None\n+\n+    def __init__(self, device_mesh=None, rank=None, empty_param=None):\n+        self.rank = rank\n+        self.device_mesh = device_mesh\n+        self.empty_param = empty_param\n \n     @staticmethod\n     def _prepare_input_fn(input_layouts, desired_input_layouts, mod, inputs, device_mesh): ...\n@@ -449,12 +488,12 @@ class GatherParallel(TensorParallelLayer):\n \n     def __init__(\n         self,\n-        *,\n         input_layouts: Placement | None = None,\n         output_layouts: Placement | None = None,\n         use_local_output: bool = True,\n+        **kwargs,\n     ):\n-        super().__init__()\n+        super().__init__(**kwargs)\n         self.input_layouts = (input_layouts or Replicate(),)\n         self.output_layouts = output_layouts\n         self.desired_input_layouts = (Replicate(),)\n@@ -475,6 +514,21 @@ def _prepare_output_fn(output_layouts, use_local_output, mod, outputs, device_me\n             dist.all_reduce(outputs[0], op=dist.ReduceOp.SUM, async_op=False)\n         return outputs\n \n+    def shard_tensor(\n+        self,\n+        param,\n+        param_type=None,\n+        param_casting_dtype=None,\n+        to_contiguous=None,\n+        rank=None,\n+        device_mesh=None,\n+        tensor_idx=None,\n+    ):\n+        shard = [Replicate()]\n+        parameter = param[...].to(param_casting_dtype)\n+        self.shard = shard\n+        return parameter, shard\n+\n     def prepare_module_tp(self, module: nn.Module, device_mesh) -> nn.Module:\n         distribute_module(\n             module,\n@@ -503,6 +557,23 @@ def _prepare_output_fn(output_layouts, use_local_output, mod, outputs, device_me\n         # TODO: figure out dynamo support for instance method and switch this to instance method\n         return outputs\n \n+    def shard_tensor(\n+        self,\n+        param,\n+        param_type=None,\n+        param_casting_dtype=None,\n+        to_contiguous=None,\n+        rank=None,\n+        device_mesh=None,\n+        tensor_idx=None,\n+    ):\n+        mesh = device_mesh or self.device_mesh\n+        parameter = param[...].to(param_casting_dtype)\n+        if mesh is not None:\n+            parameter = parameter / mesh.size()\n+        self.shard = None\n+        return parameter, None\n+\n     def partition_tensor(self, param, empty_param, param_type, param_casting_dtype, to_contiguous, rank, device_mesh):\n         param = param[...].to(param_casting_dtype)\n         if to_contiguous:\n@@ -525,8 +596,8 @@ class ReplicateParallel(TensorParallelLayer):\n     This class is used to replicate computation in a TP layer (used in SP regions when we don't use sequence parallelism for example)\n     \"\"\"\n \n-    def __init__(self, *, use_dtensor=True, use_local_output=True):\n-        super().__init__()\n+    def __init__(self, use_dtensor=True, use_local_output=True, **kwargs):\n+        super().__init__(**kwargs)\n         self.input_layouts = (Replicate(),)\n         self.output_layouts = (Replicate(),)\n         self.desired_input_layouts = (Replicate(),)\n@@ -547,12 +618,33 @@ def _prepare_input_fn(input_layouts, desired_input_layouts, mod, inputs, device_\n     def _prepare_output_fn(output_layouts, use_local_output, mod, outputs, device_mesh):\n         return outputs.to_local() if use_local_output and isinstance(outputs, DTensor) else outputs\n \n+    def shard_tensor(\n+        self,\n+        param,\n+        param_type=None,\n+        param_casting_dtype=None,\n+        to_contiguous=None,\n+        rank=None,\n+        device_mesh=None,\n+        tensor_idx=None,\n+    ):\n+        parameter = param[...].to(param_casting_dtype)\n+        shard = [Replicate()]\n+        self.shard = shard\n+        return parameter, shard\n+\n     def partition_tensor(self, param, empty_param, param_type, param_casting_dtype, to_contiguous, rank, device_mesh):\n-        param = param[...].to(param_casting_dtype)\n-        if to_contiguous:\n-            param = param.contiguous()\n-        param = DTensor.from_local(param, device_mesh, [Replicate()], run_check=False)\n-        return param\n+        parameter, shard = self.shard_tensor(\n+            param,\n+            param_type=param_type,\n+            param_casting_dtype=param_casting_dtype,\n+            to_contiguous=to_contiguous,\n+            rank=rank,\n+            device_mesh=device_mesh,\n+        )\n+        if self.use_dtensor:\n+            parameter = DTensor.from_local(parameter, device_mesh, shard, run_check=False)\n+        return parameter\n \n \n class ColwiseParallel(TensorParallelLayer):\n@@ -562,13 +654,13 @@ class ColwiseParallel(TensorParallelLayer):\n \n     def __init__(\n         self,\n-        *,\n         input_layouts: Placement | None = None,\n         output_layouts: Placement | None = None,\n         use_local_output: bool = True,\n         use_dtensor=True,\n+        **kwargs,\n     ):\n-        super().__init__()\n+        super().__init__(**kwargs)\n         self.input_layouts = (input_layouts or Replicate(),)\n         self.output_layouts = (output_layouts or Shard(-1),)\n         self.desired_input_layouts = (Replicate(),)\n@@ -588,18 +680,34 @@ def _prepare_input_fn(input_layouts, desired_input_layouts, mod, inputs, device_\n             input_tensor = input_tensor.redistribute(placements=desired_input_layouts, async_op=False)\n         return input_tensor\n \n-    def partition_tensor(self, param, empty_param, param_type, param_casting_dtype, to_contiguous, rank, device_mesh):\n-        # colwise shard weight/bias to Shard(0), weight be Shard(-2) (0 if you have 1 dim only)\n-        # means Colwise as Linear is input * weight^T + bias, where\n-        # weight would become Shard(1)\n+    def shard_tensor(\n+        self,\n+        param,\n+        param_type=None,\n+        param_casting_dtype=None,\n+        to_contiguous=None,\n+        rank=None,\n+        device_mesh=None,\n+        tensor_idx=None,\n+    ):\n+        device_mesh = self.device_mesh\n+        empty_param = self.empty_param\n+        rank = self.rank\n         if param_type == \"bias\":\n-            parameter = get_tensor_shard(param, empty_param, device_mesh, rank, -1)\n+            parameter = get_tensor_shard(param, empty_param, device_mesh, rank, -1, tensor_idx)\n             shard = [Shard(-1)]\n         else:\n             shard = [Shard(-2)]\n-            parameter = get_tensor_shard(param, empty_param, device_mesh, rank, -2)\n-\n+            parameter = get_tensor_shard(param, empty_param, device_mesh, rank, -2, tensor_idx)\n         parameter = parameter.to(param_casting_dtype)\n+        self.shard = shard\n+        return parameter, shard\n+\n+    def partition_tensor(self, param, empty_param, param_type, param_casting_dtype, to_contiguous, rank, device_mesh):\n+        # colwise shard weight/bias to Shard(0), weight be Shard(-2) (0 if you have 1 dim only)\n+        # means Colwise as Linear is input * weight^T + bias, where\n+        # weight would become Shard(1)\n+        parameter, shard = self.shard_tensor(param, param_type, param_casting_dtype, to_contiguous, rank, device_mesh)\n         if to_contiguous:\n             parameter = parameter.contiguous()\n         if self.use_dtensor:\n@@ -618,6 +726,21 @@ def _prepare_output_fn(output_layouts, use_local_output, mod, outputs, device_me\n \n \n class PackedColwiseParallel(ColwiseParallel):\n+    def shard_tensor(\n+        self,\n+        param,\n+        param_type=None,\n+        param_casting_dtype=None,\n+        to_contiguous=None,\n+        rank=None,\n+        device_mesh=None,\n+        tensor_idx=None,\n+    ):\n+        device_mesh = device_mesh or self.device_mesh\n+        empty_param = self.empty_param\n+        rank = rank if rank is not None else self.rank\n+        return get_packed_weights(param, empty_param, device_mesh, rank, -2).to(param_casting_dtype), [Shard(-2)]\n+\n     def partition_tensor(self, param, empty_param, param_type, param_casting_dtype, to_contiguous, rank, device_mesh):\n         # colwise shard weight/bias to Shard(0), weight be Shard(-2) (0 if you have 1 dim only)\n         # means Colwise as Linear is input * weight^T + bias, where\n@@ -652,18 +775,41 @@ class RowwiseParallel(TensorParallelLayer):\n \n     def __init__(\n         self,\n-        *,\n         input_layouts: Placement | None = None,\n         output_layouts: Placement | None = None,\n         use_local_output: bool = True,\n         use_dtensor=True,\n+        **kwargs,\n     ):\n-        super().__init__()\n+        super().__init__(**kwargs)\n         self.input_layouts = (input_layouts or Shard(-1),)\n         self.output_layouts = (output_layouts or Replicate(),)\n         self.use_local_output = use_local_output\n         self.use_dtensor = use_dtensor\n \n+    def shard_tensor(\n+        self,\n+        param,\n+        param_type=None,\n+        param_casting_dtype=None,\n+        to_contiguous=None,\n+        rank=None,\n+        device_mesh=None,\n+        tensor_idx=None,\n+    ):\n+        device_mesh = device_mesh or self.device_mesh\n+        empty_param = self.empty_param\n+        rank = rank if rank is not None else self.rank\n+        if param_type == \"bias\":\n+            shard = [Replicate()]\n+            parameter = param[...]\n+        else:\n+            parameter = get_tensor_shard(param, empty_param, device_mesh, rank, -1, tensor_idx=tensor_idx)\n+            shard = [Shard(-1)]\n+        parameter = parameter.to(param_casting_dtype)\n+        self.shard = shard\n+        return parameter, shard\n+\n     def partition_tensor(self, param, empty_param, param_type, param_casting_dtype, to_contiguous, rank, device_mesh):\n         # Rowwise shard weight to Shard(1), bias to Replicate(), weight be Shard(1)\n         # means Rowwise as nn.Linear is input * weight^T + bias, where\n@@ -735,6 +881,21 @@ def prepare_module_tp(self, module: nn.Module, device_mesh) -> nn.Module:\n \n \n class PackedRowwiseParallel(RowwiseParallel):\n+    def shard_tensor(\n+        self,\n+        param,\n+        param_type=None,\n+        param_casting_dtype=None,\n+        to_contiguous=None,\n+        rank=None,\n+        device_mesh=None,\n+        tensor_idx=None,\n+    ):\n+        device_mesh = device_mesh or self.device_mesh\n+        empty_param = self.empty_param\n+        rank = rank if rank is not None else self.rank\n+        return get_packed_weights(param, empty_param, device_mesh, rank, -1), [Shard(-1)]\n+\n     def partition_tensor(self, param, empty_param, param_type, param_casting_dtype, to_contiguous, rank, device_mesh):\n         # colwise shard weight/bias to Shard(0), weight be Shard(-2) (0 if you have 1 dim only)\n         # means Colwise as Linear is input * weight^T + bias, where\n@@ -793,8 +954,8 @@ class SequenceParallel(TensorParallelLayer):\n         to ensure that they are replicated.\n     \"\"\"\n \n-    def __init__(self, *, sequence_dim: int = 1, use_local_output: bool = False, use_dtensor=False):\n-        super().__init__()\n+    def __init__(self, sequence_dim: int = 1, use_local_output: bool = False, use_dtensor=False, **kwargs):\n+        super().__init__(**kwargs)\n         self.input_layouts = (Replicate(),)\n         self.desired_input_layouts = (Shard(1),)\n         self.output_layouts = (Replicate(),)\n@@ -803,6 +964,21 @@ def __init__(self, *, sequence_dim: int = 1, use_local_output: bool = False, use\n         self.sequence_sharding = (Shard(sequence_dim),)\n         self.use_local_output = use_local_output\n \n+    def shard_tensor(\n+        self,\n+        param,\n+        param_type=None,\n+        param_casting_dtype=None,\n+        to_contiguous=None,\n+        rank=None,\n+        device_mesh=None,\n+        tensor_idx=None,\n+    ):\n+        parameter = param[...].to(param_casting_dtype)\n+        shard = [Replicate()]\n+        self.shard = shard\n+        return parameter, shard\n+\n     @staticmethod\n     def _prepare_input_fn(input_layouts, desired_input_layouts, mod, inputs, device_mesh):\n         input_tensor = inputs[0]\n@@ -837,10 +1013,34 @@ class GroupedGemmParallel(TensorParallelLayer):\n     Applies Expert Parallelism to MoE experts by loading the correct experts on each device.\n     \"\"\"\n \n-    def __init__(self):\n-        super().__init__()\n+    def __init__(self, **kwargs):\n+        super().__init__(**kwargs)\n         self.use_dtensor = False\n \n+    def shard_tensor(\n+        self,\n+        param,\n+        param_type=None,\n+        param_casting_dtype=None,\n+        to_contiguous=None,\n+        rank=None,\n+        device_mesh=None,\n+        tensor_idx=None,\n+    ):\n+        empty_param = self.empty_param\n+        ep_rank = self.rank\n+        device_mesh = self.device_mesh\n+\n+        global_num_experts = empty_param.shape[0]\n+        if global_num_experts % device_mesh.size() != 0:\n+            raise ValueError(\n+                f\"Global number of experts must be divisible by number of devices: {global_num_experts} % {device_mesh.size()} != 0\"\n+            )\n+        local_num_experts = global_num_experts // device_mesh.size()\n+        parameter = param[ep_rank * local_num_experts : (ep_rank + 1) * local_num_experts].to(param_casting_dtype)\n+        self.shard = None\n+        return parameter, None\n+\n     def partition_tensor(self, param, empty_param, param_type, param_casting_dtype, to_contiguous, rank, device_mesh):\n         ep_rank = rank\n         global_num_experts = empty_param.shape[0]\n@@ -861,8 +1061,8 @@ class RouterParallel(TensorParallelLayer):\n     \"\"\"\n \n     def __init__(self, *args, **kwargs):\n+        super().__init__(**kwargs)\n         self.args = args\n-        self.kwargs = kwargs\n         self.use_dtensor = False\n \n     @staticmethod\n@@ -927,6 +1127,20 @@ def _prepare_output_fn(output_layouts, use_local_output, mod, outputs, device_me\n         )  # masking class for one hot\n         return router_scores, router_indices\n \n+    def shard_tensor(\n+        self,\n+        param,\n+        param_type=None,\n+        param_casting_dtype=None,\n+        to_contiguous=None,\n+        rank=None,\n+        device_mesh=None,\n+        tensor_idx=None,\n+    ):\n+        parameter = param[...].to(param_casting_dtype)\n+        self.shard = None\n+        return parameter, None\n+\n     def partition_tensor(self, param, empty_param, param_type, param_casting_dtype, to_contiguous, rank, device_mesh):\n         # TODO: i'd like for this to be the default\n         param = param[...].to(param_casting_dtype)\n@@ -1069,6 +1283,9 @@ def shard_and_distribute_module(\n     if current_shard_plan is not None:\n         try:\n             tp_layer = ALL_PARALLEL_STYLES[current_shard_plan]\n+            tp_layer.empty_param = empty_param\n+            tp_layer.device_mesh = device_mesh\n+            tp_layer.rank = rank\n             param = tp_layer.partition_tensor(\n                 param, empty_param, param_type, param_casting_dtype, is_contiguous, rank, device_mesh\n             )"
        },
        {
            "sha": "34650a85b4e6e7fbcaa8bc6bd2f45b025b42f690",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 330,
            "deletions": 929,
            "changes": 1259,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -26,11 +26,11 @@\n import warnings\n from abc import abstractmethod\n from collections import defaultdict\n-from collections.abc import Callable\n-from concurrent.futures import ThreadPoolExecutor, as_completed\n+from collections.abc import Callable, Sequence\n from contextlib import contextmanager\n from enum import Enum\n from functools import partial, wraps\n+from itertools import cycle\n from threading import Thread\n from typing import Any, Optional, TypeVar, Union, get_type_hints\n from zipfile import is_zipfile\n@@ -45,17 +45,17 @@\n from torch.utils.checkpoint import checkpoint\n \n from .configuration_utils import PreTrainedConfig\n+from .conversion_mapping import get_checkpoint_conversion_mapping\n+from .core_model_loading import WeightConverter, convert_and_load_state_dict_in_model, revert_weight_conversion\n from .distributed import DistributedConfig\n from .dynamic_module_utils import custom_object_save\n from .generation import CompileConfig, GenerationConfig\n from .integrations import PeftAdapterMixin, deepspeed_config, is_deepspeed_zero3_enabled, is_fsdp_enabled\n from .integrations.accelerate import (\n     _get_device_map,\n-    accelerate_disk_offload,\n     accelerate_dispatch,\n     check_and_set_device_map,\n     expand_device_map,\n-    find_tied_parameters,\n     init_empty_weights,\n )\n from .integrations.deepspeed import _load_state_dict_into_zero3_model\n@@ -122,15 +122,14 @@\n     is_sagemaker_mp_enabled,\n     is_tracing,\n )\n+from .utils.loading_report import log_state_dict_report\n from .utils.quantization_config import QuantizationMethod\n \n \n if is_accelerate_available():\n     from accelerate.hooks import add_hook_to_module\n     from accelerate.utils import (\n         extract_model_from_parallel,\n-        offload_weight,\n-        save_offload_index,\n     )\n     from accelerate.utils.modeling import get_state_dict_from_offload\n \n@@ -182,6 +181,7 @@ def is_local_dist_rank_0():\n     \"xavier_normal\": nn.init.xavier_normal,\n     \"kaiming_uniform\": nn.init.kaiming_uniform,\n     \"kaiming_normal\": nn.init.kaiming_normal,\n+    \"orthogonal_\": nn.init.orthogonal_,\n }\n \n # DO NOT MODIFY, KEPT FOR BC ONLY\n@@ -467,17 +467,13 @@ def _end_ptr(tensor: torch.Tensor) -> int:\n     return stop\n \n \n-def _get_tied_weight_keys(module: nn.Module, prefix=\"\"):\n-    tied_weight_keys = []\n-    if getattr(module, \"_tied_weights_keys\", None) is not None:\n-        names = [f\"{prefix}.{k}\" if prefix else k for k in module._tied_weights_keys]\n-        tied_weight_keys.extend(names)\n-    if getattr(module, \"_dynamic_tied_weights_keys\", None) is not None:\n-        names = [f\"{prefix}.{k}\" if prefix else k for k in module._dynamic_tied_weights_keys]\n-        tied_weight_keys.extend(names)\n-    for name, submodule in module.named_children():\n-        local_prefix = f\"{prefix}.{name}\" if prefix else name\n-        tied_weight_keys.extend(_get_tied_weight_keys(submodule, prefix=local_prefix))\n+def _get_tied_weight_keys(module: nn.Module) -> list[str]:\n+    tied_weight_keys: list[str] = []\n+    for name, submodule in module.named_modules():\n+        tied = getattr(submodule, \"_tied_weights_keys\", {}) or {}\n+        tied_weights_dict = list(tied.keys())\n+        # tied_weights_dict.extend(tied.values())\n+        tied_weight_keys.extend([f\"{name}.{k}\" if name else k for k in tied_weights_dict])\n     return tied_weight_keys\n \n \n@@ -531,289 +527,20 @@ def _find_identical(tensors: list[set[str]], state_dict: dict[str, torch.Tensor]\n     return shared_tensors, identical\n \n \n-def _infer_parameter_dtype(\n-    model: \"PreTrainedModel\",\n-    param_name: str,\n-    empty_param: torch.Tensor,\n-    hf_quantizer: Optional[HfQuantizer] = None,\n-) -> Union[bool, Optional[torch.dtype]]:\n-    try:\n-        old_param = model.get_parameter_or_buffer(param_name)\n-    except Exception as e:\n-        if hf_quantizer is not None and hf_quantizer.quantization_config.quant_method in {\n-            QuantizationMethod.HQQ,\n-            QuantizationMethod.QUARK,\n-            QuantizationMethod.MXFP4,\n-            QuantizationMethod.BITS_AND_BYTES,\n-        }:\n-            return True, None\n-        else:\n-            raise e\n-    is_torch_e4m3fn_available = hasattr(torch, \"float8_e4m3fn\")\n-    # We convert floating dtypes to the `dtype` passed except for float8_e4m3fn type. We also want to keep the buffers/params\n-    # in int/uint/bool and not cast them.\n-    casting_dtype = None\n-    is_param_float8_e4m3fn = is_torch_e4m3fn_available and empty_param.dtype == torch.float8_e4m3fn\n-    if empty_param.dtype.is_floating_point and not is_param_float8_e4m3fn:\n-        # dtype that was instantiated in the meta model -- note that this respects subconfigs dtypes\n-        if hf_quantizer is not None and hf_quantizer.param_needs_quantization(model, param_name):\n-            casting_dtype = model.config._pre_quantization_dtype\n-        else:\n-            casting_dtype = old_param.dtype\n-    return old_param is not None and old_param.is_contiguous(), casting_dtype\n-\n-\n def _load_parameter_into_model(model: \"PreTrainedModel\", param_name: str, tensor: torch.Tensor):\n     \"\"\"Cast a single parameter `param_name` into the `model`, with value `tensor`.\"\"\"\n     module, param_type = get_module_from_name(model, param_name)\n     # This will check potential shape mismatch if skipped before\n     module.load_state_dict({param_type: tensor}, strict=False, assign=True)\n \n \n-@torch.no_grad()\n-def _load_state_dict_into_meta_model(\n-    model: \"PreTrainedModel\",\n-    state_dict: dict,\n-    shard_file: str,\n-    reverse_renaming_mapping: dict[str, str],\n-    device_map: Optional[dict] = None,\n-    disk_offload_folder: Optional[str] = None,\n-    disk_offload_index: Optional[dict] = None,\n-    hf_quantizer: Optional[HfQuantizer] = None,\n-    device_mesh: Optional[\"torch.distributed.device_mesh.DeviceMesh\"] = None,\n-) -> tuple[Optional[dict], Optional[dict]]:\n-    \"\"\"Load parameters from `meta_state_dict` into the model. The parameters of the `meta_state_dict` are on the meta\n-    device in order to easily infer the shapes and dtypes that they will have. Then proper parameters are then loaded\n-    from `shard_file`, which is the actual state dict file on disk.\n-    This function takes care of correctly casting dtypes, devices, and sharding tensors in case of tensor parallelism.\n-    \"\"\"\n-    tensor_device = \"cpu\"\n-    if device_map is not None and device_map.get(\"\", None) is not None:\n-        if device_map[\"\"] not in (\"cpu\", torch.device(\"cpu\")):\n-            tensor_device = device_map[\"\"].index if isinstance(device_map[\"\"], torch.device) else device_map[\"\"]\n-    if device_map is not None:\n-        device_map_regex = \"|\".join([re.escape(k) for k in sorted(device_map.keys(), reverse=True)])\n-\n-    is_quantized = hf_quantizer is not None\n-    is_safetensors = shard_file.endswith(\".safetensors\")\n-    is_meta_state_dict = is_safetensors\n-    file_pointer = safe_open(shard_file, framework=\"pt\", device=tensor_device) if is_meta_state_dict else None\n-    params_to_load = list(state_dict.keys())\n-\n-    for param_name in params_to_load:\n-        empty_param = state_dict[param_name]\n-        # we need to use serialized_param_name as file pointer is untouched\n-        if is_meta_state_dict:\n-            # This is the name of the parameter as it appears on disk file\n-            serialized_param_name = reverse_renaming_mapping[param_name]\n-            param = file_pointer.get_slice(serialized_param_name)\n-        else:\n-            param = empty_param.to(tensor_device)  # It is actually not empty!\n-        to_contiguous, casting_dtype = _infer_parameter_dtype(model, param_name, empty_param, hf_quantizer)\n-\n-        if device_mesh is not None:\n-            if not is_quantized or not hf_quantizer.param_needs_quantization(model, param_name):\n-                # In this case, the param is already on the correct device!\n-                shard_and_distribute_module(\n-                    model,\n-                    param,\n-                    empty_param,\n-                    param_name,\n-                    casting_dtype,\n-                    to_contiguous,\n-                    device_mesh.get_local_rank(),\n-                    device_mesh,\n-                )\n-            else:\n-                # we have a device mesh but the param needs to be quantized, so we shard inside create_quantized_param\n-                sharding_kwargs = {\n-                    \"empty_param\": empty_param,\n-                    \"casting_dtype\": casting_dtype,\n-                    \"to_contiguous\": to_contiguous,\n-                    \"rank\": device_mesh.get_local_rank(),\n-                    \"device_mesh\": device_mesh,\n-                }\n-                hf_quantizer.create_quantized_param(\n-                    model,\n-                    param,\n-                    param_name,\n-                    device_mesh.get_local_rank(),\n-                    **sharding_kwargs,\n-                )\n-        else:\n-            param = param[...]\n-            if casting_dtype is not None:\n-                param = param.to(casting_dtype)\n-            if to_contiguous:\n-                param = param.contiguous()\n-\n-            if device_map is None:\n-                param_device = \"cpu\"\n-            else:\n-                module_layer = re.search(device_map_regex, param_name)\n-                if not module_layer:\n-                    raise ValueError(f\"{param_name} doesn't have any device set.\")\n-                else:\n-                    param_device = device_map[module_layer.group()]\n-\n-            if param_device == \"disk\":\n-                if not is_safetensors:\n-                    disk_offload_index = offload_weight(param, param_name, disk_offload_folder, disk_offload_index)\n-            elif not is_quantized or not hf_quantizer.param_needs_quantization(model, param_name):\n-                if is_fsdp_enabled():\n-                    param_device = \"cpu\" if is_local_dist_rank_0() else \"meta\"\n-\n-                _load_parameter_into_model(model, param_name, param.to(param_device))\n-\n-            else:\n-                # TODO naming is stupid it loads it as well\n-                hf_quantizer.create_quantized_param(model, param, param_name, param_device)\n-\n-                # For quantized modules with FSDP/DeepSpeed Stage 3, we need to quantize the parameter on the GPU\n-                # and then cast it to CPU to avoid excessive memory usage on each GPU\n-                # in comparison to the sharded model across GPUs.\n-                if is_fsdp_enabled() or is_deepspeed_zero3_enabled():\n-                    param_name = hf_quantizer.get_param_name(param_name)\n-                    module, param_type = get_module_from_name(model, param_name)\n-                    value = getattr(module, param_type)\n-                    # We need to wait until the quantized value is created\n-                    if value.device.type == \"meta\":\n-                        continue\n-                    val_kwargs = value.__dict__\n-                    if not value.is_floating_point():\n-                        val_kwargs[\"requires_grad\"] = False\n-                    device = \"meta\" if is_fsdp_enabled() and not is_local_dist_rank_0() else \"cpu\"\n-                    value = type(value)(value.data.to(device), **val_kwargs)\n-                    setattr(module, param_type, value)\n-\n-        # Remove the param from the state dict if it was not loaded on the fly to avoid wasting memory\n-        if not is_meta_state_dict:\n-            del state_dict[param_name]\n-\n-    if file_pointer is not None:\n-        file_pointer.__exit__(None, None, None)\n-\n-    return disk_offload_index\n-\n-\n-def load_shard_file(args):\n-    (\n-        shard_file,\n-        state_dict,\n-        disk_only_shard_files,\n-        is_quantized,\n-        device_map,\n-        hf_quantizer,\n-        key_renaming_mapping,\n-        weights_only,\n-        model,\n-        reverse_key_renaming_mapping,\n-        disk_offload_folder,\n-        disk_offload_index,\n-        device_mesh,\n-    ) = args\n-\n-    # Skip the load for shards that only contain disk-offloaded weights\n-    if shard_file in disk_only_shard_files:\n-        return [], disk_offload_index\n-\n-    map_location = \"cpu\"\n-    if shard_file.endswith(\".safetensors\") and not (is_deepspeed_zero3_enabled() and not is_quantized):\n-        map_location = \"meta\"\n-\n-    # If shard_file is \"\", we use the existing state_dict instead of loading it\n-    if shard_file != \"\":\n-        state_dict = load_state_dict(\n-            shard_file, is_quantized=is_quantized, map_location=map_location, weights_only=weights_only\n-        )\n-\n-    # Fix the key names\n-    state_dict = {key_renaming_mapping[k]: v for k, v in state_dict.items() if k in key_renaming_mapping}\n-\n-    error_msgs = []\n-    if is_deepspeed_zero3_enabled() and not is_quantized:\n-        error_msgs += _load_state_dict_into_zero3_model(model, state_dict)\n-    # Skip it with fsdp on ranks other than 0\n-    elif not (is_fsdp_enabled() and not is_local_dist_rank_0() and not is_quantized):\n-        disk_offload_index = _load_state_dict_into_meta_model(\n-            model,\n-            state_dict,\n-            shard_file,\n-            reverse_key_renaming_mapping,\n-            device_map=device_map,\n-            disk_offload_folder=disk_offload_folder,\n-            disk_offload_index=disk_offload_index,\n-            hf_quantizer=hf_quantizer,\n-            device_mesh=device_mesh,\n-        )\n-\n-    return error_msgs, disk_offload_index\n-\n-\n-def load_shard_files_with_threadpool(args_list):\n-    num_workers = int(os.environ.get(\"HF_PARALLEL_LOADING_WORKERS\", \"8\"))\n-\n-    # Do not spawn anymore workers than you need\n-    num_workers = min(len(args_list), num_workers)\n-\n-    logger.info(f\"Loading model weights in parallel with {num_workers} workers...\")\n-\n-    error_msgs = []\n-\n-    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n-        with logging.tqdm(total=len(args_list), desc=\"Loading checkpoint shards\") as pbar:\n-            futures = [executor.submit(load_shard_file, arg) for arg in args_list]\n-            for future in as_completed(futures):\n-                _error_msgs, disk_offload_index = future.result()\n-\n-                error_msgs += _error_msgs\n-\n-                pbar.update(1)\n-\n-    return error_msgs, disk_offload_index\n-\n-\n def _add_variant(weights_name: str, variant: Optional[str] = None) -> str:\n     if variant is not None:\n         path, name = weights_name.rsplit(\".\", 1)\n         weights_name = f\"{path}.{variant}.{name}\"\n     return weights_name\n \n \n-def update_key_name(keys):\n-    \"\"\"\n-    Updates a dictionary of keys to pack layers together as layer.{0, 1, 4} instead of layers.0, layers.1, layers.4.\n-    \"\"\"\n-    key_dict = defaultdict(list)\n-    for key in keys:\n-        all_digits = re.findall(r\".(\\d+).\", key)\n-        for i, k in enumerate(all_digits):\n-            if len(key_dict[re.sub(r\".(\\d+).\", \".*.\", key)]) <= i:\n-                key_dict[re.sub(r\".(\\d+).\", \".*.\", key)].append(set())\n-            key_dict[re.sub(r\".(\\d+).\", \".*.\", key)][i].add(int(k))\n-\n-    final_keys = set()\n-    for key in keys:\n-        text = re.sub(r\".(\\d+).\", \".*.\", key)\n-        pattern = key_dict[text]\n-        final_text = \"\"\n-        for i, part in enumerate(text.split(\"*\")):\n-            if len(pattern) <= i:\n-                final_text += part\n-            else:\n-                data = [str(i) for i in sorted(pattern[i])]\n-                if len(data) > 10:\n-                    result = f\"{data[0]}...{data[-1]}\"\n-                else:\n-                    result = \", \".join(data)  # If there are only 1 or 2 elements, show them all\n-                if len(data) > 1:\n-                    final_text += part + \"{\" + result + \"}\"\n-                else:\n-                    final_text += part + data[0]\n-        final_keys.add(final_text)\n-    return sorted(final_keys)\n-\n-\n def _get_resolved_checkpoint_files(\n     pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],\n     variant: Optional[str],\n@@ -941,7 +668,7 @@ def _get_resolved_checkpoint_files(\n                     if resolved_archive_file is not None:\n                         is_sharded = True\n                     elif use_safetensors:\n-                        if revision == \"main\":\n+                        if revision == \"main\" and not is_offline_mode():\n                             resolved_archive_file, revision, is_sharded = auto_conversion(\n                                 pretrained_model_name_or_path, **cached_file_kwargs\n                             )\n@@ -1174,102 +901,33 @@ def _get_dtype(\n     return config, dtype, dtype_orig\n \n \n-def _find_missing_and_unexpected_keys(\n-    model: \"PreTrainedModel\",\n-    original_checkpoint_keys: list[str],\n-    checkpoint_keys: list[str],\n-    loading_base_model_from_task_state_dict: bool,\n-    hf_quantizer: Optional[HfQuantizer],\n-) -> tuple[list[str], list[str]]:\n-    \"\"\"Find missing keys (keys that are part of the model parameters but were NOT found in the loaded state dict keys) and unexpected keys\n-    (keys found in the loaded state dict keys, but that are NOT part of the model parameters)\n-    \"\"\"\n-    prefix = model.base_model_prefix\n-\n-    # Compute expected keys, i.e. keys that the full model expects\n-    expected_keys = list(model.state_dict().keys())\n-    if hf_quantizer is not None:\n-        expected_keys = hf_quantizer.update_expected_keys(model, expected_keys, checkpoint_keys)\n-\n-    # Adjust prefix of the keys to make them match loaded keys before removing them\n-    missing_keys = sorted(set(expected_keys) - set(checkpoint_keys))\n-    unexpected_keys = set(checkpoint_keys) - set(expected_keys)\n-    # If a module has the same name under the base and task specific model, we have to re-add it to unexpected keys\n-    if loading_base_model_from_task_state_dict:\n-        task_specific_keys = [k for k in original_checkpoint_keys if not k.startswith(f\"{prefix}.\")]\n-        unexpected_keys.update(task_specific_keys)\n-\n-    # Remove nonpersistent buffers from unexpected keys: they are not in the expected keys (model state dict), but\n-    # may be in the loaded keys. Note that removing all buffers does the job, as they were part of the expected keys anyway\n-    model_buffers = {n for n, _ in model.named_buffers()}\n-    unexpected_keys = sorted(unexpected_keys - model_buffers)\n-\n-    tied_params = find_tied_parameters(model)\n-    for group in tied_params:\n-        missing_in_group = [k for k in missing_keys if k in group]\n-        if len(missing_in_group) > 0 and len(missing_in_group) < len(group):\n-            missing_keys = [k for k in missing_keys if k not in missing_in_group]\n-\n-    if hf_quantizer is not None:\n-        missing_keys = hf_quantizer.update_missing_keys(model, missing_keys, prefix)\n-        unexpected_keys = hf_quantizer.update_unexpected_keys(model, unexpected_keys)\n-\n-    return missing_keys, unexpected_keys\n-\n+@contextmanager\n+def guard_nn_init_functions(flag_name: str = \"_is_hf_initialized\"):\n+    import torch.nn.init as init\n \n-def _find_mismatched_keys(\n-    model: \"PreTrainedModel\",\n-    state_dict: Optional[dict],\n-    checkpoint_files: Optional[list[str]],\n-    ignore_mismatched_sizes: bool,\n-    keys_to_rename_mapping: dict[str, str],\n-    is_quantized: bool,\n-    weights_only: bool,\n-) -> tuple[list[str], list[tuple[int, int]]]:\n-    \"\"\"\n-    Find potential shape mismatch between the different state dicts and the model parameters, but only if `ignore_mismatched_sizes`\n-    is True. Otherwise, return immediately and any shape mismatch that may exist will be raised later on. This avoids checking\n-    every parameter in advance, as shape mismatch are extremely rare in practice. If we want to ignore them however, we do\n-    need to check in advance as we need to know which parameters we need to move back from meta to cpu, and initialize\n-    correctly. Indeed, as our model initialization takes place at the module level, and not the weight level, in the\n-    case of a sharded checkpoint we cannot correctly initialize the weights according to `model._init_weights()` if we perform\n-    this check on each state dict at loading time (after the first loaded checkpoint, there are no way to initialize only the\n-    mismatched weights if any, without overwriting the previously loaded weights as well because all the module will be\n-    initialized, not only the weights that are mismatched).\n-    \"\"\"\n+    originals = {}\n \n-    # An error will be raised later on anyway if there is a mismatch - this avoids running the rest of this function\n-    # if there are no mismatch (which is almost always the case)\n-    if not ignore_mismatched_sizes:\n-        return [], []\n-\n-    if state_dict is not None:\n-        checkpoint_files = [\"\"]\n-\n-    model_state_dict = model.state_dict()\n-    mismatched_keys = []\n-    mismatched_shapes = []\n-    for shard_file in checkpoint_files:\n-        # If shard_file is \"\", we use the existing state_dict instead of loading it\n-        if shard_file != \"\":\n-            state_dict = load_state_dict(\n-                shard_file, is_quantized=is_quantized, map_location=\"meta\", weights_only=weights_only\n-            )\n+    def make_wrapper(fn):\n+        @wraps(fn)\n+        def wrapped(*args, **kwargs):\n+            # Tensor can come positionally or as a kwarg (e.g. via DeviceContext)\n+            t = args[0] if args else kwargs.get(\"tensor\", kwargs.get(\"input\"))\n+            if t is not None and getattr(t, flag_name, False):\n+                # mimic init.* return convention (returns the tensor)\n+                return t\n+            return fn(*args, **kwargs)  # TODO we could set is init here.\n \n-        # Fix the key names\n-        new_state_dict = {keys_to_rename_mapping[k]: v for k, v in state_dict.items() if k in keys_to_rename_mapping}\n+        return wrapped\n \n-        for key, tensor in new_state_dict.items():\n-            if key in model_state_dict and tensor.shape != model_state_dict[key].shape:\n-                # This skips size mismatches for 4-bit weights. Two 4-bit values share an 8-bit container, causing size differences.\n-                # Without matching with module type or parameter type it seems like a practical way to detect valid 4bit weights.\n-                if not (\n-                    is_quantized and tensor.shape[-1] == 1 and tensor.numel() * 2 == model_state_dict[key].numel()\n-                ):\n-                    mismatched_keys.append(key)\n-                    mismatched_shapes.append((tensor.shape, model_state_dict[key].shape))\n-\n-    return mismatched_keys, mismatched_shapes\n+    try:\n+        for name in TORCH_INIT_FUNCTIONS:\n+            if hasattr(init, name):\n+                originals[name] = getattr(init, name)\n+                setattr(init, name, make_wrapper(originals[name]))\n+        yield\n+    finally:\n+        for name, fn in originals.items():\n+            setattr(init, name, fn)\n \n \n class PipelineParallel(Enum):\n@@ -1677,6 +1335,8 @@ class PreTrainedModel(nn.Module, EmbeddingAccessMixin, ModuleUtilsMixin, PushToH\n     # to also prevent bfloat16 casting, use the _keep_in_fp32_modules_strict flag\n     _keep_in_fp32_modules_strict = None\n \n+    dtype_plan: Optional[dict[str, torch.dtype]] = None\n+\n     # a list of `re` patterns of `state_dict` keys that should be removed from the list of missing\n     # keys we find (keys inside the model but not in the checkpoint) and avoid unnecessary warnings.\n     _keys_to_ignore_on_load_missing = None\n@@ -1841,11 +1501,18 @@ def __init__(self, config: PreTrainedConfig, *inputs, **kwargs):\n         self.name_or_path = config.name_or_path\n         self.warnings_issued = {}\n         self.generation_config = GenerationConfig.from_model_config(config) if self.can_generate() else None\n+\n         # Overwrite the class attribute to make it an instance attribute, so models like\n         # `InstructBlipForConditionalGeneration` can dynamically update it without modifying the class attribute\n         # when a different component (e.g. language_model) is used.\n         self._keep_in_fp32_modules = copy.copy(self.__class__._keep_in_fp32_modules)\n         self._keep_in_fp32_modules_strict = copy.copy(self.__class__._keep_in_fp32_modules_strict)\n+        self.dtype_plan = {}\n+\n+        if isinstance(self._keep_in_fp32_modules, list):\n+            self.dtype_plan.update(dict.fromkeys(self._keep_in_fp32_modules, torch.float32))\n+        if isinstance(self._keep_in_fp32_modules_strict, list):\n+            self.dtype_plan.update(dict.fromkeys(self._keep_in_fp32_modules_strict, torch.float32))\n \n         self._no_split_modules = self._no_split_modules or []\n         _CAN_RECORD_REGISTRY[str(self.__class__)] = self._can_record_outputs  # added for executorch support only\n@@ -1861,32 +1528,6 @@ def post_init(self):\n         self.init_weights()\n         self._backward_compatibility_gradient_checkpointing()\n \n-        # Make sure the modules correctly exist if the flag is active\n-        if self._keep_in_fp32_modules is not None or self._keep_in_fp32_modules_strict is not None:\n-            all_parameters = {name for name, _ in self.named_parameters() if len(name) > 0}\n-            unique_module_names = set()\n-            # Get all unique module names in the module graph, without the prefixes\n-            for param in all_parameters:\n-                unique_module_names.update(\n-                    [name for name in param.split(\".\") if not name.isnumeric() and name not in [\"weight\", \"bias\"]]\n-                )\n-            # Check that every module in the keep_in_fp32 list is part of the module graph\n-            if self._keep_in_fp32_modules is not None:\n-                for module in self._keep_in_fp32_modules:\n-                    if module not in unique_module_names:\n-                        raise ValueError(\n-                            f\"{module} was specified in the `_keep_in_fp32_modules` list, but is not part of the modules in\"\n-                            f\" {self.__class__.__name__}\"\n-                        )\n-\n-            if self._keep_in_fp32_modules_strict is not None:\n-                for module in self._keep_in_fp32_modules_strict:\n-                    if module not in unique_module_names:\n-                        raise ValueError(\n-                            f\"{module} was specified in the `_keep_in_fp32_modules_strict` list, but is not part of the modules in\"\n-                            f\" {self.__class__.__name__}\"\n-                        )\n-\n         self._tp_plan, self._ep_plan, self._pp_plan = {}, {}, {}\n         # If current model is a base model, attach `base_model_tp_plan` and `base_model_pp_plan` from config\n         if self.base_model is self:\n@@ -2625,26 +2266,31 @@ def set_decoder(self, decoder):\n \n         return\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"\n         Initialize the weights. This is quite general on purpose, in the spirit of what we usually do. For more complex\n         initialization scheme, it should be overridden by the derived `PreTrainedModel` class. In case a model adds an explicit\n         `nn.Parameter`, this method should also be overridden in order to initialize it correctly.\n         \"\"\"\n         if hasattr(self.config, \"initializer_range\"):\n-            std = self.config.initializer_range\n+            std = self.config.initializer_range or 0.02\n         else:\n             # 0.02 is the standard default value across the library\n             std = getattr(self.config.get_text_config(), \"initializer_range\", 0.02)\n \n         if isinstance(module, (nn.Linear, nn.Conv1d, nn.Conv2d, nn.Conv3d, nn.ConvTranspose1d, nn.ConvTranspose2d)):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n+            if getattr(module, \"weight\", None) is not None:\n+                module.weight.normal_(mean=0.0, std=std)\n+            if getattr(module, \"bias\", None) is not None:\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+            if getattr(module, \"weight\", None) is not None:\n+                module.weight.normal_(mean=0.0, std=std)\n+            if getattr(\n+                self.config, \"pad_token_id\", None\n+            ) is not None and self.config.pad_token_id < module.weight.size(0):\n+                module.weight[self.config.pad_token_id].zero_()\n         elif isinstance(module, nn.MultiheadAttention):\n             # This uses torch's original init\n             module._reset_parameters()\n@@ -2657,20 +2303,28 @@ def _init_weights(self, module):\n         ):\n             # Norms can exist without weights (in which case they are None from torch primitives)\n             if hasattr(module, \"weight\") and module.weight is not None:\n-                module.weight.data.fill_(1.0)\n+                module.weight.fill_(1.0)\n             if hasattr(module, \"bias\") and module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n+        if isinstance(getattr(module, \"gate_up_proj\", None), nn.Parameter):\n+            module.gate_up_proj.normal_(mean=0.0, std=std)\n+        if isinstance(getattr(module, \"down_proj\", None), nn.Parameter):\n+            module.down_proj.normal_(mean=0.0, std=std)\n+        if isinstance(getattr(module, \"gate\", None), nn.Parameter):\n+            module.gate.normal_(mean=0.0, std=std)\n \n     def _initialize_weights(self, module):\n         \"\"\"\n         Initialize the weights if they are not already initialized.\n         \"\"\"\n         if getattr(module, \"_is_hf_initialized\", False):\n             return\n+\n         self._init_weights(module)\n         module._is_hf_initialized = True\n \n     @torch.no_grad()\n+    @guard_nn_init_functions()\n     def initialize_weights(self):\n         \"\"\"\n         This is equivalent to calling `self.apply(self._initialize_weights)`, but correctly handles composite models.\n@@ -2681,7 +2335,7 @@ def initialize_weights(self):\n \n         Note that the `torch.no_grad()` decorator is very important as well, as most of our `_init_weights` do not use\n         `torch.nn.init` functions (which are all no_grad by default), but simply do in-place ops such as\n-        `module.weight.data.zero_()`.\n+        `module.weight.zero_()`.\n         \"\"\"\n         if not hasattr(torch.nn.Module, \"smart_apply\"):\n             # This function is equivalent to `torch.nn.Module.apply`, except that it dynamically adjust the function\n@@ -2701,155 +2355,134 @@ def smart_apply(self, fn):\n         # Let the magic happen with this simple call\n         self.smart_apply(self._initialize_weights)\n \n-    def tie_embeddings_and_encoder_decoder(self):\n+    def tie_weight_source_and_target(\n+        self,\n+        top_level: \"PreTrainedModel\",\n+        missing_keys: Optional[set[str]] = None,\n+        module_prefix: str = \"\",\n+    ):\n         \"\"\"\n         If set in the config, tie the weights between the input embeddings and the output embeddings,\n-        and the encoder and decoder.\n-        \"\"\"\n-        if getattr(self.config.get_text_config(decoder=True), \"tie_word_embeddings\", True):\n-            output_embeddings = self.get_output_embeddings()\n-            if output_embeddings is not None:\n-                self._tie_embedding_weights(output_embeddings, self.get_input_embeddings())\n-\n-        if getattr(self.config, \"is_encoder_decoder\", False) and getattr(self.config, \"tie_encoder_decoder\", False):\n-            if hasattr(self, self.base_model_prefix):\n-                self = getattr(self, self.base_model_prefix)\n-            tied_weights = self._tie_encoder_decoder_weights(\n-                self.encoder, self.decoder, self.base_model_prefix, \"encoder\"\n-            )\n-            # Setting a dynamic variable instead of `_tied_weights_keys` because it's a class\n-            # attributed not an instance member, therefore modifying it will modify the entire class\n-            # Leading to issues on subsequent calls by different tests or subsequent calls.\n-            self._dynamic_tied_weights_keys = tied_weights\n+        and the encoder and decoder. This relies on the `_tied_weights_keys` dict.\n \n-    def tie_weights(self):\n-        \"\"\"\n-        Recursively (for all submodels) tie all the weights of the model.\n-        \"\"\"\n-        # Note that `self` is included in `self.modules` so we also apply to current PreTrainedModel with this call\n-        for module in self.modules():\n-            # If it's a PreTrainedModel, may need to tie the embeddings and/or encoder/decoder weights\n-            if isinstance(module, PreTrainedModel):\n-                module.tie_embeddings_and_encoder_decoder()\n-            # Additionally, if it has a custom `_tie_weights`, honor it\n-            if hasattr(module, \"_tie_weights\"):\n-                module._tie_weights()\n+        This is very sensible! For many reasons and especially this one:\n+        ```python\n+        from torch import nn\n+        import torch\n+        class MyClass(nn.Module):\n+            def __init__(self):\n+                super().__init__()\n+                self.proj = nn.Linear(8,8)\n+                self.bias = nn.Parameter(torch.empty(8))\n+                self.proj.bias = self.bias\n+\n+        c = MyClass()\n+        print(list(c.named_parameters()))\n+        ```\n+        That's for a parameter, for a module, it will just remove the ones that are \"shared\" (that makes sense) and overwrite getattr for it.\n \n-    @staticmethod\n-    def _tie_encoder_decoder_weights(\n-        encoder: nn.Module, decoder: nn.Module, base_model_prefix: str, base_encoder_name: str\n-    ):\n-        uninitialized_encoder_weights: list[str] = []\n-        tied_weights: list[str] = []\n-        if decoder.__class__ != encoder.__class__:\n-            logger.info(\n-                f\"{decoder.__class__} and {encoder.__class__} are not equal. In this case make sure that all encoder\"\n-                \" weights are correctly initialized.\"\n-            )\n+        ```python\n+        from torch import nn\n+        import torch\n+        class Decoder(nn.Module):\n+            def __init__(self):\n+                super().__init__()\n+                self.embedding = nn.Embedding(8,8)\n+\n+        class Encoder(nn.Module):\n+            def __init__(self):\n+                super().__init__()\n+                self.embedding = nn.Embedding(8,8)\n+\n+        class EncoderDecoder(nn.Module):\n+            def __init__(self):\n+                super().__init__()\n+                self.encoder = Encoder()\n+                self.decoder = Decoder()\n+                self.encoder.embedding = self.decoder.embedding # setattr is convenient\n+\n+        c = EncoderDecoder()\n+        print(list(c.named_parameters()))\n+        ```\n+        Thus the order of the keys matters. If you tie `self.decoder.embedding` you can no longer tie anything inside it.\n \n-        def tie_encoder_to_decoder_recursively(\n-            decoder_pointer: nn.Module,\n-            encoder_pointer: nn.Module,\n-            module_name: str,\n-            base_encoder_name: str,\n-            uninitialized_encoder_weights: list[str],\n-            depth=0,\n-            total_decoder_name=\"\",\n-            total_encoder_name=\"\",\n+        If you call this function, it will always tie. There is only 1 tricky case, if all weights are missing, you still want to mention that\n+        the ones you tied were missing.\n+        \"\"\"\n+        mapping = getattr(self, \"_tied_weights_keys\", None)\n+        if not isinstance(mapping, dict):\n+            return\n+        if (  # we only tie for ourselves, so we look at our config\n+            not self.config.tie_word_embeddings\n+            and not self.config.tie_encoder_decoder  # if missing keys is None we init?\n         ):\n-            assert isinstance(decoder_pointer, nn.Module) and isinstance(encoder_pointer, nn.Module), (\n-                f\"{decoder_pointer} and {encoder_pointer} have to be of type nn.Module\"\n-            )\n-            if hasattr(decoder_pointer, \"weight\"):\n-                assert hasattr(encoder_pointer, \"weight\")\n-                encoder_pointer.weight = decoder_pointer.weight\n-                tied_weights.append(f\"{base_encoder_name}{total_encoder_name}.weight\")\n-                if hasattr(decoder_pointer, \"bias\"):\n-                    assert hasattr(encoder_pointer, \"bias\")\n-                    tied_weights.append(f\"{base_encoder_name}{total_encoder_name}.bias\")\n-                    encoder_pointer.bias = decoder_pointer.bias\n-                return\n-\n-            encoder_modules = encoder_pointer._modules\n-            decoder_modules = decoder_pointer._modules\n-            if len(decoder_modules) > 0:\n-                assert len(encoder_modules) > 0, (\n-                    f\"Encoder module {encoder_pointer} does not match decoder module {decoder_pointer}\"\n-                )\n-\n-                all_encoder_weights = {module_name + \"/\" + sub_name for sub_name in encoder_modules}\n-                encoder_layer_pos = 0\n-                for name in decoder_modules:\n-                    if name.isdigit():\n-                        encoder_name = str(int(name) + encoder_layer_pos)\n-                        decoder_name = name\n-                        if not isinstance(decoder_modules[decoder_name], type(encoder_modules[encoder_name])) and len(\n-                            encoder_modules\n-                        ) != len(decoder_modules):\n-                            # this can happen if the name corresponds to the position in a list module list of layers\n-                            # in this case the decoder has added a cross-attention that the encoder does not have\n-                            # thus skip this step and subtract one layer pos from encoder\n-                            encoder_layer_pos -= 1\n-                            continue\n-                    elif name not in encoder_modules:\n-                        continue\n-                    elif depth > 500:\n-                        raise ValueError(\n-                            \"Max depth of recursive function `tie_encoder_to_decoder` reached. It seems that there is\"\n-                            \" a circular dependency between two or more `nn.Modules` of your model.\"\n-                        )\n-                    else:\n-                        decoder_name = encoder_name = name\n-                    tie_encoder_to_decoder_recursively(\n-                        decoder_modules[decoder_name],\n-                        encoder_modules[encoder_name],\n-                        module_name + \"/\" + name,\n-                        base_encoder_name,\n-                        uninitialized_encoder_weights,\n-                        depth=depth + 1,\n-                        total_encoder_name=f\"{total_encoder_name}.{encoder_name}\",\n-                        total_decoder_name=f\"{total_decoder_name}.{decoder_name}\",\n-                    )\n-                    all_encoder_weights.remove(module_name + \"/\" + encoder_name)\n-\n-                uninitialized_encoder_weights += list(all_encoder_weights)\n+            return\n \n-        # tie weights recursively\n-        tie_encoder_to_decoder_recursively(\n-            decoder, encoder, base_model_prefix, base_encoder_name, uninitialized_encoder_weights\n+        # TODO let's pray this is not too slow :)\n+        top_level_params = dict(top_level.named_parameters(remove_duplicate=False)) | dict(\n+            top_level.named_buffers(remove_duplicate=False)\n         )\n+        for target_name, source_name in mapping.items():\n+            source_name = f\"^{module_prefix}.{source_name}\" if module_prefix else \"^\" + source_name\n+            target_name = f\"^{module_prefix}.{target_name}\" if module_prefix else \"^\" + target_name\n \n-        if len(uninitialized_encoder_weights) > 0:\n-            logger.warning(\n-                f\"The following encoder weights were not tied to the decoder {uninitialized_encoder_weights}\"\n-            )\n-        return tied_weights\n-\n-    def _tie_embedding_weights(self, output_embeddings, input_embeddings):\n-        \"\"\"Tie weights, and add hooks and flags if using TP.\"\"\"\n-        output_embeddings.weight = input_embeddings.weight\n-\n-        # Passing hooks over to the embeddings if needed\n-        # (currently limited to tensor parallel hooks and flags only)\n-        if hasattr(input_embeddings, \"_is_hooked\") and getattr(input_embeddings, \"_hf_tp_plan\", None):\n-            output_embeddings._is_hooked = input_embeddings._is_hooked\n-            output_embeddings._hf_tp_plan = input_embeddings._hf_tp_plan\n-            output_embeddings._forward_hooks = input_embeddings._forward_hooks\n-            output_embeddings._forward_pre_hooks = input_embeddings._forward_pre_hooks\n-            output_embeddings.__repr__ = (\n-                lambda: f\"{output_embeddings.__repr__()}\\nTP Plan: {output_embeddings._hf_tp_plan}\"\n+            source_is_there = bool(missing_keys) and not re.search(\n+                source_name, \"\\n\".join(missing_keys), flags=re.MULTILINE\n             )\n+            source_params = sorted(filter(lambda x: re.search(source_name, x), top_level_params.keys()))\n+            target_params = sorted(filter(lambda x: re.search(target_name, x), top_level_params.keys()))\n+            if not len(source_params) > 0 or len(target_params) % len(source_params) != 0:\n+                raise ValueError(\n+                    f\"There is an issue with your definition of `tie_weights_keys` for {source_name}:{target_name}. We found {source_params} to tie into {target_params}\"\n+                )\n+            if len(target_params) > 0:\n+                # we cycle source as it should be dispatch in many target if regex\n+                for target_n, source_n in zip(target_params, cycle(source_params)):\n+                    if \".\" in target_n:\n+                        parent_path, last = target_n.rsplit(\".\", 1)\n+                        parent = top_level.get_submodule(parent_path)\n+                    else:\n+                        parent_path, last = \"\", target_n\n+                        parent = top_level  # top-level\n+                    setattr(parent, last, top_level_params[source_n])\n+                    self._adjust_bias(parent, top_level_params[source_n])\n+                    if missing_keys and source_is_there:  # test_model_weights_reload_no_missing_tied_weights\n+                        missing_keys.discard(target_n)\n+            else:\n+                target_is_not_there = missing_keys and re.search(\n+                    target_name, \"\\n\".join(missing_keys), flags=re.MULTILINE\n+                )\n+                raise ValueError(\n+                    \"There is a problem in the way you tie your keys or the way they were saved.\\n\"\n+                    f\"source_is_there={source_is_there}, target_is_there={not target_is_not_there}, missing_keys={missing_keys},\"\n+                    \"tie_word_embeddings/tie_encoder_decoder={(self.config.tie_word_embeddings or self.config.tie_encoder_decoder)}\"\n+                )\n \n-        if getattr(output_embeddings, \"bias\", None) is not None:\n+    def _adjust_bias(self, output_embeddings, input_embeddings):\n+        if getattr(output_embeddings, \"bias\", None) is not None and hasattr(output_embeddings, \"weight\"):\n+            weight_shape = output_embeddings.weight.shape\n             output_embeddings.bias.data = nn.functional.pad(\n                 output_embeddings.bias.data,\n-                (0, output_embeddings.weight.shape[0] - output_embeddings.bias.shape[0]),\n+                (0, weight_shape[0] - output_embeddings.bias.shape[0]),\n                 \"constant\",\n                 0,\n             )\n         if hasattr(output_embeddings, \"out_features\") and hasattr(input_embeddings, \"num_embeddings\"):\n             output_embeddings.out_features = input_embeddings.num_embeddings\n \n+    def tie_weights(self, missing_keys: Optional[set[str]] = None):\n+        \"\"\"\n+        Recursively (for all submodels) tie all the weights of the model.\n+        \"\"\"\n+        # Note that `self` is included in `self.modules` so we also apply to current PreTrainedModel with this call\n+        if missing_keys is None:\n+            # called from `post_init`\n+            self.tie_weight_source_and_target(self, missing_keys, \"\")\n+        else:  # this is from_pretrained, so its not called on every sub module\n+            for module_prefix, module in self.named_modules():\n+                if isinstance(module, PreTrainedModel):\n+                    module.tie_weight_source_and_target(self, missing_keys, module_prefix)\n+\n     def _get_no_split_modules(self, device_map: str):\n         \"\"\"\n         Get the modules of the model that should not be spit when using device_map. We iterate through the modules to\n@@ -3352,9 +2985,8 @@ def init_weights(self):\n         if _init_weights:\n             # Initialize weights\n             self.initialize_weights()\n-\n-            # Tie weights should be skipped when not initializing all weights\n-            # since from_pretrained(...) calls tie weights anyways\n+            # Tie weights needs to be called as it figures out recursively if sub modules\n+            # need to tie\n             self.tie_weights()\n \n     def gradient_checkpointing_enable(self, gradient_checkpointing_kwargs=None):\n@@ -3457,6 +3089,7 @@ def save_pretrained(\n         variant: Optional[str] = None,\n         token: Optional[Union[str, bool]] = None,\n         save_peft_format: bool = True,\n+        save_original_format: bool = False,  # TODO next PR will make it go to True\n         **kwargs,\n     ):\n         \"\"\"\n@@ -3505,6 +3138,10 @@ def save_pretrained(\n                 For backward compatibility with PEFT library, in case adapter weights are attached to the model, all\n                 keys of the state dict of adapters needs to be prepended with `base_model.model`. Advanced users can\n                 disable this behaviours by setting `save_peft_format` to `False`.\n+            save_original_format (`bool`, *optional*, defaults to `True`):\n+                For backward compatibility with the previous versions of `transfomers` you can save the checkpoint with\n+                its reverse mapping. The reverse mapping needs to exists even if the model was loaded from a None legacy\n+                checkpoint.\n             kwargs (`dict[str, Any]`, *optional*):\n                 Additional key word arguments passed along to the [`~utils.PushToHubMixin.push_to_hub`] method.\n         \"\"\"\n@@ -3644,25 +3281,6 @@ def save_pretrained(\n                         module_map[name + f\".{key}\"] = module\n             state_dict = model_to_save.state_dict()\n \n-        if any(\n-            allowed_name in class_name.__name__.lower()\n-            for class_name in self.__class__.__mro__[:-1]\n-            for allowed_name in VLMS\n-        ):\n-            reverse_key_mapping = {v: k for k, v in self._checkpoint_conversion_mapping.items()}\n-\n-            original_state_dict = {}\n-            for key, value in state_dict.items():\n-                for pattern, replacement in reverse_key_mapping.items():\n-                    replacement = replacement.lstrip(\"^\")  # strip off un-needed chars and patterns\n-                    replacement = re.sub(r\"\\(.*\\)\", \"\", replacement)\n-                    key, n_replace = re.subn(pattern, replacement, key)\n-                    # Early exit of the loop\n-                    if n_replace > 0:\n-                        break\n-                original_state_dict[key] = value\n-            state_dict = original_state_dict\n-\n         # Translate state_dict from smp to hf if saving with smp >= 1.10\n         if IS_SAGEMAKER_MP_POST_1_10:\n             for smp_to_hf, _ in smp.state.module_manager.translate_functions:\n@@ -3707,7 +3325,7 @@ def save_pretrained(\n             shared_ptrs = {ptr: names for ptr, names in ptrs.items() if len(names) > 1}\n \n             # Recursively descend to find tied weight keys\n-            _tied_weights_keys = _get_tied_weight_keys(self)\n+            _tied_weights_keys = set(_get_tied_weight_keys(self))\n             error_names = []\n             to_delete_names = set()\n             for names in shared_ptrs.values():\n@@ -3748,11 +3366,23 @@ def save_pretrained(\n \n             if len(error_names) > 0:\n                 raise RuntimeError(\n-                    f\"The weights trying to be saved contained shared tensors {error_names} that are mismatching \"\n-                    \"the transformers base configuration. Try saving using `safe_serialization=False`, setting the \"\n-                    \"`_dynamic_tied_weights_keys` attribute for affected modules, or remove this tensor sharing.\",\n+                    f\"The weights trying to be saved contained shared tensors {error_names} which are not properly defined. We found `_tied_weights_keys` to be: {_tied_weights_keys}.\\n\"\n+                    \"This can also just mean that the module's tied weight keys are wrong vs the actual tied weights in the model.\",\n                 )\n \n+        if (\n+            any(\n+                allowed_name in class_name.__name__.lower()\n+                for class_name in self.__class__.__mro__[:-1]\n+                for allowed_name in VLMS\n+            )\n+            or save_original_format\n+        ):\n+            # MEGA BIG TODO HERE: self._conversion_ops needs to be used to save the final ckpt\n+            # using what was loaded. Actually self._conversion_ops wont work because we need it\n+            # even if the files are not legacy -> thus no conversion happened\n+            state_dict = revert_weight_conversion(self, state_dict)\n+\n         # Shard the model if it is too big.\n         if not _hf_peft_config_loaded:\n             weights_name = SAFE_WEIGHTS_NAME if safe_serialization else WEIGHTS_NAME\n@@ -3829,7 +3459,8 @@ def save_pretrained(\n \n             if safe_serialization:\n                 # At some point we will need to deal better with save_function (used for TPU and other distributed\n-                # joyfulness), but for now this enough.\n+                # joyfulness), but for now this enough. # TODO: we should def parallelize this we are otherwise just waiting\n+                # too much before scheduling the next write when its in a different file\n                 safe_save_file(shard, os.path.join(save_directory, shard_file), metadata=metadata)\n             else:\n                 save_function(shard, os.path.join(save_directory, shard_file))\n@@ -4071,7 +3702,7 @@ def from_pretrained(\n         local_files_only: bool = False,\n         token: Optional[Union[str, bool]] = None,\n         revision: str = \"main\",\n-        use_safetensors: Optional[bool] = None,\n+        use_safetensors: Optional[bool] = True,\n         weights_only: bool = True,\n         **kwargs,\n     ) -> SpecificPreTrainedModelType:\n@@ -4307,7 +3938,7 @@ def from_pretrained(\n         if key_mapping is None and any(\n             allowed_name in class_name.__name__.lower() for class_name in cls.__mro__[:-1] for allowed_name in VLMS\n         ):\n-            key_mapping = cls._checkpoint_conversion_mapping\n+            key_mapping = copy.copy(cls._checkpoint_conversion_mapping)\n \n         if distributed_config is not None and tp_plan is None:\n             tp_plan = \"auto\"\n@@ -4399,6 +4030,15 @@ def from_pretrained(\n             config, quantization_config, dtype, device_map, weights_only, user_agent\n         )\n \n+        weight_conversions: Optional[list[WeightConverter]] = None\n+        model_type = getattr(config, \"model_type\", None)\n+        if model_type is not None:\n+            weight_conversions = get_checkpoint_conversion_mapping(model_type)\n+            if weight_conversions is None:\n+                weight_conversions = get_checkpoint_conversion_mapping(\"legacy\")\n+            if key_mapping is not None:\n+                weight_conversions.extend([WeightConverter(k, v) for k, v in key_mapping.items()])\n+\n         if gguf_file:\n             if hf_quantizer is not None:\n                 raise ValueError(\n@@ -4454,19 +4094,14 @@ def from_pretrained(\n             # Let's make sure we don't run the init function of buffer modules\n             model = cls(config, *model_args, **model_kwargs)\n \n-        # Potentially upcast some modules to avoid loosing precision\n-        model.upcast_modules_in_fp32(hf_quantizer, dtype)\n-        # Make sure to tie the weights correctly\n-        model.tie_weights()\n-\n         # make sure we use the model's config since the __init__ call might have copied it\n         config = model.config\n \n         if hf_quantizer is not None:  # replace module with quantized modules (does not touch weights)\n             hf_quantizer.preprocess_model(\n                 model=model,\n                 device_map=device_map,\n-                keep_in_fp32_modules=model._keep_in_fp32_modules,\n+                keep_in_fp32_modules=model._keep_in_fp32_modules,  # TODO prob no longer needed?\n                 config=config,\n                 checkpoint_files=checkpoint_files,\n                 use_kernels=use_kernels,\n@@ -4496,11 +4131,10 @@ def from_pretrained(\n             dtype=dtype,\n             hf_quantizer=hf_quantizer,\n             device_mesh=device_mesh,\n-            key_mapping=key_mapping,\n             weights_only=weights_only,\n+            weight_mapping=weight_conversions,\n         )\n \n-        model.tie_weights()  # make sure token embedding weights are still tied if needed\n         model.eval()  # Set model in evaluation mode to deactivate DropOut modules by default\n         model.set_use_kernels(use_kernels, kernel_config)\n \n@@ -4517,17 +4151,16 @@ def from_pretrained(\n                 **kwargs,\n             )\n \n-        # for device_map=\"auto\" : dispatch model with hooks on all devices if necessary (not needed with a tp_plan, so we skip it as it slightly\n-        # harm performances).\n+        # for device_map=\"auto\" : dispatch model with hooks on all devices if necessary\n         if device_map is not None and device_mesh is None:\n             accelerate_dispatch(model, hf_quantizer, device_map, offload_folder, offload_index, offload_buffers)\n \n         if hf_quantizer is not None:\n             model.hf_quantizer = hf_quantizer\n-            hf_quantizer.postprocess_model(model, config=config)  # usually a no-op\n+            hf_quantizer.postprocess_model(model, config=config)  # usually a no-op but sometimes needed\n \n         if _adapter_model_path is not None:\n-            adapter_kwargs[\"key_mapping\"] = key_mapping  # TODO: Dynamic weight loader for adapters\n+            adapter_kwargs[\"key_mapping\"] = weight_conversions  # TODO: Dynamic weight loader for adapters\n             model.load_adapter(\n                 _adapter_model_path,\n                 adapter_name=adapter_name,\n@@ -4545,107 +4178,6 @@ def from_pretrained(\n             return model, loading_info\n         return model\n \n-    @staticmethod\n-    def _fix_state_dict_key_on_load(key: str) -> tuple[str, bool]:\n-        \"\"\"Replace legacy parameter names with their modern equivalents. E.g. beta -> bias, gamma -> weight.\"\"\"\n-        # Rename LayerNorm beta & gamma params for some early models ported from Tensorflow (e.g. Bert)\n-        # This rename is logged.\n-        if key.endswith(\"LayerNorm.beta\"):\n-            return key.replace(\"LayerNorm.beta\", \"LayerNorm.bias\"), True\n-        if key.endswith(\"LayerNorm.gamma\"):\n-            return key.replace(\"LayerNorm.gamma\", \"LayerNorm.weight\"), True\n-\n-        # Rename weight norm parametrizations to match changes across torch versions.\n-        # Impacts a number of speech/wav2vec models. e.g. Hubert, Wav2Vec2, and others.\n-        # This rename is not logged.\n-        if hasattr(nn.utils.parametrizations, \"weight_norm\"):\n-            if key.endswith(\"weight_g\"):\n-                return key.replace(\"weight_g\", \"parametrizations.weight.original0\"), True\n-            if key.endswith(\"weight_v\"):\n-                return key.replace(\"weight_v\", \"parametrizations.weight.original1\"), True\n-        else:\n-            if key.endswith(\"parametrizations.weight.original0\"):\n-                return key.replace(\"parametrizations.weight.original0\", \"weight_g\"), True\n-            if key.endswith(\"parametrizations.weight.original1\"):\n-                return key.replace(\"parametrizations.weight.original1\", \"weight_v\"), True\n-\n-        return key, False\n-\n-    def _get_key_renaming_mapping(\n-        self,\n-        checkpoint_keys: list[str],\n-        key_mapping: Optional[dict[str, str]] = None,\n-        loading_base_model_from_task_state_dict: bool = False,\n-        loading_task_model_from_base_state_dict: bool = False,\n-    ):\n-        \"\"\"\n-        Compute a mapping between the serialized keys on disk `checkpoint_keys`, and the keys that the model\n-        that we are loading expects. This is the single entry point for key renaming that will be used during\n-        loading.\n-        Log if any parameters have been renamed.\n-        \"\"\"\n-        prefix = self.base_model_prefix\n-        _prefix = f\"{prefix}.\"\n-\n-        if loading_task_model_from_base_state_dict:\n-            task_specific_expected_keys, base_model_keys = [], []\n-            for key in self.state_dict():\n-                if key.startswith(_prefix):\n-                    base_model_keys.append(key[len(_prefix) :])\n-                else:\n-                    task_specific_expected_keys.append(key)\n-\n-        renamed_keys = {}\n-        key_renaming_mapping = {}\n-        for key in checkpoint_keys:\n-            # Class specific rename\n-            new_key, has_changed = self._fix_state_dict_key_on_load(key)\n-\n-            # Optionally map the key according to `key_mapping`\n-            if key_mapping is not None:\n-                for pattern, replacement in key_mapping.items():\n-                    new_key, n_replace = re.subn(pattern, replacement, new_key)\n-                    # Early exit of the loop\n-                    if n_replace > 0:\n-                        has_changed = True\n-                        break\n-\n-            # In this case, we need to add the prefix to the keys, to match them to the expected keys\n-            if loading_task_model_from_base_state_dict:\n-                # small sanity check: if we find a key that is only part of the task-specific keys, we raise\n-                # (if it's also part of the base model, we do not raise and assume it comes from there)\n-                if new_key in task_specific_expected_keys and new_key not in base_model_keys:\n-                    raise ValueError(\n-                        \"The state dictionary of the model you are trying to load is corrupted. Are you sure it was \"\n-                        \"properly saved?\"\n-                    )\n-                new_key = \".\".join([prefix, new_key])\n-            # In this case we need to remove the prefix from the key to match them to the expected keys, and use\n-            # only the keys starting with the prefix\n-            elif loading_base_model_from_task_state_dict:\n-                if not new_key.startswith(_prefix):\n-                    continue\n-                new_key = new_key[len(_prefix) :]\n-\n-            key_renaming_mapping[key] = new_key\n-\n-            # track gamma/beta rename for logging\n-            if has_changed:\n-                if key.endswith(\"LayerNorm.gamma\"):\n-                    renamed_keys[\"LayerNorm.gamma\"] = (key, new_key)\n-                elif key.endswith(\"LayerNorm.beta\"):\n-                    renamed_keys[\"LayerNorm.beta\"] = (key, new_key)\n-\n-        if renamed_keys:\n-            warning_msg = f\"A pretrained model of type `{self.__class__.__name__}` \"\n-            warning_msg += \"contains parameters that have been renamed internally (a few are listed below but more are present in the model):\\n\"\n-            for old_key, new_key in renamed_keys.values():\n-                warning_msg += f\"* `{old_key}` -> `{new_key}`\\n\"\n-            warning_msg += \"If you are using a model from the Hub, consider submitting a PR to adjust these weights and help future users.\"\n-            logger.info_once(warning_msg)\n-\n-        return key_renaming_mapping\n-\n     @staticmethod\n     def _fix_state_dict_key_on_save(key) -> tuple[str, bool]:\n         \"\"\"\n@@ -4675,99 +4207,17 @@ def _load_pretrained_model(\n         dtype: Optional[torch.dtype] = None,\n         hf_quantizer: Optional[HfQuantizer] = None,\n         device_mesh: Optional[\"torch.distributed.device_mesh.DeviceMesh\"] = None,\n-        key_mapping: Optional[dict[str, str]] = None,\n         weights_only: bool = True,\n+        weight_mapping: Optional[Sequence[WeightConverter]] = None,\n     ):\n-        # TODO: we should only be calling hf_quantizer.skip_placement or something like that\n         is_quantized = hf_quantizer is not None\n         is_hqq_or_quark = is_quantized and hf_quantizer.quantization_config.quant_method in {\n             QuantizationMethod.HQQ,\n             QuantizationMethod.QUARK,\n         }\n \n-        # Get all the keys of the state dicts that we have to initialize the model with\n-        if sharded_metadata is not None:\n-            original_checkpoint_keys = sharded_metadata[\"all_checkpoint_keys\"]\n-        elif state_dict is not None:\n-            original_checkpoint_keys = list(state_dict.keys())\n-        else:\n-            original_checkpoint_keys = list(\n-                load_state_dict(checkpoint_files[0], map_location=\"meta\", weights_only=weights_only).keys()\n-            )\n-\n-        # Check if we are in a special state, i.e. loading from a state dict coming from a different architecture\n-        prefix = model.base_model_prefix\n-        has_prefix_module = any(s.startswith(prefix) for s in original_checkpoint_keys) if len(prefix) > 0 else False\n-        expects_prefix_module = hasattr(model, prefix) if len(prefix) > 0 else False\n-        loading_task_model_from_base_state_dict = not has_prefix_module and expects_prefix_module\n-        loading_base_model_from_task_state_dict = has_prefix_module and not expects_prefix_module\n-\n-        # Find the key names that the model expects from the serialized keys\n-        key_renaming_mapping = model._get_key_renaming_mapping(\n-            original_checkpoint_keys,\n-            key_mapping,\n-            loading_base_model_from_task_state_dict,\n-            loading_task_model_from_base_state_dict,\n-        )\n-        checkpoint_keys = list(key_renaming_mapping.values())\n-\n-        # Find missing and unexpected keys from the state dict\n-        missing_keys, unexpected_keys = _find_missing_and_unexpected_keys(\n-            model, original_checkpoint_keys, checkpoint_keys, loading_base_model_from_task_state_dict, hf_quantizer\n-        )\n-        # Find all the keys with shape mismatch (if we ignore the mismatch, the weights need to be newly initialized the\n-        # same way as missing keys)\n-        mismatched_keys, mismatched_shapes = _find_mismatched_keys(\n-            model,\n-            state_dict,\n-            checkpoint_files,\n-            ignore_mismatched_sizes,\n-            key_renaming_mapping,\n-            is_quantized,\n-            weights_only,\n-        )\n-\n-        # We need to update both the mapping and the list of checkpoint keys to remove the mismatched and unexpected ones\n-        key_renaming_mapping = {\n-            k: v for k, v in key_renaming_mapping.items() if v not in mismatched_keys and v not in unexpected_keys\n-        }\n-        checkpoint_keys = list(key_renaming_mapping.values())\n-\n-        # Move missing (and potentially mismatched) keys back to cpu from meta device (because they won't be moved when\n-        # loading the weights as they are not in the loaded state dict)\n-        model._move_missing_keys_from_meta_to_cpu(missing_keys + mismatched_keys, dtype, hf_quantizer)\n-\n-        # correctly initialize the missing (and potentially mismatched) keys\n-        model._initialize_missing_keys(missing_keys + mismatched_keys, is_quantized)\n-\n-        # Get reverse key mapping\n-        reverse_key_renaming_mapping = {v: k for k, v in key_renaming_mapping.items()}\n-\n-        is_offloaded_safetensors = False\n-        # This offload index if for params explicitly on the \"disk\" in the device_map\n-        disk_offload_index = None\n-        disk_only_shard_files = []\n-        # Prepare parameters offloading if needed\n-        if device_map is not None and \"disk\" in device_map.values():\n-            disk_offload_index, disk_only_shard_files, is_offloaded_safetensors = accelerate_disk_offload(\n-                disk_offload_folder,\n-                checkpoint_files,\n-                device_map,\n-                checkpoint_keys,\n-                key_renaming_mapping,\n-                sharded_metadata,\n-                dtype,\n-                reverse_key_renaming_mapping,\n-            )\n-        # To be able to iterate, even if we don't use it if the state_dict is already provided\n-        elif state_dict is not None:\n-            checkpoint_files = [\"\"]\n-\n-        # Compute expected model keys\n+        # Model's definition arriving here is final (TP hooks added, quantized layers replaces)\n         expected_keys = list(model.state_dict().keys())\n-        if hf_quantizer is not None:\n-            expected_keys = hf_quantizer.update_expected_keys(model, expected_keys, checkpoint_keys)\n-\n         if logger.level >= logging.WARNING:\n             verify_tp_plan(expected_keys, getattr(model, \"_tp_plan\", None))\n \n@@ -4776,126 +4226,122 @@ def _load_pretrained_model(\n             expanded_device_map = expand_device_map(device_map, expected_keys)\n             caching_allocator_warmup(model, expanded_device_map, hf_quantizer)\n \n-        # Prepare and compatabilize arguments for serial and parallel shard loading\n-        args_list = [\n-            (\n-                shard_file,\n-                state_dict,\n-                disk_only_shard_files,\n-                is_quantized,\n-                device_map,\n-                hf_quantizer,\n-                key_renaming_mapping,\n-                weights_only,\n+        if device_map is None:\n+            device_map = {\"\": \"cpu\"}\n+        keys = sorted(device_map.keys(), key=len, reverse=True)\n+        tp_plan = getattr(model, \"_tp_plan\", None)\n+        error_msgs = []\n+\n+        if is_deepspeed_zero3_enabled() and not is_quantized:\n+            error_msgs += _load_state_dict_into_zero3_model(model, state_dict)\n+            # This is not true but for now we assume only best-case scenario with deepspeed, i.e. perfectly matching checkpoints\n+            missing_keys, unexpected_keys, mismatched_keys, misc = set(), set(), set(), set()\n+        else:\n+            all_pointer = set()\n+            if checkpoint_files is not None and checkpoint_files[0].endswith(\".safetensors\"):\n+                pattern = re.compile(r\"(\" + \"|\".join(map(re.escape, keys)) + r\")\")\n+                if sharded_metadata is None:\n+                    k_v_iterator = dict.fromkeys(\n+                        safe_open(checkpoint_files[0], framework=\"pt\").keys(), checkpoint_files[0].rsplit(\"/\", 1)[1]\n+                    ).items()\n+                else:\n+                    k_v_iterator = sharded_metadata[\"weight_map\"].items()\n+\n+                merged_state_dict = {}\n+                for k, v in k_v_iterator:\n+                    match = pattern.match(k)\n+                    if match and match.group(1) != \"\":\n+                        device = device_map[match.group(1)]\n+                    else:\n+                        device = device_map.get(\"\", \"cpu\")\n+                        if isinstance(device, torch.device):\n+                            device = device.index  # safetensors only\n+                    if device == \"disk\":\n+                        device = \"cpu\"  # we read to cpu to then write to disk\n+                    file_pointer = safe_open(\n+                        os.path.join(checkpoint_files[0].rsplit(\"/\", 1)[0], v), framework=\"pt\", device=device\n+                    )\n+                    all_pointer.add(file_pointer)\n+                    merged_state_dict[k] = file_pointer.get_slice(k)  # don't materialize yet\n+            elif state_dict is not None:\n+                merged_state_dict = state_dict\n+            elif checkpoint_files is not None:\n+                merged_state_dict = {}\n+                for ckpt_file in checkpoint_files:\n+                    merged_state_dict.update(load_state_dict(ckpt_file))\n+            else:\n+                raise ValueError(\"Neither a state dict nor checkpoint files were found.\")\n+\n+            missing_keys, unexpected_keys, mismatched_keys, misc = convert_and_load_state_dict_in_model(\n                 model,\n-                reverse_key_renaming_mapping,\n-                disk_offload_folder,\n-                disk_offload_index,\n+                merged_state_dict,\n+                weight_mapping,\n+                tp_plan,\n+                hf_quantizer,\n+                dtype,\n+                device_map,\n+                model.dtype_plan,\n                 device_mesh,\n             )\n-            for shard_file in checkpoint_files\n-        ]\n \n-        error_msgs = []\n+            # finally close all opened file pointers\n+            for k in all_pointer:\n+                k.__exit__(None, None, None)\n \n-        if (\n-            os.environ.get(\"HF_ENABLE_PARALLEL_LOADING\", \"\").upper() in ENV_VARS_TRUE_VALUES\n-            and not is_deepspeed_zero3_enabled()\n-        ):\n-            _error_msgs, disk_offload_index = load_shard_files_with_threadpool(args_list)\n-            error_msgs += _error_msgs\n-        else:\n-            if len(args_list) > 1:\n-                args_list = logging.tqdm(args_list, desc=\"Loading checkpoint shards\")\n+        # Move missing (and potentially mismatched) keys back to cpu from meta device (because they won't be moved when\n+        # loading the weights as they are not in the loaded state dict)\n+        # Remove tied weights keys and etc\n+        miss_and_mismatched = missing_keys | {k[0] for k in mismatched_keys}\n+        model._move_missing_keys_from_meta_to_cpu(miss_and_mismatched, dtype, hf_quantizer)\n \n-            for args in args_list:\n-                _error_msgs, disk_offload_index = load_shard_file(args)\n-                error_msgs += _error_msgs\n+        # correctly initialize the missing (and potentially mismatched) keys\n+        model._initialize_missing_keys(is_quantized)\n+        missing_keys, unexpected_keys = model._adjust_missing_and_unexpected_keys(missing_keys, unexpected_keys, False)\n \n-        # Save offloaded index if needed\n-        if disk_offload_index is not None and len(disk_offload_index) > 0 and not is_offloaded_safetensors:\n-            save_offload_index(disk_offload_index, disk_offload_folder)\n-            disk_offload_index = None\n+        # We make sure we tie after _init_. We need the missing keys to remove the ones we do tie, and not random remove\n+        model.tie_weights(missing_keys)\n \n         # Post-processing for tensor parallelism\n         if device_mesh is not None:\n             # When using TP, the device map is a single device for all parameters\n             tp_device = list(device_map.values())[0]\n             # This is needed for the RotaryEmbedding, which was not initialized on the correct device as it is\n             # not part of the state_dict (persistent=False)\n-            for buffer in model.buffers():\n+            for buffer in model.buffers():  # TODO to avaoid this buffer could be added to the ckpt\n                 if buffer.device != tp_device:\n                     buffer.data = buffer.to(tp_device)\n \n             # In this case, the top-most task module weights were not moved to device and parallelized as they\n             # were not part of the loaded weights: do it now\n-            if loading_task_model_from_base_state_dict:\n-                parameters_to_initialize = {\n-                    name: param for name, param in model.named_parameters() if not name.startswith(prefix)\n-                }\n-                for name, param in parameters_to_initialize.items():\n-                    # If it is still on meta here, it means that it's a tied weight that will be tied later anyway -> skip it\n-                    if param.device.type == \"meta\":\n-                        continue\n+            if missing_keys:\n+                state_dict = model.state_dict()\n+                for name in missing_keys:\n+                    param = state_dict[name]\n                     # Shard the param\n-                    to_contiguous, casting_dtype = _infer_parameter_dtype(model, name, param)\n                     shard_and_distribute_module(\n                         model,\n                         param.to(tp_device),\n                         param,\n                         name,\n-                        casting_dtype,\n-                        to_contiguous,\n+                        None,\n+                        False,\n                         device_mesh.get_local_rank(),\n                         device_mesh,\n                     )\n \n-        # Remove potential model-specific exceptions from the warnings\n-        missing_keys, unexpected_keys = model._adjust_missing_and_unexpected_keys(\n-            missing_keys, unexpected_keys, loading_task_model_from_base_state_dict\n+        log_state_dict_report(\n+            model=model,\n+            pretrained_model_name_or_path=pretrained_model_name_or_path,\n+            logger=logger,\n+            error_msgs=error_msgs,\n+            unexpected_keys=unexpected_keys,\n+            missing_keys=missing_keys,\n+            mismatched_keys=mismatched_keys,\n+            mismatched_shapes=mismatched_keys,\n+            misc=misc,\n+            ignore_mismatched_sizes=ignore_mismatched_sizes,\n         )\n-\n-        # TODO: separate this in another function: it's not core....\n-        # All potential warnings/infos\n-        if len(error_msgs) > 0:\n-            error_msg = \"\\n\\t\".join(error_msgs)\n-            if \"size mismatch\" in error_msg:\n-                error_msg += (\n-                    \"\\n\\tYou may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.\"\n-                )\n-            raise RuntimeError(f\"Error(s) in loading state_dict for {model.__class__.__name__}:\\n\\t{error_msg}\")\n-        if len(unexpected_keys) > 0:\n-            archs = [] if model.config.architectures is None else model.config.architectures\n-            warner = logger.warning if model.__class__.__name__ in archs else logger.info\n-            warner(\n-                f\"Some weights of the model checkpoint at {pretrained_model_name_or_path} were not used when\"\n-                f\" initializing {model.__class__.__name__}: {update_key_name(unexpected_keys)}\\n- This IS expected if you are\"\n-                f\" initializing {model.__class__.__name__} from the checkpoint of a model trained on another task or\"\n-                \" with another architecture (e.g. initializing a BertForSequenceClassification model from a\"\n-                \" BertForPreTraining model).\\n- This IS NOT expected if you are initializing\"\n-                f\" {model.__class__.__name__} from the checkpoint of a model that you expect to be exactly identical\"\n-                \" (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\"\n-            )\n-        if len(missing_keys) > 0:\n-            logger.warning(\n-                f\"Some weights of {model.__class__.__name__} were not initialized from the model checkpoint at\"\n-                f\" {pretrained_model_name_or_path} and are newly initialized: {update_key_name(missing_keys)}\\nYou should probably\"\n-                \" TRAIN this model on a down-stream task to be able to use it for predictions and inference.\"\n-            )\n-        if len(mismatched_keys) > 0:\n-            mismatched_warning = \"\\n\".join(\n-                [\n-                    f\"- {key}: found shape {shape1} in the checkpoint and {shape2} in the model instantiated\"\n-                    for key, (shape1, shape2) in zip(mismatched_keys, mismatched_shapes)\n-                ]\n-            )\n-            logger.warning(\n-                f\"Some weights of {model.__class__.__name__} were not initialized from the model checkpoint at\"\n-                f\" {pretrained_model_name_or_path} and are newly initialized because the shapes did not\"\n-                f\" match:\\n{mismatched_warning}\\nYou should probably TRAIN this model on a down-stream task to be able\"\n-                \" to use it for predictions and inference.\"\n-            )\n-\n+        disk_offload_index = None\n         return model, missing_keys, unexpected_keys, mismatched_keys, disk_offload_index, error_msgs\n \n     def retrieve_modules_from_names(self, names, add_prefix=False, remove_prefix=False):\n@@ -5104,43 +4550,15 @@ def _move_missing_keys_from_meta_to_cpu(\n                 value = torch.empty_like(param, dtype=dtype, device=\"cpu\")\n                 if not is_quantized or not hf_quantizer.param_needs_quantization(self, key):\n                     _load_parameter_into_model(self, key, value)\n-                else:\n-                    hf_quantizer.create_quantized_param(self, value, key, \"cpu\")\n \n-    def _initialize_missing_keys(self, missing_keys: list[str], is_quantized: bool) -> None:\n-        \"\"\"Initialize the missing keys (keys that are part of the model parameters, but were NOT found in the loaded state dicts), according to\n+    def _initialize_missing_keys(self, is_quantized: bool) -> None:\n+        \"\"\"\n+        Initialize the missing keys (keys that are part of the model parameters, but were NOT found in the loaded state dicts), according to\n         `_initialize_weights`. Indeed, since the corresponding weights are missing from the state dict, they will not be replaced and need to\n         be initialized correctly (i.e. weight initialization distribution).\n-        Also take care of setting the `_is_hf_initialized` flag for keys that are not missing.\n-        \"\"\"\n-        for key in self.state_dict():\n-            # If it's part of the keys that will be loaded, mark it as already initialized\n-            if key not in missing_keys:\n-                param_or_buffer = self.get_parameter_or_buffer(key)\n-                param_or_buffer._is_hf_initialized = True\n-\n-        def set_is_initialized_for_modules(module):\n-            # A module is already initialized if and only if all its children are also already initialized, and all\n-            # its immediate `nn.Parameter` and persistent buffers are also already initialized\n-            if (\n-                # All immediate children are initialized\n-                all(getattr(child, \"_is_hf_initialized\", False) for child in module.children())\n-                # All immediate parameters are initialized\n-                and all(getattr(param, \"_is_hf_initialized\", False) for param in module.parameters(recurse=False))\n-                # All immediate persistent buffers are initialized\n-                and all(\n-                    getattr(buffer, \"_is_hf_initialized\", False)\n-                    for name, buffer in module.named_buffers(recurse=False)\n-                    if name not in module._non_persistent_buffers_set\n-                )\n-            ):\n-                module._is_hf_initialized = True\n-\n-        # Set the flag on the modules as well. We do it recursively (depth-first), as it's more efficient (we do not\n-        # need to check the entire state dict of each module, only the immediate children, so we only iterate once over\n-        # each param)\n-        self.apply(set_is_initialized_for_modules)\n \n+        Params that are not missing have the `is_hf_initialized` flag.\n+        \"\"\"\n         # This will only initialize submodules that are not marked as initialized by the line above.\n         if is_deepspeed_zero3_enabled() and not is_quantized:\n             import deepspeed\n@@ -5154,13 +4572,25 @@ def set_is_initialized_for_modules(module):\n         else:\n             self.initialize_weights()\n \n+        # Replace the loaded parameters class back to nn.Parameter (they were changed to easily skip initialization\n+        # when performed in-place on the tensors)\n+        for name, p in list(self.named_parameters()) + list(self.named_buffers()):\n+            # We get back the original parameter that we stored in _original. This attribute was created when we initialized LoadedParam when loading the checkpoints.\n+            if hasattr(p, \"_original\"):\n+                if \".\" in name:\n+                    module, name = name.rsplit(\".\", 1)\n+                    module = self.get_submodule(module)\n+                else:\n+                    module = self\n+                setattr(module, name, p._original)\n+\n     def _adjust_missing_and_unexpected_keys(\n-        self, missing_keys: list[str], unexpected_keys: list[str], loading_task_model_from_base_state_dict: bool\n-    ) -> tuple[list[str], list[str]]:\n+        self, missing_keys: set[str], unexpected_keys: set[str], loading_task_model_from_base_state_dict: bool\n+    ) -> tuple[set[str], set[str]]:\n         \"\"\"Adjust the `missing_keys` and `unexpected_keys` based on current model's exception rules, to avoid\n         raising unneeded warnings/errors.\n         \"\"\"\n-        # Old checkpoints may have keys for rotary_emb.inv_freq for each layer, however we moved this buffer to the main model\n+        # Old checkpoints may have keys for rotary_emb.inv_freq forach layer, however we moved this buffer to the main model\n         # (so the buffer name has changed). Remove them in such a case. This is another exception that was not added to\n         # `_keys_to_ignore_on_load_unexpected` as it touches many models -> we add it manually to the existing patterns\n         has_inv_freq_buffers = any(buffer.endswith(\"rotary_emb.inv_freq\") for buffer, _ in self.named_buffers())\n@@ -5176,17 +4606,17 @@ def _adjust_missing_and_unexpected_keys(\n \n         # Clean-up missing keys\n         if ignore_missing_regex is not None:\n-            missing_keys = [key for key in missing_keys if ignore_missing_regex.search(key) is None]\n+            missing_keys = {key for key in missing_keys if ignore_missing_regex.search(key) is None}\n \n         # Clean-up unexpected keys\n         if ignore_unexpected_regex is not None:\n-            unexpected_keys = [key for key in unexpected_keys if ignore_unexpected_regex.search(key) is None]\n+            unexpected_keys = {key for key in unexpected_keys if ignore_unexpected_regex.search(key) is None}\n \n         # Note: only the unexpected keys should remove the added prefix here, to correctly display the original name\n         # in the warnings. For missing keys, we should show the prefix in the warning as it's part of the final model\n         if loading_task_model_from_base_state_dict:\n             _prefix = f\"{self.base_model_prefix}.\"\n-            unexpected_keys = [k.removeprefix(_prefix) for k in unexpected_keys]\n+            unexpected_keys = {k.removeprefix(_prefix) for k in unexpected_keys}\n \n         return missing_keys, unexpected_keys\n \n@@ -5223,35 +4653,6 @@ def train(self, mode: bool = True):\n     def eval(self):\n         return self.train(False)\n \n-    def upcast_modules_in_fp32(self, hf_quantizer: HfQuantizer | None, dtype: torch.dtype) -> None:\n-        \"\"\"\n-        Upcast modules defined in `_keep_in_fp32_modules` and `_keep_in_fp32_modules_strict` in fp32, if\n-        `dtype` is different than fp32.\n-        \"\"\"\n-        # If the dtype is already fp32, we can skip\n-        if dtype == torch.float32:\n-            return\n-\n-        keep_in_fp32_modules = []\n-        # The _keep_in_fp32_modules flag is only used to avoid bf16 -> fp16 casting precision issues. It was introduced\n-        # in case of force loading a model that should stay bf16 in fp16 (which includes a few quantizers as this is a pre-processing\n-        # step for e.g. bitsandbytes). See https://github.com/huggingface/transformers/issues/20287 for details.\n-        if self._keep_in_fp32_modules is not None and (\n-            dtype == torch.float16 or getattr(hf_quantizer, \"use_keep_in_fp32_modules\", False)\n-        ):\n-            keep_in_fp32_modules.extend(self._keep_in_fp32_modules)\n-\n-        if self._keep_in_fp32_modules_strict is not None and (dtype == torch.float16 or dtype == torch.bfloat16):\n-            keep_in_fp32_modules.extend(self._keep_in_fp32_modules_strict)\n-\n-        if len(keep_in_fp32_modules) > 0:\n-            # We need to match exact layers, so we add either `.` on each side, or start/end of string\n-            keep_in_fp32_regex = re.compile(\"|\".join([rf\"((^|\\.){module}($|\\.))\" for module in keep_in_fp32_modules]))\n-            for name, param in self.named_parameters():\n-                if keep_in_fp32_regex.search(name):\n-                    # param = param.to(torch.float32) does not work here as only in the local scope.\n-                    param.data = param.data.to(torch.float32)\n-\n \n PreTrainedModel.push_to_hub = copy_func(PreTrainedModel.push_to_hub)\n if PreTrainedModel.push_to_hub.__doc__ is not None:"
        },
        {
            "sha": "bead6a11dd7b401f36d4b29d514c259c35c26a84",
            "filename": "src/transformers/models/aimv2/modeling_aimv2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodeling_aimv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodeling_aimv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodeling_aimv2.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -406,13 +406,14 @@ class Aimv2PreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_flex_attn = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         super()._init_weights(module)\n         if hasattr(module, \"logit_scale\"):\n             if isinstance(module.logit_scale, nn.Parameter):\n-                module.logit_scale.data.fill_(math.log(1 / 0.07))\n+                module.logit_scale.fill_(math.log(1 / 0.07))\n         elif isinstance(module, Aimv2AttentionPoolingHead):\n-            module.cls_token.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.cls_token.normal_(mean=0.0, std=self.config.initializer_range)\n \n \n @auto_docstring("
        },
        {
            "sha": "55ff92212b399b4eec8b18e1b3279f637b500b9e",
            "filename": "src/transformers/models/aimv2/modular_aimv2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodular_aimv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodular_aimv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodular_aimv2.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -449,13 +449,14 @@ class Aimv2PreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_flex_attn = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         super()._init_weights(module)\n         if hasattr(module, \"logit_scale\"):\n             if isinstance(module.logit_scale, nn.Parameter):\n-                module.logit_scale.data.fill_(math.log(1 / 0.07))\n+                module.logit_scale.fill_(math.log(1 / 0.07))\n         elif isinstance(module, Aimv2AttentionPoolingHead):\n-            module.cls_token.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.cls_token.normal_(mean=0.0, std=self.config.initializer_range)\n \n \n @auto_docstring("
        },
        {
            "sha": "ac4337e4f269e66f1e6550ceec8a857b2ca2d6e2",
            "filename": "src/transformers/models/albert/modeling_albert.py",
            "status": "modified",
            "additions": 16,
            "deletions": 18,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_albert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_albert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_albert.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -302,21 +302,22 @@ class AlbertPreTrainedModel(PreTrainedModel):\n         \"attentions\": AlbertAttention,\n     }\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights.\"\"\"\n         if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n         elif isinstance(module, AlbertMLMHead):\n-            module.bias.data.zero_()\n+            module.bias.zero_()\n \n \n @dataclass\n@@ -425,7 +426,10 @@ def forward(\n     \"\"\"\n )\n class AlbertForPreTraining(AlbertPreTrainedModel):\n-    _tied_weights_keys = [\"predictions.decoder.bias\", \"predictions.decoder.weight\"]\n+    _tied_weights_keys = {\n+        \"predictions.decoder.weight\": \"albert.embeddings.word_embeddings.weight\",\n+        \"predictions.decoder.bias\": \"predictions.bias\",\n+    }\n \n     def __init__(self, config: AlbertConfig):\n         super().__init__(config)\n@@ -525,7 +529,6 @@ def __init__(self, config: AlbertConfig):\n         self.dense = nn.Linear(config.hidden_size, config.embedding_size)\n         self.decoder = nn.Linear(config.embedding_size, config.vocab_size)\n         self.activation = ACT2FN[config.hidden_act]\n-        self.decoder.bias = self.bias\n \n     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         hidden_states = self.dense(hidden_states)\n@@ -537,14 +540,6 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n \n         return prediction_scores\n \n-    def _tie_weights(self) -> None:\n-        # For accelerate compatibility and to not break backward compatibility\n-        if self.decoder.bias.device.type == \"meta\":\n-            self.decoder.bias = self.bias\n-        else:\n-            # To tie those two weights if they get disconnected (on TPU or when the bias is resized)\n-            self.bias = self.decoder.bias\n-\n \n class AlbertSOPHead(nn.Module):\n     def __init__(self, config: AlbertConfig):\n@@ -561,7 +556,10 @@ def forward(self, pooled_output: torch.Tensor) -> torch.Tensor:\n \n @auto_docstring\n class AlbertForMaskedLM(AlbertPreTrainedModel):\n-    _tied_weights_keys = [\"predictions.decoder.bias\", \"predictions.decoder.weight\"]\n+    _tied_weights_keys = {\n+        \"predictions.decoder.weight\": \"albert.embeddings.word_embeddings.weight\",\n+        \"predictions.decoder.bias\": \"predictions.bias\",\n+    }\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "6ec6d72a4771707172caac719175603460f01885",
            "filename": "src/transformers/models/align/modeling_align.py",
            "status": "modified",
            "additions": 9,
            "deletions": 8,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Falign%2Fmodeling_align.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Falign%2Fmodeling_align.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falign%2Fmodeling_align.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -823,24 +823,25 @@ class AlignPreTrainedModel(PreTrainedModel):\n     input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module: nn.Module):\n         \"\"\"Initialize the weights\"\"\"\n         std = self.config.initializer_range\n         if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, AlignModel):\n             nn.init.xavier_uniform_(module.text_projection.weight)\n-            module.text_projection.bias.data.zero_()\n-            module.temperature.data.fill_(self.config.temperature_init_value)\n+            module.text_projection.bias.zero_()\n+            module.temperature.fill_(self.config.temperature_init_value)\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         if isinstance(module, (nn.LayerNorm, nn.BatchNorm2d)):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n \n \n @auto_docstring("
        },
        {
            "sha": "1c45432d5f20806e968ffc3639289c583b453a72",
            "filename": "src/transformers/models/altclip/modeling_altclip.py",
            "status": "modified",
            "additions": 7,
            "deletions": 8,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -770,6 +770,7 @@ class AltCLIPPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_module = []\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         factor = self.config.initializer_factor\n@@ -797,23 +798,21 @@ def _init_weights(self, module):\n                 module.text_projection.weight,\n                 std=module.text_embed_dim**-0.5 * self.config.initializer_factor,\n             )\n-            module.text_projection._is_hf_initialized = True\n             nn.init.normal_(\n                 module.visual_projection.weight,\n                 std=module.vision_embed_dim**-0.5 * self.config.initializer_factor,\n             )\n-            module.visual_projection._is_hf_initialized = True\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n         elif isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_factor)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_factor)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_factor)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_factor)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n \n \n class AltCLIPVisionTransformer(nn.Module):"
        },
        {
            "sha": "7cdde33e8ff2dbe0ea6dbdd78292a88e3907a404",
            "filename": "src/transformers/models/apertus/modeling_apertus.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fapertus%2Fmodeling_apertus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fapertus%2Fmodeling_apertus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fapertus%2Fmodeling_apertus.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -429,7 +429,7 @@ def forward(\n \n @auto_docstring\n class ApertusForCausalLM(ApertusPreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.embed_tokens.weight\"}\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n     _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n "
        },
        {
            "sha": "513162398dd7f2aa78f9f4565def6db01d0c8782",
            "filename": "src/transformers/models/arcee/modeling_arcee.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -434,7 +434,7 @@ def forward(\n \n @auto_docstring(checkpoint=\"arcee-ai/AFM-4.5B\")\n class ArceeForCausalLM(ArceePreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.embed_tokens.weight\"}\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n     _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n "
        },
        {
            "sha": "f430972d61b714bf9393e81d824e43e46500f96f",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -585,10 +585,11 @@ class AriaTextPreTrainedModel(PreTrainedModel):\n         \"attentions\": AriaTextAttention,\n     }\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, AriaGroupedExpertsGemm):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n \n \n @auto_docstring\n@@ -608,6 +609,7 @@ class AriaPreTrainedModel(PreTrainedModel):\n         \"attentions\": AriaTextAttention,\n     }\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, AriaProjector):\n@@ -760,7 +762,7 @@ def forward(\n \n @auto_docstring\n class AriaTextForCausalLM(AriaTextPreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.embed_tokens.weight\"}\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n     _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n \n@@ -890,8 +892,6 @@ class AriaModelOutputWithPast(BaseModelOutputWithPast):\n     \"\"\"\n )\n class AriaModel(AriaPreTrainedModel):\n-    _checkpoint_conversion_mapping = {\"language_model.model\": \"language_model\"}\n-\n     def __init__(self, config: AriaConfig):\n         super().__init__(config)\n         self.vision_tower = AutoModel.from_config(config.vision_config)\n@@ -1048,12 +1048,12 @@ def _create_patch_attention_mask(self, pixel_mask):\n )\n class AriaForConditionalGeneration(AriaPreTrainedModel, GenerationMixin):\n     _checkpoint_conversion_mapping = {\n-        \"^language_model.model\": \"model.language_model\",\n-        \"^vision_tower\": \"model.vision_tower\",\n-        \"^multi_modal_projector\": \"model.multi_modal_projector\",\n-        \"^language_model.lm_head\": \"lm_head\",\n+        r\"^language_model.model\": \"model.language_model\",\n+        r\"^vision_tower\": \"model.vision_tower\",\n+        r\"^multi_modal_projector\": \"model.multi_modal_projector\",\n+        r\"^language_model.lm_head\": \"lm_head\",\n     }\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.language_model.embed_tokens.weight\"}\n \n     def __init__(self, config: AriaConfig):\n         super().__init__(config)"
        },
        {
            "sha": "dce6584ed6af62527b70d30e6a2149488f375fad",
            "filename": "src/transformers/models/aria/modular_aria.py",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -1196,10 +1196,11 @@ class AriaTextPreTrainedModel(PreTrainedModel):\n         \"attentions\": AriaTextAttention,\n     }\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, AriaGroupedExpertsGemm):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n \n \n class AriaPreTrainedModel(LlamaPreTrainedModel):\n@@ -1208,6 +1209,7 @@ class AriaPreTrainedModel(LlamaPreTrainedModel):\n     _can_compile_fullgraph = False  # MoE models don't work with torch.compile (dynamic slicing)\n     _supports_attention_backend = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         PreTrainedModel._init_weights(self, module)\n         if isinstance(module, AriaProjector):\n@@ -1225,7 +1227,7 @@ def __init__(self, config: AriaTextConfig):\n \n \n class AriaTextForCausalLM(AriaTextPreTrainedModel, LlamaForCausalLM):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.embed_tokens.weight\"}\n \n     def __init__(self, config: AriaTextConfig):\n         super().__init__(config)\n@@ -1364,6 +1366,8 @@ def forward(\n     \"\"\"\n )\n class AriaForConditionalGeneration(LlavaForConditionalGeneration):\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.language_model.embed_tokens.weight\"}\n+\n     def get_image_features(\n         self,\n         pixel_values: torch.FloatTensor,"
        },
        {
            "sha": "1f270b96aa95cca2b61a940d3ae79c29e72c7485",
            "filename": "src/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py",
            "status": "modified",
            "additions": 12,
            "deletions": 9,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fmodeling_audio_spectrogram_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fmodeling_audio_spectrogram_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fmodeling_audio_spectrogram_transformer.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -300,23 +300,26 @@ class ASTPreTrainedModel(PreTrainedModel):\n         \"attentions\": ASTSelfAttention,\n     }\n \n+    @torch.no_grad()\n     def _init_weights(self, module: Union[nn.Linear, nn.Conv2d, nn.LayerNorm]) -> None:\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, (nn.Linear, nn.Conv2d)):\n             # Upcast the input in `fp32` and cast it back to desired `dtype` to avoid\n             # `trunc_normal_cpu` not implemented in `half` issues\n-            module.weight.data = nn.init.trunc_normal_(\n-                module.weight.data.to(torch.float32), mean=0.0, std=self.config.initializer_range\n-            ).to(module.weight.dtype)\n+            module.weight.copy_(\n+                nn.init.trunc_normal_(module.weight.to(torch.float32), mean=0.0, std=self.config.initializer_range).to(\n+                    module.weight.dtype\n+                )\n+            )\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n         elif isinstance(module, ASTEmbeddings):\n-            module.cls_token.data.zero_()\n-            module.position_embeddings.data.zero_()\n-            module.distillation_token.data.zero_()\n+            module.cls_token.zero_()\n+            module.position_embeddings.zero_()\n+            module.distillation_token.zero_()\n \n \n @auto_docstring"
        },
        {
            "sha": "7947fca148be7ecc71474e1452912e990587eb08",
            "filename": "src/transformers/models/audioflamingo3/modeling_audioflamingo3.py",
            "status": "modified",
            "additions": 8,
            "deletions": 11,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2Fmodeling_audioflamingo3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2Fmodeling_audioflamingo3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2Fmodeling_audioflamingo3.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -264,6 +264,7 @@ class AudioFlamingo3PreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         # important: this ported version of AudioFlamingo3 isn't meant for training from scratch - only\n         # inference and fine-tuning - so the proper init weights code has been removed\n@@ -274,16 +275,16 @@ def _init_weights(self, module):\n         )\n \n         if isinstance(module, (nn.Linear, nn.Conv1d)):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.weight.data.fill_(1.0)\n-            module.bias.data.zero_()\n+            module.weight.fill_(1.0)\n+            module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n \n \n @auto_docstring(\n@@ -435,20 +436,16 @@ def forward(self, audio_features):\n     \"\"\"\n )\n class AudioFlamingo3ForConditionalGeneration(AudioFlamingo3PreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = None\n+    _keep_in_fp32_modules_strict = None\n     _tp_plan = None\n     _pp_plan = None\n-    _keep_in_fp32_modules_strict = None\n \n     def __init__(self, config):\n         super().__init__(config)\n         self.vocab_size = config.text_config.vocab_size\n         self.audio_tower = AutoModel.from_config(config.audio_config)\n         self.language_model = AutoModelForCausalLM.from_config(config.text_config)\n         self.multi_modal_projector = AudioFlamingo3MultiModalProjector(config)\n-        # Similar to Qwen2Audio\n-        if self.language_model._tied_weights_keys is not None:\n-            self._tied_weights_keys = [f\"language_model.{k}\" for k in self.language_model._tied_weights_keys]\n \n         # Initialize weights and apply final processing\n         self.post_init()"
        },
        {
            "sha": "68da1b7646e3388a5ec37f9adfbc918da011ea08",
            "filename": "src/transformers/models/audioflamingo3/modular_audioflamingo3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2Fmodular_audioflamingo3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2Fmodular_audioflamingo3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2Fmodular_audioflamingo3.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -136,16 +136,12 @@ def __init__(self, config: AudioFlamingo3Config):\n     \"\"\"\n )\n class AudioFlamingo3ForConditionalGeneration(VoxtralForConditionalGeneration):\n-    _tied_weights_keys = None\n     _tp_plan = None\n     _pp_plan = None\n     _keep_in_fp32_modules_strict = None\n \n     def __init__(self, config):\n         super().__init__(config)\n-        # Similar to Qwen2Audio\n-        if self.language_model._tied_weights_keys is not None:\n-            self._tied_weights_keys = [f\"language_model.{k}\" for k in self.language_model._tied_weights_keys]\n \n     def get_audio_features(\n         self, input_features: torch.FloatTensor, input_features_mask: torch.Tensor"
        },
        {
            "sha": "782ef440d0a7b3c680317166a51fc1d564d288dc",
            "filename": "src/transformers/models/autoformer/modeling_autoformer.py",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -826,21 +826,22 @@ class AutoformerPreTrainedModel(PreTrainedModel):\n     main_input_name = \"past_values\"\n     supports_gradient_checkpointing = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module: nn.Module):\n         std = self.config.init_std\n         if isinstance(module, (nn.Linear, nn.Conv1d)):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, AutoformerSinusoidalPositionalEmbedding):\n             module._init_weight()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.weight.data.fill_(1.0)\n-            module.bias.data.zero_()\n+            module.weight.fill_(1.0)\n+            module.bias.zero_()\n \n     # copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n     def _update_full_mask("
        },
        {
            "sha": "6e57e0a04178d5f4b15b6657f9eea4914428d2a0",
            "filename": "src/transformers/models/aya_vision/modeling_aya_vision.py",
            "status": "modified",
            "additions": 5,
            "deletions": 8,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -90,7 +90,6 @@ def pixel_shuffle(self, image_features):  # B, S, D\n @auto_docstring\n class AyaVisionPreTrainedModel(PreTrainedModel):\n     config: AyaVisionConfig\n-    base_model_prefix = \"\"\n     input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n     _skip_keys_device_placement = \"past_key_values\"\n@@ -163,8 +162,6 @@ class AyaVisionModelOutputWithPast(BaseModelOutputWithPast):\n     \"\"\"\n )\n class AyaVisionModel(AyaVisionPreTrainedModel):\n-    _checkpoint_conversion_mapping = {\"language_model.model\": \"language_model\"}\n-\n     def __init__(self, config: AyaVisionConfig):\n         super().__init__(config)\n         self.vision_tower = AutoModel.from_config(config.vision_config)\n@@ -333,12 +330,12 @@ def forward(\n )\n class AyaVisionForConditionalGeneration(AyaVisionPreTrainedModel, GenerationMixin):\n     _checkpoint_conversion_mapping = {\n-        \"^language_model.model\": \"model.language_model\",\n-        \"^vision_tower\": \"model.vision_tower\",\n-        \"^multi_modal_projector\": \"model.multi_modal_projector\",\n-        \"^language_model.lm_head\": \"lm_head\",\n+        r\"^language_model.model\": \"model.language_model\",\n+        r\"^vision_tower\": \"model.vision_tower\",\n+        r\"^multi_modal_projector\": \"model.multi_modal_projector\",\n+        r\"^language_model.lm_head\": \"lm_head\",\n     }\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.language_model.embed_tokens.weight\"}\n \n     def __init__(self, config: AyaVisionConfig):\n         super().__init__(config)"
        },
        {
            "sha": "ed07f9345e2befa29b492117a0a91886f993def5",
            "filename": "src/transformers/models/bamba/modeling_bamba.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -1126,12 +1126,13 @@ class BambaPreTrainedModel(PreTrainedModel):\n     # Note: only supports HybridMambaAttentionDynamicCache\n     _is_stateful = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, BambaMixer):\n-            module.dt_bias.data.fill_(1.0)\n-            module.A_log.data = torch.log(torch.arange(1, module.num_heads + 1))\n-            module.D.data.fill_(1.0)\n+            module.dt_bias.fill_(1.0)\n+            module.A_log.copy_(torch.log(torch.arange(1, module.num_heads + 1)))\n+            module.D.fill_(1.0)\n \n \n @auto_docstring\n@@ -1383,7 +1384,7 @@ def _update_mamba_mask(self, attention_mask, cache_position):\n \n @auto_docstring\n class BambaForCausalLM(BambaPreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.embed_tokens.weight\"}\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n     _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n "
        },
        {
            "sha": "024e8415fffeab04143725e437358622b6746dc2",
            "filename": "src/transformers/models/bamba/modular_bamba.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -800,12 +800,13 @@ class BambaPreTrainedModel(PreTrainedModel):\n     # Note: only supports HybridMambaAttentionDynamicCache\n     _is_stateful = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, BambaMixer):\n-            module.dt_bias.data.fill_(1.0)\n-            module.A_log.data = torch.log(torch.arange(1, module.num_heads + 1))\n-            module.D.data.fill_(1.0)\n+            module.dt_bias.fill_(1.0)\n+            module.A_log.copy_(torch.log(torch.arange(1, module.num_heads + 1)))\n+            module.D.fill_(1.0)\n \n \n @auto_docstring"
        },
        {
            "sha": "e00068e34f0cf3c362811198f6279fe75aa890e7",
            "filename": "src/transformers/models/bark/modeling_bark.py",
            "status": "modified",
            "additions": 11,
            "deletions": 33,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -329,19 +329,21 @@ class BarkPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = False\n     _supports_flash_attn = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights.\"\"\"\n         if isinstance(module, (nn.Linear,)):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            if getattr(module, \"bias\", None) is not None:\n+                module.bias.zero_()\n+            module.weight.fill_(1.0)\n \n     def __init__(self, *inputs, **kwargs):\n         super().__init__(*inputs, **kwargs)\n@@ -910,6 +912,9 @@ def __init__(self, config):\n         # non-causal gpt-like model with one embedding layer and one lm_head for each codebook of Encodec\n         super().__init__(config)\n         self.config = config\n+        self._tied_weights_keys = {}\n+        for i in range(self.config.n_codes_total - self.config.n_codes_given):\n+            self._tied_weights_keys[f\"lm_heads.{i}.weight\"] = f\"input_embeds_layers.{i + 1}.weight\"\n \n         # initialize a modified non causal GPT-like model\n         # note that for there is one embedding layer and one lm_head for each codebook of Encodec\n@@ -1025,25 +1030,6 @@ def resize_token_embeddings(\n \n         return model_embeds\n \n-    def _tie_weights(self):\n-        if getattr(self.config, \"tie_word_embeddings\", True):\n-            self._tied_weights_keys = []\n-            output_embeddings = self.get_output_embeddings()\n-            input_embeddings = self.get_input_embeddings()\n-\n-            for i in range(self.config.n_codes_total - self.config.n_codes_given):\n-                # self.input_embeds_layers[i + 1].weight = self.lm_heads[i].weight\n-                self._tie_embedding_weights(output_embeddings[i], input_embeddings[i + 1])\n-                self._tied_weights_keys.append(f\"lm_heads.{i}.weight\")\n-\n-    def tie_weights(self):\n-        \"\"\"\n-        Tie the weights between the input embeddings list and the output embeddings list.\n-        \"\"\"\n-        for module in self.modules():\n-            if hasattr(module, \"_tie_weights\"):\n-                module._tie_weights()\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -1580,14 +1566,6 @@ def generate(\n \n         return audio\n \n-    def tie_weights(self):\n-        \"\"\"\n-        Tie the weights between the input embeddings list and the output embeddings list.\n-        \"\"\"\n-        for module in self.modules():\n-            if hasattr(module, \"_tie_weights\"):\n-                module._tie_weights()\n-\n \n __all__ = [\n     \"BarkFineModel\","
        },
        {
            "sha": "cb5d5062b1a70f38ea1acbd80d08c41c07aa58f6",
            "filename": "src/transformers/models/bart/configuration_bart.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fbart%2Fconfiguration_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fbart%2Fconfiguration_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fconfiguration_bart.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -164,7 +164,7 @@ def __init__(\n             forced_eos_token_id=forced_eos_token_id,\n             **kwargs,\n         )\n-\n+        self.tie_encoder_decoder = True\n         # ensure backward compatibility for BART CNN models\n         if self.forced_bos_token_id is None and kwargs.get(\"force_bos_token_to_be_generated\", False):\n             self.forced_bos_token_id = self.bos_token_id"
        },
        {
            "sha": "d08608268a15fa093a1de30d398e0b211551e722",
            "filename": "src/transformers/models/bart/modeling_bart.py",
            "status": "modified",
            "additions": 27,
            "deletions": 46,
            "changes": 73,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -476,19 +476,20 @@ class BartPreTrainedModel(PreTrainedModel):\n \n     _can_compile_fullgraph = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         std = self.config.init_std\n         if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.weight.data.fill_(1.0)\n-            module.bias.data.zero_()\n+            module.weight.fill_(1.0)\n+            module.bias.zero_()\n \n     @property\n     def dummy_inputs(self):\n@@ -527,7 +528,7 @@ class BartEncoder(BartPreTrainedModel):\n         embed_tokens (nn.Embedding): output embedding\n     \"\"\"\n \n-    def __init__(self, config: BartConfig, embed_tokens: Optional[nn.Embedding] = None):\n+    def __init__(self, config: BartConfig):\n         super().__init__(config)\n \n         self.dropout = config.dropout\n@@ -538,12 +539,9 @@ def __init__(self, config: BartConfig, embed_tokens: Optional[nn.Embedding] = No\n         self.max_source_positions = config.max_position_embeddings\n         embed_scale = math.sqrt(embed_dim) if config.scale_embedding else 1.0\n \n-        if embed_tokens is not None:\n-            self.embed_tokens = embed_tokens\n-        else:\n-            self.embed_tokens = BartScaledWordEmbedding(\n-                config.vocab_size, embed_dim, self.padding_idx, embed_scale=embed_scale\n-            )\n+        self.embed_tokens = BartScaledWordEmbedding(\n+            config.vocab_size, embed_dim, self.padding_idx, embed_scale=embed_scale\n+        )\n \n         self.embed_positions = BartLearnedPositionalEmbedding(\n             config.max_position_embeddings,\n@@ -674,20 +672,17 @@ class BartDecoder(BartPreTrainedModel):\n         embed_tokens (nn.Embedding): output embedding\n     \"\"\"\n \n-    def __init__(self, config: BartConfig, embed_tokens: Optional[nn.Embedding] = None):\n+    def __init__(self, config: BartConfig):\n         super().__init__(config)\n         self.dropout = config.dropout\n         self.layerdrop = config.decoder_layerdrop\n         self.padding_idx = config.pad_token_id\n         self.max_target_positions = config.max_position_embeddings\n         embed_scale = math.sqrt(config.d_model) if config.scale_embedding else 1.0\n \n-        if embed_tokens is not None:\n-            self.embed_tokens = embed_tokens\n-        else:\n-            self.embed_tokens = BartScaledWordEmbedding(\n-                config.vocab_size, config.d_model, self.padding_idx, embed_scale=embed_scale\n-            )\n+        self.embed_tokens = BartScaledWordEmbedding(\n+            config.vocab_size, config.d_model, self.padding_idx, embed_scale=embed_scale\n+        )\n \n         self.embed_positions = BartLearnedPositionalEmbedding(\n             config.max_position_embeddings,\n@@ -899,7 +894,10 @@ def forward(\n \n @auto_docstring\n class BartModel(BartPreTrainedModel):\n-    _tied_weights_keys = [\"encoder.embed_tokens.weight\", \"decoder.embed_tokens.weight\"]\n+    _tied_weights_keys = {\n+        \"decoder.embed_tokens.weight\": \"shared.weight\",\n+        \"encoder.embed_tokens.weight\": \"shared.weight\",\n+    }\n \n     def __init__(self, config: BartConfig):\n         super().__init__(config)\n@@ -908,24 +906,12 @@ def __init__(self, config: BartConfig):\n         embed_scale = math.sqrt(config.d_model) if config.scale_embedding else 1.0\n         self.shared = BartScaledWordEmbedding(vocab_size, config.d_model, padding_idx, embed_scale=embed_scale)\n \n-        self.encoder = BartEncoder(config, self.shared)\n-        self.decoder = BartDecoder(config, self.shared)\n+        self.encoder = BartEncoder(config)\n+        self.decoder = BartDecoder(config)\n \n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def _tie_weights(self):\n-        if self.config.tie_word_embeddings:\n-            # Some model checkpoints like \"facebook/bart-large-cnn\"'s embedding weight is in decoder.embed_tokens, need check here, see issue #36247\n-            if self.shared.weight.device == torch.device(\n-                \"meta\"\n-            ) and self.decoder.embed_tokens.weight.device != torch.device(\"meta\"):\n-                self._tie_embedding_weights(self.encoder.embed_tokens, self.decoder.embed_tokens)\n-                self._tie_embedding_weights(self.shared, self.decoder.embed_tokens)\n-            else:\n-                self._tie_embedding_weights(self.encoder.embed_tokens, self.shared)\n-                self._tie_embedding_weights(self.decoder.embed_tokens, self.shared)\n-\n     def get_input_embeddings(self):\n         return self.shared\n \n@@ -1052,7 +1038,9 @@ def forward(\n )\n class BartForConditionalGeneration(BartPreTrainedModel, GenerationMixin):\n     base_model_prefix = \"model\"\n-    _tied_weights_keys = [\"encoder.embed_tokens.weight\", \"decoder.embed_tokens.weight\", \"lm_head.weight\"]\n+    _tied_weights_keys = {\n+        \"lm_head.weight\": \"model.shared.weight\",\n+    }\n     _keys_to_ignore_on_load_missing = [\"final_logits_bias\"]\n \n     def __init__(self, config: BartConfig):\n@@ -1086,11 +1074,6 @@ def _resize_final_logits_bias(self, new_num_tokens: int) -> None:\n             new_bias = torch.cat([self.final_logits_bias, extra_bias], dim=1)\n         self.register_buffer(\"final_logits_bias\", new_bias)\n \n-    def _tie_weights(self):\n-        if self.config.tie_word_embeddings:\n-            self.model._tie_weights()\n-            self._tie_embedding_weights(self.lm_head, self.model.shared)\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -1240,8 +1223,6 @@ def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n     \"\"\"\n )\n class BartForSequenceClassification(BartPreTrainedModel):\n-    _tied_weights_keys = [\"encoder.embed_tokens.weight\", \"decoder.embed_tokens.weight\"]\n-\n     def __init__(self, config: BartConfig, **kwargs):\n         super().__init__(config, **kwargs)\n         self.model = BartModel(config)\n@@ -1374,8 +1355,6 @@ def forward(\n \n @auto_docstring\n class BartForQuestionAnswering(BartPreTrainedModel):\n-    _tied_weights_keys = [\"encoder.embed_tokens.weight\", \"decoder.embed_tokens.weight\"]\n-\n     def __init__(self, config):\n         super().__init__(config)\n \n@@ -1513,7 +1492,9 @@ def forward(self, *args, **kwargs):\n     \"\"\"\n )\n class BartForCausalLM(BartPreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\n+        \"lm_head.weight\": \"model.decoder.embed_tokens.weight\",\n+    }\n \n     def __init__(self, config):\n         config.is_decoder = True"
        },
        {
            "sha": "afa955985696d5399d36446148992814601586d0",
            "filename": "src/transformers/models/beit/modeling_beit.py",
            "status": "modified",
            "additions": 13,
            "deletions": 12,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -692,31 +692,32 @@ class BeitPreTrainedModel(PreTrainedModel):\n     _keys_to_ignore_on_load_unexpected = [r\".*relative_position_index.*\"]\n     _supports_sdpa = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, (nn.Linear, nn.Conv2d, nn.ConvTranspose2d)):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n         elif isinstance(module, BeitEmbeddings):\n-            module.cls_token.data.zero_()\n+            module.cls_token.zero_()\n             if module.mask_token is not None:\n-                module.mask_token.data.zero_()\n+                module.mask_token.zero_()\n             if module.position_embeddings is not None:\n-                module.position_embeddings.data.zero_()\n+                module.position_embeddings.zero_()\n         elif isinstance(module, BeitRelativePositionBias):\n-            module.relative_position_bias_table.data.zero_()\n+            module.relative_position_bias_table.zero_()\n         elif isinstance(module, BeitLayer):\n             if module.lambda_1 is not None:\n-                module.lambda_1.data.fill_(self.config.layer_scale_init_value)\n-                module.lambda_2.data.fill_(self.config.layer_scale_init_value)\n+                module.lambda_1.fill_(self.config.layer_scale_init_value)\n+                module.lambda_2.fill_(self.config.layer_scale_init_value)\n \n \n @auto_docstring"
        },
        {
            "sha": "bf7d54108b32b6accc2de52baa53a387066e9460",
            "filename": "src/transformers/models/bert/modeling_bert.py",
            "status": "modified",
            "additions": 21,
            "deletions": 18,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -506,16 +506,9 @@ def __init__(self, config):\n \n         # The output weights are the same as the input embeddings, but there is\n         # an output-only bias for each token.\n-        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n-\n+        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=True)\n         self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n \n-        # Need a link between the two variables so that the bias is correctly resized with `resize_token_embeddings`\n-        self.decoder.bias = self.bias\n-\n-    def _tie_weights(self):\n-        self.decoder.bias = self.bias\n-\n     def forward(self, hidden_states):\n         hidden_states = self.transform(hidden_states)\n         hidden_states = self.decoder(hidden_states)\n@@ -569,21 +562,22 @@ class BertPreTrainedModel(PreTrainedModel):\n         \"cross_attentions\": BertCrossAttention,\n     }\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n         elif isinstance(module, BertLMPredictionHead):\n-            module.bias.data.zero_()\n+            module.bias.zero_()\n \n \n @dataclass\n@@ -770,7 +764,10 @@ def _create_attention_masks(\n     \"\"\"\n )\n class BertForPreTraining(BertPreTrainedModel):\n-    _tied_weights_keys = [\"predictions.decoder.bias\", \"cls.predictions.decoder.weight\"]\n+    _tied_weights_keys = {\n+        \"cls.predictions.decoder.weight\": \"bert.embeddings.word_embeddings.weight\",\n+        \"cls.predictions.decoder.bias\": \"cls.predictions.bias\",\n+    }\n \n     def __init__(self, config):\n         super().__init__(config)\n@@ -864,7 +861,10 @@ def forward(\n     \"\"\"\n )\n class BertLMHeadModel(BertPreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"cls.predictions.decoder.bias\", \"cls.predictions.decoder.weight\"]\n+    _tied_weights_keys = {\n+        \"cls.predictions.decoder.weight\": \"bert.embeddings.word_embeddings.weight\",\n+        \"cls.predictions.decoder.bias\": \"cls.predictions.bias\",\n+    }\n \n     def __init__(self, config):\n         super().__init__(config)\n@@ -948,7 +948,10 @@ def forward(\n \n @auto_docstring\n class BertForMaskedLM(BertPreTrainedModel):\n-    _tied_weights_keys = [\"predictions.decoder.bias\", \"cls.predictions.decoder.weight\"]\n+    _tied_weights_keys = {\n+        \"cls.predictions.decoder.weight\": \"bert.embeddings.word_embeddings.weight\",\n+        \"cls.predictions.decoder.bias\": \"cls.predictions.bias\",\n+    }\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "359ef6889a450279f36a9bd43e8bb0a08a12a209",
            "filename": "src/transformers/models/bert_generation/modeling_bert_generation.py",
            "status": "modified",
            "additions": 12,
            "deletions": 17,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -456,21 +456,22 @@ class BertGenerationPreTrainedModel(PreTrainedModel):\n         \"cross_attentions\": BertGenerationCrossAttention,\n     }\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n         elif isinstance(module, BertGenerationOnlyLMHead):\n-            module.bias.data.zero_()\n+            module.bias.zero_()\n \n \n @auto_docstring(\n@@ -629,28 +630,22 @@ def __init__(self, config):\n         super().__init__()\n         self.decoder = nn.Linear(config.hidden_size, config.vocab_size)\n         self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n-        self.decoder.bias = self.bias\n \n     def forward(self, hidden_states):\n         logits = self.decoder(hidden_states)\n         return logits\n \n-    def _tie_weights(self):\n-        # For accelerate compatibility and to not break backward compatibility\n-        if self.decoder.bias.device.type == \"meta\":\n-            self.decoder.bias = self.bias\n-        else:\n-            # To tie those two weights if they get disconnected (on TPU or when the bias is resized)\n-            self.bias = self.decoder.bias\n-\n \n @auto_docstring(\n     custom_intro=\"\"\"\n     BertGeneration Model with a `language modeling` head on top for CLM fine-tuning.\n     \"\"\"\n )\n class BertGenerationDecoder(BertGenerationPreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.decoder.weight\", \"lm_head.decoder.bias\"]\n+    _tied_weights_keys = {\n+        \"lm_head.decoder.weight\": \"bert.embeddings.word_embeddings.weight\",\n+        \"lm_head.decoder.bias\": \"lm_head.bias\",\n+    }\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "ccdc0dd8b84220b4708268cda4eeddbcd618302a",
            "filename": "src/transformers/models/big_bird/modeling_big_bird.py",
            "status": "modified",
            "additions": 21,
            "deletions": 18,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -1464,16 +1464,9 @@ def __init__(self, config):\n \n         # The output weights are the same as the input embeddings, but there is\n         # an output-only bias for each token.\n-        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n-\n+        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=True)\n         self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n \n-        # Need a link between the two variables so that the bias is correctly resized with `resize_token_embeddings`\n-        self.decoder.bias = self.bias\n-\n-    def _tie_weights(self):\n-        self.decoder.bias = self.bias\n-\n     def forward(self, hidden_states):\n         hidden_states = self.transform(hidden_states)\n         hidden_states = self.decoder(hidden_states)\n@@ -1521,21 +1514,22 @@ class BigBirdPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"bert\"\n     supports_gradient_checkpointing = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n         elif isinstance(module, BigBirdLMPredictionHead):\n-            module.bias.data.zero_()\n+            module.bias.zero_()\n \n \n @dataclass\n@@ -1899,7 +1893,10 @@ def _pad_to_block_size(\n \n \n class BigBirdForPreTraining(BigBirdPreTrainedModel):\n-    _tied_weights_keys = [\"cls.predictions.decoder.weight\", \"cls.predictions.decoder.bias\"]\n+    _tied_weights_keys = {\n+        \"cls.predictions.decoder.bias\": \"cls.predictions.bias\",\n+        \"cls.predictions.decoder.weight\": \"bert.embeddings.word_embeddings.weight\",\n+    }\n \n     def __init__(self, config):\n         super().__init__(config)\n@@ -1999,7 +1996,10 @@ def forward(\n \n @auto_docstring\n class BigBirdForMaskedLM(BigBirdPreTrainedModel):\n-    _tied_weights_keys = [\"cls.predictions.decoder.weight\", \"cls.predictions.decoder.bias\"]\n+    _tied_weights_keys = {\n+        \"cls.predictions.decoder.bias\": \"cls.predictions.bias\",\n+        \"cls.predictions.decoder.weight\": \"bert.embeddings.word_embeddings.weight\",\n+    }\n \n     def __init__(self, config):\n         super().__init__(config)\n@@ -2141,7 +2141,10 @@ def prepare_inputs_for_generation(self, input_ids, attention_mask=None, **model_\n     \"\"\"\n )\n class BigBirdForCausalLM(BigBirdPreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"cls.predictions.decoder.weight\", \"cls.predictions.decoder.bias\"]\n+    _tied_weights_keys = {\n+        \"cls.predictions.decoder.bias\": \"cls.predictions.bias\",\n+        \"cls.predictions.decoder.weight\": \"bert.embeddings.word_embeddings.weight\",\n+    }\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "220b050496a174737ae561d2b2654e1df9549300",
            "filename": "src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py",
            "status": "modified",
            "additions": 18,
            "deletions": 34,
            "changes": 52,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -1539,19 +1539,20 @@ class BigBirdPegasusPreTrainedModel(PreTrainedModel):\n \n     _can_compile_fullgraph = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         std = self.config.init_std\n         if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.weight.data.fill_(1.0)\n-            module.bias.data.zero_()\n+            module.weight.fill_(1.0)\n+            module.bias.zero_()\n \n     @property\n     def dummy_inputs(self):\n@@ -1574,7 +1575,7 @@ class BigBirdPegasusEncoder(BigBirdPegasusPreTrainedModel):\n         embed_tokens (nn.Embedding): output embedding\n     \"\"\"\n \n-    def __init__(self, config: BigBirdPegasusConfig, embed_tokens: Optional[nn.Embedding] = None):\n+    def __init__(self, config: BigBirdPegasusConfig):\n         super().__init__(config)\n \n         self.attention_type = config.attention_type\n@@ -1592,9 +1593,6 @@ def __init__(self, config: BigBirdPegasusConfig, embed_tokens: Optional[nn.Embed\n             config.vocab_size, embed_dim, self.padding_idx, embed_scale=embed_scale\n         )\n \n-        if embed_tokens is not None:\n-            self.embed_tokens.weight = embed_tokens.weight\n-\n         self.embed_positions = BigBirdPegasusLearnedPositionalEmbedding(\n             config.max_position_embeddings,\n             embed_dim,\n@@ -1849,7 +1847,7 @@ class BigBirdPegasusDecoder(BigBirdPegasusPreTrainedModel):\n         embed_tokens (nn.Embedding): output embedding\n     \"\"\"\n \n-    def __init__(self, config: BigBirdPegasusConfig, embed_tokens: Optional[nn.Embedding] = None):\n+    def __init__(self, config: BigBirdPegasusConfig):\n         super().__init__(config)\n         self.dropout = config.dropout\n         self.layerdrop = config.decoder_layerdrop\n@@ -1861,9 +1859,6 @@ def __init__(self, config: BigBirdPegasusConfig, embed_tokens: Optional[nn.Embed\n             config.vocab_size, config.d_model, self.padding_idx, embed_scale=embed_scale\n         )\n \n-        if embed_tokens is not None:\n-            self.embed_tokens.weight = embed_tokens.weight\n-\n         self.embed_positions = BigBirdPegasusLearnedPositionalEmbedding(\n             config.max_position_embeddings,\n             config.d_model,\n@@ -2075,7 +2070,10 @@ def forward(\n \n @auto_docstring\n class BigBirdPegasusModel(BigBirdPegasusPreTrainedModel):\n-    _tied_weights_keys = [\"encoder.embed_tokens.weight\", \"decoder.embed_tokens.weight\"]\n+    _tied_weights_keys = {\n+        \"encoder.embed_tokens.weight\": \"shared.weight\",\n+        \"decoder.embed_tokens.weight\": \"shared.weight\",\n+    }\n \n     def __init__(self, config: BigBirdPegasusConfig):\n         super().__init__(config)\n@@ -2086,8 +2084,8 @@ def __init__(self, config: BigBirdPegasusConfig):\n             vocab_size, config.d_model, padding_idx, embed_scale=embed_scale\n         )\n \n-        self.encoder = BigBirdPegasusEncoder(config, self.shared)\n-        self.decoder = BigBirdPegasusDecoder(config, self.shared)\n+        self.encoder = BigBirdPegasusEncoder(config)\n+        self.decoder = BigBirdPegasusDecoder(config)\n \n         # Initialize weights and apply final processing\n         self.post_init()\n@@ -2100,11 +2098,6 @@ def set_input_embeddings(self, value):\n         self.encoder.embed_tokens = self.shared\n         self.decoder.embed_tokens = self.shared\n \n-    def _tie_weights(self):\n-        if self.config.tie_word_embeddings:\n-            self._tie_embedding_weights(self.encoder.embed_tokens, self.shared)\n-            self._tie_embedding_weights(self.decoder.embed_tokens, self.shared)\n-\n     def get_encoder(self):\n         return self.encoder\n \n@@ -2213,7 +2206,9 @@ def forward(\n # Copied from transformers.models.bart.modeling_bart.BartForConditionalGeneration with Bart->BigBirdPegasus, BART->BIGBIRD_PEGASUS\n class BigBirdPegasusForConditionalGeneration(BigBirdPegasusPreTrainedModel, GenerationMixin):\n     base_model_prefix = \"model\"\n-    _tied_weights_keys = [\"encoder.embed_tokens.weight\", \"decoder.embed_tokens.weight\", \"lm_head.weight\"]\n+    _tied_weights_keys = {\n+        \"lm_head.weight\": \"model.shared.weight\",\n+    }\n     _keys_to_ignore_on_load_missing = [\"final_logits_bias\"]\n \n     def __init__(self, config: BigBirdPegasusConfig):\n@@ -2247,11 +2242,6 @@ def _resize_final_logits_bias(self, new_num_tokens: int) -> None:\n             new_bias = torch.cat([self.final_logits_bias, extra_bias], dim=1)\n         self.register_buffer(\"final_logits_bias\", new_bias)\n \n-    def _tie_weights(self):\n-        if self.config.tie_word_embeddings:\n-            self.model._tie_weights()\n-            self._tie_embedding_weights(self.lm_head, self.model.shared)\n-\n     @auto_docstring\n     # Ignore copy\n     def forward(\n@@ -2374,8 +2364,6 @@ def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n     \"\"\"\n )\n class BigBirdPegasusForSequenceClassification(BigBirdPegasusPreTrainedModel):\n-    _tied_weights_keys = [\"encoder.embed_tokens.weight\", \"decoder.embed_tokens.weight\"]\n-\n     def __init__(self, config: BigBirdPegasusConfig, **kwargs):\n         super().__init__(config, **kwargs)\n         self.model = BigBirdPegasusModel(config)\n@@ -2497,8 +2485,6 @@ def forward(\n \n @auto_docstring\n class BigBirdPegasusForQuestionAnswering(BigBirdPegasusPreTrainedModel):\n-    _tied_weights_keys = [\"encoder.embed_tokens.weight\", \"decoder.embed_tokens.weight\"]\n-\n     def __init__(self, config):\n         super().__init__(config)\n \n@@ -2621,8 +2607,6 @@ def forward(self, *args, **kwargs):\n \n \n class BigBirdPegasusForCausalLM(BigBirdPegasusPreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n-\n     def __init__(self, config):\n         config.is_decoder = True\n         config.is_encoder_decoder = False"
        },
        {
            "sha": "886d80f9936a9784e56a5dc6bcf25975ab557a32",
            "filename": "src/transformers/models/biogpt/modeling_biogpt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -510,7 +510,7 @@ def forward(\n     \"\"\"\n )\n class BioGptForCausalLM(BioGptPreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"output_projection.weight\"]\n+    _tied_weights_keys = {\"output_projection.weight\": \"biogpt.embed_tokens.weight\"}\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "0a0e9958c1091c663edc2fde9fbdac59e7052fff",
            "filename": "src/transformers/models/biogpt/modular_biogpt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -332,7 +332,7 @@ def forward(\n     \"\"\"\n )\n class BioGptForCausalLM(BioGptPreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"output_projection.weight\"]\n+    _tied_weights_keys = {\"output_projection.weight\": \"biogpt.embed_tokens.weight\"}\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "fe80fcda4dc8c6ef8c162513755b140174f0d3bb",
            "filename": "src/transformers/models/bit/modeling_bit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fbit%2Fmodeling_bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fbit%2Fmodeling_bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbit%2Fmodeling_bit.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -628,6 +628,7 @@ class BitPreTrainedModel(PreTrainedModel):\n     main_input_name = \"pixel_values\"\n     _no_split_modules = [\"BitEmbeddings\"]\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         if isinstance(module, nn.Conv2d):\n             nn.init.kaiming_normal_(module.weight, mode=\"fan_out\", nonlinearity=\"relu\")"
        },
        {
            "sha": "3b4f3fd69ed008363e69b351a0a784f01b5ad2ca",
            "filename": "src/transformers/models/bitnet/modeling_bitnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -433,7 +433,7 @@ def forward(\n \n @auto_docstring\n class BitNetForCausalLM(BitNetPreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.embed_tokens.weight\"}\n     _tp_plan = None\n     _pp_plan = None\n "
        },
        {
            "sha": "093eb2428395dcce765ab53615e9124c071a7db8",
            "filename": "src/transformers/models/bitnet/modular_bitnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodular_bitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodular_bitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodular_bitnet.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -114,7 +114,7 @@ class BitNetModel(LlamaModel):\n \n \n class BitNetForCausalLM(LlamaForCausalLM):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.embed_tokens.weight\"}\n     _tp_plan = None\n     _pp_plan = None\n "
        },
        {
            "sha": "bd7790f5a7a44374c0116bcb62f3592f11609cbd",
            "filename": "src/transformers/models/blenderbot/modeling_blenderbot.py",
            "status": "modified",
            "additions": 27,
            "deletions": 25,
            "changes": 52,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -438,19 +438,20 @@ class BlenderbotPreTrainedModel(PreTrainedModel):\n \n     _can_compile_fullgraph = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         std = self.config.init_std\n         if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.weight.data.fill_(1.0)\n-            module.bias.data.zero_()\n+            module.weight.fill_(1.0)\n+            module.bias.zero_()\n \n     @property\n     def dummy_inputs(self):\n@@ -474,7 +475,7 @@ class BlenderbotEncoder(BlenderbotPreTrainedModel):\n         embed_tokens (nn.Embedding): output embedding\n     \"\"\"\n \n-    def __init__(self, config: BlenderbotConfig, embed_tokens: Optional[nn.Embedding] = None):\n+    def __init__(self, config: BlenderbotConfig):\n         super().__init__(config)\n \n         self.dropout = config.dropout\n@@ -485,12 +486,9 @@ def __init__(self, config: BlenderbotConfig, embed_tokens: Optional[nn.Embedding\n         self.max_source_positions = config.max_position_embeddings\n         embed_scale = math.sqrt(embed_dim) if config.scale_embedding else 1.0\n \n-        if embed_tokens is not None:\n-            self.embed_tokens = embed_tokens\n-        else:\n-            self.embed_tokens = BlenderbotScaledWordEmbedding(\n-                config.vocab_size, embed_dim, self.padding_idx, embed_scale=embed_scale\n-            )\n+        self.embed_tokens = BlenderbotScaledWordEmbedding(\n+            config.vocab_size, embed_dim, self.padding_idx, embed_scale=embed_scale\n+        )\n \n         self.embed_positions = BlenderbotLearnedPositionalEmbedding(\n             config.max_position_embeddings,\n@@ -623,20 +621,17 @@ class BlenderbotDecoder(BlenderbotPreTrainedModel):\n         embed_tokens (nn.Embedding): output embedding\n     \"\"\"\n \n-    def __init__(self, config: BlenderbotConfig, embed_tokens: Optional[nn.Embedding] = None):\n+    def __init__(self, config: BlenderbotConfig):\n         super().__init__(config)\n         self.dropout = config.dropout\n         self.layerdrop = config.decoder_layerdrop\n         self.padding_idx = config.pad_token_id\n         self.max_target_positions = config.max_position_embeddings\n         embed_scale = math.sqrt(config.d_model) if config.scale_embedding else 1.0\n \n-        if embed_tokens is not None:\n-            self.embed_tokens = embed_tokens\n-        else:\n-            self.embed_tokens = BlenderbotScaledWordEmbedding(\n-                config.vocab_size, config.d_model, self.padding_idx, embed_scale=embed_scale\n-            )\n+        self.embed_tokens = BlenderbotScaledWordEmbedding(\n+            config.vocab_size, config.d_model, self.padding_idx, embed_scale=embed_scale\n+        )\n \n         self.embed_positions = BlenderbotLearnedPositionalEmbedding(\n             config.max_position_embeddings,\n@@ -852,16 +847,19 @@ def forward(\n \n @auto_docstring\n class BlenderbotModel(BlenderbotPreTrainedModel):\n-    _tied_weights_keys = [\"decoder.embed_tokens.weight\", \"encoder.embed_tokens.weight\"]\n+    _tied_weights_keys = {\n+        \"encoder.embed_tokens.weight\": \"shared.weight\",\n+        \"decoder.embed_tokens.weight\": \"shared.weight\",\n+    }\n \n     def __init__(self, config: BlenderbotConfig):\n         super().__init__(config)\n \n         padding_idx, vocab_size = config.pad_token_id, config.vocab_size\n         embed_scale = math.sqrt(config.d_model) if config.scale_embedding else 1.0\n         self.shared = BlenderbotScaledWordEmbedding(vocab_size, config.d_model, padding_idx, embed_scale=embed_scale)\n-        self.encoder = BlenderbotEncoder(config, self.shared)\n-        self.decoder = BlenderbotDecoder(config, self.shared)\n+        self.encoder = BlenderbotEncoder(config)\n+        self.decoder = BlenderbotDecoder(config)\n \n         # Initialize weights and apply final processing\n         self.post_init()\n@@ -1001,7 +999,9 @@ def forward(\n class BlenderbotForConditionalGeneration(BlenderbotPreTrainedModel, GenerationMixin):\n     base_model_prefix = \"model\"\n     _keys_to_ignore_on_load_missing = [\"final_logits_bias\"]\n-    _tied_weights_keys = [\"decoder.embed_tokens.weight\", \"encoder.embed_tokens.weight\", \"lm_head.weight\"]\n+    _tied_weights_keys = {\n+        \"lm_head.weight\": \"model.shared.weight\",\n+    }\n \n     def __init__(self, config: BlenderbotConfig):\n         super().__init__(config)\n@@ -1184,7 +1184,9 @@ def forward(self, *args, **kwargs):\n \n # Copied from transformers.models.bart.modeling_bart.BartForCausalLM with Bart->Blenderbot, facebook/bart-base->facebook/blenderbot-400M-distill\n class BlenderbotForCausalLM(BlenderbotPreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\n+        \"lm_head.weight\": \"model.decoder.embed_tokens.weight\",\n+    }\n \n     def __init__(self, config):\n         config.is_decoder = True"
        },
        {
            "sha": "bd1a36cb4d22d59c1d100144e092d1ed7d918d86",
            "filename": "src/transformers/models/blenderbot_small/modeling_blenderbot_small.py",
            "status": "modified",
            "additions": 23,
            "deletions": 21,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -431,19 +431,20 @@ class BlenderbotSmallPreTrainedModel(PreTrainedModel):\n \n     _can_compile_fullgraph = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         std = self.config.init_std\n         if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.weight.data.fill_(1.0)\n-            module.bias.data.zero_()\n+            module.weight.fill_(1.0)\n+            module.bias.zero_()\n \n     @property\n     def dummy_inputs(self):\n@@ -467,7 +468,7 @@ class BlenderbotSmallEncoder(BlenderbotSmallPreTrainedModel):\n         embed_tokens (nn.Embedding): output embedding\n     \"\"\"\n \n-    def __init__(self, config: BlenderbotSmallConfig, embed_tokens: Optional[nn.Embedding] = None):\n+    def __init__(self, config: BlenderbotSmallConfig):\n         super().__init__(config)\n \n         self.dropout = config.dropout\n@@ -478,10 +479,7 @@ def __init__(self, config: BlenderbotSmallConfig, embed_tokens: Optional[nn.Embe\n         self.max_source_positions = config.max_position_embeddings\n         self.embed_scale = math.sqrt(embed_dim) if config.scale_embedding else 1.0\n \n-        if embed_tokens is not None:\n-            self.embed_tokens = embed_tokens\n-        else:\n-            self.embed_tokens = nn.Embedding(config.vocab_size, embed_dim, self.padding_idx)\n+        self.embed_tokens = nn.Embedding(config.vocab_size, embed_dim, self.padding_idx)\n \n         self.embed_positions = BlenderbotSmallLearnedPositionalEmbedding(\n             config.max_position_embeddings,\n@@ -612,18 +610,15 @@ class BlenderbotSmallDecoder(BlenderbotSmallPreTrainedModel):\n         embed_tokens (nn.Embedding): output embedding\n     \"\"\"\n \n-    def __init__(self, config: BlenderbotSmallConfig, embed_tokens: Optional[nn.Embedding] = None):\n+    def __init__(self, config: BlenderbotSmallConfig):\n         super().__init__(config)\n         self.dropout = config.dropout\n         self.layerdrop = config.decoder_layerdrop\n         self.padding_idx = config.pad_token_id\n         self.max_target_positions = config.max_position_embeddings\n         self.embed_scale = math.sqrt(config.d_model) if config.scale_embedding else 1.0\n \n-        if embed_tokens is not None:\n-            self.embed_tokens = embed_tokens\n-        else:\n-            self.embed_tokens = nn.Embedding(config.vocab_size, config.d_model, self.padding_idx)\n+        self.embed_tokens = nn.Embedding(config.vocab_size, config.d_model, self.padding_idx)\n \n         self.embed_positions = BlenderbotSmallLearnedPositionalEmbedding(\n             config.max_position_embeddings,\n@@ -838,16 +833,19 @@ def forward(\n \n @auto_docstring\n class BlenderbotSmallModel(BlenderbotSmallPreTrainedModel):\n-    _tied_weights_keys = [\"decoder.embed_tokens.weight\", \"encoder.embed_tokens.weight\"]\n+    _tied_weights_keys = {\n+        \"encoder.embed_tokens.weight\": \"shared.weight\",\n+        \"decoder.embed_tokens.weight\": \"shared.weight\",\n+    }\n \n     def __init__(self, config: BlenderbotSmallConfig):\n         super().__init__(config)\n \n         padding_idx, vocab_size = config.pad_token_id, config.vocab_size\n         self.shared = nn.Embedding(vocab_size, config.d_model, padding_idx)\n \n-        self.encoder = BlenderbotSmallEncoder(config, self.shared)\n-        self.decoder = BlenderbotSmallDecoder(config, self.shared)\n+        self.encoder = BlenderbotSmallEncoder(config)\n+        self.decoder = BlenderbotSmallDecoder(config)\n \n         # Initialize weights and apply final processing\n         self.post_init()\n@@ -974,7 +972,9 @@ def forward(\n class BlenderbotSmallForConditionalGeneration(BlenderbotSmallPreTrainedModel, GenerationMixin):\n     base_model_prefix = \"model\"\n     _keys_to_ignore_on_load_missing = [\"final_logits_bias\"]\n-    _tied_weights_keys = [\"decoder.embed_tokens.weight\", \"encoder.embed_tokens.weight\", \"lm_head.weight\"]\n+    _tied_weights_keys = {\n+        \"lm_head.weight\": \"model.shared.weight\",\n+    }\n \n     def __init__(self, config: BlenderbotSmallConfig):\n         super().__init__(config)\n@@ -1144,7 +1144,9 @@ def forward(self, *args, **kwargs):\n \n # Copied from transformers.models.bart.modeling_bart.BartForCausalLM with Bart->BlenderbotSmall, facebook/bart-base->facebook/blenderbot_small-90M\n class BlenderbotSmallForCausalLM(BlenderbotSmallPreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\n+        \"lm_head.weight\": \"model.decoder.embed_tokens.weight\",\n+    }\n \n     def __init__(self, config):\n         config.is_decoder = True"
        },
        {
            "sha": "aa812903f31122c824229e93948d36444936ff78",
            "filename": "src/transformers/models/blip/modeling_blip.py",
            "status": "modified",
            "additions": 14,
            "deletions": 8,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -419,13 +419,14 @@ class BlipPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"BlipEncoderLayer\", \"BlipTextEmbeddings\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         factor = self.config.initializer_range\n         if isinstance(module, (nn.Conv2d, nn.Embedding, nn.Linear)):\n-            module.weight.data.normal_(mean=0.0, std=factor)\n+            module.weight.normal_(mean=0.0, std=factor)\n             if hasattr(module, \"bias\") and module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n \n         if isinstance(module, BlipVisionEmbeddings):\n             if hasattr(self.config, \"vision_config\"):\n@@ -443,10 +444,10 @@ def _init_weights(self, module):\n             )\n \n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n         elif isinstance(module, nn.Linear) and module.bias is not None:\n-            module.bias.data.zero_()\n+            module.bias.zero_()\n \n \n class BlipEncoder(nn.Module):\n@@ -797,8 +798,11 @@ def forward(\n )\n class BlipForConditionalGeneration(BlipPreTrainedModel, GenerationMixin):\n     config: BlipConfig\n-    _tied_weights_keys = [\"text_decoder.cls.predictions.decoder.bias\"]\n     main_input_name = \"pixel_values\"\n+    _tied_weights_keys = {\n+        \"text_decoder.cls.predictions.decoder.bias\": \"text_decoder.cls.predictions.bias\",\n+        \"text_decoder.cls.predictions.decoder.weight\": \"text_decoder.bert.embeddings.word_embeddings.weight\",\n+    }  # TODO @arthurzucker check why we need this when for other models, their subPreTrainedModel handle it themselves.\n \n     def __init__(self, config: BlipConfig):\n         super().__init__(config)\n@@ -963,15 +967,17 @@ def generate(\n )\n class BlipForQuestionAnswering(BlipPreTrainedModel, GenerationMixin):\n     config: BlipConfig\n-    _tied_weights_keys = [\"text_decoder.cls.predictions.decoder.bias\"]\n+    _tied_weights_keys = {\n+        \"text_decoder.cls.predictions.decoder.bias\": \"text_decoder.cls.predictions.bias\",\n+        \"text_decoder.cls.predictions.decoder.weight\": \"text_decoder.bert.embeddings.word_embeddings.weight\",\n+    }\n \n     def __init__(self, config: BlipConfig):\n         super().__init__(config)\n \n         self.vision_model = BlipVisionModel(config.vision_config)\n \n         self.text_encoder = BlipTextModel(config.text_config, add_pooling_layer=False)\n-\n         self.text_decoder = BlipTextLMHeadModel(config.text_config)\n \n         self.decoder_pad_token_id = config.text_config.pad_token_id"
        },
        {
            "sha": "6e9e3bb7c2c3f4604abec17ab87631a0efa6957e",
            "filename": "src/transformers/models/blip/modeling_blip_text.py",
            "status": "modified",
            "additions": 10,
            "deletions": 13,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -473,16 +473,9 @@ def __init__(self, config):\n \n         # The output weights are the same as the input embeddings, but there is\n         # an output-only bias for each token.\n-        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n-\n+        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=True)\n         self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n \n-        # Need a link between the two variables so that the bias is correctly resized with `resize_token_embeddings`\n-        self.decoder.bias = self.bias\n-\n-    def _tie_weights(self):\n-        self.decoder.bias = self.bias\n-\n     def forward(self, hidden_states):\n         hidden_states = self.transform(hidden_states)\n         hidden_states = self.decoder(hidden_states)\n@@ -511,15 +504,16 @@ class BlipTextPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"bert\"\n     _no_split_modules = []\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, (nn.Linear, nn.Embedding)):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n         if isinstance(module, nn.Linear) and module.bias is not None:\n-            module.bias.data.zero_()\n+            module.bias.zero_()\n \n \n # Adapted from https://github.com/salesforce/BLIP/blob/3a29b7410476bf5f2ba0955827390eb6ea1f4f9d/models/med.py#L571\n@@ -744,7 +738,10 @@ def forward(\n \n # Adapted from https://github.com/salesforce/BLIP/blob/main/models/med.py#L811\n class BlipTextLMHeadModel(BlipTextPreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"cls.predictions.decoder.weight\", \"cls.predictions.decoder.bias\"]\n+    _tied_weights_keys = {\n+        \"cls.predictions.decoder.bias\": \"cls.predictions.bias\",\n+        \"cls.predictions.decoder.weight\": \"bert.embeddings.word_embeddings.weight\",\n+    }\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "175e691809356815fa17fbe061fb54faf0e9dcdf",
            "filename": "src/transformers/models/blip_2/modeling_blip_2.py",
            "status": "modified",
            "additions": 7,
            "deletions": 24,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -409,19 +409,20 @@ class Blip2PreTrainedModel(PreTrainedModel):\n     ]\n     _skip_keys_device_placement = \"past_key_values\"\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         factor = self.config.initializer_range\n \n         if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.data.normal_(mean=0.0, std=factor)\n+            module.weight.normal_(mean=0.0, std=factor)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=factor)\n+            module.weight.normal_(mean=0.0, std=factor)\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n         elif isinstance(module, Blip2VisionEmbeddings):\n             nn.init.trunc_normal_(module.position_embedding, mean=0.0, std=factor)\n             nn.init.trunc_normal_(module.class_embedding, mean=0.0, std=factor)\n@@ -435,7 +436,7 @@ def _init_weights(self, module):\n                 Blip2ForImageTextRetrieval,\n             ),\n         ):\n-            module.query_tokens.data.zero_()\n+            module.query_tokens.zero_()\n \n \n # Copied from transformers.models.blip.modeling_blip.BlipEncoder with Blip->Blip2\n@@ -1049,10 +1050,6 @@ def __init__(self, config: Blip2Config):\n         else:\n             language_model = AutoModelForSeq2SeqLM.from_config(config.text_config)\n \n-        # Update _tied_weights_keys using the base model used.\n-        if language_model._tied_weights_keys is not None:\n-            self._tied_weights_keys = [f\"language_model.{k}\" for k in language_model._tied_weights_keys]\n-\n         self.language_model = language_model\n \n         # Initialize weights and apply final processing\n@@ -1076,11 +1073,6 @@ def get_encoder(self):\n     def get_decoder(self):\n         return self.language_model.get_decoder()\n \n-    def _tie_weights(self):\n-        if not self.config.use_decoder_only_language_model:\n-            self.language_model.encoder.embed_tokens = self.language_model.shared\n-            self.language_model.decoder.embed_tokens = self.language_model.shared\n-\n     @filter_out_non_signature_kwargs()\n     @auto_docstring\n     def get_text_features(\n@@ -1612,10 +1604,6 @@ def __init__(self, config: Blip2Config):\n         else:\n             language_model = AutoModelForSeq2SeqLM.from_config(config.text_config)\n \n-        # Update _tied_weights_keys using the base model used.\n-        if language_model._tied_weights_keys is not None:\n-            self._tied_weights_keys = [f\"language_model.{k}\" for k in language_model._tied_weights_keys]\n-\n         self.language_model = language_model\n \n         # Initialize weights and apply final processing\n@@ -1639,11 +1627,6 @@ def get_encoder(self):\n     def get_decoder(self):\n         return self.language_model.get_decoder()\n \n-    def _tie_weights(self):\n-        if not self.config.use_decoder_only_language_model:\n-            self.language_model.encoder.embed_tokens = self.language_model.shared\n-            self.language_model.decoder.embed_tokens = self.language_model.shared\n-\n     def _preprocess_accelerate(self):\n         r\"\"\"\n         Some pre-processing hacks to make the model `accelerate` compatible. Check"
        },
        {
            "sha": "82a5444b20574dcc12e5ec4dd5d0555023b9e3d4",
            "filename": "src/transformers/models/bloom/modeling_bloom.py",
            "status": "modified",
            "additions": 8,
            "deletions": 7,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -425,19 +425,20 @@ class BloomPreTrainedModel(PreTrainedModel):\n     def __init__(self, *inputs, **kwargs):\n         super().__init__(*inputs, **kwargs)\n \n+    @torch.no_grad()\n     def _init_weights(self, module: nn.Module):\n         \"\"\"Initialize the weights.\"\"\"\n         if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n \n \n @auto_docstring\n@@ -722,7 +723,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n     \"\"\"\n )\n class BloomForCausalLM(BloomPreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"transformer.word_embeddings.weight\"}\n \n     def __init__(self, config: BloomConfig):\n         super().__init__(config)"
        },
        {
            "sha": "fe435876db2abe2e516d1e81fcfd183ee240b404",
            "filename": "src/transformers/models/blt/modeling_blt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fblt%2Fmodeling_blt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fblt%2Fmodeling_blt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblt%2Fmodeling_blt.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -447,7 +447,6 @@ def forward(\n @auto_docstring\n class BltPreTrainedModel(PreTrainedModel):\n     config: BltConfig\n-    base_model_prefix = \"\"\n     input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"BltTransformerLayer\"]\n@@ -1231,7 +1230,7 @@ class BltForCausalLM(BltPreTrainedModel, GenerationMixin):\n     config: BltConfig\n     _can_compile_fullgraph = False\n     base_model_prefix = \"model\"\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"model.local_encoder.embed_tokens.weight\": \"lm_head.weight\"}\n \n     def __init__(self, config: BltConfig):\n         super().__init__(config.get_text_config())"
        },
        {
            "sha": "78d5aa5a15efe0f33d892e48ef5dcca383fd380a",
            "filename": "src/transformers/models/blt/modular_blt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fblt%2Fmodular_blt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fblt%2Fmodular_blt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblt%2Fmodular_blt.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -964,7 +964,7 @@ class BltForCausalLM(MllamaForCausalLM):\n     config: BltConfig\n     _can_compile_fullgraph = False\n     base_model_prefix = \"model\"\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"model.local_encoder.embed_tokens.weight\": \"lm_head.weight\"}\n \n     def __init__(self, config: BltConfig):\n         super().__init__(config)"
        },
        {
            "sha": "289b6673a3b19842160329b37139c927d248f61b",
            "filename": "src/transformers/models/bridgetower/configuration_bridgetower.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fconfiguration_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fconfiguration_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fconfiguration_bridgetower.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -175,7 +175,6 @@ def __init__(\n         **kwargs,\n     ):\n         super().__init__(**kwargs)\n-\n         self.vocab_size = vocab_size\n         self.hidden_size = hidden_size\n         self.num_hidden_layers = num_hidden_layers\n@@ -298,7 +297,7 @@ def __init__(\n \n         self.text_config = text_config\n         self.vision_config = vision_config\n-        super().__init__(**kwargs)\n+        super().__init__(tie_word_embeddings=tie_word_embeddings, **kwargs)\n \n \n __all__ = [\"BridgeTowerConfig\", \"BridgeTowerTextConfig\", \"BridgeTowerVisionConfig\"]"
        },
        {
            "sha": "a44eb7bfabb1327b8652e358fb01553253759098",
            "filename": "src/transformers/models/bridgetower/modeling_bridgetower.py",
            "status": "modified",
            "additions": 8,
            "deletions": 7,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -919,6 +919,7 @@ class BridgeTowerPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"BridgeTowerSelfAttention\", \"BridgeTowerResidualAttention\"]\n     _skip_keys_device_placement = \"past_key_values\"\n \n+    @torch.no_grad()\n     def _init_weights(self, module: nn.Module):\n         std = self.config.initializer_factor\n         if isinstance(module, BridgeTowerVisionTransformer):\n@@ -927,23 +928,23 @@ def _init_weights(self, module: nn.Module):\n             fc_std = (2 * self.config.hidden_size) ** -0.5\n             for block in module.transformer.resblocks:\n                 nn.init.normal_(block.attn.in_proj_weight, std=attn_std * std)\n-                block.attn.in_proj_bias.data.zero_()\n+                block.attn.in_proj_bias.zero_()\n                 nn.init.normal_(block.attn.out_proj.weight, std=proj_std * std)\n                 nn.init.normal_(block.mlp.c_fc.weight, std=fc_std * std)\n                 nn.init.normal_(block.mlp.c_proj.weight, std=proj_std * std)\n \n             nn.init.normal_(module.embeddings.class_embedding, std=attn_std * std)\n             nn.init.normal_(module.embeddings.position_embedding.weight, std=attn_std * std)\n         elif isinstance(module, (nn.Linear, nn.Conv2d, nn.Embedding)):\n-            module.weight.data.normal_(mean=0.0, std=0.05 * std)\n+            module.weight.normal_(mean=0.0, std=0.05 * std)\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n         elif isinstance(module, BridgeTowerForContrastiveLearning):\n-            module.logit_scale.data.fill_(self.config.logit_scale_init_value)\n+            module.logit_scale.fill_(self.config.logit_scale_init_value)\n \n         if isinstance(module, (nn.Linear, BridgeTowerMLMHead)) and module.bias is not None:\n-            module.bias.data.zero_()\n+            module.bias.zero_()\n \n \n class BridgeTowerVisionModel(BridgeTowerPreTrainedModel):\n@@ -1497,7 +1498,7 @@ def forward(self, x):\n     \"\"\"\n )\n class BridgeTowerForMaskedLM(BridgeTowerPreTrainedModel):\n-    _tied_weights_keys = [\"mlm_score.decoder.weight\"]\n+    _tied_weights_keys = {\"mlm_score.decoder.weight\": \"bridgetower.text_model.embeddings.word_embeddings.weight\"}\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "74da9e9c8ae870864d44c85bbe4fda7e0d421f8b",
            "filename": "src/transformers/models/bros/modeling_bros.py",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fbros%2Fmodeling_bros.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fbros%2Fmodeling_bros.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbros%2Fmodeling_bros.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -514,20 +514,21 @@ class BrosPreTrainedModel(PreTrainedModel):\n     config: BrosConfig\n     base_model_prefix = \"bros\"\n \n+    @torch.no_grad()\n     def _init_weights(self, module: nn.Module):\n         \"\"\"Initialize the weights\"\"\"\n         std = self.config.initializer_range\n         if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n         elif isinstance(module, BrosRelationExtractor):\n             nn.init.normal_(module.dummy_node, std=std)\n "
        },
        {
            "sha": "267aafe5959e059c4e0e47f9d0357322723ece1f",
            "filename": "src/transformers/models/camembert/modeling_camembert.py",
            "status": "modified",
            "additions": 16,
            "deletions": 18,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -383,7 +383,6 @@ def __init__(self, config):\n \n         self.decoder = nn.Linear(config.hidden_size, config.vocab_size)\n         self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n-        self.decoder.bias = self.bias\n \n     def forward(self, features, **kwargs):\n         x = self.dense(features)\n@@ -395,14 +394,6 @@ def forward(self, features, **kwargs):\n \n         return x\n \n-    def _tie_weights(self):\n-        # To tie those two weights if they get disconnected (on TPU or when the bias is resized)\n-        # For accelerate compatibility and to not break backward compatibility\n-        if self.decoder.bias.device.type == \"meta\":\n-            self.decoder.bias = self.bias\n-        else:\n-            self.bias = self.decoder.bias\n-\n \n @auto_docstring\n class CamembertPreTrainedModel(PreTrainedModel):\n@@ -419,21 +410,22 @@ class CamembertPreTrainedModel(PreTrainedModel):\n         \"cross_attentions\": CamembertCrossAttention,\n     }\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n         elif isinstance(module, CamembertLMHead):\n-            module.bias.data.zero_()\n+            module.bias.zero_()\n \n \n class CamembertEmbeddings(nn.Module):\n@@ -745,7 +737,10 @@ def _create_attention_masks(\n \n @auto_docstring\n class CamembertForMaskedLM(CamembertPreTrainedModel):\n-    _tied_weights_keys = [\"lm_head.decoder.weight\", \"lm_head.decoder.bias\"]\n+    _tied_weights_keys = {\n+        \"lm_head.decoder.weight\": \"roberta.embeddings.word_embeddings.weight\",\n+        \"lm_head.decoder.bias\": \"lm_head.bias\",\n+    }\n \n     def __init__(self, config):\n         super().__init__(config)\n@@ -1191,7 +1186,10 @@ def forward(\n     \"\"\"\n )\n class CamembertForCausalLM(CamembertPreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.decoder.weight\", \"lm_head.decoder.bias\"]\n+    _tied_weights_keys = {\n+        \"lm_head.decoder.weight\": \"camembert.embeddings.word_embeddings.weight\",\n+        \"lm_head.decoder.bias\": \"lm_head.bias\",\n+    }\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "6a72534c91327cf2e2d2ea7439664e91e2854523",
            "filename": "src/transformers/models/camembert/modular_camembert.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodular_camembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodular_camembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodular_camembert.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -53,6 +53,11 @@ class CamembertModel(RobertaModel):\n \n \n class CamembertForMaskedLM(RobertaForMaskedLM):\n+    _tied_weights_keys = {\n+        \"lm_head.decoder.weight\": \"roberta.embeddings.word_embeddings.weight\",\n+        \"lm_head.decoder.bias\": \"lm_head.bias\",\n+    }\n+\n     def __init__(self, config):\n         super().__init__(config)\n         del self.camembert"
        },
        {
            "sha": "2b0a1e8972662942828b1eeba757e8a6f9123216",
            "filename": "src/transformers/models/canine/modeling_canine.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fcanine%2Fmodeling_canine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fcanine%2Fmodeling_canine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcanine%2Fmodeling_canine.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -688,12 +688,11 @@ def __init__(self, config):\n \n         # The output weights are the same as the input embeddings, but there is\n         # an output-only bias for each token.\n-        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n+        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=True)\n \n         self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n \n         # Need a link between the two variables so that the bias is correctly resized with `resize_token_embeddings`\n-        self.decoder.bias = self.bias\n \n     def forward(self, hidden_states: tuple[torch.FloatTensor]) -> torch.FloatTensor:\n         hidden_states = self.transform(hidden_states)\n@@ -720,19 +719,20 @@ class CaninePreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"canine\"\n     supports_gradient_checkpointing = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, (nn.Linear, nn.Conv1d)):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n \n \n @auto_docstring"
        },
        {
            "sha": "0930e44cb71877060417beac4fbbe8a6727d1389",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -1009,7 +1009,7 @@ def forward(\n     \"\"\"\n )\n class ChameleonForConditionalGeneration(ChameleonPreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.embed_tokens.weight\"}\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "815124aa45f8a352a62f256c882f20b18f8c74c6",
            "filename": "src/transformers/models/chinese_clip/modeling_chinese_clip.py",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -562,6 +562,7 @@ class ChineseCLIPPreTrainedModel(PreTrainedModel):\n     input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         factor = self.config.initializer_factor\n@@ -576,7 +577,7 @@ def _init_weights(self, module):\n             nn.init.normal_(module.token_type_embeddings.weight, mean=0.0, std=self.config.initializer_range)\n             for embedding in [module.word_embeddings, module.position_embeddings, module.token_type_embeddings]:\n                 if embedding.padding_idx is not None:\n-                    embedding.weight.data[embedding.padding_idx].zero_()\n+                    embedding.weight[embedding.padding_idx].zero_()\n         elif isinstance(module, ChineseCLIPVisionAttention):\n             factor = self.config.initializer_factor\n             in_proj_std = (module.embed_dim**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n@@ -602,12 +603,12 @@ def _init_weights(self, module):\n             )\n \n         if isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n         if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n \n \n # Copied from transformers.models.align.modeling_align.AlignTextEncoder with Align->ChineseCLIP"
        },
        {
            "sha": "0a44ecb7ffe7c65c1e4e5688b4b0db9d9d97918b",
            "filename": "src/transformers/models/clap/modeling_clap.py",
            "status": "modified",
            "additions": 10,
            "deletions": 9,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -1308,28 +1308,29 @@ class ClapPreTrainedModel(PreTrainedModel):\n     input_modalities = [\"audio\", \"text\"]\n     supports_gradient_checkpointing = False\n \n+    @torch.no_grad()\n     def _init_weights(self, module: nn.Module):\n         \"\"\"Initialize the weights\"\"\"\n         factor = self.config.initializer_factor\n \n         if isinstance(module, ClapTextEmbeddings):\n-            module.position_embeddings.weight.data.normal_(mean=0.0, std=factor * 0.02)\n-            module.token_type_embeddings.weight.data.normal_(mean=0.0, std=factor * 0.02)\n+            module.position_embeddings.weight.normal_(mean=0.0, std=factor * 0.02)\n+            module.token_type_embeddings.weight.normal_(mean=0.0, std=factor * 0.02)\n         elif isinstance(module, ClapModel):\n-            module.logit_scale_a.data.fill_(math.log(self.config.logit_scale_init_value))\n-            module.logit_scale_t.data.fill_(math.log(self.config.logit_scale_init_value))\n+            module.logit_scale_a.fill_(math.log(self.config.logit_scale_init_value))\n+            module.logit_scale_t.fill_(math.log(self.config.logit_scale_init_value))\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=factor * 0.02)\n+            module.weight.normal_(mean=0.0, std=factor * 0.02)\n         elif isinstance(module, (nn.LayerNorm, nn.BatchNorm2d)):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n         elif isinstance(module, (nn.Conv2d, nn.Linear)):\n             in_proj_std = (self.config.hidden_size**-0.5) * ((2 * self.config.num_hidden_layers) ** -0.5) * factor\n             nn.init.normal_(module.weight, std=in_proj_std)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, ClapAudioSelfAttention):\n-            module.relative_position_bias_table.data.zero_()\n+            module.relative_position_bias_table.zero_()\n \n \n class ClapAudioModel(ClapPreTrainedModel):"
        },
        {
            "sha": "8ce33c4a0dcff930b526f879335e7886699ebc84",
            "filename": "src/transformers/models/clip/modeling_clip.py",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -408,12 +408,13 @@ class CLIPPreTrainedModel(PreTrainedModel):\n         \"attentions\": CLIPAttention,\n     }\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         factor = self.config.initializer_factor\n         if isinstance(module, CLIPTextEmbeddings):\n-            module.token_embedding.weight.data.normal_(mean=0.0, std=factor * 0.02)\n-            module.position_embedding.weight.data.normal_(mean=0.0, std=factor * 0.02)\n+            module.token_embedding.weight.normal_(mean=0.0, std=factor * 0.02)\n+            module.position_embedding.weight.normal_(mean=0.0, std=factor * 0.02)\n         elif isinstance(module, CLIPVisionEmbeddings):\n             factor = self.config.initializer_factor\n             nn.init.normal_(module.class_embedding, mean=0.0, std=module.embed_dim**-0.5 * factor)\n@@ -459,10 +460,10 @@ def _init_weights(self, module):\n             )\n \n         if isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n         if isinstance(module, nn.Linear) and module.bias is not None:\n-            module.bias.data.zero_()\n+            module.bias.zero_()\n \n \n class CLIPEncoder(nn.Module):"
        },
        {
            "sha": "9f14686630ba747818377668b7411d9b164c167e",
            "filename": "src/transformers/models/clipseg/modeling_clipseg.py",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -427,12 +427,13 @@ class CLIPSegPreTrainedModel(PreTrainedModel):\n     input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         factor = self.config.initializer_factor\n         if isinstance(module, CLIPSegTextEmbeddings):\n-            module.token_embedding.weight.data.normal_(mean=0.0, std=factor * 0.02)\n-            module.position_embedding.weight.data.normal_(mean=0.0, std=factor * 0.02)\n+            module.token_embedding.weight.normal_(mean=0.0, std=factor * 0.02)\n+            module.position_embedding.weight.normal_(mean=0.0, std=factor * 0.02)\n         elif isinstance(module, CLIPSegVisionEmbeddings):\n             factor = self.config.initializer_factor\n             nn.init.normal_(module.class_embedding, mean=0.0, std=module.embed_dim**-0.5 * factor)\n@@ -463,10 +464,10 @@ def _init_weights(self, module):\n             )\n \n         if isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n         if isinstance(module, nn.Linear) and module.bias is not None:\n-            module.bias.data.zero_()\n+            module.bias.zero_()\n \n \n # Copied from transformers.models.altclip.modeling_altclip.AltCLIPEncoder with AltCLIP->CLIPSeg"
        },
        {
            "sha": "9893b6bd14429b681b6213385b627141d68e2806",
            "filename": "src/transformers/models/clvp/modeling_clvp.py",
            "status": "modified",
            "additions": 12,
            "deletions": 11,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -781,17 +781,18 @@ class ClvpPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _skip_keys_device_placement = \"past_key_values\"\n \n+    @torch.no_grad()\n     def _init_weights(self, module: nn.Module):\n         \"\"\"Initialize the weights\"\"\"\n         factor = self.config.initializer_factor\n         if isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=factor * 0.02)\n+            module.weight.normal_(mean=0.0, std=factor * 0.02)\n         elif isinstance(module, (nn.Linear, Conv1D, nn.Conv1d)):\n-            module.weight.data.normal_(mean=0.0, std=factor * 0.02)\n+            module.weight.normal_(mean=0.0, std=factor * 0.02)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, ClvpRMSNorm):\n-            module.weight.data.fill_(1.0)\n+            module.weight.fill_(1.0)\n         elif isinstance(module, ClvpEncoderMLP):\n             in_proj_std = (module.config.hidden_size**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n             fc_std = (2 * module.config.hidden_size) ** -0.5 * factor\n@@ -800,22 +801,22 @@ def _init_weights(self, module: nn.Module):\n         elif isinstance(module, ClvpEncoder):\n             config = self.config.get_text_config()\n             factor = config.initializer_factor\n-            module.projection.weight.data.normal_(mean=0.0, std=factor * (config.hidden_size**-0.5))\n+            module.projection.weight.normal_(mean=0.0, std=factor * (config.hidden_size**-0.5))\n         elif isinstance(module, ClvpConditioningEncoder):\n-            module.mel_conv.weight.data.normal_(mean=0.0, std=factor)\n-            module.mel_conv.bias.data.zero_()\n+            module.mel_conv.weight.normal_(mean=0.0, std=factor)\n+            module.mel_conv.bias.zero_()\n         elif isinstance(module, ClvpForCausalLM):\n             for name, p in module.named_parameters():\n                 if name == \"c_proj.weight\":\n-                    p.data.normal_(\n+                    p.normal_(\n                         mean=0.0, std=(self.config.initializer_range / math.sqrt(2 * self.config.num_hidden_layers))\n                     )\n         elif isinstance(module, ClvpModelForConditionalGeneration):\n-            module.logit_scale.data.fill_(self.config.logit_scale_init_value)\n+            module.logit_scale.fill_(self.config.logit_scale_init_value)\n \n         if isinstance(module, (nn.LayerNorm, nn.GroupNorm)):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n \n \n class ClvpEncoder(ClvpPreTrainedModel):"
        },
        {
            "sha": "b5e350d79d1a50438f697cb67ab4a57f8222b26a",
            "filename": "src/transformers/models/codegen/modeling_codegen.py",
            "status": "modified",
            "additions": 8,
            "deletions": 7,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -283,19 +283,20 @@ class CodeGenPreTrainedModel(PreTrainedModel):\n     def __init__(self, *inputs, **kwargs):\n         super().__init__(*inputs, **kwargs)\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights.\"\"\"\n         if isinstance(module, (nn.Linear,)):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n \n \n @auto_docstring\n@@ -560,7 +561,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n     \"\"\"\n )\n class CodeGenForCausalLM(CodeGenPreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"transformer.wte.weight\"}\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "cf73b48989cd48271d26cabab8368a37e246af52",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -466,7 +466,7 @@ def forward(\n \n @auto_docstring\n class CohereForCausalLM(CoherePreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.embed_tokens.weight\"}\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n     _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n "
        },
        {
            "sha": "a9c56cd2491c324f70f9731fdd2a42e17a08e5a1",
            "filename": "src/transformers/models/cohere2/modeling_cohere2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -447,7 +447,7 @@ def forward(\n \n @auto_docstring\n class Cohere2ForCausalLM(Cohere2PreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.embed_tokens.weight\"}\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n     _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n "
        },
        {
            "sha": "f3b6e8a8aff46066eb8265a9f9f6412208bc9185",
            "filename": "src/transformers/models/cohere2_vision/modeling_cohere2_vision.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fmodeling_cohere2_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fmodeling_cohere2_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fmodeling_cohere2_vision.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -129,7 +129,6 @@ class Cohere2VisionCausalLMOutputWithPast(ModelOutput):\n @auto_docstring\n class Cohere2VisionPreTrainedModel(PreTrainedModel):\n     config: Cohere2VisionConfig\n-    base_model_prefix = \"model\"\n     input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n     _skip_keys_device_placement = \"past_key_values\"\n@@ -143,6 +142,7 @@ class Cohere2VisionPreTrainedModel(PreTrainedModel):\n         \"hidden_states\": \"DecoderLayer\",\n         \"attentions\": \"Attention\",\n     }\n+    base_model_prefix = \"model\"\n \n \n @auto_docstring(\n@@ -268,7 +268,7 @@ def forward(\n )\n class Cohere2VisionForConditionalGeneration(Cohere2VisionPreTrainedModel, GenerationMixin):\n     _checkpoint_conversion_mapping = {}\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.language_model.embed_tokens.weight\"}\n \n     def __init__(self, config: Cohere2VisionConfig):\n         super().__init__(config)"
        },
        {
            "sha": "dab4d8651145d5576ac03b48916d3f793efd7b74",
            "filename": "src/transformers/models/colpali/convert_colpali_weights_to_hf.py",
            "status": "modified",
            "additions": 9,
            "deletions": 1,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fcolpali%2Fconvert_colpali_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fcolpali%2Fconvert_colpali_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolpali%2Fconvert_colpali_weights_to_hf.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -144,7 +144,15 @@ def convert_colpali_weights_to_hf(\n \n     # Tie the weights (following ColPali's `__init__`` step)\n     if model.vlm.language_model._tied_weights_keys is not None:\n-        model._tied_weights_keys = [f\"vlm.language_model.{k}\" for k in model.vlm.language_model._tied_weights_keys]\n+        prefix = \"vlm.language_model.\"\n+        prefixed_mapping = {\n+            f\"{prefix}{target}\": f\"{prefix}{source}\"\n+            for target, source in model.vlm.language_model._tied_weights_keys.items()\n+        }\n+        if isinstance(model._tied_weights_keys, dict):\n+            model._tied_weights_keys.update(prefixed_mapping)\n+        else:\n+            model._tied_weights_keys = prefixed_mapping\n \n     # Sanity check: ensure all keys are the same\n     state_dict_keys_old = set(original_state_dict.keys())"
        },
        {
            "sha": "954722e2b1447284c8e0d6f3ef37b418feb4d97a",
            "filename": "src/transformers/models/colpali/modeling_colpali.py",
            "status": "modified",
            "additions": 5,
            "deletions": 8,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodeling_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodeling_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodeling_colpali.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -38,6 +38,7 @@ class ColPaliPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_flex_attn = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         std = (\n             self.config.initializer_range\n@@ -46,13 +47,13 @@ def _init_weights(self, module):\n         )\n \n         if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n \n \n @dataclass\n@@ -113,7 +114,6 @@ def __init__(self, config: ColPaliConfig):\n         self.vocab_size = config.vlm_config.text_config.vocab_size\n \n         self.vlm = AutoModelForImageTextToText.from_config(config.vlm_config)\n-        self._tied_weights_keys = [f\"vlm.language_model.{k}\" for k in (self.vlm._tied_weights_keys or [])]\n \n         self.embedding_dim = self.config.embedding_dim\n         self.embedding_proj_layer = nn.Linear(\n@@ -186,9 +186,6 @@ def get_output_embeddings(self):\n     def set_output_embeddings(self, new_embeddings):\n         self.vlm.set_output_embeddings(new_embeddings)\n \n-    def tie_weights(self):\n-        return self.vlm.tie_weights()\n-\n     def resize_token_embeddings(\n         self,\n         new_num_tokens: Optional[int] = None,"
        },
        {
            "sha": "27b897f70490d6d152999335d435226f18df3ce4",
            "filename": "src/transformers/models/colqwen2/modeling_colqwen2.py",
            "status": "modified",
            "additions": 5,
            "deletions": 8,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodeling_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodeling_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodeling_colqwen2.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -46,6 +46,7 @@ class ColQwen2PreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_flex_attn = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         std = (\n             self.config.initializer_range\n@@ -54,13 +55,13 @@ def _init_weights(self, module):\n         )\n \n         if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n \n \n @dataclass\n@@ -118,7 +119,6 @@ def __init__(self, config: ColQwen2Config):\n             self.config.vlm_config.text_config.hidden_size,\n             self.embedding_dim,\n         )\n-        self._tied_weights_keys = [f\"vlm.{k}\" for k in (self.vlm._tied_weights_keys or [])]\n \n         self.post_init()\n \n@@ -222,9 +222,6 @@ def get_output_embeddings(self):\n     def set_output_embeddings(self, new_embeddings):\n         self.vlm.set_output_embeddings(new_embeddings)\n \n-    def tie_weights(self):\n-        return self.vlm.tie_weights()\n-\n     def resize_token_embeddings(\n         self,\n         new_num_tokens: Optional[int] = None,"
        },
        {
            "sha": "d7474bfd62117f10263f166282ff2aea58016eb8",
            "filename": "src/transformers/models/colqwen2/modular_colqwen2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -304,7 +304,6 @@ class ColQwen2ForRetrieval(ColPaliForRetrieval):\n     def __init__(self, config: ColQwen2Config):\n         super().__init__(config)\n         del self._tied_weights_keys\n-        self._tied_weights_keys = [f\"vlm.{k}\" for k in (self.vlm._tied_weights_keys or [])]\n \n     @can_return_tuple\n     @auto_docstring"
        },
        {
            "sha": "c358dd3c2c82b607856c0fb08e9451321f9a73d5",
            "filename": "src/transformers/models/conditional_detr/modeling_conditional_detr.py",
            "status": "modified",
            "additions": 9,
            "deletions": 8,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodeling_conditional_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodeling_conditional_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodeling_conditional_detr.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -237,10 +237,10 @@ def replace_batch_norm(model):\n             new_module = ConditionalDetrFrozenBatchNorm2d(module.num_features)\n \n             if module.weight.device != torch.device(\"meta\"):\n-                new_module.weight.data.copy_(module.weight)\n-                new_module.bias.data.copy_(module.bias)\n-                new_module.running_mean.data.copy_(module.running_mean)\n-                new_module.running_var.data.copy_(module.running_var)\n+                new_module.weight.copy_(module.weight)\n+                new_module.bias.copy_(module.bias)\n+                new_module.running_mean.copy_(module.running_mean)\n+                new_module.running_var.copy_(module.running_var)\n \n             model._modules[name] = new_module\n \n@@ -970,6 +970,7 @@ class ConditionalDetrPreTrainedModel(PreTrainedModel):\n     input_modalities = \"image\"\n     _no_split_modules = [r\"ConditionalDetrConvEncoder\", r\"ConditionalDetrEncoderLayer\", r\"ConditionalDetrDecoderLayer\"]\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         std = self.config.init_std\n         xavier_std = self.config.init_xavier_std\n@@ -983,13 +984,13 @@ def _init_weights(self, module):\n             nn.init.uniform_(module.row_embeddings.weight)\n             nn.init.uniform_(module.column_embeddings.weight)\n         if isinstance(module, (nn.Linear, nn.Conv2d, nn.BatchNorm2d)):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n \n \n # Copied from transformers.models.detr.modeling_detr.DetrEncoder with Detr->ConditionalDetr,DETR->ConditionalDETR"
        },
        {
            "sha": "392f8ec79a1c4257bfb523bd51414aad917833d0",
            "filename": "src/transformers/models/convbert/modeling_convbert.py",
            "status": "modified",
            "additions": 11,
            "deletions": 10,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fconvbert%2Fmodeling_convbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fconvbert%2Fmodeling_convbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvbert%2Fmodeling_convbert.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -108,24 +108,25 @@ class ConvBertPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"convbert\"\n     supports_gradient_checkpointing = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, (nn.Linear, nn.Conv1d)):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n         elif isinstance(module, SeparableConv1D):\n-            module.bias.data.zero_()\n+            module.bias.zero_()\n         elif isinstance(module, GroupedLinearLayer):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n-            module.bias.data.zero_()\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.bias.zero_()\n \n \n class SeparableConv1D(nn.Module):\n@@ -707,7 +708,7 @@ def forward(self, generator_hidden_states: torch.FloatTensor) -> torch.FloatTens\n \n @auto_docstring\n class ConvBertForMaskedLM(ConvBertPreTrainedModel):\n-    _tied_weights_keys = [\"generator.lm_head.weight\"]\n+    _tied_weights_keys = {\"generator_lm_head.weight\": \"convbert.embeddings.word_embeddings.weight\"}\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "c0cbc8e55476b000edae9d440b4314952131d960",
            "filename": "src/transformers/models/convnext/modeling_convnext.py",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fconvnext%2Fmodeling_convnext.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fconvnext%2Fmodeling_convnext.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnext%2Fmodeling_convnext.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -240,18 +240,19 @@ class ConvNextPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"ConvNextLayer\"]\n     _can_record_outputs = {}  # hidden states are collected explicitly\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, (nn.LayerNorm, ConvNextLayerNorm)):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n         elif isinstance(module, ConvNextLayer):\n             if module.layer_scale_parameter is not None:\n-                module.layer_scale_parameter.data.fill_(self.config.layer_scale_init_value)\n+                module.layer_scale_parameter.fill_(self.config.layer_scale_init_value)\n \n \n @auto_docstring"
        },
        {
            "sha": "de320116bd1682c749a3033e33d999a9506c8fcc",
            "filename": "src/transformers/models/convnextv2/modeling_convnextv2.py",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fconvnextv2%2Fmodeling_convnextv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fconvnextv2%2Fmodeling_convnextv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnextv2%2Fmodeling_convnextv2.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -260,18 +260,19 @@ class ConvNextV2PreTrainedModel(PreTrainedModel):\n     input_modalities = \"image\"\n     _no_split_modules = [\"ConvNextV2Layer\"]\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, (nn.LayerNorm, ConvNextV2LayerNorm)):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n         elif isinstance(module, ConvNextV2GRN):\n-            module.weight.data.zero_()\n-            module.bias.data.zero_()\n+            module.weight.zero_()\n+            module.bias.zero_()\n \n \n @auto_docstring"
        },
        {
            "sha": "9f8ce38b2b08bab896aa1a6d7ef89de06a0c92c4",
            "filename": "src/transformers/models/cpmant/modeling_cpmant.py",
            "status": "modified",
            "additions": 10,
            "deletions": 9,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fcpmant%2Fmodeling_cpmant.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fcpmant%2Fmodeling_cpmant.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcpmant%2Fmodeling_cpmant.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -525,23 +525,24 @@ class CpmAntPreTrainedModel(PreTrainedModel):\n     config: CpmAntConfig\n     base_model_prefix = \"cpmant\"\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=self.config.init_std)\n+            module.weight.normal_(mean=0.0, std=self.config.init_std)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=self.config.init_std)\n+            module.weight.normal_(mean=0.0, std=self.config.init_std)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n         elif isinstance(module, CpmAntLayerNorm):\n-            module.weight.data.fill_(1.0)\n+            module.weight.fill_(1.0)\n         elif isinstance(module, CpmAntSegmentPositionEmbedding):\n-            module.relative_attention_bias.data.normal_(mean=0.0, std=self.config.init_std)\n+            module.relative_attention_bias.normal_(mean=0.0, std=self.config.init_std)\n \n \n @auto_docstring\n@@ -698,7 +699,7 @@ def forward(\n     \"\"\"\n )\n class CpmAntForCausalLM(CpmAntPreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"cpmant.input_embedding.weight\"}\n \n     def __init__(self, config: CpmAntConfig):\n         super().__init__(config)"
        },
        {
            "sha": "7c2e8c67686437566e8e529131aab4bbc4791fef",
            "filename": "src/transformers/models/csm/modeling_csm.py",
            "status": "modified",
            "additions": 5,
            "deletions": 12,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -409,12 +409,13 @@ class CsmPreTrainedModel(PreTrainedModel):\n         \"attentions\": CsmAttention,\n     }\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, CsmCodebooksHead):\n             num_codebooks = module.num_codebooks\n             for i in range(num_codebooks - 1):\n-                module.weight.data[i].normal_(mean=0.0, std=self.config.initializer_range)\n+                module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n \n \n @auto_docstring\n@@ -769,10 +770,9 @@ def forward(\n     \"\"\"\n )\n class CsmForConditionalGeneration(CsmPreTrainedModel, CsmGenerationMixin):\n-    _tied_weights_keys = [\n-        \"backbone_model.embed_tokens.embed_audio_tokens.weight\",\n-        \"depth_decoder.model.embed_tokens.weight\",\n-    ]\n+    _tied_weights_keys = {\n+        \"backbone_model.embed_tokens.embed_audio_tokens.weight\": \"depth_decoder.model.embed_tokens.weight\"\n+    }\n \n     def __init__(self, config):\n         super().__init__(config)\n@@ -790,13 +790,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.backbone_model.embed_tokens = value\n \n-    def _tie_weights(self):\n-        if self.config.tie_codebooks_embeddings:\n-            self._tie_embedding_weights(\n-                self.backbone_model.embed_tokens.embed_audio_tokens,\n-                self.depth_decoder.model.embed_tokens,\n-            )\n-\n     @classmethod\n     def from_pretrained(cls, *args, **kwargs):\n         if kwargs.get(\"output_loading_info\", False):"
        },
        {
            "sha": "d1cb056f64bd666bb56bedbd2e661a17fb569e6a",
            "filename": "src/transformers/models/csm/modular_csm.py",
            "status": "modified",
            "additions": 5,
            "deletions": 12,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -140,12 +140,13 @@ class CsmPreTrainedModel(PreTrainedModel):\n         \"attentions\": CsmAttention,\n     }\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, CsmCodebooksHead):\n             num_codebooks = module.num_codebooks\n             for i in range(num_codebooks - 1):\n-                module.weight.data[i].normal_(mean=0.0, std=self.config.initializer_range)\n+                module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n \n \n @auto_docstring\n@@ -420,10 +421,9 @@ def forward(self, **super_kwargs):\n     \"\"\"\n )\n class CsmForConditionalGeneration(CsmPreTrainedModel, CsmGenerationMixin):\n-    _tied_weights_keys = [\n-        \"backbone_model.embed_tokens.embed_audio_tokens.weight\",\n-        \"depth_decoder.model.embed_tokens.weight\",\n-    ]\n+    _tied_weights_keys = {\n+        \"backbone_model.embed_tokens.embed_audio_tokens.weight\": \"depth_decoder.model.embed_tokens.weight\"\n+    }\n \n     def __init__(self, config):\n         super().__init__(config)\n@@ -441,13 +441,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.backbone_model.embed_tokens = value\n \n-    def _tie_weights(self):\n-        if self.config.tie_codebooks_embeddings:\n-            self._tie_embedding_weights(\n-                self.backbone_model.embed_tokens.embed_audio_tokens,\n-                self.depth_decoder.model.embed_tokens,\n-            )\n-\n     @classmethod\n     def from_pretrained(cls, *args, **kwargs):\n         if kwargs.get(\"output_loading_info\", False):"
        },
        {
            "sha": "f3a5472410cee291eb18a454be3a10b02cab7015",
            "filename": "src/transformers/models/ctrl/modeling_ctrl.py",
            "status": "modified",
            "additions": 8,
            "deletions": 7,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -188,19 +188,20 @@ class CTRLPreTrainedModel(PreTrainedModel):\n     config: CTRLConfig\n     base_model_prefix = \"transformer\"\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights.\"\"\"\n         if isinstance(module, (nn.Linear, Conv1D)):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n \n \n @auto_docstring\n@@ -384,7 +385,7 @@ def forward(\n     \"\"\"\n )\n class CTRLLMHeadModel(CTRLPreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"transformer.w.weight\"}\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "55b251a087e7f51cb6cbecc30a0c82295fe14ed5",
            "filename": "src/transformers/models/cvt/modeling_cvt.py",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fcvt%2Fmodeling_cvt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fcvt%2Fmodeling_cvt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcvt%2Fmodeling_cvt.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -489,19 +489,20 @@ class CvtPreTrainedModel(PreTrainedModel):\n     main_input_name = \"pixel_values\"\n     _no_split_modules = [\"CvtLayer\"]\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.data = nn.init.trunc_normal_(module.weight.data, mean=0.0, std=self.config.initializer_range)\n+            module.weight.copy_(nn.init.trunc_normal_(module.weight, mean=0.0, std=self.config.initializer_range))\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n         elif isinstance(module, CvtStage):\n             if self.config.cls_token[module.stage]:\n-                module.cls_token.data = nn.init.trunc_normal_(\n-                    module.cls_token.data, mean=0.0, std=self.config.initializer_range\n+                module.cls_token.copy_(\n+                    nn.init.trunc_normal_(module.cls_token, mean=0.0, std=self.config.initializer_range)\n                 )\n \n "
        },
        {
            "sha": "df9760ed1ba7f7a459575ebfdd013c7532f90da8",
            "filename": "src/transformers/models/cwm/modeling_cwm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fcwm%2Fmodeling_cwm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fcwm%2Fmodeling_cwm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcwm%2Fmodeling_cwm.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -437,7 +437,7 @@ def forward(\n \n @auto_docstring\n class CwmForCausalLM(CwmPreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.embed_tokens.weight\"}\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n     _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n "
        },
        {
            "sha": "2e426a4f32bb7e38968f16425c79992f44bb767a",
            "filename": "src/transformers/models/d_fine/configuration_d_fine.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fd_fine%2Fconfiguration_d_fine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fd_fine%2Fconfiguration_d_fine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fd_fine%2Fconfiguration_d_fine.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -396,6 +396,7 @@ def __init__(\n                 f\"Embedded dimension {self.d_model} must be divisible by decoder_attention_heads {self.decoder_attention_heads}\"\n             )\n         super().__init__(is_encoder_decoder=is_encoder_decoder, **kwargs)\n+        self.tie_encoder_decoder = True\n \n \n __all__ = [\"DFineConfig\"]"
        },
        {
            "sha": "c1a620dbb75bc81e959d9552c60bd180a6e46bb1",
            "filename": "src/transformers/models/d_fine/modeling_d_fine.py",
            "status": "modified",
            "additions": 19,
            "deletions": 15,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodeling_d_fine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodeling_d_fine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodeling_d_fine.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -444,6 +444,7 @@ class DFinePreTrainedModel(PreTrainedModel):\n     input_modalities = \"image\"\n     _no_split_modules = [r\"DFineHybridEncoder\", r\"DFineDecoderLayer\"]\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         # initialize linear layer bias value according to a given probability value.\n@@ -467,7 +468,7 @@ def _init_weights(self, module):\n                 module.up.fill_(self.config.up)\n \n         if isinstance(module, DFineMultiscaleDeformableAttention):\n-            nn.init.constant_(module.sampling_offsets.weight.data, 0.0)\n+            nn.init.constant_(module.sampling_offsets.weight, 0.0)\n             default_dtype = torch.get_default_dtype()\n             thetas = torch.arange(module.n_heads, dtype=torch.int64).to(default_dtype) * (\n                 2.0 * math.pi / module.n_heads\n@@ -478,10 +479,10 @@ def _init_weights(self, module):\n             scaling = torch.concat([torch.arange(1, n + 1) for n in module.num_points_list]).reshape(1, -1, 1)\n             grid_init *= scaling\n             with torch.no_grad():\n-                module.sampling_offsets.bias.data[...] = grid_init.flatten()\n+                module.sampling_offsets.bias[...] = grid_init.flatten()\n \n-            nn.init.constant_(module.attention_weights.weight.data, 0.0)\n-            nn.init.constant_(module.attention_weights.bias.data, 0.0)\n+            nn.init.constant_(module.attention_weights.weight, 0.0)\n+            nn.init.constant_(module.attention_weights.bias, 0.0)\n \n         if isinstance(module, DFineModel):\n             prior_prob = self.config.initializer_bias_prior_prob or 1 / (self.config.num_labels + 1)\n@@ -490,9 +491,9 @@ def _init_weights(self, module):\n             nn.init.constant_(module.enc_score_head.bias, bias)\n \n         if isinstance(module, (nn.Linear, nn.Conv2d, nn.BatchNorm2d)):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n \n         if isinstance(module, DFineGate):\n             bias = float(-math.log((1 - 0.5) / 0.5))\n@@ -504,8 +505,8 @@ def _init_weights(self, module):\n             init.constant_(module.reg_conf.layers[-1].weight, 0)\n \n         if isinstance(module, nn.LayerNorm):\n-            module.weight.data.fill_(1.0)\n-            module.bias.data.zero_()\n+            module.weight.fill_(1.0)\n+            module.bias.zero_()\n \n         if hasattr(module, \"weight_embedding\") and self.config.learn_initial_query:\n             nn.init.xavier_uniform_(module.weight_embedding.weight)\n@@ -947,10 +948,10 @@ def replace_batch_norm(model):\n             new_module = DFineFrozenBatchNorm2d(module.num_features)\n \n             if module.weight.device != torch.device(\"meta\"):\n-                new_module.weight.data.copy_(module.weight)\n-                new_module.bias.data.copy_(module.bias)\n-                new_module.running_mean.data.copy_(module.running_mean)\n-                new_module.running_var.data.copy_(module.running_var)\n+                new_module.weight.copy_(module.weight)\n+                new_module.bias.copy_(module.bias)\n+                new_module.running_mean.copy_(module.running_mean)\n+                new_module.running_var.copy_(module.running_var)\n \n             model._modules[name] = new_module\n \n@@ -1547,9 +1548,14 @@ class DFineObjectDetectionOutput(ModelOutput):\n )\n class DFineForObjectDetection(DFinePreTrainedModel):\n     # When using clones, all layers > 0 will be clones, but layer 0 *is* required\n-    _tied_weights_keys = [\"bbox_embed\", \"class_embed\"]\n     # We can't initialize the model on meta device as some weights are modified during the initialization\n     _no_split_modules = None\n+    _tied_weights_keys = {\n+        r\"bbox_embed.(?![0])\\d+\": \"bbox_embed.0\",\n+        r\"class_embed.(?![0])\\d+\": \"class_embed.0\",\n+        \"model.decoder.class_embed\": \"class_embed\",\n+        \"model.decoder.bbox_embed\": \"bbox_embed\",\n+    }\n \n     def __init__(self, config: DFineConfig):\n         super().__init__(config)\n@@ -1571,10 +1577,8 @@ def __init__(self, config: DFineConfig):\n             ]\n         )\n \n-        # here self.model.decoder.bbox_embed is null, but not self.bbox_embed\n         self.model.decoder.class_embed = self.class_embed\n         self.model.decoder.bbox_embed = self.bbox_embed\n-\n         # Initialize weights and apply final processing\n         self.post_init()\n "
        },
        {
            "sha": "4ce91d1b98a7336e20c0953ca7328be0da748b04",
            "filename": "src/transformers/models/d_fine/modular_d_fine.py",
            "status": "modified",
            "additions": 21,
            "deletions": 11,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodular_d_fine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodular_d_fine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodular_d_fine.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -415,6 +415,7 @@ def __init__(\n                 f\"Embedded dimension {self.d_model} must be divisible by decoder_attention_heads {self.decoder_attention_heads}\"\n             )\n         super().__init__(is_encoder_decoder=is_encoder_decoder, **kwargs)\n+        self.tie_encoder_decoder = True\n \n \n class DFineMultiscaleDeformableAttention(nn.Module):\n@@ -588,6 +589,7 @@ def forward(\n \n \n class DFinePreTrainedModel(RTDetrPreTrainedModel):\n+    @torch.no_grad()\n     def _init_weights(self, module):\n         # initialize linear layer bias value according to a given probability value.\n         if isinstance(module, (DFineForObjectDetection, DFineDecoder)):\n@@ -610,7 +612,7 @@ def _init_weights(self, module):\n                 module.up.fill_(self.config.up)\n \n         if isinstance(module, DFineMultiscaleDeformableAttention):\n-            nn.init.constant_(module.sampling_offsets.weight.data, 0.0)\n+            nn.init.constant_(module.sampling_offsets.weight, 0.0)\n             default_dtype = torch.get_default_dtype()\n             thetas = torch.arange(module.n_heads, dtype=torch.int64).to(default_dtype) * (\n                 2.0 * math.pi / module.n_heads\n@@ -621,10 +623,10 @@ def _init_weights(self, module):\n             scaling = torch.concat([torch.arange(1, n + 1) for n in module.num_points_list]).reshape(1, -1, 1)\n             grid_init *= scaling\n             with torch.no_grad():\n-                module.sampling_offsets.bias.data[...] = grid_init.flatten()\n+                module.sampling_offsets.bias[...] = grid_init.flatten()\n \n-            nn.init.constant_(module.attention_weights.weight.data, 0.0)\n-            nn.init.constant_(module.attention_weights.bias.data, 0.0)\n+            nn.init.constant_(module.attention_weights.weight, 0.0)\n+            nn.init.constant_(module.attention_weights.bias, 0.0)\n \n         if isinstance(module, DFineModel):\n             prior_prob = self.config.initializer_bias_prior_prob or 1 / (self.config.num_labels + 1)\n@@ -633,9 +635,9 @@ def _init_weights(self, module):\n             nn.init.constant_(module.enc_score_head.bias, bias)\n \n         if isinstance(module, (nn.Linear, nn.Conv2d, nn.BatchNorm2d)):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n \n         if isinstance(module, DFineGate):\n             bias = float(-math.log((1 - 0.5) / 0.5))\n@@ -647,8 +649,8 @@ def _init_weights(self, module):\n             init.constant_(module.reg_conf.layers[-1].weight, 0)\n \n         if isinstance(module, nn.LayerNorm):\n-            module.weight.data.fill_(1.0)\n-            module.bias.data.zero_()\n+            module.weight.fill_(1.0)\n+            module.bias.zero_()\n \n         if hasattr(module, \"weight_embedding\") and self.config.learn_initial_query:\n             nn.init.xavier_uniform_(module.weight_embedding.weight)\n@@ -874,7 +876,17 @@ def __init__(self, config: DFineConfig):\n         self.decoder = DFineDecoder(config)\n \n \n-class DFineForObjectDetection(RTDetrForObjectDetection, DFinePreTrainedModel):\n+class DFineForObjectDetection(RTDetrForObjectDetection):\n+    # When using clones, all layers > 0 will be clones, but layer 0 *is* required\n+    # We can't initialize the model on meta device as some weights are modified during the initialization\n+    _no_split_modules = None\n+    _tied_weights_keys = {\n+        r\"bbox_embed.(?![0])\\d+\": \"bbox_embed.0\",\n+        r\"class_embed.(?![0])\\d+\": \"class_embed.0\",\n+        \"model.decoder.class_embed\": \"class_embed\",\n+        \"model.decoder.bbox_embed\": \"bbox_embed\",\n+    }\n+\n     def __init__(self, config: DFineConfig):\n         DFinePreTrainedModel.__init__(self, config)\n \n@@ -895,10 +907,8 @@ def __init__(self, config: DFineConfig):\n             ]\n         )\n \n-        # here self.model.decoder.bbox_embed is null, but not self.bbox_embed\n         self.model.decoder.class_embed = self.class_embed\n         self.model.decoder.bbox_embed = self.bbox_embed\n-\n         # Initialize weights and apply final processing\n         self.post_init()\n "
        },
        {
            "sha": "80ca3175f6ee22591a8d7e72ca73273898bd07fe",
            "filename": "src/transformers/models/dab_detr/configuration_dab_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fconfiguration_dab_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fconfiguration_dab_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fconfiguration_dab_detr.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -256,6 +256,7 @@ def __init__(\n         self.sine_position_embedding_scale = sine_position_embedding_scale\n         self.initializer_bias_prior_prob = initializer_bias_prior_prob\n         super().__init__(is_encoder_decoder=is_encoder_decoder, **kwargs)\n+        self.tie_encoder_decoder = True  # weights have to be tied for this model\n \n \n __all__ = [\"DabDetrConfig\"]"
        },
        {
            "sha": "f4606ccd04998e11d171784f034d9a29020f66e1",
            "filename": "src/transformers/models/dab_detr/modeling_dab_detr.py",
            "status": "modified",
            "additions": 16,
            "deletions": 19,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fmodeling_dab_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fmodeling_dab_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fmodeling_dab_detr.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -188,10 +188,10 @@ def replace_batch_norm(model):\n             new_module = DabDetrFrozenBatchNorm2d(module.num_features)\n \n             if module.weight.device != torch.device(\"meta\"):\n-                new_module.weight.data.copy_(module.weight)\n-                new_module.bias.data.copy_(module.bias)\n-                new_module.running_mean.data.copy_(module.running_mean)\n-                new_module.running_var.data.copy_(module.running_var)\n+                new_module.weight.copy_(module.weight)\n+                new_module.bias.copy_(module.bias)\n+                new_module.running_mean.copy_(module.running_mean)\n+                new_module.running_var.copy_(module.running_var)\n \n             model._modules[name] = new_module\n \n@@ -815,6 +815,7 @@ class DabDetrPreTrainedModel(PreTrainedModel):\n     input_modalities = \"image\"\n     _no_split_modules = [r\"DabDetrConvEncoder\", r\"DabDetrEncoderLayer\", r\"DabDetrDecoderLayer\"]\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         std = self.config.init_std\n         xavier_std = self.config.init_xavier_std\n@@ -825,24 +826,24 @@ def _init_weights(self, module):\n             nn.init.xavier_uniform_(module.k_linear.weight, gain=xavier_std)\n             nn.init.xavier_uniform_(module.q_linear.weight, gain=xavier_std)\n         if isinstance(module, (nn.Linear, nn.Conv2d, nn.BatchNorm2d)):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.weight.data.fill_(1.0)\n-            module.bias.data.zero_()\n+            module.weight.fill_(1.0)\n+            module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, DabDetrForObjectDetection):\n-            nn.init.constant_(module.bbox_predictor.layers[-1].weight.data, 0)\n-            nn.init.constant_(module.bbox_predictor.layers[-1].bias.data, 0)\n+            nn.init.constant_(module.bbox_predictor.layers[-1].weight, 0)\n+            nn.init.constant_(module.bbox_predictor.layers[-1].bias, 0)\n \n             # init prior_prob setting for focal loss\n             prior_prob = self.config.initializer_bias_prior_prob or 1 / (self.config.num_labels + 1)\n             bias_value = -math.log((1 - prior_prob) / prior_prob)\n-            module.class_embed.bias.data.fill_(bias_value)\n+            module.class_embed.bias.fill_(bias_value)\n         elif isinstance(module, nn.PReLU):\n             module.reset_parameters()\n \n@@ -1429,10 +1430,7 @@ def forward(self, q, k, mask: Optional[Tensor] = None):\n )\n class DabDetrForObjectDetection(DabDetrPreTrainedModel):\n     # When using clones, all layers > 0 will be clones, but layer 0 *is* required\n-    _tied_weights_keys = [\n-        r\"bbox_predictor\\.layers\\.\\d+\\.(weight|bias)\",\n-        r\"model\\.decoder\\.bbox_embed\\.layers\\.\\d+\\.(weight|bias)\",\n-    ]\n+    _tied_weights_keys = {\"model.decoder.bbox_embed\": \"bbox_predictor\"}\n \n     def __init__(self, config: DabDetrConfig):\n         super().__init__(config)\n@@ -1443,12 +1441,11 @@ def __init__(self, config: DabDetrConfig):\n         # DAB-DETR encoder-decoder model\n         self.model = DabDetrModel(config)\n \n-        _bbox_embed = DabDetrMLP(config.hidden_size, config.hidden_size, 4, 3)\n         # Object detection heads\n         self.class_embed = nn.Linear(config.hidden_size, config.num_labels)\n \n         # Default bbox_embed_diff_each_layer is False\n-        self.bbox_predictor = _bbox_embed\n+        self.bbox_predictor = DabDetrMLP(config.hidden_size, config.hidden_size, 4, 3)\n \n         # Default iter_update is True\n         self.model.decoder.bbox_embed = self.bbox_predictor"
        },
        {
            "sha": "54f1d1a32d498ba06d46c671f834604eb65b35a8",
            "filename": "src/transformers/models/dac/modeling_dac.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdac%2Fmodeling_dac.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdac%2Fmodeling_dac.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdac%2Fmodeling_dac.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -477,16 +477,17 @@ class DacPreTrainedModel(PreTrainedAudioTokenizerBase):\n     base_model_prefix = \"dac\"\n     main_input_name = \"input_values\"\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         if isinstance(module, nn.Conv1d):\n             nn.init.trunc_normal_(module.weight, std=0.02)\n             nn.init.constant_(module.bias, 0)\n         elif isinstance(module, Snake1d):\n-            module.alpha.data.fill_(1.0)\n+            module.alpha.fill_(1.0)\n         elif isinstance(module, nn.ConvTranspose1d):\n             module.reset_parameters()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=0.02)\n+            module.weight.normal_(mean=0.0, std=0.02)\n \n     def apply_weight_norm(self):\n         weight_norm = nn.utils.weight_norm"
        },
        {
            "sha": "ac78fb0dea8c8eb12aa9ab624200bb4bb39a6cfc",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_audio.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -480,6 +480,7 @@ class Data2VecAudioPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_flex_attn = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, Data2VecAudioFeatureProjection):\n@@ -489,15 +490,15 @@ def _init_weights(self, module):\n         elif isinstance(module, Data2VecAudioPositionalConvLayer):\n             nn.init.constant_(module.conv.bias, 0)\n         elif isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n \n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, (nn.LayerNorm, nn.GroupNorm)):\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n             if module.weight is not None:\n-                module.weight.data.fill_(1.0)\n+                module.weight.fill_(1.0)\n         elif isinstance(module, nn.Conv1d):\n             nn.init.kaiming_normal_(module.weight)\n "
        },
        {
            "sha": "b7a2a7ed2300113bd0bbd99a072a1776a3f99c35",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_text.py",
            "status": "modified",
            "additions": 15,
            "deletions": 17,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -494,23 +494,24 @@ class Data2VecTextPreTrainedModel(PreTrainedModel):\n         \"cross_attentions\": Data2VecTextCrossAttention,\n     }\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, nn.Linear):\n             # Slightly different from the TF version which uses truncated_normal for initialization\n             # cf https://github.com/pytorch/pytorch/pull/5617\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, nn.LayerNorm):\n             if hasattr(module, \"bias\") and module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n             if hasattr(module, \"weight\") and module.weight is not None:\n-                module.weight.data.fill_(1.0)\n+                module.weight.fill_(1.0)\n \n \n class Data2VecTextEncoder(nn.Module):\n@@ -713,7 +714,6 @@ def __init__(self, config):\n \n         self.decoder = nn.Linear(config.hidden_size, config.vocab_size)\n         self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n-        self.decoder.bias = self.bias\n \n     def forward(self, features, **kwargs):\n         x = self.dense(features)\n@@ -725,14 +725,6 @@ def forward(self, features, **kwargs):\n \n         return x\n \n-    def _tie_weights(self):\n-        # To tie those two weights if they get disconnected (on TPU or when the bias is resized)\n-        # For accelerate compatibility and to not break backward compatibility\n-        if self.decoder.bias.device.type == \"meta\":\n-            self.decoder.bias = self.bias\n-        else:\n-            self.bias = self.decoder.bias\n-\n \n class Data2VecTextClassificationHead(nn.Module):\n     \"\"\"Head for sentence-level classification tasks.\"\"\"\n@@ -762,7 +754,10 @@ def forward(self, features, **kwargs):\n     \"\"\"\n )\n class Data2VecTextForCausalLM(Data2VecTextPreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.decoder.weight\", \"lm_head.decoder.bias\"]\n+    _tied_weights_keys = {\n+        \"lm_head.decoder.weight\": \"data2vec_text.embeddings.word_embeddings.weight\",\n+        \"lm_head.decoder.bias\": \"lm_head.bias\",\n+    }\n \n     def __init__(self, config):\n         super().__init__(config)\n@@ -861,7 +856,10 @@ def forward(\n \n @auto_docstring\n class Data2VecTextForMaskedLM(Data2VecTextPreTrainedModel):\n-    _tied_weights_keys = [\"lm_head.decoder.weight\", \"lm_head.decoder.bias\"]\n+    _tied_weights_keys = {\n+        \"lm_head.decoder.weight\": \"data2vec_text.embeddings.word_embeddings.weight\",\n+        \"lm_head.decoder.bias\": \"lm_head.bias\",\n+    }\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "ce96ea06324efc543add3267d5614e64ac7e02f9",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_vision.py",
            "status": "modified",
            "additions": 13,
            "deletions": 12,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_vision.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -706,31 +706,32 @@ class Data2VecVisionPreTrainedModel(PreTrainedModel):\n     _keys_to_ignore_on_load_unexpected = [r\".*relative_position_index.*\"]\n     _supports_sdpa = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, (nn.Linear, nn.Conv2d, nn.ConvTranspose2d)):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n         elif isinstance(module, Data2VecVisionEmbeddings):\n-            module.cls_token.data.zero_()\n+            module.cls_token.zero_()\n             if module.mask_token is not None:\n-                module.mask_token.data.zero_()\n+                module.mask_token.zero_()\n             if module.position_embeddings is not None:\n-                module.position_embeddings.data.zero_()\n+                module.position_embeddings.zero_()\n         elif isinstance(module, Data2VecVisionRelativePositionBias):\n-            module.relative_position_bias_table.data.zero_()\n+            module.relative_position_bias_table.zero_()\n         elif isinstance(module, Data2VecVisionLayer):\n             if module.lambda_1 is not None:\n-                module.lambda_1.data.fill_(self.config.layer_scale_init_value)\n-                module.lambda_2.data.fill_(self.config.layer_scale_init_value)\n+                module.lambda_1.fill_(self.config.layer_scale_init_value)\n+                module.lambda_2.fill_(self.config.layer_scale_init_value)\n \n \n @auto_docstring"
        },
        {
            "sha": "db850fa2f1d59806aac34a3d04548460ce2782c0",
            "filename": "src/transformers/models/data2vec/modular_data2vec_audio.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodular_data2vec_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodular_data2vec_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodular_data2vec_audio.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -144,6 +144,7 @@ class Data2VecAudioPreTrainedModel(PreTrainedModel, Wav2Vec2PreTrainedModel):\n     _supports_sdpa = True\n     _supports_flex_attn = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, Data2VecAudioFeatureProjection):\n@@ -153,15 +154,15 @@ def _init_weights(self, module):\n         elif isinstance(module, Data2VecAudioPositionalConvLayer):\n             nn.init.constant_(module.conv.bias, 0)\n         elif isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n \n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, (nn.LayerNorm, nn.GroupNorm)):\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n             if module.weight is not None:\n-                module.weight.data.fill_(1.0)\n+                module.weight.fill_(1.0)\n         elif isinstance(module, nn.Conv1d):\n             nn.init.kaiming_normal_(module.weight)\n "
        },
        {
            "sha": "ad0dc81c8e017e0ba7e6ff6902900351cf6d33b8",
            "filename": "src/transformers/models/data2vec/modular_data2vec_text.py",
            "status": "modified",
            "additions": 15,
            "deletions": 8,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodular_data2vec_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodular_data2vec_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodular_data2vec_text.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -81,23 +81,24 @@ class Data2VecTextPreTrainedModel(PreTrainedModel):\n         \"cross_attentions\": Data2VecTextCrossAttention,\n     }\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, nn.Linear):\n             # Slightly different from the TF version which uses truncated_normal for initialization\n             # cf https://github.com/pytorch/pytorch/pull/5617\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, nn.LayerNorm):\n             if hasattr(module, \"bias\") and module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n             if hasattr(module, \"weight\") and module.weight is not None:\n-                module.weight.data.fill_(1.0)\n+                module.weight.fill_(1.0)\n \n \n @auto_docstring\n@@ -119,7 +120,10 @@ class Data2VecTextClassificationHead(RobertaClassificationHead):\n     \"\"\"\n )\n class Data2VecTextForCausalLM(Data2VecTextPreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.decoder.weight\", \"lm_head.decoder.bias\"]\n+    _tied_weights_keys = {\n+        \"lm_head.decoder.weight\": \"data2vec_text.embeddings.word_embeddings.weight\",\n+        \"lm_head.decoder.bias\": \"lm_head.bias\",\n+    }\n \n     def __init__(self, config):\n         super().__init__(config)\n@@ -218,7 +222,10 @@ def forward(\n \n @auto_docstring\n class Data2VecTextForMaskedLM(Data2VecTextPreTrainedModel):\n-    _tied_weights_keys = [\"lm_head.decoder.weight\", \"lm_head.decoder.bias\"]\n+    _tied_weights_keys = {\n+        \"lm_head.decoder.weight\": \"data2vec_text.embeddings.word_embeddings.weight\",\n+        \"lm_head.decoder.bias\": \"lm_head.bias\",\n+    }\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "db212fd6378eb35e8e40c63c74061ac48c13cdfd",
            "filename": "src/transformers/models/dbrx/modeling_dbrx.py",
            "status": "modified",
            "additions": 11,
            "deletions": 10,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -466,24 +466,25 @@ class DbrxPreTrainedModel(PreTrainedModel):\n         \"attentions\": DbrxAttention,\n     }\n \n+    @torch.no_grad()\n     def _init_weights(self, module: nn.Module):\n         std = self.config.initializer_range\n         if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.weight.data.fill_(1.0)\n+            module.weight.fill_(1.0)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, DbrxExpertGLU):\n-            module.w1.data.normal_(mean=0.0, std=std)\n-            module.v1.data.normal_(mean=0.0, std=std)\n-            module.w2.data.normal_(mean=0.0, std=std)\n+            module.w1.normal_(mean=0.0, std=std)\n+            module.v1.normal_(mean=0.0, std=std)\n+            module.w2.normal_(mean=0.0, std=std)\n \n \n @auto_docstring\n@@ -663,7 +664,7 @@ def load_balancing_loss_func(\n \n \n class DbrxForCausalLM(DbrxPreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"transformer.wte.weight\"}\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n     _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n "
        },
        {
            "sha": "c9633e20fe1ef723135ab9b1b7053c1c9b066e86",
            "filename": "src/transformers/models/dbrx/modular_dbrx.py",
            "status": "modified",
            "additions": 11,
            "deletions": 10,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodular_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodular_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodular_dbrx.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -336,24 +336,25 @@ class DbrxPreTrainedModel(PreTrainedModel):\n         \"attentions\": DbrxAttention,\n     }\n \n+    @torch.no_grad()\n     def _init_weights(self, module: nn.Module):\n         std = self.config.initializer_range\n         if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.weight.data.fill_(1.0)\n+            module.weight.fill_(1.0)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, DbrxExpertGLU):\n-            module.w1.data.normal_(mean=0.0, std=std)\n-            module.v1.data.normal_(mean=0.0, std=std)\n-            module.w2.data.normal_(mean=0.0, std=std)\n+            module.w1.normal_(mean=0.0, std=std)\n+            module.v1.normal_(mean=0.0, std=std)\n+            module.w2.normal_(mean=0.0, std=std)\n \n \n @auto_docstring\n@@ -451,7 +452,7 @@ def forward(\n \n \n class DbrxForCausalLM(DbrxPreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"transformer.wte.weight\"}\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n     _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n "
        },
        {
            "sha": "3b2ea9b53724f8e1b5588cb656e876834f300299",
            "filename": "src/transformers/models/deberta/modeling_deberta.py",
            "status": "modified",
            "additions": 18,
            "deletions": 18,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdeberta%2Fmodeling_deberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdeberta%2Fmodeling_deberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeberta%2Fmodeling_deberta.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -614,24 +614,25 @@ class DebertaPreTrainedModel(PreTrainedModel):\n     _keys_to_ignore_on_load_unexpected = [\"position_embeddings\"]\n     supports_gradient_checkpointing = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights.\"\"\"\n         if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, (nn.LayerNorm, DebertaLayerNorm)):\n-            module.weight.data.fill_(1.0)\n-            module.bias.data.zero_()\n+            module.weight.fill_(1.0)\n+            module.bias.zero_()\n         elif isinstance(module, DisentangledSelfAttention):\n-            module.q_bias.data.zero_()\n-            module.v_bias.data.zero_()\n+            module.q_bias.zero_()\n+            module.v_bias.zero_()\n         elif isinstance(module, (LegacyDebertaLMPredictionHead, DebertaLMPredictionHead)):\n-            module.bias.data.zero_()\n+            module.bias.zero_()\n \n \n @auto_docstring\n@@ -761,16 +762,10 @@ def __init__(self, config):\n         self.embedding_size = getattr(config, \"embedding_size\", config.hidden_size)\n         # The output weights are the same as the input embeddings, but there is\n         # an output-only bias for each token.\n-        self.decoder = nn.Linear(self.embedding_size, config.vocab_size, bias=False)\n+        self.decoder = nn.Linear(self.embedding_size, config.vocab_size, bias=True)\n \n         self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n \n-        # Need a link between the two variables so that the bias is correctly resized with `resize_token_embeddings`\n-        self.decoder.bias = self.bias\n-\n-    def _tie_weights(self):\n-        self.decoder.bias = self.bias\n-\n     def forward(self, hidden_states):\n         hidden_states = self.transform(hidden_states)\n         hidden_states = self.decoder(hidden_states)\n@@ -828,7 +823,10 @@ def forward(self, sequence_output, word_embeddings):\n \n @auto_docstring\n class DebertaForMaskedLM(DebertaPreTrainedModel):\n-    _tied_weights_keys = [\"cls.predictions.decoder.weight\", \"cls.predictions.decoder.bias\"]\n+    _tied_weights_keys = {\n+        \"cls.predictions.decoder.bias\": \"cls.predictions.bias\",\n+        \"cls.predictions.decoder.weight\": \"deberta.embeddings.word_embeddings.weight\",\n+    }\n \n     def __init__(self, config):\n         super().__init__(config)\n@@ -837,7 +835,9 @@ def __init__(self, config):\n         if self.legacy:\n             self.cls = LegacyDebertaOnlyMLMHead(config)\n         else:\n-            self._tied_weights_keys = [\"lm_predictions.lm_head.weight\", \"deberta.embeddings.word_embeddings.weight\"]\n+            self._tied_weights_keys = {\n+                \"lm_predictions.lm_head.weight\": \"deberta.embeddings.word_embeddings.weight\",\n+            }\n             self.lm_predictions = DebertaOnlyMLMHead(config)\n \n         # Initialize weights and apply final processing"
        },
        {
            "sha": "791e433e4d2cbe3d75bb61de177ae4f0d154401d",
            "filename": "src/transformers/models/deberta_v2/modeling_deberta_v2.py",
            "status": "modified",
            "additions": 16,
            "deletions": 16,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fmodeling_deberta_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fmodeling_deberta_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fmodeling_deberta_v2.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -693,21 +693,22 @@ class DebertaV2PreTrainedModel(PreTrainedModel):\n     _keys_to_ignore_on_load_unexpected = [\"position_embeddings\"]\n     supports_gradient_checkpointing = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights.\"\"\"\n         if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.weight.data.fill_(1.0)\n-            module.bias.data.zero_()\n+            module.weight.fill_(1.0)\n+            module.bias.zero_()\n         elif isinstance(module, (LegacyDebertaV2LMPredictionHead, DebertaV2LMPredictionHead)):\n-            module.bias.data.zero_()\n+            module.bias.zero_()\n \n \n @auto_docstring\n@@ -839,16 +840,10 @@ def __init__(self, config):\n         self.embedding_size = getattr(config, \"embedding_size\", config.hidden_size)\n         # The output weights are the same as the input embeddings, but there is\n         # an output-only bias for each token.\n-        self.decoder = nn.Linear(self.embedding_size, config.vocab_size, bias=False)\n+        self.decoder = nn.Linear(self.embedding_size, config.vocab_size)\n \n         self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n \n-        # Need a link between the two variables so that the bias is correctly resized with `resize_token_embeddings`\n-        self.decoder.bias = self.bias\n-\n-    def _tie_weights(self):\n-        self.decoder.bias = self.bias\n-\n     def forward(self, hidden_states):\n         hidden_states = self.transform(hidden_states)\n         hidden_states = self.decoder(hidden_states)\n@@ -903,7 +898,10 @@ def forward(self, sequence_output, word_embeddings):\n \n @auto_docstring\n class DebertaV2ForMaskedLM(DebertaV2PreTrainedModel):\n-    _tied_weights_keys = [\"cls.predictions.decoder.weight\", \"cls.predictions.decoder.bias\"]\n+    _tied_weights_keys = {\n+        \"cls.predictions.decoder.bias\": \"cls.predictions.bias\",\n+        \"cls.predictions.decoder.weight\": \"deberta.embeddings.word_embeddings.weight\",\n+    }\n     _keys_to_ignore_on_load_unexpected = [r\"mask_predictions.*\"]\n \n     def __init__(self, config):\n@@ -913,7 +911,9 @@ def __init__(self, config):\n         if self.legacy:\n             self.cls = LegacyDebertaV2OnlyMLMHead(config)\n         else:\n-            self._tied_weights_keys = [\"lm_predictions.lm_head.weight\", \"deberta.embeddings.word_embeddings.weight\"]\n+            self._tied_weights_keys = {\n+                \"lm_predictions.lm_head.weight\": \"deberta.embeddings.word_embeddings.weight\",\n+            }\n             self.lm_predictions = DebertaV2OnlyMLMHead(config)\n         # Initialize weights and apply final processing\n         self.post_init()"
        },
        {
            "sha": "678b21808b1d7a11cf31f5d652f32de236b121ca",
            "filename": "src/transformers/models/decision_transformer/modeling_decision_transformer.py",
            "status": "modified",
            "additions": 19,
            "deletions": 16,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -371,30 +371,32 @@ class DecisionTransformerGPT2PreTrainedModel(PreTrainedModel):\n     def __init__(self, *inputs, **kwargs):\n         super().__init__(*inputs, **kwargs)\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights.\"\"\"\n         if isinstance(module, (nn.Linear, Conv1D)):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n \n         # Reinitialize selected weights subject to the OpenAI GPT-2 Paper Scheme:\n         #   > A modified initialization which accounts for the accumulation on the residual path with model depth. Scale\n         #   > the weights of residual layers at initialization by a factor of 1/√N where N is the # of residual layers.\n         #   >   -- GPT-2 :: https://openai.com/blog/better-language-models/\n         #\n         # Reference (Megatron-LM): https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/model/gpt_model.py\n-        for name, p in module.named_parameters():\n-            if \"c_proj\" in name and \"weight\" in name:\n-                # Special Scaled Initialization --> There are 2 Layer Norms per Transformer Block\n-                p.data.normal_(mean=0.0, std=(self.config.initializer_range / math.sqrt(2 * self.config.n_layer)))\n+        if isinstance(module, PreTrainedModel):\n+            for name, p in module.named_parameters():\n+                if \"c_proj\" in name and \"weight\" in name:\n+                    # Special Scaled Initialization --> There are 2 Layer Norms per Transformer Block\n+                    p.normal_(mean=0.0, std=(self.config.initializer_range / math.sqrt(2 * self.config.n_layer)))\n \n \n class DecisionTransformerGPT2Model(DecisionTransformerGPT2PreTrainedModel):\n@@ -612,19 +614,20 @@ class DecisionTransformerPreTrainedModel(PreTrainedModel):\n     main_input_name = \"states\"\n     supports_gradient_checkpointing = False\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n \n \n @auto_docstring("
        },
        {
            "sha": "f7b81216d332cde34f934613820b17e0e42f7ce0",
            "filename": "src/transformers/models/deepseek_v2/configuration_deepseek_v2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fconfiguration_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fconfiguration_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fconfiguration_deepseek_v2.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -127,8 +127,7 @@ class DeepseekV2Config(PreTrainedConfig):\n         \"layers.*.self_attn.q_b_proj\": \"colwise\",\n         \"layers.*.self_attn.kv_b_proj\": \"colwise\",\n         \"layers.*.self_attn.o_proj\": \"rowwise\",\n-        \"layers.*.mlp.gate_proj\": \"colwise\",\n-        \"layers.*.mlp.up_proj\": \"colwise\",\n+        \"layers.*.mlp.gate_up_proj\": \"colwise\",\n         \"layers.*.mlp.down_proj\": \"rowwise\",\n     }\n     base_model_pp_plan = {"
        },
        {
            "sha": "109ac5c1f6e3d78bcc003ee41fbcbdf14d918c79",
            "filename": "src/transformers/models/deepseek_v2/modeling_deepseek_v2.py",
            "status": "modified",
            "additions": 31,
            "deletions": 23,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -42,37 +42,43 @@\n from .configuration_deepseek_v2 import DeepseekV2Config\n \n \n-class DeepseekV2Experts(nn.ModuleList):\n-    \"\"\"\n-    ModuleList of experts.\n-    \"\"\"\n+class DeepseekV2Experts(nn.Module):\n+    \"\"\"Collection of expert weights stored as 3D tensors.\"\"\"\n \n     def __init__(self, config):\n         super().__init__()\n         self.num_experts = config.n_routed_experts\n-        for _ in range(config.n_routed_experts):\n-            self.append(DeepseekV2MLP(config, intermediate_size=config.moe_intermediate_size))\n+        self.hidden_dim = config.hidden_size\n+        self.intermediate_dim = config.moe_intermediate_size\n+        self.gate_up_proj = nn.Parameter(torch.empty(self.num_experts, 2 * self.intermediate_dim, self.hidden_dim))\n+        self.down_proj = nn.Parameter(torch.empty(self.num_experts, self.hidden_dim, self.intermediate_dim))\n+        self.act_fn = ACT2FN[config.hidden_act]\n \n     def forward(\n-        self, hidden_states: torch.Tensor, top_k_index: torch.Tensor, top_k_weights: torch.Tensor\n+        self,\n+        hidden_states: torch.Tensor,\n+        top_k_index: torch.Tensor,\n+        top_k_weights: torch.Tensor,\n     ) -> torch.Tensor:\n-        \"\"\"\n-        Args:\n-            hidden_states: (batch_size * sequence_length, hidden_dim)\n-            top_k_index: (batch_size * sequence_length, top_k)\n-            top_k_weights: (batch_size * sequence_length, top_k)\n-        Returns:\n-            (batch_size * sequence_length, hidden_dim)\n-        \"\"\"\n         final_hidden_states = torch.zeros_like(hidden_states)\n-        expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=self.num_experts).permute(2, 1, 0)\n+        num_experts = top_k_weights.shape[1]\n+        with torch.no_grad():\n+            expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=num_experts + 1)\n+            expert_mask = expert_mask.permute(2, 1, 0)\n+            expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n \n-        expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n         for expert_idx in expert_hit:\n-            idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))\n-            current_state = hidden_states[None, top_x].reshape(-1, hidden_states.shape[-1])\n-            current_hidden_states = self[expert_idx](current_state) * top_k_weights[top_x, idx, None]\n-            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n+            expert_idx = expert_idx[0]\n+            if expert_idx == num_experts:\n+                continue\n+            _, token_idx = torch.where(expert_mask[expert_idx])\n+            current_state = hidden_states[token_idx]\n+            gate, up = nn.functional.linear(current_state, self.gate_up_proj[expert_idx]).chunk(2, dim=-1)\n+            current_hidden_states = self.act_fn(gate) * up\n+            current_hidden_states = nn.functional.linear(current_hidden_states, self.down_proj[expert_idx])\n+            current_hidden_states = current_hidden_states * top_k_weights[token_idx, expert_idx, None]\n+            final_hidden_states.index_add_(0, token_idx, current_hidden_states.to(final_hidden_states.dtype))\n+\n         return final_hidden_states\n \n \n@@ -111,6 +117,7 @@ def route_tokens_to_experts(self, router_logits):\n             topk_weight, topk_idx = torch.topk(tmp_scores, k=self.top_k, dim=-1, sorted=False)\n \n         topk_weight = topk_weight * self.routed_scaling_factor\n+        topk_weight = torch.zeros_like(router_logits).scatter_(1, topk_idx, topk_weight)\n         return topk_idx, topk_weight\n \n     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n@@ -459,10 +466,11 @@ class DeepseekV2PreTrainedModel(PreTrainedModel):\n         \"attentions\": DeepseekV2Attention,\n     }\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, DeepseekV2Moe):\n-            module.gate.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.gate.weight.normal_(mean=0.0, std=self.config.initializer_range)\n \n \n @auto_docstring\n@@ -546,7 +554,7 @@ def forward(\n \n @auto_docstring\n class DeepseekV2ForCausalLM(DeepseekV2PreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.embed_tokens.weight\"}\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n     _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n "
        },
        {
            "sha": "b6fa08ddd890f0ab1889692140d52f25cdf72ed7",
            "filename": "src/transformers/models/deepseek_v2/modular_deepseek_v2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 7,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodular_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodular_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodular_deepseek_v2.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -142,8 +142,7 @@ class DeepseekV2Config(LlamaConfig):\n         \"layers.*.self_attn.q_b_proj\": \"colwise\",\n         \"layers.*.self_attn.kv_b_proj\": \"colwise\",\n         \"layers.*.self_attn.o_proj\": \"rowwise\",\n-        \"layers.*.mlp.gate_proj\": \"colwise\",\n-        \"layers.*.mlp.up_proj\": \"colwise\",\n+        \"layers.*.mlp.gate_up_proj\": \"colwise\",\n         \"layers.*.mlp.down_proj\": \"rowwise\",\n     }\n \n@@ -224,12 +223,10 @@ def apply_rotary_emb(\n     return xq_out, xk_out\n \n \n-class DeepseekV2Experts(Qwen2MoeExperts, nn.ModuleList):\n+class DeepseekV2Experts(Qwen2MoeExperts):\n     def __init__(self, config):\n-        nn.ModuleList.__init__(self)\n+        super().__init__(config)\n         self.num_experts = config.n_routed_experts\n-        for _ in range(config.n_routed_experts):\n-            self.append(DeepseekV2MLP(config, intermediate_size=config.moe_intermediate_size))\n \n \n class DeepseekV2Moe(nn.Module):\n@@ -267,6 +264,7 @@ def route_tokens_to_experts(self, router_logits):\n             topk_weight, topk_idx = torch.topk(tmp_scores, k=self.top_k, dim=-1, sorted=False)\n \n         topk_weight = topk_weight * self.routed_scaling_factor\n+        topk_weight = torch.zeros_like(router_logits).scatter_(1, topk_idx, topk_weight)\n         return topk_idx, topk_weight\n \n     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n@@ -439,10 +437,11 @@ def __init__(self, config: DeepseekV2Config, layer_idx: int):\n class DeepseekV2PreTrainedModel(LlamaPreTrainedModel):\n     _can_compile_fullgraph = False\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         PreTrainedModel._init_weights(self, module)\n         if isinstance(module, DeepseekV2Moe):\n-            module.gate.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.gate.weight.normal_(mean=0.0, std=self.config.initializer_range)\n \n \n class DeepseekV2Model(LlamaModel):"
        },
        {
            "sha": "e619afd25773e757fe2a2c3c8fd57024085b9464",
            "filename": "src/transformers/models/deepseek_v3/modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 30,
            "deletions": 23,
            "changes": 53,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -149,37 +149,43 @@ def forward(self, hidden_states):\n         return router_logits\n \n \n-class DeepseekV3NaiveMoe(nn.ModuleList):\n-    \"\"\"\n-    ModuleList of experts.\n-    \"\"\"\n+class DeepseekV3NaiveMoe(nn.Module):\n+    \"\"\"Collection of expert weights stored as 3D tensors.\"\"\"\n \n     def __init__(self, config):\n         super().__init__()\n         self.num_experts = config.num_local_experts\n-        for _ in range(self.num_experts):\n-            self.append(DeepseekV3MLP(config, intermediate_size=config.moe_intermediate_size))\n+        self.hidden_dim = config.hidden_size\n+        self.intermediate_dim = config.intermediate_size\n+        self.gate_up_proj = nn.Parameter(torch.empty(self.num_experts, 2 * self.intermediate_dim, self.hidden_dim))\n+        self.down_proj = nn.Parameter(torch.empty(self.num_experts, self.hidden_dim, self.intermediate_dim))\n+        self.act_fn = ACT2FN[config.hidden_act]\n \n     def forward(\n-        self, hidden_states: torch.Tensor, top_k_index: torch.Tensor, top_k_weights: torch.Tensor\n+        self,\n+        hidden_states: torch.Tensor,\n+        top_k_index: torch.Tensor,\n+        top_k_weights: torch.Tensor,\n     ) -> torch.Tensor:\n-        \"\"\"\n-        Args:\n-            hidden_states: (batch_size * sequence_length, hidden_dim)\n-            top_k_index: (batch_size * sequence_length, top_k)\n-            top_k_weights: (batch_size * sequence_length, top_k)\n-        Returns:\n-            (batch_size * sequence_length, hidden_dim)\n-        \"\"\"\n         final_hidden_states = torch.zeros_like(hidden_states)\n-        expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=self.num_experts).permute(2, 1, 0)\n+        num_experts = top_k_weights.shape[1]\n+        with torch.no_grad():\n+            expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=num_experts + 1)\n+            expert_mask = expert_mask.permute(2, 1, 0)\n+            expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n \n-        expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n         for expert_idx in expert_hit:\n-            idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))\n-            current_state = hidden_states[None, top_x].reshape(-1, hidden_states.shape[-1])\n-            current_hidden_states = self[expert_idx](current_state) * top_k_weights[top_x, idx, None]\n-            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n+            expert_idx = expert_idx[0]\n+            if expert_idx == num_experts:\n+                continue\n+            _, token_idx = torch.where(expert_mask[expert_idx])\n+            current_state = hidden_states[token_idx]\n+            gate, up = nn.functional.linear(current_state, self.gate_up_proj[expert_idx]).chunk(2, dim=-1)\n+            current_hidden_states = self.act_fn(gate) * up\n+            current_hidden_states = nn.functional.linear(current_hidden_states, self.down_proj[expert_idx])\n+            current_hidden_states = current_hidden_states * top_k_weights[token_idx, expert_idx, None]\n+            final_hidden_states.index_add_(0, token_idx, current_hidden_states.to(final_hidden_states.dtype))\n+\n         return final_hidden_states\n \n \n@@ -542,10 +548,11 @@ class DeepseekV3PreTrainedModel(PreTrainedModel):\n         \"attentions\": DeepseekV3Attention,\n     }\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, DeepseekV3TopkRouter):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n \n \n @auto_docstring\n@@ -631,7 +638,7 @@ def forward(\n \n @auto_docstring\n class DeepseekV3ForCausalLM(DeepseekV3PreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.embed_tokens.weight\"}\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n     _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n "
        },
        {
            "sha": "5a92d135870d3b7a9ab6b69aa515369b941bf5c3",
            "filename": "src/transformers/models/deepseek_v3/modular_deepseek_v3.py",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodular_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodular_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodular_deepseek_v3.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -102,12 +102,10 @@ def forward(self, hidden_states):\n         return router_logits\n \n \n-class DeepseekV3NaiveMoe(MixtralExperts, nn.ModuleList):\n+class DeepseekV3NaiveMoe(MixtralExperts):\n     def __init__(self, config):\n-        nn.ModuleList.__init__(self)\n+        super().__init__(config)\n         self.num_experts = config.num_local_experts\n-        for _ in range(self.num_experts):\n-            self.append(DeepseekV3MLP(config, intermediate_size=config.moe_intermediate_size))\n \n \n class DeepseekV3MoE(nn.Module):\n@@ -306,10 +304,11 @@ def __init__(self, config: DeepseekV3Config, layer_idx: int):\n class DeepseekV3PreTrainedModel(LlamaPreTrainedModel):\n     _can_compile_fullgraph = False\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         PreTrainedModel._init_weights(self, module)\n         if isinstance(module, DeepseekV3TopkRouter):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n \n \n class DeepseekV3Model(LlamaModel):"
        },
        {
            "sha": "849eb5ef34f021c09eb8072d33a93321b3efad7b",
            "filename": "src/transformers/models/deepseek_vl/modeling_deepseek_vl.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodeling_deepseek_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodeling_deepseek_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodeling_deepseek_vl.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -132,13 +132,14 @@ class DeepseekVLPreTrainedModel(PreTrainedModel):\n     _can_compile_fullgraph = True\n     _supports_param_buffer_assignment = False\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         # Required only for Linear layer in DeepseekVLAligner\n         if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=self.config.text_config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.text_config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n \n \n @auto_docstring\n@@ -243,7 +244,7 @@ def forward(\n \n \n class DeepseekVLForConditionalGeneration(DeepseekVLPreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"model.language_model.embed_tokens.weight\", \"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.language_model.embed_tokens.weight\"}\n     output_modalities = \"text\"\n     _can_compile_fullgraph = True\n "
        },
        {
            "sha": "038ffc4c8c0ac0c3b05d08b53e2b5ee2db3ed05e",
            "filename": "src/transformers/models/deepseek_vl/modular_deepseek_vl.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodular_deepseek_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodular_deepseek_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodular_deepseek_vl.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -134,13 +134,14 @@ def forward(self, vision_encodings: torch.Tensor) -> torch.Tensor:\n class DeepseekVLPreTrainedModel(JanusPreTrainedModel):\n     _no_split_modules = [\"LlamaDecoderLayer\"]\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         # Required only for Linear layer in DeepseekVLAligner\n         if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=self.config.text_config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.text_config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n \n \n @auto_docstring"
        },
        {
            "sha": "17fed96166ce221995fddb452b3fe515cc3654e0",
            "filename": "src/transformers/models/deepseek_vl_hybrid/modeling_deepseek_vl_hybrid.py",
            "status": "modified",
            "additions": 8,
            "deletions": 7,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodeling_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodeling_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodeling_deepseek_vl_hybrid.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -214,21 +214,22 @@ class DeepseekVLHybridPreTrainedModel(PreTrainedModel):\n     _can_compile_fullgraph = True\n     _supports_param_buffer_assignment = False\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=self.config.text_config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.text_config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Conv2d):\n             nn.init.kaiming_normal_(module.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, DeepseekVLHybridLayerNorm):\n-            module.weight.data.fill_(1.0)\n-            module.bias.data.zero_()\n+            module.weight.fill_(1.0)\n+            module.bias.zero_()\n         elif isinstance(module, DeepseekVLHybridModel):\n-            module.high_res_vision_alpha.data.zero_()\n+            module.high_res_vision_alpha.zero_()\n \n \n DEEPSEEK_VL_COMMON_CUSTOM_ARGS = r\"\"\"\n@@ -388,7 +389,7 @@ def get_high_res_image_features(self, pixel_values):\n \n \n class DeepseekVLHybridForConditionalGeneration(DeepseekVLHybridPreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"model.language_model.embed_tokens.weight\", \"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.language_model.embed_tokens.weight\"}\n     output_modalities = \"text\"\n     _can_compile_fullgraph = True\n "
        },
        {
            "sha": "c8f5be1638d404e13fae0c7fc0448b6091a38524",
            "filename": "src/transformers/models/deepseek_vl_hybrid/modular_deepseek_vl_hybrid.py",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodular_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodular_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodular_deepseek_vl_hybrid.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -216,21 +216,22 @@ def forward(\n \n \n class DeepseekVLHybridPreTrainedModel(DeepseekVLPreTrainedModel):\n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=self.config.text_config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.text_config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Conv2d):\n             nn.init.kaiming_normal_(module.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, DeepseekVLHybridLayerNorm):\n-            module.weight.data.fill_(1.0)\n-            module.bias.data.zero_()\n+            module.weight.fill_(1.0)\n+            module.bias.zero_()\n         elif isinstance(module, DeepseekVLHybridModel):\n-            module.high_res_vision_alpha.data.zero_()\n+            module.high_res_vision_alpha.zero_()\n \n \n class DeepseekVLHybridModel(DeepseekVLModel):"
        },
        {
            "sha": "312dac1d4b81efd0bb9fbda8f759f16c308483a5",
            "filename": "src/transformers/models/deformable_detr/configuration_deformable_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fconfiguration_deformable_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fconfiguration_deformable_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fconfiguration_deformable_detr.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -270,6 +270,7 @@ def __init__(\n         self.focal_alpha = focal_alpha\n         self.disable_custom_kernels = disable_custom_kernels\n         super().__init__(is_encoder_decoder=is_encoder_decoder, **kwargs)\n+        self.tie_encoder_decoder = True\n \n \n __all__ = [\"DeformableDetrConfig\"]"
        },
        {
            "sha": "553eb8b7a2b5ae20998634748753a9dfae5cebe4",
            "filename": "src/transformers/models/deformable_detr/modeling_deformable_detr.py",
            "status": "modified",
            "additions": 36,
            "deletions": 42,
            "changes": 78,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodeling_deformable_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodeling_deformable_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodeling_deformable_detr.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -14,7 +14,6 @@\n # limitations under the License.\n \"\"\"PyTorch Deformable DETR model.\"\"\"\n \n-import copy\n import math\n import warnings\n from dataclasses import dataclass\n@@ -234,10 +233,6 @@ class DeformableDetrObjectDetectionOutput(ModelOutput):\n     enc_outputs_coord_logits: Optional[torch.FloatTensor] = None\n \n \n-def _get_clones(module, N):\n-    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n-\n-\n def inverse_sigmoid(x, eps=1e-5):\n     x = x.clamp(min=0, max=1)\n     x1 = x.clamp(min=eps)\n@@ -299,10 +294,10 @@ def replace_batch_norm(model):\n             new_module = DeformableDetrFrozenBatchNorm2d(module.num_features)\n \n             if module.weight.device != torch.device(\"meta\"):\n-                new_module.weight.data.copy_(module.weight)\n-                new_module.bias.data.copy_(module.bias)\n-                new_module.running_mean.data.copy_(module.running_mean)\n-                new_module.running_var.data.copy_(module.running_var)\n+                new_module.weight.copy_(module.weight)\n+                new_module.bias.copy_(module.bias)\n+                new_module.running_mean.copy_(module.running_mean)\n+                new_module.running_var.copy_(module.running_var)\n \n             model._modules[name] = new_module\n \n@@ -931,14 +926,15 @@ class DeformableDetrPreTrainedModel(PreTrainedModel):\n         r\"DeformableDetrDecoderLayer\",\n     ]\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         std = self.config.init_std\n \n         if isinstance(module, DeformableDetrLearnedPositionEmbedding):\n             nn.init.uniform_(module.row_embeddings.weight)\n             nn.init.uniform_(module.column_embeddings.weight)\n         elif isinstance(module, DeformableDetrMultiscaleDeformableAttention):\n-            nn.init.constant_(module.sampling_offsets.weight.data, 0.0)\n+            nn.init.constant_(module.sampling_offsets.weight, 0.0)\n             default_dtype = torch.get_default_dtype()\n             thetas = torch.arange(module.n_heads, dtype=torch.int64).to(default_dtype) * (\n                 2.0 * math.pi / module.n_heads\n@@ -953,23 +949,23 @@ def _init_weights(self, module):\n                 grid_init[:, :, i, :] *= i + 1\n             with torch.no_grad():\n                 module.sampling_offsets.bias = nn.Parameter(grid_init.view(-1))\n-            nn.init.constant_(module.attention_weights.weight.data, 0.0)\n-            nn.init.constant_(module.attention_weights.bias.data, 0.0)\n-            nn.init.xavier_uniform_(module.value_proj.weight.data)\n-            nn.init.constant_(module.value_proj.bias.data, 0.0)\n-            nn.init.xavier_uniform_(module.output_proj.weight.data)\n-            nn.init.constant_(module.output_proj.bias.data, 0.0)\n+            nn.init.constant_(module.attention_weights.weight, 0.0)\n+            nn.init.constant_(module.attention_weights.bias, 0.0)\n+            nn.init.xavier_uniform_(module.value_proj.weight)\n+            nn.init.constant_(module.value_proj.bias, 0.0)\n+            nn.init.xavier_uniform_(module.output_proj.weight)\n+            nn.init.constant_(module.output_proj.bias, 0.0)\n         elif isinstance(module, (nn.Linear, nn.Conv2d, nn.BatchNorm2d)):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         if hasattr(module, \"reference_points\") and not self.config.two_stage:\n-            nn.init.xavier_uniform_(module.reference_points.weight.data, gain=1.0)\n-            nn.init.constant_(module.reference_points.bias.data, 0.0)\n+            nn.init.xavier_uniform_(module.reference_points.weight, gain=1.0)\n+            nn.init.constant_(module.reference_points.bias, 0.0)\n         if hasattr(module, \"level_embed\"):\n             nn.init.normal_(module.level_embed)\n \n@@ -1703,40 +1699,38 @@ def forward(self, x):\n )\n class DeformableDetrForObjectDetection(DeformableDetrPreTrainedModel):\n     # When using clones, all layers > 0 will be clones, but layer 0 *is* required\n-    _tied_weights_keys = [r\"bbox_embed\\.[1-9]\\d*\", r\"class_embed\\.[1-9]\\d*\"]\n     # We can't initialize the model on meta device as some weights are modified during the initialization\n     _no_split_modules = None\n+    _tied_weights_keys = {\n+        r\"bbox_embed.(?![0])\\d+\": \"bbox_embed.0\",\n+        r\"class_embed.(?![0])\\d+\": \"class_embed.0\",\n+    }\n \n     def __init__(self, config: DeformableDetrConfig):\n         super().__init__(config)\n-\n         # Deformable DETR encoder-decoder model\n         self.model = DeformableDetrModel(config)\n         # Detection heads on top\n-        self.class_embed = nn.Linear(config.d_model, config.num_labels)\n-        self.bbox_embed = DeformableDetrMLPPredictionHead(\n-            input_dim=config.d_model,\n-            hidden_dim=config.d_model,\n-            output_dim=4,\n-            num_layers=3,\n-        )\n-\n         # if two-stage, the last class_embed and bbox_embed is for region proposal generation\n         num_pred = (config.decoder_layers + 1) if config.two_stage else config.decoder_layers\n+        self.class_embed = nn.ModuleList([nn.Linear(config.d_model, config.num_labels) for _ in range(num_pred)])\n+        self.bbox_embed = nn.ModuleList(\n+            [\n+                DeformableDetrMLPPredictionHead(\n+                    input_dim=config.d_model,\n+                    hidden_dim=config.d_model,\n+                    output_dim=4,\n+                    num_layers=3,\n+                )\n+                for _ in range(num_pred)\n+            ]\n+        )\n         if config.with_box_refine:\n-            self.class_embed = _get_clones(self.class_embed, num_pred)\n-            self.bbox_embed = _get_clones(self.bbox_embed, num_pred)\n-            # hack implementation for iterative bounding box refinement\n             self.model.decoder.bbox_embed = self.bbox_embed\n-        else:\n-            self.class_embed = nn.ModuleList([self.class_embed for _ in range(num_pred)])\n-            self.bbox_embed = nn.ModuleList([self.bbox_embed for _ in range(num_pred)])\n-            self.model.decoder.bbox_embed = None\n+            self._tied_weights_keys[\"model.decoder.bbox_embed\"] = \"bbox_embed\"\n         if config.two_stage:\n-            # hack implementation for two-stage\n             self.model.decoder.class_embed = self.class_embed\n-\n-        # Initialize weights and apply final processing\n+            self._tied_weights_keys[\"model.decoder.class_embed\"] = \"class_embed\"\n         self.post_init()\n \n     @auto_docstring"
        },
        {
            "sha": "b80a02d83a1461f8f452d0bd19868496bdf4dd94",
            "filename": "src/transformers/models/deit/modeling_deit.py",
            "status": "modified",
            "additions": 13,
            "deletions": 10,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_deit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_deit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_deit.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -366,25 +366,28 @@ class DeiTPreTrainedModel(PreTrainedModel):\n         \"attentions\": DeiTSelfAttention,\n     }\n \n+    @torch.no_grad()\n     def _init_weights(self, module: Union[nn.Linear, nn.Conv2d, nn.LayerNorm]) -> None:\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, (nn.Linear, nn.Conv2d)):\n             # Upcast the input in `fp32` and cast it back to desired `dtype` to avoid\n             # `trunc_normal_cpu` not implemented in `half` issues\n-            module.weight.data = nn.init.trunc_normal_(\n-                module.weight.data.to(torch.float32), mean=0.0, std=self.config.initializer_range\n-            ).to(module.weight.dtype)\n+            module.weight.copy_(\n+                nn.init.trunc_normal_(module.weight.to(torch.float32), mean=0.0, std=self.config.initializer_range).to(\n+                    module.weight.dtype\n+                )\n+            )\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n         elif isinstance(module, DeiTEmbeddings):\n-            module.cls_token.data.zero_()\n-            module.position_embeddings.data.zero_()\n-            module.distillation_token.data.zero_()\n+            module.cls_token.zero_()\n+            module.position_embeddings.zero_()\n+            module.distillation_token.zero_()\n             if module.mask_token is not None:\n-                module.mask_token.data.zero_()\n+                module.mask_token.zero_()\n \n \n @auto_docstring"
        },
        {
            "sha": "d7336d304a7667dc5c6d128a0d1163ac1bed603a",
            "filename": "src/transformers/models/deprecated/deta/modeling_deta.py",
            "status": "modified",
            "additions": 18,
            "deletions": 8,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fmodeling_deta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fmodeling_deta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fmodeling_deta.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -988,6 +988,7 @@ class DetaPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [r\"DetaBackboneWithPositionalEncodings\", r\"DetaEncoderLayer\", r\"DetaDecoderLayer\"]\n     supports_gradient_checkpointing = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         std = self.config.init_std\n \n@@ -997,16 +998,16 @@ def _init_weights(self, module):\n         elif isinstance(module, DetaMultiscaleDeformableAttention):\n             module._reset_parameters()\n         elif isinstance(module, (nn.Linear, nn.Conv2d, nn.BatchNorm2d)):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         if hasattr(module, \"reference_points\") and not self.config.two_stage:\n-            nn.init.xavier_uniform_(module.reference_points.weight.data, gain=1.0)\n-            nn.init.constant_(module.reference_points.bias.data, 0.0)\n+            nn.init.xavier_uniform_(module.reference_points.weight, gain=1.0)\n+            nn.init.constant_(module.reference_points.bias, 0.0)\n         if hasattr(module, \"level_embed\"):\n             nn.init.normal_(module.level_embed)\n \n@@ -1793,13 +1794,12 @@ def forward(\n )\n class DetaForObjectDetection(DetaPreTrainedModel):\n     # When using clones, all layers > 0 will be clones, but layer 0 *is* required\n-    _tied_weights_keys = [r\"bbox_embed\\.\\d+\", r\"class_embed\\.\\d+\"]\n     # We can't initialize the model on meta device as some weights are modified during the initialization\n     _no_split_modules = None\n \n     def __init__(self, config: DetaConfig):\n         super().__init__(config)\n-\n+        self._tied_weights_keys = {}\n         # Deformable DETR encoder-decoder model\n         self.model = DetaModel(config)\n \n@@ -1823,6 +1823,11 @@ def __init__(self, config: DetaConfig):\n             nn.init.constant_(self.bbox_embed[0].layers[-1].bias.data[2:], -2.0)\n             # hack implementation for iterative bounding box refinement\n             self.model.decoder.bbox_embed = self.bbox_embed\n+            self._tied_weights_keys.update(\n+                {\n+                    \"model.decoder.bbox_embed \": \"bbox_embed\",\n+                }\n+            )\n         else:\n             nn.init.constant_(self.bbox_embed.layers[-1].bias.data[2:], -2.0)\n             self.class_embed = nn.ModuleList([self.class_embed for _ in range(num_pred)])\n@@ -1831,6 +1836,11 @@ def __init__(self, config: DetaConfig):\n         if config.two_stage:\n             # hack implementation for two-stage\n             self.model.decoder.class_embed = self.class_embed\n+            self._tied_weights_keys.update(\n+                {\n+                    \"model.decoder.class_embed \": \"class_embed\",\n+                }\n+            )\n             for box_embed in self.bbox_embed:\n                 nn.init.constant_(box_embed.layers[-1].bias.data[2:], 0.0)\n "
        },
        {
            "sha": "f3303da0f6fd5b5bd33db565446f37a3a23edc30",
            "filename": "src/transformers/models/deprecated/efficientformer/modeling_efficientformer.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fmodeling_efficientformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fmodeling_efficientformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fmodeling_efficientformer.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -498,15 +498,16 @@ class EfficientFormerPreTrainedModel(PreTrainedModel):\n     main_input_name = \"pixel_values\"\n     supports_gradient_checkpointing = False\n \n+    @torch.no_grad()\n     def _init_weights(self, module: nn.Module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n \n \n EFFICIENTFORMER_START_DOCSTRING = r\"\"\""
        },
        {
            "sha": "7ed73c5a49a88e6bdc0c885091436159620b3261",
            "filename": "src/transformers/models/deprecated/ernie_m/modeling_ernie_m.py",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fernie_m%2Fmodeling_ernie_m.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fernie_m%2Fmodeling_ernie_m.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fernie_m%2Fmodeling_ernie_m.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -368,19 +368,20 @@ class ErnieMPreTrainedModel(PreTrainedModel):\n     config: ErnieMConfig\n     base_model_prefix = \"ernie_m\"\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n \n \n ERNIE_M_START_DOCSTRING = r\"\"\""
        },
        {
            "sha": "eaeb2eb035b16bcd2c4ccd0396b897e7c51991e8",
            "filename": "src/transformers/models/deprecated/gptsan_japanese/modeling_gptsan_japanese.py",
            "status": "modified",
            "additions": 23,
            "deletions": 22,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fmodeling_gptsan_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fmodeling_gptsan_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fmodeling_gptsan_japanese.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -528,60 +528,61 @@ def dummy_inputs(self):\n         }\n         return dummy_inputs\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         factor = self.config.initializer_factor  # Used for testing weights initialization\n         if isinstance(module, nn.LayerNorm):\n-            module.weight.data.fill_(factor * 1.0)\n-            module.bias.data.zero_()\n+            module.weight.fill_(factor * 1.0)\n+            module.bias.zero_()\n         elif isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=factor * ((self.config.d_model) ** -0.5))\n+            module.weight.normal_(mean=0.0, std=factor * ((self.config.d_model) ** -0.5))\n             if hasattr(module, \"bias\") and module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=factor * 1.0)\n+            module.weight.normal_(mean=0.0, std=factor * 1.0)\n         elif isinstance(module, GPTSanJapaneseModel):\n             # Mesh TensorFlow embeddings initialization\n             # See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L1624\n-            module.embed_tokens.weight.data.normal_(mean=0.0, std=factor * 1.0)\n-            module.position_embeddings.weight.data.normal_(mean=0.0, std=factor * 1.0)\n+            module.embed_tokens.weight.normal_(mean=0.0, std=factor * 1.0)\n+            module.position_embeddings.weight.normal_(mean=0.0, std=factor * 1.0)\n             if hasattr(module, \"extra_position_embeddings\") and module.extra_position_embeddings is not None:\n-                module.extra_position_embeddings.weight.data.normal_(mean=0.0, std=factor * 1.0)\n+                module.extra_position_embeddings.weight.normal_(mean=0.0, std=factor * 1.0)\n         elif isinstance(module, (GPTSanJapaneseModel, GPTSanJapaneseForConditionalGeneration)):\n             # Mesh TensorFlow embeddings initialization\n             # See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L1624\n-            module.final_logits_bias.data.normal_(mean=0.0, std=factor * 1.0)\n+            module.final_logits_bias.normal_(mean=0.0, std=factor * 1.0)\n             if hasattr(module, \"lm_head\") and not self.config.tie_word_embeddings:\n-                module.lm_head.weight.data.normal_(mean=0.0, std=factor * 1.0)\n+                module.lm_head.weight.normal_(mean=0.0, std=factor * 1.0)\n         elif isinstance(module, GPTSanJapaneseDenseActDense):\n             # Mesh TensorFlow FF initialization\n             # See https://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/transformer/transformer_layers.py#L56\n             # and https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L89\n-            module.wi.weight.data.normal_(mean=0.0, std=factor * ((self.config.d_model) ** -0.5))\n+            module.wi.weight.normal_(mean=0.0, std=factor * ((self.config.d_model) ** -0.5))\n             if hasattr(module.wi, \"bias\") and module.wi.bias is not None:\n-                module.wi.bias.data.zero_()\n-            module.wo.weight.data.normal_(mean=0.0, std=factor * ((self.config.d_ff) ** -0.5))\n+                module.wi.bias.zero_()\n+            module.wo.weight.normal_(mean=0.0, std=factor * ((self.config.d_ff) ** -0.5))\n             if hasattr(module.wo, \"bias\") and module.wo.bias is not None:\n-                module.wo.bias.data.zero_()\n+                module.wo.bias.zero_()\n         elif isinstance(module, GPTSanJapaneseAttention):\n             # Multi-headed attention\n             d_model = self.config.d_model\n             key_value_proj_dim = self.config.d_model\n             n_heads = self.config.num_heads\n-            module.k_proj.weight.data.normal_(mean=0.0, std=factor * ((d_model * key_value_proj_dim) ** -0.5))\n-            module.v_proj.weight.data.normal_(mean=0.0, std=factor * ((d_model * key_value_proj_dim) ** -0.5))\n-            module.q_proj.weight.data.normal_(mean=0.0, std=factor * ((d_model * key_value_proj_dim) ** -0.5))\n-            module.out_proj.weight.data.normal_(mean=0.0, std=factor * ((n_heads * key_value_proj_dim) ** -0.5))\n+            module.k_proj.weight.normal_(mean=0.0, std=factor * ((d_model * key_value_proj_dim) ** -0.5))\n+            module.v_proj.weight.normal_(mean=0.0, std=factor * ((d_model * key_value_proj_dim) ** -0.5))\n+            module.q_proj.weight.normal_(mean=0.0, std=factor * ((d_model * key_value_proj_dim) ** -0.5))\n+            module.out_proj.weight.normal_(mean=0.0, std=factor * ((n_heads * key_value_proj_dim) ** -0.5))\n         elif isinstance(module, GPTSanJapaneseSparseMLP):\n             # Mesh TensorFlow attention initialization to avoid scaling before softmax\n             # See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/attention.py#L136\n             d_model = self.config.d_model\n             key_value_proj_dim = self.config.d_model\n             n_heads = self.config.num_heads\n-            module.router.classifier.weight.data.normal_(mean=0.0, std=factor * 1)\n+            module.router.classifier.weight.normal_(mean=0.0, std=factor * 1)\n             for idx in range(self.config.num_experts):\n-                module.experts[f\"expert_{idx}\"].wi.weight.data.normal_(mean=0.0, std=factor * (d_model**-0.5))\n-                module.experts[f\"expert_{idx}\"].wo.weight.data.normal_(mean=0.0, std=factor * (d_model**-0.5))\n+                module.experts[f\"expert_{idx}\"].wi.weight.normal_(mean=0.0, std=factor * (d_model**-0.5))\n+                module.experts[f\"expert_{idx}\"].wo.weight.normal_(mean=0.0, std=factor * (d_model**-0.5))\n \n     def _shift_right(self, input_ids):\n         decoder_start_token_id = self.config.decoder_start_token_id\n@@ -852,7 +853,7 @@ def load_balancing_loss_func(router_probs: torch.Tensor, expert_indices: torch.T\n \n \n class GPTSanJapaneseForConditionalGeneration(GPTSanJapanesePreTrainedModel):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.embed_tokens.weight\"}\n \n     def __init__(self, config: GPTSanJapaneseConfig):\n         super().__init__(config)"
        },
        {
            "sha": "bc74d7a5e7d5cc86246f2f9bfe28dad9a3428f14",
            "filename": "src/transformers/models/deprecated/graphormer/modeling_graphormer.py",
            "status": "modified",
            "additions": 13,
            "deletions": 12,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgraphormer%2Fmodeling_graphormer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgraphormer%2Fmodeling_graphormer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgraphormer%2Fmodeling_graphormer.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -721,7 +721,7 @@ def init_graphormer_params(self, module: Union[nn.Linear, nn.Embedding, Graphorm\n         if isinstance(module, nn.Linear):\n             self.normal_(module.weight.data)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         if isinstance(module, nn.Embedding):\n             self.normal_(module.weight.data)\n             if module.padding_idx is not None:\n@@ -731,6 +731,7 @@ def init_graphormer_params(self, module: Union[nn.Linear, nn.Embedding, Graphorm\n             self.normal_(module.k_proj.weight.data)\n             self.normal_(module.v_proj.weight.data)\n \n+    @torch.no_grad()\n     def _init_weights(\n         self,\n         module: Union[\n@@ -742,28 +743,28 @@ def _init_weights(\n         \"\"\"\n         if isinstance(module, (nn.Linear, nn.Conv2d)):\n             # We might be missing part of the Linear init, dependent on the layer num\n-            module.weight.data.normal_(mean=0.0, std=0.02)\n+            module.weight.normal_(mean=0.0, std=0.02)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=0.02)\n+            module.weight.normal_(mean=0.0, std=0.02)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, GraphormerMultiheadAttention):\n-            module.q_proj.weight.data.normal_(mean=0.0, std=0.02)\n-            module.k_proj.weight.data.normal_(mean=0.0, std=0.02)\n-            module.v_proj.weight.data.normal_(mean=0.0, std=0.02)\n+            module.q_proj.weight.normal_(mean=0.0, std=0.02)\n+            module.k_proj.weight.normal_(mean=0.0, std=0.02)\n+            module.v_proj.weight.normal_(mean=0.0, std=0.02)\n             module.reset_parameters()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n         elif isinstance(module, GraphormerGraphEncoder):\n             if module.apply_graphormer_init:\n                 module.apply(self.init_graphormer_params)\n \n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n \n \n class GraphormerModel(GraphormerPreTrainedModel):"
        },
        {
            "sha": "d71fadd8bf6cc952d1771213a3fee1982a2ecdd7",
            "filename": "src/transformers/models/deprecated/jukebox/modeling_jukebox.py",
            "status": "modified",
            "additions": 23,
            "deletions": 20,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Fmodeling_jukebox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Fmodeling_jukebox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Fmodeling_jukebox.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -601,22 +601,23 @@ class JukeboxVQVAE(PreTrainedModel):\n     config: JukeboxVQVAEConfig\n     base_model_prefix = \"vqvae\"\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         if isinstance(module, nn.Embedding):  # embed_tokens\n-            module.weight.data.normal_(mean=0.0, std=0.02 * self.config.init_scale)\n+            module.weight.normal_(mean=0.0, std=0.02 * self.config.init_scale)\n         elif isinstance(module, JukeboxConv1D):\n             if self.config.zero_out:\n-                module.weight.data.zero_()\n+                module.weight.zero_()\n             else:\n-                module.weight.data.normal_(mean=0.0, std=0.02 * self.config.init_scale)\n+                module.weight.normal_(mean=0.0, std=0.02 * self.config.init_scale)\n         elif isinstance(module, JukeboxResConv1DBlock) and self.config.zero_out:\n-            module.conv1d_2.weight.data.zero_()\n-            module.conv1d_2.bias.data.zero_()\n+            module.conv1d_2.weight.zero_()\n+            module.conv1d_2.bias.zero_()\n         if isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n         if isinstance(module, nn.Linear) and module.bias is not None:\n-            module.bias.data.zero_()\n+            module.bias.zero_()\n \n     def __init__(self, config: JukeboxVQVAEConfig):\n         super().__init__(config)\n@@ -1790,32 +1791,33 @@ class JukeboxPrior(PreTrainedModel):\n \n     config: JukeboxPriorConfig\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         init_scale = self.config.init_scale\n \n         if isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=0.02 * init_scale)\n+            module.weight.normal_(mean=0.0, std=0.02 * init_scale)\n         elif isinstance(module, JukeboxConv1D):\n             if self.config.zero_out:\n-                module.weight.data.zero_()\n+                module.weight.zero_()\n             else:\n-                module.weight.data.normal_(mean=0.0, std=0.02 * init_scale)\n+                module.weight.normal_(mean=0.0, std=0.02 * init_scale)\n         elif isinstance(module, JukeboxPositionalEmbedding):\n-            module.pos_emb.data.normal_(mean=0.0, std=0.01 * init_scale)\n+            module.pos_emb.normal_(mean=0.0, std=0.01 * init_scale)\n         elif isinstance(module, JukeboxRangeEmbedding):\n-            module.emb.weight.data.normal_(mean=0.0, std=0.01 * init_scale)\n+            module.emb.weight.normal_(mean=0.0, std=0.01 * init_scale)\n         elif isinstance(module, JukeboxConditionalAutoregressive) and hasattr(module, \"lm_head\"):\n-            module.lm_head.weight.data.normal_(mean=0.0, std=0.02 * init_scale)\n+            module.lm_head.weight.normal_(mean=0.0, std=0.02 * init_scale)\n         elif isinstance(module, JukeboxConditionalAutoregressive) and hasattr(module, \"start_token\"):\n-            module.start_token.data.normal_(mean=0.0, std=0.01 * init_scale)\n+            module.start_token.normal_(mean=0.0, std=0.01 * init_scale)\n         elif isinstance(module, JukeboxResConv1DBlock) and self.config.zero_out:\n-            module.conv1d_2.weight.data.zero_()\n-            module.conv1d_2.bias.data.zero_()\n+            module.conv1d_2.weight.zero_()\n+            module.conv1d_2.bias.zero_()\n         if isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n         if isinstance(module, nn.Linear) and module.bias is not None:\n-            module.bias.data.zero_()\n+            module.bias.zero_()\n \n     def __init__(self, config: JukeboxPriorConfig, level=None, nb_priors=3, vqvae_encoder=None, vqvae_decoder=None):\n         super().__init__(config)\n@@ -2268,6 +2270,7 @@ class JukeboxPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"jukebox\"\n     supports_gradient_checkpointing = False\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         if isinstance(module, (JukeboxPrior, JukeboxVQVAE)):\n             module.apply(module._init_weights)"
        },
        {
            "sha": "db7c475dabd4dc98d7757035837326165c946226",
            "filename": "src/transformers/models/deprecated/mctct/modeling_mctct.py",
            "status": "modified",
            "additions": 11,
            "deletions": 10,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Fmodeling_mctct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Fmodeling_mctct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Fmodeling_mctct.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -392,27 +392,28 @@ class MCTCTPreTrainedModel(PreTrainedModel):\n     main_input_name = \"input_features\"\n     supports_gradient_checkpointing = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         std = self.config.initializer_range\n         if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n         elif isinstance(module, MCTCTLayerNorm):\n-            module.singleton_weight.data.fill_(1.0)\n-            module.singleton_bias.data.zero_()\n+            module.singleton_weight.fill_(1.0)\n+            module.singleton_bias.zero_()\n         if isinstance(module, (nn.Linear, nn.Conv1d)):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n \n     def _get_feat_extract_output_lengths(self, input_lengths: torch.LongTensor):\n         \"\"\""
        },
        {
            "sha": "d66848e1d2b194c52b6b2ae414ba966619ecfd27",
            "filename": "src/transformers/models/deprecated/mega/modeling_mega.py",
            "status": "modified",
            "additions": 9,
            "deletions": 8,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmega%2Fmodeling_mega.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmega%2Fmodeling_mega.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmega%2Fmodeling_mega.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -1332,6 +1332,7 @@ class MegaPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = False\n     _no_split_modules = [\"MegaMovingAverageGatedAttention\"]\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, MegaMultiDimensionDampedEma):\n@@ -1365,16 +1366,16 @@ def _init_weights(self, module):\n             nn.init.constant_(module.qk_bias, 0.0)\n         elif isinstance(module, nn.Linear):\n             # initializes all linear layers in the entire network\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n \n \n MEGA_START_DOCSTRING = r\"\"\"\n@@ -1638,7 +1639,7 @@ def forward(\n     \"\"\"MEGA Model with a `language modeling` head on top for CLM fine-tuning.\"\"\", MEGA_START_DOCSTRING\n )\n class MegaForCausalLM(MegaPreTrainedModel):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"mega.embedding_layer.word_embeddings.weight\"}\n \n     def __init__(self, config: MegaConfig):\n         super().__init__(config)\n@@ -1785,7 +1786,7 @@ def prepare_inputs_for_generation(self, input_ids, past_key_values=None, attenti\n \n @add_start_docstrings(\"\"\"MEGA Model with a `language modeling` head on top.\"\"\", MEGA_START_DOCSTRING)\n class MegaForMaskedLM(MegaPreTrainedModel):\n-    _tied_weights_keys = [\"mlm_head.weight\"]\n+    _tied_weights_keys = {\"mlm_head.weight\": \"mega.embedding_layer.word_embeddings.weight\"}\n \n     def __init__(self, config: MegaConfig):\n         super().__init__(config)"
        },
        {
            "sha": "a43562406ce69893ec5b920be4072aa1337e2c01",
            "filename": "src/transformers/models/deprecated/nat/modeling_nat.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnat%2Fmodeling_nat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnat%2Fmodeling_nat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnat%2Fmodeling_nat.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -592,15 +592,16 @@ class NatPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"nat\"\n     main_input_name = \"pixel_values\"\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n \n \n NAT_START_DOCSTRING = r\"\"\""
        },
        {
            "sha": "8e3cb0cd3f4b82096051307914c15ab4e8757197",
            "filename": "src/transformers/models/deprecated/nezha/modeling_nezha.py",
            "status": "modified",
            "additions": 16,
            "deletions": 15,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnezha%2Fmodeling_nezha.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnezha%2Fmodeling_nezha.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnezha%2Fmodeling_nezha.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -535,16 +535,10 @@ def __init__(self, config):\n \n         # The output weights are the same as the input embeddings, but there is\n         # an output-only bias for each token.\n-        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n+        self.decoder = nn.Linear(config.hidden_size, config.vocab_size)\n \n         self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n \n-        # Need a link between the two variables so that the bias is correctly resized with `resize_token_embeddings`\n-        self.decoder.bias = self.bias\n-\n-    def _tie_weights(self):\n-        self.decoder.bias = self.bias\n-\n     def forward(self, hidden_states):\n         hidden_states = self.transform(hidden_states)\n         hidden_states = self.decoder(hidden_states)\n@@ -593,19 +587,20 @@ class NezhaPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"nezha\"\n     supports_gradient_checkpointing = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n \n \n @dataclass\n@@ -873,7 +868,10 @@ def forward(\n     NEZHA_START_DOCSTRING,\n )\n class NezhaForPreTraining(NezhaPreTrainedModel):\n-    _tied_weights_keys = [\"cls.predictions.decoder\"]\n+    _tied_weights_keys = {\n+        \"cls.predictions.decoder.weight\": \"nezha.embeddings.word_embeddings.weight\",\n+        \"cls.predictions.decoder.bias\": \"cls.predictions.bias\",\n+    }\n \n     def __init__(self, config):\n         super().__init__(config)\n@@ -974,7 +972,10 @@ def forward(\n \n @add_start_docstrings(\"\"\"Nezha Model with a `language modeling` head on top.\"\"\", NEZHA_START_DOCSTRING)\n class NezhaForMaskedLM(NezhaPreTrainedModel):\n-    _tied_weights_keys = [\"cls.predictions.decoder\"]\n+    _tied_weights_keys = {\n+        \"cls.predictions.decoder.weight\": \"nezha.embeddings.word_embeddings.weight\",\n+        \"cls.predictions.decoder.bias\": \"cls.predictions.bias\",\n+    }\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "7da07eca1e34e6c4e042c9b440d9fc86ff6a1cec",
            "filename": "src/transformers/models/deprecated/open_llama/modeling_open_llama.py",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fmodeling_open_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fmodeling_open_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fmodeling_open_llama.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -439,19 +439,20 @@ class OpenLlamaPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"OpenLlamaDecoderLayer\"]\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         std = self.config.initializer_range\n         if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n             if self.config.use_stable_embedding:\n-                torch.nn.init.xavier_normal_(module.weight.data)\n+                torch.nn.init.xavier_normal_(module.weight)\n             else:\n-                module.weight.data.normal_(mean=0.0, std=std)\n+                module.weight.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n \n \n OPEN_LLAMA_INPUTS_DOCSTRING = r\"\"\""
        },
        {
            "sha": "f395fe51d6452882b8228b9e129a2f941aac24fb",
            "filename": "src/transformers/models/deprecated/qdqbert/modeling_qdqbert.py",
            "status": "modified",
            "additions": 10,
            "deletions": 13,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fqdqbert%2Fmodeling_qdqbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fqdqbert%2Fmodeling_qdqbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fqdqbert%2Fmodeling_qdqbert.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -540,15 +540,11 @@ def __init__(self, config):\n \n         # The output weights are the same as the input embeddings, but there is\n         # an output-only bias for each token.\n-        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n+        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=True)\n \n         self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n \n         # Need a link between the two variables so that the bias is correctly resized with `resize_token_embeddings`\n-        self.decoder.bias = self.bias\n-\n-    def _tie_weights(self):\n-        self.decoder.bias = self.bias\n \n     def forward(self, hidden_states):\n         hidden_states = self.transform(hidden_states)\n@@ -601,19 +597,20 @@ class QDQBertPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"bert\"\n     supports_gradient_checkpointing = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n \n \n QDQBERT_START_DOCSTRING = r\"\"\"\n@@ -853,7 +850,7 @@ def forward(\n     \"\"\"QDQBERT Model with a `language modeling` head on top for CLM fine-tuning.\"\"\", QDQBERT_START_DOCSTRING\n )\n class QDQBertLMHeadModel(QDQBertPreTrainedModel):\n-    _tied_weights_keys = [\"predictions.decoder.weight\", \"predictions.decoder.bias\"]\n+    _tied_weights_keys = {\"predictions.decoder.weight\": \"predictions.decoder.bias\"}\n \n     def __init__(self, config):\n         super().__init__(config)\n@@ -1007,7 +1004,7 @@ def prepare_inputs_for_generation(\n \n @add_start_docstrings(\"\"\"QDQBERT Model with a `language modeling` head on top.\"\"\", QDQBERT_START_DOCSTRING)\n class QDQBertForMaskedLM(QDQBertPreTrainedModel):\n-    _tied_weights_keys = [\"predictions.decoder.weight\", \"predictions.decoder.bias\"]\n+    _tied_weights_keys = {\"predictions.decoder.weight\": \"predictions.decoder.bias\"}\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "0b8062c5c9008defba5431d78be20a9b8441c876",
            "filename": "src/transformers/models/deprecated/realm/modeling_realm.py",
            "status": "modified",
            "additions": 16,
            "deletions": 16,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Fmodeling_realm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Fmodeling_realm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Fmodeling_realm.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -624,16 +624,9 @@ def __init__(self, config):\n \n         # The output weights are the same as the input embeddings, but there is\n         # an output-only bias for each token.\n-        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n-\n+        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=True)\n         self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n \n-        # Need a link between the two variables so that the bias is correctly resized with `resize_token_embeddings`\n-        self.decoder.bias = self.bias\n-\n-    def _tie_weights(self):\n-        self.decoder.bias = self.bias\n-\n     def forward(self, hidden_states):\n         hidden_states = self.transform(hidden_states)\n         hidden_states = self.decoder(hidden_states)\n@@ -794,19 +787,20 @@ class RealmPreTrainedModel(PreTrainedModel):\n     config: RealmConfig\n     base_model_prefix = \"realm\"\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n \n     def _flatten_inputs(self, *inputs):\n         \"\"\"Flatten inputs' shape to (-1, input_shape[-1])\"\"\"\n@@ -961,7 +955,10 @@ def forward(\n     REALM_START_DOCSTRING,\n )\n class RealmEmbedder(RealmPreTrainedModel):\n-    _tied_weights_keys = [\"cls.predictions.decoder.bias\"]\n+    _tied_weights_keys = {\n+        \"cls.predictions.decoder.weight\": \"realm.embeddings.word_embeddings.weight\",\n+        \"cls.predictions.decoder.bias\": \"cls.predictions.bias\",\n+    }\n \n     def __init__(self, config):\n         super().__init__(config)\n@@ -1186,7 +1183,10 @@ def forward(\n     REALM_START_DOCSTRING,\n )\n class RealmKnowledgeAugEncoder(RealmPreTrainedModel):\n-    _tied_weights_keys = [\"cls.predictions.decoder\"]\n+    _tied_weights_keys = {\n+        \"cls.predictions.decoder.weight\": \"realm.embeddings.word_embeddings.weight\",\n+        \"cls.predictions.decoder.bias\": \"cls.predictions.bias\",\n+    }\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "7a762e46b8900069722b18d9f8ae697983df251f",
            "filename": "src/transformers/models/deprecated/retribert/modeling_retribert.py",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fretribert%2Fmodeling_retribert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fretribert%2Fmodeling_retribert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fretribert%2Fmodeling_retribert.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -42,19 +42,20 @@ class RetriBertPreTrainedModel(PreTrainedModel):\n     config: RetriBertConfig\n     base_model_prefix = \"retribert\"\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n \n \n RETRIBERT_START_DOCSTRING = r\"\"\""
        },
        {
            "sha": "821467abccbafa32ef2577da09ae6ce880c497d4",
            "filename": "src/transformers/models/deprecated/speech_to_text_2/modeling_speech_to_text_2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Fmodeling_speech_to_text_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Fmodeling_speech_to_text_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Fmodeling_speech_to_text_2.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -371,16 +371,17 @@ class Speech2Text2PreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"model\"\n     supports_gradient_checkpointing = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         std = self.config.init_std\n         if isinstance(module, (nn.Linear, nn.Conv1d)):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, Speech2Text2SinusoidalPositionalEmbedding):\n             weight = module.get_embedding(*module.weight.shape, module.padding_idx)\n             weight = nn.Parameter(weight, requires_grad=False)\n@@ -628,7 +629,7 @@ def forward(self, *args, **kwargs):\n     SPEECH_TO_TEXT_2_START_DOCSTRING,\n )\n class Speech2Text2ForCausalLM(Speech2Text2PreTrainedModel):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.decoder.embed_tokens.weight\"}\n \n     def __init__(self, config):\n         config.is_decoder = True"
        },
        {
            "sha": "2bc57636b944d9657a6e1abf3ed8048a2dc3938b",
            "filename": "src/transformers/models/deprecated/trajectory_transformer/modeling_trajectory_transformer.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftrajectory_transformer%2Fmodeling_trajectory_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftrajectory_transformer%2Fmodeling_trajectory_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftrajectory_transformer%2Fmodeling_trajectory_transformer.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -84,14 +84,15 @@ class TrajectoryTransformerPreTrainedModel(PreTrainedModel):\n     main_input_name = \"trajectories\"\n     supports_gradient_checkpointing = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         if isinstance(module, (nn.Linear, nn.Embedding)):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if isinstance(module, nn.Linear) and module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n         elif isinstance(module, EinLinear):\n             for i in range(module.n_models):\n                 nn.init.kaiming_uniform_(module.weight[i], a=math.sqrt(5) / self.config.kaiming_initializer_range)"
        },
        {
            "sha": "b28613d71b7f1e33b0ed5113db3bb3aecca183f1",
            "filename": "src/transformers/models/deprecated/transfo_xl/modeling_transfo_xl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fmodeling_transfo_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fmodeling_transfo_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fmodeling_transfo_xl.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -841,7 +841,7 @@ def forward(\n     TRANSFO_XL_START_DOCSTRING,\n )\n class TransfoXLLMHeadModel(TransfoXLPreTrainedModel):\n-    _tied_weights_keys = [r\"crit\\.out_projs\\.\\d+\", r\"crit\\.out_layers\\.\\d+\\.weight\"]\n+    _tied_weights_keys = {r\"crit\\.out_projs\\.\\d+\": r\"crit\\.out_layers\\.\\d+\\.weight\"}\n \n     def __init__(self, config):\n         super().__init__(config)\n@@ -874,9 +874,6 @@ def tie_weights(self):\n         Run this to be sure output and input (adaptive) softmax weights are tied\n         \"\"\"\n \n-        if self.config.tie_word_embeddings:\n-            for i in range(len(self.crit.out_layers)):\n-                self._tie_embedding_weights(self.crit.out_layers[i], self.transformer.word_emb.emb_layers[i])\n         if self.config.tie_projs:\n             for i, tie_proj in enumerate(self.config.tie_projs):\n                 if tie_proj and self.config.div_val == 1 and self.config.d_model != self.config.d_embed:"
        },
        {
            "sha": "fbea2e2b77a30412506f06b3b7c5ff85fe1be3ae",
            "filename": "src/transformers/models/deprecated/tvlt/modeling_tvlt.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fmodeling_tvlt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fmodeling_tvlt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fmodeling_tvlt.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -548,15 +548,16 @@ class TvltPreTrainedModel(PreTrainedModel):\n     main_input_name = \"pixel_values\"\n     supports_gradient_checkpointing = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n \n \n TVLT_START_DOCSTRING = r\"\"\""
        },
        {
            "sha": "007b74755e5da2d3d488d7fe7a8d5670365cee81",
            "filename": "src/transformers/models/deprecated/van/modeling_van.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvan%2Fmodeling_van.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvan%2Fmodeling_van.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvan%2Fmodeling_van.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -359,6 +359,7 @@ class VanPreTrainedModel(PreTrainedModel):\n     main_input_name = \"pixel_values\"\n     supports_gradient_checkpointing = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, nn.Linear):\n@@ -371,9 +372,9 @@ def _init_weights(self, module):\n         elif isinstance(module, nn.Conv2d):\n             fan_out = module.kernel_size[0] * module.kernel_size[1] * module.out_channels\n             fan_out //= module.groups\n-            module.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n+            module.weight.normal_(0, math.sqrt(2.0 / fan_out))\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n \n \n VAN_START_DOCSTRING = r\"\"\""
        },
        {
            "sha": "bbc6554ff5d5382f3fa9c28bec55a34f78659b2d",
            "filename": "src/transformers/models/deprecated/vit_hybrid/modeling_vit_hybrid.py",
            "status": "modified",
            "additions": 24,
            "deletions": 17,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2Fmodeling_vit_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2Fmodeling_vit_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2Fmodeling_vit_hybrid.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -457,31 +457,38 @@ class ViTHybridPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"ViTHybridEmbeddings\", \"ViTHybridLayer\"]\n     _supports_sdpa = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module: Union[nn.Linear, nn.Conv2d, nn.LayerNorm]) -> None:\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, (nn.Linear, nn.Conv2d)):\n             # Upcast the input in `fp32` and cast it back to desired `dtype` to avoid\n             # `trunc_normal_cpu` not implemented in `half` issues\n-            module.weight.data = nn.init.trunc_normal_(\n-                module.weight.data.to(torch.float32), mean=0.0, std=self.config.initializer_range\n-            ).to(module.weight.dtype)\n+            module.weight.copy_(\n+                nn.init.trunc_normal_(module.weight.to(torch.float32), mean=0.0, std=self.config.initializer_range).to(\n+                    module.weight.dtype\n+                )\n+            )\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n         elif isinstance(module, ViTHybridEmbeddings):\n-            module.position_embeddings.data = nn.init.trunc_normal_(\n-                module.position_embeddings.data.to(torch.float32),\n-                mean=0.0,\n-                std=self.config.initializer_range,\n-            ).to(module.position_embeddings.dtype)\n-            module.cls_token.data = nn.init.trunc_normal_(\n-                module.cls_token.data.to(torch.float32),\n-                mean=0.0,\n-                std=self.config.initializer_range,\n-            ).to(module.cls_token.dtype)\n-            module.mask_token.data.zero_()\n+            module.position_embeddings.copy_(\n+                nn.init.trunc_normal_(\n+                    module.position_embeddings.to(torch.float32),\n+                    mean=0.0,\n+                    std=self.config.initializer_range,\n+                ).to(module.position_embeddings.dtype)\n+            )\n+            module.cls_token.copy_(\n+                nn.init.trunc_normal_(\n+                    module.cls_token.to(torch.float32),\n+                    mean=0.0,\n+                    std=self.config.initializer_range,\n+                ).to(module.cls_token.dtype)\n+            )\n+            module.mask_token.zero_()\n \n \n VIT_START_DOCSTRING = r\"\"\""
        },
        {
            "sha": "c592e756b7c973a8720eb13fc8c3204a8e69508a",
            "filename": "src/transformers/models/deprecated/xlm_prophetnet/modeling_xlm_prophetnet.py",
            "status": "modified",
            "additions": 23,
            "deletions": 41,
            "changes": 64,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2Fmodeling_xlm_prophetnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2Fmodeling_xlm_prophetnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2Fmodeling_xlm_prophetnet.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -520,15 +520,16 @@ class XLMProphetNetPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"prophetnet\"\n     supports_gradient_checkpointing = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=self.config.init_std)\n+            module.weight.normal_(mean=0.0, std=self.config.init_std)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=self.config.init_std)\n+            module.weight.normal_(mean=0.0, std=self.config.init_std)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n \n     def _shift_right(self, input_ids):\n         decoder_start_token_id = self.config.decoder_start_token_id\n@@ -1169,14 +1170,10 @@ class XLMProphetNetEncoder(XLMProphetNetPreTrainedModel):\n         embeddings instead of randomly initialized word embeddings.\n     \"\"\"\n \n-    def __init__(self, config: XLMProphetNetConfig, word_embeddings: Optional[nn.Embedding] = None):\n+    def __init__(self, config: XLMProphetNetConfig):\n         super().__init__(config)\n \n-        self.word_embeddings = (\n-            word_embeddings\n-            if word_embeddings is not None\n-            else nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n-        )\n+        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n         self.position_embeddings = XLMProphetNetPositionalEmbeddings(config)\n         self.embeddings_layer_norm = LayerNorm(config.hidden_size)\n \n@@ -1287,7 +1284,7 @@ class XLMProphetNetDecoder(XLMProphetNetPreTrainedModel):\n         embeddings instead of randomly initialized word embeddings.\n     \"\"\"\n \n-    def __init__(self, config: XLMProphetNetConfig, word_embeddings: Optional[nn.Embedding] = None):\n+    def __init__(self, config: XLMProphetNetConfig):\n         super().__init__(config)\n \n         self.ngram = config.ngram\n@@ -1296,11 +1293,7 @@ def __init__(self, config: XLMProphetNetConfig, word_embeddings: Optional[nn.Emb\n         self.dropout = config.dropout\n         self.max_target_positions = config.max_position_embeddings\n \n-        self.word_embeddings = (\n-            word_embeddings\n-            if word_embeddings is not None\n-            else nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n-        )\n+        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n         self.position_embeddings = XLMProphetNetPositionalEmbeddings(config)\n \n         self.ngram_embeddings = nn.Embedding(self.ngram, config.hidden_size, None)\n@@ -1611,7 +1604,10 @@ def prepare_predict_attention_mask(self, hidden_states, attention_mask):\n     XLM_PROPHETNET_START_DOCSTRING,\n )\n class XLMProphetNetModel(XLMProphetNetPreTrainedModel):\n-    _tied_weights_keys = [\"encoder.word_embeddings.weight\", \"decoder.word_embeddings.weight\"]\n+    _tied_weights_keys = {\n+        \"encoder.word_embeddings.weight\": \"word_embeddings.weight\",\n+        \"decoder.word_embeddings.weight\": \"word_embeddings.weight\",\n+    }\n \n     def __init__(self, config: XLMProphetNetConfig):\n         super().__init__(config)\n@@ -1620,12 +1616,12 @@ def __init__(self, config: XLMProphetNetConfig):\n         encoder_config = copy.deepcopy(config)\n         encoder_config.is_encoder_decoder = False\n         encoder_config.use_cache = False\n-        self.encoder = XLMProphetNetEncoder(encoder_config, self.word_embeddings)\n+        self.encoder = XLMProphetNetEncoder(encoder_config)\n \n         decoder_config = copy.deepcopy(config)\n         decoder_config.is_decoder = True\n         decoder_config.is_encoder_decoder = False\n-        self.decoder = XLMProphetNetDecoder(decoder_config, self.word_embeddings)\n+        self.decoder = XLMProphetNetDecoder(decoder_config)\n \n         # Initialize weights and apply final processing\n         self.post_init()\n@@ -1638,11 +1634,6 @@ def set_input_embeddings(self, value):\n         self.encoder.word_embeddings = self.word_embeddings\n         self.decoder.word_embeddings = self.word_embeddings\n \n-    def _tie_weights(self):\n-        if self.config.tie_word_embeddings:\n-            self._tie_embedding_weights(self.encoder.word_embeddings, self.word_embeddings)\n-            self._tie_embedding_weights(self.decoder.word_embeddings, self.word_embeddings)\n-\n     def get_encoder(self):\n         return self.encoder\n \n@@ -1736,7 +1727,7 @@ def forward(\n     XLM_PROPHETNET_START_DOCSTRING,\n )\n class XLMProphetNetForConditionalGeneration(XLMProphetNetPreTrainedModel):\n-    _tied_weights_keys = [\"encoder.word_embeddings.weight\", \"decoder.word_embeddings.weight\", \"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"prophetnet.word_embeddings.weight\"}\n \n     def __init__(self, config: XLMProphetNetConfig):\n         super().__init__(config)\n@@ -1749,10 +1740,6 @@ def __init__(self, config: XLMProphetNetConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def _tie_weights(self):\n-        if self.config.tie_word_embeddings:\n-            self._tie_embedding_weights(self.prophetnet.word_embeddings, self.lm_head)\n-\n     def get_input_embeddings(self):\n         return self.prophetnet.word_embeddings\n \n@@ -1934,11 +1921,9 @@ def get_decoder(self):\n     XLM_PROPHETNET_START_DOCSTRING,\n )\n class XLMProphetNetForCausalLM(XLMProphetNetPreTrainedModel):\n-    _tied_weights_keys = [\n-        \"prophetnet.word_embeddings.weight\",\n-        \"prophetnet.decoder.word_embeddings.weight\",\n-        \"lm_head.weight\",\n-    ]\n+    _tied_weights_keys = {\n+        \"lm_head.weight\": \"model.decoder.embed_tokens.weight\",\n+    }\n \n     def __init__(self, config: XLMProphetNetConfig):\n         # set config for CLM\n@@ -1962,10 +1947,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.prophetnet.decoder.word_embeddings = value\n \n-    def _tie_weights(self):\n-        if self.config.tie_word_embeddings:\n-            self._tie_embedding_weights(self.prophetnet.decoder.word_embeddings, self.lm_head)\n-\n     def set_decoder(self, decoder):\n         self.prophetnet.decoder = decoder\n \n@@ -2163,6 +2144,10 @@ class XLMProphetNetDecoderWrapper(XLMProphetNetPreTrainedModel):\n     classes.\n     \"\"\"\n \n+    _tied_weights_keys = {\n+        \"model.decoder.embed_tokens.weight\": \"word_embeddings.weight\",\n+    }\n+\n     def __init__(self, config: XLMProphetNetConfig):\n         super().__init__(config)\n \n@@ -2172,9 +2157,6 @@ def __init__(self, config: XLMProphetNetConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def _tie_weights(self):\n-        self._tie_embedding_weights(self.word_embeddings, self.decoder.get_input_embeddings())\n-\n     def forward(self, *args, **kwargs):\n         return self.decoder(*args, **kwargs)\n "
        },
        {
            "sha": "d6dae7cb72eed5f502b039c776a054013c75722b",
            "filename": "src/transformers/models/depth_anything/modeling_depth_anything.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdepth_anything%2Fmodeling_depth_anything.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdepth_anything%2Fmodeling_depth_anything.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdepth_anything%2Fmodeling_depth_anything.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -216,15 +216,16 @@ class DepthAnythingPreTrainedModel(PreTrainedModel):\n     input_modalities = \"image\"\n     supports_gradient_checkpointing = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, (nn.Linear, nn.Conv2d, nn.ConvTranspose2d)):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n \n \n class DepthAnythingNeck(nn.Module):"
        },
        {
            "sha": "b754cf9074c1d245cba6a762d7d1ab46190219ea",
            "filename": "src/transformers/models/depth_pro/modeling_depth_pro.py",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fmodeling_depth_pro.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fmodeling_depth_pro.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fmodeling_depth_pro.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -608,19 +608,20 @@ class DepthProPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"DepthProPreActResidualLayer\"]\n     _keys_to_ignore_on_load_unexpected = [\"fov_model.*\"]\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n         elif isinstance(module, (nn.Conv2d, nn.ConvTranspose2d)):\n             nn.init.kaiming_normal_(module.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n \n \n @auto_docstring"
        },
        {
            "sha": "84b4fbf9af49f9e1a84536563e7eb5396a21a24b",
            "filename": "src/transformers/models/detr/modeling_detr.py",
            "status": "modified",
            "additions": 9,
            "deletions": 8,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdetr%2Fmodeling_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdetr%2Fmodeling_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdetr%2Fmodeling_detr.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -233,10 +233,10 @@ def replace_batch_norm(model):\n             new_module = DetrFrozenBatchNorm2d(module.num_features)\n \n             if module.weight.device != torch.device(\"meta\"):\n-                new_module.weight.data.copy_(module.weight)\n-                new_module.bias.data.copy_(module.bias)\n-                new_module.running_mean.data.copy_(module.running_mean)\n-                new_module.running_var.data.copy_(module.running_var)\n+                new_module.weight.copy_(module.weight)\n+                new_module.bias.copy_(module.bias)\n+                new_module.running_mean.copy_(module.running_mean)\n+                new_module.running_var.copy_(module.running_var)\n \n             model._modules[name] = new_module\n \n@@ -727,6 +727,7 @@ class DetrPreTrainedModel(PreTrainedModel):\n     input_modalities = \"image\"\n     _no_split_modules = [r\"DetrConvEncoder\", r\"DetrEncoderLayer\", r\"DetrDecoderLayer\"]\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         std = self.config.init_std\n         xavier_std = self.config.init_xavier_std\n@@ -740,13 +741,13 @@ def _init_weights(self, module):\n             nn.init.uniform_(module.row_embeddings.weight)\n             nn.init.uniform_(module.column_embeddings.weight)\n         if isinstance(module, (nn.Linear, nn.Conv2d, nn.BatchNorm2d)):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n \n \n class DetrEncoder(DetrPreTrainedModel):"
        },
        {
            "sha": "7e67ac52768cc2ad66d5ac0c49531adfa8fb7172",
            "filename": "src/transformers/models/diffllama/modeling_diffllama.py",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -596,13 +596,14 @@ class DiffLlamaPreTrainedModel(PreTrainedModel):\n         \"attentions\": DiffLlamaAttention,\n     }\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, DiffLlamaAttention):\n-            module.lambda_q1.data.normal_(0, self.config.lambda_std_dev)\n-            module.lambda_k1.data.normal_(0, self.config.lambda_std_dev)\n-            module.lambda_q2.data.normal_(0, self.config.lambda_std_dev)\n-            module.lambda_k2.data.normal_(0, self.config.lambda_std_dev)\n+            module.lambda_q1.normal_(0, self.config.lambda_std_dev)\n+            module.lambda_k1.normal_(0, self.config.lambda_std_dev)\n+            module.lambda_q2.normal_(0, self.config.lambda_std_dev)\n+            module.lambda_k2.normal_(0, self.config.lambda_std_dev)\n \n \n @auto_docstring\n@@ -686,7 +687,7 @@ def forward(\n \n @auto_docstring\n class DiffLlamaForCausalLM(DiffLlamaPreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.embed_tokens.weight\"}\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n     _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n "
        },
        {
            "sha": "97b1cc051660f5b8dfd7c8ba3f317545721af5e8",
            "filename": "src/transformers/models/diffllama/modular_diffllama.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodular_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodular_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodular_diffllama.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -399,13 +399,14 @@ class DiffLlamaPreTrainedModel(LlamaPreTrainedModel):\n     _supports_flex_attn = False\n     _supports_attention_backend = False\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         PreTrainedModel._init_weights(self, module)\n         if isinstance(module, DiffLlamaAttention):\n-            module.lambda_q1.data.normal_(0, self.config.lambda_std_dev)\n-            module.lambda_k1.data.normal_(0, self.config.lambda_std_dev)\n-            module.lambda_q2.data.normal_(0, self.config.lambda_std_dev)\n-            module.lambda_k2.data.normal_(0, self.config.lambda_std_dev)\n+            module.lambda_q1.normal_(0, self.config.lambda_std_dev)\n+            module.lambda_k1.normal_(0, self.config.lambda_std_dev)\n+            module.lambda_q2.normal_(0, self.config.lambda_std_dev)\n+            module.lambda_k2.normal_(0, self.config.lambda_std_dev)\n \n \n class DiffLlamaModel(LlamaModel):"
        },
        {
            "sha": "103e12ce5ed96d78244bcddb3efd29d41c04ee9a",
            "filename": "src/transformers/models/dinat/modeling_dinat.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdinat%2Fmodeling_dinat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdinat%2Fmodeling_dinat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinat%2Fmodeling_dinat.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -561,15 +561,16 @@ class DinatPreTrainedModel(PreTrainedModel):\n     main_input_name = \"pixel_values\"\n     input_modalities = \"image\"\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n \n \n @auto_docstring"
        },
        {
            "sha": "49693d507733a06a2103714ed4bdbc4a3641ede5",
            "filename": "src/transformers/models/dinov2/modeling_dinov2.py",
            "status": "modified",
            "additions": 26,
            "deletions": 19,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_dinov2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_dinov2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_dinov2.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -414,36 +414,43 @@ class Dinov2PreTrainedModel(PreTrainedModel):\n         \"attentions\": Dinov2SelfAttention,\n     }\n \n+    @torch.no_grad()\n     def _init_weights(self, module: Union[nn.Linear, nn.Conv2d, nn.LayerNorm]) -> None:\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, (nn.Linear, nn.Conv2d)):\n             # Upcast the input in `fp32` and cast it back to desired `dtype` to avoid\n             # `trunc_normal_cpu` not implemented in `half` issues\n-            module.weight.data = nn.init.trunc_normal_(\n-                module.weight.data.to(torch.float32), mean=0.0, std=self.config.initializer_range\n-            ).to(module.weight.dtype)\n+            module.weight.copy_(\n+                nn.init.trunc_normal_(module.weight.to(torch.float32), mean=0.0, std=self.config.initializer_range).to(\n+                    module.weight.dtype\n+                )\n+            )\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n         elif isinstance(module, Dinov2Embeddings):\n-            module.position_embeddings.data = nn.init.trunc_normal_(\n-                module.position_embeddings.data.to(torch.float32),\n-                mean=0.0,\n-                std=self.config.initializer_range,\n-            ).to(module.position_embeddings.dtype)\n-\n-            module.cls_token.data = nn.init.trunc_normal_(\n-                module.cls_token.data.to(torch.float32),\n-                mean=0.0,\n-                std=self.config.initializer_range,\n-            ).to(module.cls_token.dtype)\n+            module.position_embeddings.copy_(\n+                nn.init.trunc_normal_(\n+                    module.position_embeddings.to(torch.float32),\n+                    mean=0.0,\n+                    std=self.config.initializer_range,\n+                ).to(module.position_embeddings.dtype)\n+            )\n+\n+            module.cls_token.copy_(\n+                nn.init.trunc_normal_(\n+                    module.cls_token.to(torch.float32),\n+                    mean=0.0,\n+                    std=self.config.initializer_range,\n+                ).to(module.cls_token.dtype)\n+            )\n \n             if self.config.use_mask_token:\n-                module.mask_token.data.zero_()\n+                module.mask_token.zero_()\n         elif isinstance(module, Dinov2LayerScale):\n-            module.lambda1.data.fill_(self.config.layerscale_value)\n+            module.lambda1.fill_(self.config.layerscale_value)\n \n \n @auto_docstring"
        },
        {
            "sha": "ddbc6e05b1a5d9bf2464fd7170abeabe8d728d7d",
            "filename": "src/transformers/models/dinov2_with_registers/modeling_dinov2_with_registers.py",
            "status": "modified",
            "additions": 28,
            "deletions": 21,
            "changes": 49,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodeling_dinov2_with_registers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodeling_dinov2_with_registers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodeling_dinov2_with_registers.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -431,36 +431,43 @@ class Dinov2WithRegistersPreTrainedModel(PreTrainedModel):\n         \"attentions\": Dinov2WithRegistersSelfAttention,\n     }\n \n+    @torch.no_grad()\n     def _init_weights(self, module: Union[nn.Linear, nn.Conv2d, nn.LayerNorm]) -> None:\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, (nn.Linear, nn.Conv2d)):\n             # Upcast the input in `fp32` and cast it back to desired `dtype` to avoid\n             # `trunc_normal_cpu` not implemented in `half` issues\n-            module.weight.data = nn.init.trunc_normal_(\n-                module.weight.data.to(torch.float32), mean=0.0, std=self.config.initializer_range\n-            ).to(module.weight.dtype)\n+            module.weight.copy_(\n+                nn.init.trunc_normal_(module.weight.to(torch.float32), mean=0.0, std=self.config.initializer_range).to(\n+                    module.weight.dtype\n+                )\n+            )\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n         elif isinstance(module, Dinov2WithRegistersEmbeddings):\n-            module.position_embeddings.data = nn.init.trunc_normal_(\n-                module.position_embeddings.data.to(torch.float32),\n-                mean=0.0,\n-                std=self.config.initializer_range,\n-            ).to(module.position_embeddings.dtype)\n-\n-            module.cls_token.data = nn.init.trunc_normal_(\n-                module.cls_token.data.to(torch.float32),\n-                mean=0.0,\n-                std=self.config.initializer_range,\n-            ).to(module.cls_token.dtype)\n-\n-            module.mask_token.data.zero_()\n-            module.register_tokens.data.zero_()\n+            module.position_embeddings.copy_(\n+                nn.init.trunc_normal_(\n+                    module.position_embeddings.to(torch.float32),\n+                    mean=0.0,\n+                    std=self.config.initializer_range,\n+                ).to(module.position_embeddings.dtype)\n+            )\n+\n+            module.cls_token.copy_(\n+                nn.init.trunc_normal_(\n+                    module.cls_token.to(torch.float32),\n+                    mean=0.0,\n+                    std=self.config.initializer_range,\n+                ).to(module.cls_token.dtype)\n+            )\n+\n+            module.mask_token.zero_()\n+            module.register_tokens.zero_()\n         elif isinstance(module, Dinov2WithRegistersLayerScale):  # noqa: F821\n-            module.lambda1.data.fill_(self.config.layerscale_value)\n+            module.lambda1.fill_(self.config.layerscale_value)\n \n \n @auto_docstring"
        },
        {
            "sha": "1cb6cf79bc0b302f2e09352e2b0d402ecd73211a",
            "filename": "src/transformers/models/dinov2_with_registers/modular_dinov2_with_registers.py",
            "status": "modified",
            "additions": 28,
            "deletions": 21,
            "changes": 49,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodular_dinov2_with_registers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodular_dinov2_with_registers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodular_dinov2_with_registers.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -277,36 +277,43 @@ class Dinov2WithRegistersEncoder(Dinov2Encoder):\n \n \n class Dinov2WithRegistersPreTrainedModel(Dinov2PreTrainedModel):\n+    @torch.no_grad()\n     def _init_weights(self, module: Union[nn.Linear, nn.Conv2d, nn.LayerNorm]) -> None:\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, (nn.Linear, nn.Conv2d)):\n             # Upcast the input in `fp32` and cast it back to desired `dtype` to avoid\n             # `trunc_normal_cpu` not implemented in `half` issues\n-            module.weight.data = nn.init.trunc_normal_(\n-                module.weight.data.to(torch.float32), mean=0.0, std=self.config.initializer_range\n-            ).to(module.weight.dtype)\n+            module.weight.copy_(\n+                nn.init.trunc_normal_(module.weight.to(torch.float32), mean=0.0, std=self.config.initializer_range).to(\n+                    module.weight.dtype\n+                )\n+            )\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n         elif isinstance(module, Dinov2WithRegistersEmbeddings):\n-            module.position_embeddings.data = nn.init.trunc_normal_(\n-                module.position_embeddings.data.to(torch.float32),\n-                mean=0.0,\n-                std=self.config.initializer_range,\n-            ).to(module.position_embeddings.dtype)\n-\n-            module.cls_token.data = nn.init.trunc_normal_(\n-                module.cls_token.data.to(torch.float32),\n-                mean=0.0,\n-                std=self.config.initializer_range,\n-            ).to(module.cls_token.dtype)\n-\n-            module.mask_token.data.zero_()\n-            module.register_tokens.data.zero_()\n+            module.position_embeddings.copy_(\n+                nn.init.trunc_normal_(\n+                    module.position_embeddings.to(torch.float32),\n+                    mean=0.0,\n+                    std=self.config.initializer_range,\n+                ).to(module.position_embeddings.dtype)\n+            )\n+\n+            module.cls_token.copy_(\n+                nn.init.trunc_normal_(\n+                    module.cls_token.to(torch.float32),\n+                    mean=0.0,\n+                    std=self.config.initializer_range,\n+                ).to(module.cls_token.dtype)\n+            )\n+\n+            module.mask_token.zero_()\n+            module.register_tokens.zero_()\n         elif isinstance(module, Dinov2WithRegistersLayerScale):  # noqa: F821\n-            module.lambda1.data.fill_(self.config.layerscale_value)\n+            module.lambda1.fill_(self.config.layerscale_value)\n \n \n class Dinov2WithRegistersModel(Dinov2Model):"
        },
        {
            "sha": "286cc87c3ca34b6db1bbbd8c15283537cd7f6bbc",
            "filename": "src/transformers/models/dinov3_convnext/modeling_dinov3_convnext.py",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdinov3_convnext%2Fmodeling_dinov3_convnext.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdinov3_convnext%2Fmodeling_dinov3_convnext.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov3_convnext%2Fmodeling_dinov3_convnext.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -191,18 +191,19 @@ class DINOv3ConvNextPreTrainedModel(PreTrainedModel):\n     input_modalities = \"image\"\n     _no_split_modules = [\"DINOv3ConvNextLayer\"]\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, (nn.LayerNorm, DINOv3ConvNextLayerNorm)):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n         elif isinstance(module, DINOv3ConvNextLayer):\n             if module.gamma is not None:\n-                module.gamma.data.fill_(self.config.layer_scale_init_value)\n+                module.gamma.fill_(self.config.layer_scale_init_value)\n \n \n @auto_docstring"
        },
        {
            "sha": "ad88e87671a0f580ba54702175cf800d9aacd65d",
            "filename": "src/transformers/models/dinov3_vit/modeling_dinov3_vit.py",
            "status": "modified",
            "additions": 26,
            "deletions": 19,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fmodeling_dinov3_vit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fmodeling_dinov3_vit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fmodeling_dinov3_vit.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -448,36 +448,43 @@ class DINOv3ViTPreTrainedModel(PreTrainedModel):\n         \"attentions\": DINOv3ViTAttention,\n     }\n \n+    @torch.no_grad()\n     def _init_weights(self, module) -> None:\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, (nn.Linear, nn.Conv2d)):\n             # Upcast the input in `fp32` and cast it back to desired `dtype` to avoid\n             # `trunc_normal_cpu` not implemented in `half` issues\n-            module.weight.data = nn.init.trunc_normal_(\n-                module.weight.data.to(torch.float32),\n-                mean=0.0,\n-                std=self.config.initializer_range,\n-            ).to(module.weight.dtype)\n+            module.weight.copy_(\n+                nn.init.trunc_normal_(\n+                    module.weight.to(torch.float32),\n+                    mean=0.0,\n+                    std=self.config.initializer_range,\n+                ).to(module.weight.dtype)\n+            )\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n         elif isinstance(module, DINOv3ViTEmbeddings):\n-            module.cls_token.data = nn.init.trunc_normal_(\n-                module.cls_token.data.to(torch.float32),\n-                mean=0.0,\n-                std=self.config.initializer_range,\n-            ).to(module.cls_token.dtype)\n-            if module.config.num_register_tokens > 0:\n-                module.register_tokens.data = nn.init.trunc_normal_(\n-                    module.register_tokens.data.to(torch.float32),\n+            module.cls_token.copy_(\n+                nn.init.trunc_normal_(\n+                    module.cls_token.to(torch.float32),\n                     mean=0.0,\n                     std=self.config.initializer_range,\n-                ).to(module.register_tokens.dtype)\n-            module.mask_token.data.zero_()\n+                ).to(module.cls_token.dtype)\n+            )\n+            if module.config.num_register_tokens > 0:\n+                module.register_tokens.copy_(\n+                    nn.init.trunc_normal_(\n+                        module.register_tokens.to(torch.float32),\n+                        mean=0.0,\n+                        std=self.config.initializer_range,\n+                    ).to(module.register_tokens.dtype)\n+                )\n+            module.mask_token.zero_()\n         elif isinstance(module, DINOv3ViTLayerScale):\n-            module.lambda1.data.fill_(self.config.layerscale_value)\n+            module.lambda1.fill_(self.config.layerscale_value)\n \n \n @auto_docstring"
        },
        {
            "sha": "b773c8fb9b3d303ea30640ec7b6924235e01ac00",
            "filename": "src/transformers/models/dinov3_vit/modular_dinov3_vit.py",
            "status": "modified",
            "additions": 26,
            "deletions": 19,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fmodular_dinov3_vit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fmodular_dinov3_vit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fmodular_dinov3_vit.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -343,36 +343,43 @@ class DINOv3ViTPreTrainedModel(Dinov2PreTrainedModel):\n         \"attentions\": DINOv3ViTAttention,\n     }\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, (nn.Linear, nn.Conv2d)):\n             # Upcast the input in `fp32` and cast it back to desired `dtype` to avoid\n             # `trunc_normal_cpu` not implemented in `half` issues\n-            module.weight.data = nn.init.trunc_normal_(\n-                module.weight.data.to(torch.float32),\n-                mean=0.0,\n-                std=self.config.initializer_range,\n-            ).to(module.weight.dtype)\n+            module.weight.copy_(\n+                nn.init.trunc_normal_(\n+                    module.weight.to(torch.float32),\n+                    mean=0.0,\n+                    std=self.config.initializer_range,\n+                ).to(module.weight.dtype)\n+            )\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n         elif isinstance(module, DINOv3ViTEmbeddings):\n-            module.cls_token.data = nn.init.trunc_normal_(\n-                module.cls_token.data.to(torch.float32),\n-                mean=0.0,\n-                std=self.config.initializer_range,\n-            ).to(module.cls_token.dtype)\n-            if module.config.num_register_tokens > 0:\n-                module.register_tokens.data = nn.init.trunc_normal_(\n-                    module.register_tokens.data.to(torch.float32),\n+            module.cls_token.copy_(\n+                nn.init.trunc_normal_(\n+                    module.cls_token.to(torch.float32),\n                     mean=0.0,\n                     std=self.config.initializer_range,\n-                ).to(module.register_tokens.dtype)\n-            module.mask_token.data.zero_()\n+                ).to(module.cls_token.dtype)\n+            )\n+            if module.config.num_register_tokens > 0:\n+                module.register_tokens.copy_(\n+                    nn.init.trunc_normal_(\n+                        module.register_tokens.to(torch.float32),\n+                        mean=0.0,\n+                        std=self.config.initializer_range,\n+                    ).to(module.register_tokens.dtype)\n+                )\n+            module.mask_token.zero_()\n         elif isinstance(module, DINOv3ViTLayerScale):\n-            module.lambda1.data.fill_(self.config.layerscale_value)\n+            module.lambda1.fill_(self.config.layerscale_value)\n \n \n @auto_docstring"
        },
        {
            "sha": "0638a99124b61b8fadf4e7303919db6a5d8995d1",
            "filename": "src/transformers/models/distilbert/modeling_distilbert.py",
            "status": "modified",
            "additions": 8,
            "deletions": 7,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_distilbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_distilbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_distilbert.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -299,19 +299,20 @@ class DistilBertPreTrainedModel(PreTrainedModel):\n         \"attentions\": DistilBertSelfAttention,\n     }\n \n+    @torch.no_grad()\n     def _init_weights(self, module: nn.Module):\n         \"\"\"Initialize the weights.\"\"\"\n         if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n         elif isinstance(module, Embeddings) and self.config.sinusoidal_pos_embds:\n             create_sinusoidal_embeddings(\n                 self.config.max_position_embeddings, self.config.dim, module.position_embeddings.weight\n@@ -430,7 +431,7 @@ def forward(\n     \"\"\"\n )\n class DistilBertForMaskedLM(DistilBertPreTrainedModel):\n-    _tied_weights_keys = [\"vocab_projector.weight\"]\n+    _tied_weights_keys = {\"vocab_projector.weight\": \"distilbert.embeddings.word_embeddings.weight\"}\n \n     def __init__(self, config: PreTrainedConfig):\n         super().__init__(config)"
        },
        {
            "sha": "c3cc3033d5bf09b616d93e4b0982c216f3c3e216",
            "filename": "src/transformers/models/doge/modeling_doge.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodeling_doge.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodeling_doge.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodeling_doge.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -524,17 +524,18 @@ class DogePreTrainedModel(PreTrainedModel):\n         \"attentions\": DogeAttention,\n     }\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         super()._init_weights(module)\n         if isinstance(module, DogeAttention):\n             if hasattr(module, \"A\"):\n-                module.A.data.zero_()\n+                module.A.zero_()\n         elif isinstance(module, DogeDecoderLayer):\n             if hasattr(module, \"input_residual\"):\n-                module.input_residual.data.fill_(1.0)\n+                module.input_residual.fill_(1.0)\n             if hasattr(module, \"post_attention_residual\"):\n-                module.post_attention_residual.data.fill_(1.0)\n+                module.post_attention_residual.fill_(1.0)\n \n \n @auto_docstring\n@@ -726,7 +727,7 @@ def load_balancing_loss_func(\n \n @auto_docstring\n class DogeForCausalLM(DogePreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.embed_tokens.weight\"}\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n     _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n "
        },
        {
            "sha": "99ad20e90e37723ba360fe9944cc71d110fa7b14",
            "filename": "src/transformers/models/doge/modular_doge.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodular_doge.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodular_doge.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodular_doge.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -540,17 +540,18 @@ class DogePreTrainedModel(LlamaPreTrainedModel):\n         \"attentions\": DogeAttention,\n     }\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         PreTrainedModel._init_weights(self, module)\n         if isinstance(module, DogeAttention):\n             if hasattr(module, \"A\"):\n-                module.A.data.zero_()\n+                module.A.zero_()\n         elif isinstance(module, DogeDecoderLayer):\n             if hasattr(module, \"input_residual\"):\n-                module.input_residual.data.fill_(1.0)\n+                module.input_residual.fill_(1.0)\n             if hasattr(module, \"post_attention_residual\"):\n-                module.post_attention_residual.data.fill_(1.0)\n+                module.post_attention_residual.fill_(1.0)\n \n \n class DogeModel(MixtralModel):"
        },
        {
            "sha": "e7d9422e69e224e6af27507592cf7c262e24bba9",
            "filename": "src/transformers/models/donut/modeling_donut_swin.py",
            "status": "modified",
            "additions": 8,
            "deletions": 7,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdonut%2Fmodeling_donut_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdonut%2Fmodeling_donut_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdonut%2Fmodeling_donut_swin.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -789,22 +789,23 @@ class DonutSwinPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"DonutSwinStage\"]\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n         elif isinstance(module, DonutSwinEmbeddings):\n             if module.mask_token is not None:\n-                module.mask_token.data.zero_()\n+                module.mask_token.zero_()\n             if module.position_embeddings is not None:\n-                module.position_embeddings.data.zero_()\n+                module.position_embeddings.zero_()\n         elif isinstance(module, DonutSwinSelfAttention):\n-            module.relative_position_bias_table.data.zero_()\n+            module.relative_position_bias_table.zero_()\n \n \n @auto_docstring"
        },
        {
            "sha": "f2df365ffff43e1bbbf51b22af2a058c5413a06b",
            "filename": "src/transformers/models/dots1/modeling_dots1.py",
            "status": "modified",
            "additions": 30,
            "deletions": 23,
            "changes": 53,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -305,37 +305,43 @@ def forward(self, hidden_states):\n         return router_logits\n \n \n-class Dots1NaiveMoe(nn.ModuleList):\n-    \"\"\"\n-    ModuleList of experts.\n-    \"\"\"\n+class Dots1NaiveMoe(nn.Module):\n+    \"\"\"Collection of expert weights stored as 3D tensors.\"\"\"\n \n     def __init__(self, config):\n         super().__init__()\n         self.num_experts = config.num_local_experts\n-        for _ in range(self.num_experts):\n-            self.append(Dots1MLP(config, intermediate_size=config.moe_intermediate_size))\n+        self.hidden_dim = config.hidden_size\n+        self.intermediate_dim = config.intermediate_size\n+        self.gate_up_proj = nn.Parameter(torch.empty(self.num_experts, 2 * self.intermediate_dim, self.hidden_dim))\n+        self.down_proj = nn.Parameter(torch.empty(self.num_experts, self.hidden_dim, self.intermediate_dim))\n+        self.act_fn = ACT2FN[config.hidden_act]\n \n     def forward(\n-        self, hidden_states: torch.Tensor, top_k_index: torch.Tensor, top_k_weights: torch.Tensor\n+        self,\n+        hidden_states: torch.Tensor,\n+        top_k_index: torch.Tensor,\n+        top_k_weights: torch.Tensor,\n     ) -> torch.Tensor:\n-        \"\"\"\n-        Args:\n-            hidden_states: (batch_size * sequence_length, hidden_dim)\n-            top_k_index: (batch_size * sequence_length, top_k)\n-            top_k_weights: (batch_size * sequence_length, top_k)\n-        Returns:\n-            (batch_size * sequence_length, hidden_dim)\n-        \"\"\"\n         final_hidden_states = torch.zeros_like(hidden_states)\n-        expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=self.num_experts).permute(2, 1, 0)\n+        num_experts = top_k_weights.shape[1]\n+        with torch.no_grad():\n+            expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=num_experts + 1)\n+            expert_mask = expert_mask.permute(2, 1, 0)\n+            expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n \n-        expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n         for expert_idx in expert_hit:\n-            idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))\n-            current_state = hidden_states[None, top_x].reshape(-1, hidden_states.shape[-1])\n-            current_hidden_states = self[expert_idx](current_state) * top_k_weights[top_x, idx, None]\n-            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n+            expert_idx = expert_idx[0]\n+            if expert_idx == num_experts:\n+                continue\n+            _, token_idx = torch.where(expert_mask[expert_idx])\n+            current_state = hidden_states[token_idx]\n+            gate, up = nn.functional.linear(current_state, self.gate_up_proj[expert_idx]).chunk(2, dim=-1)\n+            current_hidden_states = self.act_fn(gate) * up\n+            current_hidden_states = nn.functional.linear(current_hidden_states, self.down_proj[expert_idx])\n+            current_hidden_states = current_hidden_states * top_k_weights[token_idx, expert_idx, None]\n+            final_hidden_states.index_add_(0, token_idx, current_hidden_states.to(final_hidden_states.dtype))\n+\n         return final_hidden_states\n \n \n@@ -460,10 +466,11 @@ class Dots1PreTrainedModel(PreTrainedModel):\n         \"attentions\": Dots1Attention,\n     }\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, Dots1TopkRouter):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n \n \n @auto_docstring\n@@ -559,7 +566,7 @@ def forward(\n \n @auto_docstring\n class Dots1ForCausalLM(Dots1PreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.embed_tokens.weight\"}\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n     _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n "
        },
        {
            "sha": "6ed58db0184c9d68f9e7e4e83e20500390d62330",
            "filename": "src/transformers/models/dpr/modeling_dpr.py",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdpr%2Fmodeling_dpr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdpr%2Fmodeling_dpr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpr%2Fmodeling_dpr.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -105,19 +105,20 @@ class DPRReaderOutput(ModelOutput):\n class DPRPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n \n \n class DPREncoder(DPRPreTrainedModel):"
        },
        {
            "sha": "6562e7891772b3080261a7d3a5bcbf29cac5a37f",
            "filename": "src/transformers/models/dpt/modeling_dpt.py",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodeling_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodeling_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodeling_dpt.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -732,18 +732,19 @@ class DPTPreTrainedModel(PreTrainedModel):\n         \"attentions\": DPTSelfAttention,\n     }\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, (nn.Linear, nn.Conv2d, nn.ConvTranspose2d)):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, (nn.LayerNorm, nn.BatchNorm2d)):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n         if isinstance(module, (DPTViTEmbeddings, DPTViTHybridEmbeddings)):\n-            module.cls_token.data.zero_()\n-            module.position_embeddings.data.zero_()\n+            module.cls_token.zero_()\n+            module.position_embeddings.zero_()\n \n \n @auto_docstring"
        },
        {
            "sha": "ecc506857fee234c8b79a61a0772c72818e9516d",
            "filename": "src/transformers/models/edgetam/modeling_edgetam.py",
            "status": "modified",
            "additions": 8,
            "deletions": 15,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fedgetam%2Fmodeling_edgetam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fedgetam%2Fmodeling_edgetam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fedgetam%2Fmodeling_edgetam.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -308,22 +308,23 @@ class EdgeTamPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn_2 = True\n     _supports_attention_backend = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         std = self.config.initializer_range\n         if isinstance(module, (nn.Linear, nn.Conv2d, nn.ConvTranspose2d)):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, (nn.LayerNorm, EdgeTamLayerNorm)):\n-            module.weight.data.fill_(1.0)\n-            module.bias.data.zero_()\n+            module.weight.fill_(1.0)\n+            module.bias.zero_()\n         if isinstance(module, EdgeTamModel):\n             if module.no_memory_embedding is not None:\n-                module.no_memory_embedding.data.zero_()\n+                module.no_memory_embedding.zero_()\n \n \n # copied and adapted from original implementation, also practically equal to DetrSinePositionEmbedding\n@@ -921,9 +922,6 @@ def _dynamic_multimask_via_stability(self, all_mask_logits, all_iou_scores):\n )\n class EdgeTamModel(EdgeTamPreTrainedModel):\n     input_modalities = [\"image\", \"text\"]\n-    _tied_weights_keys = [\"prompt_encoder.shared_embedding.positional_embedding\"]\n-    # need to be ignored, as it's a buffer and will not be correctly detected as tied weight\n-    _keys_to_ignore_on_load_missing = [\"prompt_encoder.shared_embedding.positional_embedding\"]\n     _can_record_outputs = {\"mask_decoder_attentions\": OutputRecorder(EdgeTamTwoWayAttentionBlock, index=2)}\n     _keys_to_ignore_on_load_unexpected = [\n         r\"^memory_.*\",\n@@ -953,11 +951,6 @@ def __init__(self, config: EdgeTamConfig):\n \n         self.post_init()\n \n-    def _tie_weights(self):\n-        self.prompt_encoder.shared_embedding.positional_embedding.data = (\n-            self.shared_image_embedding.positional_embedding.data\n-        )\n-\n     def get_image_wide_positional_embeddings(self) -> torch.Tensor:\n         size = self.prompt_encoder.image_embedding_size\n         target_device = self.shared_image_embedding.positional_embedding.device"
        },
        {
            "sha": "594cb6084aa0818f9a9f482572b72f58ad9825d1",
            "filename": "src/transformers/models/edgetam/modular_edgetam.py",
            "status": "modified",
            "additions": 8,
            "deletions": 7,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fedgetam%2Fmodular_edgetam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fedgetam%2Fmodular_edgetam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fedgetam%2Fmodular_edgetam.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -174,22 +174,23 @@ class EdgeTamFeedForward(Sam2FeedForward):\n \n @auto_docstring\n class EdgeTamPreTrainedModel(Sam2PreTrainedModel):\n+    @torch.no_grad()\n     def _init_weights(self, module):\n         std = self.config.initializer_range\n         if isinstance(module, (nn.Linear, nn.Conv2d, nn.ConvTranspose2d)):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, (nn.LayerNorm, EdgeTamLayerNorm)):\n-            module.weight.data.fill_(1.0)\n-            module.bias.data.zero_()\n+            module.weight.fill_(1.0)\n+            module.bias.zero_()\n         if isinstance(module, EdgeTamModel):\n             if module.no_memory_embedding is not None:\n-                module.no_memory_embedding.data.zero_()\n+                module.no_memory_embedding.zero_()\n \n \n @auto_docstring("
        },
        {
            "sha": "6b648c0eaa6a45c25d20cc985720a184def2919f",
            "filename": "src/transformers/models/edgetam_video/modeling_edgetam_video.py",
            "status": "modified",
            "additions": 17,
            "deletions": 19,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fedgetam_video%2Fmodeling_edgetam_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fedgetam_video%2Fmodeling_edgetam_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fedgetam_video%2Fmodeling_edgetam_video.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -778,31 +778,32 @@ class EdgeTamVideoPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn_2 = True\n     _supports_attention_backend = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         std = self.config.initializer_range\n         if isinstance(module, (nn.Linear, nn.Conv2d, nn.ConvTranspose2d)):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, (nn.LayerNorm, EdgeTamVideoLayerNorm)):\n-            module.weight.data.fill_(1.0)\n-            module.bias.data.zero_()\n+            module.weight.fill_(1.0)\n+            module.bias.zero_()\n         elif isinstance(module, EdgeTamVideoModel):\n             if module.no_memory_positional_encoding is not None:\n-                module.no_memory_positional_encoding.data.zero_()\n+                module.no_memory_positional_encoding.zero_()\n             if module.memory_temporal_positional_encoding is not None:\n-                module.memory_temporal_positional_encoding.data.zero_()\n+                module.memory_temporal_positional_encoding.zero_()\n             if module.no_object_pointer is not None:\n-                module.no_object_pointer.data.zero_()\n+                module.no_object_pointer.zero_()\n             if module.occlusion_spatial_embedding_parameter is not None:\n-                module.occlusion_spatial_embedding_parameter.data.zero_()\n+                module.occlusion_spatial_embedding_parameter.zero_()\n         if isinstance(module, EdgeTamVideoMemoryFuserCXBlock):\n             if module.scale is not None:\n-                module.scale.data.zero_()\n+                module.scale.zero_()\n \n \n class EdgeTamVideoInferenceCache:\n@@ -1977,11 +1978,13 @@ def get_1d_sine_pe(pos_inds, dim, temperature=10000):\n @auto_docstring\n class EdgeTamVideoModel(EdgeTamVideoPreTrainedModel):\n     input_modalities = [\"video\", \"text\"]\n-    _tied_weights_keys = [\"prompt_encoder.shared_embedding.positional_embedding\"]\n-    # need to be ignored, as it's a buffer and will not be correctly detected as tied weight\n-    _keys_to_ignore_on_load_missing = [\"prompt_encoder.shared_embedding.positional_embedding\"]\n     _can_record_outputs = {\"mask_decoder_attentions\": OutputRecorder(EdgeTamVideoTwoWayAttentionBlock, index=2)}\n     _keys_to_ignore_on_load_unexpected = []\n+    _tied_weights_keys = {\n+        \"prompt_encoder.shared_embedding.positional_embedding\": \"shared_image_embedding.positional_embedding\"\n+    }\n+    # need to be ignored, as it's a buffer and will not be correctly detected as tied weight\n+    _keys_to_ignore_on_load_missing = [\"prompt_encoder.shared_embedding.positional_embedding\"]\n \n     def __init__(self, config: EdgeTamVideoConfig):\n         super().__init__(config)\n@@ -2034,11 +2037,6 @@ def __init__(self, config: EdgeTamVideoConfig):\n \n         self.post_init()\n \n-    def _tie_weights(self):\n-        self.prompt_encoder.shared_embedding.positional_embedding.data = (\n-            self.shared_image_embedding.positional_embedding.data\n-        )\n-\n     def get_input_embeddings(self):\n         return self.vision_encoder.get_input_embeddings()\n "
        },
        {
            "sha": "06dc598a2772eb6b7b1cb8cae1a7fcef1b4e4fe6",
            "filename": "src/transformers/models/edgetam_video/modular_edgetam_video.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fedgetam_video%2Fmodular_edgetam_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fedgetam_video%2Fmodular_edgetam_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fedgetam_video%2Fmodular_edgetam_video.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -1025,7 +1025,9 @@ def _forward_2d(self, hidden_states: torch.Tensor) -> tuple[torch.Tensor, torch.\n \n @auto_docstring\n class EdgeTamVideoModel(Sam2VideoModel):\n-    _tied_weights_keys = [\"prompt_encoder.shared_embedding.positional_embedding\"]\n+    _tied_weights_keys = {\n+        \"prompt_encoder.shared_embedding.positional_embedding\": \"shared_image_embedding.positional_embedding\"\n+    }\n     # need to be ignored, as it's a buffer and will not be correctly detected as tied weight\n     _keys_to_ignore_on_load_missing = [\"prompt_encoder.shared_embedding.positional_embedding\"]\n     _keys_to_ignore_on_load_unexpected = []"
        },
        {
            "sha": "5f21d7cad00f13bb5d66253a5b8764e8fbee3429",
            "filename": "src/transformers/models/efficientloftr/modeling_efficientloftr.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fmodeling_efficientloftr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fmodeling_efficientloftr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fmodeling_efficientloftr.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -675,15 +675,16 @@ class EfficientLoFTRPreTrainedModel(PreTrainedModel):\n         \"attentions\": EfficientLoFTRAttention,\n     }\n \n+    @torch.no_grad()\n     def _init_weights(self, module: nn.Module) -> None:\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, (nn.Linear, nn.Conv2d, nn.Conv1d, nn.BatchNorm2d)):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n \n     # Copied from transformers.models.superpoint.modeling_superpoint.SuperPointPreTrainedModel.extract_one_channel_pixel_values with SuperPoint->EfficientLoFTR\n     def extract_one_channel_pixel_values(self, pixel_values: torch.FloatTensor) -> torch.FloatTensor:"
        },
        {
            "sha": "4c55a3058b98bd8dc1d8901da57acecea2d58cd9",
            "filename": "src/transformers/models/efficientnet/modeling_efficientnet.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fmodeling_efficientnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fmodeling_efficientnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fmodeling_efficientnet.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -436,12 +436,13 @@ class EfficientNetPreTrainedModel(PreTrainedModel):\n     input_modalities = \"image\"\n     _no_split_modules = []\n \n+    @torch.no_grad()\n     def _init_weights(self, module: nn.Module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, (nn.Linear, nn.Conv2d, nn.BatchNorm2d)):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n \n \n @auto_docstring"
        },
        {
            "sha": "2fd4775419862f646a55a49560c4ecb053ee0f1a",
            "filename": "src/transformers/models/electra/modeling_electra.py",
            "status": "modified",
            "additions": 9,
            "deletions": 8,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -532,19 +532,20 @@ class ElectraPreTrainedModel(PreTrainedModel):\n         \"cross_attentions\": ElectraCrossAttention,\n     }\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n \n \n @dataclass\n@@ -1004,7 +1005,7 @@ def forward(\n     \"\"\"\n )\n class ElectraForMaskedLM(ElectraPreTrainedModel):\n-    _tied_weights_keys = [\"generator_lm_head.weight\"]\n+    _tied_weights_keys = {\"generator_lm_head.weight\": \"electra.embeddings.word_embeddings.weight\"}\n \n     def __init__(self, config):\n         super().__init__(config)\n@@ -1304,7 +1305,7 @@ def forward(\n     \"\"\"\n )\n class ElectraForCausalLM(ElectraPreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"generator_lm_head.weight\"]\n+    _tied_weights_keys = {\"generator_lm_head.weight\": \"electra.embeddings.word_embeddings.weight\"}\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "8e5eaf82ac310ae8be27fe90f8d09b8126492f42",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -938,6 +938,7 @@ class Emu3VQVAE(PreTrainedModel):\n         \"Emu3VQVAEVectorQuantizer\",\n     ]\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         if isinstance(module, (nn.Conv2d, nn.Conv3d)):\n             nn.init.kaiming_normal_(module.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n@@ -955,9 +956,9 @@ def _init_weights(self, module):\n             nn.init.constant_(module.weight, 1.0)\n             nn.init.constant_(module.bias, 0.0)\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_()\n+            module.weight.normal_()\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n \n     def __init__(self, config: Emu3VQVAEConfig):\n         super().__init__(config)\n@@ -1258,7 +1259,7 @@ def forward(\n \n @auto_docstring\n class Emu3ForCausalLM(Emu3PreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.embed_tokens.weight\"}\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n     _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n     config: Emu3TextConfig\n@@ -1489,7 +1490,7 @@ def forward(\n class Emu3ForConditionalGeneration(Emu3PreTrainedModel, GenerationMixin):\n     base_model_prefix = \"\"\n     output_modalities = [\"image\", \"text\"]\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.text_model.embed_tokens.weight\"}\n     _checkpoint_conversion_mapping = {\n         \"^text_model.model\": \"model.text_model\",\n         \"^vqmodel\": \"model.vqmodel\","
        },
        {
            "sha": "88d6451a6abe19bf91cbe68e4469ad550545f404",
            "filename": "src/transformers/models/emu3/modular_emu3.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -688,6 +688,7 @@ class Emu3VQVAE(PreTrainedModel):\n         \"Emu3VQVAEVectorQuantizer\",\n     ]\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         if isinstance(module, (nn.Conv2d, nn.Conv3d)):\n             nn.init.kaiming_normal_(module.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n@@ -705,9 +706,9 @@ def _init_weights(self, module):\n             nn.init.constant_(module.weight, 1.0)\n             nn.init.constant_(module.bias, 0.0)\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_()\n+            module.weight.normal_()\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n \n     def __init__(self, config: Emu3VQVAEConfig):\n         super().__init__(config)\n@@ -1043,7 +1044,7 @@ def forward(\n class Emu3ForConditionalGeneration(Emu3PreTrainedModel, GenerationMixin):\n     base_model_prefix = \"\"\n     output_modalities = [\"image\", \"text\"]\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.text_model.embed_tokens.weight\"}\n     _checkpoint_conversion_mapping = {\n         \"^text_model.model\": \"model.text_model\",\n         \"^vqmodel\": \"model.vqmodel\","
        },
        {
            "sha": "a9449caa707fc2ebe20876080fb751cf390d3731",
            "filename": "src/transformers/models/encodec/modeling_encodec.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fencodec%2Fmodeling_encodec.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fencodec%2Fmodeling_encodec.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fencodec%2Fmodeling_encodec.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -454,11 +454,12 @@ class EncodecPreTrainedModel(PreTrainedAudioTokenizerBase):\n     base_model_prefix = \"encodec\"\n     main_input_name = \"input_values\"\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, nn.GroupNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n         elif isinstance(module, nn.Conv1d):\n             nn.init.kaiming_normal_(module.weight)\n             if module.bias is not None:"
        },
        {
            "sha": "e62cb8f623ccb695a89694f45e45585b2ea8b1c6",
            "filename": "src/transformers/models/encoder_decoder/modeling_encoder_decoder.py",
            "status": "modified",
            "additions": 1,
            "deletions": 18,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_encoder_decoder.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -166,24 +166,7 @@ def __init__(\n         # tie encoder, decoder weights if config set accordingly\n         self.tie_weights()\n \n-    def tie_weights(self):\n-        self.encoder.tie_weights()\n-        self.decoder.tie_weights()\n-        # tie encoder & decoder if needed\n-        if self.config.tie_encoder_decoder:\n-            # tie encoder and decoder base model\n-            decoder_base_model_prefix = self.decoder.base_model_prefix\n-            tied_weights = self._tie_encoder_decoder_weights(\n-                self.encoder,\n-                self.decoder._modules[decoder_base_model_prefix],\n-                self.decoder.base_model_prefix,\n-                \"encoder\",\n-            )\n-            # Setting a dynamic variable instead of `_tied_weights_keys` because it's a class\n-            # attributed not an instance member, therefore modifying it will modify the entire class\n-            # Leading to issues on subsequent calls by different tests or subsequent calls.\n-            self._dynamic_tied_weights_keys = tied_weights\n-\n+    @torch.no_grad()\n     def _init_weights(self, module):\n         if module in self.encoder.modules():\n             self.encoder._init_weights(module)"
        },
        {
            "sha": "e52e98364c09bcbbec550f2a1e9fb2d4a6491427",
            "filename": "src/transformers/models/eomt/modeling_eomt.py",
            "status": "modified",
            "additions": 10,
            "deletions": 9,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Feomt%2Fmodeling_eomt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Feomt%2Fmodeling_eomt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Feomt%2Fmodeling_eomt.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -996,6 +996,7 @@ class EomtPreTrainedModel(PreTrainedModel):\n         \"attentions\": EomtAttention,\n     }\n \n+    @torch.no_grad()\n     def _init_weights(self, module: nn.Module) -> None:\n         std = self.config.initializer_range\n         if isinstance(module, (nn.Linear, nn.Conv2d, nn.ConvTranspose2d)):\n@@ -1005,20 +1006,20 @@ def _init_weights(self, module: nn.Module) -> None:\n                 bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n                 nn.init.uniform_(module.bias, -bound, bound)\n         elif isinstance(module, nn.LayerNorm):\n-            module.weight.data.fill_(1.0)\n-            module.bias.data.zero_()\n+            module.weight.fill_(1.0)\n+            module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=1)\n+            module.weight.normal_(mean=0.0, std=1)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, EomtLayerScale):\n             if hasattr(module, \"lambda1\"):\n-                module.lambda1.data.fill_(self.config.layerscale_value)\n+                module.lambda1.fill_(self.config.layerscale_value)\n         elif isinstance(module, EomtEmbeddings):\n-            module.cls_token.data = nn.init.trunc_normal_(\n-                module.cls_token.data.to(torch.float32), mean=0.0, std=std\n-            ).to(module.cls_token.dtype)\n-            module.register_tokens.data.zero_()\n+            module.cls_token.copy_(\n+                nn.init.trunc_normal_(module.cls_token.to(torch.float32), mean=0.0, std=std).to(module.cls_token.dtype)\n+            )\n+            module.register_tokens.zero_()\n \n \n @auto_docstring("
        },
        {
            "sha": "2c95affa154eb7a973be75890c167b88c3119f9b",
            "filename": "src/transformers/models/eomt/modular_eomt.py",
            "status": "modified",
            "additions": 10,
            "deletions": 9,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Feomt%2Fmodular_eomt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Feomt%2Fmodular_eomt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Feomt%2Fmodular_eomt.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -401,6 +401,7 @@ class EomtPreTrainedModel(PreTrainedModel):\n         \"attentions\": EomtAttention,\n     }\n \n+    @torch.no_grad()\n     def _init_weights(self, module: nn.Module) -> None:\n         std = self.config.initializer_range\n         if isinstance(module, (nn.Linear, nn.Conv2d, nn.ConvTranspose2d)):\n@@ -410,20 +411,20 @@ def _init_weights(self, module: nn.Module) -> None:\n                 bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n                 nn.init.uniform_(module.bias, -bound, bound)\n         elif isinstance(module, nn.LayerNorm):\n-            module.weight.data.fill_(1.0)\n-            module.bias.data.zero_()\n+            module.weight.fill_(1.0)\n+            module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=1)\n+            module.weight.normal_(mean=0.0, std=1)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, EomtLayerScale):\n             if hasattr(module, \"lambda1\"):\n-                module.lambda1.data.fill_(self.config.layerscale_value)\n+                module.lambda1.fill_(self.config.layerscale_value)\n         elif isinstance(module, EomtEmbeddings):\n-            module.cls_token.data = nn.init.trunc_normal_(\n-                module.cls_token.data.to(torch.float32), mean=0.0, std=std\n-            ).to(module.cls_token.dtype)\n-            module.register_tokens.data.zero_()\n+            module.cls_token.copy_(\n+                nn.init.trunc_normal_(module.cls_token.to(torch.float32), mean=0.0, std=std).to(module.cls_token.dtype)\n+            )\n+            module.register_tokens.zero_()\n \n \n @auto_docstring("
        },
        {
            "sha": "24890d50ac2e4db500d0bf9893895bc9148d602b",
            "filename": "src/transformers/models/ernie/modeling_ernie.py",
            "status": "modified",
            "additions": 21,
            "deletions": 18,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -488,16 +488,9 @@ def __init__(self, config):\n \n         # The output weights are the same as the input embeddings, but there is\n         # an output-only bias for each token.\n-        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n-\n+        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=True)\n         self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n \n-        # Need a link between the two variables so that the bias is correctly resized with `resize_token_embeddings`\n-        self.decoder.bias = self.bias\n-\n-    def _tie_weights(self):\n-        self.decoder.bias = self.bias\n-\n     def forward(self, hidden_states):\n         hidden_states = self.transform(hidden_states)\n         hidden_states = self.decoder(hidden_states)\n@@ -553,23 +546,24 @@ class ErniePreTrainedModel(PreTrainedModel):\n         \"cross_attentions\": ErnieCrossAttention,\n     }\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, nn.Linear):\n             # Slightly different from the TF version which uses truncated_normal for initialization\n             # cf https://github.com/pytorch/pytorch/pull/5617\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n         elif isinstance(module, ErnieLMPredictionHead):\n-            module.bias.data.zero_()\n+            module.bias.zero_()\n \n \n @auto_docstring(\n@@ -788,7 +782,10 @@ def forward(self, sequence_output, pooled_output):\n     \"\"\"\n )\n class ErnieForPreTraining(ErniePreTrainedModel):\n-    _tied_weights_keys = [\"cls.predictions.decoder.bias\", \"cls.predictions.decoder.weight\"]\n+    _tied_weights_keys = {\n+        \"cls.predictions.decoder.bias\": \"cls.predictions.bias\",\n+        \"cls.predictions.decoder.weight\": \"ernie.embeddings.word_embeddings.weight\",\n+    }\n \n     def __init__(self, config):\n         super().__init__(config)\n@@ -899,7 +896,10 @@ def forward(self, sequence_output: torch.Tensor) -> torch.Tensor:\n     \"\"\"\n )\n class ErnieForCausalLM(ErniePreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"cls.predictions.decoder.bias\", \"cls.predictions.decoder.weight\"]\n+    _tied_weights_keys = {\n+        \"cls.predictions.decoder.weight\": \"ernie.embeddings.word_embeddings.weight\",\n+        \"cls.predictions.decoder.bias\": \"cls.predictions.bias\",\n+    }\n \n     def __init__(self, config):\n         super().__init__(config)\n@@ -990,7 +990,10 @@ def forward(\n \n @auto_docstring\n class ErnieForMaskedLM(ErniePreTrainedModel):\n-    _tied_weights_keys = [\"cls.predictions.decoder.bias\", \"cls.predictions.decoder.weight\"]\n+    _tied_weights_keys = {\n+        \"cls.predictions.decoder.bias\": \"cls.predictions.bias\",\n+        \"cls.predictions.decoder.weight\": \"ernie.embeddings.word_embeddings.weight\",\n+    }\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "4bf0440d7c1616eba5ba7626c40f33de05aacbf4",
            "filename": "src/transformers/models/ernie/modular_ernie.py",
            "status": "modified",
            "additions": 16,
            "deletions": 9,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fernie%2Fmodular_ernie.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fernie%2Fmodular_ernie.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie%2Fmodular_ernie.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -162,23 +162,24 @@ class ErniePreTrainedModel(PreTrainedModel):\n         \"cross_attentions\": ErnieCrossAttention,\n     }\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, nn.Linear):\n             # Slightly different from the TF version which uses truncated_normal for initialization\n             # cf https://github.com/pytorch/pytorch/pull/5617\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n         elif isinstance(module, ErnieLMPredictionHead):\n-            module.bias.data.zero_()\n+            module.bias.zero_()\n \n \n class ErnieModel(BertModel):\n@@ -337,7 +338,10 @@ class ErnieForPreTrainingOutput(BertForPreTrainingOutput):\n \n \n class ErnieForPreTraining(BertForPreTraining):\n-    _tied_weights_keys = [\"cls.predictions.decoder.bias\", \"cls.predictions.decoder.weight\"]\n+    _tied_weights_keys = {\n+        \"cls.predictions.decoder.bias\": \"cls.predictions.bias\",\n+        \"cls.predictions.decoder.weight\": \"ernie.embeddings.word_embeddings.weight\",\n+    }\n \n     @can_return_tuple\n     @auto_docstring\n@@ -486,7 +490,10 @@ def forward(\n \n \n class ErnieForMaskedLM(BertForMaskedLM):\n-    _tied_weights_keys = [\"cls.predictions.decoder.bias\", \"cls.predictions.decoder.weight\"]\n+    _tied_weights_keys = {\n+        \"cls.predictions.decoder.bias\": \"cls.predictions.bias\",\n+        \"cls.predictions.decoder.weight\": \"ernie.embeddings.word_embeddings.weight\",\n+    }\n \n     @can_return_tuple\n     @auto_docstring"
        },
        {
            "sha": "68d279fb9abfaec7a864e48e97b218801df5cf3d",
            "filename": "src/transformers/models/ernie4_5/modeling_ernie4_5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fmodeling_ernie4_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fmodeling_ernie4_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fmodeling_ernie4_5.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -432,7 +432,7 @@ def forward(\n \n @auto_docstring\n class Ernie4_5ForCausalLM(Ernie4_5PreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.embed_tokens.weight\"}\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n     _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n "
        },
        {
            "sha": "8ff07d9f638fe95347ef56fd29a8cbb6d4a82218",
            "filename": "src/transformers/models/ernie4_5_moe/modeling_ernie4_5_moe.py",
            "status": "modified",
            "additions": 58,
            "deletions": 24,
            "changes": 82,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -315,61 +315,94 @@ def forward(self, hidden_states):\n         return hidden_states + self.e_score_correction_bias.squeeze()\n \n \n-class Ernie4_5_MoeExperts(nn.ModuleList):\n+class Ernie4_5_MoeExperts(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n         self.num_experts = config.moe_num_experts\n-        for _ in range(self.num_experts):\n-            self.append(Ernie4_5_MoeMLP(config, config.moe_intermediate_size))\n+        self.hidden_dim = config.hidden_size\n+        self.intermediate_dim = config.moe_intermediate_size\n+        self.use_bias = config.use_bias\n+        self.act_fn = ACT2FN[config.hidden_act]\n+\n+        self.gate_up_proj = nn.Parameter(torch.zeros(self.num_experts, 2 * self.intermediate_dim, self.hidden_dim))\n+        self.down_proj = nn.Parameter(torch.zeros(self.num_experts, self.hidden_dim, self.intermediate_dim))\n+        if self.use_bias:\n+            self.gate_up_proj_bias = nn.Parameter(torch.zeros(self.num_experts, 2 * self.intermediate_dim))\n+            self.down_proj_bias = nn.Parameter(torch.zeros(self.num_experts, self.hidden_dim))\n+        else:\n+            self.gate_up_proj_bias = None\n+            self.down_proj_bias = None\n \n     def forward(\n         self, hidden_states: torch.Tensor, selected_experts: torch.Tensor, routing_weights: torch.Tensor\n     ) -> torch.Tensor:\n         final_hidden_states = torch.zeros_like(hidden_states)\n+        if selected_experts.numel() == 0:\n+            return final_hidden_states\n+\n         expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes=self.num_experts).permute(2, 1, 0)\n \n         expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n         for expert_idx in expert_hit:\n+            expert_idx = int(expert_idx.item())\n             idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))\n             current_state = hidden_states[None, top_x].reshape(-1, hidden_states.shape[-1])\n-            current_hidden_states = self[expert_idx](current_state) * routing_weights[top_x, idx, None]\n+            gate_inputs = F.linear(\n+                current_state,\n+                self.gate_up_proj[expert_idx],\n+                None if self.gate_up_proj_bias is None else self.gate_up_proj_bias[expert_idx],\n+            )\n+            gate, up = gate_inputs.chunk(2, dim=-1)\n+            current_hidden_states = self.act_fn(gate) * up\n+            current_hidden_states = F.linear(\n+                current_hidden_states,\n+                self.down_proj[expert_idx],\n+                None if self.down_proj_bias is None else self.down_proj_bias[expert_idx],\n+            )\n+            current_hidden_states = current_hidden_states * routing_weights[top_x, idx, None]\n             final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n         return final_hidden_states\n \n \n-class Ernie4_5_MoeSparseMoeBlock(nn.Module):\n+class Ernie4_5_MoeTopKRouter(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n-        self.hidden_dim = config.hidden_size\n-        self.num_experts = config.moe_num_experts\n+        self.weight = nn.Parameter(torch.zeros(config.moe_num_experts, config.hidden_size, dtype=torch.float32))\n+        self.moe_statics = Ernie4_5_MoeStatics(config)\n         self.top_k = config.moe_k\n         self.norm_min = config.moe_norm_min\n \n-        self.gate = nn.Linear(config.hidden_size, config.moe_num_experts, bias=False, dtype=torch.float32)\n-        self.moe_statics = Ernie4_5_MoeStatics(config)\n-        self.experts = Ernie4_5_MoeExperts(config)\n-\n-        self.shared_experts = None\n-        if config.moe_num_shared_experts > 0:\n-            self.shared_experts = Ernie4_5_MoeMLP(config, config.moe_intermediate_size * config.moe_num_shared_experts)\n-\n-    def route_tokens_to_experts(self, hidden_states):\n+    def forward(self, hidden_states: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n         device_type = (\n             hidden_states.device.type\n             if isinstance(hidden_states.device.type, str) and hidden_states.device.type != \"mps\"\n             else \"cpu\"\n         )\n \n         with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n-            router_logits = self.gate(hidden_states.float())\n+            router_logits = F.linear(hidden_states.float(), self.weight)\n             routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n             _, selected_experts = torch.topk(self.moe_statics(routing_weights), self.top_k, dim=-1)\n             routing_weights = torch.gather(routing_weights, dim=-1, index=selected_experts)\n             routing_weights = routing_weights / torch.clamp(\n                 routing_weights.sum(dim=-1, keepdim=True), min=self.norm_min\n             )\n         routing_weights = routing_weights.to(router_logits.dtype)\n-        return selected_experts, routing_weights\n+        return routing_weights, selected_experts\n+\n+\n+class Ernie4_5_MoeSparseMoeBlock(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.hidden_dim = config.hidden_size\n+        self.num_experts = config.moe_num_experts\n+        self.top_k = config.moe_k\n+        self.gate = Ernie4_5_MoeTopKRouter(config)\n+        self.experts = Ernie4_5_MoeExperts(config)\n+\n+        self.shared_experts = None\n+        if config.moe_num_shared_experts > 0:\n+            self.shared_experts = Ernie4_5_MoeMLP(config, config.moe_intermediate_size * config.moe_num_shared_experts)\n \n     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         batch_size, sequence_length, _ = hidden_states.shape\n@@ -378,14 +411,14 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         if self.shared_experts is not None:\n             shared_output = self.shared_experts(hidden_states)\n \n-        selected_experts, routing_weights = self.route_tokens_to_experts(hidden_states)\n+        routing_weights, selected_experts = self.gate(hidden_states)\n         final_hidden_states = self.experts(hidden_states, selected_experts, routing_weights)\n \n         if self.shared_experts is not None:\n             final_hidden_states = final_hidden_states + shared_output\n \n         final_hidden_states = final_hidden_states.reshape(batch_size, sequence_length, self.hidden_dim)\n-        return final_hidden_states\n+        return final_hidden_states.to(hidden_states.dtype)\n \n \n class Ernie4_5_MoeDecoderLayer(GradientCheckpointingLayer):\n@@ -454,18 +487,19 @@ class Ernie4_5_MoePreTrainedModel(PreTrainedModel):\n     _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n     _supports_attention_backend = True\n     _can_record_outputs = {\n-        \"router_logits\": OutputRecorder(nn.Linear, layer_name=\"mlp.gate\", index=0),\n+        \"router_logits\": OutputRecorder(Ernie4_5_MoeTopKRouter, layer_name=\"mlp.gate\", index=0),\n         \"hidden_states\": Ernie4_5_MoeDecoderLayer,\n         \"attentions\": Ernie4_5_MoeAttention,\n     }\n-    _keep_in_fp32_modules_strict = [\"gate\", \"moe_statics\"]\n     # Not supporting multi-token prediction (MTP) atm\n     _keys_to_ignore_on_load_unexpected = [\"mtp\"]\n+    _keep_in_fp32_modules_strict = [\"gate.weight\", \"moe_statics\"]\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, Ernie4_5_MoeStatics):\n-            module.e_score_correction_bias.data.zero_()\n+            module.e_score_correction_bias.zero_()\n \n \n @auto_docstring\n@@ -634,7 +668,7 @@ def load_balancing_loss_func(\n \n @auto_docstring\n class Ernie4_5_MoeForCausalLM(Ernie4_5_MoePreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.embed_tokens.weight\"}\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n     _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n "
        },
        {
            "sha": "fe403f81afadb00b89d694171113b8b60f499882",
            "filename": "src/transformers/models/ernie4_5_moe/modular_ernie4_5_moe.py",
            "status": "modified",
            "additions": 58,
            "deletions": 23,
            "changes": 81,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodular_ernie4_5_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodular_ernie4_5_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodular_ernie4_5_moe.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -19,6 +19,7 @@\n import torch.nn.functional as F\n from torch import nn\n \n+from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...masking_utils import create_causal_mask\n from ...modeling_outputs import MoeModelOutputWithPast\n@@ -96,61 +97,94 @@ def forward(self, hidden_states):\n         return hidden_states + self.e_score_correction_bias.squeeze()\n \n \n-class Ernie4_5_MoeExperts(nn.ModuleList):\n+class Ernie4_5_MoeExperts(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n         self.num_experts = config.moe_num_experts\n-        for _ in range(self.num_experts):\n-            self.append(Ernie4_5_MoeMLP(config, config.moe_intermediate_size))\n+        self.hidden_dim = config.hidden_size\n+        self.intermediate_dim = config.moe_intermediate_size\n+        self.use_bias = config.use_bias\n+        self.act_fn = ACT2FN[config.hidden_act]\n+\n+        self.gate_up_proj = nn.Parameter(torch.zeros(self.num_experts, 2 * self.intermediate_dim, self.hidden_dim))\n+        self.down_proj = nn.Parameter(torch.zeros(self.num_experts, self.hidden_dim, self.intermediate_dim))\n+        if self.use_bias:\n+            self.gate_up_proj_bias = nn.Parameter(torch.zeros(self.num_experts, 2 * self.intermediate_dim))\n+            self.down_proj_bias = nn.Parameter(torch.zeros(self.num_experts, self.hidden_dim))\n+        else:\n+            self.gate_up_proj_bias = None\n+            self.down_proj_bias = None\n \n     def forward(\n         self, hidden_states: torch.Tensor, selected_experts: torch.Tensor, routing_weights: torch.Tensor\n     ) -> torch.Tensor:\n         final_hidden_states = torch.zeros_like(hidden_states)\n+        if selected_experts.numel() == 0:\n+            return final_hidden_states\n+\n         expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes=self.num_experts).permute(2, 1, 0)\n \n         expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n         for expert_idx in expert_hit:\n+            expert_idx = int(expert_idx.item())\n             idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))\n             current_state = hidden_states[None, top_x].reshape(-1, hidden_states.shape[-1])\n-            current_hidden_states = self[expert_idx](current_state) * routing_weights[top_x, idx, None]\n+            gate_inputs = F.linear(\n+                current_state,\n+                self.gate_up_proj[expert_idx],\n+                None if self.gate_up_proj_bias is None else self.gate_up_proj_bias[expert_idx],\n+            )\n+            gate, up = gate_inputs.chunk(2, dim=-1)\n+            current_hidden_states = self.act_fn(gate) * up\n+            current_hidden_states = F.linear(\n+                current_hidden_states,\n+                self.down_proj[expert_idx],\n+                None if self.down_proj_bias is None else self.down_proj_bias[expert_idx],\n+            )\n+            current_hidden_states = current_hidden_states * routing_weights[top_x, idx, None]\n             final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n         return final_hidden_states\n \n \n-class Ernie4_5_MoeSparseMoeBlock(nn.Module):\n+class Ernie4_5_MoeTopKRouter(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n-        self.hidden_dim = config.hidden_size\n-        self.num_experts = config.moe_num_experts\n+        self.weight = nn.Parameter(torch.zeros(config.moe_num_experts, config.hidden_size, dtype=torch.float32))\n+        self.moe_statics = Ernie4_5_MoeStatics(config)\n         self.top_k = config.moe_k\n         self.norm_min = config.moe_norm_min\n \n-        self.gate = nn.Linear(config.hidden_size, config.moe_num_experts, bias=False, dtype=torch.float32)\n-        self.moe_statics = Ernie4_5_MoeStatics(config)\n-        self.experts = Ernie4_5_MoeExperts(config)\n-\n-        self.shared_experts = None\n-        if config.moe_num_shared_experts > 0:\n-            self.shared_experts = Ernie4_5_MoeMLP(config, config.moe_intermediate_size * config.moe_num_shared_experts)\n-\n-    def route_tokens_to_experts(self, hidden_states):\n+    def forward(self, hidden_states: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n         device_type = (\n             hidden_states.device.type\n             if isinstance(hidden_states.device.type, str) and hidden_states.device.type != \"mps\"\n             else \"cpu\"\n         )\n \n         with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n-            router_logits = self.gate(hidden_states.float())\n+            router_logits = F.linear(hidden_states.float(), self.weight)\n             routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n             _, selected_experts = torch.topk(self.moe_statics(routing_weights), self.top_k, dim=-1)\n             routing_weights = torch.gather(routing_weights, dim=-1, index=selected_experts)\n             routing_weights = routing_weights / torch.clamp(\n                 routing_weights.sum(dim=-1, keepdim=True), min=self.norm_min\n             )\n         routing_weights = routing_weights.to(router_logits.dtype)\n-        return selected_experts, routing_weights\n+        return routing_weights, selected_experts\n+\n+\n+class Ernie4_5_MoeSparseMoeBlock(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.hidden_dim = config.hidden_size\n+        self.num_experts = config.moe_num_experts\n+        self.top_k = config.moe_k\n+        self.gate = Ernie4_5_MoeTopKRouter(config)\n+        self.experts = Ernie4_5_MoeExperts(config)\n+\n+        self.shared_experts = None\n+        if config.moe_num_shared_experts > 0:\n+            self.shared_experts = Ernie4_5_MoeMLP(config, config.moe_intermediate_size * config.moe_num_shared_experts)\n \n     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         batch_size, sequence_length, _ = hidden_states.shape\n@@ -159,14 +193,14 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         if self.shared_experts is not None:\n             shared_output = self.shared_experts(hidden_states)\n \n-        selected_experts, routing_weights = self.route_tokens_to_experts(hidden_states)\n+        routing_weights, selected_experts = self.gate(hidden_states)\n         final_hidden_states = self.experts(hidden_states, selected_experts, routing_weights)\n \n         if self.shared_experts is not None:\n             final_hidden_states = final_hidden_states + shared_output\n \n         final_hidden_states = final_hidden_states.reshape(batch_size, sequence_length, self.hidden_dim)\n-        return final_hidden_states\n+        return final_hidden_states.to(hidden_states.dtype)\n \n \n class Ernie4_5_MoeDecoderLayer(Qwen3MoeDecoderLayer):\n@@ -193,19 +227,20 @@ def __init__(self, config, layer_idx):\n class Ernie4_5_MoePreTrainedModel(MixtralPreTrainedModel):\n     config: Ernie4_5_MoeConfig\n     _no_split_modules = [\"Ernie4_5_MoeDecoderLayer\"]\n-    _keep_in_fp32_modules_strict = [\"gate\", \"moe_statics\"]\n     # Not supporting multi-token prediction (MTP) atm\n     _keys_to_ignore_on_load_unexpected = [\"mtp\"]\n     _can_record_outputs = {\n-        \"router_logits\": OutputRecorder(nn.Linear, layer_name=\"mlp.gate\", index=0),\n+        \"router_logits\": OutputRecorder(Ernie4_5_MoeTopKRouter, layer_name=\"mlp.gate\", index=0),\n         \"hidden_states\": Ernie4_5_MoeDecoderLayer,\n         \"attentions\": Ernie4_5_MoeAttention,\n     }\n+    _keep_in_fp32_modules_strict = [\"gate.weight\", \"moe_statics\"]\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         PreTrainedModel._init_weights(self, module)\n         if isinstance(module, Ernie4_5_MoeStatics):\n-            module.e_score_correction_bias.data.zero_()\n+            module.e_score_correction_bias.zero_()\n \n \n @auto_docstring"
        },
        {
            "sha": "a3f1fbdf58b594bae3cb76c84dbdf4d057e1101e",
            "filename": "src/transformers/models/esm/modeling_esm.py",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -551,22 +551,22 @@ class EsmPreTrainedModel(PreTrainedModel):\n         ],\n     }\n \n-    # Copied from transformers.models.bert.modeling_bert.BertPreTrainedModel._init_weights with BertLMPredictionHead->EsmLMHead\n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n         elif isinstance(module, EsmLMHead):\n-            module.bias.data.zero_()\n+            module.bias.zero_()\n \n     def get_output_embeddings(self):\n         # NOTE: get_output_embeddings() must return None to prevent accidental weight tying.\n@@ -727,7 +727,7 @@ def predict_contacts(self, tokens, attention_mask):\n \n @auto_docstring\n class EsmForMaskedLM(EsmPreTrainedModel):\n-    _tied_weights_keys = [\"lm_head.decoder.weight\"]\n+    _tied_weights_keys = {\"lm_head.decoder.weight\": \"esm.embeddings.word_embeddings.weight\"}\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "0c676d631b24557eb74e61f9aaaa6bd85969a388",
            "filename": "src/transformers/models/esm/modeling_esmfold.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esmfold.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esmfold.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esmfold.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -915,6 +915,7 @@ class EsmFoldPreTrainedModel(EsmPreTrainedModel):\n     \"\"\"\n \n     # Subclass `EsMPreTrainedModel` to deal with special init\n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, EsmFoldLinear):"
        },
        {
            "sha": "994ce020f81117f445f3a61a2d78558526081525",
            "filename": "src/transformers/models/evolla/modeling_evolla.py",
            "status": "modified",
            "additions": 10,
            "deletions": 8,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -517,20 +517,21 @@ class EvollaSaProtPreTrainedModel(PreTrainedModel):\n         ],\n     }\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         std = self.config.initializer_range\n         if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n \n \n class EvollaSaProtProteinEncoder(EvollaSaProtPreTrainedModel):\n@@ -1268,15 +1269,16 @@ class EvollaPreTrainedModel(PreTrainedModel):\n         \"attentions\": EvollaAttention,\n     }\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         std = self.config.initializer_range\n         super()._init_weights(module)\n         if isinstance(module, EvollaSequenceAlignerCrossAttention):\n             module.gate_attention.zero_()\n             module.gate_ffw.zero_()\n-            module.attention_norm.weight.data.fill_(1.0)\n+            module.attention_norm.weight.fill_(1.0)\n         elif isinstance(module, EvollaSequenceCompressorResampler):\n-            module.latents.data.normal_(mean=0.0, std=std)\n+            module.latents.normal_(mean=0.0, std=std)\n \n \n class EvollaModel(EvollaPreTrainedModel):"
        },
        {
            "sha": "b31f6645c5be96bd868c1039b1227772cc002709",
            "filename": "src/transformers/models/evolla/modular_evolla.py",
            "status": "modified",
            "additions": 10,
            "deletions": 8,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodular_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodular_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodular_evolla.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -202,20 +202,21 @@ class EvollaSaProtPreTrainedModel(PreTrainedModel):\n         ],\n     }\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         std = self.config.initializer_range\n         if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n \n \n class EvollaSaProtProteinEncoder(EvollaSaProtPreTrainedModel):\n@@ -732,15 +733,16 @@ class EvollaPreTrainedModel(LlamaPreTrainedModel):\n         \"EvollaSequenceAlignerCrossAttention\",\n     ]\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         std = self.config.initializer_range\n         PreTrainedModel._init_weights(self, module)\n         if isinstance(module, EvollaSequenceAlignerCrossAttention):\n             module.gate_attention.zero_()\n             module.gate_ffw.zero_()\n-            module.attention_norm.weight.data.fill_(1.0)\n+            module.attention_norm.weight.fill_(1.0)\n         elif isinstance(module, EvollaSequenceCompressorResampler):\n-            module.latents.data.normal_(mean=0.0, std=std)\n+            module.latents.normal_(mean=0.0, std=std)\n \n \n class EvollaModel(EvollaPreTrainedModel):"
        },
        {
            "sha": "cb70c9cff142cd94efda3cbf0ec527455586a7e3",
            "filename": "src/transformers/models/exaone4/modeling_exaone4.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodeling_exaone4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodeling_exaone4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodeling_exaone4.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -455,7 +455,7 @@ def forward(\n \n @auto_docstring\n class Exaone4ForCausalLM(Exaone4PreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.embed_tokens.weight\"}\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n     _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n "
        },
        {
            "sha": "4446169eb6c63291ebda19a293b175ef7a0982bf",
            "filename": "src/transformers/models/falcon/modeling_falcon.py",
            "status": "modified",
            "additions": 8,
            "deletions": 7,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -678,19 +678,20 @@ class FalconPreTrainedModel(PreTrainedModel):\n     def __init__(self, *inputs, **kwargs):\n         super().__init__(*inputs, **kwargs)\n \n+    @torch.no_grad()\n     def _init_weights(self, module: nn.Module):\n         \"\"\"Initialize the weights.\"\"\"\n         if isinstance(module, (nn.Linear, FalconLinear)):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n \n     # Adapted from transformers.modeling_utils.PreTrainedModel._check_and_enable_sdpa\n     @classmethod\n@@ -1001,7 +1002,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n     \"\"\"\n )\n class FalconForCausalLM(FalconPreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"transformer.word_embeddings.weight\"}\n \n     def __init__(self, config: FalconConfig):\n         super().__init__(config)"
        },
        {
            "sha": "f15f8ee1c3b150e60bc2093ead69d75600454088",
            "filename": "src/transformers/models/falcon_h1/modeling_falcon_h1.py",
            "status": "modified",
            "additions": 16,
            "deletions": 14,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -1194,21 +1194,23 @@ class FalconH1PreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _is_stateful = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         std = self.config.initializer_range\n-        for name, param in module.named_parameters(recurse=True):\n-            if not param.requires_grad:\n-                continue\n-            if \"layernorm\" in name.lower() and \"weight\" in name:\n-                # LayerNorm weights usually initialized to 1\n-                param.data.fill_(1.0)\n-            elif \"bias\" in name:\n-                param.data.zero_()\n-            else:\n-                try:\n-                    param.data.normal_(mean=0.0, std=std)\n-                except Exception as e:\n-                    print(f\"Skipping init for {name} due to error: {e}\")\n+        if isinstance(module, nn.Module):\n+            for name, param in module.named_parameters(recurse=True):\n+                if not param.requires_grad:\n+                    continue\n+                if \"layernorm\" in name.lower() and \"weight\" in name:\n+                    # LayerNorm weights usually initialized to 1\n+                    param.fill_(1.0)\n+                elif \"bias\" in name:\n+                    param.zero_()\n+                else:\n+                    try:\n+                        param.normal_(mean=0.0, std=std)\n+                    except Exception as e:\n+                        print(f\"Skipping init for {name} due to error: {e}\")\n \n \n def compute_mup_vector(config):\n@@ -1503,7 +1505,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n \n @auto_docstring\n class FalconH1ForCausalLM(FalconH1PreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.embed_tokens.weight\"}\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n     _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n "
        },
        {
            "sha": "5371cab2bf2000b505eddfd7a897466b78023c0a",
            "filename": "src/transformers/models/falcon_h1/modular_falcon_h1.py",
            "status": "modified",
            "additions": 15,
            "deletions": 13,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodular_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodular_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodular_falcon_h1.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -920,21 +920,23 @@ class FalconH1PreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _is_stateful = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         std = self.config.initializer_range\n-        for name, param in module.named_parameters(recurse=True):\n-            if not param.requires_grad:\n-                continue\n-            if \"layernorm\" in name.lower() and \"weight\" in name:\n-                # LayerNorm weights usually initialized to 1\n-                param.data.fill_(1.0)\n-            elif \"bias\" in name:\n-                param.data.zero_()\n-            else:\n-                try:\n-                    param.data.normal_(mean=0.0, std=std)\n-                except Exception as e:\n-                    print(f\"Skipping init for {name} due to error: {e}\")\n+        if isinstance(module, nn.Module):\n+            for name, param in module.named_parameters(recurse=True):\n+                if not param.requires_grad:\n+                    continue\n+                if \"layernorm\" in name.lower() and \"weight\" in name:\n+                    # LayerNorm weights usually initialized to 1\n+                    param.fill_(1.0)\n+                elif \"bias\" in name:\n+                    param.zero_()\n+                else:\n+                    try:\n+                        param.normal_(mean=0.0, std=std)\n+                    except Exception as e:\n+                        print(f\"Skipping init for {name} due to error: {e}\")\n \n \n def compute_mup_vector(config):"
        },
        {
            "sha": "d7acfd8f1a533492e4180c1d35c97aa7709e097c",
            "filename": "src/transformers/models/falcon_mamba/modeling_falcon_mamba.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -568,6 +568,7 @@ class FalconMambaPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _is_stateful = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights.\"\"\"\n         std = self.config.initializer_range\n@@ -577,7 +578,7 @@ def _init_weights(self, module):\n             A = torch.arange(1, module.ssm_state_size + 1, dtype=torch.float32)[None, :]\n             A = A.expand(module.intermediate_size, -1).contiguous()\n             module.A_log.copy_(torch.log(A))\n-            module.D.data.fill_(1.0)\n+            module.D.fill_(1.0)\n \n             dt_init_std = self.config.time_step_rank**-0.5 * self.config.time_step_scale\n             if self.config.time_step_init_scheme == \"constant\":\n@@ -622,7 +623,7 @@ def _init_weights(self, module):\n                 if not getattr(module.bias, \"_no_reinit\", False):\n                     nn.init.zeros_(module.bias)\n         elif isinstance(module, FalconMambaRMSNorm):\n-            module.weight.data.fill_(1.0)\n+            module.weight.fill_(1.0)\n         elif isinstance(module, nn.Embedding):\n             nn.init.normal_(module.weight, std=std)\n \n@@ -780,7 +781,7 @@ def forward(\n     \"\"\"\n )\n class FalconMambaForCausalLM(FalconMambaPreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"backbone.embeddings.weight\"}\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "51f50d298e277f1963dc36a0cd545781a225a49a",
            "filename": "src/transformers/models/fastspeech2_conformer/modeling_fastspeech2_conformer.py",
            "status": "modified",
            "additions": 9,
            "deletions": 7,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Fmodeling_fastspeech2_conformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Fmodeling_fastspeech2_conformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Fmodeling_fastspeech2_conformer.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -991,24 +991,25 @@ class FastSpeech2ConformerPreTrainedModel(PreTrainedModel):\n \n     main_input_name = \"input_ids\"\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, nn.Linear):\n             nn.init.normal_(module.weight, std=1.0 / math.sqrt(module.weight.size(1)))\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Conv1d):\n             nn.init.kaiming_normal_(module.weight)\n             if module.bias is not None:\n                 key = math.sqrt(module.groups / (module.in_channels * module.kernel_size[0]))\n                 nn.init.uniform_(module.bias, a=-key, b=key)\n         elif isinstance(module, (nn.LayerNorm, nn.BatchNorm1d)):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_()\n+            module.weight.normal_()\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, FastSpeech2ConformerAttention):\n             nn.init.xavier_uniform_(module.pos_bias_u)\n             nn.init.xavier_uniform_(module.pos_bias_v)\n@@ -1403,12 +1404,13 @@ def __init__(self, config: FastSpeech2ConformerHifiGanConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @torch.no_grad()\n     def _init_weights(self, module: nn.Module):\n         \"\"\"Initialize the weights.\"\"\"\n         if isinstance(module, (nn.Conv1d, nn.ConvTranspose1d)):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n \n     def apply_weight_norm(self):\n         weight_norm = nn.utils.weight_norm"
        },
        {
            "sha": "5a22aff9c0477dbe6483f13e79fc66a1a9abc7c9",
            "filename": "src/transformers/models/flaubert/modeling_flaubert.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fflaubert%2Fmodeling_flaubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fflaubert%2Fmodeling_flaubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflaubert%2Fmodeling_flaubert.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -671,21 +671,22 @@ def dummy_inputs(self):\n             langs_list = None\n         return {\"input_ids\": inputs_list, \"attention_mask\": attns_list, \"langs\": langs_list}\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights.\"\"\"\n         if isinstance(module, nn.Embedding):\n             if self.config is not None and self.config.embed_init_std is not None:\n                 nn.init.normal_(module.weight, mean=0, std=self.config.embed_init_std)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         if isinstance(module, nn.Linear):\n             if self.config is not None and self.config.init_std is not None:\n                 nn.init.normal_(module.weight, mean=0, std=self.config.init_std)\n                 if module.bias is not None:\n                     nn.init.constant_(module.bias, 0.0)\n         if isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n         if isinstance(module, FlaubertModel) and self.config.sinusoidal_embeddings:\n             create_sinusoidal_embeddings(\n                 self.config.max_position_embeddings, self.config.emb_dim, out=module.position_embeddings.weight\n@@ -947,7 +948,7 @@ def forward(\n     \"\"\"\n )\n class FlaubertWithLMHeadModel(FlaubertPreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"pred_layer.proj.weight\"]\n+    _tied_weights_keys = {\"pred_layer.proj.weight\": \"transformer.embeddings.weight\"}\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "bcca5d13d528a6dac36238aad49b7184cb8eacdb",
            "filename": "src/transformers/models/flava/modeling_flava.py",
            "status": "modified",
            "additions": 20,
            "deletions": 25,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fflava%2Fmodeling_flava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fflava%2Fmodeling_flava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflava%2Fmodeling_flava.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -665,31 +665,32 @@ class FlavaPreTrainedModel(PreTrainedModel):\n     input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module: Union[nn.Linear, nn.Conv2d, nn.LayerNorm]) -> None:\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n         elif isinstance(module, FlavaMaskedPredictionHead):\n-            module.bias.data.zero_()\n+            module.bias.zero_()\n         elif isinstance(module, FlavaImageEmbeddings):\n-            module.cls_token.data.zero_()\n-            module.position_embeddings.data.zero_()\n+            module.cls_token.zero_()\n+            module.position_embeddings.zero_()\n             if module.mask_token is not None:\n-                module.mask_token.data.zero_()\n+                module.mask_token.zero_()\n         elif isinstance(module, FlavaMultimodalModel):\n             if module.use_cls_token:\n-                module.cls_token.data.zero_()\n+                module.cls_token.zero_()\n         elif isinstance(module, FlavaModel):\n-            module.logit_scale.data.fill_(self.config.logit_scale_init_value)\n+            module.logit_scale.fill_(self.config.logit_scale_init_value)\n \n \n @auto_docstring\n@@ -1445,17 +1446,11 @@ def __init__(self, config, weight=None):\n         super().__init__()\n         self.config = config\n         self.transform = FlavaPredictionHeadTransform(config)\n-        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n+        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=True)\n         self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n         if weight is not None:\n             self.decoder.weight = weight\n \n-        # Need a link between the two variables so that the bias is correctly resized with `resize_token_embeddings`\n-        self.decoder.bias = self.bias\n-\n-    def _tie_weights(self):\n-        self.decoder.bias = self.bias\n-\n     def forward(self, x):\n         x = self.transform(x)\n         x = self.decoder(x)\n@@ -1522,12 +1517,12 @@ def forward(self, image_embeddings, text_embeddings, logit_scale):\n )\n class FlavaForPreTraining(FlavaPreTrainedModel):\n     # Those are linked to xxx.bias\n-    _tied_weights_keys = [\n-        \"mmm_text_head.decoder.bias\",\n-        \"mmm_image_head.decoder.bias\",\n-        \"mlm_head.decoder.bias\",\n-        \"mim_head.decoder.bias\",\n-    ]\n+    _tied_weights_keys = {\n+        \"mmm_text_head.bias\": \"mmm_text_head.decoder.bias\",\n+        \"mim_head.bias\": \"mim_head.decoder.bias\",\n+        \"mlm_head.bias\": \"mlm_head.decoder.bias\",\n+        \"mmm_image_head.bias\": \"mmm_image_head.decoder.bias\",\n+    }\n \n     def __init__(self, config: FlavaConfig, image_codebook: Optional[nn.Module] = None):\n         r\"\"\""
        },
        {
            "sha": "ff93c53ebe7f3bd1cbc607ad5e4e68caf3c039d5",
            "filename": "src/transformers/models/flex_olmo/configuration_flex_olmo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fconfiguration_flex_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fconfiguration_flex_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fconfiguration_flex_olmo.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -109,6 +109,7 @@ class FlexOlmoConfig(PreTrainedConfig):\n \n     model_type = \"flex_olmo\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n+    attribute_map = {\"num_local_experts\": \"num_experts\"}\n     base_model_tp_plan = {\n         \"layers.*.self_attn.q_proj\": \"colwise_rep\",  # we need to replicate here due to the added norm on q and k\n         \"layers.*.self_attn.k_proj\": \"colwise_rep\",  # we need to replicate here due to the added norm on q and k"
        },
        {
            "sha": "e55c8e02a1508229991904c2838e4eebf42f6246",
            "filename": "src/transformers/models/flex_olmo/modeling_flex_olmo.py",
            "status": "modified",
            "additions": 62,
            "deletions": 38,
            "changes": 100,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fmodeling_flex_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fmodeling_flex_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fmodeling_flex_olmo.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -23,6 +23,7 @@\n from typing import Optional, Union\n \n import torch\n+import torch.nn.functional as F\n from torch import nn\n \n from ...activations import ACT2FN\n@@ -291,64 +292,77 @@ def forward(\n         return attn_output, attn_weights\n \n \n-class FlexOlmoExperts(nn.ModuleList):\n-    \"\"\"\n-    ModuleList of experts.\n-    \"\"\"\n+class FlexOlmoExperts(nn.Module):\n+    \"\"\"Collection of expert weights stored as 3D tensors.\"\"\"\n \n-    def __init__(self, config):\n+    def __init__(self, config: FlexOlmoConfig):\n         super().__init__()\n-        for _ in range(config.num_experts):\n-            self.append(FlexOlmoMLP(config))\n-        self.num_experts = config.num_experts\n-        self.top_k = config.num_experts_per_tok\n-        self.norm_topk_prob = config.norm_topk_prob\n+        self.num_experts = config.num_local_experts\n+        self.hidden_dim = config.hidden_size\n+        self.intermediate_dim = config.intermediate_size\n+        self.gate_up_proj = nn.Parameter(torch.empty(self.num_experts, 2 * self.intermediate_dim, self.hidden_dim))\n+        self.down_proj = nn.Parameter(torch.empty(self.num_experts, self.hidden_dim, self.intermediate_dim))\n+        self.act_fn = ACT2FN[config.hidden_act]\n \n     def forward(\n-        self, hidden_states: torch.Tensor, top_k_index: torch.Tensor, top_k_weights: torch.Tensor\n+        self,\n+        hidden_states: torch.Tensor,\n+        top_k_index: torch.Tensor,\n+        top_k_weights: torch.Tensor,\n     ) -> torch.Tensor:\n-        \"\"\"\n-        Args:\n-            hidden_states: (batch_size * sequence_length, hidden_dim)\n-            top_k_index: (batch_size * sequence_length, top_k)\n-            top_k_weights: (batch_size * sequence_length, top_k)\n-        Returns:\n-            (batch_size * sequence_length, hidden_dim)\n-        \"\"\"\n         final_hidden_states = torch.zeros_like(hidden_states)\n-        expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=self.num_experts).permute(2, 1, 0)\n+        num_experts = top_k_weights.shape[1]\n+        with torch.no_grad():\n+            expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=num_experts + 1)\n+            expert_mask = expert_mask.permute(2, 1, 0)\n+            expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n \n-        expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n         for expert_idx in expert_hit:\n-            idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))\n-            current_state = hidden_states[None, top_x].reshape(-1, hidden_states.shape[-1])\n-            current_hidden_states = self[expert_idx](current_state) * top_k_weights[top_x, idx, None]\n-            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n+            expert_idx = expert_idx[0]\n+            if expert_idx == num_experts:\n+                continue\n+            _, token_idx = torch.where(expert_mask[expert_idx])\n+            current_state = hidden_states[token_idx]\n+            gate, up = nn.functional.linear(current_state, self.gate_up_proj[expert_idx]).chunk(2, dim=-1)\n+            current_hidden_states = self.act_fn(gate) * up\n+            current_hidden_states = nn.functional.linear(current_hidden_states, self.down_proj[expert_idx])\n+            current_hidden_states = current_hidden_states * top_k_weights[token_idx, expert_idx, None]\n+            final_hidden_states.index_add_(0, token_idx, current_hidden_states.to(final_hidden_states.dtype))\n+\n         return final_hidden_states\n \n \n-class FlexOlmoSparseMoeBlock(nn.Module):\n+class FlexOlmoTopKRouter(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n-        self.num_experts = config.num_experts\n         self.top_k = config.num_experts_per_tok\n+        self.num_experts = config.num_experts\n         self.norm_topk_prob = config.norm_topk_prob\n-        self.gate = nn.Linear(config.hidden_size, self.num_experts, bias=False)\n-        self.experts = FlexOlmoExperts(config)\n+        self.hidden_dim = config.hidden_size\n+        self.weight = nn.Parameter(torch.zeros(self.num_experts, self.hidden_dim))\n \n-    def route_tokens_to_experts(self, hidden_states, router_logits):\n-        routing_weights = torch.nn.functional.softmax(router_logits.float(), dim=-1)\n-        top_k_weights, top_k_index = torch.topk(routing_weights, self.top_k, dim=-1)\n+    def forward(self, hidden_states):\n+        hidden_states = hidden_states.reshape(-1, self.hidden_dim)\n+        router_logits = F.linear(hidden_states, self.weight)  # (seq_len, num_experts)\n+        router_logits = torch.nn.functional.softmax(router_logits, dtype=torch.float, dim=-1)\n+        router_top_value, router_indices = torch.topk(router_logits, self.top_k, dim=-1)  # (seq_len, top_k)\n         if self.norm_topk_prob:\n-            top_k_weights /= top_k_weights.sum(dim=-1, keepdim=True)\n-        top_k_weights = top_k_weights.to(hidden_states.dtype)\n-        return top_k_index, top_k_weights\n+            router_top_value /= router_top_value.sum(dim=-1, keepdim=True)\n+        router_top_value = router_top_value.to(router_logits.dtype)\n+        router_scores = torch.zeros_like(router_logits).scatter_(1, router_indices, router_top_value)\n+        return router_scores, router_indices\n+\n+\n+class FlexOlmoSparseMoeBlock(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.gate = FlexOlmoTopKRouter(config)\n+        self.experts = FlexOlmoExperts(config)\n \n     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         batch_size, sequence_length, hidden_dim = hidden_states.shape\n         hidden_states = hidden_states.view(-1, hidden_dim)\n-        router_logits = self.gate(hidden_states)\n-        top_k_index, top_k_weights = self.route_tokens_to_experts(hidden_states, router_logits)\n+        top_k_weights, top_k_index = self.gate(hidden_states)\n         final_hidden_states = self.experts(hidden_states, top_k_index, top_k_weights).reshape(\n             batch_size, sequence_length, hidden_dim\n         )\n@@ -415,6 +429,16 @@ class FlexOlmoPreTrainedModel(PreTrainedModel):\n         \"attentions\": FlexOlmoAttention,\n     }\n \n+    @torch.no_grad()\n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+        std = self.config.initializer_range\n+        if isinstance(module, FlexOlmoExperts):\n+            module.gate_up_proj.normal_(mean=0.0, std=std)\n+            module.down_proj.normal_(mean=0.0, std=std)\n+        elif isinstance(module, FlexOlmoTopKRouter):\n+            module.weight.normal_(mean=0.0, std=std)\n+\n \n @auto_docstring\n class FlexOlmoModel(FlexOlmoPreTrainedModel):\n@@ -582,7 +606,7 @@ def load_balancing_loss_func(\n \n @auto_docstring\n class FlexOlmoForCausalLM(FlexOlmoPreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.embed_tokens.weight\"}\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n     _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n "
        },
        {
            "sha": "34aa2f8f454dadd043c850eed12fd24d643f14a4",
            "filename": "src/transformers/models/florence2/modeling_florence2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 10,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodeling_florence2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodeling_florence2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodeling_florence2.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -615,7 +615,6 @@ class Florence2Seq2SeqLMOutput(Seq2SeqLMOutput):\n @auto_docstring\n class Florence2PreTrainedModel(PreTrainedModel):\n     config: Florence2Config\n-    base_model_prefix = \"model\"\n     input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n     _skip_keys_device_placement = \"past_key_values\"\n@@ -628,6 +627,7 @@ class Florence2PreTrainedModel(PreTrainedModel):\n \n     _supports_attention_backend = False\n     config_class = Florence2Config\n+    base_model_prefix = \"model\"\n \n \n @auto_docstring(\n@@ -637,10 +637,6 @@ class Florence2PreTrainedModel(PreTrainedModel):\n )\n class Florence2Model(Florence2PreTrainedModel):\n     _checkpoint_conversion_mapping = {}\n-    _tied_weights_keys = [\n-        \"language_model.encoder.embed_tokens.weight\",\n-        \"language_model.decoder.embed_tokens.weight\",\n-    ]\n \n     def __init__(self, config: Florence2Config):\n         super().__init__(config)\n@@ -806,11 +802,9 @@ def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start\n )\n class Florence2ForConditionalGeneration(Florence2PreTrainedModel, GenerationMixin):\n     _checkpoint_conversion_mapping = {}\n-    _tied_weights_keys = [\n-        \"model.language_model.encoder.embed_tokens.weight\",\n-        \"model.language_model.decoder.embed_tokens.weight\",\n-        \"lm_head.weight\",\n-    ]\n+    _tied_weights_keys = {\n+        \"lm_head.weight\": \"model.language_model.shared.weight\",\n+    }\n \n     def __init__(self, config: Florence2Config):\n         super().__init__(config)"
        },
        {
            "sha": "d2bf13544b1f9915b9dabc98cd79440900634336",
            "filename": "src/transformers/models/florence2/modular_florence2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 9,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodular_florence2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodular_florence2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodular_florence2.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -1508,10 +1508,6 @@ class Florence2PreTrainedModel(LlavaPreTrainedModel):\n )\n class Florence2Model(LlavaModel):\n     _checkpoint_conversion_mapping = {}\n-    _tied_weights_keys = [\n-        \"language_model.encoder.embed_tokens.weight\",\n-        \"language_model.decoder.embed_tokens.weight\",\n-    ]\n \n     def __init__(self, config: Florence2Config):\n         super().__init__(config)\n@@ -1624,11 +1620,9 @@ def forward(\n )\n class Florence2ForConditionalGeneration(LlavaForConditionalGeneration):\n     _checkpoint_conversion_mapping = {}\n-    _tied_weights_keys = [\n-        \"model.language_model.encoder.embed_tokens.weight\",\n-        \"model.language_model.decoder.embed_tokens.weight\",\n-        \"lm_head.weight\",\n-    ]\n+    _tied_weights_keys = {\n+        \"lm_head.weight\": \"model.language_model.shared.weight\",\n+    }\n \n     def get_encoder(self):\n         return self.model.get_encoder()"
        },
        {
            "sha": "5cc5c870fa9eafac07e558cfcbe2e808a4e63cef",
            "filename": "src/transformers/models/fnet/modeling_fnet.py",
            "status": "modified",
            "additions": 15,
            "deletions": 21,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Ffnet%2Fmodeling_fnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Ffnet%2Fmodeling_fnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffnet%2Fmodeling_fnet.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -325,27 +325,14 @@ class FNetLMPredictionHead(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n         self.transform = FNetPredictionHeadTransform(config)\n-\n-        # The output weights are the same as the input embeddings, but there is\n-        # an output-only bias for each token.\n         self.decoder = nn.Linear(config.hidden_size, config.vocab_size)\n-\n         self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n-        self.decoder.bias = self.bias\n \n     def forward(self, hidden_states):\n         hidden_states = self.transform(hidden_states)\n         hidden_states = self.decoder(hidden_states)\n         return hidden_states\n \n-    def _tie_weights(self) -> None:\n-        # For accelerate compatibility and to not break backward compatibility\n-        if self.decoder.bias.device.type == \"meta\":\n-            self.decoder.bias = self.bias\n-        else:\n-            # To tie those two weights if they get disconnected (on TPU or when the bias is resized)\n-            self.bias = self.decoder.bias\n-\n \n class FNetOnlyMLMHead(nn.Module):\n     def __init__(self, config):\n@@ -387,20 +374,21 @@ class FNetPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"fnet\"\n     supports_gradient_checkpointing = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             # NOTE: Original code uses same initialization as weights for biases as well.\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n \n \n @dataclass\n@@ -536,7 +524,10 @@ def forward(\n     \"\"\"\n )\n class FNetForPreTraining(FNetPreTrainedModel):\n-    _tied_weights_keys = [\"cls.predictions.decoder.bias\", \"cls.predictions.decoder.weight\"]\n+    _tied_weights_keys = {\n+        \"cls.predictions.decoder.bias\": \"cls.predictions.bias\",\n+        \"cls.predictions.decoder.weight\": \"fnet.embeddings.word_embeddings.weight\",\n+    }\n \n     def __init__(self, config):\n         super().__init__(config)\n@@ -626,7 +617,10 @@ def forward(\n \n @auto_docstring\n class FNetForMaskedLM(FNetPreTrainedModel):\n-    _tied_weights_keys = [\"cls.predictions.decoder.bias\", \"cls.predictions.decoder.weight\"]\n+    _tied_weights_keys = {\n+        \"cls.predictions.decoder.bias\": \"cls.predictions.bias\",\n+        \"cls.predictions.decoder.weight\": \"fnet.embeddings.word_embeddings.weight\",\n+    }\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "a297378f549269e1848ef71a2887db02ce1a6d8a",
            "filename": "src/transformers/models/focalnet/modeling_focalnet.py",
            "status": "modified",
            "additions": 8,
            "deletions": 7,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Ffocalnet%2Fmodeling_focalnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Ffocalnet%2Fmodeling_focalnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffocalnet%2Fmodeling_focalnet.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -581,22 +581,23 @@ class FocalNetPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"FocalNetStage\"]\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n         elif isinstance(module, FocalNetEmbeddings):\n             if module.mask_token is not None:\n-                module.mask_token.data.zero_()\n+                module.mask_token.zero_()\n         elif isinstance(module, FocalNetLayer):\n             if self.config.use_layerscale:\n-                module.gamma_1.data.fill_(self.config.layerscale_value)\n-                module.gamma_2.data.fill_(self.config.layerscale_value)\n+                module.gamma_1.fill_(self.config.layerscale_value)\n+                module.gamma_2.fill_(self.config.layerscale_value)\n \n \n @auto_docstring"
        },
        {
            "sha": "7cfc86744e74f67d4263b8980968d2101face3d2",
            "filename": "src/transformers/models/fsmt/modeling_fsmt.py",
            "status": "modified",
            "additions": 20,
            "deletions": 42,
            "changes": 62,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Ffsmt%2Fmodeling_fsmt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Ffsmt%2Fmodeling_fsmt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffsmt%2Fmodeling_fsmt.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -37,7 +37,6 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n-from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -220,21 +219,22 @@ class PretrainedFSMTModel(PreTrainedModel):\n     config: FSMTConfig\n     base_model_prefix = \"model\"\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         std = self.config.init_std\n         if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, SinusoidalPositionalEmbedding):\n             weight = module.get_embedding(*module.weight.shape, module.padding_idx)\n             weight = nn.Parameter(weight, requires_grad=False)\n             weight.detach_()\n             module.weight = weight\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n \n     @property\n     def dummy_inputs(self):\n@@ -338,13 +338,13 @@ class FSMTEncoder(nn.Module):\n         config: FSMTConfig\n     \"\"\"\n \n-    def __init__(self, config: FSMTConfig, embed_tokens):\n+    def __init__(self, config: FSMTConfig):\n         super().__init__()\n         self.dropout = config.dropout\n         self.layerdrop = config.encoder_layerdrop\n-        self.padding_idx = embed_tokens.padding_idx\n-        self.embed_tokens = embed_tokens\n-        embed_dim = embed_tokens.embedding_dim\n+        self.padding_idx = config.pad_token_id\n+        self.embed_tokens = nn.Embedding(config.src_vocab_size, config.d_model, config.pad_token_id)\n+        embed_dim = self.embed_tokens.embedding_dim\n         self.embed_scale = math.sqrt(embed_dim) if config.scale_embedding else 1.0\n         self.embed_positions = SinusoidalPositionalEmbedding(\n             config.max_position_embeddings + self.padding_idx + 1, embed_dim, self.padding_idx\n@@ -531,31 +531,19 @@ class FSMTDecoder(nn.Module):\n         embed_tokens (nn.Embedding): output embedding\n     \"\"\"\n \n-    def __init__(self, config: FSMTConfig, embed_tokens: nn.Embedding):\n+    def __init__(self, config: FSMTConfig):\n         super().__init__()\n         self.dropout = config.dropout\n         self.layerdrop = config.decoder_layerdrop\n-        self.padding_idx = embed_tokens.padding_idx\n+        self.padding_idx = config.pad_token_id\n         self.embed_scale = math.sqrt(config.d_model) if config.scale_embedding else 1.0\n-        self.embed_tokens = embed_tokens\n-        embed_dim = embed_tokens.embedding_dim\n+        self.embed_tokens = nn.Embedding(config.tgt_vocab_size, config.d_model, self.padding_idx)\n+        embed_dim = self.embed_tokens.embedding_dim\n         self.embed_positions = SinusoidalPositionalEmbedding(\n             config.max_position_embeddings + self.padding_idx + 1, embed_dim, self.padding_idx\n         )\n         self.layers = nn.ModuleList([DecoderLayer(config, layer_idx=i) for i in range(config.decoder_layers)])  # type: list[DecoderLayer]\n-\n-        if is_deepspeed_zero3_enabled():\n-            import deepspeed\n-\n-            with deepspeed.zero.GatheredParameters(self.embed_tokens.weight, modifier_rank=None):\n-                embed_tokens_weight_shape = self.embed_tokens.weight.shape\n-        else:\n-            embed_tokens_weight_shape = self.embed_tokens.weight.shape\n-        self.output_projection = nn.Linear(embed_tokens_weight_shape[1], embed_tokens_weight_shape[0], bias=False)\n-        self.output_projection.weight = self.embed_tokens.weight\n-\n-    def _tie_weights(self):\n-        self.embed_tokens.weight = self.output_projection.weight\n+        self.output_projection = nn.Linear(config.d_model, config.tgt_vocab_size, bias=False)\n \n     def forward(\n         self,\n@@ -828,29 +816,20 @@ def _get_shape(t):\n \n @auto_docstring\n class FSMTModel(PretrainedFSMTModel):\n-    _tied_weights_keys = [\"decoder.embed_tokens.weight\", \"decoder.output_projection.weight\"]\n+    _tied_weights_keys = {\n+        \"encoder.embed_tokens.weight\": \"decoder.embed_tokens.weight\",\n+        \"decoder.output_projection.weight\": \"decoder.embed_tokens.weight\",\n+    }\n \n     def __init__(self, config: FSMTConfig):\n         super().__init__(config)\n-\n-        padding_idx = config.pad_token_id\n-        encoder_embed_tokens = nn.Embedding(config.src_vocab_size, config.d_model, padding_idx)\n-        decoder_embed_tokens = nn.Embedding(config.tgt_vocab_size, config.d_model, padding_idx)\n-\n-        self.encoder = FSMTEncoder(config, encoder_embed_tokens)\n-        self.decoder = FSMTDecoder(config, decoder_embed_tokens)\n-\n-        # Initialize weights and apply final processing\n+        self.encoder = FSMTEncoder(config)\n+        self.decoder = FSMTDecoder(config)\n         self.post_init()\n \n     def get_encoder(self):\n         return self.encoder\n \n-    def _tie_weights(self):\n-        if self.config.tie_word_embeddings:\n-            self._tie_embedding_weights(self.decoder.embed_tokens, self.get_input_embeddings())\n-            self._tie_embedding_weights(self.decoder.output_projection, self.get_input_embeddings())\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -978,7 +957,6 @@ def set_output_embeddings(self, value):\n )\n class FSMTForConditionalGeneration(PretrainedFSMTModel, GenerationMixin):\n     base_model_prefix = \"model\"\n-    _tied_weights_keys = [\"decoder.embed_tokens.weight\", \"decoder.output_projection.weight\"]\n \n     def __init__(self, config: FSMTConfig):\n         super().__init__(config)"
        },
        {
            "sha": "7290c54e091ad5f7a896f997b766526d015edcb7",
            "filename": "src/transformers/models/funnel/modeling_funnel.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Ffunnel%2Fmodeling_funnel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Ffunnel%2Fmodeling_funnel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffunnel%2Fmodeling_funnel.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -672,6 +672,7 @@ class FunnelPreTrainedModel(PreTrainedModel):\n     config: FunnelConfig\n     base_model_prefix = \"funnel\"\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         classname = module.__class__.__name__\n         if classname.find(\"Linear\") != -1:\n@@ -694,7 +695,7 @@ def _init_weights(self, module):\n             std = 1.0 if self.config.initializer_std is None else self.config.initializer_std\n             nn.init.normal_(module.word_embeddings.weight, std=std)\n             if module.word_embeddings.padding_idx is not None:\n-                module.word_embeddings.weight.data[module.word_embeddings.padding_idx].zero_()\n+                module.word_embeddings.weight[module.word_embeddings.padding_idx].zero_()\n \n \n class FunnelClassificationHead(nn.Module):\n@@ -982,7 +983,7 @@ def forward(\n \n @auto_docstring\n class FunnelForMaskedLM(FunnelPreTrainedModel):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"funnel.embeddings.word_embeddings.weight\"}\n \n     def __init__(self, config: FunnelConfig) -> None:\n         super().__init__(config)"
        },
        {
            "sha": "0adb011378a568897bfb66199711938d70d8758e",
            "filename": "src/transformers/models/fuyu/modeling_fuyu.py",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -44,16 +44,17 @@ class FuyuPreTrainedModel(PreTrainedModel):\n     _no_split_modules = []\n     _skip_keys_device_placement = \"past_key_values\"\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         std = self.config.initializer_range\n         if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n \n \n @auto_docstring(\n@@ -257,7 +258,7 @@ class FuyuForCausalLM(FuyuPreTrainedModel, GenerationMixin):\n         \"^vision_embed_tokens\": \"model.vision_embed_tokens\",\n         \"^language_model.lm_head\": \"lm_head\",\n     }\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.language_model.embed_tokens.weight\"}\n \n     def __init__(self, config: FuyuConfig):\n         super().__init__(config)"
        },
        {
            "sha": "1acb039017dc8a8e375a2e5ab6d5db732ddd0afb",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -349,12 +349,13 @@ class GemmaPreTrainedModel(PreTrainedModel):\n         \"attentions\": GemmaAttention,\n     }\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         super()._init_weights(module)\n \n         # We initialize with 0s to be 1 centered as the RMSNorm here does (1 + weight)\n         if \"RMSNorm\" in module.__class__.__name__:\n-            module.weight.data.zero_()\n+            module.weight.zero_()\n \n \n @auto_docstring\n@@ -447,7 +448,7 @@ def forward(\n \n @auto_docstring\n class GemmaForCausalLM(GemmaPreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.embed_tokens.weight\"}\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n     _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n "
        },
        {
            "sha": "d1b3070a5ad006a250f3b34db728d81eb9a4af3d",
            "filename": "src/transformers/models/gemma/modular_gemma.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -394,12 +394,13 @@ def __init__(self, config: GemmaConfig, layer_idx: int):\n \n \n class GemmaPreTrainedModel(LlamaPreTrainedModel):\n+    @torch.no_grad()\n     def _init_weights(self, module):\n         PreTrainedModel._init_weights(self, module)\n \n         # We initialize with 0s to be 1 centered as the RMSNorm here does (1 + weight)\n         if \"RMSNorm\" in module.__class__.__name__:\n-            module.weight.data.zero_()\n+            module.weight.zero_()\n \n \n class GemmaModel(LlamaModel):"
        },
        {
            "sha": "6db748900375431666681d1a77e9d78b4bf6a917",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -381,12 +381,13 @@ class Gemma2PreTrainedModel(PreTrainedModel):\n         \"attentions\": Gemma2Attention,\n     }\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         super()._init_weights(module)\n \n         # We initialize with 0s to be 1 centered as the RMSNorm here does (1 + weight)\n         if \"RMSNorm\" in module.__class__.__name__:\n-            module.weight.data.zero_()\n+            module.weight.zero_()\n \n \n @auto_docstring\n@@ -519,7 +520,7 @@ def forward(\n \n @auto_docstring\n class Gemma2ForCausalLM(Gemma2PreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.embed_tokens.weight\"}\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n     _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n "
        },
        {
            "sha": "e17cbddfb4c61f73f8f47b3dec589e12d1dfb584",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -466,13 +466,14 @@ class Gemma3PreTrainedModel(PreTrainedModel):\n     }\n     input_modalities = [\"image\", \"text\"]\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, Gemma3MultiModalProjector):\n-            module.mm_input_projection_weight.data.zero_()\n+            module.mm_input_projection_weight.zero_()\n         # We initialize with 0s to be 1 centered as the RMSNorm here does (1 + weight)\n         elif \"RMSNorm\" in module.__class__.__name__:\n-            module.weight.data.zero_()\n+            module.weight.zero_()\n \n \n def _bidirectional_window_overlay(sliding_window: int) -> Callable[[int, int, int, int], bool]:\n@@ -626,7 +627,7 @@ def forward(\n \n @auto_docstring\n class Gemma3ForCausalLM(Gemma3PreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.embed_tokens.weight\"}\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n     _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n     config: Gemma3TextConfig\n@@ -1044,7 +1045,7 @@ class Gemma3ForConditionalGeneration(Gemma3PreTrainedModel, GenerationMixin):\n         \"^multi_modal_projector\": \"model.multi_modal_projector\",\n         \"^language_model.lm_head\": \"lm_head\",\n     }\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.language_model.embed_tokens.weight\"}\n     # we are filtering the logits/labels so we shouldn't divide the loss based on num_items_in_batch\n     # Fix: https://github.com/huggingface/transformers/issues/40564\n     accepts_loss_kwargs = False"
        },
        {
            "sha": "b3d34234dcd89d893853dfb620d54118682d6dd4",
            "filename": "src/transformers/models/gemma3/modular_gemma3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -569,13 +569,14 @@ class Gemma3PreTrainedModel(Gemma2PreTrainedModel):\n         \"SiglipMultiheadAttentionPoolingHead\",\n     ]\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         PreTrainedModel._init_weights(self, module)\n         if isinstance(module, Gemma3MultiModalProjector):\n-            module.mm_input_projection_weight.data.zero_()\n+            module.mm_input_projection_weight.zero_()\n         # We initialize with 0s to be 1 centered as the RMSNorm here does (1 + weight)\n         elif \"RMSNorm\" in module.__class__.__name__:\n-            module.weight.data.zero_()\n+            module.weight.zero_()\n \n \n def _bidirectional_window_overlay(sliding_window: int) -> Callable[[int, int, int, int], bool]:"
        },
        {
            "sha": "1f8631e156ec0fac259ba8d1c33f030b937a313f",
            "filename": "src/transformers/models/gemma3n/modeling_gemma3n.py",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -1601,14 +1601,15 @@ class Gemma3nPreTrainedModel(PreTrainedModel):\n     }\n     input_modalities = [\"image\", \"text\", \"audio\"]\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, Gemma3nAudioCumulativeGroupNorm):\n-            module.weight.data.fill_(1.0)\n+            module.weight.fill_(1.0)\n         elif isinstance(module, Gemma3nAudioAttention):\n-            module.per_dim_scale.data.zero_()\n+            module.per_dim_scale.zero_()\n         elif isinstance(module, Gemma3nTextAltUp):\n-            module.correct_output_scale.data.zero_()\n+            module.correct_output_scale.zero_()\n \n \n class Gemma3nRotaryEmbedding(nn.Module):\n@@ -1933,7 +1934,7 @@ def project_per_layer_inputs(\n \n @auto_docstring(custom_intro=\"The base Gemma 3n language model with a language modeling head.\")\n class Gemma3nForCausalLM(Gemma3nPreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.embed_tokens.weight\"}\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n     _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n     config: Gemma3nTextConfig\n@@ -2346,7 +2347,7 @@ def get_audio_features(\n )\n class Gemma3nForConditionalGeneration(Gemma3nPreTrainedModel, GenerationMixin):\n     _checkpoint_conversion_mapping = {}\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.language_model.embed_tokens.weight\"}\n     base_model_prefix = \"model\"\n \n     def __init__(self, config: Gemma3nConfig):"
        },
        {
            "sha": "7a58d2fc6313e6910aa50ed43618aaa07b9bbba8",
            "filename": "src/transformers/models/gemma3n/modular_gemma3n.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -1875,14 +1875,15 @@ class Gemma3nPreTrainedModel(Gemma2PreTrainedModel):\n     input_modalities = [\"image\", \"text\", \"audio\"]\n     _no_split_modules = [\"Gemma3nTextDecoderLayer\"]\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         PreTrainedModel._init_weights(self, module)\n         if isinstance(module, Gemma3nAudioCumulativeGroupNorm):\n-            module.weight.data.fill_(1.0)\n+            module.weight.fill_(1.0)\n         elif isinstance(module, Gemma3nAudioAttention):\n-            module.per_dim_scale.data.zero_()\n+            module.per_dim_scale.zero_()\n         elif isinstance(module, Gemma3nTextAltUp):\n-            module.correct_output_scale.data.zero_()\n+            module.correct_output_scale.zero_()\n \n \n @auto_docstring(custom_intro=\"The base Gemma 3n language model without a language modeling head.\")"
        },
        {
            "sha": "24ce421e1d5e5a7cc12e084b80d6d532e324c1c6",
            "filename": "src/transformers/models/git/modeling_git.py",
            "status": "modified",
            "additions": 8,
            "deletions": 7,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -388,23 +388,24 @@ class GitPreTrainedModel(PreTrainedModel):\n     input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, GitVisionEmbeddings):\n             nn.init.normal_(module.class_embedding, mean=0.0, std=self.config.initializer_range)\n             nn.init.normal_(module.patch_embedding.weight, std=self.config.initializer_range)\n             nn.init.normal_(module.position_embedding.weight, std=self.config.initializer_range)\n         if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n \n \n # Copied from transformers.models.clip.modeling_clip.CLIPVisionEmbeddings with CLIP->Git\n@@ -1119,7 +1120,7 @@ def forward(\n     \"\"\"\n )\n class GitForCausalLM(GitPreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"output.weight\"]\n+    _tied_weights_keys = {\"output.weight\": \"git.embeddings.word_embeddings.weight\"}\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "a4880c0145e9d9c1fdcd4e10395e4a537818a1f5",
            "filename": "src/transformers/models/glm/modeling_glm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -450,7 +450,7 @@ def forward(\n \n @auto_docstring\n class GlmForCausalLM(GlmPreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.embed_tokens.weight\"}\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n     _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n "
        },
        {
            "sha": "ba07da7cab548389eee8caf70444e547f2ac5b03",
            "filename": "src/transformers/models/glm4/modeling_glm4.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -454,7 +454,7 @@ def forward(\n \n @auto_docstring\n class Glm4ForCausalLM(Glm4PreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.embed_tokens.weight\"}\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n     _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n "
        },
        {
            "sha": "de56ee2ad2a7f73a64c9ba405308c695b0370036",
            "filename": "src/transformers/models/glm4_moe/modeling_glm4_moe.py",
            "status": "modified",
            "additions": 30,
            "deletions": 23,
            "changes": 53,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -330,37 +330,43 @@ def extra_repr(self):\n         return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n \n \n-class Glm4MoeNaiveMoe(nn.ModuleList):\n-    \"\"\"\n-    ModuleList of experts.\n-    \"\"\"\n+class Glm4MoeNaiveMoe(nn.Module):\n+    \"\"\"Collection of expert weights stored as 3D tensors.\"\"\"\n \n     def __init__(self, config):\n         super().__init__()\n         self.num_experts = config.num_local_experts\n-        for _ in range(self.num_experts):\n-            self.append(Glm4MoeMLP(config, intermediate_size=config.moe_intermediate_size))\n+        self.hidden_dim = config.hidden_size\n+        self.intermediate_dim = config.intermediate_size\n+        self.gate_up_proj = nn.Parameter(torch.empty(self.num_experts, 2 * self.intermediate_dim, self.hidden_dim))\n+        self.down_proj = nn.Parameter(torch.empty(self.num_experts, self.hidden_dim, self.intermediate_dim))\n+        self.act_fn = ACT2FN[config.hidden_act]\n \n     def forward(\n-        self, hidden_states: torch.Tensor, top_k_index: torch.Tensor, top_k_weights: torch.Tensor\n+        self,\n+        hidden_states: torch.Tensor,\n+        top_k_index: torch.Tensor,\n+        top_k_weights: torch.Tensor,\n     ) -> torch.Tensor:\n-        \"\"\"\n-        Args:\n-            hidden_states: (batch_size * sequence_length, hidden_dim)\n-            top_k_index: (batch_size * sequence_length, top_k)\n-            top_k_weights: (batch_size * sequence_length, top_k)\n-        Returns:\n-            (batch_size * sequence_length, hidden_dim)\n-        \"\"\"\n         final_hidden_states = torch.zeros_like(hidden_states)\n-        expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=self.num_experts).permute(2, 1, 0)\n+        num_experts = top_k_weights.shape[1]\n+        with torch.no_grad():\n+            expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=num_experts + 1)\n+            expert_mask = expert_mask.permute(2, 1, 0)\n+            expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n \n-        expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n         for expert_idx in expert_hit:\n-            idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))\n-            current_state = hidden_states[None, top_x].reshape(-1, hidden_states.shape[-1])\n-            current_hidden_states = self[expert_idx](current_state) * top_k_weights[top_x, idx, None]\n-            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n+            expert_idx = expert_idx[0]\n+            if expert_idx == num_experts:\n+                continue\n+            _, token_idx = torch.where(expert_mask[expert_idx])\n+            current_state = hidden_states[token_idx]\n+            gate, up = nn.functional.linear(current_state, self.gate_up_proj[expert_idx]).chunk(2, dim=-1)\n+            current_hidden_states = self.act_fn(gate) * up\n+            current_hidden_states = nn.functional.linear(current_hidden_states, self.down_proj[expert_idx])\n+            current_hidden_states = current_hidden_states * top_k_weights[token_idx, expert_idx, None]\n+            final_hidden_states.index_add_(0, token_idx, current_hidden_states.to(final_hidden_states.dtype))\n+\n         return final_hidden_states\n \n \n@@ -486,10 +492,11 @@ class Glm4MoePreTrainedModel(PreTrainedModel):\n         \"attentions\": Glm4MoeAttention,\n     }\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, Glm4MoeTopkRouter):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n \n \n @auto_docstring\n@@ -575,7 +582,7 @@ def forward(\n \n @auto_docstring\n class Glm4MoeForCausalLM(Glm4MoePreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.embed_tokens.weight\"}\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n     _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n "
        },
        {
            "sha": "1e3cc8de5ca9e17c53d07269cdc7d160744be3ae",
            "filename": "src/transformers/models/glm4v/modeling_glm4v.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -1364,7 +1364,7 @@ class Glm4vCausalLMOutputWithPast(ModelOutput):\n \n class Glm4vForConditionalGeneration(Glm4vPreTrainedModel, GenerationMixin):\n     _checkpoint_conversion_mapping = {}\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.language_model.embed_tokens.weight\"}\n     # Reference: fix gemma3 grad acc #37208\n     accepts_loss_kwargs = False\n "
        },
        {
            "sha": "6c46eeac851a6b4f81497fedba944230f50a3d37",
            "filename": "src/transformers/models/glm4v_moe/modeling_glm4v_moe.py",
            "status": "modified",
            "additions": 30,
            "deletions": 23,
            "changes": 53,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -351,37 +351,43 @@ def forward(self, hidden_states):\n         return router_logits\n \n \n-class Glm4vMoeTextNaiveMoe(nn.ModuleList):\n-    \"\"\"\n-    ModuleList of experts.\n-    \"\"\"\n+class Glm4vMoeTextNaiveMoe(nn.Module):\n+    \"\"\"Collection of expert weights stored as 3D tensors.\"\"\"\n \n     def __init__(self, config):\n         super().__init__()\n         self.num_experts = config.num_local_experts\n-        for _ in range(self.num_experts):\n-            self.append(Glm4vMoeTextMLP(config, intermediate_size=config.moe_intermediate_size))\n+        self.hidden_dim = config.hidden_size\n+        self.intermediate_dim = config.intermediate_size\n+        self.gate_up_proj = nn.Parameter(torch.empty(self.num_experts, 2 * self.intermediate_dim, self.hidden_dim))\n+        self.down_proj = nn.Parameter(torch.empty(self.num_experts, self.hidden_dim, self.intermediate_dim))\n+        self.act_fn = ACT2FN[config.hidden_act]\n \n     def forward(\n-        self, hidden_states: torch.Tensor, top_k_index: torch.Tensor, top_k_weights: torch.Tensor\n+        self,\n+        hidden_states: torch.Tensor,\n+        top_k_index: torch.Tensor,\n+        top_k_weights: torch.Tensor,\n     ) -> torch.Tensor:\n-        \"\"\"\n-        Args:\n-            hidden_states: (batch_size * sequence_length, hidden_dim)\n-            top_k_index: (batch_size * sequence_length, top_k)\n-            top_k_weights: (batch_size * sequence_length, top_k)\n-        Returns:\n-            (batch_size * sequence_length, hidden_dim)\n-        \"\"\"\n         final_hidden_states = torch.zeros_like(hidden_states)\n-        expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=self.num_experts).permute(2, 1, 0)\n+        num_experts = top_k_weights.shape[1]\n+        with torch.no_grad():\n+            expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=num_experts + 1)\n+            expert_mask = expert_mask.permute(2, 1, 0)\n+            expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n \n-        expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n         for expert_idx in expert_hit:\n-            idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))\n-            current_state = hidden_states[None, top_x].reshape(-1, hidden_states.shape[-1])\n-            current_hidden_states = self[expert_idx](current_state) * top_k_weights[top_x, idx, None]\n-            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n+            expert_idx = expert_idx[0]\n+            if expert_idx == num_experts:\n+                continue\n+            _, token_idx = torch.where(expert_mask[expert_idx])\n+            current_state = hidden_states[token_idx]\n+            gate, up = nn.functional.linear(current_state, self.gate_up_proj[expert_idx]).chunk(2, dim=-1)\n+            current_hidden_states = self.act_fn(gate) * up\n+            current_hidden_states = nn.functional.linear(current_hidden_states, self.down_proj[expert_idx])\n+            current_hidden_states = current_hidden_states * top_k_weights[token_idx, expert_idx, None]\n+            final_hidden_states.index_add_(0, token_idx, current_hidden_states.to(final_hidden_states.dtype))\n+\n         return final_hidden_states\n \n \n@@ -547,10 +553,11 @@ class Glm4vMoePreTrainedModel(PreTrainedModel):\n     }\n     input_modalities = [\"text\", \"image\", \"video\"]\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, Glm4vMoeTextTopkRouter):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n \n \n @dataclass\n@@ -1572,7 +1579,7 @@ def load_balancing_loss_func(\n \n class Glm4vMoeForConditionalGeneration(Glm4vMoePreTrainedModel, GenerationMixin):\n     _checkpoint_conversion_mapping = {}\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.language_model.embed_tokens.weight\"}\n     # Reference: fix gemma3 grad acc #37208\n     accepts_loss_kwargs = False\n "
        },
        {
            "sha": "4255ae22f47f65f4057885d2a656d07657c3d7ca",
            "filename": "src/transformers/models/glpn/modeling_glpn.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fglpn%2Fmodeling_glpn.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fglpn%2Fmodeling_glpn.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglpn%2Fmodeling_glpn.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -389,20 +389,20 @@ class GLPNPreTrainedModel(PreTrainedModel):\n     input_modalities = \"image\"\n     _no_split_modules = []\n \n-    # Copied from transformers.models.segformer.modeling_segformer.SegformerPreTrainedModel._init_weights\n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, (nn.LayerNorm, nn.BatchNorm2d)):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n \n \n @auto_docstring"
        },
        {
            "sha": "578fff8248171427903dfeb7c7db37596673f076",
            "filename": "src/transformers/models/got_ocr2/modeling_got_ocr2.py",
            "status": "modified",
            "additions": 9,
            "deletions": 11,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -276,7 +276,6 @@ def forward(self, hidden_states: torch.Tensor) -> tuple[torch.FloatTensor]:\n @auto_docstring\n class GotOcr2PreTrainedModel(PreTrainedModel):\n     config: GotOcr2Config\n-    base_model_prefix = \"\"\n     input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n     _skip_keys_device_placement = \"past_key_values\"\n@@ -287,15 +286,16 @@ class GotOcr2PreTrainedModel(PreTrainedModel):\n     _supports_flex_attn = False\n     _supports_attention_backend = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, GotOcr2VisionAttention):\n             if module.use_rel_pos:\n-                module.rel_pos_h.data.zero_()\n-                module.rel_pos_w.data.zero_()\n+                module.rel_pos_h.zero_()\n+                module.rel_pos_w.zero_()\n         elif isinstance(module, GotOcr2VisionEncoder):\n             if module.pos_embed is not None:\n-                module.pos_embed.data.zero_()\n+                module.pos_embed.zero_()\n \n \n @dataclass\n@@ -531,8 +531,6 @@ class GotOcr2ModelOutputWithPast(BaseModelOutputWithPast):\n     \"\"\"\n )\n class GotOcr2Model(GotOcr2PreTrainedModel):\n-    _checkpoint_conversion_mapping = {\"language_model.model\": \"language_model\"}\n-\n     def __init__(self, config: GotOcr2Config):\n         super().__init__(config)\n         self.vision_tower = GotOcr2VisionEncoder(config.vision_config)\n@@ -658,12 +656,12 @@ def forward(\n )\n class GotOcr2ForConditionalGeneration(GotOcr2PreTrainedModel, GenerationMixin):\n     _checkpoint_conversion_mapping = {\n-        \"^language_model.model\": \"model.language_model\",\n-        \"^vision_tower\": \"model.vision_tower\",\n-        \"^multi_modal_projector\": \"model.multi_modal_projector\",\n-        \"^language_model.lm_head\": \"lm_head\",\n+        r\"^language_model.model\": \"model.language_model\",\n+        r\"^vision_tower\": \"model.vision_tower\",\n+        r\"^multi_modal_projector\": \"model.multi_modal_projector\",\n+        r\"^language_model.lm_head\": \"lm_head\",\n     }\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.language_model.embed_tokens.weight\"}\n \n     def __init__(self, config: GotOcr2Config):\n         super().__init__(config)"
        },
        {
            "sha": "9312ed42ff3895ca773615d840b77cc8e65ee1f2",
            "filename": "src/transformers/models/got_ocr2/modular_got_ocr2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -289,15 +289,16 @@ class GotOcr2PreTrainedModel(LlavaPreTrainedModel):\n     _supports_sdpa = False\n     _supports_flex_attn = False\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         PreTrainedModel._init_weights(self, module)\n         if isinstance(module, GotOcr2VisionAttention):\n             if module.use_rel_pos:\n-                module.rel_pos_h.data.zero_()\n-                module.rel_pos_w.data.zero_()\n+                module.rel_pos_h.zero_()\n+                module.rel_pos_w.zero_()\n         elif isinstance(module, GotOcr2VisionEncoder):\n             if module.pos_embed is not None:\n-                module.pos_embed.data.zero_()\n+                module.pos_embed.zero_()\n \n \n class GotOcr2Model(LlavaModel):"
        },
        {
            "sha": "824a781c5b58567d53c8f50fee1aba33dd9111bc",
            "filename": "src/transformers/models/gpt2/modeling_gpt2.py",
            "status": "modified",
            "additions": 14,
            "deletions": 12,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -480,30 +480,32 @@ class GPT2PreTrainedModel(PreTrainedModel):\n     def __init__(self, *inputs, **kwargs):\n         super().__init__(*inputs, **kwargs)\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights.\"\"\"\n         if isinstance(module, (nn.Linear, Conv1D)):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n \n         # Reinitialize selected weights subject to the OpenAI GPT-2 Paper Scheme:\n         #   > A modified initialization which accounts for the accumulation on the residual path with model depth. Scale\n         #   > the weights of residual layers at initialization by a factor of 1/√N where N is the # of residual layers.\n         #   >   -- GPT-2 :: https://openai.com/blog/better-language-models/\n         #\n         # Reference (Megatron-LM): https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/model/gpt_model.py\n-        for name, p in module.named_parameters():\n-            if name == \"c_proj.weight\":\n-                # Special Scaled Initialization --> There are 2 Layer Norms per Transformer Block\n-                p.data.normal_(mean=0.0, std=(self.config.initializer_range / math.sqrt(2 * self.config.n_layer)))\n+        if isinstance(module, PreTrainedModel):\n+            for name, p in module.named_parameters():\n+                if name == \"c_proj.weight\":\n+                    # Special Scaled Initialization --> There are 2 Layer Norms per Transformer Block\n+                    p.normal_(mean=0.0, std=(self.config.initializer_range / math.sqrt(2 * self.config.n_layer)))\n \n \n @dataclass\n@@ -748,7 +750,7 @@ def forward(\n     \"\"\"\n )\n class GPT2LMHeadModel(GPT2PreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"transformer.wte.weight\"}\n \n     def __init__(self, config):\n         super().__init__(config)\n@@ -851,7 +853,7 @@ def forward(\n     \"\"\"\n )\n class GPT2DoubleHeadsModel(GPT2PreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"transformer.wte.weight\"}\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "ce2b34e775c3c13feb7079ecb7d8b594240ceaa9",
            "filename": "src/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py",
            "status": "modified",
            "additions": 9,
            "deletions": 8,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -362,6 +362,7 @@ class GPTBigCodePreTrainedModel(PreTrainedModel):\n     def __init__(self, *inputs, **kwargs):\n         super().__init__(*inputs, **kwargs)\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights.\"\"\"\n         if isinstance(module, (GPTBigCodeMLP, GPTBigCodeAttention)):\n@@ -371,21 +372,21 @@ def _init_weights(self, module):\n             #   >   -- GPT-2 :: https://openai.com/blog/better-language-models/\n             #\n             # Reference (Megatron-LM): https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/model/gpt_model.py\n-            module.c_proj.weight.data.normal_(\n+            module.c_proj.weight.normal_(\n                 mean=0.0, std=(self.config.initializer_range / math.sqrt(2 * self.config.n_layer))\n             )\n             module.c_proj._is_hf_initialized = True\n         elif isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n \n \n @auto_docstring\n@@ -577,7 +578,7 @@ def forward(\n     \"\"\"\n )\n class GPTBigCodeForCausalLM(GPTBigCodePreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"transformer.wte.weight\"}\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "c591ef2ec9148f25da29d083e8213df9ba8be581",
            "filename": "src/transformers/models/gpt_neo/modeling_gpt_neo.py",
            "status": "modified",
            "additions": 8,
            "deletions": 7,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -384,19 +384,20 @@ class GPTNeoPreTrainedModel(PreTrainedModel):\n     def __init__(self, *inputs, **kwargs):\n         super().__init__(*inputs, **kwargs)\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights.\"\"\"\n         if isinstance(module, (nn.Linear,)):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n \n \n @auto_docstring\n@@ -667,7 +668,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n     \"\"\"\n )\n class GPTNeoForCausalLM(GPTNeoPreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"transformer.wte.weight\"}\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "fc7d6fd40a80888b0ebc5aa87fd59e403c307da9",
            "filename": "src/transformers/models/gpt_neox/modeling_gpt_neox.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -517,7 +517,7 @@ def set_input_embeddings(self, value):\n     \"\"\"\n )\n class GPTNeoXForCausalLM(GPTNeoXPreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"embed_out.weight\"]\n+    _tied_weights_keys = {\"embed_out.weight\": \"gpt_neox.embed_in.weight\"}\n     _tp_plan = {\"embed_out\": \"colwise_rep\"}\n     _pp_plan = {\"embed_out\": ([\"hidden_states\"], [\"logits\"])}\n "
        },
        {
            "sha": "c267753db3504ee6b2b81b55978c4fbb823ef114",
            "filename": "src/transformers/models/gpt_neox/modular_gpt_neox.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodular_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodular_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodular_gpt_neox.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -390,7 +390,7 @@ def forward(\n     \"\"\"\n )\n class GPTNeoXForCausalLM(GPTNeoXPreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"embed_out.weight\"]\n+    _tied_weights_keys = {\"embed_out.weight\": \"gpt_neox.embed_in.weight\"}\n     _tp_plan = {\"embed_out\": \"colwise_rep\"}\n     _pp_plan = {\"embed_out\": ([\"hidden_states\"], [\"logits\"])}\n "
        },
        {
            "sha": "a906004dd41eda64c09b7de839c11ebf804a8b85",
            "filename": "src/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py",
            "status": "modified",
            "additions": 9,
            "deletions": 8,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -50,22 +50,23 @@ class GPTNeoXJapanesePreTrainedModel(PreTrainedModel):\n \n     _can_compile_fullgraph = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n         elif isinstance(module, GPTNeoXJapaneseAttention):\n             if module.dense_bias is not None:\n-                module.dense_bias.data.zero_()\n+                module.dense_bias.zero_()\n \n \n # Copied from transformers.models.llama.modeling_llama.LlamaRotaryEmbedding with Llama->GPTNeoXJapanese\n@@ -656,7 +657,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n     \"\"\"\n )\n class GPTNeoXJapaneseForCausalLM(GPTNeoXJapanesePreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"embed_out.weight\"]\n+    _tied_weights_keys = {\"embed_out.weight\": \"gpt_neox_japanese.embed_in.weight\"}\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "11e323544806a27bfc995167cb4fd8a247ed7a6b",
            "filename": "src/transformers/models/gpt_oss/modeling_gpt_oss.py",
            "status": "modified",
            "additions": 20,
            "deletions": 19,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -71,10 +71,10 @@ def __init__(self, config):\n         self.num_experts = config.num_local_experts\n         self.hidden_size = config.hidden_size\n         self.expert_dim = self.intermediate_size\n-        self.gate_up_proj = nn.Parameter(torch.empty(self.num_experts, self.hidden_size, 2 * self.expert_dim))\n-        self.gate_up_proj_bias = nn.Parameter(torch.empty(self.num_experts, 2 * self.expert_dim))\n+        self.gate_up_proj = nn.Parameter(torch.zeros(self.num_experts, self.hidden_size, 2 * self.expert_dim))\n+        self.gate_up_proj_bias = nn.Parameter(torch.zeros(self.num_experts, 2 * self.expert_dim))\n         self.down_proj = nn.Parameter(torch.empty((self.num_experts, self.expert_dim, self.hidden_size)))\n-        self.down_proj_bias = nn.Parameter(torch.empty(self.num_experts, self.hidden_size))\n+        self.down_proj_bias = nn.Parameter(torch.zeros(self.num_experts, self.hidden_size))\n         self.alpha = 1.702\n         self.limit = 7.0\n \n@@ -146,8 +146,8 @@ def __init__(self, config):\n         self.top_k = config.num_experts_per_tok\n         self.num_experts = config.num_local_experts\n         self.hidden_dim = config.hidden_size\n-        self.weight = nn.Parameter(torch.empty(self.num_experts, self.hidden_dim))\n-        self.bias = nn.Parameter(torch.empty(self.num_experts))\n+        self.weight = nn.Parameter(torch.zeros(self.num_experts, self.hidden_dim))\n+        self.bias = nn.Parameter(torch.zeros(self.num_experts))\n \n     def forward(self, hidden_states):\n         hidden_states = hidden_states.reshape(-1, self.hidden_dim)\n@@ -440,30 +440,31 @@ class GptOssPreTrainedModel(PreTrainedModel):\n     _supports_flash_attention = False\n     _supports_flex_attention = False\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         std = self.config.initializer_range\n         if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Parameter):\n-            module.data.normal_(mean=0.0, std=std)\n+            module.normal_(mean=0.0, std=std)\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, GptOssRMSNorm):\n-            module.weight.data.fill_(1.0)\n+            module.weight.fill_(1.0)\n         elif isinstance(module, GptOssExperts):\n-            module.gate_up_proj.data.normal_(mean=0.0, std=std)\n-            module.gate_up_proj_bias.data.zero_()\n-            module.down_proj.data.normal_(mean=0.0, std=std)\n-            module.down_proj_bias.data.zero_()\n+            module.gate_up_proj.normal_(mean=0.0, std=std)\n+            module.gate_up_proj_bias.zero_()\n+            module.down_proj.normal_(mean=0.0, std=std)\n+            module.down_proj_bias.zero_()\n         elif isinstance(module, GptOssAttention):\n-            module.sinks.data.normal_(mean=0.0, std=std)\n+            module.sinks.normal_(mean=0.0, std=std)\n         elif isinstance(module, GptOssTopKRouter):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            module.bias.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n+            module.bias.normal_(mean=0.0, std=std)\n \n \n @auto_docstring\n@@ -635,7 +636,7 @@ def load_balancing_loss_func(\n \n @auto_docstring\n class GptOssForCausalLM(GptOssPreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.embed_tokens.weight\"}\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n     _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n "
        },
        {
            "sha": "4f33517001b3fb239f8087afe8dd90573c31baf5",
            "filename": "src/transformers/models/gpt_oss/modular_gpt_oss.py",
            "status": "modified",
            "additions": 19,
            "deletions": 18,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodular_gpt_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodular_gpt_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodular_gpt_oss.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -69,10 +69,10 @@ def __init__(self, config):\n         self.num_experts = config.num_local_experts\n         self.hidden_size = config.hidden_size\n         self.expert_dim = self.intermediate_size\n-        self.gate_up_proj = nn.Parameter(torch.empty(self.num_experts, self.hidden_size, 2 * self.expert_dim))\n-        self.gate_up_proj_bias = nn.Parameter(torch.empty(self.num_experts, 2 * self.expert_dim))\n+        self.gate_up_proj = nn.Parameter(torch.zeros(self.num_experts, self.hidden_size, 2 * self.expert_dim))\n+        self.gate_up_proj_bias = nn.Parameter(torch.zeros(self.num_experts, 2 * self.expert_dim))\n         self.down_proj = nn.Parameter(torch.empty((self.num_experts, self.expert_dim, self.hidden_size)))\n-        self.down_proj_bias = nn.Parameter(torch.empty(self.num_experts, self.hidden_size))\n+        self.down_proj_bias = nn.Parameter(torch.zeros(self.num_experts, self.hidden_size))\n         self.alpha = 1.702\n         self.limit = 7.0\n \n@@ -144,8 +144,8 @@ def __init__(self, config):\n         self.top_k = config.num_experts_per_tok\n         self.num_experts = config.num_local_experts\n         self.hidden_dim = config.hidden_size\n-        self.weight = nn.Parameter(torch.empty(self.num_experts, self.hidden_dim))\n-        self.bias = nn.Parameter(torch.empty(self.num_experts))\n+        self.weight = nn.Parameter(torch.zeros(self.num_experts, self.hidden_dim))\n+        self.bias = nn.Parameter(torch.zeros(self.num_experts))\n \n     def forward(self, hidden_states):\n         hidden_states = hidden_states.reshape(-1, self.hidden_dim)\n@@ -356,30 +356,31 @@ class GptOssPreTrainedModel(LlamaPreTrainedModel):\n         \"attentions\": GptOssAttention,\n     }\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         std = self.config.initializer_range\n         if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Parameter):\n-            module.data.normal_(mean=0.0, std=std)\n+            module.normal_(mean=0.0, std=std)\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, GptOssRMSNorm):\n-            module.weight.data.fill_(1.0)\n+            module.weight.fill_(1.0)\n         elif isinstance(module, GptOssExperts):\n-            module.gate_up_proj.data.normal_(mean=0.0, std=std)\n-            module.gate_up_proj_bias.data.zero_()\n-            module.down_proj.data.normal_(mean=0.0, std=std)\n-            module.down_proj_bias.data.zero_()\n+            module.gate_up_proj.normal_(mean=0.0, std=std)\n+            module.gate_up_proj_bias.zero_()\n+            module.down_proj.normal_(mean=0.0, std=std)\n+            module.down_proj_bias.zero_()\n         elif isinstance(module, GptOssAttention):\n-            module.sinks.data.normal_(mean=0.0, std=std)\n+            module.sinks.normal_(mean=0.0, std=std)\n         elif isinstance(module, GptOssTopKRouter):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            module.bias.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n+            module.bias.normal_(mean=0.0, std=std)\n \n \n class GptOssModel(MixtralModel):"
        },
        {
            "sha": "8d8004577e57b69d857ffac425861a5d8a4bcd59",
            "filename": "src/transformers/models/gptj/modeling_gptj.py",
            "status": "modified",
            "additions": 8,
            "deletions": 7,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -447,19 +447,20 @@ class GPTJPreTrainedModel(PreTrainedModel):\n     def __init__(self, *inputs, **kwargs):\n         super().__init__(*inputs, **kwargs)\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights.\"\"\"\n         if isinstance(module, (nn.Linear,)):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n \n \n @auto_docstring\n@@ -722,7 +723,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n     \"\"\"\n )\n class GPTJForCausalLM(GPTJPreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"transformer.wte.weight\"}\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "42de2e0724f3efc1598970370abaae94858029a3",
            "filename": "src/transformers/models/granite/modeling_granite.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -502,7 +502,7 @@ def forward(\n \n @auto_docstring\n class GraniteForCausalLM(GranitePreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.embed_tokens.weight\"}\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n     _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n "
        },
        {
            "sha": "07e7c2573e99861b5b0f4d29672534753fd7b4b6",
            "filename": "src/transformers/models/granite_speech/modeling_granite_speech.py",
            "status": "modified",
            "additions": 8,
            "deletions": 10,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fmodeling_granite_speech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fmodeling_granite_speech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fmodeling_granite_speech.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -286,23 +286,24 @@ class GraniteSpeechPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = False  # `blip_2_qformer` dependency does not allow for this\n     _supports_sdpa = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module: nn.Module):\n         \"\"\"Initialize the weights.\"\"\"\n         std = self.config.initializer_range\n \n         if isinstance(module, (nn.Linear, nn.Conv1d)):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, (nn.LayerNorm, nn.BatchNorm1d)):\n-            module.weight.data.fill_(1.0)\n-            module.bias.data.zero_()\n+            module.weight.fill_(1.0)\n+            module.bias.zero_()\n         elif isinstance(module, GraniteSpeechEncoderProjector):\n-            module.query.data.normal_()\n+            module.query.normal_()\n \n \n @auto_docstring(\n@@ -319,9 +320,6 @@ def __init__(self, config: GraniteSpeechConfig):\n         # model; don't need to consider it twice\n         self.language_model = AutoModelForCausalLM.from_config(config.text_config)\n \n-        if self.language_model._tied_weights_keys is not None:\n-            self._tied_weights_keys = [f\"language_model.{k}\" for k in self.language_model._tied_weights_keys]\n-\n         self.encoder = GraniteSpeechCTCEncoder(config.encoder_config)\n         self.projector = GraniteSpeechEncoderProjector(config)\n "
        },
        {
            "sha": "0b3a893b98834a5b1a4c06f7aba0c9174eb1ad8e",
            "filename": "src/transformers/models/granitemoe/modeling_granitemoe.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -411,10 +411,9 @@ def __init__(self, config: GraniteMoeConfig, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n         self.self_attn = GraniteMoeAttention(config=config, layer_idx=layer_idx)\n-        self.block_sparse_moe = GraniteMoeMoE(config)\n         self.input_layernorm = GraniteMoeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = GraniteMoeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n-\n+        self.block_sparse_moe = GraniteMoeMoE(config)\n         self.residual_multiplier = config.residual_multiplier  # Only diff with mixtral!\n \n     def forward(\n@@ -462,10 +461,11 @@ class GraniteMoePreTrainedModel(PreTrainedModel):\n         \"attentions\": GraniteMoeAttention,\n     }\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, GraniteMoeParallelExperts):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n \n \n @auto_docstring\n@@ -635,7 +635,7 @@ def load_balancing_loss_func(\n \n @auto_docstring\n class GraniteMoeForCausalLM(GraniteMoePreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.embed_tokens.weight\"}\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n     _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n "
        },
        {
            "sha": "53692da91773efa454e48d54c2dc5848cbec37eb",
            "filename": "src/transformers/models/granitemoe/modular_granitemoe.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodular_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodular_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodular_granitemoe.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -105,7 +105,8 @@ def __init__(self, config: GraniteMoeConfig, layer_idx: int):\n         self.block_sparse_moe = GraniteMoeMoE(config)\n         self.input_layernorm = GraniteMoeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = GraniteMoeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n-\n+        del self.mlp\n+        self.block_sparse_moe = GraniteMoeMoE(config)\n         self.residual_multiplier = config.residual_multiplier  # Only diff with mixtral!\n \n     def forward(\n@@ -147,10 +148,11 @@ class GraniteMoePreTrainedModel(LlamaPreTrainedModel, PreTrainedModel):\n \n     _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         PreTrainedModel._init_weights(self, module)\n         if isinstance(module, GraniteMoeParallelExperts):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n \n \n @auto_docstring"
        },
        {
            "sha": "dc39370b75599b4d1e83c47a6cf1c896b4b09778",
            "filename": "src/transformers/models/granitemoehybrid/modeling_granitemoehybrid.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -1119,10 +1119,9 @@ def __init__(self, config: GraniteMoeHybridConfig, layer_idx: int):\n         self.hidden_size = config.hidden_size\n         # Either attention or mamba will be initialized, depending on the layer type.\n         self.self_attn = None\n-        self.block_sparse_moe = GraniteMoeHybridMoE(config)\n         self.input_layernorm = GraniteMoeHybridRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = GraniteMoeHybridRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n-\n+        self.block_sparse_moe = GraniteMoeHybridMoE(config)\n         self.residual_multiplier = config.residual_multiplier  # Only diff with mixtral!\n         self.shared_mlp = GraniteMoeHybridMLP(config)\n         self.mamba = None\n@@ -1202,16 +1201,17 @@ class GraniteMoeHybridPreTrainedModel(PreTrainedModel):\n     }\n     _is_stateful = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, GraniteMoeHybridParallelExperts):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n         if isinstance(module, GraniteMoeHybridMambaLayer):\n-            module.dt_bias.data.fill_(1.0)\n-            module.A_log.data = torch.log(torch.arange(1, module.num_heads + 1))\n-            module.D.data.fill_(1.0)\n+            module.dt_bias.fill_(1.0)\n+            module.A_log.copy_(torch.log(torch.arange(1, module.num_heads + 1)))\n+            module.D.fill_(1.0)\n         elif isinstance(module, GraniteMoeHybridRMSNormGated):\n-            module.weight.data.fill_(1.0)\n+            module.weight.fill_(1.0)\n \n \n @auto_docstring\n@@ -1395,7 +1395,7 @@ def load_balancing_loss_func(\n \n @auto_docstring\n class GraniteMoeHybridForCausalLM(GraniteMoeHybridPreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.embed_tokens.weight\"}\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n     _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n "
        },
        {
            "sha": "ed0676752fbc31394bc839aaf887cf3760457db3",
            "filename": "src/transformers/models/granitemoehybrid/modular_granitemoehybrid.py",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodular_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodular_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodular_granitemoehybrid.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -176,14 +176,15 @@ class GraniteMoeHybridPreTrainedModel(GraniteMoeSharedPreTrainedModel):\n     _no_split_modules = [\"GraniteMoeHybridDecoderLayer\"]\n     _is_stateful = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, GraniteMoeHybridMambaLayer):\n-            module.dt_bias.data.fill_(1.0)\n-            module.A_log.data = torch.log(torch.arange(1, module.num_heads + 1))\n-            module.D.data.fill_(1.0)\n+            module.dt_bias.fill_(1.0)\n+            module.A_log.copy_(torch.log(torch.arange(1, module.num_heads + 1)))\n+            module.D.fill_(1.0)\n         elif isinstance(module, GraniteMoeHybridRMSNormGated):\n-            module.weight.data.fill_(1.0)\n+            module.weight.fill_(1.0)\n \n \n class GraniteMoeHybridModel(GraniteMoeSharedModel):\n@@ -273,7 +274,7 @@ def _update_mamba_mask(self, attention_mask, cache_position):\n \n \n class GraniteMoeHybridForCausalLM(GraniteMoeSharedForCausalLM):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.embed_tokens.weight\"}\n \n     def __init__(self, config: GraniteMoeHybridConfig):\n         super().__init__(config)"
        },
        {
            "sha": "d2f228d0f19709198d41529e0a7c6f9eb4949ae3",
            "filename": "src/transformers/models/granitemoeshared/modeling_granitemoeshared.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -401,10 +401,9 @@ def __init__(self, config: GraniteMoeSharedConfig, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n         self.self_attn = GraniteMoeSharedAttention(config=config, layer_idx=layer_idx)\n-        self.block_sparse_moe = GraniteMoeSharedMoE(config)\n         self.input_layernorm = GraniteMoeSharedRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = GraniteMoeSharedRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n-\n+        self.block_sparse_moe = GraniteMoeSharedMoE(config)\n         self.residual_multiplier = config.residual_multiplier  # Only diff with mixtral!\n         self.shared_mlp = None if config.shared_intermediate_size == 0 else GraniteMoeSharedMLP(config)\n \n@@ -468,10 +467,11 @@ class GraniteMoeSharedPreTrainedModel(PreTrainedModel):\n         \"attentions\": GraniteMoeSharedAttention,\n     }\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, GraniteMoeSharedParallelExperts):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n \n \n class GraniteMoeSharedRotaryEmbedding(nn.Module):\n@@ -706,7 +706,7 @@ def load_balancing_loss_func(\n \n @auto_docstring\n class GraniteMoeSharedForCausalLM(GraniteMoeSharedPreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.embed_tokens.weight\"}\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n     _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n "
        },
        {
            "sha": "4bc8f66e85c957161117e20d7d255804e0abaf9e",
            "filename": "src/transformers/models/granitemoeshared/modular_granitemoeshared.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodular_granitemoeshared.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodular_granitemoeshared.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodular_granitemoeshared.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -146,7 +146,7 @@ def __init__(self, config: GraniteMoeSharedConfig):\n \n \n class GraniteMoeSharedForCausalLM(GraniteMoeForCausalLM):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.embed_tokens.weight\"}\n \n     def __init__(self, config: GraniteMoeSharedConfig):\n         super().__init__(config)"
        },
        {
            "sha": "560c59191a012dc84a68e8e6516f3b59fcccc3b7",
            "filename": "src/transformers/models/grounding_dino/configuration_grounding_dino.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fconfiguration_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fconfiguration_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fconfiguration_grounding_dino.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -286,6 +286,8 @@ def __init__(\n         self.init_std = init_std\n         self.layer_norm_eps = layer_norm_eps\n         super().__init__(is_encoder_decoder=is_encoder_decoder, **kwargs)\n+        self.tie_encoder_decoder = True\n+        self.tie_encoder_decoder = True\n \n \n __all__ = [\"GroundingDinoConfig\"]"
        },
        {
            "sha": "5333f222fb39283445ffd9b33dcfe7c86f7d172a",
            "filename": "src/transformers/models/grounding_dino/modeling_grounding_dino.py",
            "status": "modified",
            "additions": 52,
            "deletions": 56,
            "changes": 108,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -351,10 +351,10 @@ def replace_batch_norm(model):\n             new_module = GroundingDinoFrozenBatchNorm2d(module.num_features)\n \n             if module.weight.device != torch.device(\"meta\"):\n-                new_module.weight.data.copy_(module.weight)\n-                new_module.bias.data.copy_(module.bias)\n-                new_module.running_mean.data.copy_(module.running_mean)\n-                new_module.running_var.data.copy_(module.running_var)\n+                new_module.weight.copy_(module.weight)\n+                new_module.bias.copy_(module.bias)\n+                new_module.running_mean.copy_(module.running_mean)\n+                new_module.running_var.copy_(module.running_var)\n \n             model._modules[name] = new_module\n \n@@ -1369,14 +1369,15 @@ class GroundingDinoPreTrainedModel(PreTrainedModel):\n     main_input_name = \"pixel_values\"\n     input_modalities = [\"image\", \"text\"]\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         std = self.config.init_std\n \n         if isinstance(module, GroundingDinoLearnedPositionEmbedding):\n             nn.init.uniform_(module.row_embeddings.weight)\n             nn.init.uniform_(module.column_embeddings.weight)\n         elif isinstance(module, GroundingDinoMultiscaleDeformableAttention):\n-            nn.init.constant_(module.sampling_offsets.weight.data, 0.0)\n+            nn.init.constant_(module.sampling_offsets.weight, 0.0)\n             default_dtype = torch.get_default_dtype()\n             thetas = torch.arange(module.n_heads, dtype=torch.int64).to(default_dtype) * (\n                 2.0 * math.pi / module.n_heads\n@@ -1391,46 +1392,46 @@ def _init_weights(self, module):\n                 grid_init[:, :, i, :] *= i + 1\n             with torch.no_grad():\n                 module.sampling_offsets.bias = nn.Parameter(grid_init.view(-1))\n-            nn.init.constant_(module.attention_weights.weight.data, 0.0)\n-            nn.init.constant_(module.attention_weights.bias.data, 0.0)\n-            nn.init.xavier_uniform_(module.value_proj.weight.data)\n-            nn.init.constant_(module.value_proj.bias.data, 0.0)\n-            nn.init.xavier_uniform_(module.output_proj.weight.data)\n-            nn.init.constant_(module.output_proj.bias.data, 0.0)\n+            nn.init.constant_(module.attention_weights.weight, 0.0)\n+            nn.init.constant_(module.attention_weights.bias, 0.0)\n+            nn.init.xavier_uniform_(module.value_proj.weight)\n+            nn.init.constant_(module.value_proj.bias, 0.0)\n+            nn.init.xavier_uniform_(module.output_proj.weight)\n+            nn.init.constant_(module.output_proj.bias, 0.0)\n         elif isinstance(module, GroundingDinoBiMultiHeadAttention):\n             nn.init.xavier_uniform_(module.vision_proj.weight)\n-            module.vision_proj.bias.data.fill_(0)\n+            module.vision_proj.bias.fill_(0)\n             nn.init.xavier_uniform_(module.text_proj.weight)\n-            module.text_proj.bias.data.fill_(0)\n+            module.text_proj.bias.fill_(0)\n             nn.init.xavier_uniform_(module.values_vision_proj.weight)\n-            module.values_vision_proj.bias.data.fill_(0)\n+            module.values_vision_proj.bias.fill_(0)\n             nn.init.xavier_uniform_(module.values_text_proj.weight)\n-            module.values_text_proj.bias.data.fill_(0)\n+            module.values_text_proj.bias.fill_(0)\n             nn.init.xavier_uniform_(module.out_vision_proj.weight)\n-            module.out_vision_proj.bias.data.fill_(0)\n+            module.out_vision_proj.bias.fill_(0)\n             nn.init.xavier_uniform_(module.out_text_proj.weight)\n-            module.out_text_proj.bias.data.fill_(0)\n+            module.out_text_proj.bias.fill_(0)\n         elif isinstance(module, GroundingDinoFusionLayer):\n-            module.vision_param.data.fill_(1e-4)\n-            module.text_param.data.fill_(1e-4)\n+            module.vision_param.fill_(1e-4)\n+            module.text_param.fill_(1e-4)\n         elif isinstance(module, (nn.Linear, nn.Conv2d, nn.BatchNorm2d)):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, (nn.LayerNorm, nn.GroupNorm)):\n-            module.weight.data.fill_(1.0)\n-            module.bias.data.zero_()\n+            module.weight.fill_(1.0)\n+            module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, GroundingDinoMLPPredictionHead):\n-            nn.init.constant_(module.layers[-1].weight.data, 0)\n-            nn.init.constant_(module.layers[-1].bias.data, 0)\n+            nn.init.constant_(module.layers[-1].weight, 0)\n+            nn.init.constant_(module.layers[-1].bias, 0)\n \n         if hasattr(module, \"reference_points\") and not self.config.two_stage:\n-            nn.init.xavier_uniform_(module.reference_points.weight.data, gain=1.0)\n-            nn.init.constant_(module.reference_points.bias.data, 0.0)\n+            nn.init.xavier_uniform_(module.reference_points.weight, gain=1.0)\n+            nn.init.constant_(module.reference_points.bias, 0.0)\n         if hasattr(module, \"level_embed\"):\n             nn.init.normal_(module.level_embed)\n \n@@ -2412,41 +2413,36 @@ def build_text_mask(logits, attention_mask):\n class GroundingDinoForObjectDetection(GroundingDinoPreTrainedModel):\n     # When using clones, all layers > 0 will be clones, but layer 0 *is* required\n     # the bbox_embed in the decoder are all clones though\n-    _tied_weights_keys = [r\"bbox_embed\\.[1-9]\\d*\", r\"model\\.decoder\\.bbox_embed\\.[0-9]\\d*\"]\n+    _tied_weights_keys = {\n+        r\"bbox_embed.(?![0])\\d+\": \"bbox_embed.0\",\n+        \"model.decoder.bbox_embed\": \"bbox_embed\",\n+    }\n \n     def __init__(self, config: GroundingDinoConfig):\n         super().__init__(config)\n \n         self.model = GroundingDinoModel(config)\n-        _class_embed = GroundingDinoContrastiveEmbedding(config)\n-\n-        if config.decoder_bbox_embed_share:\n-            # a single shared instance\n-            shared_head = GroundingDinoMLPPredictionHead(\n-                input_dim=config.d_model, hidden_dim=config.d_model, output_dim=4, num_layers=3\n-            )\n-            self.bbox_embed = nn.ModuleList([shared_head] * config.decoder_layers)\n-        else:\n-            # each layer has its own head (implicit deep copy through a new instance)\n-            self.bbox_embed = nn.ModuleList(\n-                [\n-                    GroundingDinoMLPPredictionHead(\n-                        input_dim=config.d_model,\n-                        hidden_dim=config.d_model,\n-                        output_dim=4,\n-                        num_layers=3,\n-                    )\n-                    for _ in range(config.decoder_layers)\n-                ]\n-            )\n+        if not config.decoder_bbox_embed_share:\n+            del self._tied_weights_keys[r\"bbox_embed.(?![0])\\d+\"]\n+\n+        self.bbox_embed = nn.ModuleList(\n+            [\n+                GroundingDinoMLPPredictionHead(\n+                    input_dim=config.d_model,\n+                    hidden_dim=config.d_model,\n+                    output_dim=4,\n+                    num_layers=3,\n+                )\n+                for _ in range(config.decoder_layers)\n+            ]\n+        )\n \n-        self.class_embed = nn.ModuleList([_class_embed for _ in range(config.decoder_layers)])\n+        self.class_embed = nn.ModuleList(\n+            [GroundingDinoContrastiveEmbedding(config) for _ in range(config.decoder_layers)]\n+        )\n         # hack for box-refinement\n+        self.model.decoder.class_embed = self.class_embed  # class embed has no weights so nothing to tie\n         self.model.decoder.bbox_embed = self.bbox_embed\n-        # hack implementation for two-stage\n-        self.model.decoder.class_embed = self.class_embed\n-\n-        # Initialize weights and apply final processing\n         self.post_init()\n \n     @auto_docstring"
        },
        {
            "sha": "0c51c9052afcb68f611cd7a0e6f861a25d266562",
            "filename": "src/transformers/models/groupvit/modeling_groupvit.py",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fmodeling_groupvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fmodeling_groupvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fmodeling_groupvit.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -748,22 +748,23 @@ class GroupViTPreTrainedModel(PreTrainedModel):\n     input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n \n         init_range = self.config.initializer_range\n         if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.data.normal_(mean=0.0, std=init_range)\n+            module.weight.normal_(mean=0.0, std=init_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n \n         factor = self.config.initializer_factor\n         if isinstance(module, GroupViTTextEmbeddings):\n-            module.token_embedding.weight.data.normal_(mean=0.0, std=factor * 0.02)\n-            module.position_embedding.weight.data.normal_(mean=0.0, std=factor * 0.02)\n+            module.token_embedding.weight.normal_(mean=0.0, std=factor * 0.02)\n+            module.position_embedding.weight.normal_(mean=0.0, std=factor * 0.02)\n         elif isinstance(module, GroupViTAttention):\n             factor = self.config.initializer_factor\n             in_proj_std = (module.embed_dim**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor"
        },
        {
            "sha": "2e7626714834ff28f23a96697a8658766ab19dc0",
            "filename": "src/transformers/models/helium/modeling_helium.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -433,7 +433,7 @@ def forward(\n \n @auto_docstring\n class HeliumForCausalLM(HeliumPreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.embed_tokens.weight\"}\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n     _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n "
        },
        {
            "sha": "85cfa57ca7d874c5da172c134ee80ae930f45fd8",
            "filename": "src/transformers/models/hiera/modeling_hiera.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fhiera%2Fmodeling_hiera.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fhiera%2Fmodeling_hiera.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhiera%2Fmodeling_hiera.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -776,6 +776,7 @@ class HieraPreTrainedModel(PreTrainedModel):\n     input_modalities = \"image\"\n     supports_gradient_checkpointing = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module) -> None:\n         \"\"\"Initialize the weights\"\"\"\n         std = self.config.initializer_range"
        },
        {
            "sha": "84a8c98749fcdfaa7196f8e606e2688ed42044f6",
            "filename": "src/transformers/models/hubert/modeling_hubert.py",
            "status": "modified",
            "additions": 12,
            "deletions": 11,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -638,36 +638,37 @@ class HubertPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_flex_attn = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, (nn.LayerNorm, nn.GroupNorm, nn.BatchNorm1d)):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n         elif isinstance(module, nn.Conv1d):\n             if is_deepspeed_zero3_enabled():\n                 import deepspeed\n \n                 if hasattr(module, \"weight_v\") and hasattr(module, \"weight_g\"):\n                     with deepspeed.zero.GatheredParameters([module.weight_v, module.weight_g], modifier_rank=0):\n-                        nn.init.kaiming_normal_(module.weight.data)\n+                        nn.init.kaiming_normal_(module.weight)\n                 else:\n                     with deepspeed.zero.GatheredParameters(module.weight, modifier_rank=0):\n-                        nn.init.kaiming_normal_(module.weight.data)\n+                        nn.init.kaiming_normal_(module.weight)\n             else:\n-                nn.init.kaiming_normal_(module.weight.data)\n+                nn.init.kaiming_normal_(module.weight)\n \n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, HubertModel):\n             if hasattr(module, \"masked_spec_embed\"):\n-                module.masked_spec_embed.data.uniform_()\n+                module.masked_spec_embed.uniform_()\n         elif isinstance(module, HubertForSequenceClassification):\n             if hasattr(module, \"layer_weights\"):\n-                module.layer_weights.data.fill_(1.0 / (self.config.num_hidden_layers + 1))\n+                module.layer_weights.fill_(1.0 / (self.config.num_hidden_layers + 1))\n \n     def _get_feat_extract_output_lengths(self, input_lengths: Union[torch.LongTensor, int]):\n         \"\"\"\n@@ -992,7 +993,7 @@ def __init__(self, config, target_lang: Optional[str] = None):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def tie_weights(self):\n+    def tie_weights(self, missing_keys=None):\n         \"\"\"\n         This method overwrites [`~PreTrainedModel.tie_weights`] so that adapter weights can be correctly loaded when\n         passing `target_lang=...` to `from_pretrained(...)`."
        },
        {
            "sha": "d23cbc489b0941ddf5a53f0919b4ebd7efac933e",
            "filename": "src/transformers/models/hubert/modular_hubert.py",
            "status": "modified",
            "additions": 11,
            "deletions": 10,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodular_hubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodular_hubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodular_hubert.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -134,36 +134,37 @@ class HubertPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_flex_attn = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, (nn.LayerNorm, nn.GroupNorm, nn.BatchNorm1d)):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n         elif isinstance(module, nn.Conv1d):\n             if is_deepspeed_zero3_enabled():\n                 import deepspeed\n \n                 if hasattr(module, \"weight_v\") and hasattr(module, \"weight_g\"):\n                     with deepspeed.zero.GatheredParameters([module.weight_v, module.weight_g], modifier_rank=0):\n-                        nn.init.kaiming_normal_(module.weight.data)\n+                        nn.init.kaiming_normal_(module.weight)\n                 else:\n                     with deepspeed.zero.GatheredParameters(module.weight, modifier_rank=0):\n-                        nn.init.kaiming_normal_(module.weight.data)\n+                        nn.init.kaiming_normal_(module.weight)\n             else:\n-                nn.init.kaiming_normal_(module.weight.data)\n+                nn.init.kaiming_normal_(module.weight)\n \n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, HubertModel):\n             if hasattr(module, \"masked_spec_embed\"):\n-                module.masked_spec_embed.data.uniform_()\n+                module.masked_spec_embed.uniform_()\n         elif isinstance(module, HubertForSequenceClassification):\n             if hasattr(module, \"layer_weights\"):\n-                module.layer_weights.data.fill_(1.0 / (self.config.num_hidden_layers + 1))\n+                module.layer_weights.fill_(1.0 / (self.config.num_hidden_layers + 1))\n \n     def _get_feat_extract_output_lengths(self, input_lengths: Union[torch.LongTensor, int]):\n         \"\"\""
        },
        {
            "sha": "b55d9e3ccf5eee2f9593c783691738a50cbe36eb",
            "filename": "src/transformers/models/hunyuan_v1_dense/modeling_hunyuan_v1_dense.py",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fmodeling_hunyuan_v1_dense.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fmodeling_hunyuan_v1_dense.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fmodeling_hunyuan_v1_dense.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -290,16 +290,17 @@ class HunYuanDenseV1PreTrainedModel(PreTrainedModel):\n         \"attentions\": HunYuanDenseV1Attention,\n     }\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         std = self.config.initializer_range\n         if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n \n \n class HunYuanDenseV1RotaryEmbedding(nn.Module):\n@@ -458,7 +459,7 @@ def forward(\n \n @auto_docstring\n class HunYuanDenseV1ForCausalLM(HunYuanDenseV1PreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.embed_tokens.weight\"}\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n     _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n "
        },
        {
            "sha": "945d2d1c27b1f58d5c08754318016d1065242806",
            "filename": "src/transformers/models/hunyuan_v1_dense/modular_hunyuan_v1_dense.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fmodular_hunyuan_v1_dense.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fmodular_hunyuan_v1_dense.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fmodular_hunyuan_v1_dense.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -120,16 +120,17 @@ def __init__(self, config: HunYuanDenseV1Config, layer_idx: int):\n \n \n class HunYuanDenseV1PreTrainedModel(LlamaPreTrainedModel):\n+    @torch.no_grad()\n     def _init_weights(self, module):\n         std = self.config.initializer_range\n         if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n \n \n class HunYuanDenseV1RotaryEmbedding(LlamaRotaryEmbedding):"
        },
        {
            "sha": "a9d125d65da907d121bd11fd8b41724c6e9c1275",
            "filename": "src/transformers/models/hunyuan_v1_moe/modeling_hunyuan_v1_moe.py",
            "status": "modified",
            "additions": 33,
            "deletions": 34,
            "changes": 67,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fmodeling_hunyuan_v1_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fmodeling_hunyuan_v1_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fmodeling_hunyuan_v1_moe.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -243,38 +243,43 @@ def forward(self, hidden_states):\n         return logits\n \n \n-class HunYuanMoEV1Experts(nn.ModuleList):\n-    \"\"\"\n-    ModuleList of experts.\n-    \"\"\"\n+class HunYuanMoEV1Experts(nn.Module):\n+    \"\"\"Collection of expert weights stored as 3D tensors.\"\"\"\n \n     def __init__(self, config: HunYuanMoEV1Config):\n         super().__init__()\n-        self.top_k = config.num_experts_per_tok\n         self.num_experts = config.num_local_experts\n-        for _ in range(self.num_experts):\n-            self.append(HunYuanMoEV1MLP(config))\n+        self.hidden_dim = config.hidden_size\n+        self.intermediate_dim = config.intermediate_size\n+        self.gate_up_proj = nn.Parameter(torch.empty(self.num_experts, 2 * self.intermediate_dim, self.hidden_dim))\n+        self.down_proj = nn.Parameter(torch.empty(self.num_experts, self.hidden_dim, self.intermediate_dim))\n+        self.act_fn = ACT2FN[config.hidden_act]\n \n     def forward(\n-        self, hidden_states: torch.Tensor, top_k_index: torch.Tensor, top_k_weights: torch.Tensor\n+        self,\n+        hidden_states: torch.Tensor,\n+        top_k_index: torch.Tensor,\n+        top_k_weights: torch.Tensor,\n     ) -> torch.Tensor:\n-        \"\"\"\n-        Args:\n-            hidden_states: (batch_size * sequence_length, hidden_dim)\n-            top_k_index: (batch_size * sequence_length, top_k)\n-            top_k_weights: (batch_size * sequence_length, top_k)\n-        Returns:\n-            (batch_size * sequence_length, hidden_dim)\n-        \"\"\"\n         final_hidden_states = torch.zeros_like(hidden_states)\n-        expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=self.num_experts).permute(2, 1, 0)\n+        num_experts = top_k_weights.shape[1]\n+        with torch.no_grad():\n+            expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=num_experts + 1)\n+            expert_mask = expert_mask.permute(2, 1, 0)\n+            expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n \n-        expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n         for expert_idx in expert_hit:\n-            idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))\n-            current_state = hidden_states[None, top_x].reshape(-1, hidden_states.shape[-1])\n-            current_hidden_states = self[expert_idx](current_state) * top_k_weights[top_x, idx, None]\n-            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n+            expert_idx = expert_idx[0]\n+            if expert_idx == num_experts:\n+                continue\n+            _, token_idx = torch.where(expert_mask[expert_idx])\n+            current_state = hidden_states[token_idx]\n+            gate, up = nn.functional.linear(current_state, self.gate_up_proj[expert_idx]).chunk(2, dim=-1)\n+            current_hidden_states = self.act_fn(gate) * up\n+            current_hidden_states = nn.functional.linear(current_hidden_states, self.down_proj[expert_idx])\n+            current_hidden_states = current_hidden_states * top_k_weights[token_idx, expert_idx, None]\n+            final_hidden_states.index_add_(0, token_idx, current_hidden_states.to(final_hidden_states.dtype))\n+\n         return final_hidden_states\n \n \n@@ -293,6 +298,11 @@ def route_tokens_to_experts(self, hidden_states):\n         routing_weights = F.softmax(hidden_states, dim=1, dtype=torch.float)\n         routing_weights, selected_experts = torch.topk(routing_weights, self.top_k, dim=-1)\n         routing_weights /= routing_weights.sum(dim=-1, keepdim=True)\n+        routing_weights = torch.zeros_like(hidden_states, dtype=torch.float32).scatter_(\n+            1, selected_experts, routing_weights\n+        )\n+        return selected_experts, routing_weights.to(hidden_states.dtype)\n+\n         return selected_experts, routing_weights.to(hidden_states.dtype)\n \n     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n@@ -368,17 +378,6 @@ class HunYuanMoEV1PreTrainedModel(PreTrainedModel):\n         \"attentions\": HunYuanMoEV1Attention,\n     }\n \n-    def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-\n \n class HunYuanMoEV1RotaryEmbedding(nn.Module):\n     inv_freq: torch.Tensor  # fix linting for `register_buffer`\n@@ -536,7 +535,7 @@ def forward(\n \n @auto_docstring\n class HunYuanMoEV1ForCausalLM(HunYuanMoEV1PreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.embed_tokens.weight\"}\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n     _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n "
        },
        {
            "sha": "7244f761f32c928a32f90cebdf00d6116d76810e",
            "filename": "src/transformers/models/hunyuan_v1_moe/modular_hunyuan_v1_moe.py",
            "status": "modified",
            "additions": 5,
            "deletions": 11,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fmodular_hunyuan_v1_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fmodular_hunyuan_v1_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fmodular_hunyuan_v1_moe.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -149,6 +149,11 @@ def route_tokens_to_experts(self, hidden_states):\n         routing_weights = F.softmax(hidden_states, dim=1, dtype=torch.float)\n         routing_weights, selected_experts = torch.topk(routing_weights, self.top_k, dim=-1)\n         routing_weights /= routing_weights.sum(dim=-1, keepdim=True)\n+        routing_weights = torch.zeros_like(hidden_states, dtype=torch.float32).scatter_(\n+            1, selected_experts, routing_weights\n+        )\n+        return selected_experts, routing_weights.to(hidden_states.dtype)\n+\n         return selected_experts, routing_weights.to(hidden_states.dtype)\n \n     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n@@ -177,17 +182,6 @@ def __init__(self, config: HunYuanMoEV1Config, layer_idx: int):\n class HunYuanMoEV1PreTrainedModel(LlamaPreTrainedModel):\n     _can_compile_fullgraph = False\n \n-    def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-\n \n class HunYuanMoEV1RotaryEmbedding(HunYuanDenseV1RotaryEmbedding):\n     pass"
        },
        {
            "sha": "d62058cb7ab94b8ac780d5f0659952d89982c8a9",
            "filename": "src/transformers/models/ibert/modeling_ibert.py",
            "status": "modified",
            "additions": 12,
            "deletions": 17,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fibert%2Fmodeling_ibert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fibert%2Fmodeling_ibert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fibert%2Fmodeling_ibert.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -585,21 +585,22 @@ class IBertPreTrainedModel(PreTrainedModel):\n     config: IBertConfig\n     base_model_prefix = \"ibert\"\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, (QuantLinear, nn.Linear)):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, (QuantEmbedding, nn.Embedding)):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, (IntLayerNorm, nn.LayerNorm)):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n         elif isinstance(module, IBertLMHead):\n-            module.bias.data.zero_()\n+            module.bias.zero_()\n \n     def resize_token_embeddings(self, new_num_tokens=None):\n         raise NotImplementedError(\"`resize_token_embeddings` is not supported for I-BERT.\")\n@@ -710,7 +711,10 @@ def forward(\n \n @auto_docstring\n class IBertForMaskedLM(IBertPreTrainedModel):\n-    _tied_weights_keys = [\"lm_head.decoder.bias\", \"lm_head.decoder.weight\"]\n+    _tied_weights_keys = {\n+        \"lm_head.decoder.weight\": \"ibert.embeddings.word_embeddings.weight$\",\n+        \"lm_head.decoder.bias\": \"lm_head.bias\",\n+    }\n \n     def __init__(self, config):\n         super().__init__(config)\n@@ -789,7 +793,6 @@ def __init__(self, config):\n \n         self.decoder = nn.Linear(config.hidden_size, config.vocab_size)\n         self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n-        self.decoder.bias = self.bias\n \n     def forward(self, features, **kwargs):\n         x = self.dense(features)\n@@ -801,14 +804,6 @@ def forward(self, features, **kwargs):\n \n         return x\n \n-    def _tie_weights(self) -> None:\n-        # For accelerate compatibility and to not break backward compatibility\n-        if self.decoder.bias.device.type == \"meta\":\n-            self.decoder.bias = self.bias\n-        else:\n-            # To tie those two weights if they get disconnected (on TPU or when the bias is resized)\n-            self.bias = self.decoder.bias\n-\n \n @auto_docstring(\n     custom_intro=\"\"\""
        },
        {
            "sha": "1e7fdb05360c13d2b381478cf6daeee621c72a02",
            "filename": "src/transformers/models/idefics/modeling_idefics.py",
            "status": "modified",
            "additions": 18,
            "deletions": 17,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -831,38 +831,39 @@ class IdeficsPreTrainedModel(PreTrainedModel):\n         \"attentions\": OutputRecorder(IdeficsAttention, index=1, layer_name=\"self_attn\"),\n     }\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         # important: this ported version of Idefics isn't meant for training from scratch - only\n         # inference and fine-tuning - so the proper init weights code has been removed - the m4 code\n         # base should be used for training from scratch and it contains the correct code.\n         std = self.config.initializer_range\n         if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.weight.data.fill_(1.0)\n-            module.bias.data.zero_()\n+            module.weight.fill_(1.0)\n+            module.bias.zero_()\n         elif isinstance(module, IdeficsRMSNorm):\n-            module.weight.data.fill_(1.0)\n+            module.weight.fill_(1.0)\n         elif isinstance(module, IdeficsVisionEmbeddings):\n-            module.class_embedding.data.normal_()\n+            module.class_embedding.normal_()\n         elif isinstance(module, IdeficsGatedCrossAttentionLayer):\n             if self.config.alpha_initializer == \"zeros\":\n-                module.alpha_cross_attn.data.zero_()\n-                module.alpha_dense.data.zero_()\n+                module.alpha_cross_attn.zero_()\n+                module.alpha_dense.zero_()\n             elif self.config.alpha_initializer == \"ones\":\n-                module.alpha_cross_attn.data.fill_(1.0)\n-                module.alpha_dense.data.fill_(1.0)\n+                module.alpha_cross_attn.fill_(1.0)\n+                module.alpha_dense.fill_(1.0)\n             elif self.config.alpha_initializer in {\"normal\", \"gaussian\", \"random\"}:\n-                module.alpha_cross_attn.data.normal_(mean=0.0, std=self.config.alphas_initializer_range)\n-                module.alpha_dense.data.normal_(mean=0.0, std=self.config.alphas_initializer_range)\n+                module.alpha_cross_attn.normal_(mean=0.0, std=self.config.alphas_initializer_range)\n+                module.alpha_dense.normal_(mean=0.0, std=self.config.alphas_initializer_range)\n         elif isinstance(module, IdeficsPerceiverResampler):\n-            module.latents.data.normal_()\n+            module.latents.normal_()\n \n \n @auto_docstring\n@@ -1105,7 +1106,7 @@ def forward(\n \n \n class IdeficsForVisionText2Text(IdeficsPreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"model.embed_tokens.weight\", \"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.embed_tokens.weight\"}\n \n     def __init__(self, config, vision_model=None):\n         super().__init__(config)\n@@ -1122,7 +1123,7 @@ def __init__(self, config, vision_model=None):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def tie_weights(self):\n+    def tie_weights(self, missing_keys=None):\n         \"\"\"\n         Overwrite `transformers.modeling_utils.PreTrainedModel.tie_weights` to handle the case of\n         IdeficsDecoupledLinear and IdeficsDecoupledEmbedding."
        },
        {
            "sha": "2caaf2ab2706130bde340e92723dae92fe502bc2",
            "filename": "src/transformers/models/idefics2/modeling_idefics2.py",
            "status": "modified",
            "additions": 11,
            "deletions": 10,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -417,28 +417,29 @@ class Idefics2PreTrainedModel(PreTrainedModel):\n \n     _supports_attention_backend = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         std = getattr(self.config, \"initializer_range\", self.config.get_text_config().initializer_range)\n \n         if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.weight.data.fill_(1.0)\n-            module.bias.data.zero_()\n+            module.weight.fill_(1.0)\n+            module.bias.zero_()\n         elif isinstance(module, Idefics2RMSNorm):\n-            module.weight.data.fill_(1.0)\n+            module.weight.fill_(1.0)\n         elif isinstance(module, nn.MultiheadAttention):\n             module._reset_parameters()  # native torch init\n         elif isinstance(module, Idefics2MultiheadAttentionPoolingHead):\n-            module.probe.data.normal_()\n+            module.probe.normal_()\n         elif isinstance(module, Idefics2PerceiverResampler):\n-            module.latents.data.fill_(1.0)\n+            module.latents.fill_(1.0)\n \n \n @auto_docstring(\n@@ -1010,7 +1011,7 @@ def forward(\n     \"\"\"\n )\n class Idefics2ForConditionalGeneration(Idefics2PreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.text_model.embed_tokens.weight\"}\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "6a57af9d49d882b3bf540ca0623599f3a7ba8cf0",
            "filename": "src/transformers/models/idefics3/modeling_idefics3.py",
            "status": "modified",
            "additions": 9,
            "deletions": 8,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -433,22 +433,23 @@ class Idefics3PreTrainedModel(PreTrainedModel):\n \n     _supports_attention_backend = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         std = getattr(self.config, \"initializer_range\", self.config.get_text_config().initializer_range)\n \n         if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.weight.data.fill_(1.0)\n-            module.bias.data.zero_()\n+            module.weight.fill_(1.0)\n+            module.bias.zero_()\n         elif isinstance(module, Idefics3RMSNorm):\n-            module.weight.data.fill_(1.0)\n+            module.weight.fill_(1.0)\n \n \n @auto_docstring(\n@@ -769,7 +770,7 @@ def forward(\n     \"\"\"\n )\n class Idefics3ForConditionalGeneration(Idefics3PreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.text_model.embed_tokens.weight\"}\n \n     # Copied from transformers.models.idefics2.modeling_idefics2.Idefics2ForConditionalGeneration.__init__ with Idefics2->Idefics3\n     def __init__(self, config):"
        },
        {
            "sha": "a8c5878f35efddacebb1c987653d088e74adb71c",
            "filename": "src/transformers/models/ijepa/modeling_ijepa.py",
            "status": "modified",
            "additions": 17,
            "deletions": 12,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodeling_ijepa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodeling_ijepa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodeling_ijepa.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -324,27 +324,32 @@ class IJepaPreTrainedModel(PreTrainedModel):\n         \"attentions\": IJepaSelfAttention,\n     }\n \n+    @torch.no_grad()\n     def _init_weights(self, module: Union[nn.Linear, nn.Conv2d, nn.LayerNorm]) -> None:\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, (nn.Linear, nn.Conv2d)):\n             # Upcast the input in `fp32` and cast it back to desired `dtype` to avoid\n             # `trunc_normal_cpu` not implemented in `half` issues\n-            module.weight.data = nn.init.trunc_normal_(\n-                module.weight.data.to(torch.float32), mean=0.0, std=self.config.initializer_range\n-            ).to(module.weight.dtype)\n+            module.weight.copy_(\n+                nn.init.trunc_normal_(module.weight.to(torch.float32), mean=0.0, std=self.config.initializer_range).to(\n+                    module.weight.dtype\n+                )\n+            )\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n         elif isinstance(module, IJepaEmbeddings):\n-            module.position_embeddings.data = nn.init.trunc_normal_(\n-                module.position_embeddings.data.to(torch.float32),\n-                mean=0.0,\n-                std=self.config.initializer_range,\n-            ).to(module.position_embeddings.dtype)\n+            module.position_embeddings.copy_(\n+                nn.init.trunc_normal_(\n+                    module.position_embeddings.to(torch.float32),\n+                    mean=0.0,\n+                    std=self.config.initializer_range,\n+                ).to(module.position_embeddings.dtype)\n+            )\n             if module.mask_token is not None:\n-                module.mask_token.data.zero_()\n+                module.mask_token.zero_()\n \n \n class IJepaEncoder(nn.Module):"
        },
        {
            "sha": "095945a3f39df40dfbbb15d60276bab749782246",
            "filename": "src/transformers/models/ijepa/modular_ijepa.py",
            "status": "modified",
            "additions": 17,
            "deletions": 12,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodular_ijepa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodular_ijepa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodular_ijepa.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -87,27 +87,32 @@ def forward(\n \n @auto_docstring\n class IJepaPreTrainedModel(ViTPreTrainedModel):\n+    @torch.no_grad()\n     def _init_weights(self, module: Union[nn.Linear, nn.Conv2d, nn.LayerNorm]) -> None:\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, (nn.Linear, nn.Conv2d)):\n             # Upcast the input in `fp32` and cast it back to desired `dtype` to avoid\n             # `trunc_normal_cpu` not implemented in `half` issues\n-            module.weight.data = nn.init.trunc_normal_(\n-                module.weight.data.to(torch.float32), mean=0.0, std=self.config.initializer_range\n-            ).to(module.weight.dtype)\n+            module.weight.copy_(\n+                nn.init.trunc_normal_(module.weight.to(torch.float32), mean=0.0, std=self.config.initializer_range).to(\n+                    module.weight.dtype\n+                )\n+            )\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n         elif isinstance(module, IJepaEmbeddings):\n-            module.position_embeddings.data = nn.init.trunc_normal_(\n-                module.position_embeddings.data.to(torch.float32),\n-                mean=0.0,\n-                std=self.config.initializer_range,\n-            ).to(module.position_embeddings.dtype)\n+            module.position_embeddings.copy_(\n+                nn.init.trunc_normal_(\n+                    module.position_embeddings.to(torch.float32),\n+                    mean=0.0,\n+                    std=self.config.initializer_range,\n+                ).to(module.position_embeddings.dtype)\n+            )\n             if module.mask_token is not None:\n-                module.mask_token.data.zero_()\n+                module.mask_token.zero_()\n \n \n class IJepaModel(IJepaPreTrainedModel, ViTModel):"
        },
        {
            "sha": "b4c844eb4f498b9bac0714925361035ee33d77ef",
            "filename": "src/transformers/models/imagegpt/modeling_imagegpt.py",
            "status": "modified",
            "additions": 12,
            "deletions": 10,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -369,29 +369,31 @@ class ImageGPTPreTrainedModel(PreTrainedModel):\n     def __init__(self, *inputs, **kwargs):\n         super().__init__(*inputs, **kwargs)\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights.\"\"\"\n         if isinstance(module, (nn.Linear, Conv1D)):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, ImageGPTLayerNorm):\n-            module.weight.data.fill_(1.0)\n+            module.weight.fill_(1.0)\n \n         # Reinitialize selected weights subject to the OpenAI GPT-2 Paper Scheme:\n         #   > A modified initialization which accounts for the accumulation on the residual path with model depth. Scale\n         #   > the weights of residual layers at initialization by a factor of 1/√N where N is the # of residual layers.\n         #   >   -- GPT-2 :: https://openai.com/blog/better-language-models/\n         #\n         # Reference (Megatron-LM): https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/model/gpt_model.py\n-        for name, p in module.named_parameters():\n-            if \"c_proj\" in name and \"weight\" in name:\n-                # Special Scaled Initialization --> There are 2 Layer Norms per Transformer Block\n-                p.data.normal_(mean=0.0, std=(self.config.initializer_range / math.sqrt(2 * self.config.n_layer)))\n+        if isinstance(module, PreTrainedModel):\n+            for name, p in module.named_parameters():\n+                if \"c_proj\" in name and \"weight\" in name:\n+                    # Special Scaled Initialization --> There are 2 Layer Norms per Transformer Block\n+                    p.normal_(mean=0.0, std=(self.config.initializer_range / math.sqrt(2 * self.config.n_layer)))\n \n \n @auto_docstring\n@@ -606,7 +608,7 @@ def forward(\n     \"\"\"\n )\n class ImageGPTForCausalImageModeling(ImageGPTPreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"transformer.wte.weight\"}\n \n     def __init__(self, config: ImageGPTConfig):\n         super().__init__(config)"
        },
        {
            "sha": "a8f618a43b69521867529fffea412159c9d24a61",
            "filename": "src/transformers/models/informer/modeling_informer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -250,6 +250,7 @@ class InformerPreTrainedModel(PreTrainedModel):\n     input_modalities = \"time\"\n     supports_gradient_checkpointing = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module: nn.Module):\n         super()._init_weights(module)\n         if isinstance(module, InformerSinusoidalPositionalEmbedding):"
        },
        {
            "sha": "0066f41a3e47ed1fbf94e9c2af747539c38d77ea",
            "filename": "src/transformers/models/informer/modular_informer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Finformer%2Fmodular_informer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Finformer%2Fmodular_informer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finformer%2Fmodular_informer.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -86,6 +86,7 @@ class InformerPreTrainedModel(PreTrainedModel):\n     input_modalities = \"time\"\n     supports_gradient_checkpointing = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module: nn.Module):\n         super()._init_weights(module)\n         if isinstance(module, InformerSinusoidalPositionalEmbedding):"
        },
        {
            "sha": "25b54f2d2b9f9ee2166f7941b69583150f735226",
            "filename": "src/transformers/models/instructblip/modeling_instructblip.py",
            "status": "modified",
            "additions": 7,
            "deletions": 17,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -324,24 +324,25 @@ class InstructBlipPreTrainedModel(PreTrainedModel):\n         \"InstructBlipQFormerSelfOutput\",\n     ]\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         factor = self.config.initializer_range\n \n         if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.data.normal_(mean=0.0, std=factor)\n+            module.weight.normal_(mean=0.0, std=factor)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=factor)\n+            module.weight.normal_(mean=0.0, std=factor)\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n         elif isinstance(module, InstructBlipVisionEmbeddings):\n             nn.init.trunc_normal_(module.position_embedding, mean=0.0, std=factor)\n             nn.init.trunc_normal_(module.class_embedding, mean=0.0, std=factor)\n         elif isinstance(module, (InstructBlipForConditionalGeneration, InstructBlipModel)):\n-            module.query_tokens.data.zero_()\n+            module.query_tokens.zero_()\n \n \n # Copied from transformers.models.blip.modeling_blip.BlipEncoder with Blip->InstructBlip\n@@ -961,11 +962,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.language_model.set_input_embeddings(value)\n \n-    def _tie_weights(self):\n-        if not self.config.use_decoder_only_language_model:\n-            self.language_model.encoder.embed_tokens = self.language_model.shared\n-            self.language_model.decoder.embed_tokens = self.language_model.shared\n-\n     def _preprocess_accelerate(self):\n         r\"\"\"\n         Some pre-processing hacks to make the model `accelerate` compatible. Check\n@@ -1160,12 +1156,6 @@ def get_encoder(self):\n     def get_decoder(self):\n         return self.language_model.get_decoder()\n \n-    # Copied from transformers.models.instructblip.modeling_instructblip.InstructBlipModel._tie_weights\n-    def _tie_weights(self):\n-        if not self.config.use_decoder_only_language_model:\n-            self.language_model.encoder.embed_tokens = self.language_model.shared\n-            self.language_model.decoder.embed_tokens = self.language_model.shared\n-\n     # Copied from transformers.models.instructblip.modeling_instructblip.InstructBlipModel._preprocess_accelerate\n     def _preprocess_accelerate(self):\n         r\"\"\""
        },
        {
            "sha": "f48baf11b9253dad882b88b33db080076f6f29e4",
            "filename": "src/transformers/models/instructblipvideo/modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 7,
            "deletions": 16,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -147,24 +147,25 @@ class InstructBlipVideoPreTrainedModel(PreTrainedModel):\n         \"InstructBlipVideoQFormerSelfOutput\",\n     ]\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         factor = self.config.initializer_range\n \n         if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.data.normal_(mean=0.0, std=factor)\n+            module.weight.normal_(mean=0.0, std=factor)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=factor)\n+            module.weight.normal_(mean=0.0, std=factor)\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n         elif isinstance(module, InstructBlipVideoVisionEmbeddings):\n             nn.init.trunc_normal_(module.position_embedding, mean=0.0, std=factor)\n             nn.init.trunc_normal_(module.class_embedding, mean=0.0, std=factor)\n         elif isinstance(module, (InstructBlipVideoForConditionalGeneration, InstructBlipVideoModel)):\n-            module.query_tokens.data.zero_()\n+            module.query_tokens.zero_()\n \n \n # Adapted from transformers.models.siglip.modeling_siglip.eager_attention_forward -> InstructBlipVideo doesn't cast attn weights to fp32\n@@ -958,11 +959,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.language_model.set_input_embeddings(value)\n \n-    def _tie_weights(self):\n-        if not self.config.use_decoder_only_language_model:\n-            self.language_model.encoder.embed_tokens = self.language_model.shared\n-            self.language_model.decoder.embed_tokens = self.language_model.shared\n-\n     def _preprocess_accelerate(self):\n         r\"\"\"\n         Some pre-processing hacks to make the model `accelerate` compatible. Check\n@@ -1190,11 +1186,6 @@ def get_encoder(self):\n     def get_decoder(self):\n         return self.language_model.get_decoder()\n \n-    def _tie_weights(self):\n-        if not self.config.use_decoder_only_language_model:\n-            self.language_model.encoder.embed_tokens = self.language_model.shared\n-            self.language_model.decoder.embed_tokens = self.language_model.shared\n-\n     def _preprocess_accelerate(self):\n         r\"\"\"\n         Some pre-processing hacks to make the model `accelerate` compatible. Check"
        },
        {
            "sha": "3d41bb9aba32dbac9290f462b51d2fdbd8d54ecf",
            "filename": "src/transformers/models/internvl/modeling_internvl.py",
            "status": "modified",
            "additions": 11,
            "deletions": 13,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -411,18 +411,19 @@ class InternVLVisionPreTrainedModel(PreTrainedModel):\n         \"attentions\": InternVLVisionAttention,\n     }\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         super()._init_weights(module)\n         if isinstance(module, InternVLVisionEmbeddings):\n-            module.cls_token.data.zero_()\n+            module.cls_token.zero_()\n             if module.mask_token is not None:\n-                module.mask_token.data.zero_()\n+                module.mask_token.zero_()\n             if module.position_embeddings is not None:\n-                module.position_embeddings.data.zero_()\n+                module.position_embeddings.zero_()\n         elif isinstance(module, InternVLVisionLayer):\n-            module.lambda_1.data.fill_(self.config.layer_scale_init_value)\n-            module.lambda_2.data.fill_(self.config.layer_scale_init_value)\n+            module.lambda_1.fill_(self.config.layer_scale_init_value)\n+            module.lambda_2.fill_(self.config.layer_scale_init_value)\n \n \n @auto_docstring\n@@ -471,7 +472,6 @@ def forward(\n @auto_docstring\n class InternVLPreTrainedModel(PreTrainedModel):\n     config: InternVLConfig\n-    base_model_prefix = \"\"\n     input_modalities = [\"image\", \"text\", \"video\"]\n     supports_gradient_checkpointing = True\n     _skip_keys_device_placement = \"past_key_values\"\n@@ -529,8 +529,6 @@ class InternVLModelOutputWithPast(BaseModelOutputWithPast):\n     \"\"\"\n )\n class InternVLModel(InternVLPreTrainedModel):\n-    _checkpoint_conversion_mapping = {\"language_model.model\": \"language_model\"}\n-\n     def __init__(self, config: InternVLConfig):\n         super().__init__(config)\n         self.vision_tower = AutoModel.from_config(config.vision_config)\n@@ -761,12 +759,12 @@ class InternVLCausalLMOutputWithPast(ModelOutput):\n )\n class InternVLForConditionalGeneration(InternVLPreTrainedModel, GenerationMixin):\n     _checkpoint_conversion_mapping = {\n-        \"^language_model.model\": \"model.language_model\",\n-        \"^vision_tower\": \"model.vision_tower\",\n-        \"^multi_modal_projector\": \"model.multi_modal_projector\",\n-        \"^language_model.lm_head\": \"lm_head\",\n+        r\"^language_model.model\": \"model.language_model\",\n+        r\"^vision_tower\": \"model.vision_tower\",\n+        r\"^multi_modal_projector\": \"model.multi_modal_projector\",\n+        r\"^language_model.lm_head\": \"lm_head\",\n     }\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.language_model.embed_tokens.weight\"}\n \n     def __init__(self, config: InternVLConfig):\n         super().__init__(config)"
        },
        {
            "sha": "62ee383ce56615ec75a9e2b0699ba99e868a6d78",
            "filename": "src/transformers/models/internvl/modular_internvl.py",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -368,18 +368,19 @@ class InternVLVisionPreTrainedModel(PreTrainedModel):\n         \"attentions\": InternVLVisionAttention,\n     }\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         super()._init_weights(module)\n         if isinstance(module, InternVLVisionEmbeddings):\n-            module.cls_token.data.zero_()\n+            module.cls_token.zero_()\n             if module.mask_token is not None:\n-                module.mask_token.data.zero_()\n+                module.mask_token.zero_()\n             if module.position_embeddings is not None:\n-                module.position_embeddings.data.zero_()\n+                module.position_embeddings.zero_()\n         elif isinstance(module, InternVLVisionLayer):\n-            module.lambda_1.data.fill_(self.config.layer_scale_init_value)\n-            module.lambda_2.data.fill_(self.config.layer_scale_init_value)\n+            module.lambda_1.fill_(self.config.layer_scale_init_value)\n+            module.lambda_2.fill_(self.config.layer_scale_init_value)\n \n \n @auto_docstring"
        },
        {
            "sha": "609fff07ab8023de96efb7a69f4a8fab98c0aa60",
            "filename": "src/transformers/models/jamba/modeling_jamba.py",
            "status": "modified",
            "additions": 31,
            "deletions": 25,
            "changes": 56,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -557,38 +557,43 @@ def forward(self, x):\n         return down_proj\n \n \n-class JambaExperts(nn.ModuleList):\n-    \"\"\"\n-    ModuleList of experts.\n-    \"\"\"\n+class JambaExperts(nn.Module):\n+    \"\"\"Collection of expert weights stored as 3D tensors.\"\"\"\n \n     def __init__(self, config: JambaConfig):\n         super().__init__()\n-        self.top_k = config.num_experts_per_tok\n         self.num_experts = config.num_local_experts\n-        for _ in range(self.num_experts):\n-            self.append(JambaMLP(config))\n+        self.hidden_dim = config.hidden_size\n+        self.intermediate_dim = config.intermediate_size\n+        self.gate_up_proj = nn.Parameter(torch.empty(self.num_experts, 2 * self.intermediate_dim, self.hidden_dim))\n+        self.down_proj = nn.Parameter(torch.empty(self.num_experts, self.hidden_dim, self.intermediate_dim))\n+        self.act_fn = ACT2FN[config.hidden_act]\n \n     def forward(\n-        self, hidden_states: torch.Tensor, top_k_index: torch.Tensor, top_k_weights: torch.Tensor\n+        self,\n+        hidden_states: torch.Tensor,\n+        top_k_index: torch.Tensor,\n+        top_k_weights: torch.Tensor,\n     ) -> torch.Tensor:\n-        \"\"\"\n-        Args:\n-            hidden_states: (batch_size * sequence_length, hidden_dim)\n-            top_k_index: (batch_size * sequence_length, top_k)\n-            top_k_weights: (batch_size * sequence_length, top_k)\n-        Returns:\n-            (batch_size * sequence_length, hidden_dim)\n-        \"\"\"\n         final_hidden_states = torch.zeros_like(hidden_states)\n-        expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=self.num_experts).permute(2, 1, 0)\n+        num_experts = top_k_weights.shape[1]\n+        with torch.no_grad():\n+            expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=num_experts + 1)\n+            expert_mask = expert_mask.permute(2, 1, 0)\n+            expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n \n-        expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n         for expert_idx in expert_hit:\n-            idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))\n-            current_state = hidden_states[None, top_x].reshape(-1, hidden_states.shape[-1])\n-            current_hidden_states = self[expert_idx](current_state) * top_k_weights[top_x, idx, None]\n-            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n+            expert_idx = expert_idx[0]\n+            if expert_idx == num_experts:\n+                continue\n+            _, token_idx = torch.where(expert_mask[expert_idx])\n+            current_state = hidden_states[token_idx]\n+            gate, up = nn.functional.linear(current_state, self.gate_up_proj[expert_idx]).chunk(2, dim=-1)\n+            current_hidden_states = self.act_fn(gate) * up\n+            current_hidden_states = nn.functional.linear(current_hidden_states, self.down_proj[expert_idx])\n+            current_hidden_states = current_hidden_states * top_k_weights[token_idx, expert_idx, None]\n+            final_hidden_states.index_add_(0, token_idx, current_hidden_states.to(final_hidden_states.dtype))\n+\n         return final_hidden_states\n \n \n@@ -717,13 +722,14 @@ class JambaPreTrainedModel(PreTrainedModel):\n         \"router_logits\": OutputRecorder(nn.Linear, layer_name=\"router\"),\n     }\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, JambaMambaMixer):\n             A = torch.arange(1, module.ssm_state_size + 1)[None, :]\n             A = A.expand(module.intermediate_size, -1).contiguous()\n-            module.A_log.data.copy_(torch.log(A))\n-            module.D.data.fill_(1.0)\n+            module.A_log.copy_(torch.log(A))\n+            module.D.fill_(1.0)\n \n \n ALL_DECODER_LAYER_TYPES = {\"attention\": JambaAttentionDecoderLayer, \"mamba\": JambaMambaDecoderLayer}\n@@ -916,7 +922,7 @@ def load_balancing_loss_func(\n \n @auto_docstring\n class JambaForCausalLM(JambaPreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.embed_tokens.weight\"}\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n     _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n "
        },
        {
            "sha": "1c362c3f802ae0dc6a62c08febee79b85545d51e",
            "filename": "src/transformers/models/jamba/modular_jamba.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodular_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodular_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodular_jamba.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -607,13 +607,14 @@ class JambaPreTrainedModel(PreTrainedModel):\n         \"router_logits\": OutputRecorder(nn.Linear, layer_name=\"router\"),\n     }\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, JambaMambaMixer):\n             A = torch.arange(1, module.ssm_state_size + 1)[None, :]\n             A = A.expand(module.intermediate_size, -1).contiguous()\n-            module.A_log.data.copy_(torch.log(A))\n-            module.D.data.fill_(1.0)\n+            module.A_log.copy_(torch.log(A))\n+            module.D.fill_(1.0)\n \n \n @auto_docstring"
        },
        {
            "sha": "2fed1ceabd3a26e8d10775481adb7d0eac148d91",
            "filename": "src/transformers/models/janus/modeling_janus.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -1164,7 +1164,7 @@ def forward(\n \n \n class JanusForConditionalGeneration(JanusPreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"model.language_model.embed_tokens.weight\", \"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.language_model.embed_tokens.weight\"}\n     output_modalities = [\"image\", \"text\"]\n     _can_compile_fullgraph = True\n "
        },
        {
            "sha": "a2df0266d7030b33247db99e11f18b3e948d5cd9",
            "filename": "src/transformers/models/janus/modular_janus.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -980,7 +980,7 @@ def forward(\n \n \n class JanusForConditionalGeneration(JanusPreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"model.language_model.embed_tokens.weight\", \"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.language_model.embed_tokens.weight\"}\n     output_modalities = [\"image\", \"text\"]\n     _can_compile_fullgraph = True\n "
        },
        {
            "sha": "28a3dc151d70495f59f543ecdc8f51ca7ff2cbb0",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 9,
            "deletions": 8,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -582,22 +582,23 @@ class JetMoePreTrainedModel(PreTrainedModel):\n         \"attentions\": OutputRecorder(JetMoeAttention, index=1),\n     }\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights.\"\"\"\n         if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, JetMoeRMSNorm):\n-            module.weight.data.fill_(1.0)\n+            module.weight.fill_(1.0)\n         elif isinstance(module, JetMoeParallelExperts):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n         elif isinstance(module, JetMoeMoA | JetMoeMoE):\n-            module.bias.data.zero_()\n+            module.bias.zero_()\n \n \n @auto_docstring\n@@ -766,7 +767,7 @@ def load_balancing_loss_func(\n \n \n class JetMoeForCausalLM(JetMoePreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.embed_tokens.weight\"}\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "82c8e582d070025f6f6bdcbfd1dc782e994311d4",
            "filename": "src/transformers/models/jetmoe/modular_jetmoe.py",
            "status": "modified",
            "additions": 9,
            "deletions": 8,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodular_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodular_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodular_jetmoe.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -435,22 +435,23 @@ class JetMoePreTrainedModel(MixtralPreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights.\"\"\"\n         if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, JetMoeRMSNorm):\n-            module.weight.data.fill_(1.0)\n+            module.weight.fill_(1.0)\n         elif isinstance(module, JetMoeParallelExperts):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n         elif isinstance(module, JetMoeMoA | JetMoeMoE):\n-            module.bias.data.zero_()\n+            module.bias.zero_()\n \n \n @auto_docstring\n@@ -532,7 +533,7 @@ def forward(\n \n \n class JetMoeForCausalLM(JetMoePreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.embed_tokens.weight\"}\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "5726eeacaad639bfcc5aa1fb0a3eb0f10e9a89c0",
            "filename": "src/transformers/models/kosmos2/modeling_kosmos2.py",
            "status": "modified",
            "additions": 8,
            "deletions": 7,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -1120,6 +1120,7 @@ class Kosmos2PreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module: nn.Module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(self, Kosmos2VisionModel):\n@@ -1162,15 +1163,15 @@ def _init_weights(self, module: nn.Module):\n             nn.init.normal_(module.dense.weight, std=std)\n             nn.init.normal_(module.latent_query)\n         elif isinstance(module, Kosmos2TextTransformer):\n-            module.embed_tokens.weight.data.normal_(mean=0.0, std=std)\n+            module.embed_tokens.weight.normal_(mean=0.0, std=std)\n             if module.embed_tokens.padding_idx is not None:\n-                module.embed_tokens.weight.data[module.embed_tokens.padding_idx].zero_()\n+                module.embed_tokens.weight[module.embed_tokens.padding_idx].zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.weight.data.fill_(1.0)\n-            module.bias.data.zero_()\n+            module.weight.fill_(1.0)\n+            module.bias.zero_()\n \n         if isinstance(module, nn.Linear) and module.bias is not None:\n-            module.bias.data.zero_()\n+            module.bias.zero_()\n \n \n class Kosmos2VisionModel(Kosmos2PreTrainedModel):\n@@ -1277,7 +1278,7 @@ def forward(\n )\n class Kosmos2TextForCausalLM(Kosmos2PreTrainedModel, GenerationMixin):\n     config: Kosmos2TextConfig\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.embed_tokens.weight\"}\n \n     def __init__(self, config: Kosmos2TextConfig):\n         super().__init__(config)\n@@ -1617,7 +1618,7 @@ def forward(\n class Kosmos2ForConditionalGeneration(Kosmos2PreTrainedModel, GenerationMixin):\n     config: Kosmos2Config\n     main_input_name = \"pixel_values\"\n-    _tied_weights_keys = [\"text_model.lm_head.weight\"]\n+    _tied_weights_keys = {\"text_model.lm_head.weight\": \"text_model.model.embed_tokens.weight\"}\n \n     def __init__(self, config: Kosmos2Config):\n         super().__init__(config)"
        },
        {
            "sha": "c0313f33eca2f7f6484dd4d20dd26d6f02039b39",
            "filename": "src/transformers/models/kosmos2_5/modeling_kosmos2_5.py",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fmodeling_kosmos2_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fmodeling_kosmos2_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fmodeling_kosmos2_5.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -1227,6 +1227,7 @@ class Kosmos2_5PreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_attention_backend = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(self, Kosmos2_5VisionModel):\n@@ -1237,19 +1238,19 @@ def _init_weights(self, module):\n         elif isinstance(self, (Kosmos2_5Model, Kosmos2_5ForConditionalGeneration)):\n             std = self.config.text_config.init_std\n         if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, (nn.LayerNorm, Kosmos2_5LayerNorm)):\n-            module.weight.data.fill_(1.0)\n+            module.weight.fill_(1.0)\n             if getattr(module, \"bias\", None) is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, Kosmos2_5ImageToTextProjection):\n-            module.latent_query.data.normal_(mean=0.0, std=1.0)\n+            module.latent_query.normal_(mean=0.0, std=1.0)\n \n \n class Kosmos2_5VisionModel(Kosmos2_5PreTrainedModel):\n@@ -1503,7 +1504,7 @@ def forward(\n class Kosmos2_5TextForCausalLM(Kosmos2_5PreTrainedModel):\n     config_class = Kosmos2_5TextConfig\n     input_modalities = \"text\"\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.embed_tokens.weight\"}\n \n     def __init__(self, config: Kosmos2_5TextConfig):\n         super().__init__(config)\n@@ -1660,7 +1661,6 @@ def prepare_inputs_for_generation(\n )\n class Kosmos2_5ForConditionalGeneration(Kosmos2_5PreTrainedModel, GenerationMixin):\n     config_class = Kosmos2_5Config\n-    _tied_weights_keys = [\"text_model.lm_head.weight\"]\n \n     def __init__(self, config: Kosmos2_5Config):\n         super().__init__(config)"
        },
        {
            "sha": "989fd9706c79fc90ebfe76a0baff78eef1cbdfde",
            "filename": "src/transformers/models/kyutai_speech_to_text/modeling_kyutai_speech_to_text.py",
            "status": "modified",
            "additions": 8,
            "deletions": 7,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -124,21 +124,22 @@ class KyutaiSpeechToTextPreTrainedModel(PreTrainedModel):\n \n     main_input_name = \"input_ids\"\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         std = self.config.initializer_range\n \n         if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, KyutaiSpeechToTextFlexibleLinear):\n-            module.weight.data.normal_()\n+            module.weight.normal_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, KyutaiSpeechToTextRMSNorm):\n-            module.weight.data.fill_(1.0)\n+            module.weight.fill_(1.0)\n \n \n class KyutaiSpeechToTextConv1dPaddingCache:\n@@ -1090,7 +1091,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n \n @auto_docstring\n class KyutaiSpeechToTextForConditionalGeneration(KyutaiSpeechToTextPreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.embed_tokens.weight\"}\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n     _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n     _keep_in_fp32_modules_strict = [\"codec_model\"]"
        },
        {
            "sha": "146c395aa9eeed1a7af96c20f3d8553ef17a12d2",
            "filename": "src/transformers/models/layoutlm/modeling_layoutlm.py",
            "status": "modified",
            "additions": 13,
            "deletions": 16,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Flayoutlm%2Fmodeling_layoutlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Flayoutlm%2Fmodeling_layoutlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlm%2Fmodeling_layoutlm.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -398,16 +398,9 @@ def __init__(self, config):\n \n         # The output weights are the same as the input embeddings, but there is\n         # an output-only bias for each token.\n-        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n-\n+        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=True)\n         self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n \n-        # Need a link between the two variables so that the bias is correctly resized with `resize_token_embeddings`\n-        self.decoder.bias = self.bias\n-\n-    def _tie_weights(self):\n-        self.decoder.bias = self.bias\n-\n     def forward(self, hidden_states):\n         hidden_states = self.transform(hidden_states)\n         hidden_states = self.decoder(hidden_states)\n@@ -431,21 +424,22 @@ class LayoutLMPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"layoutlm\"\n     supports_gradient_checkpointing = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, LayoutLMLayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n         elif isinstance(module, LayoutLMLMPredictionHead):\n-            module.bias.data.zero_()\n+            module.bias.zero_()\n \n \n @auto_docstring\n@@ -577,7 +571,10 @@ def forward(\n \n @auto_docstring\n class LayoutLMForMaskedLM(LayoutLMPreTrainedModel):\n-    _tied_weights_keys = [\"cls.predictions.decoder.bias\", \"cls.predictions.decoder.weight\"]\n+    _tied_weights_keys = {\n+        \"cls.predictions.decoder.bias\": \"cls.predictions.bias\",\n+        \"cls.predictions.decoder.weight\": \"layoutlm.embeddings.word_embeddings.weight\",\n+    }\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "e276407a720b11476b86ddcd746f87b7fe0c78a6",
            "filename": "src/transformers/models/layoutlmv2/modeling_layoutlmv2.py",
            "status": "modified",
            "additions": 10,
            "deletions": 9,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fmodeling_layoutlmv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fmodeling_layoutlmv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fmodeling_layoutlmv2.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -458,26 +458,27 @@ class LayoutLMv2PreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"layoutlmv2\"\n     input_modalities = [\"image\", \"text\"]\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n         elif isinstance(module, LayoutLMv2SelfAttention):\n             if self.config.fast_qkv:\n-                module.q_bias.data.zero_()\n-                module.v_bias.data.zero_()\n+                module.q_bias.zero_()\n+                module.v_bias.zero_()\n         elif isinstance(module, LayoutLMv2Model):\n             if hasattr(module, \"visual_segment_embedding\"):\n-                module.visual_segment_embedding.data.normal_(mean=0.0, std=self.config.initializer_range)\n+                module.visual_segment_embedding.normal_(mean=0.0, std=self.config.initializer_range)\n \n \n def my_convert_sync_batchnorm(module, process_group=None):"
        },
        {
            "sha": "a04875e726466b23c14e8ec94a42cf39d00fec52",
            "filename": "src/transformers/models/layoutlmv3/modeling_layoutlmv3.py",
            "status": "modified",
            "additions": 9,
            "deletions": 8,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fmodeling_layoutlmv3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fmodeling_layoutlmv3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fmodeling_layoutlmv3.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -203,23 +203,24 @@ class LayoutLMv3PreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"layoutlmv3\"\n     input_modalities = [\"image\", \"text\"]\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n         elif isinstance(module, LayoutLMv3Model):\n             if self.config.visual_embed:\n-                module.cls_token.data.zero_()\n-                module.pos_embed.data.zero_()\n+                module.cls_token.zero_()\n+                module.pos_embed.zero_()\n \n \n class LayoutLMv3SelfAttention(nn.Module):"
        },
        {
            "sha": "418f60f77a617b3908e36f4a2985e9efde920eeb",
            "filename": "src/transformers/models/led/modeling_led.py",
            "status": "modified",
            "additions": 18,
            "deletions": 22,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -1067,16 +1067,17 @@ class LEDPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"led\"\n     supports_gradient_checkpointing = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         std = self.config.init_std\n         if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n \n     @property\n     def dummy_inputs(self):\n@@ -1290,7 +1291,7 @@ class LEDEncoder(LEDPreTrainedModel):\n         embed_tokens (nn.Embedding): output embedding\n     \"\"\"\n \n-    def __init__(self, config: LEDConfig, embed_tokens: Optional[nn.Embedding] = None):\n+    def __init__(self, config: LEDConfig):\n         super().__init__(config)\n \n         self.dropout = config.dropout\n@@ -1313,10 +1314,7 @@ def __init__(self, config: LEDConfig, embed_tokens: Optional[nn.Embedding] = Non\n                     f\"Expected {config.num_hidden_layers}, given {len(config.attention_window)}\"\n                 )\n \n-        if embed_tokens is not None:\n-            self.embed_tokens = embed_tokens\n-        else:\n-            self.embed_tokens = nn.Embedding(config.vocab_size, embed_dim, self.padding_idx)\n+        self.embed_tokens = nn.Embedding(config.vocab_size, embed_dim, self.padding_idx)\n \n         self.embed_positions = LEDLearnedPositionalEmbedding(\n             self.max_source_positions,\n@@ -1553,17 +1551,14 @@ class LEDDecoder(LEDPreTrainedModel):\n         embed_tokens (nn.Embedding): output embedding\n     \"\"\"\n \n-    def __init__(self, config: LEDConfig, embed_tokens: Optional[nn.Embedding] = None):\n+    def __init__(self, config: LEDConfig):\n         super().__init__(config)\n         self.dropout = config.dropout\n         self.layerdrop = config.decoder_layerdrop\n         self.padding_idx = config.pad_token_id\n         self.max_target_positions = config.max_decoder_position_embeddings\n \n-        if embed_tokens is not None:\n-            self.embed_tokens = embed_tokens\n-        else:\n-            self.embed_tokens = nn.Embedding(config.vocab_size, config.d_model, self.padding_idx)\n+        self.embed_tokens = nn.Embedding(config.vocab_size, config.d_model, self.padding_idx)\n \n         self.embed_positions = LEDLearnedPositionalEmbedding(\n             self.max_target_positions,\n@@ -1763,16 +1758,19 @@ def forward(\n \n @auto_docstring\n class LEDModel(LEDPreTrainedModel):\n-    _tied_weights_keys = [\"decoder.embed_tokens.weight\", \"encoder.embed_tokens.weight\"]\n+    _tied_weights_keys = {\n+        \"encoder.embed_tokens.weight\": \"shared.weight\",\n+        \"decoder.embed_tokens.weight\": \"shared.weight\",\n+    }\n \n     def __init__(self, config: LEDConfig):\n         super().__init__(config)\n \n         padding_idx, vocab_size = config.pad_token_id, config.vocab_size\n         self.shared = nn.Embedding(vocab_size, config.d_model, padding_idx)\n \n-        self.encoder = LEDEncoder(config, self.shared)\n-        self.decoder = LEDDecoder(config, self.shared)\n+        self.encoder = LEDEncoder(config)\n+        self.decoder = LEDDecoder(config)\n \n         # Initialize weights and apply final processing\n         self.post_init()\n@@ -1908,7 +1906,9 @@ def forward(\n class LEDForConditionalGeneration(LEDPreTrainedModel, GenerationMixin):\n     base_model_prefix = \"led\"\n     _keys_to_ignore_on_load_missing = [\"final_logits_bias\"]\n-    _tied_weights_keys = [\"decoder.embed_tokens.weight\", \"encoder.embed_tokens.weight\", \"lm_head.weight\"]\n+    _tied_weights_keys = {\n+        \"lm_head.weight\": \"led.shared.weight\",\n+    }\n \n     def __init__(self, config: LEDConfig):\n         super().__init__(config)\n@@ -2106,8 +2106,6 @@ def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n     \"\"\"\n )\n class LEDForSequenceClassification(LEDPreTrainedModel):\n-    _tied_weights_keys = [\"decoder.embed_tokens.weight\", \"encoder.embed_tokens.weight\"]\n-\n     def __init__(self, config: LEDConfig, **kwargs):\n         warnings.warn(\n             \"The `transformers.LEDForSequenceClassification` class is deprecated and will be removed in version 5 of\"\n@@ -2252,8 +2250,6 @@ def forward(\n \n @auto_docstring\n class LEDForQuestionAnswering(LEDPreTrainedModel):\n-    _tied_weights_keys = [\"decoder.embed_tokens.weight\", \"encoder.embed_tokens.weight\"]\n-\n     def __init__(self, config):\n         super().__init__(config)\n "
        },
        {
            "sha": "ca7cc7589be70f51c63cb975299cbd0695916e79",
            "filename": "src/transformers/models/levit/modeling_levit.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Flevit%2Fmodeling_levit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Flevit%2Fmodeling_levit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flevit%2Fmodeling_levit.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -472,15 +472,16 @@ class LevitPreTrainedModel(PreTrainedModel):\n     input_modalities = \"image\"\n     _no_split_modules = [\"LevitResidualLayer\"]\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, (nn.BatchNorm1d, nn.BatchNorm2d)):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n \n \n @auto_docstring"
        },
        {
            "sha": "75b25544c7507578b592933092b46cc5fa88f3f1",
            "filename": "src/transformers/models/lfm2/modeling_lfm2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -695,7 +695,7 @@ def forward(\n \n @auto_docstring\n class Lfm2ForCausalLM(Lfm2PreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.embed_tokens.weight\"}\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n     _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n "
        },
        {
            "sha": "72bc6d19cf76e974d04896989bf4cbf2d4138767",
            "filename": "src/transformers/models/lfm2_moe/modeling_lfm2_moe.py",
            "status": "modified",
            "additions": 29,
            "deletions": 22,
            "changes": 51,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fmodeling_lfm2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fmodeling_lfm2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fmodeling_lfm2_moe.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -24,6 +24,7 @@\n import torch.nn.functional as F\n from torch import nn\n \n+from ...activations import ACT2FN\n from ...cache_utils import Cache\n from ...generation import GenerationMixin\n from ...integrations import use_kernel_forward_from_hub\n@@ -144,37 +145,43 @@ def forward(self, x):\n         return self.w2(F.silu(self.w1(x)) * self.w3(x))\n \n \n-class Lfm2MoeExperts(nn.ModuleList):\n-    \"\"\"\n-    ModuleList of experts.\n-    \"\"\"\n+class Lfm2MoeExperts(nn.Module):\n+    \"\"\"Collection of expert weights stored as 3D tensors.\"\"\"\n \n     def __init__(self, config):\n         super().__init__()\n         self.num_experts = config.num_experts\n-        for _ in range(config.num_experts):\n-            self.append(Lfm2MoeMLP(config, intermediate_size=config.moe_intermediate_size))\n+        self.hidden_dim = config.hidden_size\n+        self.intermediate_dim = config.moe_intermediate_size\n+        self.gate_up_proj = nn.Parameter(torch.empty(self.num_experts, 2 * self.intermediate_dim, self.hidden_dim))\n+        self.down_proj = nn.Parameter(torch.empty(self.num_experts, self.hidden_dim, self.intermediate_dim))\n+        self.act_fn = ACT2FN[config.hidden_act]\n \n     def forward(\n-        self, hidden_states: torch.Tensor, top_k_index: torch.Tensor, top_k_weights: torch.Tensor\n+        self,\n+        hidden_states: torch.Tensor,\n+        top_k_index: torch.Tensor,\n+        top_k_weights: torch.Tensor,\n     ) -> torch.Tensor:\n-        \"\"\"\n-        Args:\n-            hidden_states: (batch_size * sequence_length, hidden_dim)\n-            top_k_index: (batch_size * sequence_length, top_k)\n-            top_k_weights: (batch_size * sequence_length, top_k)\n-        Returns:\n-            (batch_size * sequence_length, hidden_dim)\n-        \"\"\"\n         final_hidden_states = torch.zeros_like(hidden_states)\n-        expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=self.num_experts).permute(2, 1, 0)\n+        num_experts = top_k_weights.shape[1]\n+        with torch.no_grad():\n+            expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=num_experts + 1)\n+            expert_mask = expert_mask.permute(2, 1, 0)\n+            expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n \n-        expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n         for expert_idx in expert_hit:\n-            idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))\n-            current_state = hidden_states[None, top_x].reshape(-1, hidden_states.shape[-1])\n-            current_hidden_states = self[expert_idx](current_state) * top_k_weights[top_x, idx, None]\n-            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n+            expert_idx = expert_idx[0]\n+            if expert_idx == num_experts:\n+                continue\n+            _, token_idx = torch.where(expert_mask[expert_idx])\n+            current_state = hidden_states[token_idx]\n+            gate, up = nn.functional.linear(current_state, self.gate_up_proj[expert_idx]).chunk(2, dim=-1)\n+            current_hidden_states = self.act_fn(gate) * up\n+            current_hidden_states = nn.functional.linear(current_hidden_states, self.down_proj[expert_idx])\n+            current_hidden_states = current_hidden_states * top_k_weights[token_idx, expert_idx, None]\n+            final_hidden_states.index_add_(0, token_idx, current_hidden_states.to(final_hidden_states.dtype))\n+\n         return final_hidden_states\n \n \n@@ -762,7 +769,7 @@ def forward(\n \n @auto_docstring\n class Lfm2MoeForCausalLM(Lfm2MoePreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.embed_tokens.weight\"}\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n     _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n "
        },
        {
            "sha": "34d35d7cda8a4cbaca741879a037841cf9288d77",
            "filename": "src/transformers/models/lfm2_vl/modeling_lfm2_vl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fmodeling_lfm2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fmodeling_lfm2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fmodeling_lfm2_vl.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -76,7 +76,6 @@ def pixel_unshuffle(self, hidden_states: torch.Tensor):\n @auto_docstring\n class Lfm2VlPreTrainedModel(PreTrainedModel):\n     config: Lfm2VlConfig\n-    base_model_prefix = \"model\"\n     input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n     _skip_keys_device_placement = \"past_key_values\"\n@@ -86,6 +85,7 @@ class Lfm2VlPreTrainedModel(PreTrainedModel):\n     _can_compile_fullgraph = False\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n+    base_model_prefix = \"model\"\n \n \n @dataclass\n@@ -307,7 +307,7 @@ def forward(\n )\n class Lfm2VlForConditionalGeneration(Lfm2VlPreTrainedModel, GenerationMixin):\n     _checkpoint_conversion_mapping = {}\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.language_model.embed_tokens.weight\"}\n \n     def __init__(self, config: Lfm2VlConfig):\n         super().__init__(config)"
        },
        {
            "sha": "ec924e5000d66f30fb20ce1ca446a33e16cafd82",
            "filename": "src/transformers/models/lilt/modeling_lilt.py",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Flilt%2Fmodeling_lilt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Flilt%2Fmodeling_lilt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flilt%2Fmodeling_lilt.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -500,19 +500,20 @@ class LiltPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = []\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n \n \n @auto_docstring"
        },
        {
            "sha": "2000c8092fb2583db68c2dad0bd7dfcd1b8faeaf",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -438,7 +438,7 @@ def forward(\n \n @auto_docstring\n class LlamaForCausalLM(LlamaPreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.embed_tokens.weight\"}\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n     _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n "
        },
        {
            "sha": "c58848fbf29938c7e59ee89571b3e11643fcf737",
            "filename": "src/transformers/models/llama4/modeling_llama4.py",
            "status": "modified",
            "additions": 14,
            "deletions": 13,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -54,7 +54,7 @@ def __init__(self, config: Llama4TextConfig):\n         self.intermediate_size = config.intermediate_size\n         self.hidden_size = config.hidden_size\n         self.expert_dim = self.intermediate_size\n-        self.gate_up_proj = nn.Parameter(torch.empty(self.num_experts, self.hidden_size, 2 * self.expert_dim))\n+        self.gate_up_proj = nn.Parameter(torch.zeros(self.num_experts, self.hidden_size, 2 * self.expert_dim))\n         self.down_proj = nn.Parameter(torch.empty((self.num_experts, self.expert_dim, self.hidden_size)))\n         self.act_fn = ACT2FN[config.hidden_act]\n \n@@ -473,31 +473,32 @@ class Llama4PreTrainedModel(PreTrainedModel):\n     _can_compile_fullgraph = True\n     _supports_attention_backend = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         std = (\n             self.config.initializer_range\n             if hasattr(self.config, \"initializer_range\")\n             else self.config.text_config.initializer_range\n         )\n         if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.weight.data.fill_(1.0)\n-            module.bias.data.zero_()\n+            module.weight.fill_(1.0)\n+            module.bias.zero_()\n         elif isinstance(module, Llama4TextRMSNorm):\n-            module.weight.data.fill_(1.0)\n+            module.weight.fill_(1.0)\n         elif isinstance(module, Llama4TextExperts):\n-            module.gate_up_proj.data.normal_(mean=0.0, std=std)\n-            module.down_proj.data.normal_(mean=0.0, std=std)\n+            module.gate_up_proj.normal_(mean=0.0, std=std)\n+            module.down_proj.normal_(mean=0.0, std=std)\n         elif isinstance(module, Llama4VisionModel):\n-            module.class_embedding.data.normal_(std=module.scale)\n-            module.positional_embedding_vlm.data.normal_(std=module.scale)\n+            module.class_embedding.normal_(std=module.scale)\n+            module.positional_embedding_vlm.normal_(std=module.scale)\n \n \n @auto_docstring\n@@ -604,7 +605,7 @@ def forward(\n class Llama4ForCausalLM(Llama4PreTrainedModel, GenerationMixin):\n     _no_split_modules = [\"Llama4TextDecoderLayer\"]\n     base_model_prefix = \"language_model\"\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.embed_tokens.weight\"}\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n     config: Llama4TextConfig\n "
        },
        {
            "sha": "0541b91765020e978d295f272dbd0c8767608dd7",
            "filename": "src/transformers/models/llava/modeling_llava.py",
            "status": "modified",
            "additions": 5,
            "deletions": 8,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -110,7 +110,6 @@ def forward(self, image_features):\n @auto_docstring\n class LlavaPreTrainedModel(PreTrainedModel):\n     config: LlavaConfig\n-    base_model_prefix = \"\"\n     input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n     _skip_keys_device_placement = \"past_key_values\"\n@@ -129,8 +128,6 @@ class LlavaPreTrainedModel(PreTrainedModel):\n     \"\"\"\n )\n class LlavaModel(LlavaPreTrainedModel):\n-    _checkpoint_conversion_mapping = {\"language_model.model\": \"language_model\"}\n-\n     def __init__(self, config: LlavaConfig):\n         super().__init__(config)\n         self.vision_tower = AutoModel.from_config(config.vision_config)\n@@ -308,12 +305,12 @@ def forward(\n )\n class LlavaForConditionalGeneration(LlavaPreTrainedModel, GenerationMixin):\n     _checkpoint_conversion_mapping = {\n-        \"^language_model.model\": \"model.language_model\",\n-        \"^vision_tower\": \"model.vision_tower\",\n-        \"^multi_modal_projector\": \"model.multi_modal_projector\",\n-        \"^language_model.lm_head\": \"lm_head\",\n+        r\"^language_model.model\": \"model.language_model\",\n+        r\"^vision_tower\": \"model.vision_tower\",\n+        r\"^multi_modal_projector\": \"model.multi_modal_projector\",\n+        r\"^language_model.lm_head\": \"lm_head\",\n     }\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.language_model.embed_tokens.weight\"}\n \n     def __init__(self, config: LlavaConfig):\n         super().__init__(config)"
        },
        {
            "sha": "d494e0400f2a93235e94769b318347ccbacb3fd4",
            "filename": "src/transformers/models/llava_next/modeling_llava_next.py",
            "status": "modified",
            "additions": 14,
            "deletions": 10,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -235,16 +235,17 @@ class LlavaNextPreTrainedModel(PreTrainedModel):\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         std = getattr(self.config, \"initializer_range\", self.config.get_text_config().initializer_range)\n \n         if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, LlavaNextModel):\n             embed_std = 1 / math.sqrt(self.config.text_config.hidden_size)\n-            module.image_newline.data.normal_(mean=0.0, std=embed_std)\n+            module.image_newline.normal_(mean=0.0, std=embed_std)\n \n \n @auto_docstring(\n@@ -253,7 +254,10 @@ def _init_weights(self, module):\n     \"\"\"\n )\n class LlavaNextModel(LlavaNextPreTrainedModel):\n-    _checkpoint_conversion_mapping = {\"language_model.model\": \"language_model\"}\n+    _checkpoint_conversion_mapping = {\n+        r\"^language_model.model\": \"language_model\",\n+    }\n+    base_model_prefix = \"model\"\n \n     def __init__(self, config: LlavaNextConfig):\n         super().__init__(config)\n@@ -534,13 +538,13 @@ def forward(\n )\n class LlavaNextForConditionalGeneration(LlavaNextPreTrainedModel, GenerationMixin):\n     _checkpoint_conversion_mapping = {\n-        \"^language_model.model\": \"model.language_model\",\n-        \"^vision_tower\": \"model.vision_tower\",\n-        \"^multi_modal_projector\": \"model.multi_modal_projector\",\n-        \"^image_newline\": \"model.image_newline\",\n-        \"^language_model.lm_head\": \"lm_head\",\n+        r\"^language_model.model\": \"model.language_model\",\n+        r\"^vision_tower\": \"model.vision_tower\",\n+        r\"^multi_modal_projector\": \"model.multi_modal_projector\",\n+        r\"^image_newline\": \"model.image_newline\",\n+        r\"^language_model.lm_head\": \"lm_head\",\n     }\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.language_model.embed_tokens.weight\"}\n \n     def __init__(self, config: LlavaNextConfig):\n         super().__init__(config)"
        },
        {
            "sha": "e4bb765e4a2a1fba48b7e5814d00e134689f7bef",
            "filename": "src/transformers/models/llava_next_video/modeling_llava_next_video.py",
            "status": "modified",
            "additions": 14,
            "deletions": 10,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -176,16 +176,17 @@ class LlavaNextVideoPreTrainedModel(PreTrainedModel):\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         std = getattr(self.config, \"initializer_range\", self.config.get_text_config().initializer_range)\n \n         if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, LlavaNextVideoModel):\n             embed_std = 1 / math.sqrt(self.config.text_config.hidden_size)\n-            module.image_newline.data.normal_(mean=0.0, std=embed_std)\n+            module.image_newline.normal_(mean=0.0, std=embed_std)\n \n \n def get_anyres_image_grid_shape(image_size, grid_pinpoints, patch_size):\n@@ -301,7 +302,10 @@ def unpad_image(tensor, original_size):\n     \"\"\"\n )\n class LlavaNextVideoModel(LlavaNextVideoPreTrainedModel):\n-    _checkpoint_conversion_mapping = {\"language_model.model\": \"language_model\"}\n+    _checkpoint_conversion_mapping = {\n+        r\"^language_model.model\": \"language_model\",\n+    }\n+    base_model_prefix = \"model\"\n \n     def __init__(\n         self,\n@@ -673,13 +677,13 @@ def get_video_features(\n )\n class LlavaNextVideoForConditionalGeneration(LlavaNextVideoPreTrainedModel, GenerationMixin):\n     _checkpoint_conversion_mapping = {\n-        \"^language_model.model\": \"model.language_model\",\n-        \"^vision_tower\": \"model.vision_tower\",\n-        \"^multi_modal_projector\": \"model.multi_modal_projector\",\n-        \"^image_newline\": \"model.image_newline\",\n-        \"^language_model.lm_head\": \"lm_head\",\n+        r\"^language_model.model\": \"model.language_model\",\n+        r\"^vision_tower\": \"model.vision_tower\",\n+        r\"^multi_modal_projector\": \"model.multi_modal_projector\",\n+        r\"^image_newline\": \"model.image_newline\",\n+        r\"^language_model.lm_head\": \"lm_head\",\n     }\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.language_model.embed_tokens.weight\"}\n \n     def __init__(self, config: LlavaNextVideoConfig):\n         super().__init__(config)"
        },
        {
            "sha": "193ab3a2ea047150474a2f0e83c5f898901c08d4",
            "filename": "src/transformers/models/llava_onevision/modeling_llava_onevision.py",
            "status": "modified",
            "additions": 14,
            "deletions": 10,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -117,16 +117,17 @@ class LlavaOnevisionPreTrainedModel(PreTrainedModel):\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         std = getattr(self.config, \"initializer_range\", self.config.get_text_config().initializer_range)\n \n         if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, LlavaOnevisionModel):\n             embed_std = 1 / math.sqrt(self.config.text_config.hidden_size)\n-            module.image_newline.data.normal_(mean=0.0, std=embed_std)\n+            module.image_newline.normal_(mean=0.0, std=embed_std)\n \n \n class LlavaOnevisionMultiModalProjector(nn.Module):\n@@ -264,7 +265,10 @@ def unpad_image(tensor, original_size):\n     \"\"\"\n )\n class LlavaOnevisionModel(LlavaOnevisionPreTrainedModel):\n-    _checkpoint_conversion_mapping = {\"language_model.model\": \"language_model\"}\n+    _checkpoint_conversion_mapping = {\n+        r\"^language_model.model\": \"language_model\",\n+    }\n+    base_model_prefix = \"model\"\n \n     def __init__(self, config):\n         super().__init__(config)\n@@ -661,13 +665,13 @@ def apply_pooling(self, image_features):\n )\n class LlavaOnevisionForConditionalGeneration(LlavaOnevisionPreTrainedModel, GenerationMixin):\n     _checkpoint_conversion_mapping = {\n-        \"^language_model.model\": \"model.language_model\",\n-        \"^vision_tower\": \"model.vision_tower\",\n-        \"^multi_modal_projector\": \"model.multi_modal_projector\",\n-        \"^image_newline\": \"model.image_newline\",\n-        \"^language_model.lm_head\": \"lm_head\",\n+        r\"^language_model.model\": \"model.language_model\",\n+        r\"^vision_tower\": \"model.vision_tower\",\n+        r\"^multi_modal_projector\": \"model.multi_modal_projector\",\n+        r\"^image_newline\": \"model.image_newline\",\n+        r\"^language_model.lm_head\": \"lm_head\",\n     }\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.language_model.embed_tokens.weight\"}\n \n     def __init__(self, config: LlavaOnevisionConfig):\n         super().__init__(config)"
        },
        {
            "sha": "516bfee996772730d0953caff4fdba2ad80832cd",
            "filename": "src/transformers/models/longcat_flash/modeling_longcat_flash.py",
            "status": "modified",
            "additions": 45,
            "deletions": 19,
            "changes": 64,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fmodeling_longcat_flash.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fmodeling_longcat_flash.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fmodeling_longcat_flash.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -164,7 +164,7 @@ def forward(self, hidden_states):\n         topk_indices = self.get_topk_indices(scores)\n         topk_weights = scores.gather(1, topk_indices)\n         topk_weights = topk_weights * self.routed_scaling_factor\n-        return topk_indices, topk_weights\n+        return topk_weights.to(router_logits.dtype), topk_indices\n \n     @torch.no_grad()\n     def get_topk_indices(self, scores):\n@@ -173,29 +173,51 @@ def get_topk_indices(self, scores):\n         return topk_indices\n \n \n-class LongcatFlashExperts(nn.ModuleList):\n+class LongcatFlashExperts(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n         self.intermediate_size = config.expert_ffn_hidden_size\n         self.hidden_size = config.hidden_size\n-        self.num_experts = config.n_routed_experts + config.zero_expert_num\n-        self.zero_expert_num = config.zero_expert_num\n+        self.num_routed_experts = config.n_routed_experts\n+        self.zero_expert_num = config.zero_expert_num or 0\n+        self.total_experts = self.num_routed_experts + self.zero_expert_num\n+        self.act_fn = ACT2FN[config.hidden_act]\n \n-        self.extend(\n-            [LongcatFlashMLP(config, intermediate_size=self.intermediate_size) for _ in range(self.num_experts)]\n-            + [nn.Identity() for _ in range(self.zero_expert_num)]\n-        )\n+        if self.num_routed_experts > 0:\n+            self.gate_up_proj = nn.Parameter(\n+                torch.empty(self.total_experts, 2 * self.intermediate_size, self.hidden_size)\n+            )\n+            self.down_proj = nn.Parameter(\n+                torch.empty(self.num_routed_experts, self.hidden_size, self.intermediate_size)\n+            )\n+        else:\n+            self.register_parameter(\"gate_up_proj\", None)\n+            self.register_parameter(\"down_proj\", None)\n \n     def forward(self, hidden_states, top_k_index, top_k_weights):\n         final_hidden_states = torch.zeros_like(hidden_states)\n-        expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=self.num_experts).permute(2, 1, 0)\n-\n-        expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n-        for expert_idx in expert_hit:\n-            idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))\n-            current_state = hidden_states[None, top_x].reshape(-1, hidden_states.shape[-1])\n-            current_hidden_states = self[expert_idx](current_state) * top_k_weights[top_x, idx, None]\n-            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n+        if top_k_index.numel() == 0:\n+            return final_hidden_states\n+\n+        expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=self.total_experts).permute(2, 1, 0)\n+\n+        expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero(as_tuple=False)\n+        for expert_idx_tensor in expert_hit:\n+            expert_idx = int(expert_idx_tensor.item())\n+            selection_idx, token_idx = torch.where(expert_mask[expert_idx].squeeze(0))\n+            if token_idx.numel() == 0:\n+                continue\n+            current_state = hidden_states[token_idx]\n+\n+            if expert_idx >= self.num_routed_experts or self.gate_up_proj is None:\n+                current_hidden_states = current_state\n+            else:\n+                gate, up = F.linear(current_state, self.gate_up_proj[expert_idx]).chunk(2, dim=-1)\n+                current_hidden_states = self.act_fn(gate) * up\n+                current_hidden_states = F.linear(current_hidden_states, self.down_proj[expert_idx])\n+\n+            current_hidden_states = current_hidden_states * top_k_weights[token_idx, selection_idx, None]\n+            final_hidden_states.index_add_(0, token_idx, current_hidden_states.to(hidden_states.dtype))\n         return final_hidden_states\n \n \n@@ -215,7 +237,7 @@ def __init__(self, config):\n \n     def forward(self, hidden_states):\n         orig_shape = hidden_states.shape\n-        topk_indices, topk_weights = self.router(hidden_states)\n+        topk_weights, topk_indices = self.router(hidden_states)\n         hidden_states = hidden_states.view(-1, hidden_states.shape[-1])\n         hidden_states = self.experts(hidden_states, topk_indices, topk_weights).view(*orig_shape)\n         return hidden_states\n@@ -535,10 +557,14 @@ class LongcatFlashPreTrainedModel(PreTrainedModel):\n         \"attentions\": LongcatFlashMLA,\n     }\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, LongcatFlashTopkRouter):\n-            module.classifier.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.classifier.weight.normal_(mean=0.0, std=self.config.initializer_range)\n+        if isinstance(module, LongcatFlashExperts):\n+            module.gate_up_proj.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.down_proj.normal_(mean=0.0, std=self.config.initializer_range)\n \n \n @auto_docstring\n@@ -630,7 +656,7 @@ def forward(\n \n @auto_docstring\n class LongcatFlashForCausalLM(LongcatFlashPreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.embed_tokens.weight\"}\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n     _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n     _keys_to_ignore_on_load_unexpected = [r\"model\\.mtp.*\"]"
        },
        {
            "sha": "6a9148dab61752f7392623d334814ed6388d1b67",
            "filename": "src/transformers/models/longcat_flash/modular_longcat_flash.py",
            "status": "modified",
            "additions": 61,
            "deletions": 23,
            "changes": 84,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fmodular_longcat_flash.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fmodular_longcat_flash.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fmodular_longcat_flash.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -20,26 +20,27 @@\n import torch.nn.functional as F\n from torch import nn\n \n+from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...masking_utils import create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import TransformersKwargs, logging\n+from ...utils import TransformersKwargs, auto_docstring, logging\n from ..deepseek_v3.modeling_deepseek_v3 import (\n     DeepseekV3Attention,\n     DeepseekV3ForCausalLM,\n     DeepseekV3MLP,\n     DeepseekV3Model,\n-    DeepseekV3PreTrainedModel,\n     DeepseekV3RMSNorm,\n     DeepseekV3RotaryEmbedding,\n     DeepseekV3TopkRouter,\n     apply_rotary_pos_emb_interleave,\n     eager_attention_forward,\n )\n+from .configuration_longcat_flash import LongcatFlashConfig\n \n \n logger = logging.get_logger(__name__)\n@@ -90,32 +91,54 @@ def forward(self, hidden_states):\n         topk_indices = self.get_topk_indices(scores)\n         topk_weights = scores.gather(1, topk_indices)\n         topk_weights = topk_weights * self.routed_scaling_factor\n-        return topk_indices, topk_weights\n+        return topk_weights.to(router_logits.dtype), topk_indices\n \n \n-class LongcatFlashExperts(nn.ModuleList):\n+class LongcatFlashExperts(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n         self.intermediate_size = config.expert_ffn_hidden_size\n         self.hidden_size = config.hidden_size\n-        self.num_experts = config.n_routed_experts + config.zero_expert_num\n-        self.zero_expert_num = config.zero_expert_num\n-\n-        self.extend(\n-            [LongcatFlashMLP(config, intermediate_size=self.intermediate_size) for _ in range(self.num_experts)]\n-            + [nn.Identity() for _ in range(self.zero_expert_num)]\n-        )\n+        self.num_routed_experts = config.n_routed_experts\n+        self.zero_expert_num = config.zero_expert_num or 0\n+        self.total_experts = self.num_routed_experts + self.zero_expert_num\n+        self.act_fn = ACT2FN[config.hidden_act]\n+\n+        if self.num_routed_experts > 0:\n+            self.gate_up_proj = nn.Parameter(\n+                torch.empty(self.total_experts, 2 * self.intermediate_size, self.hidden_size)\n+            )\n+            self.down_proj = nn.Parameter(\n+                torch.empty(self.num_routed_experts, self.hidden_size, self.intermediate_size)\n+            )\n+        else:\n+            self.register_parameter(\"gate_up_proj\", None)\n+            self.register_parameter(\"down_proj\", None)\n \n     def forward(self, hidden_states, top_k_index, top_k_weights):\n         final_hidden_states = torch.zeros_like(hidden_states)\n-        expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=self.num_experts).permute(2, 1, 0)\n-\n-        expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n-        for expert_idx in expert_hit:\n-            idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))\n-            current_state = hidden_states[None, top_x].reshape(-1, hidden_states.shape[-1])\n-            current_hidden_states = self[expert_idx](current_state) * top_k_weights[top_x, idx, None]\n-            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n+        if top_k_index.numel() == 0:\n+            return final_hidden_states\n+\n+        expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=self.total_experts).permute(2, 1, 0)\n+\n+        expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero(as_tuple=False)\n+        for expert_idx_tensor in expert_hit:\n+            expert_idx = int(expert_idx_tensor.item())\n+            selection_idx, token_idx = torch.where(expert_mask[expert_idx].squeeze(0))\n+            if token_idx.numel() == 0:\n+                continue\n+            current_state = hidden_states[token_idx]\n+\n+            if expert_idx >= self.num_routed_experts or self.gate_up_proj is None:\n+                current_hidden_states = current_state\n+            else:\n+                gate, up = F.linear(current_state, self.gate_up_proj[expert_idx]).chunk(2, dim=-1)\n+                current_hidden_states = self.act_fn(gate) * up\n+                current_hidden_states = F.linear(current_hidden_states, self.down_proj[expert_idx])\n+\n+            current_hidden_states = current_hidden_states * top_k_weights[token_idx, selection_idx, None]\n+            final_hidden_states.index_add_(0, token_idx, current_hidden_states.to(hidden_states.dtype))\n         return final_hidden_states\n \n \n@@ -135,7 +158,7 @@ def __init__(self, config):\n \n     def forward(self, hidden_states):\n         orig_shape = hidden_states.shape\n-        topk_indices, topk_weights = self.router(hidden_states)\n+        topk_weights, topk_indices = self.router(hidden_states)\n         hidden_states = hidden_states.view(-1, hidden_states.shape[-1])\n         hidden_states = self.experts(hidden_states, topk_indices, topk_weights).view(*orig_shape)\n         return hidden_states\n@@ -301,16 +324,31 @@ def forward(\n         return hidden_states\n \n \n-class LongcatFlashPreTrainedModel(DeepseekV3PreTrainedModel):\n+@auto_docstring\n+class LongcatFlashPreTrainedModel(PreTrainedModel):\n+    config: LongcatFlashConfig\n+    base_model_prefix = \"model\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"LongcatFlashDecoderLayer\"]\n+    _skip_keys_device_placement = [\"past_key_values\"]\n+    _supports_flash_attn = True\n+    _supports_sdpa = True\n+    _supports_flex_attn = True\n+    _can_compile_fullgraph = False\n+    _supports_attention_backend = True\n     _can_record_outputs = {\n         \"hidden_states\": LongcatFlashDecoderLayer,\n         \"attentions\": LongcatFlashMLA,\n     }\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n-        PreTrainedModel._init_weights(self, module)\n+        super()._init_weights(module)\n         if isinstance(module, LongcatFlashTopkRouter):\n-            module.classifier.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.classifier.weight.normal_(mean=0.0, std=self.config.initializer_range)\n+        if isinstance(module, LongcatFlashExperts):\n+            module.gate_up_proj.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.down_proj.normal_(mean=0.0, std=self.config.initializer_range)\n \n \n class LongcatFlashModel(DeepseekV3Model):"
        },
        {
            "sha": "1168e9366f1d8bdb66ee57f0ad2c324b1f431c65",
            "filename": "src/transformers/models/longformer/modeling_longformer.py",
            "status": "modified",
            "additions": 11,
            "deletions": 16,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Flongformer%2Fmodeling_longformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Flongformer%2Fmodeling_longformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongformer%2Fmodeling_longformer.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -1273,7 +1273,6 @@ def __init__(self, config):\n \n         self.decoder = nn.Linear(config.hidden_size, config.vocab_size)\n         self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n-        self.decoder.bias = self.bias\n \n     def forward(self, features, **kwargs):\n         x = self.dense(features)\n@@ -1285,14 +1284,6 @@ def forward(self, features, **kwargs):\n \n         return x\n \n-    def _tie_weights(self):\n-        # To tie those two weights if they get disconnected (on TPU or when the bias is resized)\n-        # For accelerate compatibility and to not break backward compatibility\n-        if self.decoder.bias.device.type == \"meta\":\n-            self.decoder.bias = self.bias\n-        else:\n-            self.bias = self.decoder.bias\n-\n \n @auto_docstring\n class LongformerPreTrainedModel(PreTrainedModel):\n@@ -1301,19 +1292,20 @@ class LongformerPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"LongformerSelfAttention\"]\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n \n \n @auto_docstring\n@@ -1557,7 +1549,10 @@ def forward(\n \n @auto_docstring\n class LongformerForMaskedLM(LongformerPreTrainedModel):\n-    _tied_weights_keys = [\"lm_head.decoder\"]\n+    _tied_weights_keys = {\n+        \"lm_head.decoder.weight\": \"longformer.embeddings.word_embeddings.weight\",\n+        \"lm_head.decoder.bias\": \"lm_head.bias\",\n+    }\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "0aea13dc01b801b710c6bb810d2ffcac42772531",
            "filename": "src/transformers/models/longt5/modeling_longt5.py",
            "status": "modified",
            "additions": 38,
            "deletions": 76,
            "changes": 114,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -1176,75 +1176,45 @@ def dummy_inputs(self):\n         }\n         return dummy_inputs\n \n-    def _try_load_missing_tied_module(self, key):\n-        module = self\n-        key = key.removesuffix(\".weight\")\n-        for sub_key in key.split(\".\"):\n-            if not hasattr(module, sub_key):\n-                return\n-            module = getattr(module, sub_key)\n-\n-        self._tie_embedding_weights(module, self.shared)\n-\n-    @classmethod\n-    def from_pretrained(self, *args, **kwargs):\n-        requested_loading_info = kwargs.get(\"output_loading_info\", False)\n-        kwargs[\"output_loading_info\"] = True\n-        model, loading_info = super().from_pretrained(*args, **kwargs)\n-        missing_keys = loading_info.get(\"missing_keys\", [])\n-\n-        if hasattr(model, \"shared\") and hasattr(model, \"_tied_weights_keys\"):\n-            for missing_key in missing_keys:\n-                logger.warning(\n-                    f\"Recovering a missing tied weight {missing_key} from a legacy LongT5 checkpoint. \"\n-                    f\"Consider saving {missing_key} in your checkpoint or updating the config (tie_word_embeddings=true).\"\n-                )\n-                model._try_load_missing_tied_module(missing_key)\n-\n-        if requested_loading_info:\n-            return model, loading_info\n-        return model\n-\n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         factor = self.config.initializer_factor  # Used for testing weights initialization\n         if isinstance(module, LongT5LayerNorm):\n-            module.weight.data.fill_(factor * 1.0)\n+            module.weight.fill_(factor * 1.0)\n         elif isinstance(module, (LongT5Model, LongT5ForConditionalGeneration, LongT5EncoderModel)):\n-            module.shared.weight.data.normal_(mean=0.0, std=factor * 1.0)\n+            module.shared.weight.normal_(mean=0.0, std=factor * 1.0)\n             if hasattr(module, \"lm_head\") and not self.config.tie_word_embeddings:\n-                module.lm_head.weight.data.normal_(mean=0.0, std=factor * 1.0)\n+                module.lm_head.weight.normal_(mean=0.0, std=factor * 1.0)\n         elif isinstance(module, LongT5DenseActDense):\n-            module.wi.weight.data.normal_(mean=0.0, std=factor * ((self.config.d_model) ** -0.5))\n+            module.wi.weight.normal_(mean=0.0, std=factor * ((self.config.d_model) ** -0.5))\n             if hasattr(module.wi, \"bias\") and module.wi.bias is not None:\n-                module.wi.bias.data.zero_()\n-            module.wo.weight.data.normal_(mean=0.0, std=factor * ((self.config.d_ff) ** -0.5))\n+                module.wi.bias.zero_()\n+            module.wo.weight.normal_(mean=0.0, std=factor * ((self.config.d_ff) ** -0.5))\n             if hasattr(module.wo, \"bias\") and module.wo.bias is not None:\n-                module.wo.bias.data.zero_()\n+                module.wo.bias.zero_()\n         elif isinstance(module, LongT5DenseGatedActDense):\n-            module.wi_0.weight.data.normal_(mean=0.0, std=factor * ((self.config.d_model) ** -0.5))\n+            module.wi_0.weight.normal_(mean=0.0, std=factor * ((self.config.d_model) ** -0.5))\n             if hasattr(module.wi_0, \"bias\") and module.wi_0.bias is not None:\n-                module.wi_0.bias.data.zero_()\n-            module.wi_1.weight.data.normal_(mean=0.0, std=factor * ((self.config.d_model) ** -0.5))\n+                module.wi_0.bias.zero_()\n+            module.wi_1.weight.normal_(mean=0.0, std=factor * ((self.config.d_model) ** -0.5))\n             if hasattr(module.wi_1, \"bias\") and module.wi_1.bias is not None:\n-                module.wi_1.bias.data.zero_()\n-            module.wo.weight.data.normal_(mean=0.0, std=factor * ((self.config.d_ff) ** -0.5))\n+                module.wi_1.bias.zero_()\n+            module.wo.weight.normal_(mean=0.0, std=factor * ((self.config.d_ff) ** -0.5))\n             if hasattr(module.wo, \"bias\") and module.wo.bias is not None:\n-                module.wo.bias.data.zero_()\n+                module.wo.bias.zero_()\n         elif isinstance(module, (LongT5Attention, LongT5LocalAttention, LongT5TransientGlobalAttention)):\n             d_model = self.config.d_model\n             key_value_proj_dim = self.config.d_kv\n             n_heads = self.config.num_heads\n-            module.q.weight.data.normal_(mean=0.0, std=factor * ((d_model * key_value_proj_dim) ** -0.5))\n-            module.k.weight.data.normal_(mean=0.0, std=factor * (d_model**-0.5))\n-            module.v.weight.data.normal_(mean=0.0, std=factor * (d_model**-0.5))\n-            module.o.weight.data.normal_(mean=0.0, std=factor * ((n_heads * key_value_proj_dim) ** -0.5))\n+            module.q.weight.normal_(mean=0.0, std=factor * ((d_model * key_value_proj_dim) ** -0.5))\n+            module.k.weight.normal_(mean=0.0, std=factor * (d_model**-0.5))\n+            module.v.weight.normal_(mean=0.0, std=factor * (d_model**-0.5))\n+            module.o.weight.normal_(mean=0.0, std=factor * ((n_heads * key_value_proj_dim) ** -0.5))\n             if module.has_relative_attention_bias:\n-                module.relative_attention_bias.weight.data.normal_(mean=0.0, std=factor * ((d_model) ** -0.5))\n+                module.relative_attention_bias.weight.normal_(mean=0.0, std=factor * ((d_model) ** -0.5))\n                 if isinstance(module, LongT5TransientGlobalAttention):\n-                    module.global_relative_attention_bias.weight.data.normal_(\n-                        mean=0.0, std=factor * ((d_model) ** -0.5)\n-                    )\n+                    module.global_relative_attention_bias.weight.normal_(mean=0.0, std=factor * ((d_model) ** -0.5))\n \n     # Copied from transformers.models.t5.modeling_t5.T5PreTrainedModel._shift_right with T5->LongT5\n     def _shift_right(self, input_ids):\n@@ -1270,12 +1240,10 @@ def _shift_right(self, input_ids):\n \n \n class LongT5Stack(LongT5PreTrainedModel):\n-    def __init__(self, config, embed_tokens=None):\n+    def __init__(self, config):\n         super().__init__(config)\n \n         self.embed_tokens = nn.Embedding(config.vocab_size, config.d_model)\n-        if embed_tokens is not None:\n-            self.embed_tokens.weight = embed_tokens.weight\n         self.is_decoder = config.is_decoder\n \n         self.local_radius = config.local_radius\n@@ -1599,7 +1567,10 @@ class LongT5Model(LongT5PreTrainedModel):\n     _keys_to_ignore_on_load_unexpected = [\n         r\"decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight\",\n     ]\n-    _tied_weights_keys = [\"encoder.embed_tokens.weight\", \"decoder.embed_tokens.weight\"]\n+    _tied_weights_keys = {\n+        \"encoder.embed_tokens.weight\": \"shared.weight\",\n+        \"decoder.embed_tokens.weight\": \"shared.weight\",\n+    }\n \n     def __init__(self, config: LongT5Config):\n         super().__init__(config)\n@@ -1609,13 +1580,13 @@ def __init__(self, config: LongT5Config):\n         encoder_config.is_decoder = False\n         encoder_config.use_cache = False\n         encoder_config.tie_encoder_decoder = False\n-        self.encoder = LongT5Stack(encoder_config, self.shared)\n+        self.encoder = LongT5Stack(encoder_config)\n \n         decoder_config = copy.deepcopy(config)\n         decoder_config.is_decoder = True\n         decoder_config.tie_encoder_decoder = False\n         decoder_config.num_layers = config.num_decoder_layers\n-        self.decoder = LongT5Stack(decoder_config, self.shared)\n+        self.decoder = LongT5Stack(decoder_config)\n \n         # Initialize weights and apply final processing\n         self.post_init()\n@@ -1628,11 +1599,6 @@ def set_input_embeddings(self, new_embeddings):\n         self.encoder.set_input_embeddings(new_embeddings)\n         self.decoder.set_input_embeddings(new_embeddings)\n \n-    def _tie_weights(self):\n-        if self.config.tie_word_embeddings:\n-            self._tie_embedding_weights(self.encoder.embed_tokens, self.shared)\n-            self._tie_embedding_weights(self.decoder.embed_tokens, self.shared)\n-\n     def get_encoder(self):\n         return self.encoder\n \n@@ -1763,7 +1729,11 @@ class LongT5ForConditionalGeneration(LongT5PreTrainedModel, GenerationMixin):\n     _keys_to_ignore_on_load_unexpected = [\n         r\"decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight\",\n     ]\n-    _tied_weights_keys = [\"encoder.embed_tokens.weight\", \"decoder.embed_tokens.weight\", \"lm_head.weight\"]\n+    _tied_weights_keys = {\n+        \"encoder.embed_tokens.weight\": \"shared.weight\",\n+        \"decoder.embed_tokens.weight\": \"shared.weight\",\n+        \"lm_head.weight\": \"shared.weight\",\n+    }\n \n     def __init__(self, config: LongT5Config):\n         super().__init__(config)\n@@ -1775,13 +1745,13 @@ def __init__(self, config: LongT5Config):\n         encoder_config.is_decoder = False\n         encoder_config.use_cache = False\n         encoder_config.tie_encoder_decoder = False\n-        self.encoder = LongT5Stack(encoder_config, self.shared)\n+        self.encoder = LongT5Stack(encoder_config)\n \n         decoder_config = copy.deepcopy(config)\n         decoder_config.is_decoder = True\n         decoder_config.tie_encoder_decoder = False\n         decoder_config.num_layers = config.num_decoder_layers\n-        self.decoder = LongT5Stack(decoder_config, self.shared)\n+        self.decoder = LongT5Stack(decoder_config)\n \n         self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n \n@@ -1796,11 +1766,6 @@ def set_input_embeddings(self, new_embeddings):\n         self.encoder.set_input_embeddings(new_embeddings)\n         self.decoder.set_input_embeddings(new_embeddings)\n \n-    def _tie_weights(self):\n-        if self.config.tie_word_embeddings:\n-            self._tie_embedding_weights(self.encoder.embed_tokens, self.shared)\n-            self._tie_embedding_weights(self.decoder.embed_tokens, self.shared)\n-\n     def get_encoder(self):\n         return self.encoder\n \n@@ -1952,7 +1917,9 @@ def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n \n @auto_docstring\n class LongT5EncoderModel(LongT5PreTrainedModel):\n-    _tied_weights_keys = [\"encoder.embed_tokens.weight\"]\n+    _tied_weights_keys = {\n+        \"encoder.embed_tokens.weight\": \"shared.weight\",\n+    }\n     _keys_to_ignore_on_load_unexpected = [r\"decoder\"]\n \n     def __init__(self, config: LongT5Config):\n@@ -1961,8 +1928,7 @@ def __init__(self, config: LongT5Config):\n \n         encoder_config = copy.deepcopy(config)\n         encoder_config.use_cache = False\n-        encoder_config.tie_encoder_decoder = False\n-        self.encoder = LongT5Stack(encoder_config, self.shared)\n+        self.encoder = LongT5Stack(encoder_config)\n \n         # Initialize weights and apply final processing\n         self.post_init()\n@@ -1974,10 +1940,6 @@ def set_input_embeddings(self, new_embeddings):\n         self.shared = new_embeddings\n         self.encoder.set_input_embeddings(new_embeddings)\n \n-    def _tie_weights(self):\n-        if self.config.tie_word_embeddings:\n-            self._tie_embedding_weights(self.encoder.embed_tokens, self.shared)\n-\n     def get_encoder(self):\n         return self.encoder\n "
        },
        {
            "sha": "79b63ac33d86e143bc76e09c473f0d33aaa889bc",
            "filename": "src/transformers/models/luke/modeling_luke.py",
            "status": "modified",
            "additions": 12,
            "deletions": 21,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fluke%2Fmodeling_luke.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fluke%2Fmodeling_luke.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fluke%2Fmodeling_luke.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -766,22 +766,23 @@ class LukePreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"LukeAttention\", \"LukeEntityEmbeddings\"]\n \n+    @torch.no_grad()\n     def _init_weights(self, module: nn.Module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n             if module.embedding_dim == 1:  # embedding for bias parameters\n-                module.weight.data.zero_()\n+                module.weight.zero_()\n             else:\n-                module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+                module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n \n \n @auto_docstring(\n@@ -1024,7 +1025,6 @@ def __init__(self, config):\n \n         self.decoder = nn.Linear(config.hidden_size, config.vocab_size)\n         self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n-        self.decoder.bias = self.bias\n \n     def forward(self, features, **kwargs):\n         x = self.dense(features)\n@@ -1036,14 +1036,6 @@ def forward(self, features, **kwargs):\n \n         return x\n \n-    def _tie_weights(self):\n-        # To tie those two weights if they get disconnected (on TPU or when the bias is resized)\n-        # For accelerate compatibility and to not break backward compatibility\n-        if self.decoder.bias.device.type == \"meta\":\n-            self.decoder.bias = self.bias\n-        else:\n-            self.bias = self.decoder.bias\n-\n \n @auto_docstring(\n     custom_intro=\"\"\"\n@@ -1052,7 +1044,10 @@ def _tie_weights(self):\n     \"\"\"\n )\n class LukeForMaskedLM(LukePreTrainedModel):\n-    _tied_weights_keys = [\"lm_head.decoder.weight\", \"lm_head.decoder.bias\", \"entity_predictions.decoder.weight\"]\n+    _tied_weights_keys = {\n+        \"entity_predictions.decoder.weight\": \"luke.entity_embeddings.entity_embeddings.weight\",\n+        \"lm_head.bias\": \"lm_head.decoder.bias\",\n+    }\n \n     def __init__(self, config):\n         super().__init__(config)\n@@ -1067,10 +1062,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def tie_weights(self):\n-        super().tie_weights()\n-        self._tie_embedding_weights(self.entity_predictions.decoder, self.luke.entity_embeddings.entity_embeddings)\n-\n     def get_output_embeddings(self):\n         return self.lm_head.decoder\n "
        },
        {
            "sha": "69fc0eb1b71acfd0c92a3f5991d41518e19105ab",
            "filename": "src/transformers/models/lxmert/modeling_lxmert.py",
            "status": "modified",
            "additions": 18,
            "deletions": 25,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Flxmert%2Fmodeling_lxmert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Flxmert%2Fmodeling_lxmert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flxmert%2Fmodeling_lxmert.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -597,19 +597,11 @@ def forward(self, hidden_states):\n \n \n class LxmertLMPredictionHead(nn.Module):\n-    def __init__(self, config, lxmert_model_embedding_weights):\n+    def __init__(self, config):\n         super().__init__()\n         self.transform = LxmertPredictionHeadTransform(config)\n-\n-        # The output weights are the same as the input embeddings, but there is\n-        # an output-only bias for each token.\n-        self.decoder = nn.Linear(\n-            lxmert_model_embedding_weights.size(1),\n-            lxmert_model_embedding_weights.size(0),\n-            bias=False,\n-        )\n-        self.decoder.weight = lxmert_model_embedding_weights\n-        self.bias = nn.Parameter(torch.zeros(lxmert_model_embedding_weights.size(0)))\n+        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n+        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n \n     def forward(self, hidden_states):\n         hidden_states = self.transform(hidden_states)\n@@ -664,9 +656,9 @@ def forward(self, hidden_states):\n \n \n class LxmertPreTrainingHeads(nn.Module):\n-    def __init__(self, config, lxmert_model_embedding_weights):\n+    def __init__(self, config):\n         super().__init__()\n-        self.predictions = LxmertLMPredictionHead(config, lxmert_model_embedding_weights)\n+        self.predictions = LxmertLMPredictionHead(config)\n         self.seq_relationship = nn.Linear(config.hidden_size, 2)\n \n     def forward(self, sequence_output, pooled_output):\n@@ -682,21 +674,22 @@ class LxmertPreTrainedModel(PreTrainedModel):\n     input_modalities = [\"image\", \"text\"]\n     _supports_param_buffer_assignment = False\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n         elif isinstance(module, LxmertLMPredictionHead):\n-            module.bias.data.zero_()\n+            module.bias.zero_()\n \n \n @auto_docstring\n@@ -851,7 +844,10 @@ def forward(\n \n @auto_docstring\n class LxmertForPreTraining(LxmertPreTrainedModel):\n-    _tied_weights_keys = [\"cls.predictions.decoder.weight\"]\n+    # help saving them\n+    _tied_weights_keys = {\n+        \"cls.predictions.decoder.weight\": \"lxmert.embeddings.word_embeddings.weight\",\n+    }\n \n     def __init__(self, config):\n         super().__init__(config)\n@@ -870,7 +866,7 @@ def __init__(self, config):\n         self.lxmert = LxmertModel(config)\n \n         # Pre-training heads\n-        self.cls = LxmertPreTrainingHeads(config, self.lxmert.embeddings.word_embeddings.weight)\n+        self.cls = LxmertPreTrainingHeads(config)\n         if self.task_obj_predict:\n             self.obj_predict_head = LxmertVisualObjHead(config)\n         if self.task_qa:\n@@ -908,9 +904,6 @@ def __init__(self, config):\n             }\n         self.visual_losses = visual_losses\n \n-    def _tie_weights(self):\n-        self.cls.predictions.decoder.weight = self.lxmert.embeddings.word_embeddings.weight\n-\n     def resize_token_embeddings(\n         self, new_num_tokens: int, pad_to_multiple_of: Optional[int] = None, mean_resizing: bool = True\n     ) -> nn.Embedding:"
        },
        {
            "sha": "60f41cd6ad007397cebe13554e80a820ac38ba6e",
            "filename": "src/transformers/models/m2m_100/modeling_m2m_100.py",
            "status": "modified",
            "additions": 16,
            "deletions": 23,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -516,19 +516,20 @@ class M2M100PreTrainedModel(PreTrainedModel):\n     # Doesn't support `compile` (dynamic control flow). Can be fixed but low usage model\n     _can_compile_fullgraph = False\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         std = self.config.init_std\n         if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.weight.data.fill_(1.0)\n-            module.bias.data.zero_()\n+            module.weight.fill_(1.0)\n+            module.bias.zero_()\n \n \n class M2M100Encoder(M2M100PreTrainedModel):\n@@ -541,7 +542,7 @@ class M2M100Encoder(M2M100PreTrainedModel):\n         embed_tokens (nn.Embedding): output embedding\n     \"\"\"\n \n-    def __init__(self, config: M2M100Config, embed_tokens: Optional[nn.Embedding] = None):\n+    def __init__(self, config: M2M100Config):\n         super().__init__(config)\n \n         self.dropout = config.dropout\n@@ -556,9 +557,6 @@ def __init__(self, config: M2M100Config, embed_tokens: Optional[nn.Embedding] =\n             config.vocab_size, embed_dim, self.padding_idx, embed_scale=embed_scale\n         )\n \n-        if embed_tokens is not None:\n-            self.embed_tokens.weight = embed_tokens.weight\n-\n         self.embed_positions = M2M100SinusoidalPositionalEmbedding(\n             config.max_position_embeddings,\n             embed_dim,\n@@ -694,7 +692,7 @@ class M2M100Decoder(M2M100PreTrainedModel):\n         embed_tokens (nn.Embedding): output embedding\n     \"\"\"\n \n-    def __init__(self, config: M2M100Config, embed_tokens: Optional[nn.Embedding] = None):\n+    def __init__(self, config: M2M100Config):\n         super().__init__(config)\n         self.dropout = config.dropout\n         self.layerdrop = config.decoder_layerdrop\n@@ -706,9 +704,6 @@ def __init__(self, config: M2M100Config, embed_tokens: Optional[nn.Embedding] =\n             config.vocab_size, config.d_model, self.padding_idx, embed_scale=embed_scale\n         )\n \n-        if embed_tokens is not None:\n-            self.embed_tokens.weight = embed_tokens.weight\n-\n         self.embed_positions = M2M100SinusoidalPositionalEmbedding(\n             config.max_position_embeddings,\n             config.d_model,\n@@ -920,7 +915,10 @@ def forward(\n \n @auto_docstring\n class M2M100Model(M2M100PreTrainedModel):\n-    _tied_weights_keys = [\"encoder.embed_tokens.weight\", \"decoder.embed_tokens.weight\"]\n+    _tied_weights_keys = {\n+        \"decoder.embed_tokens.weight\": \"shared.weight\",\n+        \"encoder.embed_tokens.weight\": \"shared.weight\",\n+    }\n \n     def __init__(self, config: M2M100Config):\n         super().__init__(config)\n@@ -929,8 +927,8 @@ def __init__(self, config: M2M100Config):\n         embed_scale = math.sqrt(config.d_model) if config.scale_embedding else 1.0\n         self.shared = M2M100ScaledWordEmbedding(vocab_size, config.d_model, padding_idx, embed_scale=embed_scale)\n \n-        self.encoder = M2M100Encoder(config, self.shared)\n-        self.decoder = M2M100Decoder(config, self.shared)\n+        self.encoder = M2M100Encoder(config)\n+        self.decoder = M2M100Decoder(config)\n \n         # Initialize weights and apply final processing\n         self.post_init()\n@@ -943,11 +941,6 @@ def set_input_embeddings(self, value):\n         self.encoder.embed_tokens = self.shared\n         self.decoder.embed_tokens = self.shared\n \n-    def _tie_weights(self):\n-        if self.config.tie_word_embeddings:\n-            self._tie_embedding_weights(self.encoder.embed_tokens, self.shared)\n-            self._tie_embedding_weights(self.decoder.embed_tokens, self.shared)\n-\n     def get_encoder(self):\n         return self.encoder\n \n@@ -1045,7 +1038,7 @@ def forward(\n )\n class M2M100ForConditionalGeneration(M2M100PreTrainedModel, GenerationMixin):\n     base_model_prefix = \"model\"\n-    _tied_weights_keys = [\"encoder.embed_tokens.weight\", \"decoder.embed_tokens.weight\", \"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.shared.weight\"}\n \n     def __init__(self, config: M2M100Config):\n         super().__init__(config)"
        },
        {
            "sha": "f17bd66649af7d7f290b635afbbe91ba292095b0",
            "filename": "src/transformers/models/mamba/modeling_mamba.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -504,6 +504,7 @@ class MambaPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _is_stateful = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights.\"\"\"\n         std = self.config.initializer_range\n@@ -513,7 +514,7 @@ def _init_weights(self, module):\n             A = torch.arange(1, module.ssm_state_size + 1, dtype=torch.float32)[None, :]\n             A = A.expand(module.intermediate_size, -1).contiguous()\n             module.A_log.copy_(torch.log(A))\n-            module.D.data.fill_(1.0)\n+            module.D.fill_(1.0)\n \n             dt_init_std = self.config.time_step_rank**-0.5 * self.config.time_step_scale\n             if self.config.time_step_init_scheme == \"constant\":\n@@ -558,7 +559,7 @@ def _init_weights(self, module):\n                 if not getattr(module.bias, \"_no_reinit\", False):\n                     nn.init.zeros_(module.bias)\n         elif isinstance(module, MambaRMSNorm):\n-            module.weight.data.fill_(1.0)\n+            module.weight.fill_(1.0)\n         elif isinstance(module, nn.Embedding):\n             nn.init.normal_(module.weight, std=std)\n \n@@ -721,7 +722,7 @@ def forward(\n     \"\"\"\n )\n class MambaForCausalLM(MambaPreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"backbone.embeddings.weight\"}\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "716f62e5d1b1c394000186a9a19165db8b322d2d",
            "filename": "src/transformers/models/mamba2/modeling_mamba2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -717,6 +717,7 @@ class Mamba2PreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _is_stateful = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights.\"\"\"\n         std = self.config.initializer_range\n@@ -725,7 +726,7 @@ def _init_weights(self, module):\n             # The core is to load them, compute the discrete states, then write the updated state. Keeps the memory bounded\n             A = torch.arange(1, self.config.num_heads + 1)\n             module.A_log.copy_(torch.log(A))\n-            module.D.data.fill_(1.0)\n+            module.D.fill_(1.0)\n \n             dt = torch.exp(\n                 torch.rand(self.config.num_heads)\n@@ -765,7 +766,7 @@ def _init_weights(self, module):\n                 if not getattr(module.bias, \"_no_reinit\", False):\n                     nn.init.zeros_(module.bias)\n         elif isinstance(module, (Mamba2RMSNorm, MambaRMSNormGated)):\n-            module.weight.data.fill_(1.0)\n+            module.weight.fill_(1.0)\n         elif isinstance(module, nn.Embedding):\n             nn.init.normal_(module.weight, std=std)\n \n@@ -934,7 +935,7 @@ def forward(\n     \"\"\"\n )\n class Mamba2ForCausalLM(Mamba2PreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = []\n+    _tied_weights_keys = {}\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "d9932a21f54e79cba25e3b5b80002f824ba3b018",
            "filename": "src/transformers/models/marian/configuration_marian.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fmarian%2Fconfiguration_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fmarian%2Fconfiguration_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarian%2Fconfiguration_marian.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -147,6 +147,7 @@ def __init__(\n         self.num_hidden_layers = encoder_layers\n         self.scale_embedding = scale_embedding  # scale factor will be sqrt(d_model) if True\n         self.share_encoder_decoder_embeddings = share_encoder_decoder_embeddings\n+        kwargs[\"tie_encoder_decoder\"] = share_encoder_decoder_embeddings\n         super().__init__(\n             pad_token_id=pad_token_id,\n             eos_token_id=eos_token_id,"
        },
        {
            "sha": "11adf1cdbe2086daeb312031fa6eb980b15a62a5",
            "filename": "src/transformers/models/marian/modeling_marian.py",
            "status": "modified",
            "additions": 36,
            "deletions": 57,
            "changes": 93,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -14,7 +14,6 @@\n # limitations under the License.\n \"\"\"PyTorch MarianMTModel model, ported from the Marian C++ repo.\"\"\"\n \n-import copy\n import math\n from collections.abc import Callable\n from typing import Optional, Union\n@@ -446,21 +445,22 @@ class MarianPreTrainedModel(PreTrainedModel):\n \n     _can_compile_fullgraph = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module: Union[nn.Linear, nn.Embedding, MarianSinusoidalPositionalEmbedding]):\n         std = self.config.init_std\n         if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, MarianSinusoidalPositionalEmbedding):\n             module._init_weight()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.weight.data.fill_(1.0)\n-            module.bias.data.zero_()\n+            module.weight.fill_(1.0)\n+            module.bias.zero_()\n \n     @property\n     def dummy_inputs(self):\n@@ -484,7 +484,7 @@ class MarianEncoder(MarianPreTrainedModel):\n         embed_tokens (nn.Embedding): output embedding\n     \"\"\"\n \n-    def __init__(self, config: MarianConfig, embed_tokens: Optional[nn.Embedding] = None):\n+    def __init__(self, config: MarianConfig):\n         super().__init__(config)\n \n         self.dropout = config.dropout\n@@ -495,10 +495,7 @@ def __init__(self, config: MarianConfig, embed_tokens: Optional[nn.Embedding] =\n         self.max_source_positions = config.max_position_embeddings\n         self.embed_scale = math.sqrt(embed_dim) if config.scale_embedding else 1.0\n \n-        if embed_tokens is not None:\n-            self.embed_tokens = embed_tokens\n-        else:\n-            self.embed_tokens = nn.Embedding(config.vocab_size, embed_dim, self.padding_idx)\n+        self.embed_tokens = nn.Embedding(config.vocab_size, embed_dim, self.padding_idx)\n \n         self.embed_positions = MarianSinusoidalPositionalEmbedding(\n             config.max_position_embeddings, embed_dim, self.padding_idx\n@@ -626,18 +623,15 @@ class MarianDecoder(MarianPreTrainedModel):\n         embed_tokens (nn.Embedding): output embedding\n     \"\"\"\n \n-    def __init__(self, config: MarianConfig, embed_tokens: Optional[nn.Embedding] = None):\n+    def __init__(self, config: MarianConfig):\n         super().__init__(config)\n         self.dropout = config.dropout\n         self.layerdrop = config.decoder_layerdrop\n         self.padding_idx = config.pad_token_id\n         self.max_target_positions = config.max_position_embeddings\n         self.embed_scale = math.sqrt(config.d_model) if config.scale_embedding else 1.0\n \n-        if embed_tokens is not None:\n-            self.embed_tokens = embed_tokens\n-        else:\n-            self.embed_tokens = nn.Embedding(config.decoder_vocab_size, config.d_model, self.padding_idx)\n+        self.embed_tokens = nn.Embedding(config.decoder_vocab_size, config.d_model, self.padding_idx)\n \n         self.embed_positions = MarianSinusoidalPositionalEmbedding(\n             config.max_position_embeddings, config.d_model, self.padding_idx\n@@ -846,26 +840,28 @@ def forward(\n \n @auto_docstring\n class MarianModel(MarianPreTrainedModel):\n-    _tied_weights_keys = [\"encoder.embed_tokens.weight\", \"decoder.embed_tokens.weight\"]\n+    _keys_to_ignore_on_load_missing = [\n+        \"model.encoder.embed_positions.weight\",\n+        \"model.decoder.embed_positions.weight\",\n+    ]\n \n     def __init__(self, config: MarianConfig):\n         super().__init__(config)\n \n         padding_idx, vocab_size = config.pad_token_id, config.vocab_size\n \n         # We always use self.shared for token embeddings to ensure compatibility with all marian models\n-        self.shared = nn.Embedding(vocab_size, config.d_model, padding_idx)\n         if self.config.share_encoder_decoder_embeddings:\n-            encoder_embed_tokens = decoder_embed_tokens = self.shared\n+            self.shared = nn.Embedding(vocab_size, config.d_model, padding_idx)\n+            self._tied_weights_keys = {\n+                \"decoder.embed_tokens.weight\": \"shared.weight\",\n+                \"encoder.embed_tokens.weight\": \"shared.weight\",\n+            }\n         else:\n-            # Since the embeddings are not shared, deepcopy the embeddings here for encoder\n-            # and decoder to make sure they are not tied.\n-            encoder_embed_tokens = copy.deepcopy(self.shared)\n-            decoder_embed_tokens = copy.deepcopy(self.shared)\n-            self.shared = None\n+            self._tied_weights_keys = None\n \n-        self.encoder = MarianEncoder(config, encoder_embed_tokens)\n-        self.decoder = MarianDecoder(config, decoder_embed_tokens)\n+        self.encoder = MarianEncoder(config)\n+        self.decoder = MarianDecoder(config)\n \n         # Initialize weights and apply final processing\n         self.post_init()\n@@ -983,9 +979,9 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        use_cache = use_cache if use_cache is not None else self.config.use_cache\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n+        # If encoder_outputs are not given, pass the inputs to the encoder\n         if encoder_outputs is None:\n             encoder_outputs = self.encoder(\n                 input_ids=input_ids,\n@@ -1042,15 +1038,21 @@ class MarianMTModel(MarianPreTrainedModel, GenerationMixin):\n     base_model_prefix = \"model\"\n     _keys_to_ignore_on_load_missing = [\n         \"final_logits_bias\",\n-        \"encoder.embed_positions.weight\",\n-        \"decoder.embed_positions.weight\",\n+        \"model.encoder.embed_positions.weight\",\n+        \"model.decoder.embed_positions.weight\",\n     ]\n     _keys_to_ignore_on_save = [\"model.encoder.embed_positions.weight\", \"model.decoder.embed_positions.weight\"]\n-    _tied_weights_keys = [\"model.encoder.embed_tokens.weight\", \"model.decoder.embed_tokens.weight\", \"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.decoder.embed_tokens.weight\"}\n \n     def __init__(self, config: MarianConfig):\n         super().__init__(config)\n         self.model = MarianModel(config)\n+        if self.config.share_encoder_decoder_embeddings:\n+            self._tied_weights_keys = {\n+                \"lm_head.weight\": \"model.shared.weight\",\n+                \"model.decoder.embed_tokens.weight\": \"model.shared.weight\",\n+                \"model.encoder.embed_tokens.weight\": \"model.shared.weight\",\n+            }\n \n         target_vocab_size = config.vocab_size if config.share_encoder_decoder_embeddings else config.decoder_vocab_size\n         self.register_buffer(\"final_logits_bias\", torch.zeros((1, target_vocab_size)))\n@@ -1140,31 +1142,6 @@ def _resize_final_logits_bias(self, new_num_tokens: int) -> None:\n     def set_output_embeddings(self, new_embeddings: nn.Embedding):\n         self.lm_head = new_embeddings\n \n-    def tie_weights(self):\n-        \"\"\"\n-        Tie the weights between the input embeddings and the output embeddings.\n-        \"\"\"\n-        output_embeddings = self.get_output_embeddings()\n-        if output_embeddings is not None and getattr(self.config, \"tie_word_embeddings\", True):\n-            # if embeddings are shared this will return shared embeddings otherwise decoder embed_tokens\n-            word_embeddings = self.get_decoder().get_input_embeddings()\n-            self._tie_embedding_weights(output_embeddings, word_embeddings)\n-\n-        if getattr(self.config, \"is_encoder_decoder\", False) and getattr(self.config, \"tie_encoder_decoder\", False):\n-            if hasattr(self, self.base_model_prefix):\n-                self = getattr(self, self.base_model_prefix)\n-            tied_weights = self._tie_encoder_decoder_weights(\n-                self.encoder, self.decoder, self.base_model_prefix, \"encoder\"\n-            )\n-            # Setting a dynamic variable instead of `_tied_weights_keys` because it's a class\n-            # attributed not an instance member, therefore modifying it will modify the entire class\n-            # Leading to issues on subsequent calls by different tests or subsequent calls.\n-            self._dynamic_tied_weights_keys = tied_weights\n-\n-        for module in self.modules():\n-            if hasattr(module, \"_tie_weights\"):\n-                module._tie_weights()\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -1293,7 +1270,9 @@ def forward(self, *args, **kwargs):\n \n # Copied from transformers.models.bart.modeling_bart.BartForCausalLM with Bart->Marian, facebook/bart-base->Helsinki-NLP/opus-mt-fr-en\n class MarianForCausalLM(MarianPreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\n+        \"lm_head.weight\": \"model.decoder.embed_tokens.weight\",\n+    }\n \n     def __init__(self, config):\n         config.is_decoder = True"
        },
        {
            "sha": "60be191c82850703abfa4f71c4b536a4dd3597cf",
            "filename": "src/transformers/models/markuplm/modeling_markuplm.py",
            "status": "modified",
            "additions": 9,
            "deletions": 16,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Fmodeling_markuplm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Fmodeling_markuplm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Fmodeling_markuplm.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -294,16 +294,9 @@ def __init__(self, config):\n \n         # The output weights are the same as the input embeddings, but there is\n         # an output-only bias for each token.\n-        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n-\n+        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=True)\n         self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n \n-        # Need a link between the two variables so that the bias is correctly resized with `resize_token_embeddings`\n-        self.decoder.bias = self.bias\n-\n-    def _tie_weights(self):\n-        self.decoder.bias = self.bias\n-\n     def forward(self, hidden_states):\n         hidden_states = self.transform(hidden_states)\n         hidden_states = self.decoder(hidden_states)\n@@ -517,22 +510,22 @@ class MarkupLMPreTrainedModel(PreTrainedModel):\n     config: MarkupLMConfig\n     base_model_prefix = \"markuplm\"\n \n-    # Copied from transformers.models.bert.modeling_bert.BertPreTrainedModel._init_weights with Bert->MarkupLM\n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n         elif isinstance(module, MarkupLMLMPredictionHead):\n-            module.bias.data.zero_()\n+            module.bias.zero_()\n \n     @classmethod\n     def from_pretrained(cls, pretrained_model_name_or_path: Optional[Union[str, os.PathLike]], *model_args, **kwargs):"
        },
        {
            "sha": "24b1d1078b822ca0b97ebb49f154051a3e484e08",
            "filename": "src/transformers/models/mask2former/modeling_mask2former.py",
            "status": "modified",
            "additions": 17,
            "deletions": 16,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -2102,6 +2102,7 @@ class Mask2FormerPreTrainedModel(PreTrainedModel):\n     main_input_name = \"pixel_values\"\n     input_modalities = \"image\"\n \n+    @torch.no_grad()\n     def _init_weights(self, module: nn.Module):\n         xavier_std = self.config.init_xavier_std\n         std = self.config.init_std\n@@ -2114,7 +2115,7 @@ def _init_weights(self, module: nn.Module):\n                         nn.init.constant_(input_projection.bias, 0)\n \n         elif isinstance(module, Mask2FormerPixelDecoderEncoderMultiscaleDeformableAttention):\n-            nn.init.constant_(module.sampling_offsets.weight.data, 0.0)\n+            nn.init.constant_(module.sampling_offsets.weight, 0.0)\n             thetas = torch.arange(module.n_heads, dtype=torch.int64).float() * (2.0 * math.pi / module.n_heads)\n             grid_init = torch.stack([thetas.cos(), thetas.sin()], -1)\n             grid_init = (\n@@ -2127,39 +2128,39 @@ def _init_weights(self, module: nn.Module):\n             with torch.no_grad():\n                 module.sampling_offsets.bias = nn.Parameter(grid_init.view(-1))\n \n-            nn.init.constant_(module.attention_weights.weight.data, 0.0)\n-            nn.init.constant_(module.attention_weights.bias.data, 0.0)\n-            nn.init.xavier_uniform_(module.value_proj.weight.data)\n-            nn.init.constant_(module.value_proj.bias.data, 0.0)\n-            nn.init.xavier_uniform_(module.output_proj.weight.data)\n-            nn.init.constant_(module.output_proj.bias.data, 0.0)\n+            nn.init.constant_(module.attention_weights.weight, 0.0)\n+            nn.init.constant_(module.attention_weights.bias, 0.0)\n+            nn.init.xavier_uniform_(module.value_proj.weight)\n+            nn.init.constant_(module.value_proj.bias, 0.0)\n+            nn.init.xavier_uniform_(module.output_proj.weight)\n+            nn.init.constant_(module.output_proj.bias, 0.0)\n \n         elif isinstance(module, Mask2FormerMaskedAttentionDecoderLayer):\n             for p in module.parameters():\n                 if p.dim() > 1:\n                     nn.init.xavier_uniform_(p, gain=xavier_std)\n-            module.cross_attn.in_proj_bias.data.zero_()\n+            module.cross_attn.in_proj_bias.zero_()\n \n         elif isinstance(module, Mask2FormerPixelDecoder):\n             nn.init.normal_(module.level_embed, std=0)\n \n         elif isinstance(module, (nn.Linear, nn.Conv2d, nn.BatchNorm2d)):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n \n         elif isinstance(module, (nn.LayerNorm, nn.GroupNorm)):\n-            module.weight.data.fill_(1.0)\n-            module.bias.data.zero_()\n+            module.weight.fill_(1.0)\n+            module.bias.zero_()\n \n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n \n         if hasattr(module, \"reference_points\"):\n-            nn.init.xavier_uniform_(module.reference_points.weight.data, gain=1.0)\n-            nn.init.constant_(module.reference_points.bias.data, 0.0)\n+            nn.init.xavier_uniform_(module.reference_points.weight, gain=1.0)\n+            nn.init.constant_(module.reference_points.bias, 0.0)\n \n \n @auto_docstring"
        },
        {
            "sha": "b2dc868f0138852768df373c34d3eb8cf9bf2f76",
            "filename": "src/transformers/models/maskformer/modeling_maskformer.py",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -1436,6 +1436,7 @@ class MaskFormerPreTrainedModel(PreTrainedModel):\n     main_input_name = \"pixel_values\"\n     input_modalities = \"image\"\n \n+    @torch.no_grad()\n     def _init_weights(self, module: nn.Module):\n         xavier_std = self.config.init_xavier_std\n         std = self.config.init_std\n@@ -1461,17 +1462,17 @@ def _init_weights(self, module: nn.Module):\n                     nn.init.xavier_uniform_(submodule.weight, gain=xavier_std)\n                     nn.init.constant_(submodule.bias, 0)\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n         # copied from DETR\n         if isinstance(module, (nn.Linear, nn.Conv2d, nn.BatchNorm2d)):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n \n \n @auto_docstring"
        },
        {
            "sha": "b735b419c10d47c921773cf9ea5202c2aa435116",
            "filename": "src/transformers/models/maskformer/modeling_maskformer_swin.py",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer_swin.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -701,20 +701,21 @@ class MaskFormerSwinPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"MaskFormerSwinStage\"]\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n         elif isinstance(module, MaskFormerSwinEmbeddings):\n             if module.position_embeddings is not None:\n-                module.position_embeddings.data.zero_()\n+                module.position_embeddings.zero_()\n         elif isinstance(module, MaskFormerSwinSelfAttention):\n-            module.relative_position_bias_table.data.zero_()\n+            module.relative_position_bias_table.zero_()\n \n \n class MaskFormerSwinModel(MaskFormerSwinPreTrainedModel):"
        },
        {
            "sha": "08cde27d7cceb43d133ac34104b51acf22f86a7c",
            "filename": "src/transformers/models/mbart/modeling_mbart.py",
            "status": "modified",
            "additions": 19,
            "deletions": 28,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -479,19 +479,20 @@ class MBartPreTrainedModel(PreTrainedModel):\n \n     _can_compile_fullgraph = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         std = self.config.init_std\n         if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+                module.weight[module.padding_idx].zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.weight.data.fill_(1.0)\n-            module.bias.data.zero_()\n+            module.weight.fill_(1.0)\n+            module.bias.zero_()\n \n     @property\n     def dummy_inputs(self):\n@@ -514,7 +515,7 @@ class MBartEncoder(MBartPreTrainedModel):\n         embed_tokens (nn.Embedding): output embedding\n     \"\"\"\n \n-    def __init__(self, config: MBartConfig, embed_tokens: Optional[nn.Embedding] = None):\n+    def __init__(self, config: MBartConfig):\n         super().__init__(config)\n \n         self.dropout = config.dropout\n@@ -529,9 +530,6 @@ def __init__(self, config: MBartConfig, embed_tokens: Optional[nn.Embedding] = N\n             config.vocab_size, embed_dim, self.padding_idx, embed_scale=embed_scale\n         )\n \n-        if embed_tokens is not None:\n-            self.embed_tokens.weight = embed_tokens.weight\n-\n         self.embed_positions = MBartLearnedPositionalEmbedding(\n             config.max_position_embeddings,\n             embed_dim,\n@@ -670,7 +668,7 @@ class MBartDecoder(MBartPreTrainedModel):\n         embed_tokens (nn.Embedding): output embedding\n     \"\"\"\n \n-    def __init__(self, config: MBartConfig, embed_tokens: Optional[nn.Embedding] = None):\n+    def __init__(self, config: MBartConfig):\n         super().__init__(config)\n         self.dropout = config.dropout\n         self.layerdrop = config.decoder_layerdrop\n@@ -682,9 +680,6 @@ def __init__(self, config: MBartConfig, embed_tokens: Optional[nn.Embedding] = N\n             config.vocab_size, config.d_model, self.padding_idx, embed_scale=embed_scale\n         )\n \n-        if embed_tokens is not None:\n-            self.embed_tokens.weight = embed_tokens.weight\n-\n         self.embed_positions = MBartLearnedPositionalEmbedding(\n             config.max_position_embeddings,\n             config.d_model,\n@@ -898,7 +893,10 @@ def forward(\n \n @auto_docstring\n class MBartModel(MBartPreTrainedModel):\n-    _tied_weights_keys = [\"encoder.embed_tokens.weight\", \"decoder.embed_tokens.weight\"]\n+    _tied_weights_keys = {\n+        \"decoder.embed_tokens.weight\": \"shared.weight\",\n+        \"encoder.embed_tokens.weight\": \"shared.weight\",\n+    }\n \n     def __init__(self, config: MBartConfig):\n         super().__init__(config)\n@@ -907,8 +905,8 @@ def __init__(self, config: MBartConfig):\n         embed_scale = math.sqrt(config.d_model) if config.scale_embedding else 1.0\n         self.shared = MBartScaledWordEmbedding(vocab_size, config.d_model, padding_idx, embed_scale=embed_scale)\n \n-        self.encoder = MBartEncoder(config, self.shared)\n-        self.decoder = MBartDecoder(config, self.shared)\n+        self.encoder = MBartEncoder(config)\n+        self.decoder = MBartDecoder(config)\n \n         # Initialize weights and apply final processing\n         self.post_init()\n@@ -924,11 +922,6 @@ def set_input_embeddings(self, value):\n     def get_encoder(self):\n         return self.encoder\n \n-    def _tie_weights(self):\n-        if self.config.tie_word_embeddings:\n-            self._tie_embedding_weights(self.encoder.embed_tokens, self.get_input_embeddings())\n-            self._tie_embedding_weights(self.decoder.embed_tokens, self.get_input_embeddings())\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -1034,7 +1027,7 @@ def forward(\n class MBartForConditionalGeneration(MBartPreTrainedModel, GenerationMixin):\n     base_model_prefix = \"model\"\n     _keys_to_ignore_on_load_missing = [\"final_logits_bias\"]\n-    _tied_weights_keys = [\"model.encoder.embed_tokens.weight\", \"model.decoder.embed_tokens.weight\", \"lm_head.weight\"]\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.shared.weight\"}\n \n     def __init__(self, config: MBartConfig):\n         super().__init__(config)\n@@ -1207,8 +1200,6 @@ def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n     \"\"\"\n )\n class MBartForSequenceClassification(MBartPreTrainedModel):\n-    _tied_weights_keys = [\"model.encoder.embed_tokens.weight\", \"model.decoder.embed_tokens.weight\"]\n-\n     def __init__(self, config: MBartConfig, **kwargs):\n         super().__init__(config, **kwargs)\n         self.model = MBartModel(config)\n@@ -1342,8 +1333,6 @@ def forward(\n \n @auto_docstring\n class MBartForQuestionAnswering(MBartPreTrainedModel):\n-    _tied_weights_keys = [\"model.encoder.embed_tokens.weight\", \"model.decoder.embed_tokens.weight\"]\n-\n     def __init__(self, config):\n         super().__init__(config)\n \n@@ -1479,7 +1468,9 @@ def forward(self, *args, **kwargs):\n \n # Copied from transformers.models.bart.modeling_bart.BartForCausalLM with Bart->MBart, facebook/bart-base->facebook/mbart-large-cc25\n class MBartForCausalLM(MBartPreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tied_weights_keys = {\n+        \"lm_head.weight\": \"model.decoder.embed_tokens.weight\",\n+    }\n \n     def __init__(self, config):\n         config.is_decoder = True"
        },
        {
            "sha": "d7a869cfd89a9aa27d9cca66ba624aea513a888a",
            "filename": "src/transformers/models/megatron_bert/modeling_megatron_bert.py",
            "status": "modified",
            "additions": 19,
            "deletions": 16,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -471,16 +471,9 @@ def __init__(self, config):\n \n         # The output weights are the same as the input embeddings, but there is\n         # an output-only bias for each token.\n-        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n-\n+        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=True)\n         self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n \n-        # Need a link between the two variables so that the bias is correctly resized with `resize_token_embeddings`\n-        self.decoder.bias = self.bias\n-\n-    def _tie_weights(self):\n-        self.decoder.bias = self.bias\n-\n     def forward(self, hidden_states):\n         hidden_states = self.transform(hidden_states)\n         hidden_states = self.decoder(hidden_states)\n@@ -528,17 +521,18 @@ class MegatronBertPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"bert\"\n     supports_gradient_checkpointing = True\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, (nn.Linear, nn.Embedding)):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n             if hasattr(module, \"bias\") and module.bias is not None:\n-                module.bias.data.zero_()\n+                module.bias.zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n         elif isinstance(module, MegatronBertLMPredictionHead):\n-            module.bias.data.zero_()\n+            module.bias.zero_()\n \n \n @dataclass\n@@ -708,7 +702,10 @@ def forward(\n     \"\"\"\n )\n class MegatronBertForPreTraining(MegatronBertPreTrainedModel):\n-    _tied_weights_keys = [\"cls.predictions.decoder\"]\n+    _tied_weights_keys = {\n+        \"cls.predictions.decoder.weight\": \"bert.embeddings.word_embeddings.weight\",\n+        \"cls.predictions.decoder.bias\": \"cls.predictions.bias\",\n+    }\n \n     def __init__(self, config, add_binary_head=True):\n         r\"\"\"\n@@ -813,7 +810,10 @@ def forward(\n     \"\"\"\n )\n class MegatronBertForCausalLM(MegatronBertPreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"cls.predictions.decoder\"]\n+    _tied_weights_keys = {\n+        \"cls.predictions.decoder.weight\": \"bert.embeddings.word_embeddings.weight\",\n+        \"cls.predictions.decoder.bias\": \"cls.predictions.bias\",\n+    }\n \n     def __init__(self, config):\n         super().__init__(config)\n@@ -919,7 +919,10 @@ def forward(\n \n @auto_docstring\n class MegatronBertForMaskedLM(MegatronBertPreTrainedModel):\n-    _tied_weights_keys = [\"cls.predictions.decoder\"]\n+    _tied_weights_keys = {\n+        \"cls.predictions.decoder.weight\": \"bert.embeddings.word_embeddings.weight\",\n+        \"cls.predictions.decoder.bias\": \"cls.predictions.bias\",\n+    }\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "c66bababfbe57de7dceb70d8c69cc4be957f2ece",
            "filename": "src/transformers/models/metaclip_2/modeling_metaclip_2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodeling_metaclip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodeling_metaclip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodeling_metaclip_2.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -298,12 +298,13 @@ class MetaClip2PreTrainedModel(PreTrainedModel):\n         \"attentions\": MetaClip2Attention,\n     }\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         factor = self.config.initializer_factor\n         if isinstance(module, MetaClip2TextEmbeddings):\n-            module.token_embedding.weight.data.normal_(mean=0.0, std=factor * 0.02)\n-            module.position_embedding.weight.data.normal_(mean=0.0, std=factor * 0.02)\n+            module.token_embedding.weight.normal_(mean=0.0, std=factor * 0.02)\n+            module.position_embedding.weight.normal_(mean=0.0, std=factor * 0.02)\n         elif isinstance(module, MetaClip2VisionEmbeddings):\n             factor = self.config.initializer_factor\n             nn.init.normal_(module.class_embedding, mean=0.0, std=module.embed_dim**-0.5 * factor)\n@@ -349,10 +350,10 @@ def _init_weights(self, module):\n             )\n \n         if isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n         if isinstance(module, nn.Linear) and module.bias is not None:\n-            module.bias.data.zero_()\n+            module.bias.zero_()\n \n \n class MetaClip2Encoder(nn.Module):"
        },
        {
            "sha": "79cdf35be7e99305845d3155096e52fce0f70ed2",
            "filename": "src/transformers/models/metaclip_2/modular_metaclip_2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodular_metaclip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f6095e0cf509f7384d3ce0c1804013ef6cafd5f/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodular_metaclip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodular_metaclip_2.py?ref=6f6095e0cf509f7384d3ce0c1804013ef6cafd5f",
            "patch": "@@ -217,12 +217,13 @@ class MetaClip2MLP(CLIPMLP):\n class MetaClip2PreTrainedModel(CLIPPreTrainedModel):\n     base_model_prefix = \"metaclip_2\"\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         factor = self.config.initializer_factor\n         if isinstance(module, MetaClip2TextEmbeddings):\n-            module.token_embedding.weight.data.normal_(mean=0.0, std=factor * 0.02)\n-            module.position_embedding.weight.data.normal_(mean=0.0, std=factor * 0.02)\n+            module.token_embedding.weight.normal_(mean=0.0, std=factor * 0.02)\n+            module.position_embedding.weight.normal_(mean=0.0, std=factor * 0.02)\n         elif isinstance(module, MetaClip2VisionEmbeddings):\n             factor = self.config.initializer_factor\n             nn.init.normal_(module.class_embedding, mean=0.0, std=module.embed_dim**-0.5 * factor)\n@@ -268,10 +269,10 @@ def _init_weights(self, module):\n             )\n \n         if isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+            module.bias.zero_()\n+            module.weight.fill_(1.0)\n         if isinstance(module, nn.Linear) and module.bias is not None:\n-            module.bias.data.zero_()\n+            module.bias.zero_()\n \n \n class MetaClip2TextTransformer(CLIPTextTransformer):"
        }
    ],
    "stats": {
        "total": 15922,
        "additions": 8733,
        "deletions": 7189
    }
}