{
    "author": "vasqu",
    "message": "[`PEFT`] Fix the general test for prefix tuning (#42185)\n\nfix",
    "sha": "5c6d6bed4d2ed5ee3380c5003f61c500049b9dc5",
    "files": [
        {
            "sha": "3f64ca07ae66076d0830546f3e9df9b98b03b4cf",
            "filename": "tests/utils/test_modeling_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c6d6bed4d2ed5ee3380c5003f61c500049b9dc5/tests%2Futils%2Ftest_modeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c6d6bed4d2ed5ee3380c5003f61c500049b9dc5/tests%2Futils%2Ftest_modeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_utils.py?ref=5c6d6bed4d2ed5ee3380c5003f61c500049b9dc5",
            "patch": "@@ -1818,6 +1818,8 @@ def test_cache_when_needed_at_train_time(self):\n         self.assertTrue(model.training)\n \n         # We can also disable the cache to skip a few operations, if the training loop doesn't need cache\n+        # NOTE: after #41900, we need to pass the correct attention mask size\n+        model_inputs[\"attention_mask\"] = model_inputs[\"attention_mask\"][:, :-num_virtual_tokens]\n         model_outputs = model(**model_inputs, use_cache=False)\n         self.assertIsNone(model_outputs.past_key_values)\n         self.assertTrue(model.training)"
        }
    ],
    "stats": {
        "total": 2,
        "additions": 2,
        "deletions": 0
    }
}