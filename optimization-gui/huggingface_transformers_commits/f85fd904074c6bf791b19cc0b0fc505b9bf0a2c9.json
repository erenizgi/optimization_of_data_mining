{
    "author": "gante",
    "message": "[cleanup] delete deprecated kwargs in qwen2_audio ðŸ§¹  (#38404)\n\ndelete deprecated",
    "sha": "f85fd904074c6bf791b19cc0b0fc505b9bf0a2c9",
    "files": [
        {
            "sha": "9ac1fd008a99ac832bf9fc4a2bbb8a57acd94fc7",
            "filename": "src/transformers/models/qwen2_audio/modeling_qwen2_audio.py",
            "status": "modified",
            "additions": 0,
            "deletions": 19,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/f85fd904074c6bf791b19cc0b0fc505b9bf0a2c9/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f85fd904074c6bf791b19cc0b0fc505b9bf0a2c9/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py?ref=f85fd904074c6bf791b19cc0b0fc505b9bf0a2c9",
            "patch": "@@ -29,7 +29,6 @@\n from ...modeling_outputs import BaseModelOutput, ModelOutput\n from ...modeling_utils import PreTrainedModel\n from ...utils import auto_docstring, logging\n-from ...utils.deprecation import deprecate_kwarg\n from ..auto import AutoModel, AutoModelForCausalLM\n from .configuration_qwen2_audio import Qwen2AudioConfig, Qwen2AudioEncoderConfig\n \n@@ -130,18 +129,12 @@ def __init__(\n     def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n         return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n \n-    @deprecate_kwarg(\"key_value_states\", version=\"4.52\")\n-    @deprecate_kwarg(\"past_key_value\", version=\"4.52\")\n-    @deprecate_kwarg(\"cache_position\", version=\"4.52\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n@@ -203,18 +196,12 @@ def __init__(self, *args, **kwargs):\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n         self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n \n-    @deprecate_kwarg(\"key_value_states\", version=\"4.52\")\n-    @deprecate_kwarg(\"past_key_value\", version=\"4.52\")\n-    @deprecate_kwarg(\"cache_position\", version=\"4.52\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         # Qwen2AudioFlashAttention2 attention does not support output_attentions\n         if output_attentions:\n@@ -283,18 +270,12 @@ def forward(\n \n \n class Qwen2AudioSdpaAttention(Qwen2AudioAttention):\n-    @deprecate_kwarg(\"key_value_states\", version=\"4.52\")\n-    @deprecate_kwarg(\"past_key_value\", version=\"4.52\")\n-    @deprecate_kwarg(\"cache_position\", version=\"4.52\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n         if output_attentions:"
        }
    ],
    "stats": {
        "total": 19,
        "additions": 0,
        "deletions": 19
    }
}