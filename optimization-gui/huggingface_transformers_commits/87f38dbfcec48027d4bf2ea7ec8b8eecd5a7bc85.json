{
    "author": "RyanMullins",
    "message": "add: embedding model (#40694)\n\n* Gemma 3 for Embeddings\n\n* Style fixes\n\n* Rename conversion file for consistency\n\n* Default padding side emb vs gen\n\n* Corrected 270m config\n\n* style fixes\n\n* EmbeddingGemma config\n\n* TODO for built-in prompts\n\n* Resolving the sentence similarity bug and updating the architecture\n\n* code style\n\n* Add query prompt for SentenceTransformers\n\n* Code quality\n\n* Fixing or_mask_function return types\n\n* Adding placeholder prompts for document and passage\n\n* Finalizing prompt templates\n\n* Adding Retrieval ro preconfigured prompts\n\n* Add Gemma 3 270M Config\n\n* Correcting num_linear_layers flag default\n\n* Export Sentence Transformer in correct dtype\n\n---------\n\nCo-authored-by: Sindhu Raghuram <sindhuraghuram@google.com>",
    "sha": "87f38dbfcec48027d4bf2ea7ec8b8eecd5a7bc85",
    "files": [
        {
            "sha": "b1ec3311ba662c264c17fb146a917b0d391bc31f",
            "filename": "src/transformers/models/gemma3/configuration_gemma3.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/87f38dbfcec48027d4bf2ea7ec8b8eecd5a7bc85/src%2Ftransformers%2Fmodels%2Fgemma3%2Fconfiguration_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/87f38dbfcec48027d4bf2ea7ec8b8eecd5a7bc85/src%2Ftransformers%2Fmodels%2Fgemma3%2Fconfiguration_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fconfiguration_gemma3.py?ref=87f38dbfcec48027d4bf2ea7ec8b8eecd5a7bc85",
            "patch": "@@ -136,6 +136,8 @@ class Gemma3TextConfig(PretrainedConfig):\n                     Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n         rope_local_base_freq (float, *optional*, defaults to 10000.0):\n             The base period of the RoPE embeddings for local attention.\n+        use_bidirectional_attention (`bool`, *optional*, defaults to `False`): If True, the model will attend to all\n+            text tokens instead of using a causal mask. This does not change behavior for vision tokens.\n \n     ```python\n     >>> from transformers import Gemma3TextModel, Gemma3TextConfig\n@@ -193,6 +195,7 @@ def __init__(\n         attn_logit_softcapping=None,\n         rope_scaling=None,\n         rope_local_base_freq=10_000.0,\n+        use_bidirectional_attention=False,\n         **kwargs,\n     ):\n         super().__init__(\n@@ -222,6 +225,7 @@ def __init__(\n         self.final_logit_softcapping = final_logit_softcapping\n         self.attn_logit_softcapping = attn_logit_softcapping\n         self.layer_types = layer_types\n+        self.use_bidirectional_attention = use_bidirectional_attention\n \n         self.rope_local_base_freq = rope_local_base_freq\n         self.rope_scaling = rope_scaling"
        },
        {
            "sha": "8d7a212191970a87bdb791d1e9ba906f652f01ab",
            "filename": "src/transformers/models/gemma3/convert_gemma3_weights.py",
            "status": "renamed",
            "additions": 121,
            "deletions": 26,
            "changes": 147,
            "blob_url": "https://github.com/huggingface/transformers/blob/87f38dbfcec48027d4bf2ea7ec8b8eecd5a7bc85/src%2Ftransformers%2Fmodels%2Fgemma3%2Fconvert_gemma3_weights.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/87f38dbfcec48027d4bf2ea7ec8b8eecd5a7bc85/src%2Ftransformers%2Fmodels%2Fgemma3%2Fconvert_gemma3_weights.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fconvert_gemma3_weights.py?ref=87f38dbfcec48027d4bf2ea7ec8b8eecd5a7bc85",
            "patch": "@@ -16,15 +16,15 @@\n \n r\"\"\"Utility to convert Gemma models from Orbax to HF Transformers checkpoint.\n \n-python -m transformers.models.gemma3.convert_gemma3_weights_orbax_to_hf \\\n+python src/transformers/models/gemma3/convert_gemma3_weights.py \\\n     --variant='gemma3_4b' \\\n     --tokenizer_path=\"$HOME/gemma3/tokenizer/gemma3_cleaned_262144_v2.spiece.model\" \\\n     --checkpoint_path=\"$HOME/gemma3/gemma3_4b_pt_orbax/\" \\\n     --output_path=\"$HOME/gemma3/gemma3_4b_pt_safetensors/\"\n \"\"\"\n \n from collections.abc import Iterator, Sequence\n-from typing import Any\n+from typing import Any, Optional\n \n import accelerate\n import numpy as np\n@@ -40,6 +40,7 @@\n     Gemma3ImageProcessor,\n     Gemma3Processor,\n     Gemma3TextConfig,\n+    Gemma3TextModel,\n     GemmaTokenizerFast,\n     GenerationConfig,\n     SiglipVisionConfig,\n@@ -100,10 +101,10 @@\n _SIGLIP_TRANSFORMER_ENCODER_BLOCK_LEN = len(_SIGLIP_TRANSFORMER_ENCODER_BLOCK)\n _SIGLIP_TRANSFORMER_ENCODER_NORM = \"SigLiPFromPatches_0/siglip_encoder/Transformer/encoder_norm\"\n \n-_TRANSFORMER_DECODER_BLOCK = \"transformer/layer_\"\n+_TRANSFORMER_DECODER_BLOCK = \"/layer_\"\n _TRANSFORMER_DECODER_BLOCK_LEN = len(_TRANSFORMER_DECODER_BLOCK)\n-_TRANSFORMER_EMBEDDER = \"transformer/embedder\"\n-_TRANSFORMER_FINAL_NORM = \"transformer/final_norm\"\n+_TRANSFORMER_EMBEDDER = \"/embedder\"\n+_TRANSFORMER_FINAL_NORM = \"/final_norm\"\n _TRANSFORMER_POST_TRAINING_PREFIX = \"rlx_networks/policy_network/\"\n _TRANSFORMER_POST_TRAINING_PREFIX_LEN = len(_TRANSFORMER_POST_TRAINING_PREFIX)\n \n@@ -121,11 +122,46 @@\n     \"vision_use_head\": False,\n }\n \n+_VARIANT_EMBEDDINGGEMMA = \"embedding\"\n+_VARIANT_GEMMA_3_270M = \"gemma3_270m\"\n _VARIANT_GEMMA_3_1B = \"gemma3_1b\"\n _VARIANT_GEMMA_3_4B = \"gemma3_4b\"\n _VARIANT_GEMMA_3_12B = \"gemma3_12b\"\n _VARIANT_GEMMA_3_27B = \"gemma3_27b\"\n _VARIANTS = {\n+    _VARIANT_EMBEDDINGGEMMA: Gemma3Config(\n+        text_config=Gemma3TextConfig(\n+            vocab_size=262_144,\n+            hidden_size=768,\n+            intermediate_size=1152,\n+            num_hidden_layers=24,\n+            num_attention_heads=3,\n+            num_key_value_heads=1,\n+            head_dim=256,\n+            max_position_embeddings=1024,\n+            query_pre_attn_scalar=256,\n+            sliding_window=512,\n+            rope_scaling=None,\n+            use_bidirectional_attention=True,\n+        ),\n+        vision_config=None,\n+    ),\n+    _VARIANT_GEMMA_3_270M: Gemma3Config(\n+        text_config=Gemma3TextConfig(\n+            vocab_size=262_144,\n+            hidden_size=640,\n+            intermediate_size=2048,\n+            num_hidden_layers=18,\n+            num_attention_heads=4,\n+            num_key_value_heads=1,\n+            head_dim=256,\n+            max_position_embeddings=32768,\n+            query_pre_attn_scalar=256,\n+            sliding_window=512,\n+            rope_scaling=None,\n+        ),\n+        vision_config=None,\n+    ),\n     _VARIANT_GEMMA_3_1B: Gemma3Config(\n         text_config=Gemma3TextConfig(\n             vocab_size=262_144,\n@@ -200,6 +236,8 @@\n     ),\n }\n \n+_TEXT_ONLY_VARIANTS = (_VARIANT_EMBEDDINGGEMMA, _VARIANT_GEMMA_3_270M, _VARIANT_GEMMA_3_1B)\n+\n # ==== Flags ====\n \n _CHECKPOINT_PATH = flags.DEFINE_string(\n@@ -220,6 +258,12 @@\n     required=True,\n )\n \n+_NUM_LINEAR_LAYERS = flags.DEFINE_integer(\n+    name=\"num_linear_layers\",\n+    default=2,\n+    help=\"Number of linear projection layers at the end of the Sentence Transformer.\",\n+)\n+\n _TRANSFORMER_DTYPE = flags.DEFINE_enum(\n     name=\"text_dtype\",\n     default=\"bfloat16\",\n@@ -358,12 +402,12 @@ def convert_transformer_weights(\n     attn_head_dim = config.num_attention_heads * config.head_dim\n     kv_head_dim = config.num_key_value_heads * config.head_dim\n \n-    if path == _TRANSFORMER_EMBEDDER:\n+    if path.endswith(_TRANSFORMER_EMBEDDER):\n         if prop == \"input_embedding\":\n             # Tied to language_model.lm_head.weight, assigned at the end.\n             converted_paths = [\"language_model.model.embed_tokens.weight\"]\n \n-            if _VARIANT.value != _VARIANT_GEMMA_3_1B:\n+            if _VARIANT.value not in _TEXT_ONLY_VARIANTS:\n                 # Gemma3 model doesn't have image soft token in input and output embeddings, resize to avoid bugs we had with Mllama\n                 pre_expansion_embeddings = weights\n                 mu = np.mean(pre_expansion_embeddings, axis=0)\n@@ -372,12 +416,12 @@ def convert_transformer_weights(\n                 weights = np.vstack([pre_expansion_embeddings, new_embeddings])\n \n             converted_weights = [weights]\n-        elif _VARIANT.value == _VARIANT_GEMMA_3_1B or prop in (\"mm_output_embedding\", \"mm_input_embedding_extra\"):\n+        elif _VARIANT.value in _TEXT_ONLY_VARIANTS or prop in (\"mm_output_embedding\", \"mm_input_embedding_extra\"):\n             return zip([], [])\n         else:\n             raise ValueError(f\"Unexpected member, {prop}, in Embedder.\")\n     elif path.startswith(f\"{_TRANSFORMER_EMBEDDER}/mm\"):\n-        if _VARIANT.value == _VARIANT_GEMMA_3_1B:\n+        if _VARIANT.value in _TEXT_ONLY_VARIANTS:\n             return zip([], [])\n \n         if path.endswith(\"/mm_input_projection\"):\n@@ -388,14 +432,16 @@ def convert_transformer_weights(\n             converted_weights = [weights]\n         else:\n             raise ValueError(f\"Unexpected subpath, `{path}`, in Embedder.\")\n-    elif path == _TRANSFORMER_FINAL_NORM:\n+    elif path.endswith(_TRANSFORMER_FINAL_NORM):\n         converted_paths = [\"language_model.model.norm.weight\"]\n         converted_weights = [weights]\n-    elif path.startswith(_TRANSFORMER_DECODER_BLOCK):\n-        decoder_block_path = path[_TRANSFORMER_DECODER_BLOCK_LEN:]\n-        next_path_separator_idx = decoder_block_path.find(\"/\")\n-        layer_idx = decoder_block_path[:next_path_separator_idx]\n-        decoder_block_path = decoder_block_path[next_path_separator_idx:]\n+    elif _TRANSFORMER_DECODER_BLOCK in path:\n+        decoder_block_start = path.find(_TRANSFORMER_DECODER_BLOCK)\n+        decoder_block_offset = decoder_block_start + _TRANSFORMER_DECODER_BLOCK_LEN\n+        decoder_block_path = path[decoder_block_offset:]\n+        next_path_seperator_idx = decoder_block_path.find(\"/\")\n+        layer_idx = decoder_block_path[:next_path_seperator_idx]\n+        decoder_block_path = decoder_block_path[next_path_seperator_idx:]\n \n         base_path = f\"language_model.model.layers.{layer_idx}\"\n \n@@ -445,8 +491,6 @@ def convert_transformer_weights(\n             converted_weights = [weights]\n         else:\n             raise ValueError(f\"Unexpected path `{path}` in Decoder Block.\")\n-    else:\n-        raise ValueError(f\"Unexpected path `{path}`.\")\n \n     if (cpl := len(converted_paths)) != (cwl := len(converted_weights)):\n         raise ValueError(\n@@ -457,11 +501,14 @@ def convert_transformer_weights(\n     return zip(converted_paths, converted_weights)\n \n \n-def convert(checkpoint_path: str, config: Gemma3Config) -> dict[str, torch.Tensor]:\n+def convert(\n+    checkpoint_path: str, config: Gemma3Config, variant: str\n+) -> tuple[dict[str, torch.Tensor], Optional[Sequence[np.ndarray]]]:\n     \"\"\"Loads Orbax checkpoint from `input_path` and converts it to HF tree.\"\"\"\n     checkpointer = obc.PyTreeCheckpointer()\n     ckpt = checkpointer.restore(checkpoint_path)\n     hf_tree: dict[str, torch.Tensor] = {}\n+    orbax_tree_flat = tree.flatten_with_path(ckpt)\n \n     def update_tree(path: str, weights: np.ndarray, target_dtype: torch.dtype) -> None:\n         hf_tree[path] = torch.from_numpy(weights.astype(\"float32\")).type(target_dtype)\n@@ -473,7 +520,7 @@ def update_tree(path: str, weights: np.ndarray, target_dtype: torch.dtype) -> No\n                 target_dtype,\n             )\n \n-    for paths, value in tree.flatten_with_path(ckpt):\n+    for paths, value in orbax_tree_flat:\n         if paths[0].startswith(\"SigLiPFromPatches_\"):\n             if config.vision_config is None:\n                 continue\n@@ -482,17 +529,21 @@ def update_tree(path: str, weights: np.ndarray, target_dtype: torch.dtype) -> No\n             update_tree(path, weights, config.vision_config.dtype)\n         else:\n             for path, weights in convert_transformer_weights(config=config.text_config, paths=paths, weights=value):\n-                if config.vision_config is None:\n+                if variant in _TEXT_ONLY_VARIANTS:\n                     path = path[len(\"language_model.\") :]\n+                if variant == _VARIANT_EMBEDDINGGEMMA:\n+                    path = path[len(\"model.\") :]\n \n                 update_tree(path, weights, config.text_config.dtype)\n \n-    if config.vision_config is None:\n+    if variant == _VARIANT_EMBEDDINGGEMMA:\n+        return hf_tree, [weight[1].T for weight in orbax_tree_flat[: _NUM_LINEAR_LAYERS.value]]\n+    elif config.vision_config is None:\n         hf_tree[\"lm_head.weight\"] = hf_tree[\"model.embed_tokens.weight\"]\n     else:\n         hf_tree[\"language_model.lm_head.weight\"] = hf_tree[\"language_model.model.embed_tokens.weight\"]\n \n-    return hf_tree\n+    return hf_tree, None\n \n \n def main(*args):\n@@ -504,7 +555,7 @@ def main(*args):\n     config = _VARIANTS[variant]\n     config.text_config.dtype = getattr(torch, _TRANSFORMER_DTYPE.value)\n \n-    if variant == _VARIANT_GEMMA_3_1B:\n+    if variant in _TEXT_ONLY_VARIANTS:\n         config.vision_config = None\n     else:\n         config.vision_config.dtype = getattr(torch, _VISION_DTYPE.value)\n@@ -520,11 +571,13 @@ def main(*args):\n         _TRANSFORMER_DTYPE.value,\n         _VISION_DTYPE.value,\n     )\n-    state_tree = convert(_CHECKPOINT_PATH.value, config)\n+    state_tree, st_linears = convert(_CHECKPOINT_PATH.value, config, variant)\n     logging.info(\"Converted Gemma 3 (%s) state tree from Orbax to Hugging Face.\", variant)\n \n     with accelerate.init_empty_weights():\n-        if variant == _VARIANT_GEMMA_3_1B:\n+        if variant == _VARIANT_EMBEDDINGGEMMA:\n+            model = Gemma3TextModel(config=config.text_config)\n+        elif variant in _TEXT_ONLY_VARIANTS:\n             model = Gemma3ForCausalLM(config=config.text_config)\n         else:\n             model = Gemma3ForConditionalGeneration(config)\n@@ -548,6 +601,8 @@ def main(*args):\n     tokenizer = GemmaTokenizerFast(\n         _TOKENIZER_PATH.value,\n         add_bos_token=True,\n+        add_eos_token=variant == _VARIANT_EMBEDDINGGEMMA,\n+        padding_side=\"right\" if variant == _VARIANT_EMBEDDINGGEMMA else \"left\",\n         extra_special_tokens={\n             \"image_token\": \"<image_soft_token>\",  # Should be ID=262_144\n             \"boi_token\": \"<start_of_image>\",  # Should be ID=255_999\n@@ -558,7 +613,7 @@ def main(*args):\n     tokenizer.save_pretrained(output_path)\n     logging.info(\"Saved GemmaTokenizer for %s to %s\", variant, output_path)\n \n-    if variant != _VARIANT_GEMMA_3_1B:\n+    if variant not in _TEXT_ONLY_VARIANTS:\n         image_processor = Gemma3ImageProcessor(\n             image_seq_length=256,\n             image_mean=(0.5,) * 3,\n@@ -589,6 +644,46 @@ def main(*args):\n     )\n     generation_config.save_pretrained(output_path)\n \n+    if variant == _VARIANT_EMBEDDINGGEMMA:\n+        from sentence_transformers import SentenceTransformer, models\n+\n+        # TODO: Support Retrieval tasks where we use `\"title: {title} | text: {passage}\"` interally and construct this\n+        # from split-records cached data, but externally these come through as a single string with components\n+        # separated by a newline. This should be used for `passage` for SentenceTransformers and the relevant MTEB\n+        # Retrieval tasks.\n+        # https://github.com/embeddings-benchmark/mteb/blob/main/docs/usage/usage.md#running-sentencetransformer-model-with-prompts\n+        task_prompts = {\n+            \"query\": \"task: search result | query: \",\n+            \"document\": \"title: none | text: \",\n+            \"BitextMining\": \"task: search result | query: \",\n+            \"Clustering\": \"task: clustering | query: \",\n+            \"Classification\": \"task: classification | query: \",\n+            \"InstructionRetrieval\": \"task: code retrieval | query: \",\n+            \"MultilabelClassification\": \"task: classification | query: \",\n+            \"PairClassification\": \"task: sentence similarity | query: \",\n+            \"Reranking\": \"task: search result | query: \",\n+            \"Retrieval\": \"task: search result | query: \",\n+            \"Retrieval-query\": \"task: search result | query: \",\n+            \"Retrieval-document\": \"title: none | text: \",\n+            \"STS\": \"task: sentence similarity | query: \",\n+            \"Summarization\": \"task: summarization | query: \",\n+        }\n+\n+        transformer = models.Transformer(output_path)\n+        pooling = models.Pooling(config.text_config.hidden_size, pooling_mode=\"mean\")\n+        normalize = models.Normalize()\n+        linears = []\n+\n+        for linear_weight in st_linears:\n+            out_size, in_size = linear_weight.shape[:2]\n+            dense = models.Dense(in_size, out_size, bias=False, activation_function=None)\n+            dense.linear.weight.data = torch.from_numpy(linear_weight.astype(\"float32\"))\n+            linears.append(dense)\n+\n+        model = SentenceTransformer(modules=[transformer, pooling, *linears, normalize], prompts=task_prompts)\n+        model = model.to(getattr(torch, _TRANSFORMER_DTYPE.value))\n+        model.save_pretrained(output_path)\n+\n \n if __name__ == \"__main__\":\n     app.run(main)",
            "previous_filename": "src/transformers/models/gemma3/convert_gemma3_weights_orbax_to_hf.py"
        },
        {
            "sha": "d2ba04298dec22ccde4658dbc081e15ef874a6bc",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 20,
            "deletions": 1,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/87f38dbfcec48027d4bf2ea7ec8b8eecd5a7bc85/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/87f38dbfcec48027d4bf2ea7ec8b8eecd5a7bc85/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=87f38dbfcec48027d4bf2ea7ec8b8eecd5a7bc85",
            "patch": "@@ -443,6 +443,19 @@ def _init_weights(self, module):\n             module.mm_input_projection_weight.data.zero_()\n \n \n+def _bidirectional_window_overlay(sliding_window: int) -> Callable[[int, int, int, int], bool]:\n+    \"\"\"\n+    Enables a bidirectional mask within the sliding window.\n+    \"\"\"\n+\n+    def inner_mask(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool:\n+        \"\"\"A token can attend to any other token if their absolute distance is within\n+        half the sliding window size (distance <= sliding_window // 2).\"\"\"\n+        return abs(q_idx - kv_idx) <= sliding_window // 2\n+\n+    return inner_mask\n+\n+\n @auto_docstring\n class Gemma3TextModel(Gemma3PreTrainedModel):\n     config: Gemma3TextConfig\n@@ -531,10 +544,16 @@ def forward(\n                 \"past_key_values\": past_key_values,\n                 \"position_ids\": position_ids,\n             }\n+            sliding_mask_kwargs = mask_kwargs.copy()\n+\n+            if self.config.use_bidirectional_attention:\n+                mask_kwargs[\"or_mask_function\"] = lambda *args: torch.tensor(True, dtype=torch.bool)\n+                sliding_mask_kwargs[\"or_mask_function\"] = _bidirectional_window_overlay(self.config.sliding_window)\n+\n             # Create the masks\n             causal_mask_mapping = {\n                 \"full_attention\": create_causal_mask(**mask_kwargs),\n-                \"sliding_attention\": create_sliding_window_causal_mask(**mask_kwargs),\n+                \"sliding_attention\": create_sliding_window_causal_mask(**sliding_mask_kwargs),\n             }\n \n         # embed positions"
        },
        {
            "sha": "fc70fa6e9d8e04d8cdf1eb89bf4169d9b1d4ee5f",
            "filename": "src/transformers/models/gemma3/modular_gemma3.py",
            "status": "modified",
            "additions": 24,
            "deletions": 1,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/87f38dbfcec48027d4bf2ea7ec8b8eecd5a7bc85/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/87f38dbfcec48027d4bf2ea7ec8b8eecd5a7bc85/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py?ref=87f38dbfcec48027d4bf2ea7ec8b8eecd5a7bc85",
            "patch": "@@ -162,6 +162,8 @@ class Gemma3TextConfig(Gemma2Config, PretrainedConfig):\n                     Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n         rope_local_base_freq (float, *optional*, defaults to 10000.0):\n             The base period of the RoPE embeddings for local attention.\n+        use_bidirectional_attention (`bool`, *optional*, defaults to `False`): If True, the model will attend to all\n+            text tokens instead of using a causal mask. This does not change behavior for vision tokens.\n \n     ```python\n     >>> from transformers import Gemma3TextModel, Gemma3TextConfig\n@@ -204,6 +206,7 @@ def __init__(\n         attn_logit_softcapping=None,\n         rope_scaling=None,\n         rope_local_base_freq=10_000.0,\n+        use_bidirectional_attention=False,\n         **kwargs,\n     ):\n         PretrainedConfig.__init__(\n@@ -233,6 +236,7 @@ def __init__(\n         self.final_logit_softcapping = final_logit_softcapping\n         self.attn_logit_softcapping = attn_logit_softcapping\n         self.layer_types = layer_types\n+        self.use_bidirectional_attention = use_bidirectional_attention\n \n         self.rope_local_base_freq = rope_local_base_freq\n         self.rope_scaling = rope_scaling\n@@ -535,6 +539,19 @@ def _init_weights(self, module):\n             module.mm_input_projection_weight.data.zero_()\n \n \n+def _bidirectional_window_overlay(sliding_window: int) -> Callable[[int, int, int, int], bool]:\n+    \"\"\"\n+    Enables a bidirectional mask within the sliding window.\n+    \"\"\"\n+\n+    def inner_mask(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool:\n+        \"\"\"A token can attend to any other token if their absolute distance is within\n+        half the sliding window size (distance <= sliding_window // 2).\"\"\"\n+        return abs(q_idx - kv_idx) <= sliding_window // 2\n+\n+    return inner_mask\n+\n+\n class Gemma3TextModel(Gemma2Model):\n     config: Gemma3TextConfig\n \n@@ -609,10 +626,16 @@ def forward(\n                 \"past_key_values\": past_key_values,\n                 \"position_ids\": position_ids,\n             }\n+            sliding_mask_kwargs = mask_kwargs.copy()\n+\n+            if self.config.use_bidirectional_attention:\n+                mask_kwargs[\"or_mask_function\"] = lambda *args: torch.tensor(True, dtype=torch.bool)\n+                sliding_mask_kwargs[\"or_mask_function\"] = _bidirectional_window_overlay(self.config.sliding_window)\n+\n             # Create the masks\n             causal_mask_mapping = {\n                 \"full_attention\": create_causal_mask(**mask_kwargs),\n-                \"sliding_attention\": create_sliding_window_causal_mask(**mask_kwargs),\n+                \"sliding_attention\": create_sliding_window_causal_mask(**sliding_mask_kwargs),\n             }\n \n         # embed positions"
        }
    ],
    "stats": {
        "total": 197,
        "additions": 169,
        "deletions": 28
    }
}