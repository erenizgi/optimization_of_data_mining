{
    "author": "Cyrilvallez",
    "message": "Fix fsdp for generic-task models (#40191)\n\n* remove abc inheritance\n\n* add fast test",
    "sha": "421175685d715214b216f5ee71f78992e772691c",
    "files": [
        {
            "sha": "eea5595dc49e4a19e5b6195271b32bc3d346e3c0",
            "filename": "src/transformers/modeling_layers.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/421175685d715214b216f5ee71f78992e772691c/src%2Ftransformers%2Fmodeling_layers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/421175685d715214b216f5ee71f78992e772691c/src%2Ftransformers%2Fmodeling_layers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_layers.py?ref=421175685d715214b216f5ee71f78992e772691c",
            "patch": "@@ -11,7 +11,6 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from abc import ABC\n from functools import partial\n from typing import Optional\n \n@@ -96,7 +95,7 @@ def __call__(self, *args, **kwargs):\n \n \n @auto_docstring\n-class GenericForSequenceClassification(ABC):\n+class GenericForSequenceClassification(object):\n     base_model_prefix = \"model\"\n \n     def __init__(self, config):\n@@ -171,7 +170,7 @@ def forward(\n \n \n @auto_docstring\n-class GenericForQuestionAnswering(ABC):\n+class GenericForQuestionAnswering(object):\n     base_model_prefix = \"model\"\n \n     def __init__(self, config):\n@@ -232,7 +231,7 @@ def forward(\n \n \n @auto_docstring\n-class GenericForTokenClassification(ABC):\n+class GenericForTokenClassification(object):\n     base_model_prefix = \"model\"\n \n     def __init__(self, config):"
        },
        {
            "sha": "a8f944ad93f18bd6c199e15a6cc60cd0a0b355d4",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 20,
            "deletions": 0,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/421175685d715214b216f5ee71f78992e772691c/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/421175685d715214b216f5ee71f78992e772691c/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=421175685d715214b216f5ee71f78992e772691c",
            "patch": "@@ -3471,3 +3471,23 @@ def find_expectation(self, properties: DeviceProperties = (None, None, None)) ->\n \n     def __repr__(self):\n         return f\"{self.data}\"\n+\n+\n+def torchrun(script: str, nproc_per_node: int, is_torchrun: bool = True, env: Optional[dict] = None):\n+    \"\"\"Run the `script` using `torchrun` command for multi-processing in a subprocess. Captures errors as necessary.\"\"\"\n+    with tempfile.NamedTemporaryFile(mode=\"w+\", suffix=\".py\") as tmp:\n+        tmp.write(script)\n+        tmp.flush()\n+        tmp.seek(0)\n+        if is_torchrun:\n+            cmd = (\n+                f\"torchrun --nproc_per_node {nproc_per_node} --master_port {get_torch_dist_unique_port()} {tmp.name}\"\n+            ).split()\n+        else:\n+            cmd = [\"python3\", tmp.name]\n+\n+        # Note that the subprocess will be waited for here, and raise an error if not successful\n+        try:\n+            _ = subprocess.run(cmd, capture_output=True, env=env, text=True, check=True)\n+        except subprocess.CalledProcessError as e:\n+            raise Exception(f\"The following error was captured: {e.stderr}\")"
        },
        {
            "sha": "f1dcbe9daed55f8020957675e0bfefe4b5548240",
            "filename": "tests/generation/test_fsdp.py",
            "status": "modified",
            "additions": 29,
            "deletions": 0,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/421175685d715214b216f5ee71f78992e772691c/tests%2Fgeneration%2Ftest_fsdp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/421175685d715214b216f5ee71f78992e772691c/tests%2Fgeneration%2Ftest_fsdp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_fsdp.py?ref=421175685d715214b216f5ee71f78992e772691c",
            "patch": "@@ -13,6 +13,7 @@\n # limitations under the License.\n \n import argparse\n+import textwrap\n from typing import Any, Callable\n \n from transformers import is_torch_available, is_torch_xpu_available\n@@ -24,6 +25,7 @@\n     get_torch_dist_unique_port,\n     require_torch_multi_accelerator,\n     torch_device,\n+    torchrun,\n )\n from transformers.utils import is_ccl_available, is_ipex_available\n \n@@ -141,6 +143,33 @@ def test_fsdp2_generate(self):\n         # successful return here == success - any errors would have caused an error in the sub-call\n \n \n+class TestFSDPGenericTaskModel(TestCasePlus):\n+    nproc_per_node = 2\n+\n+    def test_generic_task_model_can_be_sharded(self):\n+        script_to_run = textwrap.dedent(\n+            \"\"\"\n+            import torch\n+            from torch.distributed.fsdp import fully_shard\n+            from transformers import AutoModelForTokenClassification\n+\n+            torch.distributed.init_process_group(\n+                backend=\"nccl\" if torch.cuda.is_available() else \"gloo\", init_method=\"env://\"\n+            )\n+            rank = torch.distributed.get_rank()\n+            if torch.cuda.is_available():\n+                torch.cuda.set_device(rank)\n+\n+            # Make sure it works\n+            model = AutoModelForTokenClassification.from_pretrained(\"Qwen/Qwen2-0.5B\")\n+            module = fully_shard(model)\n+\n+            torch.distributed.destroy_process_group()\n+            \"\"\"\n+        )\n+        torchrun(script_to_run, self.nproc_per_node, env=self.get_env())\n+\n+\n if __name__ == \"__main__\":\n     # The script below is meant to be run under torch.distributed, on a machine with multiple GPUs:\n     #"
        },
        {
            "sha": "14fd1a0904b49e0fb89efed8fd5c8d755cc82f7c",
            "filename": "tests/tensor_parallel/test_tensor_parallel.py",
            "status": "modified",
            "additions": 5,
            "deletions": 25,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/421175685d715214b216f5ee71f78992e772691c/tests%2Ftensor_parallel%2Ftest_tensor_parallel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/421175685d715214b216f5ee71f78992e772691c/tests%2Ftensor_parallel%2Ftest_tensor_parallel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftensor_parallel%2Ftest_tensor_parallel.py?ref=421175685d715214b216f5ee71f78992e772691c",
            "patch": "@@ -15,7 +15,6 @@\n # Run the test: CUDA_VISIBLE_DEVICES=0,1 RUN_SLOW=1 pytest -sv tests/tensor_parallel/test_tensor_parallel.py\n \n import os\n-import subprocess\n import tempfile\n import textwrap\n \n@@ -24,10 +23,10 @@\n from transformers.testing_utils import (\n     TestCasePlus,\n     backend_device_count,\n-    get_torch_dist_unique_port,\n     require_huggingface_hub_greater_or_equal,\n     require_torch_multi_accelerator,\n     torch_device,\n+    torchrun,\n )\n \n \n@@ -67,25 +66,6 @@ def size(self):\n class TestTensorParallel(TestCasePlus):\n     nproc_per_node = 2\n \n-    def torchrun(self, script: str, is_torchrun: bool = True):\n-        \"\"\"Run the `script` using `torchrun` command for multi-processing in a subprocess. Captures errors as necessary.\"\"\"\n-        with tempfile.NamedTemporaryFile(mode=\"w+\", suffix=\".py\") as tmp:\n-            tmp.write(script)\n-            tmp.flush()\n-            tmp.seek(0)\n-            if is_torchrun:\n-                cmd = (\n-                    f\"torchrun --nproc_per_node {self.nproc_per_node} --master_port {get_torch_dist_unique_port()} {tmp.name}\"\n-                ).split()\n-            else:\n-                cmd = [\"python3\", tmp.name]\n-\n-            # Note that the subprocess will be waited for here, and raise an error if not successful\n-            try:\n-                _ = subprocess.run(cmd, capture_output=True, env=self.get_env(), text=True, check=True)\n-            except subprocess.CalledProcessError as e:\n-                raise Exception(f\"The following error was captured: {e.stderr}\")\n-\n     def test_model_forward(self):\n         script_to_run = textwrap.dedent(\n             \"\"\"\n@@ -124,7 +104,7 @@ def test_model_forward(self):\n             torch.distributed.destroy_process_group()\n             \"\"\"\n         )\n-        self.torchrun(script_to_run)\n+        torchrun(script_to_run, self.nproc_per_node, env=self.get_env())\n \n     def test_model_backward_pass(self):\n         script_to_run = textwrap.dedent(\n@@ -150,7 +130,7 @@ def test_model_backward_pass(self):\n             torch.distributed.destroy_process_group()\n             \"\"\"\n         )\n-        self.torchrun(script_to_run)\n+        torchrun(script_to_run, self.nproc_per_node, env=self.get_env())\n \n     def test_model_generate(self):\n         script_to_run = textwrap.dedent(\n@@ -190,7 +170,7 @@ def test_model_generate(self):\n             torch.distributed.destroy_process_group()\n             \"\"\"\n         )\n-        self.torchrun(script_to_run)\n+        torchrun(script_to_run, self.nproc_per_node, env=self.get_env())\n \n     @require_huggingface_hub_greater_or_equal(\"0.31.4\")\n     def test_model_save(self):\n@@ -217,7 +197,7 @@ def test_model_save(self):\n                     model.save_pretrained(result_dir)\n                     \"\"\"\n                 )\n-                self.torchrun(script_to_run, is_torchrun=is_torchrun)\n+                torchrun(script_to_run, self.nproc_per_node, is_torchrun=is_torchrun, env=self.get_env())\n \n             non_tp_model_path = os.path.join(tmp_dir, \"nontp\")\n             tp_model_path = os.path.join(tmp_dir, \"tp\")"
        }
    ],
    "stats": {
        "total": 86,
        "additions": 57,
        "deletions": 29
    }
}