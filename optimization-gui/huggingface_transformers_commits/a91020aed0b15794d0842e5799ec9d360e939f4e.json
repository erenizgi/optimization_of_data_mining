{
    "author": "jinan-zhou",
    "message": "Add TimesFM Time Series Forecasting Model (#34082)\n\n* initial documentation\n\n* rename mask to attention_mask\n\n* smaller tests\n\n* fixup\n\n* fix copies\n\n* move to time series section\n\n* sort docs\n\n* isort fix\n\n* batch_size is not a configuration\n\n* rename to TimesFMModelForPrediction\n\n* initial script\n\n* add check_outputs\n\n* remove dropout_rate\n\n* works with torch.Tensor inputs\n\n* rename script\n\n* fix docstrings\n\n* fix freq when window_size is given\n\n* add loss\n\n* fix _quantile_loss\n\n* formatting\n\n* fix isort\n\n* add weight init\n\n* add support for sdpa and flash_attention_2\n\n* fixes for flash_attention\n\n* formatting\n\n* remove flash_attention\n\n* fix tests\n\n* fix file name\n\n* fix quantile loss\n\n* added initial TimesFMModelIntegrationTests\n\n* fix formatting\n\n* fix import order\n\n* fix _quantile_loss\n\n* add doc for SDPA\n\n* use timesfm 2.0\n\n* bug fix in timesfm decode function.\n\n* compare mean forecasts\n\n* refactor type hints, use CamelCase\n\n* consolidate decode func\n\n* more readable code for weight conversion\n\n* fix-copies\n\n* simpler init\n\n* renaem TimesFmMLP\n\n* use T5LayerNorm\n\n* fix tests\n\n* use initializer_range\n\n* TimesFmModel instead of TimesFmDecoder\n\n* TimesFmPositionalEmbedding takes config for its init\n\n* 2.0-500m-pytorch default configs\n\n* use TimesFmModel\n\n* fix formatting\n\n* ignore TimesFmModel for testing\n\n* fix docstring\n\n* override generate as its not needed\n\n* add doc strings\n\n* fix logging\n\n* add docstrings to output data classes\n\n* initial copy from t5\n\n* added config and attention layers\n\n* add TimesFMPositionalEmbedding\n\n* calcuate scale_factor once\n\n* add more configs and TimesFMResidualBlock\n\n* fix input_dims\n\n* standardize code format with black\n\n* remove unneeded modules\n\n* TimesFM Model\n\n* order of imports\n\n* copy from Google official implementation\n\n* remove covariate forecasting\n\n* Adapting TimesFM to HF format\n\n* restructing in progress\n\n* adapted to HF convention\n\n* timesfm test\n\n* the model runs\n\n* fixing unit tests\n\n* fixing unit tests in progress\n\n* add post_init\n\n* do not change TimesFMOutput\n\n* fixing unit tests\n\n* all unit tests passed\n\n* remove timesfm_layers\n\n* add intermediate_size and initialize with config\n\n* initial documentation\n\n* rename mask to attention_mask\n\n* smaller tests\n\n* fixup\n\n* fix copies\n\n* move to time series section\n\n* sort docs\n\n* isort fix\n\n* batch_size is not a configuration\n\n* rename to TimesFMModelForPrediction\n\n* initial script\n\n* add check_outputs\n\n* remove dropout_rate\n\n* works with torch.Tensor inputs\n\n* rename script\n\n* fix docstrings\n\n* fix freq when window_size is given\n\n* add loss\n\n* fix _quantile_loss\n\n* formatting\n\n* fix isort\n\n* add weight init\n\n* add support for sdpa and flash_attention_2\n\n* fixes for flash_attention\n\n* formatting\n\n* remove flash_attention\n\n* fix tests\n\n* fix file name\n\n* fix quantile loss\n\n* added initial TimesFMModelIntegrationTests\n\n* fix formatting\n\n* fix import order\n\n* fix _quantile_loss\n\n* add doc for SDPA\n\n* use timesfm 2.0\n\n* bug fix in timesfm decode function.\n\n* compare mean forecasts\n\n* refactor type hints, use CamelCase\n\n* consolidate decode func\n\n* more readable code for weight conversion\n\n* fix-copies\n\n* simpler init\n\n* renaem TimesFmMLP\n\n* use T5LayerNorm\n\n* fix tests\n\n* use initializer_range\n\n* TimesFmModel instead of TimesFmDecoder\n\n* TimesFmPositionalEmbedding takes config for its init\n\n* 2.0-500m-pytorch default configs\n\n* use TimesFmModel\n\n* fix formatting\n\n* ignore TimesFmModel for testing\n\n* fix docstring\n\n* override generate as its not needed\n\n* add doc strings\n\n* fix logging\n\n* add docstrings to output data classes\n\n* add _CHECKPOINT_FOR_DOC\n\n* fix comments\n\n* Revert \"fix comments\"\n\nThis reverts commit 8deeb3e191b3671bc1d74dbfe77b736a066c3d34.\n\n* add _prepare_4d_attention_mask\n\n* we do not have generative model classes\n\n* use Cache\n\n* return past_key_values\n\n* modules initialized with config only\n\n* update year\n\n* Update docs/source/en/model_doc/timesfm.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* add layer_idx to cache\n\n* modular timesfm\n\n* fix test\n\n* unwrap sequential class\n\n* fix toctree\n\n* remove TimesFmOnnxConfig\n\n* fix modular\n\n* remove TimesFmStackedDecoder\n\n* split qkv layer into individual layers\n\n* rename projection layers\n\n* use ALL_ATTENTION_FUNCTIONS\n\n* is_causal is True\n\n* rename config\n\n* does not support flash_attn_2\n\n* formatting\n\n* fix typo in docsstring\n\n* rename inputs\n\n* add time series mapping\n\n* Update src/transformers/models/olmo2/modeling_olmo2.py\n\n* Update src/transformers/models/moonshine/modeling_moonshine.py\n\n* use updated arguments\n\n* fix class name\n\n* add MODEL_FOR_TIME_SERIES_PREDICTION_MAPPING\n\n* isort\n\n* consolidate _preprocess into forward\n\n* fix a typo\n\n* fix a typo\n\n* fix toc\n\n* fix modular\n\n* remove aaserts\n\n* use self.config._attn_implementation\n\n* move to _postprocess_output\n\n* remove timesfm_get_large_negative_number\n\n* use view unstead of multiple unsqueeze\n\n* make helpers static methods of the Model\n\n* use to_tuple\n\n* use to_tuple if not return_dict\n\n* remove unused intitialization block as its incorporated in nn.Linear\n\n* remove unused num_key_value_groups\n\n* use the same convention as the masking method\n\n* update modular\n\n* do not use unsqueeze\n\n* use view instead of unsqueeze\n\n* use buffer for inv_timescales\n\n* formatting\n\n* modular conversion\n\n* remove unneeded intialization\n\n* add missing docstrings\n\n* remove cache\n\n* use simple_eager_attention_forward\n\n* support tp_plan\n\n* support for flex and flash attention masks\n\n* Revert \"support for flex and flash attention masks\"\n\nThis reverts commit def36c4fcf31599b3f4937c9334b7da1a20132c3.\n\n* fix device\n\n* fix tests on gpu\n\n* remove unsued large model test\n\n* removed unneeded comments\n\n* add example usage\n\n* fix style\n\n* add import\n\n* Update docs/source/en/model_doc/timesfm.md\n\nCo-authored-by: Cyril Vallez <cyril.vallez@gmail.com>\n\n* inherit from LlamaRMSNorm\n\n* use can_return_tuple decorator\n\n* remvoe return_dict\n\n* fix year\n\n* Update docs/source/en/model_doc/timesfm.md\n\nCo-authored-by: Cyril Vallez <cyril.vallez@gmail.com>\n\n* pretrained does not inherit from GenerationMixin\n\n* use model for integration test\n\n---------\n\nCo-authored-by: Kashif Rasul <kashif.rasul@gmail.com>\nCo-authored-by: Rajat Sen <rsen91@gmail.com>\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\nCo-authored-by: Cyril Vallez <cyril.vallez@gmail.com>\nCo-authored-by: Cyril Vallez <cyril.vallez@huggingface.co>",
    "sha": "a91020aed0b15794d0842e5799ec9d360e939f4e",
    "files": [
        {
            "sha": "0bd54e5a3b18df7234bd3f3609ac08d6a43313ea",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a91020aed0b15794d0842e5799ec9d360e939f4e/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/a91020aed0b15794d0842e5799ec9d360e939f4e/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=a91020aed0b15794d0842e5799ec9d360e939f4e",
            "patch": "@@ -1059,6 +1059,8 @@\n         title: PatchTST\n       - local: model_doc/time_series_transformer\n         title: Time Series Transformer\n+      - local: model_doc/timesfm\n+        title: TimesFM\n       title: Time series models\n     - sections:\n       - local: model_doc/graphormer"
        },
        {
            "sha": "f5e279949197e839647820cff63205b2ed2f4c3c",
            "filename": "docs/source/en/model_doc/timesfm.md",
            "status": "added",
            "additions": 88,
            "deletions": 0,
            "changes": 88,
            "blob_url": "https://github.com/huggingface/transformers/blob/a91020aed0b15794d0842e5799ec9d360e939f4e/docs%2Fsource%2Fen%2Fmodel_doc%2Ftimesfm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a91020aed0b15794d0842e5799ec9d360e939f4e/docs%2Fsource%2Fen%2Fmodel_doc%2Ftimesfm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ftimesfm.md?ref=a91020aed0b15794d0842e5799ec9d360e939f4e",
            "patch": "@@ -0,0 +1,88 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# TimesFM\n+\n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n+## Overview\n+\n+TimesFM (Time Series Foundation Model) is a pretrained time-series foundation model proposed in [A decoder-only foundation model for time-series forecasting](https://huggingface.co/papers/2310.10688) by Abhimanyu Das, Weihao Kong, Rajat Sen, and  Yichen Zhou. It is a decoder only model that uses non-overlapping patches of time-series data as input and outputs some output patch length prediction in an autoregressive fashion.\n+\n+\n+The abstract from the paper is the following:\n+\n+*Motivated by recent advances in large language models for Natural Language Processing (NLP), we design a time-series foundation model for forecasting whose out-of-the-box zero-shot performance on a variety of public datasets comes close to the accuracy of state-of-the-art supervised forecasting models for each individual dataset. Our model is based on pretraining a patched-decoder style attention model on a large time-series corpus, and can work well across different forecasting history lengths, prediction lengths and temporal granularities.*\n+\n+\n+This model was contributed by [kashif](https://huggingface.co/kashif).\n+The original code can be found [here](https://github.com/google-research/timesfm).\n+\n+\n+To use the model:\n+\n+```python\n+import torch\n+from transformers import TimesFmModelForPrediction\n+\n+\n+model = TimesFmModelForPrediction.from_pretrained(\n+    \"google/timesfm-2.0-500m-pytorch\",\n+    torch_dtype=torch.bfloat16,\n+    attn_implementation=\"sdpa\",\n+    device_map=\"cuda\" if torch.cuda.is_available() else None\n+)\n+\n+\n+ # Create dummy inputs\n+forecast_input = [\n+    np.sin(np.linspace(0, 20, 100)),\n+    np.sin(np.linspace(0, 20, 200)),\n+    np.sin(np.linspace(0, 20, 400)),\n+]\n+frequency_input = [0, 1, 2]\n+\n+# Convert inputs to sequence of tensors\n+forecast_input_tensor = [\n+    torch.tensor(ts, dtype=torch.bfloat16).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n+    for ts in forecast_input\n+]\n+frequency_input_tensor = torch.tensor(frequency_input, dtype=torch.long).to(\n+    \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+)\n+\n+# Get predictions from the pre-trained model\n+with torch.no_grad():\n+    outputs = model(past_values=forecast_input_tensor, freq=frequency_input_tensor, return_dict=True)\n+    point_forecast_conv = outputs.mean_predictions.float().cpu().numpy()\n+    quantile_forecast_conv = outputs.full_predictions.float().cpu().numpy()\n+```\n+\n+## TimesFmConfig\n+\n+[[autodoc]] TimesFmConfig\n+\n+## TimesFmModel\n+\n+[[autodoc]] TimesFmModel\n+    - forward\n+\n+## TimesFmModelForPrediction\n+\n+[[autodoc]] TimesFmModelForPrediction\n+    - forward"
        },
        {
            "sha": "94a68374cb72a13f06a8059360628e12d305ac55",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a91020aed0b15794d0842e5799ec9d360e939f4e/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a91020aed0b15794d0842e5799ec9d360e939f4e/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=a91020aed0b15794d0842e5799ec9d360e939f4e",
            "patch": "@@ -279,6 +279,7 @@\n     from .tapas import *\n     from .textnet import *\n     from .time_series_transformer import *\n+    from .timesfm import *\n     from .timesformer import *\n     from .timm_backbone import *\n     from .timm_wrapper import *"
        },
        {
            "sha": "c28ec163d1cbf73cf6dc7de29158881e9ec0962e",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a91020aed0b15794d0842e5799ec9d360e939f4e/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a91020aed0b15794d0842e5799ec9d360e939f4e/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=a91020aed0b15794d0842e5799ec9d360e939f4e",
            "patch": "@@ -313,6 +313,7 @@\n         (\"tapas\", \"TapasConfig\"),\n         (\"textnet\", \"TextNetConfig\"),\n         (\"time_series_transformer\", \"TimeSeriesTransformerConfig\"),\n+        (\"timesfm\", \"TimesFmConfig\"),\n         (\"timesformer\", \"TimesformerConfig\"),\n         (\"timm_backbone\", \"TimmBackboneConfig\"),\n         (\"timm_wrapper\", \"TimmWrapperConfig\"),\n@@ -681,6 +682,7 @@\n         (\"tapex\", \"TAPEX\"),\n         (\"textnet\", \"TextNet\"),\n         (\"time_series_transformer\", \"Time Series Transformer\"),\n+        (\"timesfm\", \"TimesFm\"),\n         (\"timesformer\", \"TimeSformer\"),\n         (\"timm_backbone\", \"TimmBackbone\"),\n         (\"timm_wrapper\", \"TimmWrapperModel\"),"
        },
        {
            "sha": "af832ee2393cae0aa48f040d5f38a343439d56a6",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 21,
            "deletions": 0,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/a91020aed0b15794d0842e5799ec9d360e939f4e/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a91020aed0b15794d0842e5799ec9d360e939f4e/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=a91020aed0b15794d0842e5799ec9d360e939f4e",
            "patch": "@@ -281,6 +281,7 @@\n         (\"tapas\", \"TapasModel\"),\n         (\"textnet\", \"TextNetModel\"),\n         (\"time_series_transformer\", \"TimeSeriesTransformerModel\"),\n+        (\"timesfm\", \"TimesFmModel\"),\n         (\"timesformer\", \"TimesformerModel\"),\n         (\"timm_backbone\", \"TimmBackbone\"),\n         (\"timm_wrapper\", \"TimmWrapperModel\"),\n@@ -1542,6 +1543,12 @@\n     ]\n )\n \n+MODEL_FOR_TIME_SERIES_PREDICTION_MAPPING_NAMES = OrderedDict(\n+    [\n+        (\"timesfm\", \"TimesFmModelForPrediction\"),\n+    ]\n+)\n+\n MODEL_FOR_IMAGE_TO_IMAGE_MAPPING_NAMES = OrderedDict(\n     [\n         (\"swin2sr\", \"Swin2SRForImageSuperResolution\"),\n@@ -1650,6 +1657,10 @@\n     CONFIG_MAPPING_NAMES, MODEL_FOR_TIME_SERIES_REGRESSION_MAPPING_NAMES\n )\n \n+MODEL_FOR_TIME_SERIES_PREDICTION_MAPPING = _LazyAutoMapping(\n+    CONFIG_MAPPING_NAMES, MODEL_FOR_TIME_SERIES_PREDICTION_MAPPING_NAMES\n+)\n+\n MODEL_FOR_IMAGE_TO_IMAGE_MAPPING = _LazyAutoMapping(CONFIG_MAPPING_NAMES, MODEL_FOR_IMAGE_TO_IMAGE_MAPPING_NAMES)\n \n \n@@ -1820,6 +1831,15 @@ class AutoModelForSemanticSegmentation(_BaseAutoModelClass):\n )\n \n \n+class AutoModelForTimeSeriesPrediction(_BaseAutoModelClass):\n+    _model_mapping = MODEL_FOR_TIME_SERIES_PREDICTION_MAPPING\n+\n+\n+AutoModelForTimeSeriesPrediction = auto_class_update(\n+    AutoModelForTimeSeriesPrediction, head_doc=\"time-series prediction\"\n+)\n+\n+\n class AutoModelForUniversalSegmentation(_BaseAutoModelClass):\n     _model_mapping = MODEL_FOR_UNIVERSAL_SEGMENTATION_MAPPING\n \n@@ -1994,6 +2014,7 @@ def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):\n     \"MODEL_FOR_TEXT_ENCODING_MAPPING\",\n     \"MODEL_FOR_TEXT_TO_WAVEFORM_MAPPING\",\n     \"MODEL_FOR_TEXT_TO_SPECTROGRAM_MAPPING\",\n+    \"MODEL_FOR_TIME_SERIES_PREDICTION_MAPPING\",\n     \"MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING\",\n     \"MODEL_FOR_UNIVERSAL_SEGMENTATION_MAPPING\",\n     \"MODEL_FOR_VIDEO_CLASSIFICATION_MAPPING\","
        },
        {
            "sha": "12f1541b9c549256b67b3a304cc482091842e94e",
            "filename": "src/transformers/models/timesfm/__init__.py",
            "status": "added",
            "additions": 27,
            "deletions": 0,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/a91020aed0b15794d0842e5799ec9d360e939f4e/src%2Ftransformers%2Fmodels%2Ftimesfm%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a91020aed0b15794d0842e5799ec9d360e939f4e/src%2Ftransformers%2Fmodels%2Ftimesfm%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftimesfm%2F__init__.py?ref=a91020aed0b15794d0842e5799ec9d360e939f4e",
            "patch": "@@ -0,0 +1,27 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_timesfm import *\n+    from .modeling_timesfm import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "bd371cf1b21879ada9d8923a6d01372476d3efc9",
            "filename": "src/transformers/models/timesfm/configuration_timesfm.py",
            "status": "added",
            "additions": 129,
            "deletions": 0,
            "changes": 129,
            "blob_url": "https://github.com/huggingface/transformers/blob/a91020aed0b15794d0842e5799ec9d360e939f4e/src%2Ftransformers%2Fmodels%2Ftimesfm%2Fconfiguration_timesfm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a91020aed0b15794d0842e5799ec9d360e939f4e/src%2Ftransformers%2Fmodels%2Ftimesfm%2Fconfiguration_timesfm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftimesfm%2Fconfiguration_timesfm.py?ref=a91020aed0b15794d0842e5799ec9d360e939f4e",
            "patch": "@@ -0,0 +1,129 @@\n+# coding=utf-8\n+# Copyright 2025 Google LLC and HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"TimesFM model configuration\"\"\"\n+\n+from typing import List\n+\n+from ...configuration_utils import PretrainedConfig\n+from ...utils import logging\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class TimesFmConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`TimesFmModelForPrediction`] or a [`TFTimesFmModel`]. It is used to\n+    instantiate a TimesFM model according to the specified arguments, defining the model architecture. Instantiating a\n+    configuration with the defaults will yield a similar configuration to that of the TimesFM\n+    [google/timesfm-2.0-500m-pytorch](https://huggingface.co/google/timesfm-2.0-500m-pytorch) architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Arguments:\n+        patch_length (`int`, *optional*, defaults to 32):\n+            The length of one patch in the input sequence.\n+        context_length (`int`, *optional*, defaults to 512):\n+            The length of the input context.\n+        horizon_length (`int`, *optional*, defaults to 128):\n+            The length of the prediction horizon.\n+        freq_size (`int`, *optional*, defaults to 3):\n+            The number of frequency embeddings.\n+        num_hidden_layers (`int`, *optional*, defaults to 50):\n+            Number of Transformer layers.\n+        hidden_size (`int`, *optional*, defaults to 1280):\n+            Size of the hidden layers in the feed-forward networks.\n+        intermediate_size (`int`, *optional*, defaults to 1280):\n+            Dimension of the MLP representations.\n+        head_dim (`int`, *optional*, defaults to 80):\n+            Size of the key, query, value projections per attention head. The `inner_dim` of the projection layer will\n+            be defined as `num_attention_heads * head_dim`.\n+        num_attention_heads (`int`, *optional*, defaults to 16):\n+            Number of attention heads for each attention layer in the Transformer encoder.\n+        tolerance (`float`, *optional*, defaults to 1e-06):\n+            The tolerance for the quantile loss.\n+        rms_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            The epsilon used by the RMS normalization layers.\n+        quantiles (`List[float]`, *optional*, defaults to `[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]`):\n+            The quantiles to predict.\n+        pad_val (`float`, *optional*, defaults to 1123581321.0):\n+            The value used to pad the predictions.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout probability for the attention scores.\n+        use_positional_embedding (`bool`, *optional*, defaults to `False`):\n+            Whether to add positional embeddings.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        min_timescale (`int`, *optional*, defaults to 1):\n+            The start of the geometric positional index. Determines the periodicity of\n+            the added signal.\n+        max_timescale (`int`, *optional*, defaults to 10000):\n+            The end of the geometric positional index. Determines the frequency of the\n+            added signal.\n+    \"\"\"\n+\n+    model_type = \"timesfm\"\n+    keys_to_ignore_at_inference = []\n+    is_encoder_decoder = False\n+\n+    def __init__(\n+        self,\n+        patch_length: int = 32,\n+        context_length: int = 512,\n+        horizon_length: int = 128,\n+        freq_size: int = 3,\n+        num_hidden_layers: int = 50,\n+        hidden_size: int = 1280,\n+        intermediate_size: int = 1280,\n+        head_dim: int = 80,\n+        num_attention_heads: int = 16,\n+        tolerance: float = 1e-6,\n+        rms_norm_eps: float = 1e-6,\n+        quantiles: List[float] = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n+        pad_val: float = 1123581321.0,\n+        attention_dropout: float = 0.0,\n+        use_positional_embedding: bool = False,\n+        initializer_range: float = 0.02,\n+        min_timescale: int = 1,\n+        max_timescale: int = 10_000,\n+        **kwargs,\n+    ):\n+        self.patch_length = patch_length\n+        self.context_length = context_length\n+        self.horizon_length = horizon_length\n+        self.quantiles = quantiles\n+        self.pad_val = pad_val\n+        self.freq_size = freq_size\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.head_dim = head_dim\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.tolerance = tolerance\n+        self.rms_norm_eps = rms_norm_eps\n+        self.attention_dropout = attention_dropout\n+        self.use_positional_embedding = use_positional_embedding\n+        self.initializer_range = initializer_range\n+        self.min_timescale = min_timescale\n+        self.max_timescale = max_timescale\n+\n+        super().__init__(\n+            is_encoder_decoder=self.is_encoder_decoder,\n+            **kwargs,\n+        )\n+\n+\n+__all__ = [\"TimesFmConfig\"]"
        },
        {
            "sha": "06674fb087c5e44f0b5091bb427b50d76711a9ad",
            "filename": "src/transformers/models/timesfm/convert_timesfm_orignal_to_hf.py",
            "status": "added",
            "additions": 275,
            "deletions": 0,
            "changes": 275,
            "blob_url": "https://github.com/huggingface/transformers/blob/a91020aed0b15794d0842e5799ec9d360e939f4e/src%2Ftransformers%2Fmodels%2Ftimesfm%2Fconvert_timesfm_orignal_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a91020aed0b15794d0842e5799ec9d360e939f4e/src%2Ftransformers%2Fmodels%2Ftimesfm%2Fconvert_timesfm_orignal_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftimesfm%2Fconvert_timesfm_orignal_to_hf.py?ref=a91020aed0b15794d0842e5799ec9d360e939f4e",
            "patch": "@@ -0,0 +1,275 @@\n+import argparse\n+import os\n+import re\n+import shutil\n+\n+import numpy as np\n+import timesfm\n+import torch\n+\n+from transformers import TimesFmConfig, TimesFmModelForPrediction\n+\n+\n+\"\"\"\n+Sample usage:\n+\n+```\n+python src/transformers/models/timesfm/convert_timesfm_orignal_to_hf.py \\\n+    --output_dir /output/path\n+```\n+\"\"\"\n+\n+\n+def get_nested_attr(obj, key):\n+    \"\"\"Recursively retrieves an attribute from an object, handling list/tuple indexing if present.\"\"\"\n+    parts = key.split(\".\")\n+    for part in parts:\n+        match = re.match(r\"(.*)\\[(\\d+)\\]\", part)  # Handle list indexing like `layers[0]`\n+        if match:\n+            attr_name, index = match.groups()\n+            obj = getattr(obj, attr_name)[int(index)]  # Access list/tuple element\n+        else:\n+            obj = getattr(obj, part)  # Regular attribute access\n+    return obj\n+\n+\n+def write_model(model_path, safe_serialization=True, huggingface_repo_id=\"google/timesfm-2.0-500m-pytorch\"):\n+    os.makedirs(model_path, exist_ok=True)\n+    tmp_model_path = os.path.join(model_path, \"tmp\")\n+    os.makedirs(tmp_model_path, exist_ok=True)\n+\n+    tfm = timesfm.TimesFm(\n+        hparams=timesfm.TimesFmHparams(\n+            backend=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n+            per_core_batch_size=32,\n+            horizon_len=128,\n+            input_patch_len=32,\n+            output_patch_len=128,\n+            num_layers=50,\n+            model_dims=1280,\n+            use_positional_embedding=False,\n+        ),\n+        checkpoint=timesfm.TimesFmCheckpoint(huggingface_repo_id=huggingface_repo_id),\n+    )\n+\n+    timesfm_config = TimesFmConfig(\n+        patch_length=tfm.hparams.input_patch_len,\n+        context_length=tfm.hparams.context_len,\n+        horizon_length=tfm.hparams.horizon_len,\n+        num_hidden_layers=tfm.hparams.num_layers,\n+        hidden_size=tfm.hparams.model_dims,\n+        intermediate_size=tfm.hparams.model_dims,\n+        head_dim=tfm.hparams.model_dims // tfm.hparams.num_heads,\n+        num_attention_heads=tfm.hparams.num_heads,\n+        use_positional_embedding=tfm.hparams.use_positional_embedding,\n+    )\n+    timesfm_config.save_pretrained(tmp_model_path)\n+    timesfm_model = TimesFmModelForPrediction(timesfm_config)\n+\n+    # copy the weights from the original model to the new model making\n+    original_model = tfm._model\n+\n+    # mapping of the layers from the original model to the transformer model\n+    MODEL_LAYER_MAPPING = {\n+        \"input_ff_layer.hidden_layer[0].weight\": \"decoder.input_ff_layer.input_layer.weight\",\n+        \"input_ff_layer.hidden_layer[0].bias\": \"decoder.input_ff_layer.input_layer.bias\",\n+        \"input_ff_layer.output_layer.weight\": \"decoder.input_ff_layer.output_layer.weight\",\n+        \"input_ff_layer.output_layer.bias\": \"decoder.input_ff_layer.output_layer.bias\",\n+        \"input_ff_layer.residual_layer.weight\": \"decoder.input_ff_layer.residual_layer.weight\",\n+        \"input_ff_layer.residual_layer.bias\": \"decoder.input_ff_layer.residual_layer.bias\",\n+        \"freq_emb.weight\": \"decoder.freq_emb.weight\",\n+        \"horizon_ff_layer.hidden_layer[0].weight\": \"horizon_ff_layer.input_layer.weight\",\n+        \"horizon_ff_layer.hidden_layer[0].bias\": \"horizon_ff_layer.input_layer.bias\",\n+        \"horizon_ff_layer.output_layer.weight\": \"horizon_ff_layer.output_layer.weight\",\n+        \"horizon_ff_layer.output_layer.bias\": \"horizon_ff_layer.output_layer.bias\",\n+        \"horizon_ff_layer.residual_layer.weight\": \"horizon_ff_layer.residual_layer.weight\",\n+        \"horizon_ff_layer.residual_layer.bias\": \"horizon_ff_layer.residual_layer.bias\",\n+    }\n+\n+    TRANSFORMER_LAYER_MAPPING = {\n+        \"stacked_transformer.layers[{i}].self_attn.qkv_proj.weight\": \"decoder.layers[{i}].self_attn.qkv_proj.weight\",\n+        \"stacked_transformer.layers[{i}].self_attn.qkv_proj.bias\": \"decoder.layers[{i}].self_attn.qkv_proj.bias\",\n+        \"stacked_transformer.layers[{i}].self_attn.o_proj.weight\": \"decoder.layers[{i}].self_attn.o_proj.weight\",\n+        \"stacked_transformer.layers[{i}].self_attn.o_proj.bias\": \"decoder.layers[{i}].self_attn.o_proj.bias\",\n+        \"stacked_transformer.layers[{i}].self_attn.scaling\": \"decoder.layers[{i}].self_attn.scaling\",\n+        \"stacked_transformer.layers[{i}].mlp.gate_proj.weight\": \"decoder.layers[{i}].mlp.gate_proj.weight\",\n+        \"stacked_transformer.layers[{i}].mlp.gate_proj.bias\": \"decoder.layers[{i}].mlp.gate_proj.bias\",\n+        \"stacked_transformer.layers[{i}].mlp.down_proj.weight\": \"decoder.layers[{i}].mlp.down_proj.weight\",\n+        \"stacked_transformer.layers[{i}].mlp.down_proj.bias\": \"decoder.layers[{i}].mlp.down_proj.bias\",\n+        \"stacked_transformer.layers[{i}].mlp.layer_norm.weight\": \"decoder.layers[{i}].mlp.layer_norm.weight\",\n+        \"stacked_transformer.layers[{i}].mlp.layer_norm.bias\": \"decoder.layers[{i}].mlp.layer_norm.bias\",\n+        \"stacked_transformer.layers[{i}].input_layernorm.weight\": \"decoder.layers[{i}].input_layernorm.weight\",\n+    }\n+\n+    for old_key, new_key in MODEL_LAYER_MAPPING.items():\n+        try:\n+            old_attr = get_nested_attr(original_model, old_key)  # Get tensor from original model\n+            new_attr = get_nested_attr(timesfm_model, new_key)  # Get corresponding attribute in new model\n+            new_attr.data.copy_(old_attr.data)  # Copy data\n+        except AttributeError:\n+            print(f\"Skipping {old_key} (not found in original model).\")\n+\n+    num_layers = len(timesfm_model.decoder.layers)\n+    for i in range(num_layers):\n+        for old_template, new_template in TRANSFORMER_LAYER_MAPPING.items():\n+            old_key = old_template.format(i=i)\n+            new_key = new_template.format(i=i)\n+\n+            try:\n+                # Get tensor from original model\n+                old_attr = get_nested_attr(original_model, old_key)\n+                if \"qkv_proj\" in old_key:\n+                    # Split the tensor into q, k, v projections\n+                    q_proj, k_proj, v_proj = (\n+                        old_attr[: tfm.hparams.model_dims, ...],\n+                        old_attr[tfm.hparams.model_dims : tfm.hparams.model_dims * 2, ...],\n+                        old_attr[tfm.hparams.model_dims * 2 :, ...],\n+                    )\n+                    # Get corresponding attribute in new model\n+                    q_key = new_key.replace(\"qkv_proj\", \"q_proj\")\n+                    q_attr = get_nested_attr(timesfm_model, q_key)\n+                    q_attr.data.copy_(q_proj.data)  # Copy data\n+                    k_key = new_key.replace(\"qkv_proj\", \"k_proj\")\n+                    k_attr = get_nested_attr(timesfm_model, k_key)\n+                    k_attr.data.copy_(k_proj.data)  # Copy data\n+                    v_key = new_key.replace(\"qkv_proj\", \"v_proj\")\n+                    v_attr = get_nested_attr(timesfm_model, v_key)\n+                    v_attr.data.copy_(v_proj.data)  # Copy data\n+                else:\n+                    # Get corresponding attribute in new model\n+                    new_attr = get_nested_attr(timesfm_model, new_key)\n+                    new_attr.data.copy_(old_attr.data)  # Copy data\n+            except AttributeError:\n+                print(f\"Skipping {old_key} (not found in original model).\")\n+\n+    timesfm_model.save_pretrained(model_path, safe_serialization=safe_serialization)\n+    shutil.rmtree(tmp_model_path)\n+\n+\n+def check_outputs(model_path, huggingface_repo_id):\n+    \"\"\"Compares outputs between original and converted models.\"\"\"\n+    print(\"\\nChecking model outputs...\")\n+\n+    # Load original model\n+    tfm = timesfm.TimesFm(\n+        hparams=timesfm.TimesFmHparams(\n+            backend=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n+            per_core_batch_size=32,\n+            horizon_len=128,\n+            input_patch_len=32,\n+            output_patch_len=128,\n+            num_layers=50,\n+            model_dims=1280,\n+            use_positional_embedding=False,\n+            point_forecast_mode=\"mean\",\n+        ),\n+        checkpoint=timesfm.TimesFmCheckpoint(huggingface_repo_id=huggingface_repo_id),\n+    )\n+\n+    # Load converted model\n+    converted_model = TimesFmModelForPrediction.from_pretrained(\n+        model_path,\n+        torch_dtype=torch.bfloat16,\n+        attn_implementation=\"sdpa\",\n+    ).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n+    converted_model.eval()  # Set to evaluation mode\n+\n+    # Create test inputs\n+    forecast_input = [\n+        np.sin(np.linspace(0, 20, 100)),\n+        np.sin(np.linspace(0, 20, 200)),\n+        np.sin(np.linspace(0, 20, 400)),\n+    ]\n+    frequency_input = [0, 1, 2]\n+\n+    # Get predictions from original model\n+    point_forecast_orig, quantile_forecast_orig = tfm.forecast(\n+        forecast_input,\n+        freq=frequency_input,\n+    )\n+\n+    # Convert inputs to sequence of tensors\n+    forecast_input_tensor = [\n+        torch.tensor(ts, dtype=torch.bfloat16).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n+        for ts in forecast_input\n+    ]\n+    frequency_input_tensor = torch.tensor(frequency_input, dtype=torch.long).to(\n+        \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+    )\n+\n+    # Get predictions from converted model\n+    with torch.no_grad():\n+        outputs = converted_model(past_values=forecast_input_tensor, freq=frequency_input_tensor, return_dict=True)\n+        point_forecast_conv = outputs.mean_predictions.float().cpu().numpy()\n+        quantile_forecast_conv = outputs.full_predictions.float().cpu().numpy()\n+\n+    # Compare outputs\n+    point_forecast_diff = np.abs(point_forecast_orig - point_forecast_conv)\n+    quantile_forecast_diff = np.abs(quantile_forecast_orig - quantile_forecast_conv)\n+\n+    max_point_diff = point_forecast_diff.max()\n+    mean_point_diff = point_forecast_diff.mean()\n+    max_quantile_diff = quantile_forecast_diff.max()\n+    mean_quantile_diff = quantile_forecast_diff.mean()\n+\n+    print(\"\\nOutput comparison:\")\n+    print(f\"Point forecast - Max difference: {max_point_diff:.6f}\")\n+    print(f\"Point forecast - Mean difference: {mean_point_diff:.6f}\")\n+    print(f\"Quantile forecast - Max difference: {max_quantile_diff:.6f}\")\n+    print(f\"Quantile forecast - Mean difference: {mean_quantile_diff:.6f}\")\n+\n+    # Define acceptable thresholds\n+    POINT_THRESHOLD = 1e-5\n+    QUANTILE_THRESHOLD = 1e-5\n+\n+    if max_point_diff > POINT_THRESHOLD or max_quantile_diff > QUANTILE_THRESHOLD:\n+        raise ValueError(\n+            f\"Output mismatch detected!\\n\"\n+            f\"Point forecast max diff: {max_point_diff} (threshold: {POINT_THRESHOLD})\\n\"\n+            f\"Quantile forecast max diff: {max_quantile_diff} (threshold: {QUANTILE_THRESHOLD})\"\n+        )\n+\n+    print(\"\\nâœ“ All outputs match within acceptable tolerance!\")\n+\n+    # Optional: Print shapes for verification\n+    print(\"\\nOutput shapes:\")\n+    print(f\"Original point forecast: {point_forecast_orig.shape}\")\n+    print(f\"Converted point forecast: {point_forecast_conv.shape}\")\n+    print(f\"Original quantile forecast: {quantile_forecast_orig.shape}\")\n+    print(f\"Converted quantile forecast: {quantile_forecast_conv.shape}\")\n+\n+\n+def main():\n+    parser = argparse.ArgumentParser()\n+    parser.add_argument(\n+        \"--output_dir\",\n+        required=True,\n+        help=\"Location to write HF model and tokenizer\",\n+    )\n+    parser.add_argument(\n+        \"--safe_serialization\", type=bool, default=True, help=\"Whether or not to save using `safetensors`.\"\n+    )\n+    parser.add_argument(\n+        \"--huggingface_repo_id\",\n+        type=str,\n+        default=\"google/timesfm-2.0-500m-pytorch\",\n+        help=\"The Hugging Face repository ID to use for the model.\",\n+    )\n+    args = parser.parse_args()\n+\n+    # if the saved model file exists, skip the conversion\n+    if os.path.exists(\n+        os.path.join(args.output_dir, \"model.safetensors\" if args.safe_serialization else \"pytorch_model.bin\")\n+    ):\n+        print(f\"Model already exists in {args.output_dir}, skipping conversion.\")\n+    else:\n+        write_model(\n+            model_path=args.output_dir,\n+            safe_serialization=args.safe_serialization,\n+            huggingface_repo_id=args.huggingface_repo_id,\n+        )\n+    check_outputs(args.output_dir, args.huggingface_repo_id)\n+\n+\n+if __name__ == \"__main__\":\n+    main()"
        },
        {
            "sha": "8a12b2c56cfc4bb7a3eaba21264b56a556cbbb52",
            "filename": "src/transformers/models/timesfm/modeling_timesfm.py",
            "status": "added",
            "additions": 904,
            "deletions": 0,
            "changes": 904,
            "blob_url": "https://github.com/huggingface/transformers/blob/a91020aed0b15794d0842e5799ec9d360e939f4e/src%2Ftransformers%2Fmodels%2Ftimesfm%2Fmodeling_timesfm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a91020aed0b15794d0842e5799ec9d360e939f4e/src%2Ftransformers%2Fmodels%2Ftimesfm%2Fmodeling_timesfm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftimesfm%2Fmodeling_timesfm.py?ref=a91020aed0b15794d0842e5799ec9d360e939f4e",
            "patch": "@@ -0,0 +1,904 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/timesfm/modular_timesfm.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_timesfm.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright 2025 Google LLC and HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import math\n+from dataclasses import dataclass\n+from typing import Callable, Optional, Sequence, Union\n+\n+import torch\n+import torch.nn as nn\n+import torch.nn.functional as F\n+\n+from ...integrations import use_kernel_forward_from_hub\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_outputs import BaseModelOutput\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import (\n+    add_code_sample_docstrings,\n+    add_start_docstrings,\n+    add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n+    logging,\n+    replace_return_docstrings,\n+)\n+from .configuration_timesfm import TimesFmConfig\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+_CHECKPOINT_FOR_DOC = \"google/timesfm-2.0-500m-pytorch\"\n+_CONFIG_FOR_DOC = \"TimesFmConfig\"\n+\n+\n+@dataclass\n+class TimesFmOutput(BaseModelOutput):\n+    \"\"\"\n+    Args:\n+        loc (`torch.Tensor` of shape `(batch_size, )`):\n+            The mean of the time series inputs.\n+        scale (`torch.Tensor` of shape `(batch_size,)`):\n+            The scale of the time series inputs.\n+    \"\"\"\n+\n+    loc: Optional[torch.Tensor] = None\n+    scale: Optional[torch.Tensor] = None\n+\n+\n+@dataclass\n+class TimesFmOutputForPrediction(BaseModelOutput):\n+    \"\"\"\n+    Args:\n+        mean_predictions (`torch.Tensor` of shape `(batch_size, sequence_length)`):\n+            The mean predictions of the time series.\n+        full_predictions (`torch.Tensor` of shape `(batch_size, sequence_length)`):\n+            The full predictions of the time series including the mean and the quantiles.\n+        loss (`torch.Tensor` of shape `(1,)`, *optional*, returned when `future_values` is provided):\n+            The loss of the TimesFM model.\n+    \"\"\"\n+\n+    mean_predictions: Optional[torch.Tensor] = None\n+    full_predictions: Optional[torch.Tensor] = None\n+    loss: Optional[Union[torch.Tensor, float]] = None\n+\n+\n+class TimesFmMLP(nn.Module):\n+    \"\"\"Pax MLP in pytorch.\"\"\"\n+\n+    def __init__(self, config: TimesFmConfig):\n+        super().__init__()\n+        hidden_size = config.hidden_size\n+        intermediate_size = config.intermediate_size\n+\n+        self.gate_proj = nn.Linear(hidden_size, intermediate_size)\n+        self.down_proj = nn.Linear(intermediate_size, hidden_size)\n+        self.layer_norm = nn.LayerNorm(normalized_shape=hidden_size, eps=1e-6)\n+\n+    def forward(self, x, paddings=None):\n+        gate_inp = self.layer_norm(x)\n+        gate = self.gate_proj(gate_inp)\n+        gate = F.relu(gate)\n+        outputs = self.down_proj(gate)\n+        if paddings is not None:\n+            outputs = outputs * (1.0 - paddings[:, :, None])\n+        return outputs + x\n+\n+\n+class TimesFmResidualBlock(nn.Module):\n+    \"\"\"TimesFM residual block.\"\"\"\n+\n+    def __init__(self, input_dims, hidden_dims, output_dims):\n+        super().__init__()\n+        self.input_dims = input_dims\n+        self.hidden_dims = hidden_dims\n+        self.output_dims = output_dims\n+\n+        self.input_layer = nn.Linear(input_dims, hidden_dims)\n+        self.activation = nn.SiLU()\n+        self.output_layer = nn.Linear(hidden_dims, output_dims)\n+        self.residual_layer = nn.Linear(input_dims, output_dims)\n+\n+    def forward(self, x):\n+        hidden = self.input_layer(x)\n+        hidden = self.activation(hidden)\n+        output = self.output_layer(hidden)\n+        residual = self.residual_layer(x)\n+        return output + residual\n+\n+\n+@use_kernel_forward_from_hub(\"RMSNorm\")\n+class TimesFmRMSNorm(nn.Module):\n+    def __init__(self, hidden_size, eps=1e-6):\n+        \"\"\"\n+        TimesFmRMSNorm is equivalent to T5LayerNorm\n+        \"\"\"\n+        super().__init__()\n+        self.weight = nn.Parameter(torch.ones(hidden_size))\n+        self.variance_epsilon = eps\n+\n+    def forward(self, hidden_states):\n+        input_dtype = hidden_states.dtype\n+        hidden_states = hidden_states.to(torch.float32)\n+        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n+        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n+        return self.weight * hidden_states.to(input_dtype)\n+\n+    def extra_repr(self):\n+        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n+\n+\n+class TimesFmPositionalEmbedding(nn.Module):\n+    \"\"\"Generates position embedding for a given 1-d sequence.\"\"\"\n+\n+    def __init__(self, config: TimesFmConfig):\n+        super().__init__()\n+        min_timescale = config.min_timescale\n+        max_timescale = config.max_timescale\n+        self.embedding_dims = config.hidden_size\n+\n+        num_timescales = self.embedding_dims // 2\n+        log_timescale_increment = math.log(float(max_timescale) / float(min_timescale)) / max(num_timescales - 1, 1)\n+        self.register_buffer(\n+            \"inv_timescales\",\n+            min_timescale * torch.exp(torch.arange(num_timescales, dtype=torch.float32) * -log_timescale_increment),\n+        )\n+\n+    def forward(self, seq_length=None, position=None):\n+        \"\"\"Generates a Tensor of sinusoids with different frequencies.\n+\n+        Args:\n+            seq_length: an optional Python int defining the output sequence length.\n+              if the `position` argument is specified.\n+            position: [B, seq_length], optional position for each token in the\n+              sequence, only required when the sequence is packed.\n+\n+        Returns:\n+            [B, seqlen, D] if `position` is specified, else [1, seqlen, D]\n+        \"\"\"\n+        if position is None and seq_length is None:\n+            raise ValueError(\"Either position or seq_length must be provided\")\n+\n+        if position is None:\n+            # [1, seqlen]\n+            position = torch.arange(seq_length, dtype=torch.float32, device=self.inv_timescales.device).unsqueeze(0)\n+        elif position.ndim != 2:\n+            raise ValueError(f\"position must be 2-dimensional, got shape {position.shape}\")\n+\n+        scaled_time = position.view(*position.shape, 1) * self.inv_timescales.view(1, 1, -1)\n+        signal = torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], dim=2)\n+\n+        # Padding to ensure correct embedding dimension\n+        signal = F.pad(signal, (0, 0, 0, self.embedding_dims % 2))\n+        return signal\n+\n+\n+def simple_eager_attention_forward(\n+    module: nn.Module,\n+    query_states: torch.Tensor,\n+    key_states: torch.Tensor,\n+    value_states: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n+class TimesFmAttention(nn.Module):\n+    \"\"\"Implements the attention used in TimesFM. One key difference is that there is _per_dim_scaling of the query.\"\"\"\n+\n+    def __init__(self, config: TimesFmConfig, layer_idx: int):\n+        super().__init__()\n+        self.config = config\n+        self.is_causal = True\n+        self.attention_dropout = config.attention_dropout\n+        self.layer_idx = layer_idx\n+\n+        self.num_heads = config.num_attention_heads\n+        self.hidden_size = config.hidden_size\n+        self.head_dim = config.head_dim\n+\n+        self.q_size = self.num_heads * self.head_dim\n+        self.kv_size = self.num_heads * self.head_dim\n+        self.scaling = nn.Parameter(torch.empty((self.head_dim,)))\n+\n+        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim)\n+        self.k_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim)\n+        self.v_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim)\n+        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size)\n+\n+    def _scale_query(self, query: torch.Tensor) -> torch.Tensor:\n+        scale = F.softplus(self.scaling).mul(1.442695041 / math.sqrt(self.head_dim))\n+        return query * scale[None, None, None, :]\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        query_states = self._scale_query(query_states)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+\n+        attention_interface: Callable = simple_eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n+            else:\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=1.0,\n+            **kwargs,\n+        )\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n+\n+\n+class TimesFmDecoderLayer(nn.Module):\n+    \"\"\"Transformer layer.\"\"\"\n+\n+    def __init__(self, config: TimesFmConfig, layer_idx: int):\n+        super().__init__()\n+\n+        self.self_attn = TimesFmAttention(config, layer_idx=layer_idx)\n+        self.mlp = TimesFmMLP(config)\n+        self.input_layernorm = TimesFmRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: torch.Tensor,\n+        paddings: torch.Tensor,\n+        output_attentions: bool = False,\n+    ) -> tuple[Optional[torch.Tensor], torch.Tensor]:\n+        # Self Attention\n+        residual = hidden_states\n+        hidden_states = self.input_layernorm(hidden_states)\n+        hidden_states, scores = self.self_attn(\n+            hidden_states=hidden_states,\n+            attention_mask=attention_mask,\n+            output_attentions=output_attentions,\n+        )\n+        hidden_states = residual + hidden_states\n+\n+        # MLP\n+        hidden_states = self.mlp(hidden_states, paddings=paddings)\n+\n+        return scores, hidden_states\n+\n+\n+TIMESFM_START_DOCSTRING = r\"\"\"\n+    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n+    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n+    etc.)\n+\n+    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n+    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n+    and behavior.\n+\n+    Parameters:\n+        config ([`TimesFmConfig`]):\n+            Model configuration class with all the parameters of the model. Initializing with a config file does not\n+            load the weights associated with the model, only the configuration. Check out the\n+            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"The bare TimesFM Model outputting raw hidden-states without any specific head on top.\",\n+    TIMESFM_START_DOCSTRING,\n+)\n+class TimesFmPreTrainedModel(PreTrainedModel):\n+    \"\"\"handles the loading for all models.\"\"\"\n+\n+    config_class = TimesFmConfig\n+    base_model_prefix = \"timesfm\"\n+    _no_split_modules = [\"TimesFmDecoderLayer\"]\n+    main_input_name = \"past_values\"\n+    _supports_sdpa = True\n+\n+    def _init_weights(self, module):\n+        if isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0, std=self.config.initializer_range)\n+\n+        elif isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0, std=self.config.initializer_range)\n+            if module.bias is not None:\n+                nn.init.zeros_(module.bias)\n+\n+        elif isinstance(module, nn.LayerNorm):\n+            nn.init.ones_(module.weight)\n+            nn.init.zeros_(module.bias)\n+\n+        elif isinstance(module, TimesFmRMSNorm):\n+            nn.init.zeros_(module.weight)\n+\n+        elif isinstance(module, TimesFmAttention):\n+            # Initialize scaling parameter\n+            nn.init.ones_(module.scaling)\n+\n+\n+TIMESFM_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        past_values: list of time series forecast contexts. Each context time series\n+            can be a torch Tensor of potentially different context lengths.\n+        freq: frequency of each context time series in the inputs. 0 for high frequency\n+            (default), 1 for medium, and 2 for low.\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail. tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"The bare TimesFM Model outputting raw hidden-states without any specific head on top.\",\n+    TIMESFM_START_DOCSTRING,\n+)\n+class TimesFmModel(TimesFmPreTrainedModel):\n+    \"\"\"Patched time-series decoder without any specific output layer.\"\"\"\n+\n+    def __init__(self, config: TimesFmConfig):\n+        super().__init__(config)\n+\n+        self.config = config\n+        self.input_ff_layer = TimesFmResidualBlock(\n+            input_dims=2 * config.patch_length,\n+            output_dims=config.hidden_size,\n+            hidden_dims=config.intermediate_size,\n+        )\n+        self.freq_emb = nn.Embedding(num_embeddings=config.freq_size, embedding_dim=config.hidden_size)\n+        self.layers = nn.ModuleList(\n+            [TimesFmDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+        if self.config.use_positional_embedding:\n+            self.position_emb = TimesFmPositionalEmbedding(config=config)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def _forward_transform(\n+        self, inputs: torch.Tensor, patched_pads: torch.Tensor\n+    ) -> tuple[torch.Tensor, tuple[torch.Tensor, torch.Tensor]]:\n+        \"\"\"Input is of shape [B, N, P].\"\"\"\n+        mu, sigma = self._timesfm_masked_mean_std(inputs, patched_pads)\n+        sigma = torch.where(\n+            sigma < self.config.tolerance,\n+            torch.tensor(1.0, dtype=sigma.dtype, device=sigma.device),\n+            sigma,\n+        )\n+\n+        # Normalize each patch\n+        outputs = (inputs - mu[:, None, None]) / sigma[:, None, None]\n+        outputs = torch.where(\n+            torch.abs(inputs - self.config.pad_val) < self.config.tolerance,\n+            torch.tensor(self.config.pad_val, dtype=outputs.dtype, device=outputs.device),\n+            outputs,\n+        )\n+        return outputs, (mu, sigma)\n+\n+    @can_return_tuple\n+    @add_start_docstrings_to_model_forward(TIMESFM_INPUTS_DOCSTRING)\n+    def forward(\n+        self,\n+        past_values: torch.Tensor,\n+        past_values_padding: torch.LongTensor,\n+        freq: torch.Tensor,\n+        output_attentions: bool = False,\n+        output_hidden_states: bool = False,\n+    ) -> TimesFmOutput:\n+        \"\"\"\n+        past_values_padding (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n+            The padding indicator of the time series.\n+        \"\"\"\n+        # Reshape into patches (using view for efficiency)\n+        bsize = past_values.shape[0]\n+        patched_inputs = past_values.view(bsize, -1, self.config.patch_length)\n+        patched_pads = past_values_padding.view(bsize, -1, self.config.patch_length)\n+\n+        patched_inputs = torch.where(\n+            torch.abs(patched_pads - 1.0) < self.config.tolerance,\n+            torch.tensor(0.0, dtype=patched_inputs.dtype, device=patched_inputs.device),\n+            patched_inputs,\n+        )\n+        patched_pads = torch.where(\n+            torch.abs(patched_inputs - self.config.pad_val) < self.config.tolerance,\n+            torch.tensor(1.0, dtype=patched_pads.dtype, device=patched_pads.device),\n+            patched_pads,\n+        )\n+        patched_inputs, stats = self._forward_transform(patched_inputs, patched_pads)\n+\n+        # B x N x D\n+        patched_inputs = patched_inputs * (1.0 - patched_pads)\n+        concat_inputs = torch.cat([patched_inputs, patched_pads], dim=-1)\n+        model_input = self.input_ff_layer(concat_inputs)\n+\n+        # A patch should not be padded even if there is at least one zero.\n+        patched_padding = torch.min(patched_pads, dim=-1)[0]  # Get the values from the min result\n+        if self.config.use_positional_embedding:\n+            pos_emb = self.position_emb(model_input.shape[1])\n+            pos_emb = torch.concat([pos_emb] * model_input.shape[0], dim=0)\n+            pos_emb = self._timesfm_shift_padded_seq(patched_padding, pos_emb)\n+            model_input += pos_emb\n+\n+        f_emb = self.freq_emb(freq)  # B x 1 x D\n+        model_input += f_emb\n+\n+        # Convert paddings to attention mask and combine with causal mask\n+        hidden_states = model_input\n+        attention_mask = self._prepare_4d_attention_mask(\n+            attention_mask=patched_padding,\n+            sequence_length=hidden_states.shape[1],\n+            dtype=hidden_states.dtype,\n+            device=hidden_states.device,\n+            is_causal=True,\n+        )\n+\n+        all_attentions = []\n+        all_hidden_states = []\n+\n+        for layer in self.layers[: self.config.num_hidden_layers]:\n+            scores, hidden_states = layer(\n+                hidden_states=hidden_states,\n+                attention_mask=attention_mask,\n+                paddings=patched_padding,\n+                output_attentions=output_attentions,\n+            )\n+            if output_attentions:\n+                all_attentions.append(scores)\n+            if output_hidden_states:\n+                all_hidden_states.append(hidden_states)\n+\n+        if output_hidden_states:\n+            all_hidden_states = [model_input] + all_hidden_states\n+        else:\n+            all_hidden_states = None\n+\n+        return TimesFmOutput(\n+            last_hidden_state=hidden_states,\n+            hidden_states=all_hidden_states,\n+            attentions=all_attentions if output_attentions else None,\n+            loc=stats[0],\n+            scale=stats[1],\n+        )\n+\n+    @staticmethod\n+    def _prepare_4d_attention_mask(\n+        attention_mask: Optional[torch.Tensor],\n+        sequence_length: int,\n+        dtype: torch.dtype,\n+        device: torch.device,\n+        is_causal: bool = True,\n+    ) -> Optional[torch.Tensor]:\n+        \"\"\"\n+        Creates 4D attention mask and combines causal and padding masks if needed.\n+\n+        Args:\n+            attention_mask: Optional tensor of shape (batch_size, seq_length) containing padding mask\n+            sequence_length: Length of the sequence\n+            dtype: Data type of the mask\n+            device: Device of the mask\n+            is_causal: Whether to apply causal masking\n+\n+        Returns:\n+            4D attention mask of shape (batch_size, 1, seq_length, seq_length)\n+        \"\"\"\n+        # Get minimum value for the dtype\n+        min_value = torch.finfo(dtype).min if dtype.is_floating_point else torch.iinfo(dtype).min\n+\n+        # Handle padding mask\n+        if attention_mask is not None:\n+            # Convert 2D padding mask to 4D attention mask\n+            attention_mask = attention_mask.view(attention_mask.shape[0], 1, 1, -1)\n+            attention_mask = attention_mask * min_value\n+\n+        # Create causal mask if needed\n+        if is_causal:\n+            causal_mask = torch.triu(\n+                torch.ones((sequence_length, sequence_length), dtype=dtype, device=device) * min_value,\n+                diagonal=1,\n+            )\n+            causal_mask = causal_mask.view(1, 1, sequence_length, sequence_length)\n+\n+            # Combine with padding mask if it exists\n+            if attention_mask is not None:\n+                attention_mask = torch.minimum(attention_mask, causal_mask)\n+            else:\n+                attention_mask = causal_mask\n+\n+        return attention_mask\n+\n+    @staticmethod\n+    def _timesfm_masked_mean_std(inputs: torch.Tensor, padding: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n+        \"\"\"Calculates mean and standard deviation of `inputs` across axis 1.\n+\n+        It excludes values where `padding` is 1.\n+\n+        Args:\n+            inputs: A PyTorch tensor of shape [b, n, p].\n+            padding: A PyTorch tensor of shape [b, n, p] with values 0 or 1.\n+\n+        Returns:\n+            A tuple containing the mean and standard deviation.\n+            We return the statistics of the first patch with more than three non-padded values.\n+        \"\"\"\n+\n+        # Selecting the first patch with more than 3 unpadded values.\n+        def _get_patch_index(arr: torch.Tensor):\n+            indices = torch.argmax((arr >= 3).to(torch.int32), dim=1)\n+            row_sum = (arr >= 3).to(torch.int32).sum(dim=1)\n+            return torch.where(row_sum == 0, arr.shape[1] - 1, indices)\n+\n+        pad_sum = torch.sum(1 - padding, dim=2)\n+        patch_indices = _get_patch_index(pad_sum)\n+        bidxs = torch.arange(inputs.shape[0])\n+\n+        arr = inputs[bidxs, patch_indices, :]\n+        pad = padding[bidxs, patch_indices, :]\n+\n+        # Create a mask where padding is 0\n+        mask = 1 - pad\n+\n+        # Calculate the number of valid elements\n+        num_valid_elements = torch.sum(mask, dim=1)\n+        num_valid_elements = torch.where(\n+            num_valid_elements == 0,\n+            torch.tensor(1, dtype=num_valid_elements.dtype, device=num_valid_elements.device),\n+            num_valid_elements,\n+        )\n+\n+        # Calculate the masked sum and squared sum\n+        masked_sum = torch.sum(arr * mask, dim=1)\n+        masked_squared_sum = torch.sum((arr * mask) ** 2, dim=1)\n+\n+        # Calculate the masked mean and standard deviation\n+        masked_mean = masked_sum / num_valid_elements\n+        masked_var = masked_squared_sum / num_valid_elements - masked_mean**2\n+        masked_var = torch.where(\n+            masked_var < 0.0,\n+            torch.tensor(0.0, dtype=masked_var.dtype, device=masked_var.device),\n+            masked_var,\n+        )\n+        masked_std = torch.sqrt(masked_var)\n+\n+        return masked_mean, masked_std\n+\n+    @staticmethod\n+    def _timesfm_shift_padded_seq(mask: torch.Tensor, seq: torch.Tensor) -> torch.Tensor:\n+        \"\"\"Shifts rows of seq based on the first 0 in each row of the mask.\n+\n+        Args:\n+            mask: mask tensor of shape [B, N]\n+            seq: seq tensor of shape [B, N, P]\n+\n+        Returns:\n+            The shifted sequence.\n+        \"\"\"\n+        batch_size, num_seq, feature_dim = seq.shape\n+\n+        new_mask: torch.BoolTensor = mask == 0\n+\n+        # Use argmax to find the first True value in each row\n+        indices = new_mask.to(torch.int32).argmax(dim=1)\n+\n+        # Handle rows with all zeros\n+        indices[~new_mask.any(dim=1)] = -1\n+\n+        # Create index ranges for each sequence in the batch\n+        idx_range = torch.arange(num_seq, device=seq.device).view(1, -1, 1).expand(batch_size, -1, feature_dim)\n+\n+        # Calculate shifted indices for each element in each sequence\n+        shifted_idx = (idx_range - indices[:, None, None]) % num_seq\n+\n+        # Gather values from seq using shifted indices\n+        shifted_seq = seq.gather(1, shifted_idx)\n+\n+        return shifted_seq\n+\n+\n+class TimesFmModelForPrediction(TimesFmPreTrainedModel):\n+    \"\"\"TimesFM model for quantile and mean prediction.\"\"\"\n+\n+    def __init__(self, config: TimesFmConfig):\n+        super().__init__(config)\n+\n+        self.config = config\n+        self.context_len = config.context_length\n+        self.horizon_len = config.horizon_length\n+\n+        self.decoder = TimesFmModel(config)\n+\n+        # quantile and mean output\n+        self.horizon_ff_layer = TimesFmResidualBlock(\n+            input_dims=config.hidden_size,\n+            output_dims=config.horizon_length * (1 + len(config.quantiles)),\n+            hidden_dims=config.intermediate_size,\n+        )\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def _preprocess(\n+        self, inputs: Sequence[torch.Tensor], freq: Sequence[int]\n+    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n+        \"\"\"Formats and pads raw inputs to feed into the model.\n+\n+        This function both pads each time series to match the context length, and\n+        pads the inputs to meet the SPMD shape requirement.\n+\n+        Args:\n+          inputs: A list of 1d Tensors. Each Tensor is the context time series of\n+            a single forecast task.\n+          freq: list of frequencies\n+\n+        Returns:\n+        A tuple of:\n+        - the padded input time series to meet the model required context.\n+        - the padding indicator.\n+        - the number of padded examples for SPMD so that each core has the same\n+            number (a multiple of `batch_size`) of examples.\n+        \"\"\"\n+        input_ts, input_padding, inp_freq = [], [], []\n+\n+        for i, ts in enumerate(inputs):\n+            input_len = ts.shape[0]\n+            padding = torch.zeros(input_len + self.horizon_len, dtype=ts.dtype, device=ts.device)\n+            if input_len < self.context_len:\n+                num_front_pad = self.context_len - input_len\n+                ts = torch.cat([torch.zeros(num_front_pad, dtype=ts.dtype, device=ts.device), ts], dim=0)\n+                padding = torch.cat([torch.ones(num_front_pad, dtype=ts.dtype, device=padding.device), padding], dim=0)\n+            elif input_len > self.context_len:\n+                ts = ts[-self.context_len :]\n+                padding = padding[-(self.context_len + self.horizon_len) :]\n+\n+            input_ts.append(ts)\n+            input_padding.append(padding)\n+            inp_freq.append(freq[i])\n+\n+        return (\n+            torch.stack(input_ts, dim=0),\n+            torch.stack(input_padding, dim=0),\n+            torch.tensor(inp_freq, dtype=torch.int32).reshape(-1, 1),\n+        )\n+\n+    def _postprocess_output(\n+        self, model_output: torch.Tensor, stats: tuple[torch.Tensor, torch.Tensor]\n+    ) -> torch.Tensor:\n+        \"\"\"Postprocess output of stacked transformer.\"\"\"\n+\n+        # B x N x (H.Q)\n+        output_ts = self.horizon_ff_layer(model_output)\n+\n+        # Reshape using view\n+        b, n, _ = output_ts.shape\n+        output_ts = output_ts.view(b, n, self.config.horizon_length, len(self.config.quantiles) + 1)\n+\n+        mu, sigma = stats\n+        return output_ts * sigma[:, None, None, None] + mu[:, None, None, None]\n+\n+    def _quantile_loss(self, predictions: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n+        losses = []\n+        for i, q in enumerate(self.config.quantiles):\n+            errors = targets - predictions[..., i]\n+            loss = torch.max((q - 1) * errors, q * errors)\n+            losses.append(loss.mean())\n+        return torch.stack(losses).mean()\n+\n+    @can_return_tuple\n+    @add_start_docstrings_to_model_forward(TIMESFM_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=TimesFmOutputForPrediction, config_class=_CONFIG_FOR_DOC)\n+    @add_code_sample_docstrings(\n+        checkpoint=_CHECKPOINT_FOR_DOC,\n+        output_type=TimesFmOutputForPrediction,\n+        config_class=_CONFIG_FOR_DOC,\n+    )\n+    def forward(\n+        self,\n+        past_values: Sequence[torch.Tensor],\n+        freq: Optional[Sequence[Union[torch.Tensor, int]]] = None,\n+        window_size: Optional[int] = None,\n+        future_values: Optional[torch.Tensor] = None,\n+        forecast_context_len: Optional[int] = None,\n+        return_forecast_on_context: bool = False,\n+        truncate_negative: bool = False,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+    ) -> TimesFmOutputForPrediction:\n+        r\"\"\"\n+        window_size (`int`, *optional*):\n+            Window size of trend + residual decomposition. If None then we do not do decomposition.\n+        future_values (`torch.Tensor`, *optional*):\n+            Optional future time series values to be used for loss computation.\n+        forecast_context_len (`int`, *optional*):\n+            Optional max context length.\n+        return_forecast_on_context (`bool`, *optional*):\n+            True to return the forecast on the context when available, i.e. after the first input patch.\n+        truncate_negative (`bool`, *optional*):\n+            Truncate to only non-negative values if any of the contexts have non-negative values,\n+            otherwise do nothing.\n+        output_attentions (`bool`, *optional*):\n+            Whether to output the attentions.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether to output the hidden states.\n+\n+        Returns:\n+            A TimesFmOutputForPrediction object or a tuple containing:\n+                - the mean forecast of size (# past_values, # forecast horizon),\n+                - the full forecast (mean + quantiles) of size\n+                    (# past_values,  # forecast horizon, 1 + # quantiles).\n+                - loss: the mean squared error loss + quantile loss if `future_values` is provided.\n+        \"\"\"\n+        if forecast_context_len is None:\n+            fcontext_len = self.context_len\n+        else:\n+            fcontext_len = forecast_context_len\n+\n+        # Get device from first input tensor\n+        device = past_values[0].device\n+\n+        # Truncate inputs to forecast_context_len\n+        inputs = [ts[-fcontext_len:] for ts in past_values]\n+        inp_min = torch.min(torch.stack([torch.min(ts) for ts in inputs]))\n+\n+        if window_size is not None:\n+            new_inputs = []\n+            new_freqs = []\n+            for i, ts in enumerate(inputs):\n+                new_inputs.extend(self._timesfm_moving_average(ts, window_size))\n+                if freq is not None:\n+                    new_freqs.extend([freq[i]] * 2)\n+            inputs = new_inputs\n+            if freq is not None:\n+                freq = new_freqs\n+\n+        if freq is None:\n+            logger.info(\"No frequency provided via `freq`. Default to high (0).\")\n+            freq = [0] * len(inputs)\n+\n+        if output_attentions is None:\n+            output_attentions = self.config.output_attentions\n+        if output_hidden_states is None:\n+            output_hidden_states = self.config.output_hidden_states\n+\n+        input_ts, input_padding, inp_freq = self._preprocess(inputs, freq)\n+        # Move tensors to the same device as input\n+        input_ts = input_ts.to(device)\n+        input_padding = input_padding.to(device)\n+        inp_freq = inp_freq.to(device)\n+\n+        final_out = input_ts\n+        context_len = final_out.shape[1]\n+        full_outputs = []\n+\n+        if input_padding.shape[1] != final_out.shape[1] + self.horizon_len:\n+            raise ValueError(\n+                \"Length of paddings must match length of input + horizon_len:\"\n+                f\" {input_padding.shape[1]} != {final_out.shape[1]} + {self.horizon_len}\"\n+            )\n+        output_patch_len = self.config.horizon_length\n+\n+        num_decode_patches = (self.horizon_len + output_patch_len - 1) // output_patch_len\n+        for step_index in range(num_decode_patches):\n+            current_padding = input_padding[:, 0 : final_out.shape[1]]\n+            input_ts = final_out[:, -fcontext_len:]\n+            input_padding = current_padding[:, -fcontext_len:]\n+            decoder_output = self.decoder(\n+                past_values=input_ts,\n+                past_values_padding=input_padding,\n+                freq=inp_freq,\n+                output_attentions=output_attentions,\n+                output_hidden_states=output_hidden_states,\n+            )\n+            fprop_outputs = self._postprocess_output(\n+                decoder_output.last_hidden_state,\n+                (decoder_output.loc, decoder_output.scale),\n+            )\n+\n+            if return_forecast_on_context and step_index == 0:\n+                # For the first decodings step, collect the model forecast on the\n+                # context except the unavailable first input batch forecast.\n+                new_full_ts = fprop_outputs[:, :-1, : self.config.patch_length, :]\n+                # We have to use reshape and not view for non-contiguous memory\n+                new_full_ts = new_full_ts.reshape(new_full_ts.size(0), -1, new_full_ts.size(3))\n+\n+                full_outputs.append(new_full_ts)\n+\n+            # (full batch, last patch, output_patch_len, index of mean forecast = 0)\n+            new_ts = fprop_outputs[:, -1, :output_patch_len, 0]\n+            new_full_ts = fprop_outputs[:, -1, :output_patch_len, :]\n+            # (full batch, last patch, output_patch_len, all output indices)\n+            full_outputs.append(new_full_ts)\n+            final_out = torch.concatenate([final_out, new_ts], axis=-1)\n+\n+        if return_forecast_on_context:\n+            # `full_outputs` indexing starts at after the first input patch.\n+            full_outputs = torch.concatenate(full_outputs, axis=1)[\n+                :, : (context_len - self.config.patch_length + self.horizon_len), :\n+            ]\n+        else:\n+            # `full_outputs` indexing starts at the forecast horizon.\n+            full_outputs = torch.concatenate(full_outputs, axis=1)[:, 0 : self.horizon_len, :]\n+\n+        mean_outputs = full_outputs[:, :, 0]\n+        if window_size is not None:\n+            mean_outputs = mean_outputs[0::2, ...] + mean_outputs[1::2, ...]\n+            full_outputs = full_outputs[0::2, ...] + full_outputs[1::2, ...]\n+        if inp_min >= 0 and truncate_negative:\n+            mean_outputs = torch.maximum(mean_outputs, 0.0)\n+            full_outputs = torch.maximum(full_outputs, 0.0)\n+\n+        loss = None\n+        if future_values is not None:\n+            mse_loss = F.mse_loss(mean_outputs, future_values)\n+            quantile_loss = self._quantile_loss(full_outputs[:, :, 1:], future_values)\n+            loss = mse_loss + quantile_loss\n+\n+        return TimesFmOutputForPrediction(\n+            last_hidden_state=decoder_output.last_hidden_state,\n+            attentions=decoder_output.attentions if output_attentions else None,\n+            hidden_states=decoder_output.hidden_states if output_hidden_states else None,\n+            mean_predictions=mean_outputs,\n+            full_predictions=full_outputs,\n+            loss=loss,\n+        )\n+\n+    @staticmethod\n+    def _timesfm_moving_average(arr: torch.Tensor, window_size: int) -> list[torch.Tensor]:\n+        \"\"\"Calculates the moving average using PyTorch's convolution function.\"\"\"\n+        # Pad with zeros to handle initial window positions\n+        arr_padded = F.pad(arr, (window_size - 1, 0), \"constant\", 0)\n+        # Create a convolution kernel\n+        kernel = torch.ones(window_size, dtype=arr.dtype, device=arr.device) / window_size\n+        # Apply convolution to calculate the moving average\n+        smoothed_arr = F.conv1d(arr_padded.view(1, 1, -1), kernel.view(1, 1, -1)).squeeze()\n+        return [smoothed_arr, arr - smoothed_arr]\n+\n+\n+__all__ = [\"TimesFmModelForPrediction\", \"TimesFmPreTrainedModel\", \"TimesFmModel\"]"
        },
        {
            "sha": "4a62752484973b8162914eafb0b014f9ff96f871",
            "filename": "src/transformers/models/timesfm/modular_timesfm.py",
            "status": "added",
            "additions": 860,
            "deletions": 0,
            "changes": 860,
            "blob_url": "https://github.com/huggingface/transformers/blob/a91020aed0b15794d0842e5799ec9d360e939f4e/src%2Ftransformers%2Fmodels%2Ftimesfm%2Fmodular_timesfm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a91020aed0b15794d0842e5799ec9d360e939f4e/src%2Ftransformers%2Fmodels%2Ftimesfm%2Fmodular_timesfm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftimesfm%2Fmodular_timesfm.py?ref=a91020aed0b15794d0842e5799ec9d360e939f4e",
            "patch": "@@ -0,0 +1,860 @@\n+# coding=utf-8\n+# Copyright 2025 Google LLC and HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"PyTorch TimesFM model.\"\"\"\n+\n+import math\n+from dataclasses import dataclass\n+from typing import Callable, Optional, Sequence, Union\n+\n+import torch\n+import torch.nn as nn\n+import torch.nn.functional as F\n+\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_outputs import BaseModelOutput\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import (\n+    add_code_sample_docstrings,\n+    add_start_docstrings,\n+    add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n+    logging,\n+    replace_return_docstrings,\n+)\n+from ..llama.modeling_llama import LlamaRMSNorm\n+from ..phi4_multimodal.modeling_phi4_multimodal import simple_eager_attention_forward\n+from .configuration_timesfm import TimesFmConfig\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+_CHECKPOINT_FOR_DOC = \"google/timesfm-2.0-500m-pytorch\"\n+_CONFIG_FOR_DOC = \"TimesFmConfig\"\n+\n+\n+@dataclass\n+class TimesFmOutput(BaseModelOutput):\n+    \"\"\"\n+    Args:\n+        loc (`torch.Tensor` of shape `(batch_size, )`):\n+            The mean of the time series inputs.\n+        scale (`torch.Tensor` of shape `(batch_size,)`):\n+            The scale of the time series inputs.\n+    \"\"\"\n+\n+    loc: Optional[torch.Tensor] = None\n+    scale: Optional[torch.Tensor] = None\n+\n+\n+@dataclass\n+class TimesFmOutputForPrediction(BaseModelOutput):\n+    \"\"\"\n+    Args:\n+        mean_predictions (`torch.Tensor` of shape `(batch_size, sequence_length)`):\n+            The mean predictions of the time series.\n+        full_predictions (`torch.Tensor` of shape `(batch_size, sequence_length)`):\n+            The full predictions of the time series including the mean and the quantiles.\n+        loss (`torch.Tensor` of shape `(1,)`, *optional*, returned when `future_values` is provided):\n+            The loss of the TimesFM model.\n+    \"\"\"\n+\n+    mean_predictions: Optional[torch.Tensor] = None\n+    full_predictions: Optional[torch.Tensor] = None\n+    loss: Optional[Union[torch.Tensor, float]] = None\n+\n+\n+class TimesFmMLP(nn.Module):\n+    \"\"\"Pax MLP in pytorch.\"\"\"\n+\n+    def __init__(self, config: TimesFmConfig):\n+        super().__init__()\n+        hidden_size = config.hidden_size\n+        intermediate_size = config.intermediate_size\n+\n+        self.gate_proj = nn.Linear(hidden_size, intermediate_size)\n+        self.down_proj = nn.Linear(intermediate_size, hidden_size)\n+        self.layer_norm = nn.LayerNorm(normalized_shape=hidden_size, eps=1e-6)\n+\n+    def forward(self, x, paddings=None):\n+        gate_inp = self.layer_norm(x)\n+        gate = self.gate_proj(gate_inp)\n+        gate = F.relu(gate)\n+        outputs = self.down_proj(gate)\n+        if paddings is not None:\n+            outputs = outputs * (1.0 - paddings[:, :, None])\n+        return outputs + x\n+\n+\n+class TimesFmResidualBlock(nn.Module):\n+    \"\"\"TimesFM residual block.\"\"\"\n+\n+    def __init__(self, input_dims, hidden_dims, output_dims):\n+        super().__init__()\n+        self.input_dims = input_dims\n+        self.hidden_dims = hidden_dims\n+        self.output_dims = output_dims\n+\n+        self.input_layer = nn.Linear(input_dims, hidden_dims)\n+        self.activation = nn.SiLU()\n+        self.output_layer = nn.Linear(hidden_dims, output_dims)\n+        self.residual_layer = nn.Linear(input_dims, output_dims)\n+\n+    def forward(self, x):\n+        hidden = self.input_layer(x)\n+        hidden = self.activation(hidden)\n+        output = self.output_layer(hidden)\n+        residual = self.residual_layer(x)\n+        return output + residual\n+\n+\n+class TimesFmRMSNorm(LlamaRMSNorm):\n+    pass\n+\n+\n+class TimesFmPositionalEmbedding(nn.Module):\n+    \"\"\"Generates position embedding for a given 1-d sequence.\"\"\"\n+\n+    def __init__(self, config: TimesFmConfig):\n+        super().__init__()\n+        min_timescale = config.min_timescale\n+        max_timescale = config.max_timescale\n+        self.embedding_dims = config.hidden_size\n+\n+        num_timescales = self.embedding_dims // 2\n+        log_timescale_increment = math.log(float(max_timescale) / float(min_timescale)) / max(num_timescales - 1, 1)\n+        self.register_buffer(\n+            \"inv_timescales\",\n+            min_timescale * torch.exp(torch.arange(num_timescales, dtype=torch.float32) * -log_timescale_increment),\n+        )\n+\n+    def forward(self, seq_length=None, position=None):\n+        \"\"\"Generates a Tensor of sinusoids with different frequencies.\n+\n+        Args:\n+            seq_length: an optional Python int defining the output sequence length.\n+              if the `position` argument is specified.\n+            position: [B, seq_length], optional position for each token in the\n+              sequence, only required when the sequence is packed.\n+\n+        Returns:\n+            [B, seqlen, D] if `position` is specified, else [1, seqlen, D]\n+        \"\"\"\n+        if position is None and seq_length is None:\n+            raise ValueError(\"Either position or seq_length must be provided\")\n+\n+        if position is None:\n+            # [1, seqlen]\n+            position = torch.arange(seq_length, dtype=torch.float32, device=self.inv_timescales.device).unsqueeze(0)\n+        elif position.ndim != 2:\n+            raise ValueError(f\"position must be 2-dimensional, got shape {position.shape}\")\n+\n+        scaled_time = position.view(*position.shape, 1) * self.inv_timescales.view(1, 1, -1)\n+        signal = torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], dim=2)\n+\n+        # Padding to ensure correct embedding dimension\n+        signal = F.pad(signal, (0, 0, 0, self.embedding_dims % 2))\n+        return signal\n+\n+\n+class TimesFmAttention(nn.Module):\n+    \"\"\"Implements the attention used in TimesFM. One key difference is that there is _per_dim_scaling of the query.\"\"\"\n+\n+    def __init__(self, config: TimesFmConfig, layer_idx: int):\n+        super().__init__()\n+        self.config = config\n+        self.is_causal = True\n+        self.attention_dropout = config.attention_dropout\n+        self.layer_idx = layer_idx\n+\n+        self.num_heads = config.num_attention_heads\n+        self.hidden_size = config.hidden_size\n+        self.head_dim = config.head_dim\n+\n+        self.q_size = self.num_heads * self.head_dim\n+        self.kv_size = self.num_heads * self.head_dim\n+        self.scaling = nn.Parameter(torch.empty((self.head_dim,)))\n+\n+        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim)\n+        self.k_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim)\n+        self.v_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim)\n+        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size)\n+\n+    def _scale_query(self, query: torch.Tensor) -> torch.Tensor:\n+        scale = F.softplus(self.scaling).mul(1.442695041 / math.sqrt(self.head_dim))\n+        return query * scale[None, None, None, :]\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        query_states = self._scale_query(query_states)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+\n+        attention_interface: Callable = simple_eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n+            else:\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=1.0,\n+            **kwargs,\n+        )\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n+\n+\n+class TimesFmDecoderLayer(nn.Module):\n+    \"\"\"Transformer layer.\"\"\"\n+\n+    def __init__(self, config: TimesFmConfig, layer_idx: int):\n+        super().__init__()\n+\n+        self.self_attn = TimesFmAttention(config, layer_idx=layer_idx)\n+        self.mlp = TimesFmMLP(config)\n+        self.input_layernorm = TimesFmRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: torch.Tensor,\n+        paddings: torch.Tensor,\n+        output_attentions: bool = False,\n+    ) -> tuple[Optional[torch.Tensor], torch.Tensor]:\n+        # Self Attention\n+        residual = hidden_states\n+        hidden_states = self.input_layernorm(hidden_states)\n+        hidden_states, scores = self.self_attn(\n+            hidden_states=hidden_states,\n+            attention_mask=attention_mask,\n+            output_attentions=output_attentions,\n+        )\n+        hidden_states = residual + hidden_states\n+\n+        # MLP\n+        hidden_states = self.mlp(hidden_states, paddings=paddings)\n+\n+        return scores, hidden_states\n+\n+\n+TIMESFM_START_DOCSTRING = r\"\"\"\n+    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n+    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n+    etc.)\n+\n+    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n+    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n+    and behavior.\n+\n+    Parameters:\n+        config ([`TimesFmConfig`]):\n+            Model configuration class with all the parameters of the model. Initializing with a config file does not\n+            load the weights associated with the model, only the configuration. Check out the\n+            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"The bare TimesFM Model outputting raw hidden-states without any specific head on top.\",\n+    TIMESFM_START_DOCSTRING,\n+)\n+class TimesFmPreTrainedModel(PreTrainedModel):\n+    \"\"\"handles the loading for all models.\"\"\"\n+\n+    config_class = TimesFmConfig\n+    base_model_prefix = \"timesfm\"\n+    _no_split_modules = [\"TimesFmDecoderLayer\"]\n+    main_input_name = \"past_values\"\n+    _supports_sdpa = True\n+\n+    def _init_weights(self, module):\n+        if isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0, std=self.config.initializer_range)\n+\n+        elif isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0, std=self.config.initializer_range)\n+            if module.bias is not None:\n+                nn.init.zeros_(module.bias)\n+\n+        elif isinstance(module, nn.LayerNorm):\n+            nn.init.ones_(module.weight)\n+            nn.init.zeros_(module.bias)\n+\n+        elif isinstance(module, TimesFmRMSNorm):\n+            nn.init.zeros_(module.weight)\n+\n+        elif isinstance(module, TimesFmAttention):\n+            # Initialize scaling parameter\n+            nn.init.ones_(module.scaling)\n+\n+\n+TIMESFM_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        past_values: list of time series forecast contexts. Each context time series\n+            can be a torch Tensor of potentially different context lengths.\n+        freq: frequency of each context time series in the inputs. 0 for high frequency\n+            (default), 1 for medium, and 2 for low.\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail. tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"The bare TimesFM Model outputting raw hidden-states without any specific head on top.\",\n+    TIMESFM_START_DOCSTRING,\n+)\n+class TimesFmModel(TimesFmPreTrainedModel):\n+    \"\"\"Patched time-series decoder without any specific output layer.\"\"\"\n+\n+    def __init__(self, config: TimesFmConfig):\n+        super().__init__(config)\n+\n+        self.config = config\n+        self.input_ff_layer = TimesFmResidualBlock(\n+            input_dims=2 * config.patch_length,\n+            output_dims=config.hidden_size,\n+            hidden_dims=config.intermediate_size,\n+        )\n+        self.freq_emb = nn.Embedding(num_embeddings=config.freq_size, embedding_dim=config.hidden_size)\n+        self.layers = nn.ModuleList(\n+            [TimesFmDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+        if self.config.use_positional_embedding:\n+            self.position_emb = TimesFmPositionalEmbedding(config=config)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def _forward_transform(\n+        self, inputs: torch.Tensor, patched_pads: torch.Tensor\n+    ) -> tuple[torch.Tensor, tuple[torch.Tensor, torch.Tensor]]:\n+        \"\"\"Input is of shape [B, N, P].\"\"\"\n+        mu, sigma = self._timesfm_masked_mean_std(inputs, patched_pads)\n+        sigma = torch.where(\n+            sigma < self.config.tolerance,\n+            torch.tensor(1.0, dtype=sigma.dtype, device=sigma.device),\n+            sigma,\n+        )\n+\n+        # Normalize each patch\n+        outputs = (inputs - mu[:, None, None]) / sigma[:, None, None]\n+        outputs = torch.where(\n+            torch.abs(inputs - self.config.pad_val) < self.config.tolerance,\n+            torch.tensor(self.config.pad_val, dtype=outputs.dtype, device=outputs.device),\n+            outputs,\n+        )\n+        return outputs, (mu, sigma)\n+\n+    @can_return_tuple\n+    @add_start_docstrings_to_model_forward(TIMESFM_INPUTS_DOCSTRING)\n+    def forward(\n+        self,\n+        past_values: torch.Tensor,\n+        past_values_padding: torch.LongTensor,\n+        freq: torch.Tensor,\n+        output_attentions: bool = False,\n+        output_hidden_states: bool = False,\n+    ) -> TimesFmOutput:\n+        \"\"\"\n+        past_values_padding (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n+            The padding indicator of the time series.\n+        \"\"\"\n+        # Reshape into patches (using view for efficiency)\n+        bsize = past_values.shape[0]\n+        patched_inputs = past_values.view(bsize, -1, self.config.patch_length)\n+        patched_pads = past_values_padding.view(bsize, -1, self.config.patch_length)\n+\n+        patched_inputs = torch.where(\n+            torch.abs(patched_pads - 1.0) < self.config.tolerance,\n+            torch.tensor(0.0, dtype=patched_inputs.dtype, device=patched_inputs.device),\n+            patched_inputs,\n+        )\n+        patched_pads = torch.where(\n+            torch.abs(patched_inputs - self.config.pad_val) < self.config.tolerance,\n+            torch.tensor(1.0, dtype=patched_pads.dtype, device=patched_pads.device),\n+            patched_pads,\n+        )\n+        patched_inputs, stats = self._forward_transform(patched_inputs, patched_pads)\n+\n+        # B x N x D\n+        patched_inputs = patched_inputs * (1.0 - patched_pads)\n+        concat_inputs = torch.cat([patched_inputs, patched_pads], dim=-1)\n+        model_input = self.input_ff_layer(concat_inputs)\n+\n+        # A patch should not be padded even if there is at least one zero.\n+        patched_padding = torch.min(patched_pads, dim=-1)[0]  # Get the values from the min result\n+        if self.config.use_positional_embedding:\n+            pos_emb = self.position_emb(model_input.shape[1])\n+            pos_emb = torch.concat([pos_emb] * model_input.shape[0], dim=0)\n+            pos_emb = self._timesfm_shift_padded_seq(patched_padding, pos_emb)\n+            model_input += pos_emb\n+\n+        f_emb = self.freq_emb(freq)  # B x 1 x D\n+        model_input += f_emb\n+\n+        # Convert paddings to attention mask and combine with causal mask\n+        hidden_states = model_input\n+        attention_mask = self._prepare_4d_attention_mask(\n+            attention_mask=patched_padding,\n+            sequence_length=hidden_states.shape[1],\n+            dtype=hidden_states.dtype,\n+            device=hidden_states.device,\n+            is_causal=True,\n+        )\n+\n+        all_attentions = []\n+        all_hidden_states = []\n+\n+        for layer in self.layers[: self.config.num_hidden_layers]:\n+            scores, hidden_states = layer(\n+                hidden_states=hidden_states,\n+                attention_mask=attention_mask,\n+                paddings=patched_padding,\n+                output_attentions=output_attentions,\n+            )\n+            if output_attentions:\n+                all_attentions.append(scores)\n+            if output_hidden_states:\n+                all_hidden_states.append(hidden_states)\n+\n+        if output_hidden_states:\n+            all_hidden_states = [model_input] + all_hidden_states\n+        else:\n+            all_hidden_states = None\n+\n+        return TimesFmOutput(\n+            last_hidden_state=hidden_states,\n+            hidden_states=all_hidden_states,\n+            attentions=all_attentions if output_attentions else None,\n+            loc=stats[0],\n+            scale=stats[1],\n+        )\n+\n+    @staticmethod\n+    def _prepare_4d_attention_mask(\n+        attention_mask: Optional[torch.Tensor],\n+        sequence_length: int,\n+        dtype: torch.dtype,\n+        device: torch.device,\n+        is_causal: bool = True,\n+    ) -> Optional[torch.Tensor]:\n+        \"\"\"\n+        Creates 4D attention mask and combines causal and padding masks if needed.\n+\n+        Args:\n+            attention_mask: Optional tensor of shape (batch_size, seq_length) containing padding mask\n+            sequence_length: Length of the sequence\n+            dtype: Data type of the mask\n+            device: Device of the mask\n+            is_causal: Whether to apply causal masking\n+\n+        Returns:\n+            4D attention mask of shape (batch_size, 1, seq_length, seq_length)\n+        \"\"\"\n+        # Get minimum value for the dtype\n+        min_value = torch.finfo(dtype).min if dtype.is_floating_point else torch.iinfo(dtype).min\n+\n+        # Handle padding mask\n+        if attention_mask is not None:\n+            # Convert 2D padding mask to 4D attention mask\n+            attention_mask = attention_mask.view(attention_mask.shape[0], 1, 1, -1)\n+            attention_mask = attention_mask * min_value\n+\n+        # Create causal mask if needed\n+        if is_causal:\n+            causal_mask = torch.triu(\n+                torch.ones((sequence_length, sequence_length), dtype=dtype, device=device) * min_value,\n+                diagonal=1,\n+            )\n+            causal_mask = causal_mask.view(1, 1, sequence_length, sequence_length)\n+\n+            # Combine with padding mask if it exists\n+            if attention_mask is not None:\n+                attention_mask = torch.minimum(attention_mask, causal_mask)\n+            else:\n+                attention_mask = causal_mask\n+\n+        return attention_mask\n+\n+    @staticmethod\n+    def _timesfm_masked_mean_std(inputs: torch.Tensor, padding: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n+        \"\"\"Calculates mean and standard deviation of `inputs` across axis 1.\n+\n+        It excludes values where `padding` is 1.\n+\n+        Args:\n+            inputs: A PyTorch tensor of shape [b, n, p].\n+            padding: A PyTorch tensor of shape [b, n, p] with values 0 or 1.\n+\n+        Returns:\n+            A tuple containing the mean and standard deviation.\n+            We return the statistics of the first patch with more than three non-padded values.\n+        \"\"\"\n+\n+        # Selecting the first patch with more than 3 unpadded values.\n+        def _get_patch_index(arr: torch.Tensor):\n+            indices = torch.argmax((arr >= 3).to(torch.int32), dim=1)\n+            row_sum = (arr >= 3).to(torch.int32).sum(dim=1)\n+            return torch.where(row_sum == 0, arr.shape[1] - 1, indices)\n+\n+        pad_sum = torch.sum(1 - padding, dim=2)\n+        patch_indices = _get_patch_index(pad_sum)\n+        bidxs = torch.arange(inputs.shape[0])\n+\n+        arr = inputs[bidxs, patch_indices, :]\n+        pad = padding[bidxs, patch_indices, :]\n+\n+        # Create a mask where padding is 0\n+        mask = 1 - pad\n+\n+        # Calculate the number of valid elements\n+        num_valid_elements = torch.sum(mask, dim=1)\n+        num_valid_elements = torch.where(\n+            num_valid_elements == 0,\n+            torch.tensor(1, dtype=num_valid_elements.dtype, device=num_valid_elements.device),\n+            num_valid_elements,\n+        )\n+\n+        # Calculate the masked sum and squared sum\n+        masked_sum = torch.sum(arr * mask, dim=1)\n+        masked_squared_sum = torch.sum((arr * mask) ** 2, dim=1)\n+\n+        # Calculate the masked mean and standard deviation\n+        masked_mean = masked_sum / num_valid_elements\n+        masked_var = masked_squared_sum / num_valid_elements - masked_mean**2\n+        masked_var = torch.where(\n+            masked_var < 0.0,\n+            torch.tensor(0.0, dtype=masked_var.dtype, device=masked_var.device),\n+            masked_var,\n+        )\n+        masked_std = torch.sqrt(masked_var)\n+\n+        return masked_mean, masked_std\n+\n+    @staticmethod\n+    def _timesfm_shift_padded_seq(mask: torch.Tensor, seq: torch.Tensor) -> torch.Tensor:\n+        \"\"\"Shifts rows of seq based on the first 0 in each row of the mask.\n+\n+        Args:\n+            mask: mask tensor of shape [B, N]\n+            seq: seq tensor of shape [B, N, P]\n+\n+        Returns:\n+            The shifted sequence.\n+        \"\"\"\n+        batch_size, num_seq, feature_dim = seq.shape\n+\n+        new_mask: torch.BoolTensor = mask == 0\n+\n+        # Use argmax to find the first True value in each row\n+        indices = new_mask.to(torch.int32).argmax(dim=1)\n+\n+        # Handle rows with all zeros\n+        indices[~new_mask.any(dim=1)] = -1\n+\n+        # Create index ranges for each sequence in the batch\n+        idx_range = torch.arange(num_seq, device=seq.device).view(1, -1, 1).expand(batch_size, -1, feature_dim)\n+\n+        # Calculate shifted indices for each element in each sequence\n+        shifted_idx = (idx_range - indices[:, None, None]) % num_seq\n+\n+        # Gather values from seq using shifted indices\n+        shifted_seq = seq.gather(1, shifted_idx)\n+\n+        return shifted_seq\n+\n+\n+class TimesFmModelForPrediction(TimesFmPreTrainedModel):\n+    \"\"\"TimesFM model for quantile and mean prediction.\"\"\"\n+\n+    def __init__(self, config: TimesFmConfig):\n+        super().__init__(config)\n+\n+        self.config = config\n+        self.context_len = config.context_length\n+        self.horizon_len = config.horizon_length\n+\n+        self.decoder = TimesFmModel(config)\n+\n+        # quantile and mean output\n+        self.horizon_ff_layer = TimesFmResidualBlock(\n+            input_dims=config.hidden_size,\n+            output_dims=config.horizon_length * (1 + len(config.quantiles)),\n+            hidden_dims=config.intermediate_size,\n+        )\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def _preprocess(\n+        self, inputs: Sequence[torch.Tensor], freq: Sequence[int]\n+    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n+        \"\"\"Formats and pads raw inputs to feed into the model.\n+\n+        This function both pads each time series to match the context length, and\n+        pads the inputs to meet the SPMD shape requirement.\n+\n+        Args:\n+          inputs: A list of 1d Tensors. Each Tensor is the context time series of\n+            a single forecast task.\n+          freq: list of frequencies\n+\n+        Returns:\n+        A tuple of:\n+        - the padded input time series to meet the model required context.\n+        - the padding indicator.\n+        - the number of padded examples for SPMD so that each core has the same\n+            number (a multiple of `batch_size`) of examples.\n+        \"\"\"\n+        input_ts, input_padding, inp_freq = [], [], []\n+\n+        for i, ts in enumerate(inputs):\n+            input_len = ts.shape[0]\n+            padding = torch.zeros(input_len + self.horizon_len, dtype=ts.dtype, device=ts.device)\n+            if input_len < self.context_len:\n+                num_front_pad = self.context_len - input_len\n+                ts = torch.cat([torch.zeros(num_front_pad, dtype=ts.dtype, device=ts.device), ts], dim=0)\n+                padding = torch.cat([torch.ones(num_front_pad, dtype=ts.dtype, device=padding.device), padding], dim=0)\n+            elif input_len > self.context_len:\n+                ts = ts[-self.context_len :]\n+                padding = padding[-(self.context_len + self.horizon_len) :]\n+\n+            input_ts.append(ts)\n+            input_padding.append(padding)\n+            inp_freq.append(freq[i])\n+\n+        return (\n+            torch.stack(input_ts, dim=0),\n+            torch.stack(input_padding, dim=0),\n+            torch.tensor(inp_freq, dtype=torch.int32).reshape(-1, 1),\n+        )\n+\n+    def _postprocess_output(\n+        self, model_output: torch.Tensor, stats: tuple[torch.Tensor, torch.Tensor]\n+    ) -> torch.Tensor:\n+        \"\"\"Postprocess output of stacked transformer.\"\"\"\n+\n+        # B x N x (H.Q)\n+        output_ts = self.horizon_ff_layer(model_output)\n+\n+        # Reshape using view\n+        b, n, _ = output_ts.shape\n+        output_ts = output_ts.view(b, n, self.config.horizon_length, len(self.config.quantiles) + 1)\n+\n+        mu, sigma = stats\n+        return output_ts * sigma[:, None, None, None] + mu[:, None, None, None]\n+\n+    def _quantile_loss(self, predictions: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n+        losses = []\n+        for i, q in enumerate(self.config.quantiles):\n+            errors = targets - predictions[..., i]\n+            loss = torch.max((q - 1) * errors, q * errors)\n+            losses.append(loss.mean())\n+        return torch.stack(losses).mean()\n+\n+    @can_return_tuple\n+    @add_start_docstrings_to_model_forward(TIMESFM_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=TimesFmOutputForPrediction, config_class=_CONFIG_FOR_DOC)\n+    @add_code_sample_docstrings(\n+        checkpoint=_CHECKPOINT_FOR_DOC,\n+        output_type=TimesFmOutputForPrediction,\n+        config_class=_CONFIG_FOR_DOC,\n+    )\n+    def forward(\n+        self,\n+        past_values: Sequence[torch.Tensor],\n+        freq: Optional[Sequence[Union[torch.Tensor, int]]] = None,\n+        window_size: Optional[int] = None,\n+        future_values: Optional[torch.Tensor] = None,\n+        forecast_context_len: Optional[int] = None,\n+        return_forecast_on_context: bool = False,\n+        truncate_negative: bool = False,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+    ) -> TimesFmOutputForPrediction:\n+        r\"\"\"\n+        window_size (`int`, *optional*):\n+            Window size of trend + residual decomposition. If None then we do not do decomposition.\n+        future_values (`torch.Tensor`, *optional*):\n+            Optional future time series values to be used for loss computation.\n+        forecast_context_len (`int`, *optional*):\n+            Optional max context length.\n+        return_forecast_on_context (`bool`, *optional*):\n+            True to return the forecast on the context when available, i.e. after the first input patch.\n+        truncate_negative (`bool`, *optional*):\n+            Truncate to only non-negative values if any of the contexts have non-negative values,\n+            otherwise do nothing.\n+        output_attentions (`bool`, *optional*):\n+            Whether to output the attentions.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether to output the hidden states.\n+\n+        Returns:\n+            A TimesFmOutputForPrediction object or a tuple containing:\n+                - the mean forecast of size (# past_values, # forecast horizon),\n+                - the full forecast (mean + quantiles) of size\n+                    (# past_values,  # forecast horizon, 1 + # quantiles).\n+                - loss: the mean squared error loss + quantile loss if `future_values` is provided.\n+        \"\"\"\n+        if forecast_context_len is None:\n+            fcontext_len = self.context_len\n+        else:\n+            fcontext_len = forecast_context_len\n+\n+        # Get device from first input tensor\n+        device = past_values[0].device\n+\n+        # Truncate inputs to forecast_context_len\n+        inputs = [ts[-fcontext_len:] for ts in past_values]\n+        inp_min = torch.min(torch.stack([torch.min(ts) for ts in inputs]))\n+\n+        if window_size is not None:\n+            new_inputs = []\n+            new_freqs = []\n+            for i, ts in enumerate(inputs):\n+                new_inputs.extend(self._timesfm_moving_average(ts, window_size))\n+                if freq is not None:\n+                    new_freqs.extend([freq[i]] * 2)\n+            inputs = new_inputs\n+            if freq is not None:\n+                freq = new_freqs\n+\n+        if freq is None:\n+            logger.info(\"No frequency provided via `freq`. Default to high (0).\")\n+            freq = [0] * len(inputs)\n+\n+        if output_attentions is None:\n+            output_attentions = self.config.output_attentions\n+        if output_hidden_states is None:\n+            output_hidden_states = self.config.output_hidden_states\n+\n+        input_ts, input_padding, inp_freq = self._preprocess(inputs, freq)\n+        # Move tensors to the same device as input\n+        input_ts = input_ts.to(device)\n+        input_padding = input_padding.to(device)\n+        inp_freq = inp_freq.to(device)\n+\n+        final_out = input_ts\n+        context_len = final_out.shape[1]\n+        full_outputs = []\n+\n+        if input_padding.shape[1] != final_out.shape[1] + self.horizon_len:\n+            raise ValueError(\n+                \"Length of paddings must match length of input + horizon_len:\"\n+                f\" {input_padding.shape[1]} != {final_out.shape[1]} + {self.horizon_len}\"\n+            )\n+        output_patch_len = self.config.horizon_length\n+\n+        num_decode_patches = (self.horizon_len + output_patch_len - 1) // output_patch_len\n+        for step_index in range(num_decode_patches):\n+            current_padding = input_padding[:, 0 : final_out.shape[1]]\n+            input_ts = final_out[:, -fcontext_len:]\n+            input_padding = current_padding[:, -fcontext_len:]\n+            decoder_output = self.decoder(\n+                past_values=input_ts,\n+                past_values_padding=input_padding,\n+                freq=inp_freq,\n+                output_attentions=output_attentions,\n+                output_hidden_states=output_hidden_states,\n+            )\n+            fprop_outputs = self._postprocess_output(\n+                decoder_output.last_hidden_state,\n+                (decoder_output.loc, decoder_output.scale),\n+            )\n+\n+            if return_forecast_on_context and step_index == 0:\n+                # For the first decodings step, collect the model forecast on the\n+                # context except the unavailable first input batch forecast.\n+                new_full_ts = fprop_outputs[:, :-1, : self.config.patch_length, :]\n+                # We have to use reshape and not view for non-contiguous memory\n+                new_full_ts = new_full_ts.reshape(new_full_ts.size(0), -1, new_full_ts.size(3))\n+\n+                full_outputs.append(new_full_ts)\n+\n+            # (full batch, last patch, output_patch_len, index of mean forecast = 0)\n+            new_ts = fprop_outputs[:, -1, :output_patch_len, 0]\n+            new_full_ts = fprop_outputs[:, -1, :output_patch_len, :]\n+            # (full batch, last patch, output_patch_len, all output indices)\n+            full_outputs.append(new_full_ts)\n+            final_out = torch.concatenate([final_out, new_ts], axis=-1)\n+\n+        if return_forecast_on_context:\n+            # `full_outputs` indexing starts at after the first input patch.\n+            full_outputs = torch.concatenate(full_outputs, axis=1)[\n+                :, : (context_len - self.config.patch_length + self.horizon_len), :\n+            ]\n+        else:\n+            # `full_outputs` indexing starts at the forecast horizon.\n+            full_outputs = torch.concatenate(full_outputs, axis=1)[:, 0 : self.horizon_len, :]\n+\n+        mean_outputs = full_outputs[:, :, 0]\n+        if window_size is not None:\n+            mean_outputs = mean_outputs[0::2, ...] + mean_outputs[1::2, ...]\n+            full_outputs = full_outputs[0::2, ...] + full_outputs[1::2, ...]\n+        if inp_min >= 0 and truncate_negative:\n+            mean_outputs = torch.maximum(mean_outputs, 0.0)\n+            full_outputs = torch.maximum(full_outputs, 0.0)\n+\n+        loss = None\n+        if future_values is not None:\n+            mse_loss = F.mse_loss(mean_outputs, future_values)\n+            quantile_loss = self._quantile_loss(full_outputs[:, :, 1:], future_values)\n+            loss = mse_loss + quantile_loss\n+\n+        return TimesFmOutputForPrediction(\n+            last_hidden_state=decoder_output.last_hidden_state,\n+            attentions=decoder_output.attentions if output_attentions else None,\n+            hidden_states=decoder_output.hidden_states if output_hidden_states else None,\n+            mean_predictions=mean_outputs,\n+            full_predictions=full_outputs,\n+            loss=loss,\n+        )\n+\n+    @staticmethod\n+    def _timesfm_moving_average(arr: torch.Tensor, window_size: int) -> list[torch.Tensor]:\n+        \"\"\"Calculates the moving average using PyTorch's convolution function.\"\"\"\n+        # Pad with zeros to handle initial window positions\n+        arr_padded = F.pad(arr, (window_size - 1, 0), \"constant\", 0)\n+        # Create a convolution kernel\n+        kernel = torch.ones(window_size, dtype=arr.dtype, device=arr.device) / window_size\n+        # Apply convolution to calculate the moving average\n+        smoothed_arr = F.conv1d(arr_padded.view(1, 1, -1), kernel.view(1, 1, -1)).squeeze()\n+        return [smoothed_arr, arr - smoothed_arr]\n+\n+\n+__all__ = [\"TimesFmModelForPrediction\", \"TimesFmPreTrainedModel\", \"TimesFmModel\"]"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/timesfm/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/a91020aed0b15794d0842e5799ec9d360e939f4e/tests%2Fmodels%2Ftimesfm%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a91020aed0b15794d0842e5799ec9d360e939f4e/tests%2Fmodels%2Ftimesfm%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftimesfm%2F__init__.py?ref=a91020aed0b15794d0842e5799ec9d360e939f4e"
        },
        {
            "sha": "6d69a97d352035ba7db57695c53f55b75caf5095",
            "filename": "tests/models/timesfm/test_modeling_timesfm.py",
            "status": "added",
            "additions": 197,
            "deletions": 0,
            "changes": 197,
            "blob_url": "https://github.com/huggingface/transformers/blob/a91020aed0b15794d0842e5799ec9d360e939f4e/tests%2Fmodels%2Ftimesfm%2Ftest_modeling_timesfm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a91020aed0b15794d0842e5799ec9d360e939f4e/tests%2Fmodels%2Ftimesfm%2Ftest_modeling_timesfm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftimesfm%2Ftest_modeling_timesfm.py?ref=a91020aed0b15794d0842e5799ec9d360e939f4e",
            "patch": "@@ -0,0 +1,197 @@\n+# coding=utf-8\n+# Copyright 2025 Google LLC and HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import inspect\n+import unittest\n+from typing import List\n+\n+import numpy as np\n+import torch\n+\n+from transformers import TimesFmConfig, is_torch_available\n+from transformers.testing_utils import require_torch, slow, torch_device\n+from transformers.utils import is_torch_fx_available\n+\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import ModelTesterMixin\n+\n+\n+if is_torch_fx_available():\n+    pass\n+\n+if is_torch_available():\n+    from transformers import TimesFmModelForPrediction\n+\n+TOLERANCE = 1e-4\n+\n+\n+class TimesFmModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        patch_length: int = 32,\n+        context_length: int = 512,\n+        horizon_length: int = 128,\n+        freq_size: int = 3,\n+        num_hidden_layers: int = 1,\n+        hidden_size: int = 16,\n+        intermediate_size: int = 32,\n+        head_dim: int = 8,\n+        num_heads: int = 2,\n+        tolerance: float = 1e-6,\n+        rms_norm_eps: float = 1e-6,\n+        quantiles: List[float] = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n+        pad_val: float = 1123581321.0,\n+        use_positional_embedding: bool = True,\n+        initializer_factor: float = 0.0,\n+        is_training: bool = False,\n+        batch_size: int = 3,\n+    ):\n+        self.parent = parent\n+        self.patch_length = patch_length\n+        self.context_length = context_length\n+        self.horizon_length = horizon_length\n+        self.quantiles = quantiles\n+        self.pad_val = pad_val\n+        self.freq_size = freq_size\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.head_dim = head_dim\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_heads\n+        self.tolerance = tolerance\n+        self.rms_norm_eps = rms_norm_eps\n+        self.use_positional_embedding = use_positional_embedding\n+        self.initializer_factor = initializer_factor\n+        self.is_training = is_training\n+        self.batch_size = batch_size\n+\n+        # The size of test input\n+        self.seq_length = context_length // patch_length\n+        self.hidden_size = hidden_size\n+\n+    def get_config(self):\n+        return TimesFmConfig(\n+            patch_length=self.patch_length,\n+            context_length=self.context_length,\n+            horizon_length=self.horizon_length,\n+            quantiles=self.quantiles,\n+            pad_val=self.pad_val,\n+            freq_size=self.freq_size,\n+            hidden_size=self.hidden_size,\n+            intermediate_size=self.intermediate_size,\n+            head_dim=self.head_dim,\n+            num_hidden_layers=self.num_hidden_layers,\n+            num_attention_heads=self.num_attention_heads,\n+            tolerance=self.tolerance,\n+            rms_norm_eps=self.rms_norm_eps,\n+            use_positional_embedding=self.use_positional_embedding,\n+            initializer_factor=self.initializer_factor,\n+        )\n+\n+    def get_pipeline_config(self):\n+        return self.get_config()\n+\n+    def prepare_config_and_inputs(self):\n+        forecast_input = [\n+            torch.tensor(np.sin(np.linspace(0, 20, 100)), dtype=torch.float32, device=torch_device),\n+            torch.tensor(np.cos(np.linspace(0, 20, 100)), dtype=torch.float32, device=torch_device),\n+            torch.tensor(np.tan(np.linspace(0, 20, 100)), dtype=torch.float32, device=torch_device),\n+        ]\n+        frequency_input = torch.tensor([0, 1, 2], dtype=torch.long, device=torch_device)\n+\n+        return (self.get_config(), torch.stack(forecast_input, dim=0), frequency_input)\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        (config, forecast_input, frequency_input) = self.prepare_config_and_inputs()\n+\n+        inputs_dict = {\n+            \"past_values\": forecast_input,\n+            \"freq\": frequency_input,\n+        }\n+        return config, inputs_dict\n+\n+\n+@require_torch\n+class TimesFmModelTest(ModelTesterMixin, unittest.TestCase):\n+    all_model_classes = (TimesFmModelForPrediction,) if is_torch_available() else ()\n+    all_generative_model_classes = ()\n+    all_parallelizable_model_classes = ()\n+    fx_compatible = False\n+    test_pruning = False\n+    test_resize_embeddings = False\n+    test_model_parallel = False\n+    is_encoder_decoder = False\n+    test_inputs_embeds = False\n+\n+    def setUp(self):\n+        self.model_tester = TimesFmModelTester(self)\n+        self.config_tester = ConfigTester(self, config_class=TimesFmConfig)\n+\n+    def test_create_and_run_model(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        model = TimesFmModelForPrediction(config)\n+        model.to(torch_device)\n+        model.eval()\n+        results = model(**inputs_dict)\n+        assert results.mean_predictions is not None\n+\n+    @unittest.skip(reason=\"Compile not yet supported because of masks\")\n+    def test_sdpa_can_dispatch_on_flash(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Model does not have input embeddings\")\n+    def test_model_get_set_embeddings(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Model does not have head mask\")\n+    def test_headmasking(self):\n+        pass\n+\n+    # the main input name is `inputs`\n+    def test_model_main_input_name(self):\n+        model_signature = inspect.signature(getattr(TimesFmModelForPrediction, \"forward\"))\n+        # The main input is the name of the argument after `self`\n+        observed_main_input_name = list(model_signature.parameters.keys())[1]\n+        self.assertEqual(TimesFmModelForPrediction.main_input_name, observed_main_input_name)\n+\n+\n+@require_torch\n+@slow\n+class TimesFmModelIntegrationTests(unittest.TestCase):\n+    def test_inference_no_head(self):\n+        model = TimesFmModelForPrediction.from_pretrained(\"google/timesfm-2.0-500m-pytorch\", revision=\"refs/pr/7\").to(\n+            torch_device\n+        )\n+        forecast_input = [\n+            np.sin(np.linspace(0, 20, 100)),\n+            np.sin(np.linspace(0, 20, 200)),\n+            np.sin(np.linspace(0, 20, 400)),\n+        ]\n+        forecast_input_tensor = [torch.tensor(ts, dtype=torch.float32, device=torch_device) for ts in forecast_input]\n+        frequency_input = [0, 1, 2]\n+\n+        with torch.no_grad():\n+            output = model(past_values=forecast_input_tensor, freq=frequency_input).last_hidden_state\n+\n+        self.assertEqual(\n+            output.shape,\n+            torch.Size([3, model.config.context_length // model.config.patch_length, model.config.hidden_size]),\n+        )\n+        expected_slice = torch.tensor(\n+            [[-0.4267, -0.7273, -0.3932], [-0.4267, -0.7273, -0.3932], [-0.4267, -0.7273, -0.3932]],\n+            device=torch_device,\n+        )\n+        self.assertTrue(torch.allclose(output[0, :3, :3], expected_slice, atol=TOLERANCE))"
        },
        {
            "sha": "85178b663e450fe3cdce24542dc8db99c5cc308d",
            "filename": "utils/check_repo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a91020aed0b15794d0842e5799ec9d360e939f4e/utils%2Fcheck_repo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a91020aed0b15794d0842e5799ec9d360e939f4e/utils%2Fcheck_repo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_repo.py?ref=a91020aed0b15794d0842e5799ec9d360e939f4e",
            "patch": "@@ -156,6 +156,7 @@\n         \"Llama4VisionModel\",  # Building part of bigger (tested) model. # TODO: add tests\n         \"Emu3VQVAE\",  # Building part of bigger (tested) model\n         \"Emu3TextModel\",  # Building part of bigger (tested) model\n+        \"TimesFmModel\",  # Building part of bigger (tested) model\n     ]\n )\n "
        }
    ],
    "stats": {
        "total": 2507,
        "additions": 2507,
        "deletions": 0
    }
}