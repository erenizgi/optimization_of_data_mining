{
    "author": "jiqing-feng",
    "message": "Remove ipex/ccl in CPU training doc (#42866)\n\n* rm ipex and ccl on cpu training doc\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* fix format\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* Update docs/source/en/perf_train_cpu_many.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n---------\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "f404f150aab0403359c85b59ea9a8c792f0cba4c",
    "files": [
        {
            "sha": "524815ee3af7c4a76817110a79e51e1402edb416",
            "filename": "docs/source/en/perf_train_cpu_many.md",
            "status": "modified",
            "additions": 7,
            "deletions": 43,
            "changes": 50,
            "blob_url": "https://github.com/huggingface/transformers/blob/f404f150aab0403359c85b59ea9a8c792f0cba4c/docs%2Fsource%2Fen%2Fperf_train_cpu_many.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f404f150aab0403359c85b59ea9a8c792f0cba4c/docs%2Fsource%2Fen%2Fperf_train_cpu_many.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_train_cpu_many.md?ref=f404f150aab0403359c85b59ea9a8c792f0cba4c",
            "patch": "@@ -19,48 +19,21 @@ CPUs are commonly available and can be a cost-effective training option when GPU\n \n This guide demonstrates how to perform distributed training with multiple CPUs using a [DistributedDataParallel (DDP)](./perf_train_gpu_many#distributeddataparallel) strategy on bare metal with [`Trainer`] and a Kubernetes cluster. All examples shown in this guide depend on the [Intel oneAPI HPC Toolkit](https://www.intel.com/content/www/us/en/developer/tools/oneapi/hpc-toolkit.html).\n \n-There are two toolkits you'll need from Intel oneAPI.\n-\n-1. [oneCCL](https://www.intel.com/content/www/us/en/developer/tools/oneapi/oneccl.html) includes efficient implementations of collectives commonly used in deep learning such as all-gather, all-reduce, and reduce-scatter. To install from a prebuilt wheel, make sure you always use the latest release. Refer to the table [here](https://github.com/intel/torch-ccl#install-prebuilt-wheel) to check if a version of oneCCL is supported for a Python and PyTorch version.\n-\n-```bash\n-# installs oneCCL for PyTorch 2.4.0\n-pip install oneccl_bind_pt==2.4.0 -f https://developer.intel.com/ipex-whl-stable-cpu\n-```\n-\n-> [!TIP]\n-> Refer to the oneCCL [installation](https://github.com/intel/torch-ccl#installation) for more details.\n-\n-1. [MPI](https://www.intel.com/content/www/us/en/developer/tools/oneapi/mpi-library.html) is a message-passing interface for communications between hardware and networks. The oneCCL toolkit is installed along with MPI, but you need to source the environment as shown below before using it.\n-\n-```bash\n-oneccl_bindings_for_pytorch_path=$(python -c \"from oneccl_bindings_for_pytorch import cwd; print(cwd)\")\n-source $oneccl_bindings_for_pytorch_path/env/setvars.sh\n-```\n-\n-Lastly, install the [Intex Extension for PyTorch (IPEX)](https://intel.github.io/intel-extension-for-pytorch/index.html) which enables additional performance optimizations for Intel hardware such as weight sharing and better thread runtime control.\n-\n-```bash\n-pip install intel_extension_for_pytorch==<version_name> -f https://developer.intel.com/ipex-whl-stable-cpu\n-```\n-\n-> [!TIP]\n-> Refer to the IPEX [installation](https://intel.github.io/intel-extension-for-pytorch/index.html#installation) for more details.\n+The only toolkit you'll need from Intel oneAPI is [MPI](https://www.intel.com/content/www/us/en/developer/tools/oneapi/mpi-library.html), a message-passing interface for communications between hardware and networks.\n \n ## Trainer\n \n-[`Trainer`] supports distributed training with CPUs with the oneCCL backend. Add the `--ddp_backend ccl` parameter in the command arguments to enable it.\n+[`Trainer`] supports distributed training with CPUs.\n \n <hfoptions id=\"distrib-cpu\">\n <hfoption id=\"single node\">\n \n The example below demonstrates the [run_qa.py](https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering) script. It enables training with two processes on one Xeon CPU, with one process running per socket.\n \n > [!TIP]\n-> Tune the variable `OMP_NUM_THREADS/CCL_WORKER_COUNT` for optimal performance.\n+> Tune the variable `OMP_NUM_THREADS` for optimal performance.\n \n ```bash\n-export CCL_WORKER_COUNT=1\n export MASTER_ADDR=127.0.0.1\n mpirun -n 2 -genv OMP_NUM_THREADS=23 \\\n python3 run_qa.py \\\n@@ -73,9 +46,7 @@ python3 run_qa.py \\\n  --num_train_epochs 2  \\\n  --max_seq_length 384 \\\n  --doc_stride 128  \\\n- --output_dir /tmp/debug_squad/ \\\n- --no_cuda \\\n- --ddp_backend ccl\n+ --output_dir /tmp/debug_squad/\n ```\n \n </hfoption>\n@@ -94,10 +65,9 @@ xxx.xxx.xxx.xxx #node1 ip\n Run the script below on `node0` to enable DDP on `node0` and `node1` and train with bf16 auto mixed precision.\n \n > [!TIP]\n-> Tune the variable `OMP_NUM_THREADS/CCL_WORKER_COUNT` for optimal performance.\n+> Tune the variable `OMP_NUM_THREADS` for optimal performance.\n \n ```bash\n-export CCL_WORKER_COUNT=1\n export MASTER_ADDR=xxx.xxx.xxx.xxx #node0 ip\n mpirun -f hostfile -n 4 -ppn 2 \\\n  -genv OMP_NUM_THREADS=23 \\\n@@ -112,8 +82,7 @@ python3 run_qa.py \\\n  --max_seq_length 384 \\\n  --doc_stride 128  \\\n  --output_dir /tmp/debug_squad/ \\\n- --no_cuda \\\n- --ddp_backend ccl \\\n+ --use_cpu \\\n  --bf16\n ```\n \n@@ -127,7 +96,7 @@ Distributed training with CPUs can also be deployed to a Kubernetes cluster with\n 1. Ensure you have access to a Kubernetes cluster with [Kubeflow](https://www.kubeflow.org/docs/started/installing-kubeflow/) installed.\n 1. Install and configure [kubectl](https://kubernetes.io/docs/tasks/tools) to interact with the cluster.\n 1. Set up a [PersistentVolumeClaim (PVC)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) to store datasets and model files. There are multiple options to choose from, including a [StorageClass](https://kubernetes.io/docs/concepts/storage/storage-classes/) or a cloud storage bucket.\n-1. Set up a Docker container for the training script and all required dependencies such as PyTorch, Transformers, IPEX, oneCCL, and OpenSSH to facilitate communicattion between containers.\n+1. Set up a Docker container for the training script and all required dependencies such as PyTorch, Transformers, and OpenSSH to facilitate communication between containers.\n \n The example Dockerfile below uses a base image that supports distributed training with CPUs, and extracts Transformers to the `/workspace` directory to include the training scripts in the image. The image needs to be built and copied to the clusters nodes or pushed to a container registry prior to deployment.\n \n@@ -185,7 +154,6 @@ spec:\n                 - >-\n                   cd /workspace/transformers;\n                   pip install -r /workspace/transformers/examples/pytorch/question-answering/requirements.txt;\n-                  source /usr/local/lib/python3.10/dist-packages/oneccl_bindings_for_pytorch/env/setvars.sh;\n                   torchrun /workspace/transformers/examples/pytorch/question-answering/run_qa.py \\\n                     --model_name_or_path distilbert/distilbert-base-uncased \\\n                     --dataset_name squad \\\n@@ -197,8 +165,6 @@ spec:\n                     --max_seq_length 384 \\\n                     --doc_stride 128 \\\n                     --output_dir /tmp/pvc-mount/output_$(date +%Y%m%d_%H%M%S) \\\n-                    --no_cuda \\\n-                    --ddp_backend ccl \\\n                     --bf16;\n               env:\n               - name: LD_PRELOAD\n@@ -209,8 +175,6 @@ spec:\n                 value: \"/tmp/pvc-mount/hf_datasets_cache\"\n               - name: LOGLEVEL\n                 value: \"INFO\"\n-              - name: CCL_WORKER_COUNT\n-                value: \"1\"\n               - name: OMP_NUM_THREADS  # Can be tuned for optimal performance\n                 value: \"240\"\n               resources:"
        },
        {
            "sha": "d73f13128e06cb4dc44de2f9826cdb70d89a3e55",
            "filename": "src/transformers/training_args.py",
            "status": "modified",
            "additions": 8,
            "deletions": 10,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/f404f150aab0403359c85b59ea9a8c792f0cba4c/src%2Ftransformers%2Ftraining_args.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f404f150aab0403359c85b59ea9a8c792f0cba4c/src%2Ftransformers%2Ftraining_args.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftraining_args.py?ref=f404f150aab0403359c85b59ea9a8c792f0cba4c",
            "patch": "@@ -1530,16 +1530,14 @@ def __post_init__(self):\n             self.greater_is_better = not self.metric_for_best_model.endswith(\"loss\")\n         if is_torch_available():\n             if self.bf16 or self.bf16_full_eval:\n-                if self.use_cpu and not is_torch_xla_available():\n-                    # cpu\n-                    raise ValueError(\"Your setup doesn't support bf16/(cpu, tpu, neuroncore). You need torch>=1.10\")\n-                elif not self.use_cpu:\n-                    if not is_torch_bf16_gpu_available() and not is_torch_xla_available():  # added for tpu support\n-                        error_message = \"Your setup doesn't support bf16/gpu.\"\n-                        if is_torch_cuda_available():\n-                            error_message += \" You need Ampere+ GPU with cuda>=11.0\"\n-                        # gpu\n-                        raise ValueError(error_message)\n+                if (\n+                    not self.use_cpu and not is_torch_bf16_gpu_available() and not is_torch_xla_available()\n+                ):  # added for tpu support\n+                    error_message = \"Your setup doesn't support bf16/gpu. You need to assign use_cpu if you want to train the model on CPU\"\n+                    if is_torch_cuda_available():\n+                        error_message += \" You need Ampere+ GPU with cuda>=11.0\"\n+                    # gpu\n+                    raise ValueError(error_message)\n \n         if self.fp16 and self.bf16:\n             raise ValueError(\"At most one of fp16 and bf16 can be True, but not both\")"
        }
    ],
    "stats": {
        "total": 68,
        "additions": 15,
        "deletions": 53
    }
}