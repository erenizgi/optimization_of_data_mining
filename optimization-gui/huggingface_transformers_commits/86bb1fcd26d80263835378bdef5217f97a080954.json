{
    "author": "zucchini-nlp",
    "message": "Revert FA2 kwargs construction (#40029)\n\n* revert\n\n* use imports\n\n* went way too high in imports level\n\n* style",
    "sha": "86bb1fcd26d80263835378bdef5217f97a080954",
    "files": [
        {
            "sha": "30dcd896dcd6cd5d9cf7b4f8b6c6db8e2cb6a51c",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/86bb1fcd26d80263835378bdef5217f97a080954/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/86bb1fcd26d80263835378bdef5217f97a080954/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=86bb1fcd26d80263835378bdef5217f97a080954",
            "patch": "@@ -46,6 +46,7 @@\n from ..integrations.deepspeed import is_deepspeed_zero3_enabled\n from ..integrations.fsdp import is_fsdp_managed_module\n from ..masking_utils import create_masks_for_generate\n+from ..modeling_flash_attention_utils import prepare_fa_kwargs_from_position_ids\n from ..modeling_outputs import CausalLMOutputWithPast, Seq2SeqLMOutput\n from ..pytorch_utils import isin_mps_friendly\n from ..tokenization_utils import ExtensionsTrie\n@@ -677,6 +678,17 @@ def prepare_inputs_for_generation(\n         if encoder_attention_mask is not None:\n             model_inputs[\"attention_mask\"] = encoder_attention_mask\n \n+        if \"flash\" in self.config._attn_implementation and self._supports_attention_backend:\n+            cu_seq_lens_q, cu_seq_lens_k, max_length_q, max_length_k = prepare_fa_kwargs_from_position_ids(\n+                position_ids, is_packed_sequence=False\n+            )\n+            model_inputs.update(\n+                cu_seq_lens_q=cu_seq_lens_q.to(self.device),\n+                cu_seq_lens_k=cu_seq_lens_k.to(self.device),\n+                max_length_q=max_length_q,\n+                max_length_k=max_length_k,\n+            )\n+\n         # 7. Forward ALL kwargs that are uninitialized (e.g. `use_cache`).\n         for key, value in kwargs.items():\n             if key not in model_inputs:"
        },
        {
            "sha": "e845e0cbc4a408490543003cd7b0d29c53b1431b",
            "filename": "src/transformers/modeling_flash_attention_utils.py",
            "status": "modified",
            "additions": 57,
            "deletions": 31,
            "changes": 88,
            "blob_url": "https://github.com/huggingface/transformers/blob/86bb1fcd26d80263835378bdef5217f97a080954/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/86bb1fcd26d80263835378bdef5217f97a080954/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_flash_attention_utils.py?ref=86bb1fcd26d80263835378bdef5217f97a080954",
            "patch": "@@ -190,46 +190,28 @@ def _upad_input(\n     )\n \n \n-def _prepare_from_posids(query, key, value, position_ids, query_length):\n+def prepare_fa_kwargs_from_position_ids(position_ids, is_packed_sequence: bool = True):\n     \"\"\"\n-    This function returns necessary arguments to call `flash_attn_varlen_func`.\n-    All three query, key, value states will be flattened.\n-    Cumulative lengths of each examples in the batch will be extracted from position_ids.\n-    NOTE: ideally cumulative lengths should be prepared at the data collator stage\n+    This function returns all the necessary kwargs to call `flash_attn_varlen_func`\n+    extracted from position_ids.The `position_ids` can be either packed sequence or\n+    the usual padded position ids, for example in inference time..\n     Arguments:\n-        query (`torch.Tensor`):\n-            Query state with padding. Shape: (batch_size, query_length, num_heads, head_dim).\n-        key (`torch.Tensor`):\n-            Key state with padding. Shape: (batch_size, kv_seq_len, num_key_value_heads, head_dim).\n-        value (`torch.Tensor`):\n-            Value state with padding. Shape: (batch_size, kv_seq_len, num_key_value_heads, head_dim).\n         position_ids (`torch.Tensor`):\n             Boolean or int tensor of shape (batch_size, sequence_length), 1 means valid and 0 means not valid.\n-        query_length (`int`):\n-            Sequence length of the input queries.\n+        is_packed_sequence (`bool`, *optional*, defaults to `True`):\n+            Whether the input position ids are a packed sequence or not.\n     Return:\n-        query (`torch.Tensor`):\n-            Query state without padding. Shape: (total_target_length, num_heads, head_dim).\n-        key (`torch.Tensor`):\n-            Key state with padding. Shape: (total_source_length, num_key_value_heads, head_dim).\n-        value (`torch.Tensor`):\n-            Value state with padding. Shape: (total_source_length, num_key_value_heads, head_dim).\n         (cu_seqlens_q, cu_seqlens_k) (`tuple[int]`):\n-            The cumulative sequence lengths for the target (query) and source (key, value), used to index into ragged (unpadded) tensors. `cu_seqlens` shape is (batch_size + 1,).\n+            The cumulative sequence lengths for the target (query) and source (key, value), used to index into\n+            ragged (unpadded) tensors. `cu_seqlens` shape is (batch_size + 1,).\n         (max_seqlen_in_batch_q, max_seqlen_in_batch_k) (`tuple[int]`):\n-            Maximum sequence length in batch (`max_seqlen_in_batch_q` for the target sequence i.e. query, `max_seqlen_in_batch_k` for the source sequence i.e. key/value).\n+            Maximum sequence length in batch (`max_seqlen_in_batch_q` for the target sequence i.e. query,\n+            `max_seqlen_in_batch_k` for the source sequence i.e. key/value).\n     \"\"\"\n-    kv_length = key.shape[1]\n-    query = query.contiguous().view(-1, query.size(-2), query.size(-1))\n-    key = key.contiguous().view(-1, key.size(-2), key.size(-1))\n-    value = value.contiguous().view(-1, value.size(-2), value.size(-1))\n-\n     # If the lengths are not equal, most probably we are in decoding stage with cache\n     # In that case the position ids will not always start with `0` and we need a better way to infer\n     # cumulative seq lengths.\n-    if query_length != kv_length:\n-        indices_q = torch.arange(position_ids.size(0), device=position_ids.device, dtype=torch.int32)\n-\n+    if not is_packed_sequence:\n         tensor_kws = {\"dtype\": torch.int32, \"device\": position_ids.device}\n         last_position_ids = position_ids[:, -1]\n \n@@ -238,8 +220,9 @@ def _prepare_from_posids(query, key, value, position_ids, query_length):\n         )\n         max_length_k = int(last_position_ids.max()) + 1\n \n-        batch_size, seq_len = query.shape[:2]\n-        q_len = torch.ones(batch_size, **tensor_kws) if query_length == 1 else last_position_ids.add(1)\n+        q_len = (\n+            torch.ones(position_ids.size(0), **tensor_kws) if position_ids.shape[-1] == 1 else last_position_ids.add(1)\n+        )\n         cu_seq_lens_q = torch.cat([torch.zeros(1, **tensor_kws), q_len.cumsum(0).to(torch.int32)], 0)\n         max_length_q = int(q_len.max())\n     else:\n@@ -264,6 +247,49 @@ def _prepare_from_posids(query, key, value, position_ids, query_length):\n         # for some models (e.g. qwen2-vl).\n         max_length_q = cu_seq_lens_q.diff().max().item()\n         max_length_k = max_length_q\n+    return (cu_seq_lens_q, cu_seq_lens_k), (max_length_q, max_length_k)\n+\n+\n+def _prepare_from_posids(query, key, value, position_ids, query_length):\n+    \"\"\"\n+    This function returns necessary arguments to call `flash_attn_varlen_func`.\n+    All three query, key, value states will be flattened.\n+    Cumulative lengths of each examples in the batch will be extracted from position_ids.\n+    NOTE: ideally cumulative lengths should be prepared at the data collator stage\n+    Arguments:\n+        query (`torch.Tensor`):\n+            Query state with padding. Shape: (batch_size, query_length, num_heads, head_dim).\n+        key (`torch.Tensor`):\n+            Key state with padding. Shape: (batch_size, kv_seq_len, num_key_value_heads, head_dim).\n+        value (`torch.Tensor`):\n+            Value state with padding. Shape: (batch_size, kv_seq_len, num_key_value_heads, head_dim).\n+        position_ids (`torch.Tensor`):\n+            Boolean or int tensor of shape (batch_size, sequence_length), 1 means valid and 0 means not valid.\n+        query_length (`int`):\n+            Sequence length of the input queries.\n+    Return:\n+        query (`torch.Tensor`):\n+            Query state without padding. Shape: (total_target_length, num_heads, head_dim).\n+        key (`torch.Tensor`):\n+            Key state with padding. Shape: (total_source_length, num_key_value_heads, head_dim).\n+        value (`torch.Tensor`):\n+            Value state with padding. Shape: (total_source_length, num_key_value_heads, head_dim).\n+        (cu_seqlens_q, cu_seqlens_k) (`tuple[int]`):\n+            The cumulative sequence lengths for the target (query) and source (key, value), used to index into\n+            ragged (unpadded) tensors. `cu_seqlens` shape is (batch_size + 1,).\n+        (max_seqlen_in_batch_q, max_seqlen_in_batch_k) (`tuple[int]`):\n+            Maximum sequence length in batch (`max_seqlen_in_batch_q` for the target sequence i.e. query,\n+            `max_seqlen_in_batch_k` for the source sequence i.e. key/value).\n+    \"\"\"\n+    kv_length = key.shape[1]\n+    query = query.contiguous().view(-1, query.size(-2), query.size(-1))\n+    key = key.contiguous().view(-1, key.size(-2), key.size(-1))\n+    value = value.contiguous().view(-1, value.size(-2), value.size(-1))\n+    is_packed_sequence = query_length == kv_length\n+\n+    cu_seq_lens_q, cu_seq_lens_k, max_length_q, max_length_k = prepare_fa_kwargs_from_position_ids(\n+        position_ids, is_packed_sequence=is_packed_sequence\n+    )\n     return (query, key, value, (cu_seq_lens_q, cu_seq_lens_k), (max_length_q, max_length_k))\n \n "
        }
    ],
    "stats": {
        "total": 100,
        "additions": 69,
        "deletions": 31
    }
}