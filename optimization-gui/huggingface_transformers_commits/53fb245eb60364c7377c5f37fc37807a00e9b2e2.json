{
    "author": "Rocketknight1",
    "message": ":rotating_light: :rotating_light: Inherited CausalLM Tests (#37590)\n\n* stash commit\n\n* Experiment 1: Try just Gemma\n\n* Experiment 1: Just try Gemma\n\n* make fixup\n\n* Trigger tests\n\n* stash commit\n\n* Try adding Gemma3 as well\n\n* make fixup\n\n* Correct attrib names\n\n* Correct pipeline model mapping\n\n* Add in all_model_classes for Gemma1 again\n\n* Move the pipeline model mapping around again\n\n* make fixup\n\n* Revert Gemma3 changes since it's a VLM\n\n* Let's try Falcon\n\n* Correct attributes\n\n* Correct attributes\n\n* Let's try just overriding get_config() for now\n\n* Do Nemotron too\n\n* And Llama!\n\n* Do llama/persimmon\n\n* Correctly skip tests\n\n* Fix Persimmon\n\n* Include Phimoe\n\n* Fix Gemma2\n\n* Set model_tester_class correctly\n\n* Add GLM\n\n* More models!\n\n* models models models\n\n* make fixup\n\n* Add Qwen3 + Qwen3MoE\n\n* Correct import\n\n* make fixup\n\n* Add the QuestionAnswering classes\n\n* Add the QuestionAnswering classes\n\n* Move pipeline mapping to the right place\n\n* Jetmoe too\n\n* Stop RoPE testing models with no RoPE\n\n* Fix up JetMOE a bit\n\n* Fix up JetMOE a bit\n\n* Can we just force pad_token_id all the time?\n\n* make fixup\n\n* fix starcoder2\n\n* Move pipeline mapping\n\n* Fix RoPE skipping\n\n* Fix RecurrentGemma tests\n\n* Fix Falcon tests\n\n* Add MoE attributes\n\n* Fix values for RoPE testing\n\n* Make sure we set bos_token_id and eos_token_id in an appropriate range\n\n* make fixup\n\n* Fix GLM4\n\n* Add mamba attributes\n\n* Revert bits of JetMOE\n\n* Re-add the JetMOE skips\n\n* Update tests/causal_lm_tester.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* Add licence\n\n---------\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>",
    "sha": "53fb245eb60364c7377c5f37fc37807a00e9b2e2",
    "files": [
        {
            "sha": "2ef760e0fbe5a493a1b7827192cc49b34fd56834",
            "filename": "tests/causal_lm_tester.py",
            "status": "added",
            "additions": 479,
            "deletions": 0,
            "changes": 479,
            "blob_url": "https://github.com/huggingface/transformers/blob/53fb245eb60364c7377c5f37fc37807a00e9b2e2/tests%2Fcausal_lm_tester.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/53fb245eb60364c7377c5f37fc37807a00e9b2e2/tests%2Fcausal_lm_tester.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fcausal_lm_tester.py?ref=53fb245eb60364c7377c5f37fc37807a00e9b2e2",
            "patch": "@@ -0,0 +1,479 @@\n+# Copyright 2025 HuggingFace Inc.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import tempfile\n+from inspect import signature\n+\n+import pytest\n+from parameterized import parameterized\n+\n+from transformers import set_seed\n+from transformers.testing_utils import (\n+    is_flaky,\n+    require_flash_attn,\n+    require_torch_accelerator,\n+    require_torch_gpu,\n+    require_torch_sdpa,\n+    slow,\n+)\n+\n+from .test_configuration_common import ConfigTester\n+from .test_modeling_common import (\n+    GenerationTesterMixin,\n+    ModelTesterMixin,\n+    ids_tensor,\n+    is_torch_available,\n+    require_torch,\n+    torch_device,\n+)\n+from .test_pipeline_mixin import PipelineTesterMixin\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+\n+class CausalLMModelTester:\n+    _required_attributes = (\"base_model_class\", \"config_class\", \"causal_lm_class\")\n+    forced_config_args = [\n+        \"pad_token_id\"\n+    ]  # Arguments that should be passed to the config class even if not in its signature\n+    config_class = None\n+    base_model_class = None\n+    causal_lm_class = None\n+    sequence_classification_class = None\n+    token_classification_class = None\n+    question_answering_class = None\n+\n+    def _verify_model_attributes(self):\n+        for required_attribute in self._required_attributes:\n+            if getattr(self, required_attribute) is None:\n+                raise ValueError(\n+                    f\"You have inherited from CausalLMModelTester but did not set the {required_attribute} attribute.\"\n+                )\n+\n+    @property\n+    def all_model_classes(self):\n+        return [\n+            model_class\n+            for model_class in (\n+                self.base_model_class,\n+                self.causal_lm_class,\n+                self.sequence_classification_class,\n+                self.token_classification_class,\n+            )\n+            if model_class is not None\n+        ]\n+\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=13,\n+        seq_length=7,\n+        is_training=True,\n+        use_input_mask=True,\n+        use_token_type_ids=False,\n+        use_labels=True,\n+        vocab_size=99,\n+        hidden_size=32,\n+        num_hidden_layers=2,\n+        num_attention_heads=2,\n+        num_key_value_heads=2,\n+        intermediate_size=37,\n+        hidden_act=\"gelu\",\n+        hidden_dropout_prob=0.1,\n+        attention_probs_dropout_prob=0.1,\n+        max_position_embeddings=512,\n+        type_vocab_size=16,\n+        type_sequence_label_size=2,\n+        initializer_range=0.02,\n+        num_labels=3,\n+        num_choices=4,\n+        pad_token_id=0,\n+        bos_token_id=1,\n+        eos_token_id=2,\n+        is_decoder=False,\n+        scope=None,\n+        expert_interval=1,\n+        moe_intermediate_size=12,\n+        shared_expert_intermediate_size=36,\n+        shared_expert_gate=True,\n+        num_experts_per_tok=2,\n+        num_experts=8,\n+        mamba_n_groups=1,\n+        mamba_n_heads=16,\n+        mamba_d_state=16,\n+        mamba_d_conv=4,\n+        mamba_expand=2,\n+        mamba_chunk_size=16,\n+    ):\n+        self._verify_model_attributes()\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.seq_length = seq_length\n+        self.is_training = is_training\n+        self.use_input_mask = use_input_mask\n+        self.use_token_type_ids = use_token_type_ids\n+        self.use_labels = use_labels\n+        self.vocab_size = vocab_size\n+        self.hidden_size = hidden_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.num_key_value_heads = num_key_value_heads\n+        self.intermediate_size = intermediate_size\n+        self.hidden_act = hidden_act\n+        self.hidden_dropout_prob = hidden_dropout_prob\n+        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n+        self.max_position_embeddings = max_position_embeddings\n+        self.type_vocab_size = type_vocab_size\n+        self.type_sequence_label_size = type_sequence_label_size\n+        self.initializer_range = initializer_range\n+        self.num_labels = num_labels\n+        self.num_choices = num_choices\n+        self.pad_token_id = pad_token_id\n+        self.bos_token_id = bos_token_id\n+        self.eos_token_id = eos_token_id\n+        self.scope = scope\n+        self.head_dim = self.hidden_size // self.num_attention_heads\n+        self.is_decoder = is_decoder\n+        self.expert_interval = expert_interval\n+        self.moe_intermediate_size = moe_intermediate_size\n+        self.shared_expert_intermediate_size = shared_expert_intermediate_size\n+        self.shared_expert_gate = shared_expert_gate\n+        self.num_experts_per_tok = num_experts_per_tok\n+        self.num_experts = num_experts\n+        self.mamba_n_groups = mamba_n_groups\n+        self.mamba_n_heads = mamba_n_heads\n+        self.mamba_d_state = mamba_d_state\n+        self.mamba_d_conv = mamba_d_conv\n+        self.mamba_expand = mamba_expand\n+        self.mamba_chunk_size = mamba_chunk_size\n+\n+    def prepare_config_and_inputs(self):\n+        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n+\n+        input_mask = None\n+        if self.use_input_mask:\n+            input_mask = torch.tril(torch.ones_like(input_ids).to(torch_device))\n+\n+        token_type_ids = None\n+        if self.use_token_type_ids:\n+            token_type_ids = ids_tensor([self.batch_size, self.seq_length], self.type_vocab_size)\n+\n+        sequence_labels = None\n+        token_labels = None\n+        choice_labels = None\n+        if self.use_labels:\n+            sequence_labels = ids_tensor([self.batch_size], self.type_sequence_label_size)\n+            token_labels = ids_tensor([self.batch_size, self.seq_length], self.num_labels)\n+            choice_labels = ids_tensor([self.batch_size], self.num_choices)\n+\n+        config = self.get_config()\n+\n+        return config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n+\n+    def get_config(self):\n+        kwarg_names = list(signature(self.config_class.__init__).parameters.keys())\n+        kwargs = {\n+            k: getattr(self, k) for k in kwarg_names + self.forced_config_args if hasattr(self, k) and k != \"self\"\n+        }\n+        return self.config_class(**kwargs)\n+\n+    def create_and_check_model(\n+        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n+    ):\n+        model = self.base_model_class(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        result = model(input_ids, attention_mask=input_mask)\n+        result = model(input_ids)\n+        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        (\n+            config,\n+            input_ids,\n+            token_type_ids,\n+            input_mask,\n+            sequence_labels,\n+            token_labels,\n+            choice_labels,\n+        ) = config_and_inputs\n+        inputs_dict = {\"input_ids\": input_ids, \"attention_mask\": input_mask}\n+        return config, inputs_dict\n+\n+\n+@require_torch\n+class CausalLMModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin):\n+    test_headmasking = False\n+    test_pruning = False\n+    model_tester_class = None\n+    all_model_classes = None\n+    rotary_embedding_layer = None  # Enables RoPE tests if set\n+    pipeline_model_mapping = None\n+\n+    def setUp(self):\n+        if self.model_tester_class is None:\n+            raise ValueError(\n+                \"You have inherited from CausalLMModelTest but did not set the model_tester_class attribute.\"\n+            )\n+        self.model_tester = self.model_tester_class(self)\n+        self.config_tester = ConfigTester(self, config_class=self.model_tester.config_class)\n+        if self.all_model_classes is None:\n+            self.all_model_classes = self.model_tester.all_model_classes\n+        if self.pipeline_model_mapping is None:\n+            raise ValueError(\n+                \"You have inherited from CausalLMModelTest but did not set the pipeline_model_mapping attribute.\"\n+            )\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n+    def test_model(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_model(*config_and_inputs)\n+\n+    def test_sequence_classification_model(self):\n+        if self.model_tester.sequence_classification_class is None:\n+            self.skipTest(\"Model does not support sequence classification\")\n+        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        config.num_labels = 3\n+        input_ids = input_dict[\"input_ids\"]\n+        attention_mask = input_ids.ne(1).to(torch_device)\n+        sequence_labels = ids_tensor([self.model_tester.batch_size], self.model_tester.type_sequence_label_size)\n+        model = self.model_tester.sequence_classification_class(config)\n+        model.to(torch_device)\n+        model.eval()\n+        result = model(input_ids, attention_mask=attention_mask, labels=sequence_labels)\n+        self.assertEqual(result.logits.shape, (self.model_tester.batch_size, self.model_tester.num_labels))\n+\n+    def test_sequence_classification_model_for_single_label(self):\n+        if self.model_tester.sequence_classification_class is None:\n+            self.skipTest(\"Model does not support sequence classification\")\n+        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        config.num_labels = 3\n+        config.problem_type = \"single_label_classification\"\n+        input_ids = input_dict[\"input_ids\"]\n+        attention_mask = input_ids.ne(1).to(torch_device)\n+        sequence_labels = ids_tensor([self.model_tester.batch_size], self.model_tester.type_sequence_label_size)\n+        model = self.model_tester.sequence_classification_class(config)\n+        model.to(torch_device)\n+        model.eval()\n+        result = model(input_ids, attention_mask=attention_mask, labels=sequence_labels)\n+        self.assertEqual(result.logits.shape, (self.model_tester.batch_size, self.model_tester.num_labels))\n+\n+    def test_sequence_classification_model_for_multi_label(self):\n+        if self.model_tester.sequence_classification_class is None:\n+            self.skipTest(\"Model does not support sequence classification\")\n+        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        config.num_labels = 3\n+        config.problem_type = \"multi_label_classification\"\n+        input_ids = input_dict[\"input_ids\"]\n+        attention_mask = input_ids.ne(1).to(torch_device)\n+        sequence_labels = ids_tensor(\n+            [self.model_tester.batch_size, config.num_labels], self.model_tester.type_sequence_label_size\n+        ).to(torch.float)\n+        model = self.model_tester.sequence_classification_class(config)\n+        model.to(torch_device)\n+        model.eval()\n+        result = model(input_ids, attention_mask=attention_mask, labels=sequence_labels)\n+        self.assertEqual(result.logits.shape, (self.model_tester.batch_size, self.model_tester.num_labels))\n+\n+    def test_token_classification_model(self):\n+        if self.model_tester.token_classification_class is None:\n+            self.skipTest(\"Model does not support token classification\")\n+        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        config.num_labels = 3\n+        input_ids = input_dict[\"input_ids\"]\n+        attention_mask = input_ids.ne(1).to(torch_device)\n+        token_labels = ids_tensor([self.model_tester.batch_size, self.model_tester.seq_length], config.num_labels)\n+        model = self.model_tester.token_classification_class(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        result = model(input_ids, attention_mask=attention_mask, labels=token_labels)\n+        self.assertEqual(\n+            result.logits.shape,\n+            (self.model_tester.batch_size, self.model_tester.seq_length, self.model_tester.num_labels),\n+        )\n+\n+    @parameterized.expand([(\"linear\",), (\"dynamic\",), (\"yarn\",)])\n+    def test_model_rope_scaling_from_config(self, scaling_type):\n+        if self.rotary_embedding_layer is None:\n+            self.skipTest(\"Rotary embedding layer not set\")\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+        short_input = ids_tensor([1, 10], config.vocab_size)\n+        long_input = ids_tensor([1, int(config.max_position_embeddings * 1.5)], config.vocab_size)\n+\n+        set_seed(42)  # Fixed seed at init time so the two models get the same random weights\n+        original_model = self.model_tester_class.base_model_class(config)\n+        original_model.to(torch_device)\n+        original_model.eval()\n+        original_short_output = original_model(short_input).last_hidden_state\n+        original_long_output = original_model(long_input).last_hidden_state\n+\n+        set_seed(42)  # Fixed seed at init time so the two models get the same random weights\n+        config.rope_scaling = {\"type\": scaling_type, \"factor\": 10.0}\n+        scaled_model = self.model_tester_class.base_model_class(config)\n+        scaled_model.to(torch_device)\n+        scaled_model.eval()\n+        scaled_short_output = scaled_model(short_input).last_hidden_state\n+        scaled_long_output = scaled_model(long_input).last_hidden_state\n+\n+        # Dynamic scaling does not change the RoPE embeddings until it receives an input longer than the original\n+        # maximum sequence length, so the outputs for the short input should match.\n+        if scaling_type == \"dynamic\":\n+            torch.testing.assert_close(original_short_output, scaled_short_output, rtol=1e-5, atol=1e-5)\n+        else:\n+            self.assertFalse(torch.allclose(original_short_output, scaled_short_output, atol=1e-5))\n+\n+        # The output should be different for long inputs\n+        self.assertFalse(torch.allclose(original_long_output, scaled_long_output, atol=1e-5))\n+\n+    def test_model_rope_scaling(self):\n+        if self.rotary_embedding_layer is None:\n+            self.skipTest(\"Rotary embedding layer not set\")\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+        scaling_factor = 10\n+        short_input_length = 10\n+        long_input_length = int(config.max_position_embeddings * 1.5)\n+\n+        # Inputs\n+        x = torch.randn(\n+            1, dtype=torch.float32, device=torch_device\n+        )  # used exclusively to get the dtype and the device\n+        position_ids_short = torch.arange(short_input_length, dtype=torch.long, device=torch_device)\n+        position_ids_short = position_ids_short.unsqueeze(0)\n+        position_ids_long = torch.arange(long_input_length, dtype=torch.long, device=torch_device)\n+        position_ids_long = position_ids_long.unsqueeze(0)\n+\n+        # Sanity check original RoPE\n+        original_rope = self.rotary_embedding_layer(config=config).to(torch_device)\n+        original_cos_short, original_sin_short = original_rope(x, position_ids_short)\n+        original_cos_long, original_sin_long = original_rope(x, position_ids_long)\n+        torch.testing.assert_close(original_cos_short, original_cos_long[:, :short_input_length, :])\n+        torch.testing.assert_close(original_sin_short, original_sin_long[:, :short_input_length, :])\n+\n+        # Sanity check linear RoPE scaling\n+        # New position \"x\" should match original position with index \"x/scaling_factor\"\n+        config.rope_scaling = {\"type\": \"linear\", \"factor\": scaling_factor}\n+        linear_scaling_rope = self.rotary_embedding_layer(config=config).to(torch_device)\n+        linear_cos_short, linear_sin_short = linear_scaling_rope(x, position_ids_short)\n+        linear_cos_long, linear_sin_long = linear_scaling_rope(x, position_ids_long)\n+        torch.testing.assert_close(linear_cos_short, linear_cos_long[:, :short_input_length, :])\n+        torch.testing.assert_close(linear_sin_short, linear_sin_long[:, :short_input_length, :])\n+        for new_position in range(0, long_input_length, scaling_factor):\n+            original_position = int(new_position // scaling_factor)\n+            torch.testing.assert_close(linear_cos_long[:, new_position, :], original_cos_long[:, original_position, :])\n+            torch.testing.assert_close(linear_sin_long[:, new_position, :], original_sin_long[:, original_position, :])\n+\n+        # Sanity check Dynamic NTK RoPE scaling\n+        # Scaling should only be observed after a long input is fed. We can observe that the frequencies increase\n+        # with scaling_factor (or that `inv_freq` decreases)\n+        config.rope_scaling = {\"type\": \"dynamic\", \"factor\": scaling_factor}\n+        ntk_scaling_rope = self.rotary_embedding_layer(config=config).to(torch_device)\n+        ntk_cos_short, ntk_sin_short = ntk_scaling_rope(x, position_ids_short)\n+        ntk_cos_long, ntk_sin_long = ntk_scaling_rope(x, position_ids_long)\n+        torch.testing.assert_close(ntk_cos_short, original_cos_short)\n+        torch.testing.assert_close(ntk_sin_short, original_sin_short)\n+        with self.assertRaises(AssertionError):\n+            torch.testing.assert_close(ntk_cos_long, original_cos_long)\n+        with self.assertRaises(AssertionError):\n+            torch.testing.assert_close(ntk_sin_long, original_sin_long)\n+        self.assertTrue((ntk_scaling_rope.inv_freq <= original_rope.inv_freq).all())\n+\n+        # Sanity check Yarn RoPE scaling\n+        # Scaling should be over the entire input\n+        config.rope_scaling = {\"type\": \"yarn\", \"factor\": scaling_factor}\n+        yarn_scaling_rope = self.rotary_embedding_layer(config=config).to(torch_device)\n+        yarn_cos_short, yarn_sin_short = yarn_scaling_rope(x, position_ids_short)\n+        yarn_cos_long, yarn_sin_long = yarn_scaling_rope(x, position_ids_long)\n+        torch.testing.assert_close(yarn_cos_short, yarn_cos_long[:, :short_input_length, :])\n+        torch.testing.assert_close(yarn_sin_short, yarn_sin_long[:, :short_input_length, :])\n+        with self.assertRaises(AssertionError):\n+            torch.testing.assert_close(yarn_cos_short, original_cos_short)\n+        with self.assertRaises(AssertionError):\n+            torch.testing.assert_close(yarn_sin_short, original_sin_short)\n+        with self.assertRaises(AssertionError):\n+            torch.testing.assert_close(yarn_cos_long, original_cos_long)\n+        with self.assertRaises(AssertionError):\n+            torch.testing.assert_close(yarn_sin_long, original_sin_long)\n+\n+    @require_torch_sdpa\n+    @require_torch_accelerator\n+    @slow\n+    def test_sdpa_equivalence(self):\n+        for model_class in self.all_model_classes:\n+            if not model_class._supports_sdpa:\n+                self.skipTest(reason=\"Model does not support SDPA\")\n+\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            model = model_class(config)\n+\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+                model_sdpa = model_class.from_pretrained(\n+                    tmpdirname, torch_dtype=torch.bfloat16, attn_implementation=\"sdpa\"\n+                )\n+                model_sdpa.to(torch_device)\n+\n+                model = model_class.from_pretrained(\n+                    tmpdirname, torch_dtype=torch.bfloat16, attn_implementation=\"eager\"\n+                )\n+                model.to(torch_device)\n+\n+                dummy_input = inputs_dict[model_class.main_input_name]\n+                dummy_input = dummy_input.to(torch_device)\n+                outputs = model(dummy_input, output_hidden_states=True)\n+                outputs_sdpa = model_sdpa(dummy_input, output_hidden_states=True)\n+\n+                logits = outputs.hidden_states[-1]\n+                logits_sdpa = outputs_sdpa.hidden_states[-1]\n+\n+                assert torch.allclose(logits_sdpa, logits, atol=2e-3)\n+\n+    @require_flash_attn\n+    @require_torch_gpu\n+    @pytest.mark.flash_attn_test\n+    @is_flaky()\n+    @slow\n+    def test_flash_attn_2_equivalence(self):\n+        for model_class in self.all_model_classes:\n+            if not model_class._supports_flash_attn_2:\n+                self.skipTest(reason=\"Model does not support Flash Attention 2\")\n+\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            model = model_class(config)\n+\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+                model_fa = model_class.from_pretrained(\n+                    tmpdirname, torch_dtype=torch.bfloat16, attn_implementation=\"flash_attention_2\"\n+                )\n+                model_fa.to(torch_device)\n+\n+                model = model_class.from_pretrained(\n+                    tmpdirname, torch_dtype=torch.bfloat16, attn_implementation=\"eager\"\n+                )\n+                model.to(torch_device)\n+\n+                dummy_input = inputs_dict[model_class.main_input_name]\n+                dummy_input = dummy_input.to(torch_device)\n+                outputs = model(dummy_input, output_hidden_states=True)\n+                outputs_fa = model_fa(dummy_input, output_hidden_states=True)\n+\n+                logits = outputs.hidden_states[-1]\n+                logits_fa = outputs_fa.hidden_states[-1]\n+\n+                assert torch.allclose(logits_fa, logits, atol=2e-3)"
        },
        {
            "sha": "7b6cfc5b081d3fd0aaae099384a86f178e8d76a1",
            "filename": "tests/models/dbrx/test_modeling_dbrx.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/53fb245eb60364c7377c5f37fc37807a00e9b2e2/tests%2Fmodels%2Fdbrx%2Ftest_modeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/53fb245eb60364c7377c5f37fc37807a00e9b2e2/tests%2Fmodels%2Fdbrx%2Ftest_modeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdbrx%2Ftest_modeling_dbrx.py?ref=53fb245eb60364c7377c5f37fc37807a00e9b2e2",
            "patch": "@@ -179,7 +179,6 @@ def get_config(self):\n         )\n         return config\n \n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.create_and_check_model with Llama->Dbrx\n     def create_and_check_model(\n         self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n     ):\n@@ -190,7 +189,6 @@ def create_and_check_model(\n         result = model(input_ids)\n         self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n \n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.prepare_config_and_inputs_for_common with Llama->Dbrx\n     def prepare_config_and_inputs_for_common(self):\n         config_and_inputs = self.prepare_config_and_inputs()\n         ("
        },
        {
            "sha": "661ba98cf16913ce36e0bed59e5c57e57e932bf7",
            "filename": "tests/models/falcon/test_modeling_falcon.py",
            "status": "modified",
            "additions": 14,
            "deletions": 263,
            "changes": 277,
            "blob_url": "https://github.com/huggingface/transformers/blob/53fb245eb60364c7377c5f37fc37807a00e9b2e2/tests%2Fmodels%2Ffalcon%2Ftest_modeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/53fb245eb60364c7377c5f37fc37807a00e9b2e2/tests%2Fmodels%2Ffalcon%2Ftest_modeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffalcon%2Ftest_modeling_falcon.py?ref=53fb245eb60364c7377c5f37fc37807a00e9b2e2",
            "patch": "@@ -15,14 +15,11 @@\n \n import unittest\n \n-from parameterized import parameterized\n-\n from transformers import (\n     AutoModelForCausalLM,\n     AutoTokenizer,\n     FalconConfig,\n     is_torch_available,\n-    set_seed,\n )\n from transformers.testing_utils import (\n     require_bitsandbytes,\n@@ -32,10 +29,7 @@\n     torch_device,\n )\n \n-from ...generation.test_utils import GenerationTesterMixin\n-from ...test_configuration_common import ConfigTester\n-from ...test_modeling_common import ModelTesterMixin, ids_tensor, random_attention_mask\n-from ...test_pipeline_mixin import PipelineTesterMixin\n+from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n \n \n if is_torch_available():\n@@ -48,126 +42,24 @@\n         FalconForTokenClassification,\n         FalconModel,\n     )\n-    from transformers.models.falcon.modeling_falcon import (\n-        FalconRotaryEmbedding,\n-    )\n-\n \n-class FalconModelTester:\n-    def __init__(\n-        self,\n-        parent,\n-        batch_size=3,\n-        seq_length=7,\n-        is_training=True,\n-        use_input_mask=True,\n-        use_token_type_ids=False,\n-        use_labels=True,\n-        vocab_size=99,\n-        hidden_size=32,\n-        num_hidden_layers=2,\n-        num_attention_heads=4,\n-        intermediate_size=37,\n-        hidden_act=\"gelu\",\n-        hidden_dropout_prob=0.1,\n-        attention_probs_dropout_prob=0.1,\n-        max_position_embeddings=512,\n-        type_vocab_size=16,\n-        type_sequence_label_size=2,\n-        initializer_range=0.02,\n-        num_labels=3,\n-        num_choices=4,\n-        scope=None,\n-    ):\n-        self.parent = parent\n-        self.batch_size = batch_size\n-        self.seq_length = seq_length\n-        self.is_training = is_training\n-        self.use_input_mask = use_input_mask\n-        self.use_token_type_ids = use_token_type_ids\n-        self.use_labels = use_labels\n-        self.vocab_size = vocab_size\n-        self.hidden_size = hidden_size\n-        self.num_hidden_layers = num_hidden_layers\n-        self.num_attention_heads = num_attention_heads\n-        self.intermediate_size = intermediate_size\n-        self.hidden_act = hidden_act\n-        self.hidden_dropout_prob = hidden_dropout_prob\n-        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n-        self.max_position_embeddings = max_position_embeddings\n-        self.type_vocab_size = type_vocab_size\n-        self.type_sequence_label_size = type_sequence_label_size\n-        self.initializer_range = initializer_range\n-        self.num_labels = num_labels\n-        self.num_choices = num_choices\n-        self.scope = scope\n-\n-    def prepare_config_and_inputs(self):\n-        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n-\n-        input_mask = None\n-        if self.use_input_mask:\n-            input_mask = random_attention_mask([self.batch_size, self.seq_length])\n-\n-        token_type_ids = None\n-\n-        sequence_labels = None\n-        token_labels = None\n-        choice_labels = None\n-        if self.use_labels:\n-            sequence_labels = ids_tensor([self.batch_size], self.type_sequence_label_size)\n-            token_labels = ids_tensor([self.batch_size, self.seq_length], self.num_labels)\n-            choice_labels = ids_tensor([self.batch_size], self.num_choices)\n-\n-        config = self.get_config()\n-\n-        return config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-\n-    def get_config(self):\n-        return FalconConfig(\n-            vocab_size=self.vocab_size,\n-            hidden_size=self.hidden_size,\n-            num_hidden_layers=self.num_hidden_layers,\n-            num_attention_heads=self.num_attention_heads,\n-            intermediate_size=self.intermediate_size,\n-            hidden_act=self.hidden_act,\n-            hidden_dropout_prob=self.hidden_dropout_prob,\n-            attention_probs_dropout_prob=self.attention_probs_dropout_prob,\n-            max_position_embeddings=self.max_position_embeddings,\n-            type_vocab_size=self.type_vocab_size,\n-            is_decoder=False,\n-            initializer_range=self.initializer_range,\n-            pad_token_id=1,\n-            new_decoder_architecture=True,\n-        )\n \n-    def create_and_check_model(\n-        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        model = FalconModel(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=input_mask)\n-        result = model(input_ids)\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n+class FalconModelTester(CausalLMModelTester):\n+    if is_torch_available():\n+        config_class = FalconConfig\n+        base_model_class = FalconModel\n+        causal_lm_class = FalconForCausalLM\n+        sequence_class = FalconForSequenceClassification\n+        token_class = FalconForTokenClassification\n \n-    def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-        (\n-            config,\n-            input_ids,\n-            token_type_ids,\n-            input_mask,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-        ) = config_and_inputs\n-        inputs_dict = {\"input_ids\": input_ids, \"attention_mask\": input_mask}\n-        return config, inputs_dict\n+    def __init__(self, parent, new_decoder_architecture=True):\n+        super().__init__(parent)\n+        self.new_decoder_architecture = new_decoder_architecture\n \n \n @require_torch\n-class FalconModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+class FalconModelTest(CausalLMModelTest, unittest.TestCase):\n+    model_tester_class = FalconModelTester\n     all_model_classes = (\n         (\n             FalconModel,\n@@ -182,10 +74,9 @@ class FalconModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMix\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": FalconModel,\n-            \"question-answering\": FalconForQuestionAnswering,\n             \"text-classification\": FalconForSequenceClassification,\n-            \"text-generation\": FalconForCausalLM,\n             \"token-classification\": FalconForTokenClassification,\n+            \"text-generation\": FalconForCausalLM,\n             \"zero-shot\": FalconForSequenceClassification,\n         }\n         if is_torch_available()\n@@ -207,146 +98,6 @@ def is_pipeline_test_to_skip(\n     ):\n         return True\n \n-    def setUp(self):\n-        self.model_tester = FalconModelTester(self)\n-        self.config_tester = ConfigTester(self, config_class=FalconConfig, hidden_size=37)\n-\n-    def test_config(self):\n-        self.config_tester.run_common_tests()\n-\n-    def test_model(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_model(*config_and_inputs)\n-\n-    def test_position_embedding_types(self):\n-        config, *inputs = self.model_tester.prepare_config_and_inputs()\n-        for alibi in [True, False]:\n-            config.alibi = alibi\n-            self.model_tester.create_and_check_model(config, *inputs)\n-\n-    def test_falcon_sequence_classification_model(self):\n-        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.num_labels = 3\n-        input_ids = input_dict[\"input_ids\"]\n-        attention_mask = input_ids.ne(1).to(torch_device)\n-        sequence_labels = ids_tensor([self.model_tester.batch_size], self.model_tester.type_sequence_label_size)\n-        model = FalconForSequenceClassification(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=attention_mask, labels=sequence_labels)\n-        self.assertEqual(result.logits.shape, (self.model_tester.batch_size, self.model_tester.num_labels))\n-\n-    def test_falcon_sequence_classification_model_for_single_label(self):\n-        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.num_labels = 3\n-        config.problem_type = \"single_label_classification\"\n-        input_ids = input_dict[\"input_ids\"]\n-        attention_mask = input_ids.ne(1).to(torch_device)\n-        sequence_labels = ids_tensor([self.model_tester.batch_size], self.model_tester.type_sequence_label_size)\n-        model = FalconForSequenceClassification(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=attention_mask, labels=sequence_labels)\n-        self.assertEqual(result.logits.shape, (self.model_tester.batch_size, self.model_tester.num_labels))\n-\n-    def test_falcon_sequence_classification_model_for_multi_label(self):\n-        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.num_labels = 3\n-        config.problem_type = \"multi_label_classification\"\n-        input_ids = input_dict[\"input_ids\"]\n-        attention_mask = input_ids.ne(1).to(torch_device)\n-        sequence_labels = ids_tensor(\n-            [self.model_tester.batch_size, config.num_labels], self.model_tester.type_sequence_label_size\n-        ).to(torch.float)\n-        model = FalconForSequenceClassification(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=attention_mask, labels=sequence_labels)\n-        self.assertEqual(result.logits.shape, (self.model_tester.batch_size, self.model_tester.num_labels))\n-\n-    @parameterized.expand([(\"linear\",), (\"dynamic\",)])\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTest.test_model_rope_scaling_from_config with Llama->Falcon\n-    def test_model_rope_scaling_from_config(self, scaling_type):\n-        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-        short_input = ids_tensor([1, 10], config.vocab_size)\n-        long_input = ids_tensor([1, int(config.max_position_embeddings * 1.5)], config.vocab_size)\n-\n-        set_seed(42)  # Fixed seed at init time so the two models get the same random weights\n-        original_model = FalconModel(config)\n-        original_model.to(torch_device)\n-        original_model.eval()\n-        original_short_output = original_model(short_input).last_hidden_state\n-        original_long_output = original_model(long_input).last_hidden_state\n-\n-        set_seed(42)  # Fixed seed at init time so the two models get the same random weights\n-        config.rope_scaling = {\"type\": scaling_type, \"factor\": 10.0}\n-        scaled_model = FalconModel(config)\n-        scaled_model.to(torch_device)\n-        scaled_model.eval()\n-        scaled_short_output = scaled_model(short_input).last_hidden_state\n-        scaled_long_output = scaled_model(long_input).last_hidden_state\n-\n-        # Dynamic scaling does not change the RoPE embeddings until it receives an input longer than the original\n-        # maximum sequence length, so the outputs for the short input should match.\n-        if scaling_type == \"dynamic\":\n-            torch.testing.assert_close(original_short_output, scaled_short_output, rtol=1e-5, atol=1e-5)\n-        else:\n-            self.assertFalse(torch.allclose(original_short_output, scaled_short_output, atol=1e-5))\n-\n-        # The output should be different for long inputs\n-        self.assertFalse(torch.allclose(original_long_output, scaled_long_output, atol=1e-5))\n-\n-    # Copied from tests.models.gpt_neox.test_modeling_gpt_neox.GPTNeoXModelTest.test_model_rope_scaling with GPTNeoX->Falcon\n-    def test_model_rope_scaling(self):\n-        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-        scaling_factor = 10\n-        short_input_length = 10\n-        long_input_length = int(config.max_position_embeddings * 1.5)\n-\n-        # Inputs\n-        x = torch.randn(\n-            1, dtype=torch.float32, device=torch_device\n-        )  # used exclusively to get the dtype and the device\n-        position_ids_short = torch.arange(short_input_length, dtype=torch.long, device=torch_device)\n-        position_ids_short = position_ids_short.unsqueeze(0)\n-        position_ids_long = torch.arange(long_input_length, dtype=torch.long, device=torch_device)\n-        position_ids_long = position_ids_long.unsqueeze(0)\n-\n-        # Sanity check original RoPE\n-        original_rope = FalconRotaryEmbedding(config).to(torch_device)\n-        original_cos_short, original_sin_short = original_rope(x, position_ids_short)\n-        original_cos_long, original_sin_long = original_rope(x, position_ids_long)\n-        torch.testing.assert_close(original_cos_short, original_cos_long[:, :short_input_length, :])\n-        torch.testing.assert_close(original_sin_short, original_sin_long[:, :short_input_length, :])\n-\n-        # Sanity check linear RoPE scaling\n-        # New position \"x\" should match original position with index \"x/scaling_factor\"\n-        config.rope_scaling = {\"type\": \"linear\", \"factor\": scaling_factor}\n-        linear_scaling_rope = FalconRotaryEmbedding(config).to(torch_device)\n-        linear_cos_short, linear_sin_short = linear_scaling_rope(x, position_ids_short)\n-        linear_cos_long, linear_sin_long = linear_scaling_rope(x, position_ids_long)\n-        torch.testing.assert_close(linear_cos_short, linear_cos_long[:, :short_input_length, :])\n-        torch.testing.assert_close(linear_sin_short, linear_sin_long[:, :short_input_length, :])\n-        for new_position in range(0, long_input_length, scaling_factor):\n-            original_position = int(new_position // scaling_factor)\n-            torch.testing.assert_close(linear_cos_long[:, new_position, :], original_cos_long[:, original_position, :])\n-            torch.testing.assert_close(linear_sin_long[:, new_position, :], original_sin_long[:, original_position, :])\n-\n-        # Sanity check Dynamic NTK RoPE scaling\n-        # Scaling should only be observed after a long input is fed. We can observe that the frequencies increase\n-        # with scaling_factor (or that `inv_freq` decreases)\n-        config.rope_scaling = {\"type\": \"dynamic\", \"factor\": scaling_factor}\n-        ntk_scaling_rope = FalconRotaryEmbedding(config).to(torch_device)\n-        ntk_cos_short, ntk_sin_short = ntk_scaling_rope(x, position_ids_short)\n-        ntk_cos_long, ntk_sin_long = ntk_scaling_rope(x, position_ids_long)\n-        torch.testing.assert_close(ntk_cos_short, original_cos_short)\n-        torch.testing.assert_close(ntk_sin_short, original_sin_short)\n-        with self.assertRaises(AssertionError):\n-            torch.testing.assert_close(ntk_cos_long, original_cos_long)\n-        with self.assertRaises(AssertionError):\n-            torch.testing.assert_close(ntk_sin_long, original_sin_long)\n-        self.assertTrue((ntk_scaling_rope.inv_freq <= original_rope.inv_freq).all())\n-\n \n @require_torch\n class FalconLanguageGenerationTest(unittest.TestCase):"
        },
        {
            "sha": "649c837b9c263a375e23c5d8052e1d7e5af950ce",
            "filename": "tests/models/gemma/test_modeling_gemma.py",
            "status": "modified",
            "additions": 8,
            "deletions": 249,
            "changes": 257,
            "blob_url": "https://github.com/huggingface/transformers/blob/53fb245eb60364c7377c5f37fc37807a00e9b2e2/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/53fb245eb60364c7377c5f37fc37807a00e9b2e2/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py?ref=53fb245eb60364c7377c5f37fc37807a00e9b2e2",
            "patch": "@@ -33,10 +33,7 @@\n     torch_device,\n )\n \n-from ...generation.test_utils import GenerationTesterMixin\n-from ...test_configuration_common import ConfigTester\n-from ...test_modeling_common import ModelTesterMixin, ids_tensor\n-from ...test_pipeline_mixin import PipelineTesterMixin\n+from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n \n \n if is_torch_available():\n@@ -51,138 +48,17 @@\n \n \n @require_torch\n-class GemmaModelTester:\n+class GemmaModelTester(CausalLMModelTester):\n     config_class = GemmaConfig\n     if is_torch_available():\n-        model_class = GemmaModel\n-        for_causal_lm_class = GemmaForCausalLM\n-        for_sequence_class = GemmaForSequenceClassification\n-        for_token_class = GemmaForTokenClassification\n-\n-    def __init__(\n-        self,\n-        parent,\n-        batch_size=13,\n-        seq_length=7,\n-        is_training=True,\n-        use_input_mask=True,\n-        use_token_type_ids=False,\n-        use_labels=True,\n-        vocab_size=99,\n-        hidden_size=32,\n-        num_hidden_layers=2,\n-        num_attention_heads=4,\n-        num_key_value_heads=2,\n-        intermediate_size=37,\n-        hidden_act=\"gelu\",\n-        hidden_dropout_prob=0.1,\n-        attention_probs_dropout_prob=0.1,\n-        max_position_embeddings=512,\n-        type_vocab_size=16,\n-        type_sequence_label_size=2,\n-        initializer_range=0.02,\n-        num_labels=3,\n-        num_choices=4,\n-        pad_token_id=0,\n-        scope=None,\n-    ):\n-        self.parent = parent\n-        self.batch_size = batch_size\n-        self.seq_length = seq_length\n-        self.is_training = is_training\n-        self.use_input_mask = use_input_mask\n-        self.use_token_type_ids = use_token_type_ids\n-        self.use_labels = use_labels\n-        self.vocab_size = vocab_size\n-        self.hidden_size = hidden_size\n-        self.num_hidden_layers = num_hidden_layers\n-        self.num_attention_heads = num_attention_heads\n-        self.num_key_value_heads = num_key_value_heads\n-        self.intermediate_size = intermediate_size\n-        self.hidden_act = hidden_act\n-        self.hidden_dropout_prob = hidden_dropout_prob\n-        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n-        self.max_position_embeddings = max_position_embeddings\n-        self.type_vocab_size = type_vocab_size\n-        self.type_sequence_label_size = type_sequence_label_size\n-        self.initializer_range = initializer_range\n-        self.num_labels = num_labels\n-        self.num_choices = num_choices\n-        self.pad_token_id = pad_token_id\n-        self.scope = scope\n-        self.head_dim = self.hidden_size // self.num_attention_heads\n-\n-    # Copied from tests.models.mistral.test_modeling_mistral.MistralModelTester.prepare_config_and_inputs\n-    def prepare_config_and_inputs(self):\n-        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n-\n-        input_mask = None\n-        if self.use_input_mask:\n-            input_mask = torch.tril(torch.ones_like(input_ids).to(torch_device))\n-\n-        token_type_ids = None\n-        if self.use_token_type_ids:\n-            token_type_ids = ids_tensor([self.batch_size, self.seq_length], self.type_vocab_size)\n-\n-        sequence_labels = None\n-        token_labels = None\n-        choice_labels = None\n-        if self.use_labels:\n-            sequence_labels = ids_tensor([self.batch_size], self.type_sequence_label_size)\n-            token_labels = ids_tensor([self.batch_size, self.seq_length], self.num_labels)\n-            choice_labels = ids_tensor([self.batch_size], self.num_choices)\n-\n-        config = self.get_config()\n-\n-        return config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-\n-    def get_config(self):\n-        return self.config_class(\n-            vocab_size=self.vocab_size,\n-            hidden_size=self.hidden_size,\n-            num_hidden_layers=self.num_hidden_layers,\n-            num_attention_heads=self.num_attention_heads,\n-            num_key_value_heads=self.num_key_value_heads,\n-            intermediate_size=self.intermediate_size,\n-            hidden_act=self.hidden_act,\n-            hidden_dropout_prob=self.hidden_dropout_prob,\n-            attention_probs_dropout_prob=self.attention_probs_dropout_prob,\n-            max_position_embeddings=self.max_position_embeddings,\n-            type_vocab_size=self.type_vocab_size,\n-            is_decoder=False,\n-            initializer_range=self.initializer_range,\n-            pad_token_id=self.pad_token_id,\n-            head_dim=self.head_dim,\n-        )\n-\n-    def create_and_check_model(\n-        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        model = self.model_class(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=input_mask)\n-        result = model(input_ids)\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.prepare_config_and_inputs_for_common with Llama->Gemma\n-    def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-        (\n-            config,\n-            input_ids,\n-            token_type_ids,\n-            input_mask,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-        ) = config_and_inputs\n-        inputs_dict = {\"input_ids\": input_ids, \"attention_mask\": input_mask}\n-        return config, inputs_dict\n+        base_model_class = GemmaModel\n+        causal_lm_class = GemmaForCausalLM\n+        sequence_classification_class = GemmaForSequenceClassification\n+        token_classification_class = GemmaForTokenClassification\n \n \n @require_torch\n-class GemmaModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+class GemmaModelTest(CausalLMModelTest, unittest.TestCase):\n     all_model_classes = (\n         (GemmaModel, GemmaForCausalLM, GemmaForSequenceClassification, GemmaForTokenClassification)\n         if is_torch_available()\n@@ -199,12 +75,7 @@ class GemmaModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixi\n         if is_torch_available()\n         else {}\n     )\n-    test_headmasking = False\n-    test_pruning = False\n-\n-    # Need to remove 0.9 in `test_cpu_offload`\n-    # This is because we are hitting edge cases with the causal_mask buffer\n-    model_split_percents = [0.5, 0.6]\n+    model_tester_class = GemmaModelTester\n \n     # used in `test_torch_compile_for_training`\n     _torch_compile_train_cls = GemmaForCausalLM if is_torch_available() else None\n@@ -222,125 +93,13 @@ def is_pipeline_test_to_skip(\n     ):\n         return True\n \n-    def setUp(self):\n-        self.model_tester = GemmaModelTester(self)\n-        self.config_tester = ConfigTester(self, config_class=GemmaConfig, hidden_size=37)\n-\n-    def test_config(self):\n-        self.config_tester.run_common_tests()\n-\n-    def test_model(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_model(*config_and_inputs)\n-\n-    def test_model_various_embeddings(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        for type in [\"absolute\", \"relative_key\", \"relative_key_query\"]:\n-            config_and_inputs[0].position_embedding_type = type\n-            self.model_tester.create_and_check_model(*config_and_inputs)\n-\n-    def test_Gemma_sequence_classification_model(self):\n-        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.num_labels = 3\n-        input_ids = input_dict[\"input_ids\"]\n-        attention_mask = input_ids.ne(1).to(torch_device)\n-        sequence_labels = ids_tensor([self.model_tester.batch_size], self.model_tester.type_sequence_label_size)\n-        model = self.model_tester.for_sequence_class(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=attention_mask, labels=sequence_labels)\n-        self.assertEqual(result.logits.shape, (self.model_tester.batch_size, self.model_tester.num_labels))\n-\n-    def test_Gemma_sequence_classification_model_for_single_label(self):\n-        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.num_labels = 3\n-        config.problem_type = \"single_label_classification\"\n-        input_ids = input_dict[\"input_ids\"]\n-        attention_mask = input_ids.ne(1).to(torch_device)\n-        sequence_labels = ids_tensor([self.model_tester.batch_size], self.model_tester.type_sequence_label_size)\n-        model = self.model_tester.for_sequence_class(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=attention_mask, labels=sequence_labels)\n-        self.assertEqual(result.logits.shape, (self.model_tester.batch_size, self.model_tester.num_labels))\n-\n-    def test_Gemma_sequence_classification_model_for_multi_label(self):\n-        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.num_labels = 3\n-        config.problem_type = \"multi_label_classification\"\n-        input_ids = input_dict[\"input_ids\"]\n-        attention_mask = input_ids.ne(1).to(torch_device)\n-        sequence_labels = ids_tensor(\n-            [self.model_tester.batch_size, config.num_labels], self.model_tester.type_sequence_label_size\n-        ).to(torch.float)\n-        model = self.model_tester.for_sequence_class(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=attention_mask, labels=sequence_labels)\n-        self.assertEqual(result.logits.shape, (self.model_tester.batch_size, self.model_tester.num_labels))\n-\n-    def test_Gemma_token_classification_model(self):\n-        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.num_labels = 3\n-        input_ids = input_dict[\"input_ids\"]\n-        attention_mask = input_ids.ne(1).to(torch_device)\n-        token_labels = ids_tensor([self.model_tester.batch_size, self.model_tester.seq_length], config.num_labels)\n-        model = self.model_tester.for_token_class(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=attention_mask, labels=token_labels)\n-        self.assertEqual(\n-            result.logits.shape,\n-            (self.model_tester.batch_size, self.model_tester.seq_length, self.model_tester.num_labels),\n-        )\n-\n     @require_flash_attn\n     @require_torch_gpu\n     @pytest.mark.flash_attn_test\n     @slow\n     def test_flash_attn_2_inference_equivalence_right_padding(self):\n         self.skipTest(reason=\"Gemma flash attention does not support right padding\")\n \n-    @require_torch_sdpa\n-    @require_torch_accelerator\n-    def test_sdpa_equivalence(self):\n-        for model_class in self.all_model_classes:\n-            if not model_class._supports_sdpa:\n-                self.skipTest(reason=\"Model does not support SDPA\")\n-\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-            model = model_class(config).to(torch_device)\n-            dummy_input = inputs_dict[model_class.main_input_name].to(torch_device)\n-\n-            model.config._attn_implementation = \"sdpa\"\n-            states_sdpa = model(dummy_input, output_hidden_states=True).hidden_states[-1]\n-\n-            model.config._attn_implementation = \"eager\"\n-            states_eager = model(dummy_input, output_hidden_states=True).hidden_states[-1]\n-\n-            torch.testing.assert_close(states_sdpa, states_eager, atol=1e-5, rtol=1e-5)\n-\n-    @require_flash_attn\n-    @require_torch_gpu\n-    @pytest.mark.flash_attn_test\n-    def test_flash_attn_2_equivalence(self):\n-        for model_class in self.all_model_classes:\n-            if not model_class._supports_flash_attn_2:\n-                self.skipTest(reason=\"Model does not support Flash Attention 2\")\n-\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-            model = model_class(config).to(device=torch_device, dtype=torch.float16)\n-            dummy_input = inputs_dict[model_class.main_input_name].to(torch_device)\n-\n-            model.config._attn_implementation = \"flash_attention_2\"\n-            states_sdpa = model(dummy_input, output_hidden_states=True).hidden_states[1]\n-\n-            model.config._attn_implementation = \"eager\"\n-            states_eager = model(dummy_input, output_hidden_states=True).hidden_states[1]\n-\n-            # Here we use higher tolerance and the output of the 2nd layer because otherwise small diffs add-up\n-            torch.testing.assert_close(states_sdpa, states_eager, atol=1e-3, rtol=1e-3)\n-\n \n @slow\n @require_torch_accelerator"
        },
        {
            "sha": "cb98a5a0e693823c1be3a23c7322cea3c41b6720",
            "filename": "tests/models/gemma2/test_modeling_gemma2.py",
            "status": "modified",
            "additions": 20,
            "deletions": 7,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/53fb245eb60364c7377c5f37fc37807a00e9b2e2/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/53fb245eb60364c7377c5f37fc37807a00e9b2e2/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py?ref=53fb245eb60364c7377c5f37fc37807a00e9b2e2",
            "patch": "@@ -33,7 +33,7 @@\n     torch_device,\n )\n \n-from ...models.gemma.test_modeling_gemma import GemmaModelTest, GemmaModelTester\n+from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n from ...test_configuration_common import ConfigTester\n \n \n@@ -48,17 +48,28 @@\n     )\n \n \n-class Gemma2ModelTester(GemmaModelTester):\n+class Gemma2ModelTester(CausalLMModelTester):\n     if is_torch_available():\n         config_class = Gemma2Config\n-        model_class = Gemma2Model\n-        for_causal_lm_class = Gemma2ForCausalLM\n-        for_sequence_class = Gemma2ForSequenceClassification\n-        for_token_class = Gemma2ForTokenClassification\n+        base_model_class = Gemma2Model\n+        causal_lm_class = Gemma2ForCausalLM\n+        sequence_class = Gemma2ForSequenceClassification\n+        token_class = Gemma2ForTokenClassification\n+    pipeline_model_mapping = (\n+        {\n+            \"feature-extraction\": Gemma2Model,\n+            \"text-classification\": Gemma2ForSequenceClassification,\n+            \"token-classification\": Gemma2ForTokenClassification,\n+            \"text-generation\": Gemma2ForCausalLM,\n+            \"zero-shot\": Gemma2ForSequenceClassification,\n+        }\n+        if is_torch_available()\n+        else {}\n+    )\n \n \n @require_torch\n-class Gemma2ModelTest(GemmaModelTest, unittest.TestCase):\n+class Gemma2ModelTest(CausalLMModelTest, unittest.TestCase):\n     all_model_classes = (\n         (Gemma2Model, Gemma2ForCausalLM, Gemma2ForSequenceClassification, Gemma2ForTokenClassification)\n         if is_torch_available()\n@@ -75,10 +86,12 @@ class Gemma2ModelTest(GemmaModelTest, unittest.TestCase):\n         if is_torch_available()\n         else {}\n     )\n+\n     test_headmasking = False\n     test_pruning = False\n     _is_stateful = True\n     model_split_percents = [0.5, 0.6]\n+    model_tester_class = Gemma2ModelTester\n \n     def setUp(self):\n         self.model_tester = Gemma2ModelTester(self)"
        },
        {
            "sha": "e246ea867a095f2a9a36f3504704bfc55c005eac",
            "filename": "tests/models/glm/test_modeling_glm.py",
            "status": "modified",
            "additions": 9,
            "deletions": 239,
            "changes": 248,
            "blob_url": "https://github.com/huggingface/transformers/blob/53fb245eb60364c7377c5f37fc37807a00e9b2e2/tests%2Fmodels%2Fglm%2Ftest_modeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/53fb245eb60364c7377c5f37fc37807a00e9b2e2/tests%2Fmodels%2Fglm%2Ftest_modeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fglm%2Ftest_modeling_glm.py?ref=53fb245eb60364c7377c5f37fc37807a00e9b2e2",
            "patch": "@@ -19,7 +19,6 @@\n \n from transformers import AutoModelForCausalLM, AutoTokenizer, GlmConfig, is_torch_available\n from transformers.testing_utils import (\n-    is_flaky,\n     require_flash_attn,\n     require_torch,\n     require_torch_large_accelerator,\n@@ -28,10 +27,7 @@\n     torch_device,\n )\n \n-from ...generation.test_utils import GenerationTesterMixin\n-from ...test_configuration_common import ConfigTester\n-from ...test_modeling_common import ModelTesterMixin, ids_tensor\n-from ...test_pipeline_mixin import PipelineTesterMixin\n+from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n \n \n if is_torch_available():\n@@ -46,133 +42,17 @@\n \n \n @require_torch\n-class GlmModelTester:\n+class GlmModelTester(CausalLMModelTester):\n     config_class = GlmConfig\n     if is_torch_available():\n-        model_class = GlmModel\n-        for_causal_lm_class = GlmForCausalLM\n-        for_sequence_class = GlmForSequenceClassification\n-        for_token_class = GlmForTokenClassification\n-\n-    def __init__(\n-        self,\n-        parent,\n-        batch_size=13,\n-        seq_length=7,\n-        is_training=True,\n-        use_input_mask=True,\n-        use_token_type_ids=False,\n-        use_labels=True,\n-        vocab_size=99,\n-        hidden_size=32,\n-        num_hidden_layers=2,\n-        num_attention_heads=4,\n-        num_key_value_heads=2,\n-        intermediate_size=37,\n-        hidden_act=\"silu\",\n-        attention_dropout=0.1,\n-        max_position_embeddings=512,\n-        type_vocab_size=16,\n-        type_sequence_label_size=2,\n-        initializer_range=0.02,\n-        num_labels=3,\n-        num_choices=4,\n-        pad_token_id=0,\n-        scope=None,\n-    ):\n-        self.parent = parent\n-        self.batch_size = batch_size\n-        self.seq_length = seq_length\n-        self.is_training = is_training\n-        self.use_input_mask = use_input_mask\n-        self.use_token_type_ids = use_token_type_ids\n-        self.use_labels = use_labels\n-        self.vocab_size = vocab_size\n-        self.hidden_size = hidden_size\n-        self.num_hidden_layers = num_hidden_layers\n-        self.num_attention_heads = num_attention_heads\n-        self.num_key_value_heads = num_key_value_heads\n-        self.intermediate_size = intermediate_size\n-        self.hidden_act = hidden_act\n-        self.attention_dropout = attention_dropout\n-        self.max_position_embeddings = max_position_embeddings\n-        self.type_vocab_size = type_vocab_size\n-        self.type_sequence_label_size = type_sequence_label_size\n-        self.initializer_range = initializer_range\n-        self.num_labels = num_labels\n-        self.num_choices = num_choices\n-        self.pad_token_id = pad_token_id\n-        self.scope = scope\n-        self.head_dim = self.hidden_size // self.num_attention_heads\n-\n-    # Copied from tests.models.mistral.test_modeling_mistral.MistralModelTester.prepare_config_and_inputs\n-    def prepare_config_and_inputs(self):\n-        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n-\n-        input_mask = None\n-        if self.use_input_mask:\n-            input_mask = torch.tril(torch.ones_like(input_ids).to(torch_device))\n-\n-        token_type_ids = None\n-        if self.use_token_type_ids:\n-            token_type_ids = ids_tensor([self.batch_size, self.seq_length], self.type_vocab_size)\n-\n-        sequence_labels = None\n-        token_labels = None\n-        choice_labels = None\n-        if self.use_labels:\n-            sequence_labels = ids_tensor([self.batch_size], self.type_sequence_label_size)\n-            token_labels = ids_tensor([self.batch_size, self.seq_length], self.num_labels)\n-            choice_labels = ids_tensor([self.batch_size], self.num_choices)\n-\n-        config = self.get_config()\n-\n-        return config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-\n-    def get_config(self):\n-        return self.config_class(\n-            vocab_size=self.vocab_size,\n-            hidden_size=self.hidden_size,\n-            num_hidden_layers=self.num_hidden_layers,\n-            num_attention_heads=self.num_attention_heads,\n-            num_key_value_heads=self.num_key_value_heads,\n-            intermediate_size=self.intermediate_size,\n-            hidden_act=self.hidden_act,\n-            attention_dropout=self.attention_dropout,\n-            max_position_embeddings=self.max_position_embeddings,\n-            initializer_range=self.initializer_range,\n-            pad_token_id=self.pad_token_id,\n-            head_dim=self.head_dim,\n-        )\n-\n-    def create_and_check_model(\n-        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        model = self.model_class(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=input_mask)\n-        result = model(input_ids)\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.prepare_config_and_inputs_for_common with Llama->Glm\n-    def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-        (\n-            config,\n-            input_ids,\n-            token_type_ids,\n-            input_mask,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-        ) = config_and_inputs\n-        inputs_dict = {\"input_ids\": input_ids, \"attention_mask\": input_mask}\n-        return config, inputs_dict\n+        base_model_class = GlmModel\n+        causal_lm_class = GlmForCausalLM\n+        sequence_class = GlmForSequenceClassification\n+        token_class = GlmForTokenClassification\n \n \n @require_torch\n-class GlmModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+class GlmModelTest(CausalLMModelTest, unittest.TestCase):\n     all_model_classes = (\n         (GlmModel, GlmForCausalLM, GlmForSequenceClassification, GlmForTokenClassification)\n         if is_torch_available()\n@@ -188,120 +68,10 @@ class GlmModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin,\n         if is_torch_available()\n         else {}\n     )\n+\n     test_headmasking = False\n     test_pruning = False\n-\n-    def setUp(self):\n-        self.model_tester = GlmModelTester(self)\n-        self.config_tester = ConfigTester(self, config_class=GlmConfig, hidden_size=37)\n-\n-    def test_config(self):\n-        self.config_tester.run_common_tests()\n-\n-    def test_model(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_model(*config_and_inputs)\n-\n-    def test_model_various_embeddings(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        for type in [\"absolute\", \"relative_key\", \"relative_key_query\"]:\n-            config_and_inputs[0].position_embedding_type = type\n-            self.model_tester.create_and_check_model(*config_and_inputs)\n-\n-    def test_Glm_sequence_classification_model(self):\n-        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        print(config)\n-        config.num_labels = 3\n-        input_ids = input_dict[\"input_ids\"]\n-        attention_mask = input_ids.ne(1).to(torch_device)\n-        sequence_labels = ids_tensor([self.model_tester.batch_size], self.model_tester.type_sequence_label_size)\n-        model = self.model_tester.for_sequence_class(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=attention_mask, labels=sequence_labels)\n-        self.assertEqual(result.logits.shape, (self.model_tester.batch_size, self.model_tester.num_labels))\n-\n-    def test_Glm_sequence_classification_model_for_single_label(self):\n-        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.num_labels = 3\n-        config.problem_type = \"single_label_classification\"\n-        input_ids = input_dict[\"input_ids\"]\n-        attention_mask = input_ids.ne(1).to(torch_device)\n-        sequence_labels = ids_tensor([self.model_tester.batch_size], self.model_tester.type_sequence_label_size)\n-        model = self.model_tester.for_sequence_class(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=attention_mask, labels=sequence_labels)\n-        self.assertEqual(result.logits.shape, (self.model_tester.batch_size, self.model_tester.num_labels))\n-\n-    def test_Glm_sequence_classification_model_for_multi_label(self):\n-        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.num_labels = 3\n-        config.problem_type = \"multi_label_classification\"\n-        input_ids = input_dict[\"input_ids\"]\n-        attention_mask = input_ids.ne(1).to(torch_device)\n-        sequence_labels = ids_tensor(\n-            [self.model_tester.batch_size, config.num_labels], self.model_tester.type_sequence_label_size\n-        ).to(torch.float)\n-        model = self.model_tester.for_sequence_class(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=attention_mask, labels=sequence_labels)\n-        self.assertEqual(result.logits.shape, (self.model_tester.batch_size, self.model_tester.num_labels))\n-\n-    def test_Glm_token_classification_model(self):\n-        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.num_labels = 3\n-        input_ids = input_dict[\"input_ids\"]\n-        attention_mask = input_ids.ne(1).to(torch_device)\n-        token_labels = ids_tensor([self.model_tester.batch_size, self.model_tester.seq_length], config.num_labels)\n-        model = self.model_tester.for_token_class(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=attention_mask, labels=token_labels)\n-        self.assertEqual(\n-            result.logits.shape,\n-            (self.model_tester.batch_size, self.model_tester.seq_length, self.model_tester.num_labels),\n-        )\n-\n-    @is_flaky()\n-    def test_custom_4d_attention_mask(self):\n-        \"\"\"Overwrite the common test to use atol=1e-3 instead of 1e-4. Can still rarely fail, thus flaky.\"\"\"\n-        for model_class in self.all_generative_model_classes:\n-            if not model_class._supports_static_cache:\n-                self.skipTest(f\"{model_class.__name__} is not guaranteed to work with custom 4D attention masks\")\n-            config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-            if getattr(config, \"sliding_window\", 0) is not None and getattr(config, \"sliding_window\", 0) > 0:\n-                self.skipTest(f\"{model_class.__name__} with sliding window attention is not supported by this test\")\n-            model = model_class(config).to(device=torch_device, dtype=torch.float32)\n-\n-            (\n-                input_ids,\n-                position_ids,\n-                input_ids_shared_prefix,\n-                mask_shared_prefix,\n-                position_ids_shared_prefix,\n-            ) = self._get_custom_4d_mask_test_data()\n-\n-            logits = model.forward(input_ids, position_ids=position_ids).logits\n-            # logits.shape == torch.Size([3, 4, ...])\n-\n-            logits_shared_prefix = model(\n-                input_ids_shared_prefix,\n-                attention_mask=mask_shared_prefix,\n-                position_ids=position_ids_shared_prefix,\n-            )[0]\n-            # logits_shared_prefix.shape == torch.Size([1, 6, ...])\n-\n-            out_last_tokens = logits[:, -1, :]  # last tokens in each batch line\n-            out_shared_prefix_last_tokens = logits_shared_prefix[0, -3:, :]  # last three tokens\n-\n-            # comparing softmax-normalized logits:\n-            normalized_0 = torch.nn.functional.softmax(out_last_tokens)\n-            normalized_1 = torch.nn.functional.softmax(out_shared_prefix_last_tokens)\n-            print(torch.abs(normalized_0 - normalized_1).max())\n-\n-            torch.testing.assert_close(normalized_0, normalized_1, rtol=1e-3, atol=1e-3)\n+    model_tester_class = GlmModelTester\n \n \n @slow"
        },
        {
            "sha": "295954fe20c3725b275a8080437c49aa9faa5512",
            "filename": "tests/models/glm4/test_modeling_glm4.py",
            "status": "modified",
            "additions": 8,
            "deletions": 12,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/53fb245eb60364c7377c5f37fc37807a00e9b2e2/tests%2Fmodels%2Fglm4%2Ftest_modeling_glm4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/53fb245eb60364c7377c5f37fc37807a00e9b2e2/tests%2Fmodels%2Fglm4%2Ftest_modeling_glm4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fglm4%2Ftest_modeling_glm4.py?ref=53fb245eb60364c7377c5f37fc37807a00e9b2e2",
            "patch": "@@ -28,8 +28,7 @@\n     torch_device,\n )\n \n-from ...models.gemma.test_modeling_gemma import GemmaModelTest, GemmaModelTester\n-from ...test_configuration_common import ConfigTester\n+from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n \n \n if is_torch_available():\n@@ -43,17 +42,18 @@\n     )\n \n \n-class Glm4ModelTester(GemmaModelTester):\n+class Glm4ModelTester(CausalLMModelTester):\n     if is_torch_available():\n         config_class = Glm4Config\n-        model_class = Glm4Model\n-        for_causal_lm_class = Glm4ForCausalLM\n-        for_sequence_class = Glm4ForSequenceClassification\n-        for_token_class = Glm4ForTokenClassification\n+        base_model_class = Glm4Model\n+        causal_lm_class = Glm4ForCausalLM\n+        sequence_classification_class = Glm4ForSequenceClassification\n+        token_classification_class = Glm4ForTokenClassification\n \n \n @require_torch\n-class Glm4ModelTest(GemmaModelTest, unittest.TestCase):\n+class Glm4ModelTest(CausalLMModelTest, unittest.TestCase):\n+    model_tester_class = Glm4ModelTester\n     all_model_classes = (\n         (Glm4Model, Glm4ForCausalLM, Glm4ForSequenceClassification, Glm4ForTokenClassification)\n         if is_torch_available()\n@@ -75,10 +75,6 @@ class Glm4ModelTest(GemmaModelTest, unittest.TestCase):\n     _is_stateful = True\n     model_split_percents = [0.5, 0.6]\n \n-    def setUp(self):\n-        self.model_tester = Glm4ModelTester(self)\n-        self.config_tester = ConfigTester(self, config_class=Glm4Config, hidden_size=37)\n-\n \n @slow\n @require_torch_large_gpu"
        },
        {
            "sha": "b0a0a6a3ccb487faab9a1b385d7f7732b17d8cbf",
            "filename": "tests/models/gpt_neox/test_modeling_gpt_neox.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/53fb245eb60364c7377c5f37fc37807a00e9b2e2/tests%2Fmodels%2Fgpt_neox%2Ftest_modeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/53fb245eb60364c7377c5f37fc37807a00e9b2e2/tests%2Fmodels%2Fgpt_neox%2Ftest_modeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt_neox%2Ftest_modeling_gpt_neox.py?ref=53fb245eb60364c7377c5f37fc37807a00e9b2e2",
            "patch": "@@ -341,7 +341,6 @@ def test_feed_forward_chunking(self):\n         pass\n \n     @parameterized.expand([(\"linear\",), (\"dynamic\",)])\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTest.test_model_rope_scaling_from_config with Llama->GPTNeoX\n     def test_model_rope_scaling_from_config(self, scaling_type):\n         config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n         short_input = ids_tensor([1, 10], config.vocab_size)"
        },
        {
            "sha": "7dd6ca728af0872adb64268c522668506d9b9835",
            "filename": "tests/models/jetmoe/test_modeling_jetmoe.py",
            "status": "modified",
            "additions": 18,
            "deletions": 143,
            "changes": 161,
            "blob_url": "https://github.com/huggingface/transformers/blob/53fb245eb60364c7377c5f37fc37807a00e9b2e2/tests%2Fmodels%2Fjetmoe%2Ftest_modeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/53fb245eb60364c7377c5f37fc37807a00e9b2e2/tests%2Fmodels%2Fjetmoe%2Ftest_modeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fjetmoe%2Ftest_modeling_jetmoe.py?ref=53fb245eb60364c7377c5f37fc37807a00e9b2e2",
            "patch": "@@ -28,10 +28,7 @@\n     torch_device,\n )\n \n-from ...generation.test_utils import GenerationTesterMixin\n-from ...test_configuration_common import ConfigTester\n-from ...test_modeling_common import ModelTesterMixin, ids_tensor\n-from ...test_pipeline_mixin import PipelineTesterMixin\n+from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n \n \n if is_torch_available():\n@@ -44,7 +41,14 @@\n     )\n \n \n-class JetMoeModelTester:\n+class JetMoeModelTester(CausalLMModelTester):\n+    config_class = JetMoeConfig\n+    forced_config_args = [\"pad_token_id\"]\n+    if is_torch_available():\n+        base_model_class = JetMoeModel\n+        causal_lm_class = JetMoeForCausalLM\n+        sequence_class = JetMoeForSequenceClassification\n+\n     def __init__(\n         self,\n         parent,\n@@ -72,6 +76,7 @@ def __init__(\n         pad_token_id=0,\n         scope=None,\n     ):\n+        super().__init__(parent)\n         self.parent = parent\n         self.batch_size = batch_size\n         self.seq_length = seq_length\n@@ -98,158 +103,28 @@ def __init__(\n         self.pad_token_id = pad_token_id\n         self.scope = scope\n \n-    def prepare_config_and_inputs(self):\n-        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n-\n-        input_mask = None\n-        if self.use_input_mask:\n-            input_mask = torch.ones(self.batch_size, self.seq_length).to(torch_device)\n-\n-        token_type_ids = None\n-        if self.use_token_type_ids:\n-            token_type_ids = ids_tensor([self.batch_size, self.seq_length], self.type_vocab_size)\n-\n-        sequence_labels = None\n-        token_labels = None\n-        choice_labels = None\n-        if self.use_labels:\n-            sequence_labels = ids_tensor([self.batch_size], self.type_sequence_label_size)\n-            token_labels = ids_tensor([self.batch_size, self.seq_length], self.num_labels)\n-            choice_labels = ids_tensor([self.batch_size], self.num_choices)\n-\n-        config = self.get_config()\n-\n-        return config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-\n-    def get_config(self):\n-        return JetMoeConfig(\n-            vocab_size=self.vocab_size,\n-            hidden_size=self.hidden_size,\n-            num_hidden_layers=self.num_hidden_layers,\n-            num_key_value_heads=self.num_key_value_heads,\n-            kv_channels=self.kv_channels,\n-            intermediate_size=self.intermediate_size,\n-            activation_function=self.hidden_act,\n-            num_local_experts=self.num_local_experts,\n-            num_experts_per_tok=self.num_experts_per_tok,\n-            max_position_embeddings=self.max_position_embeddings,\n-            type_vocab_size=self.type_vocab_size,\n-            is_decoder=False,\n-            initializer_range=self.initializer_range,\n-            pad_token_id=self.pad_token_id,\n-        )\n-\n-    def create_and_check_model(\n-        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        model = JetMoeModel(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=input_mask)\n-        result = model(input_ids)\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-\n-    def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-        (\n-            config,\n-            input_ids,\n-            token_type_ids,\n-            input_mask,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-        ) = config_and_inputs\n-        inputs_dict = {\"input_ids\": input_ids, \"attention_mask\": input_mask}\n-        return config, inputs_dict\n-\n \n @require_torch\n-class JetMoeModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+class JetMoeModelTest(CausalLMModelTest, unittest.TestCase):\n     all_model_classes = (\n         (JetMoeModel, JetMoeForCausalLM, JetMoeForSequenceClassification) if is_torch_available() else ()\n     )\n+    test_headmasking = False\n+    test_pruning = False\n+    test_mismatched_shapes = False\n+    test_cpu_offload = False\n+    test_disk_offload_bin = False\n+    test_disk_offload_safetensors = False\n+    model_tester_class = JetMoeModelTester\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": JetMoeModel,\n             \"text-classification\": JetMoeForSequenceClassification,\n             \"text-generation\": JetMoeForCausalLM,\n-            \"zero-shot\": JetMoeForSequenceClassification,\n         }\n         if is_torch_available()\n         else {}\n     )\n-    test_headmasking = False\n-    test_pruning = False\n-    test_mismatched_shapes = False\n-    test_cpu_offload = False\n-    test_disk_offload_bin = False\n-    test_disk_offload_safetensors = False\n-\n-    def setUp(self):\n-        self.model_tester = JetMoeModelTester(self)\n-        self.config_tester = ConfigTester(\n-            self, config_class=JetMoeConfig, common_properties=[\"hidden_size\", \"num_hidden_layers\"]\n-        )\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTest.test_config\n-    def test_config(self):\n-        self.config_tester.run_common_tests()\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTest.test_model\n-    def test_model(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_model(*config_and_inputs)\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTest.test_model_various_embeddings\n-    def test_model_various_embeddings(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        for type in [\"absolute\", \"relative_key\", \"relative_key_query\"]:\n-            config_and_inputs[0].position_embedding_type = type\n-            self.model_tester.create_and_check_model(*config_and_inputs)\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTest.test_llama_sequence_classification_model with llama->jetmoe, Llama->JetMoe\n-    def test_jetmoe_sequence_classification_model(self):\n-        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.num_labels = 3\n-        input_ids = input_dict[\"input_ids\"]\n-        attention_mask = input_ids.ne(1).to(torch_device)\n-        sequence_labels = ids_tensor([self.model_tester.batch_size], self.model_tester.type_sequence_label_size)\n-        model = JetMoeForSequenceClassification(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=attention_mask, labels=sequence_labels)\n-        self.assertEqual(result.logits.shape, (self.model_tester.batch_size, self.model_tester.num_labels))\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTest.test_llama_sequence_classification_model_for_single_label with llama->jetmoe, Llama->JetMoe\n-    def test_jetmoe_sequence_classification_model_for_single_label(self):\n-        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.num_labels = 3\n-        config.problem_type = \"single_label_classification\"\n-        input_ids = input_dict[\"input_ids\"]\n-        attention_mask = input_ids.ne(1).to(torch_device)\n-        sequence_labels = ids_tensor([self.model_tester.batch_size], self.model_tester.type_sequence_label_size)\n-        model = JetMoeForSequenceClassification(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=attention_mask, labels=sequence_labels)\n-        self.assertEqual(result.logits.shape, (self.model_tester.batch_size, self.model_tester.num_labels))\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTest.test_llama_sequence_classification_model_for_multi_label with llama->jetmoe, Llama->JetMoe\n-    def test_jetmoe_sequence_classification_model_for_multi_label(self):\n-        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.num_labels = 3\n-        config.problem_type = \"multi_label_classification\"\n-        input_ids = input_dict[\"input_ids\"]\n-        attention_mask = input_ids.ne(1).to(torch_device)\n-        sequence_labels = ids_tensor(\n-            [self.model_tester.batch_size, config.num_labels], self.model_tester.type_sequence_label_size\n-        ).to(torch.float)\n-        model = JetMoeForSequenceClassification(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=attention_mask, labels=sequence_labels)\n-        self.assertEqual(result.logits.shape, (self.model_tester.batch_size, self.model_tester.num_labels))\n \n     @require_flash_attn\n     @require_torch_gpu"
        },
        {
            "sha": "a1e6c9444707d90e09be369db42c8fca9ba99279",
            "filename": "tests/models/llama/test_modeling_llama.py",
            "status": "modified",
            "additions": 13,
            "deletions": 345,
            "changes": 358,
            "blob_url": "https://github.com/huggingface/transformers/blob/53fb245eb60364c7377c5f37fc37807a00e9b2e2/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/53fb245eb60364c7377c5f37fc37807a00e9b2e2/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py?ref=53fb245eb60364c7377c5f37fc37807a00e9b2e2",
            "patch": "@@ -16,9 +16,8 @@\n import unittest\n \n from packaging import version\n-from parameterized import parameterized\n \n-from transformers import AutoTokenizer, LlamaConfig, StaticCache, is_torch_available, set_seed\n+from transformers import AutoTokenizer, StaticCache, is_torch_available\n from transformers.generation.configuration_utils import GenerationConfig\n from transformers.testing_utils import (\n     Expectations,\n@@ -30,16 +29,14 @@\n     torch_device,\n )\n \n-from ...generation.test_utils import GenerationTesterMixin\n-from ...test_configuration_common import ConfigTester\n-from ...test_modeling_common import ModelTesterMixin, ids_tensor\n-from ...test_pipeline_mixin import PipelineTesterMixin\n+from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n \n \n if is_torch_available():\n     import torch\n \n     from transformers import (\n+        LlamaConfig,\n         LlamaForCausalLM,\n         LlamaForQuestionAnswering,\n         LlamaForSequenceClassification,\n@@ -50,124 +47,17 @@\n     from transformers.models.llama.modeling_llama import LlamaRotaryEmbedding\n \n \n-class LlamaModelTester:\n-    def __init__(\n-        self,\n-        parent,\n-        batch_size=13,\n-        seq_length=7,\n-        is_training=True,\n-        use_input_mask=True,\n-        use_token_type_ids=False,\n-        use_labels=True,\n-        vocab_size=99,\n-        hidden_size=32,\n-        num_hidden_layers=2,\n-        num_attention_heads=4,\n-        intermediate_size=37,\n-        hidden_act=\"gelu\",\n-        hidden_dropout_prob=0.1,\n-        attention_probs_dropout_prob=0.1,\n-        max_position_embeddings=512,\n-        type_vocab_size=16,\n-        type_sequence_label_size=2,\n-        initializer_range=0.02,\n-        num_labels=3,\n-        num_choices=4,\n-        pad_token_id=0,\n-        scope=None,\n-    ):\n-        self.parent = parent\n-        self.batch_size = batch_size\n-        self.seq_length = seq_length\n-        self.is_training = is_training\n-        self.use_input_mask = use_input_mask\n-        self.use_token_type_ids = use_token_type_ids\n-        self.use_labels = use_labels\n-        self.vocab_size = vocab_size\n-        self.hidden_size = hidden_size\n-        self.num_hidden_layers = num_hidden_layers\n-        self.num_attention_heads = num_attention_heads\n-        self.intermediate_size = intermediate_size\n-        self.hidden_act = hidden_act\n-        self.hidden_dropout_prob = hidden_dropout_prob\n-        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n-        self.max_position_embeddings = max_position_embeddings\n-        self.type_vocab_size = type_vocab_size\n-        self.type_sequence_label_size = type_sequence_label_size\n-        self.initializer_range = initializer_range\n-        self.num_labels = num_labels\n-        self.num_choices = num_choices\n-        self.pad_token_id = pad_token_id\n-        self.scope = scope\n-\n-    def prepare_config_and_inputs(self):\n-        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n-\n-        input_mask = None\n-        if self.use_input_mask:\n-            input_mask = torch.tril(torch.ones_like(input_ids).to(torch_device))\n-\n-        token_type_ids = None\n-        if self.use_token_type_ids:\n-            token_type_ids = ids_tensor([self.batch_size, self.seq_length], self.type_vocab_size)\n-\n-        sequence_labels = None\n-        token_labels = None\n-        choice_labels = None\n-        if self.use_labels:\n-            sequence_labels = ids_tensor([self.batch_size], self.type_sequence_label_size)\n-            token_labels = ids_tensor([self.batch_size, self.seq_length], self.num_labels)\n-            choice_labels = ids_tensor([self.batch_size], self.num_choices)\n-\n-        config = self.get_config()\n-\n-        return config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-\n-    def get_config(self):\n-        return LlamaConfig(\n-            vocab_size=self.vocab_size,\n-            hidden_size=self.hidden_size,\n-            num_hidden_layers=self.num_hidden_layers,\n-            num_attention_heads=self.num_attention_heads,\n-            intermediate_size=self.intermediate_size,\n-            hidden_act=self.hidden_act,\n-            hidden_dropout_prob=self.hidden_dropout_prob,\n-            attention_probs_dropout_prob=self.attention_probs_dropout_prob,\n-            max_position_embeddings=self.max_position_embeddings,\n-            type_vocab_size=self.type_vocab_size,\n-            is_decoder=False,\n-            initializer_range=self.initializer_range,\n-            pad_token_id=self.pad_token_id,\n-        )\n-\n-    def create_and_check_model(\n-        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        model = LlamaModel(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=input_mask)\n-        result = model(input_ids)\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-\n-    def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-        (\n-            config,\n-            input_ids,\n-            token_type_ids,\n-            input_mask,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-        ) = config_and_inputs\n-        inputs_dict = {\"input_ids\": input_ids, \"attention_mask\": input_mask}\n-        return config, inputs_dict\n+class LlamaModelTester(CausalLMModelTester):\n+    if is_torch_available():\n+        config_class = LlamaConfig\n+        base_model_class = LlamaModel\n+        causal_lm_class = LlamaForCausalLM\n+        sequence_class = LlamaForSequenceClassification\n+        token_class = LlamaForTokenClassification\n \n \n @require_torch\n-class LlamaModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+class LlamaModelTest(CausalLMModelTest, unittest.TestCase):\n     all_model_classes = (\n         (\n             LlamaModel,\n@@ -194,6 +84,8 @@ class LlamaModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixi\n     test_headmasking = False\n     test_pruning = False\n     fx_compatible = False  # Broken by attention refactor cc @Cyrilvallez\n+    model_tester_class = LlamaModelTester\n+    rotary_embedding_layer = LlamaRotaryEmbedding  # Enables RoPE tests if set\n \n     # Need to use `0.8` instead of `0.9` for `test_cpu_offload`\n     # This is because we are hitting edge cases with the causal_mask buffer\n@@ -202,230 +94,6 @@ class LlamaModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixi\n     # used in `test_torch_compile_for_training`\n     _torch_compile_train_cls = LlamaForCausalLM if is_torch_available() else None\n \n-    def setUp(self):\n-        self.model_tester = LlamaModelTester(self)\n-        self.config_tester = ConfigTester(self, config_class=LlamaConfig, hidden_size=37)\n-\n-    def test_config(self):\n-        self.config_tester.run_common_tests()\n-\n-    def test_model(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_model(*config_and_inputs)\n-\n-    def test_model_various_embeddings(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        for type in [\"absolute\", \"relative_key\", \"relative_key_query\"]:\n-            config_and_inputs[0].position_embedding_type = type\n-            self.model_tester.create_and_check_model(*config_and_inputs)\n-\n-    def test_llama_sequence_classification_model(self):\n-        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.num_labels = 3\n-        input_ids = input_dict[\"input_ids\"]\n-        attention_mask = input_ids.ne(1).to(torch_device)\n-        sequence_labels = ids_tensor([self.model_tester.batch_size], self.model_tester.type_sequence_label_size)\n-        model = LlamaForSequenceClassification(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=attention_mask, labels=sequence_labels)\n-        self.assertEqual(result.logits.shape, (self.model_tester.batch_size, self.model_tester.num_labels))\n-\n-    def test_llama_sequence_classification_model_for_single_label(self):\n-        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.num_labels = 3\n-        config.problem_type = \"single_label_classification\"\n-        input_ids = input_dict[\"input_ids\"]\n-        attention_mask = input_ids.ne(1).to(torch_device)\n-        sequence_labels = ids_tensor([self.model_tester.batch_size], self.model_tester.type_sequence_label_size)\n-        model = LlamaForSequenceClassification(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=attention_mask, labels=sequence_labels)\n-        self.assertEqual(result.logits.shape, (self.model_tester.batch_size, self.model_tester.num_labels))\n-\n-    def test_llama_sequence_classification_model_for_multi_label(self):\n-        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.num_labels = 3\n-        config.problem_type = \"multi_label_classification\"\n-        input_ids = input_dict[\"input_ids\"]\n-        attention_mask = input_ids.ne(1).to(torch_device)\n-        sequence_labels = ids_tensor(\n-            [self.model_tester.batch_size, config.num_labels], self.model_tester.type_sequence_label_size\n-        ).to(torch.float)\n-        model = LlamaForSequenceClassification(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=attention_mask, labels=sequence_labels)\n-        self.assertEqual(result.logits.shape, (self.model_tester.batch_size, self.model_tester.num_labels))\n-\n-    def test_llama_token_classification_model(self):\n-        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.num_labels = 3\n-        input_ids = input_dict[\"input_ids\"]\n-        attention_mask = input_ids.ne(1).to(torch_device)\n-        token_labels = ids_tensor([self.model_tester.batch_size, self.model_tester.seq_length], config.num_labels)\n-        model = LlamaForTokenClassification(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=attention_mask, labels=token_labels)\n-        self.assertEqual(\n-            result.logits.shape,\n-            (self.model_tester.batch_size, self.model_tester.seq_length, self.model_tester.num_labels),\n-        )\n-\n-    @parameterized.expand([(\"linear\",), (\"dynamic\",), (\"yarn\",)])\n-    def test_model_rope_scaling_from_config(self, scaling_type):\n-        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-        short_input = ids_tensor([1, 10], config.vocab_size)\n-        long_input = ids_tensor([1, int(config.max_position_embeddings * 1.5)], config.vocab_size)\n-\n-        set_seed(42)  # Fixed seed at init time so the two models get the same random weights\n-        original_model = LlamaModel(config)\n-        original_model.to(torch_device)\n-        original_model.eval()\n-        original_short_output = original_model(short_input).last_hidden_state\n-        original_long_output = original_model(long_input).last_hidden_state\n-\n-        set_seed(42)  # Fixed seed at init time so the two models get the same random weights\n-        config.rope_scaling = {\"type\": scaling_type, \"factor\": 10.0}\n-        scaled_model = LlamaModel(config)\n-        scaled_model.to(torch_device)\n-        scaled_model.eval()\n-        scaled_short_output = scaled_model(short_input).last_hidden_state\n-        scaled_long_output = scaled_model(long_input).last_hidden_state\n-\n-        # Dynamic scaling does not change the RoPE embeddings until it receives an input longer than the original\n-        # maximum sequence length, so the outputs for the short input should match.\n-        if scaling_type == \"dynamic\":\n-            torch.testing.assert_close(original_short_output, scaled_short_output, rtol=1e-5, atol=1e-5)\n-        else:\n-            self.assertFalse(torch.allclose(original_short_output, scaled_short_output, atol=1e-5))\n-\n-        # The output should be different for long inputs\n-        self.assertFalse(torch.allclose(original_long_output, scaled_long_output, atol=1e-5))\n-\n-    def test_model_rope_scaling(self):\n-        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-        scaling_factor = 10\n-        short_input_length = 10\n-        long_input_length = int(config.max_position_embeddings * 1.5)\n-\n-        # Inputs\n-        x = torch.randn(\n-            1, dtype=torch.float32, device=torch_device\n-        )  # used exclusively to get the dtype and the device\n-        position_ids_short = torch.arange(short_input_length, dtype=torch.long, device=torch_device)\n-        position_ids_short = position_ids_short.unsqueeze(0)\n-        position_ids_long = torch.arange(long_input_length, dtype=torch.long, device=torch_device)\n-        position_ids_long = position_ids_long.unsqueeze(0)\n-\n-        # Sanity check original RoPE\n-        original_rope = LlamaRotaryEmbedding(config=config).to(torch_device)\n-        original_cos_short, original_sin_short = original_rope(x, position_ids_short)\n-        original_cos_long, original_sin_long = original_rope(x, position_ids_long)\n-        torch.testing.assert_close(original_cos_short, original_cos_long[:, :short_input_length, :])\n-        torch.testing.assert_close(original_sin_short, original_sin_long[:, :short_input_length, :])\n-\n-        # Sanity check linear RoPE scaling\n-        # New position \"x\" should match original position with index \"x/scaling_factor\"\n-        config.rope_scaling = {\"type\": \"linear\", \"factor\": scaling_factor}\n-        linear_scaling_rope = LlamaRotaryEmbedding(config=config).to(torch_device)\n-        linear_cos_short, linear_sin_short = linear_scaling_rope(x, position_ids_short)\n-        linear_cos_long, linear_sin_long = linear_scaling_rope(x, position_ids_long)\n-        torch.testing.assert_close(linear_cos_short, linear_cos_long[:, :short_input_length, :])\n-        torch.testing.assert_close(linear_sin_short, linear_sin_long[:, :short_input_length, :])\n-        for new_position in range(0, long_input_length, scaling_factor):\n-            original_position = int(new_position // scaling_factor)\n-            torch.testing.assert_close(linear_cos_long[:, new_position, :], original_cos_long[:, original_position, :])\n-            torch.testing.assert_close(linear_sin_long[:, new_position, :], original_sin_long[:, original_position, :])\n-\n-        # Sanity check Dynamic NTK RoPE scaling\n-        # Scaling should only be observed after a long input is fed. We can observe that the frequencies increase\n-        # with scaling_factor (or that `inv_freq` decreases)\n-        config.rope_scaling = {\"type\": \"dynamic\", \"factor\": scaling_factor}\n-        ntk_scaling_rope = LlamaRotaryEmbedding(config=config).to(torch_device)\n-        ntk_cos_short, ntk_sin_short = ntk_scaling_rope(x, position_ids_short)\n-        ntk_cos_long, ntk_sin_long = ntk_scaling_rope(x, position_ids_long)\n-        torch.testing.assert_close(ntk_cos_short, original_cos_short)\n-        torch.testing.assert_close(ntk_sin_short, original_sin_short)\n-        with self.assertRaises(AssertionError):\n-            torch.testing.assert_close(ntk_cos_long, original_cos_long)\n-        with self.assertRaises(AssertionError):\n-            torch.testing.assert_close(ntk_sin_long, original_sin_long)\n-        self.assertTrue((ntk_scaling_rope.inv_freq <= original_rope.inv_freq).all())\n-\n-        # Sanity check Yarn RoPE scaling\n-        # Scaling should be over the entire input\n-        config.rope_scaling = {\"type\": \"yarn\", \"factor\": scaling_factor}\n-        yarn_scaling_rope = LlamaRotaryEmbedding(config=config).to(torch_device)\n-        yarn_cos_short, yarn_sin_short = yarn_scaling_rope(x, position_ids_short)\n-        yarn_cos_long, yarn_sin_long = yarn_scaling_rope(x, position_ids_long)\n-        torch.testing.assert_close(yarn_cos_short, yarn_cos_long[:, :short_input_length, :])\n-        torch.testing.assert_close(yarn_sin_short, yarn_sin_long[:, :short_input_length, :])\n-        with self.assertRaises(AssertionError):\n-            torch.testing.assert_close(yarn_cos_short, original_cos_short)\n-        with self.assertRaises(AssertionError):\n-            torch.testing.assert_close(yarn_sin_short, original_sin_short)\n-        with self.assertRaises(AssertionError):\n-            torch.testing.assert_close(yarn_cos_long, original_cos_long)\n-        with self.assertRaises(AssertionError):\n-            torch.testing.assert_close(yarn_sin_long, original_sin_long)\n-\n-    def test_model_loading_old_rope_configs(self):\n-        def _reinitialize_config(base_config, new_kwargs):\n-            # Reinitialize the config with the new kwargs, forcing the config to go through its __init__ validation\n-            # steps.\n-            base_config_dict = base_config.to_dict()\n-            new_config = LlamaConfig.from_dict(config_dict={**base_config_dict, **new_kwargs})\n-            return new_config\n-\n-        # from untouched config -> \n-        base_config, model_inputs = self.model_tester.prepare_config_and_inputs_for_common()\n-        original_model = LlamaForCausalLM(base_config).to(torch_device)\n-        original_model(**model_inputs)\n-\n-        # from a config with the expected rope configuration -> \n-        config = _reinitialize_config(base_config, {\"rope_scaling\": {\"rope_type\": \"linear\", \"factor\": 10.0}})\n-        original_model = LlamaForCausalLM(config).to(torch_device)\n-        original_model(**model_inputs)\n-\n-        # from a config with the old rope configuration ('type' instead of 'rope_type')  ->  we gracefully handle BC\n-        config = _reinitialize_config(base_config, {\"rope_scaling\": {\"type\": \"linear\", \"factor\": 10.0}})\n-        original_model = LlamaForCausalLM(config).to(torch_device)\n-        original_model(**model_inputs)\n-\n-        # from a config with both 'type' and 'rope_type'  ->  they can coexist (and both are present in the config)\n-        config = _reinitialize_config(\n-            base_config, {\"rope_scaling\": {\"type\": \"linear\", \"rope_type\": \"linear\", \"factor\": 10.0}}\n-        )\n-        self.assertTrue(config.rope_scaling[\"type\"] == \"linear\")\n-        self.assertTrue(config.rope_scaling[\"rope_type\"] == \"linear\")\n-        original_model = LlamaForCausalLM(config).to(torch_device)\n-        original_model(**model_inputs)\n-\n-        # from a config with parameters in a bad range ('factor' should be >= 1.0) ->  throws a warning\n-        with self.assertLogs(\"transformers.modeling_rope_utils\", level=\"WARNING\") as logs:\n-            config = _reinitialize_config(base_config, {\"rope_scaling\": {\"rope_type\": \"linear\", \"factor\": -999.0}})\n-            original_model = LlamaForCausalLM(config).to(torch_device)\n-            original_model(**model_inputs)\n-            self.assertEqual(len(logs.output), 1)\n-            self.assertIn(\"factor field\", logs.output[0])\n-\n-        # from a config with unknown parameters ('foo' isn't a rope option) ->  throws a warning\n-        with self.assertLogs(\"transformers.modeling_rope_utils\", level=\"WARNING\") as logs:\n-            config = _reinitialize_config(\n-                base_config, {\"rope_scaling\": {\"rope_type\": \"linear\", \"factor\": 10.0, \"foo\": \"bar\"}}\n-            )\n-            original_model = LlamaForCausalLM(config).to(torch_device)\n-            original_model(**model_inputs)\n-            self.assertEqual(len(logs.output), 1)\n-            self.assertIn(\"Unrecognized keys\", logs.output[0])\n-\n-        # from a config with specific rope type but missing one of its mandatory parameters ->  throws exception\n-        with self.assertRaises(KeyError):\n-            config = _reinitialize_config(base_config, {\"rope_scaling\": {\"rope_type\": \"linear\"}})  # missing \"factor\"\n-\n \n @require_torch_accelerator\n class LlamaIntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "bb5a24c3cec0df475a89abbcdf9816914c4b147f",
            "filename": "tests/models/mistral/test_modeling_mistral.py",
            "status": "modified",
            "additions": 11,
            "deletions": 203,
            "changes": 214,
            "blob_url": "https://github.com/huggingface/transformers/blob/53fb245eb60364c7377c5f37fc37807a00e9b2e2/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/53fb245eb60364c7377c5f37fc37807a00e9b2e2/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py?ref=53fb245eb60364c7377c5f37fc37807a00e9b2e2",
            "patch": "@@ -34,11 +34,6 @@\n     torch_device,\n )\n \n-from ...generation.test_utils import GenerationTesterMixin\n-from ...test_configuration_common import ConfigTester\n-from ...test_modeling_common import ModelTesterMixin, ids_tensor\n-from ...test_pipeline_mixin import PipelineTesterMixin\n-\n \n if is_torch_available():\n     import torch\n@@ -51,131 +46,21 @@\n         MistralModel,\n     )\n \n+from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n \n-class MistralModelTester:\n-    def __init__(\n-        self,\n-        parent,\n-        batch_size=13,\n-        seq_length=7,\n-        is_training=True,\n-        use_input_mask=True,\n-        use_token_type_ids=False,\n-        use_labels=True,\n-        vocab_size=99,\n-        hidden_size=32,\n-        num_hidden_layers=2,\n-        num_attention_heads=4,\n-        num_key_value_heads=2,\n-        intermediate_size=37,\n-        hidden_act=\"gelu\",\n-        hidden_dropout_prob=0.1,\n-        attention_probs_dropout_prob=0.1,\n-        max_position_embeddings=512,\n-        type_vocab_size=16,\n-        type_sequence_label_size=2,\n-        initializer_range=0.02,\n-        num_labels=3,\n-        num_choices=4,\n-        pad_token_id=0,\n-        scope=None,\n-    ):\n-        self.parent = parent\n-        self.batch_size = batch_size\n-        self.seq_length = seq_length\n-        self.is_training = is_training\n-        self.use_input_mask = use_input_mask\n-        self.use_token_type_ids = use_token_type_ids\n-        self.use_labels = use_labels\n-        self.vocab_size = vocab_size\n-        self.hidden_size = hidden_size\n-        self.num_hidden_layers = num_hidden_layers\n-        self.num_attention_heads = num_attention_heads\n-        self.num_key_value_heads = num_key_value_heads\n-        self.intermediate_size = intermediate_size\n-        self.hidden_act = hidden_act\n-        self.hidden_dropout_prob = hidden_dropout_prob\n-        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n-        self.max_position_embeddings = max_position_embeddings\n-        self.type_vocab_size = type_vocab_size\n-        self.type_sequence_label_size = type_sequence_label_size\n-        self.initializer_range = initializer_range\n-        self.num_labels = num_labels\n-        self.num_choices = num_choices\n-        self.pad_token_id = pad_token_id\n-        self.scope = scope\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.prepare_config_and_inputs\n-    def prepare_config_and_inputs(self):\n-        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n-\n-        input_mask = None\n-        if self.use_input_mask:\n-            input_mask = torch.tril(torch.ones_like(input_ids).to(torch_device))\n-\n-        token_type_ids = None\n-        if self.use_token_type_ids:\n-            token_type_ids = ids_tensor([self.batch_size, self.seq_length], self.type_vocab_size)\n-\n-        sequence_labels = None\n-        token_labels = None\n-        choice_labels = None\n-        if self.use_labels:\n-            sequence_labels = ids_tensor([self.batch_size], self.type_sequence_label_size)\n-            token_labels = ids_tensor([self.batch_size, self.seq_length], self.num_labels)\n-            choice_labels = ids_tensor([self.batch_size], self.num_choices)\n-\n-        config = self.get_config()\n-\n-        return config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-\n-    def get_config(self):\n-        return MistralConfig(\n-            vocab_size=self.vocab_size,\n-            hidden_size=self.hidden_size,\n-            num_hidden_layers=self.num_hidden_layers,\n-            num_attention_heads=self.num_attention_heads,\n-            num_key_value_heads=self.num_key_value_heads,\n-            intermediate_size=self.intermediate_size,\n-            hidden_act=self.hidden_act,\n-            hidden_dropout_prob=self.hidden_dropout_prob,\n-            attention_probs_dropout_prob=self.attention_probs_dropout_prob,\n-            max_position_embeddings=self.max_position_embeddings,\n-            type_vocab_size=self.type_vocab_size,\n-            is_decoder=False,\n-            initializer_range=self.initializer_range,\n-            pad_token_id=self.pad_token_id,\n-        )\n \n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.create_and_check_model with Llama->Mistral\n-    def create_and_check_model(\n-        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        model = MistralModel(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=input_mask)\n-        result = model(input_ids)\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.prepare_config_and_inputs_for_common\n-    def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-        (\n-            config,\n-            input_ids,\n-            token_type_ids,\n-            input_mask,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-        ) = config_and_inputs\n-        inputs_dict = {\"input_ids\": input_ids, \"attention_mask\": input_mask}\n-        return config, inputs_dict\n+class MistralModelTester(CausalLMModelTester):\n+    config_class = MistralConfig\n+    if is_torch_available():\n+        base_model_class = MistralModel\n+        causal_lm_class = MistralForCausalLM\n+        sequence_class = MistralForSequenceClassification\n+        token_class = MistralForTokenClassification\n+        question_answering_class = MistralForQuestionAnswering\n \n \n @require_torch\n-class MistralModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+class MistralModelTest(CausalLMModelTest, unittest.TestCase):\n     all_model_classes = (\n         (\n             MistralModel,\n@@ -193,15 +78,14 @@ class MistralModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMi\n             \"text-classification\": MistralForSequenceClassification,\n             \"token-classification\": MistralForTokenClassification,\n             \"text-generation\": MistralForCausalLM,\n-            \"zero-shot\": MistralForSequenceClassification,\n             \"question-answering\": MistralForQuestionAnswering,\n         }\n         if is_torch_available()\n         else {}\n     )\n     test_headmasking = False\n     test_pruning = False\n-    fx_compatible = False  # Broken by attention refactor cc @Cyrilvallez\n+    model_tester_class = MistralModelTester\n \n     # TODO (ydshieh): Check this. See https://app.circleci.com/pipelines/github/huggingface/transformers/79245/workflows/9490ef58-79c2-410d-8f51-e3495156cf9c/jobs/1012146\n     def is_pipeline_test_to_skip(\n@@ -216,82 +100,6 @@ def is_pipeline_test_to_skip(\n     ):\n         return True\n \n-    def setUp(self):\n-        self.model_tester = MistralModelTester(self)\n-        self.config_tester = ConfigTester(self, config_class=MistralConfig, hidden_size=37)\n-\n-    def test_config(self):\n-        self.config_tester.run_common_tests()\n-\n-    def test_model(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_model(*config_and_inputs)\n-\n-    def test_model_various_embeddings(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        for type in [\"absolute\", \"relative_key\", \"relative_key_query\"]:\n-            config_and_inputs[0].position_embedding_type = type\n-            self.model_tester.create_and_check_model(*config_and_inputs)\n-\n-    def test_torch_fx_output_loss(self):\n-        super().test_torch_fx_output_loss()\n-\n-    def test_Mistral_sequence_classification_model(self):\n-        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.num_labels = 3\n-        input_ids = input_dict[\"input_ids\"]\n-        attention_mask = input_ids.ne(1).to(torch_device)\n-        sequence_labels = ids_tensor([self.model_tester.batch_size], self.model_tester.type_sequence_label_size)\n-        model = MistralForSequenceClassification(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=attention_mask, labels=sequence_labels)\n-        self.assertEqual(result.logits.shape, (self.model_tester.batch_size, self.model_tester.num_labels))\n-\n-    def test_Mistral_sequence_classification_model_for_single_label(self):\n-        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.num_labels = 3\n-        config.problem_type = \"single_label_classification\"\n-        input_ids = input_dict[\"input_ids\"]\n-        attention_mask = input_ids.ne(1).to(torch_device)\n-        sequence_labels = ids_tensor([self.model_tester.batch_size], self.model_tester.type_sequence_label_size)\n-        model = MistralForSequenceClassification(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=attention_mask, labels=sequence_labels)\n-        self.assertEqual(result.logits.shape, (self.model_tester.batch_size, self.model_tester.num_labels))\n-\n-    def test_Mistral_sequence_classification_model_for_multi_label(self):\n-        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.num_labels = 3\n-        config.problem_type = \"multi_label_classification\"\n-        input_ids = input_dict[\"input_ids\"]\n-        attention_mask = input_ids.ne(1).to(torch_device)\n-        sequence_labels = ids_tensor(\n-            [self.model_tester.batch_size, config.num_labels], self.model_tester.type_sequence_label_size\n-        ).to(torch.float)\n-        model = MistralForSequenceClassification(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=attention_mask, labels=sequence_labels)\n-        self.assertEqual(result.logits.shape, (self.model_tester.batch_size, self.model_tester.num_labels))\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTest.test_llama_token_classification_model with Llama->Mistral,llama->Mistral\n-    def test_Mistral_token_classification_model(self):\n-        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.num_labels = 3\n-        input_ids = input_dict[\"input_ids\"]\n-        attention_mask = input_ids.ne(1).to(torch_device)\n-        token_labels = ids_tensor([self.model_tester.batch_size, self.model_tester.seq_length], config.num_labels)\n-        model = MistralForTokenClassification(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=attention_mask, labels=token_labels)\n-        self.assertEqual(\n-            result.logits.shape,\n-            (self.model_tester.batch_size, self.model_tester.seq_length, self.model_tester.num_labels),\n-        )\n-\n     @require_flash_attn\n     @require_torch_gpu\n     @pytest.mark.flash_attn_test"
        },
        {
            "sha": "532ebb7348a9381f35632a3b167da1e39c79f2e8",
            "filename": "tests/models/mixtral/test_modeling_mixtral.py",
            "status": "modified",
            "additions": 13,
            "deletions": 210,
            "changes": 223,
            "blob_url": "https://github.com/huggingface/transformers/blob/53fb245eb60364c7377c5f37fc37807a00e9b2e2/tests%2Fmodels%2Fmixtral%2Ftest_modeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/53fb245eb60364c7377c5f37fc37807a00e9b2e2/tests%2Fmodels%2Fmixtral%2Ftest_modeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmixtral%2Ftest_modeling_mixtral.py?ref=53fb245eb60364c7377c5f37fc37807a00e9b2e2",
            "patch": "@@ -27,11 +27,6 @@\n     torch_device,\n )\n \n-from ...generation.test_utils import GenerationTesterMixin\n-from ...test_configuration_common import ConfigTester\n-from ...test_modeling_common import ModelTesterMixin, ids_tensor\n-from ...test_pipeline_mixin import PipelineTesterMixin\n-\n \n if is_torch_available():\n     import torch\n@@ -44,137 +39,21 @@\n         MixtralModel,\n     )\n \n+from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n \n-class MixtralModelTester:\n-    def __init__(\n-        self,\n-        parent,\n-        batch_size=13,\n-        seq_length=7,\n-        is_training=True,\n-        use_input_mask=True,\n-        use_token_type_ids=False,\n-        use_labels=True,\n-        vocab_size=99,\n-        hidden_size=32,\n-        num_hidden_layers=2,\n-        num_attention_heads=4,\n-        num_key_value_heads=2,\n-        intermediate_size=37,\n-        hidden_act=\"gelu\",\n-        hidden_dropout_prob=0.1,\n-        attention_probs_dropout_prob=0.1,\n-        max_position_embeddings=512,\n-        type_vocab_size=16,\n-        type_sequence_label_size=2,\n-        initializer_range=0.02,\n-        num_labels=3,\n-        num_choices=4,\n-        pad_token_id=0,\n-        scope=None,\n-        router_jitter_noise=0.1,\n-    ):\n-        self.parent = parent\n-        self.batch_size = batch_size\n-        self.seq_length = seq_length\n-        self.is_training = is_training\n-        self.use_input_mask = use_input_mask\n-        self.use_token_type_ids = use_token_type_ids\n-        self.use_labels = use_labels\n-        self.vocab_size = vocab_size\n-        self.hidden_size = hidden_size\n-        self.num_hidden_layers = num_hidden_layers\n-        self.num_attention_heads = num_attention_heads\n-        self.num_key_value_heads = num_key_value_heads\n-        self.intermediate_size = intermediate_size\n-        self.hidden_act = hidden_act\n-        self.hidden_dropout_prob = hidden_dropout_prob\n-        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n-        self.max_position_embeddings = max_position_embeddings\n-        self.type_vocab_size = type_vocab_size\n-        self.type_sequence_label_size = type_sequence_label_size\n-        self.initializer_range = initializer_range\n-        self.num_labels = num_labels\n-        self.num_choices = num_choices\n-        self.pad_token_id = pad_token_id\n-        self.scope = scope\n-        self.router_jitter_noise = router_jitter_noise\n-\n-    # Copied from tests.models.mistral.test_modeling_mistral.MistralModelTester.prepare_config_and_inputs\n-    def prepare_config_and_inputs(self):\n-        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n-\n-        input_mask = None\n-        if self.use_input_mask:\n-            input_mask = torch.tril(torch.ones_like(input_ids).to(torch_device))\n-\n-        token_type_ids = None\n-        if self.use_token_type_ids:\n-            token_type_ids = ids_tensor([self.batch_size, self.seq_length], self.type_vocab_size)\n-\n-        sequence_labels = None\n-        token_labels = None\n-        choice_labels = None\n-        if self.use_labels:\n-            sequence_labels = ids_tensor([self.batch_size], self.type_sequence_label_size)\n-            token_labels = ids_tensor([self.batch_size, self.seq_length], self.num_labels)\n-            choice_labels = ids_tensor([self.batch_size], self.num_choices)\n-\n-        config = self.get_config()\n-\n-        return config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-\n-    def get_config(self):\n-        return MixtralConfig(\n-            vocab_size=self.vocab_size,\n-            hidden_size=self.hidden_size,\n-            num_hidden_layers=self.num_hidden_layers,\n-            num_attention_heads=self.num_attention_heads,\n-            num_key_value_heads=self.num_key_value_heads,\n-            intermediate_size=self.intermediate_size,\n-            hidden_act=self.hidden_act,\n-            hidden_dropout_prob=self.hidden_dropout_prob,\n-            attention_probs_dropout_prob=self.attention_probs_dropout_prob,\n-            max_position_embeddings=self.max_position_embeddings,\n-            type_vocab_size=self.type_vocab_size,\n-            is_decoder=False,\n-            initializer_range=self.initializer_range,\n-            pad_token_id=self.pad_token_id,\n-            num_experts_per_tok=2,\n-            num_local_experts=2,\n-            router_jitter_noise=self.router_jitter_noise,\n-        )\n \n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.create_and_check_model with Llama->Mixtral\n-    def create_and_check_model(\n-        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        model = MixtralModel(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=input_mask)\n-        result = model(input_ids)\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.prepare_config_and_inputs_for_common with Llama->Mixtral\n-    def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-        (\n-            config,\n-            input_ids,\n-            token_type_ids,\n-            input_mask,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-        ) = config_and_inputs\n-        inputs_dict = {\"input_ids\": input_ids, \"attention_mask\": input_mask}\n-        return config, inputs_dict\n+class MixtralModelTester(CausalLMModelTester):\n+    config_class = MixtralConfig\n+    if is_torch_available():\n+        base_model_class = MixtralModel\n+        causal_lm_class = MixtralForCausalLM\n+        sequence_class = MixtralForSequenceClassification\n+        token_class = MixtralForTokenClassification\n+        question_answering_class = MixtralForQuestionAnswering\n \n \n @require_torch\n-# Copied from tests.models.mistral.test_modeling_mistral.MistralModelTest with Mistral->Mixtral\n-class MixtralModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+class MistralModelTest(CausalLMModelTest, unittest.TestCase):\n     all_model_classes = (\n         (\n             MixtralModel,\n@@ -192,15 +71,15 @@ class MixtralModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMi\n             \"text-classification\": MixtralForSequenceClassification,\n             \"token-classification\": MixtralForTokenClassification,\n             \"text-generation\": MixtralForCausalLM,\n-            \"zero-shot\": MixtralForSequenceClassification,\n             \"question-answering\": MixtralForQuestionAnswering,\n         }\n         if is_torch_available()\n         else {}\n     )\n+\n     test_headmasking = False\n     test_pruning = False\n-    fx_compatible = False  # Broken by attention refactor cc @Cyrilvallez\n+    model_tester_class = MixtralModelTester\n \n     # TODO (ydshieh): Check this. See https://app.circleci.com/pipelines/github/huggingface/transformers/79245/workflows/9490ef58-79c2-410d-8f51-e3495156cf9c/jobs/1012146\n     def is_pipeline_test_to_skip(\n@@ -215,88 +94,12 @@ def is_pipeline_test_to_skip(\n     ):\n         return True\n \n-    def setUp(self):\n-        self.model_tester = MixtralModelTester(self)\n-        self.config_tester = ConfigTester(self, config_class=MixtralConfig, hidden_size=37)\n-\n-    def test_config(self):\n-        self.config_tester.run_common_tests()\n-\n-    def test_model(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_model(*config_and_inputs)\n-\n-    def test_model_various_embeddings(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        for type in [\"absolute\", \"relative_key\", \"relative_key_query\"]:\n-            config_and_inputs[0].position_embedding_type = type\n-            self.model_tester.create_and_check_model(*config_and_inputs)\n-\n-    def test_torch_fx_output_loss(self):\n-        super().test_torch_fx_output_loss()\n-\n-    def test_Mixtral_sequence_classification_model(self):\n-        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.num_labels = 3\n-        input_ids = input_dict[\"input_ids\"]\n-        attention_mask = input_ids.ne(1).to(torch_device)\n-        sequence_labels = ids_tensor([self.model_tester.batch_size], self.model_tester.type_sequence_label_size)\n-        model = MixtralForSequenceClassification(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=attention_mask, labels=sequence_labels)\n-        self.assertEqual(result.logits.shape, (self.model_tester.batch_size, self.model_tester.num_labels))\n-\n-    def test_Mixtral_sequence_classification_model_for_single_label(self):\n-        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.num_labels = 3\n-        config.problem_type = \"single_label_classification\"\n-        input_ids = input_dict[\"input_ids\"]\n-        attention_mask = input_ids.ne(1).to(torch_device)\n-        sequence_labels = ids_tensor([self.model_tester.batch_size], self.model_tester.type_sequence_label_size)\n-        model = MixtralForSequenceClassification(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=attention_mask, labels=sequence_labels)\n-        self.assertEqual(result.logits.shape, (self.model_tester.batch_size, self.model_tester.num_labels))\n-\n-    def test_Mixtral_sequence_classification_model_for_multi_label(self):\n-        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.num_labels = 3\n-        config.problem_type = \"multi_label_classification\"\n-        input_ids = input_dict[\"input_ids\"]\n-        attention_mask = input_ids.ne(1).to(torch_device)\n-        sequence_labels = ids_tensor(\n-            [self.model_tester.batch_size, config.num_labels], self.model_tester.type_sequence_label_size\n-        ).to(torch.float)\n-        model = MixtralForSequenceClassification(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=attention_mask, labels=sequence_labels)\n-        self.assertEqual(result.logits.shape, (self.model_tester.batch_size, self.model_tester.num_labels))\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTest.test_llama_token_classification_model with Llama->Mixtral,llama->Mixtral\n-    def test_Mixtral_token_classification_model(self):\n-        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.num_labels = 3\n-        input_ids = input_dict[\"input_ids\"]\n-        attention_mask = input_ids.ne(1).to(torch_device)\n-        token_labels = ids_tensor([self.model_tester.batch_size, self.model_tester.seq_length], config.num_labels)\n-        model = MixtralForTokenClassification(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=attention_mask, labels=token_labels)\n-        self.assertEqual(\n-            result.logits.shape,\n-            (self.model_tester.batch_size, self.model_tester.seq_length, self.model_tester.num_labels),\n-        )\n-\n     @require_flash_attn\n     @require_torch_gpu\n     @pytest.mark.flash_attn_test\n     @slow\n     def test_flash_attn_2_inference_equivalence_right_padding(self):\n-        self.skipTest(reason=\"Mixtral flash attention does not support right padding\")\n+        self.skipTest(reason=\"Mistral flash attention does not support right padding\")\n \n     # Ignore copy\n     def test_load_balancing_loss(self):"
        },
        {
            "sha": "9ef543edeb72ac977a7bc09e89a81f43630dc02c",
            "filename": "tests/models/nemotron/test_modeling_nemotron.py",
            "status": "modified",
            "additions": 8,
            "deletions": 47,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/53fb245eb60364c7377c5f37fc37807a00e9b2e2/tests%2Fmodels%2Fnemotron%2Ftest_modeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/53fb245eb60364c7377c5f37fc37807a00e9b2e2/tests%2Fmodels%2Fnemotron%2Ftest_modeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fnemotron%2Ftest_modeling_nemotron.py?ref=53fb245eb60364c7377c5f37fc37807a00e9b2e2",
            "patch": "@@ -14,25 +14,19 @@\n # limitations under the License.\n \"\"\"Testing suite for the PyTorch Nemotron model.\"\"\"\n \n-import tempfile\n import unittest\n \n-import pytest\n-\n from transformers import NemotronConfig, is_torch_available\n from transformers.testing_utils import (\n     Expectations,\n-    is_flaky,\n-    require_flash_attn,\n     require_read_token,\n     require_torch,\n     require_torch_accelerator,\n-    require_torch_gpu,\n     slow,\n     torch_device,\n )\n \n-from ...models.gemma.test_modeling_gemma import GemmaModelTest, GemmaModelTester\n+from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n from ...test_configuration_common import ConfigTester\n \n \n@@ -49,17 +43,18 @@\n     )\n \n \n-class NemotronModelTester(GemmaModelTester):\n+class NemotronModelTester(CausalLMModelTester):\n     if is_torch_available():\n         config_class = NemotronConfig\n-        model_class = NemotronModel\n-        for_causal_lm_class = NemotronForCausalLM\n-        for_sequence_class = NemotronForSequenceClassification\n-        for_token_class = NemotronForTokenClassification\n+        base_model_class = NemotronModel\n+        causal_lm_class = NemotronForCausalLM\n+        sequence_class = NemotronForSequenceClassification\n+        token_class = NemotronForTokenClassification\n \n \n @require_torch\n-class NemotronModelTest(GemmaModelTest):\n+class NemotronModelTest(CausalLMModelTest, unittest.TestCase):\n+    model_tester_class = NemotronModelTester\n     # Need to use `0.8` instead of `0.9` for `test_cpu_offload`\n     # This is because we are hitting edge cases with the causal_mask buffer\n     model_split_percents = [0.5, 0.7, 0.8]\n@@ -101,40 +96,6 @@ def setUp(self):\n     def test_model_outputs_equivalence(self, **kwargs):\n         pass\n \n-    @require_flash_attn\n-    @require_torch_gpu\n-    @pytest.mark.flash_attn_test\n-    @is_flaky()\n-    @slow\n-    def test_flash_attn_2_equivalence(self):\n-        for model_class in self.all_model_classes:\n-            if not model_class._supports_flash_attn_2:\n-                self.skipTest(reason=\"Model does not support Flash Attention 2\")\n-\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-                model_fa = model_class.from_pretrained(\n-                    tmpdirname, torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\"\n-                )\n-                model_fa.to(torch_device)\n-\n-                model = model_class.from_pretrained(tmpdirname, torch_dtype=torch.float16, attn_implementation=\"eager\")\n-                model.to(torch_device)\n-\n-                dummy_input = inputs_dict[model_class.main_input_name]\n-                dummy_input = dummy_input.to(torch_device)\n-                outputs = model(dummy_input, output_hidden_states=True)\n-                outputs_fa = model_fa(dummy_input, output_hidden_states=True)\n-\n-                logits = outputs.hidden_states[-1]\n-                logits_fa = outputs_fa.hidden_states[-1]\n-\n-                # nemotron flash attention 2 needs a high tolerance\n-                assert torch.allclose(logits_fa, logits, atol=1e-2)\n-\n \n @require_torch_accelerator\n class NemotronIntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "2ac23a7e306679fe543f3363ff3ba2c417b61ad8",
            "filename": "tests/models/persimmon/test_modeling_persimmon.py",
            "status": "modified",
            "additions": 12,
            "deletions": 287,
            "changes": 299,
            "blob_url": "https://github.com/huggingface/transformers/blob/53fb245eb60364c7377c5f37fc37807a00e9b2e2/tests%2Fmodels%2Fpersimmon%2Ftest_modeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/53fb245eb60364c7377c5f37fc37807a00e9b2e2/tests%2Fmodels%2Fpersimmon%2Ftest_modeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpersimmon%2Ftest_modeling_persimmon.py?ref=53fb245eb60364c7377c5f37fc37807a00e9b2e2",
            "patch": "@@ -16,9 +16,7 @@\n import gc\n import unittest\n \n-from parameterized import parameterized\n-\n-from transformers import PersimmonConfig, is_torch_available, set_seed\n+from transformers import PersimmonConfig, is_torch_available\n from transformers.testing_utils import (\n     backend_empty_cache,\n     require_bitsandbytes,\n@@ -29,11 +27,6 @@\n     torch_device,\n )\n \n-from ...generation.test_utils import GenerationTesterMixin\n-from ...test_configuration_common import ConfigTester\n-from ...test_modeling_common import ModelTesterMixin, ids_tensor\n-from ...test_pipeline_mixin import PipelineTesterMixin\n-\n \n if is_torch_available():\n     import torch\n@@ -45,128 +38,22 @@\n         PersimmonForTokenClassification,\n         PersimmonModel,\n     )\n-    from transformers.models.persimmon.modeling_persimmon import PersimmonRotaryEmbedding\n-\n-\n-# Copied from tests.models.llama.test_modeling_llama.LlamaModelTester with Llama->Persimmon\n-class PersimmonModelTester:\n-    def __init__(\n-        self,\n-        parent,\n-        batch_size=13,\n-        seq_length=7,\n-        is_training=True,\n-        use_input_mask=True,\n-        use_token_type_ids=False,\n-        use_labels=True,\n-        vocab_size=99,\n-        hidden_size=32,\n-        num_hidden_layers=2,\n-        num_attention_heads=4,\n-        intermediate_size=37,\n-        hidden_act=\"gelu\",\n-        hidden_dropout_prob=0.1,\n-        attention_probs_dropout_prob=0.1,\n-        max_position_embeddings=512,\n-        type_vocab_size=16,\n-        type_sequence_label_size=2,\n-        initializer_range=0.02,\n-        num_labels=3,\n-        num_choices=4,\n-        pad_token_id=0,\n-        scope=None,\n-    ):\n-        self.parent = parent\n-        self.batch_size = batch_size\n-        self.seq_length = seq_length\n-        self.is_training = is_training\n-        self.use_input_mask = use_input_mask\n-        self.use_token_type_ids = use_token_type_ids\n-        self.use_labels = use_labels\n-        self.vocab_size = vocab_size\n-        self.hidden_size = hidden_size\n-        self.num_hidden_layers = num_hidden_layers\n-        self.num_attention_heads = num_attention_heads\n-        self.intermediate_size = intermediate_size\n-        self.hidden_act = hidden_act\n-        self.hidden_dropout_prob = hidden_dropout_prob\n-        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n-        self.max_position_embeddings = max_position_embeddings\n-        self.type_vocab_size = type_vocab_size\n-        self.type_sequence_label_size = type_sequence_label_size\n-        self.initializer_range = initializer_range\n-        self.num_labels = num_labels\n-        self.num_choices = num_choices\n-        self.pad_token_id = pad_token_id\n-        self.scope = scope\n \n-    def prepare_config_and_inputs(self):\n-        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n+from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n \n-        input_mask = None\n-        if self.use_input_mask:\n-            input_mask = torch.tril(torch.ones_like(input_ids).to(torch_device))\n \n-        token_type_ids = None\n-        if self.use_token_type_ids:\n-            token_type_ids = ids_tensor([self.batch_size, self.seq_length], self.type_vocab_size)\n-\n-        sequence_labels = None\n-        token_labels = None\n-        choice_labels = None\n-        if self.use_labels:\n-            sequence_labels = ids_tensor([self.batch_size], self.type_sequence_label_size)\n-            token_labels = ids_tensor([self.batch_size, self.seq_length], self.num_labels)\n-            choice_labels = ids_tensor([self.batch_size], self.num_choices)\n-\n-        config = self.get_config()\n-\n-        return config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-\n-    def get_config(self):\n-        return PersimmonConfig(\n-            vocab_size=self.vocab_size,\n-            hidden_size=self.hidden_size,\n-            num_hidden_layers=self.num_hidden_layers,\n-            num_attention_heads=self.num_attention_heads,\n-            intermediate_size=self.intermediate_size,\n-            hidden_act=self.hidden_act,\n-            hidden_dropout_prob=self.hidden_dropout_prob,\n-            attention_probs_dropout_prob=self.attention_probs_dropout_prob,\n-            max_position_embeddings=self.max_position_embeddings,\n-            type_vocab_size=self.type_vocab_size,\n-            is_decoder=False,\n-            initializer_range=self.initializer_range,\n-            pad_token_id=self.pad_token_id,\n-        )\n-\n-    def create_and_check_model(\n-        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        model = PersimmonModel(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=input_mask)\n-        result = model(input_ids)\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-\n-    def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-        (\n-            config,\n-            input_ids,\n-            token_type_ids,\n-            input_mask,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-        ) = config_and_inputs\n-        inputs_dict = {\"input_ids\": input_ids, \"attention_mask\": input_mask}\n-        return config, inputs_dict\n+class PersimmonModelTester(CausalLMModelTester):\n+    if is_torch_available():\n+        config_class = PersimmonConfig\n+        base_model_class = PersimmonModel\n+        causal_lm_class = PersimmonForCausalLM\n+        sequence_class = PersimmonForSequenceClassification\n+        token_class = PersimmonForTokenClassification\n \n \n @require_torch\n-class PersimmonModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+class PersimmonModelTest(CausalLMModelTest, unittest.TestCase):\n+    model_tester_class = PersimmonModelTester\n     all_model_classes = (\n         (PersimmonModel, PersimmonForCausalLM, PersimmonForSequenceClassification, PersimmonForTokenClassification)\n         if is_torch_available()\n@@ -184,173 +71,11 @@ class PersimmonModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTester\n         if is_torch_available()\n         else {}\n     )\n+    model_tester_class = PersimmonModelTester\n \n     test_headmasking = False\n     test_pruning = False\n \n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTest.setUp with Llama->Persimmon\n-    def setUp(self):\n-        self.model_tester = PersimmonModelTester(self)\n-        self.config_tester = ConfigTester(self, config_class=PersimmonConfig, hidden_size=37)\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTest.test_config\n-    def test_config(self):\n-        self.config_tester.run_common_tests()\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTest.test_model\n-    def test_model(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_model(*config_and_inputs)\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTest.test_model_various_embeddings\n-    def test_model_various_embeddings(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        for type in [\"absolute\", \"relative_key\", \"relative_key_query\"]:\n-            config_and_inputs[0].position_embedding_type = type\n-            self.model_tester.create_and_check_model(*config_and_inputs)\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTest.test_llama_sequence_classification_model with Llama->Persimmon,llama->persimmon\n-    def test_persimmon_sequence_classification_model(self):\n-        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.num_labels = 3\n-        input_ids = input_dict[\"input_ids\"]\n-        attention_mask = input_ids.ne(1).to(torch_device)\n-        sequence_labels = ids_tensor([self.model_tester.batch_size], self.model_tester.type_sequence_label_size)\n-        model = PersimmonForSequenceClassification(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=attention_mask, labels=sequence_labels)\n-        self.assertEqual(result.logits.shape, (self.model_tester.batch_size, self.model_tester.num_labels))\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTest.test_llama_sequence_classification_model_for_single_label with Llama->Persimmon,llama->persimmon\n-    def test_persimmon_sequence_classification_model_for_single_label(self):\n-        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.num_labels = 3\n-        config.problem_type = \"single_label_classification\"\n-        input_ids = input_dict[\"input_ids\"]\n-        attention_mask = input_ids.ne(1).to(torch_device)\n-        sequence_labels = ids_tensor([self.model_tester.batch_size], self.model_tester.type_sequence_label_size)\n-        model = PersimmonForSequenceClassification(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=attention_mask, labels=sequence_labels)\n-        self.assertEqual(result.logits.shape, (self.model_tester.batch_size, self.model_tester.num_labels))\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTest.test_llama_sequence_classification_model_for_multi_label with Llama->Persimmon,llama->persimmon\n-    def test_persimmon_sequence_classification_model_for_multi_label(self):\n-        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.num_labels = 3\n-        config.problem_type = \"multi_label_classification\"\n-        input_ids = input_dict[\"input_ids\"]\n-        attention_mask = input_ids.ne(1).to(torch_device)\n-        sequence_labels = ids_tensor(\n-            [self.model_tester.batch_size, config.num_labels], self.model_tester.type_sequence_label_size\n-        ).to(torch.float)\n-        model = PersimmonForSequenceClassification(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=attention_mask, labels=sequence_labels)\n-        self.assertEqual(result.logits.shape, (self.model_tester.batch_size, self.model_tester.num_labels))\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTest.test_llama_token_classification_model with Llama->Persimmon,llama->persimmon\n-    def test_persimmon_token_classification_model(self):\n-        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.num_labels = 3\n-        input_ids = input_dict[\"input_ids\"]\n-        attention_mask = input_ids.ne(1).to(torch_device)\n-        token_labels = ids_tensor([self.model_tester.batch_size, self.model_tester.seq_length], config.num_labels)\n-        model = PersimmonForTokenClassification(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=attention_mask, labels=token_labels)\n-        self.assertEqual(\n-            result.logits.shape,\n-            (self.model_tester.batch_size, self.model_tester.seq_length, self.model_tester.num_labels),\n-        )\n-\n-    @parameterized.expand([(\"linear\",), (\"dynamic\",)])\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTest.test_model_rope_scaling_from_config with Llama->Persimmon\n-    def test_model_rope_scaling_from_config(self, scaling_type):\n-        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-        short_input = ids_tensor([1, 10], config.vocab_size)\n-        long_input = ids_tensor([1, int(config.max_position_embeddings * 1.5)], config.vocab_size)\n-\n-        set_seed(42)  # Fixed seed at init time so the two models get the same random weights\n-        original_model = PersimmonModel(config)\n-        original_model.to(torch_device)\n-        original_model.eval()\n-        original_short_output = original_model(short_input).last_hidden_state\n-        original_long_output = original_model(long_input).last_hidden_state\n-\n-        set_seed(42)  # Fixed seed at init time so the two models get the same random weights\n-        config.rope_scaling = {\"type\": scaling_type, \"factor\": 10.0}\n-        scaled_model = PersimmonModel(config)\n-        scaled_model.to(torch_device)\n-        scaled_model.eval()\n-        scaled_short_output = scaled_model(short_input).last_hidden_state\n-        scaled_long_output = scaled_model(long_input).last_hidden_state\n-\n-        # Dynamic scaling does not change the RoPE embeddings until it receives an input longer than the original\n-        # maximum sequence length, so the outputs for the short input should match.\n-        if scaling_type == \"dynamic\":\n-            torch.testing.assert_close(original_short_output, scaled_short_output, rtol=1e-5, atol=1e-5)\n-        else:\n-            self.assertFalse(torch.allclose(original_short_output, scaled_short_output, atol=1e-5))\n-\n-        # The output should be different for long inputs\n-        self.assertFalse(torch.allclose(original_long_output, scaled_long_output, atol=1e-5))\n-\n-    # Copied from tests.models.gpt_neox.test_modeling_gpt_neox.GPTNeoXModelTest.test_model_rope_scaling with GPTNeoX->Persimmon\n-    def test_model_rope_scaling(self):\n-        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-        scaling_factor = 10\n-        short_input_length = 10\n-        long_input_length = int(config.max_position_embeddings * 1.5)\n-\n-        # Inputs\n-        x = torch.randn(\n-            1, dtype=torch.float32, device=torch_device\n-        )  # used exclusively to get the dtype and the device\n-        position_ids_short = torch.arange(short_input_length, dtype=torch.long, device=torch_device)\n-        position_ids_short = position_ids_short.unsqueeze(0)\n-        position_ids_long = torch.arange(long_input_length, dtype=torch.long, device=torch_device)\n-        position_ids_long = position_ids_long.unsqueeze(0)\n-\n-        # Sanity check original RoPE\n-        original_rope = PersimmonRotaryEmbedding(config).to(torch_device)\n-        original_cos_short, original_sin_short = original_rope(x, position_ids_short)\n-        original_cos_long, original_sin_long = original_rope(x, position_ids_long)\n-        torch.testing.assert_close(original_cos_short, original_cos_long[:, :short_input_length, :])\n-        torch.testing.assert_close(original_sin_short, original_sin_long[:, :short_input_length, :])\n-\n-        # Sanity check linear RoPE scaling\n-        # New position \"x\" should match original position with index \"x/scaling_factor\"\n-        config.rope_scaling = {\"type\": \"linear\", \"factor\": scaling_factor}\n-        linear_scaling_rope = PersimmonRotaryEmbedding(config).to(torch_device)\n-        linear_cos_short, linear_sin_short = linear_scaling_rope(x, position_ids_short)\n-        linear_cos_long, linear_sin_long = linear_scaling_rope(x, position_ids_long)\n-        torch.testing.assert_close(linear_cos_short, linear_cos_long[:, :short_input_length, :])\n-        torch.testing.assert_close(linear_sin_short, linear_sin_long[:, :short_input_length, :])\n-        for new_position in range(0, long_input_length, scaling_factor):\n-            original_position = int(new_position // scaling_factor)\n-            torch.testing.assert_close(linear_cos_long[:, new_position, :], original_cos_long[:, original_position, :])\n-            torch.testing.assert_close(linear_sin_long[:, new_position, :], original_sin_long[:, original_position, :])\n-\n-        # Sanity check Dynamic NTK RoPE scaling\n-        # Scaling should only be observed after a long input is fed. We can observe that the frequencies increase\n-        # with scaling_factor (or that `inv_freq` decreases)\n-        config.rope_scaling = {\"type\": \"dynamic\", \"factor\": scaling_factor}\n-        ntk_scaling_rope = PersimmonRotaryEmbedding(config).to(torch_device)\n-        ntk_cos_short, ntk_sin_short = ntk_scaling_rope(x, position_ids_short)\n-        ntk_cos_long, ntk_sin_long = ntk_scaling_rope(x, position_ids_long)\n-        torch.testing.assert_close(ntk_cos_short, original_cos_short)\n-        torch.testing.assert_close(ntk_sin_short, original_sin_short)\n-        with self.assertRaises(AssertionError):\n-            torch.testing.assert_close(ntk_cos_long, original_cos_long)\n-        with self.assertRaises(AssertionError):\n-            torch.testing.assert_close(ntk_sin_long, original_sin_long)\n-        self.assertTrue((ntk_scaling_rope.inv_freq <= original_rope.inv_freq).all())\n-\n \n @require_torch\n class PersimmonIntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "c2a7f26b31f1e4fd5255bee5dd449fd90e104ef0",
            "filename": "tests/models/phi/test_modeling_phi.py",
            "status": "modified",
            "additions": 13,
            "deletions": 264,
            "changes": 277,
            "blob_url": "https://github.com/huggingface/transformers/blob/53fb245eb60364c7377c5f37fc37807a00e9b2e2/tests%2Fmodels%2Fphi%2Ftest_modeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/53fb245eb60364c7377c5f37fc37807a00e9b2e2/tests%2Fmodels%2Fphi%2Ftest_modeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fphi%2Ftest_modeling_phi.py?ref=53fb245eb60364c7377c5f37fc37807a00e9b2e2",
            "patch": "@@ -16,19 +16,14 @@\n \n import unittest\n \n-from parameterized import parameterized\n-\n-from transformers import PhiConfig, is_torch_available, set_seed\n+from transformers import PhiConfig, is_torch_available\n from transformers.testing_utils import (\n     require_torch,\n     slow,\n     torch_device,\n )\n \n-from ...generation.test_utils import GenerationTesterMixin\n-from ...test_configuration_common import ConfigTester\n-from ...test_modeling_common import ModelTesterMixin, ids_tensor, random_attention_mask\n-from ...test_pipeline_mixin import PipelineTesterMixin\n+from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n \n \n if is_torch_available():\n@@ -44,124 +39,17 @@\n     from transformers.models.phi.modeling_phi import PhiRotaryEmbedding\n \n \n-class PhiModelTester:\n-    def __init__(\n-        self,\n-        parent,\n-        batch_size=13,\n-        seq_length=7,\n-        is_training=True,\n-        use_input_mask=True,\n-        use_token_type_ids=False,\n-        use_labels=True,\n-        vocab_size=99,\n-        hidden_size=32,\n-        num_hidden_layers=2,\n-        num_attention_heads=4,\n-        intermediate_size=37,\n-        hidden_act=\"gelu\",\n-        hidden_dropout_prob=0.1,\n-        attention_probs_dropout_prob=0.1,\n-        max_position_embeddings=512,\n-        type_vocab_size=16,\n-        type_sequence_label_size=2,\n-        initializer_range=0.02,\n-        num_labels=3,\n-        num_choices=4,\n-        pad_token_id=0,\n-        scope=None,\n-    ):\n-        self.parent = parent\n-        self.batch_size = batch_size\n-        self.seq_length = seq_length\n-        self.is_training = is_training\n-        self.use_input_mask = use_input_mask\n-        self.use_token_type_ids = use_token_type_ids\n-        self.use_labels = use_labels\n-        self.vocab_size = vocab_size\n-        self.hidden_size = hidden_size\n-        self.num_hidden_layers = num_hidden_layers\n-        self.num_attention_heads = num_attention_heads\n-        self.intermediate_size = intermediate_size\n-        self.hidden_act = hidden_act\n-        self.hidden_dropout_prob = hidden_dropout_prob\n-        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n-        self.max_position_embeddings = max_position_embeddings\n-        self.type_vocab_size = type_vocab_size\n-        self.type_sequence_label_size = type_sequence_label_size\n-        self.initializer_range = initializer_range\n-        self.num_labels = num_labels\n-        self.num_choices = num_choices\n-        self.pad_token_id = pad_token_id\n-        self.scope = scope\n-\n-    def prepare_config_and_inputs(self):\n-        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n-\n-        input_mask = None\n-        if self.use_input_mask:\n-            input_mask = random_attention_mask([self.batch_size, self.seq_length])\n-\n-        token_type_ids = None\n-        if self.use_token_type_ids:\n-            token_type_ids = ids_tensor([self.batch_size, self.seq_length], self.type_vocab_size)\n-\n-        sequence_labels = None\n-        token_labels = None\n-        choice_labels = None\n-        if self.use_labels:\n-            sequence_labels = ids_tensor([self.batch_size], self.type_sequence_label_size)\n-            token_labels = ids_tensor([self.batch_size, self.seq_length], self.num_labels)\n-            choice_labels = ids_tensor([self.batch_size], self.num_choices)\n-\n-        config = self.get_config()\n-\n-        return config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-\n-    def get_config(self):\n-        return PhiConfig(\n-            vocab_size=self.vocab_size,\n-            hidden_size=self.hidden_size,\n-            num_hidden_layers=self.num_hidden_layers,\n-            num_attention_heads=self.num_attention_heads,\n-            intermediate_size=self.intermediate_size,\n-            hidden_act=self.hidden_act,\n-            hidden_dropout_prob=self.hidden_dropout_prob,\n-            attention_probs_dropout_prob=self.attention_probs_dropout_prob,\n-            max_position_embeddings=self.max_position_embeddings,\n-            type_vocab_size=self.type_vocab_size,\n-            is_decoder=False,\n-            initializer_range=self.initializer_range,\n-            pad_token_id=self.pad_token_id,\n-        )\n-\n-    def create_and_check_model(\n-        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        model = PhiModel(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=input_mask)\n-        result = model(input_ids)\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-\n-    def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-        (\n-            config,\n-            input_ids,\n-            token_type_ids,\n-            input_mask,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-        ) = config_and_inputs\n-        inputs_dict = {\"input_ids\": input_ids, \"attention_mask\": input_mask}\n-        return config, inputs_dict\n+class PhiModelTester(CausalLMModelTester):\n+    config_class = PhiConfig\n+    if is_torch_available():\n+        base_model_class = PhiModel\n+        causal_lm_class = PhiForCausalLM\n+        sequence_class = PhiForSequenceClassification\n+        token_class = PhiForTokenClassification\n \n \n @require_torch\n-class PhiModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+class PhiModelTest(CausalLMModelTest, unittest.TestCase):\n     all_model_classes = (\n         (PhiModel, PhiForCausalLM, PhiForSequenceClassification, PhiForTokenClassification)\n         if is_torch_available()\n@@ -171,16 +59,17 @@ class PhiModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin,\n         {\n             \"feature-extraction\": PhiModel,\n             \"text-classification\": PhiForSequenceClassification,\n-            \"text-generation\": PhiForCausalLM,\n             \"token-classification\": PhiForTokenClassification,\n-            \"zero-shot\": PhiForSequenceClassification,\n+            \"text-generation\": PhiForCausalLM,\n         }\n         if is_torch_available()\n         else {}\n     )\n \n     test_headmasking = False\n     test_pruning = False\n+    model_tester_class = PhiModelTester\n+    rotary_embedding_layer = PhiRotaryEmbedding\n \n     # TODO (ydshieh): Check this. See https://app.circleci.com/pipelines/github/huggingface/transformers/79292/workflows/fa2ba644-8953-44a6-8f67-ccd69ca6a476/jobs/1012905\n     def is_pipeline_test_to_skip(\n@@ -195,146 +84,6 @@ def is_pipeline_test_to_skip(\n     ):\n         return True\n \n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTest.setUp with Llama->Phi\n-    def setUp(self):\n-        self.model_tester = PhiModelTester(self)\n-        self.config_tester = ConfigTester(self, config_class=PhiConfig, hidden_size=37)\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTest.test_config\n-    def test_config(self):\n-        self.config_tester.run_common_tests()\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTest.test_model\n-    def test_model(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_model(*config_and_inputs)\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTest.test_llama_sequence_classification_model with Llama->Phi,llama->phi\n-    def test_phi_sequence_classification_model(self):\n-        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.num_labels = 3\n-        input_ids = input_dict[\"input_ids\"]\n-        attention_mask = input_ids.ne(1).to(torch_device)\n-        sequence_labels = ids_tensor([self.model_tester.batch_size], self.model_tester.type_sequence_label_size)\n-        model = PhiForSequenceClassification(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=attention_mask, labels=sequence_labels)\n-        self.assertEqual(result.logits.shape, (self.model_tester.batch_size, self.model_tester.num_labels))\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTest.test_llama_sequence_classification_model_for_single_label with Llama->Phi,llama->phi\n-    def test_phi_sequence_classification_model_for_single_label(self):\n-        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.num_labels = 3\n-        config.problem_type = \"single_label_classification\"\n-        input_ids = input_dict[\"input_ids\"]\n-        attention_mask = input_ids.ne(1).to(torch_device)\n-        sequence_labels = ids_tensor([self.model_tester.batch_size], self.model_tester.type_sequence_label_size)\n-        model = PhiForSequenceClassification(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=attention_mask, labels=sequence_labels)\n-        self.assertEqual(result.logits.shape, (self.model_tester.batch_size, self.model_tester.num_labels))\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTest.test_llama_sequence_classification_model_for_multi_label with Llama->Phi,llama->phi\n-    def test_phi_sequence_classification_model_for_multi_label(self):\n-        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.num_labels = 3\n-        config.problem_type = \"multi_label_classification\"\n-        input_ids = input_dict[\"input_ids\"]\n-        attention_mask = input_ids.ne(1).to(torch_device)\n-        sequence_labels = ids_tensor(\n-            [self.model_tester.batch_size, config.num_labels], self.model_tester.type_sequence_label_size\n-        ).to(torch.float)\n-        model = PhiForSequenceClassification(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=attention_mask, labels=sequence_labels)\n-        self.assertEqual(result.logits.shape, (self.model_tester.batch_size, self.model_tester.num_labels))\n-\n-    @parameterized.expand([(\"linear\",), (\"dynamic\",)])\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTest.test_model_rope_scaling_from_config with Llama->Phi\n-    def test_model_rope_scaling_from_config(self, scaling_type):\n-        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-        short_input = ids_tensor([1, 10], config.vocab_size)\n-        long_input = ids_tensor([1, int(config.max_position_embeddings * 1.5)], config.vocab_size)\n-\n-        set_seed(42)  # Fixed seed at init time so the two models get the same random weights\n-        original_model = PhiModel(config)\n-        original_model.to(torch_device)\n-        original_model.eval()\n-        original_short_output = original_model(short_input).last_hidden_state\n-        original_long_output = original_model(long_input).last_hidden_state\n-\n-        set_seed(42)  # Fixed seed at init time so the two models get the same random weights\n-        config.rope_scaling = {\"type\": scaling_type, \"factor\": 10.0}\n-        scaled_model = PhiModel(config)\n-        scaled_model.to(torch_device)\n-        scaled_model.eval()\n-        scaled_short_output = scaled_model(short_input).last_hidden_state\n-        scaled_long_output = scaled_model(long_input).last_hidden_state\n-\n-        # Dynamic scaling does not change the RoPE embeddings until it receives an input longer than the original\n-        # maximum sequence length, so the outputs for the short input should match.\n-        if scaling_type == \"dynamic\":\n-            torch.testing.assert_close(original_short_output, scaled_short_output, rtol=1e-5, atol=1e-5)\n-        else:\n-            self.assertFalse(torch.allclose(original_short_output, scaled_short_output, atol=1e-5))\n-\n-        # The output should be different for long inputs\n-        self.assertFalse(torch.allclose(original_long_output, scaled_long_output, atol=1e-5))\n-\n-    # Copied from tests.models.gpt_neox.test_modeling_gpt_neox.GPTNeoXModelTest.test_model_rope_scaling with GPTNeoX->Phi\n-    def test_model_rope_scaling(self):\n-        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-        scaling_factor = 10\n-        short_input_length = 10\n-        long_input_length = int(config.max_position_embeddings * 1.5)\n-\n-        # Inputs\n-        x = torch.randn(\n-            1, dtype=torch.float32, device=torch_device\n-        )  # used exclusively to get the dtype and the device\n-        position_ids_short = torch.arange(short_input_length, dtype=torch.long, device=torch_device)\n-        position_ids_short = position_ids_short.unsqueeze(0)\n-        position_ids_long = torch.arange(long_input_length, dtype=torch.long, device=torch_device)\n-        position_ids_long = position_ids_long.unsqueeze(0)\n-\n-        # Sanity check original RoPE\n-        original_rope = PhiRotaryEmbedding(config).to(torch_device)\n-        original_cos_short, original_sin_short = original_rope(x, position_ids_short)\n-        original_cos_long, original_sin_long = original_rope(x, position_ids_long)\n-        torch.testing.assert_close(original_cos_short, original_cos_long[:, :short_input_length, :])\n-        torch.testing.assert_close(original_sin_short, original_sin_long[:, :short_input_length, :])\n-\n-        # Sanity check linear RoPE scaling\n-        # New position \"x\" should match original position with index \"x/scaling_factor\"\n-        config.rope_scaling = {\"type\": \"linear\", \"factor\": scaling_factor}\n-        linear_scaling_rope = PhiRotaryEmbedding(config).to(torch_device)\n-        linear_cos_short, linear_sin_short = linear_scaling_rope(x, position_ids_short)\n-        linear_cos_long, linear_sin_long = linear_scaling_rope(x, position_ids_long)\n-        torch.testing.assert_close(linear_cos_short, linear_cos_long[:, :short_input_length, :])\n-        torch.testing.assert_close(linear_sin_short, linear_sin_long[:, :short_input_length, :])\n-        for new_position in range(0, long_input_length, scaling_factor):\n-            original_position = int(new_position // scaling_factor)\n-            torch.testing.assert_close(linear_cos_long[:, new_position, :], original_cos_long[:, original_position, :])\n-            torch.testing.assert_close(linear_sin_long[:, new_position, :], original_sin_long[:, original_position, :])\n-\n-        # Sanity check Dynamic NTK RoPE scaling\n-        # Scaling should only be observed after a long input is fed. We can observe that the frequencies increase\n-        # with scaling_factor (or that `inv_freq` decreases)\n-        config.rope_scaling = {\"type\": \"dynamic\", \"factor\": scaling_factor}\n-        ntk_scaling_rope = PhiRotaryEmbedding(config).to(torch_device)\n-        ntk_cos_short, ntk_sin_short = ntk_scaling_rope(x, position_ids_short)\n-        ntk_cos_long, ntk_sin_long = ntk_scaling_rope(x, position_ids_long)\n-        torch.testing.assert_close(ntk_cos_short, original_cos_short)\n-        torch.testing.assert_close(ntk_sin_short, original_sin_short)\n-        with self.assertRaises(AssertionError):\n-            torch.testing.assert_close(ntk_cos_long, original_cos_long)\n-        with self.assertRaises(AssertionError):\n-            torch.testing.assert_close(ntk_sin_long, original_sin_long)\n-        self.assertTrue((ntk_scaling_rope.inv_freq <= original_rope.inv_freq).all())\n-\n \n @slow\n @require_torch"
        },
        {
            "sha": "cb9dc86d43b02a17ac75f5c0ec3f8fc123554e01",
            "filename": "tests/models/phi3/test_modeling_phi3.py",
            "status": "modified",
            "additions": 14,
            "deletions": 271,
            "changes": 285,
            "blob_url": "https://github.com/huggingface/transformers/blob/53fb245eb60364c7377c5f37fc37807a00e9b2e2/tests%2Fmodels%2Fphi3%2Ftest_modeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/53fb245eb60364c7377c5f37fc37807a00e9b2e2/tests%2Fmodels%2Fphi3%2Ftest_modeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fphi3%2Ftest_modeling_phi3.py?ref=53fb245eb60364c7377c5f37fc37807a00e9b2e2",
            "patch": "@@ -16,20 +16,15 @@\n \n import unittest\n \n-from parameterized import parameterized\n-\n-from transformers import Phi3Config, StaticCache, is_torch_available, set_seed\n+from transformers import Phi3Config, StaticCache, is_torch_available\n from transformers.models.auto.configuration_auto import AutoConfig\n from transformers.testing_utils import (\n     require_torch,\n     slow,\n     torch_device,\n )\n \n-from ...generation.test_utils import GenerationTesterMixin\n-from ...test_configuration_common import ConfigTester\n-from ...test_modeling_common import ModelTesterMixin, ids_tensor\n-from ...test_pipeline_mixin import PipelineTesterMixin\n+from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n \n \n if is_torch_available():\n@@ -42,6 +37,7 @@\n         Phi3ForTokenClassification,\n         Phi3Model,\n     )\n+    from transformers.models.phi3.modeling_phi3 import Phi3RotaryEmbedding\n \n     end_of_text_token = 32000\n \n@@ -93,127 +89,17 @@ def generate(model: Phi3ForCausalLM, prompt_tokens: torch.LongTensor, max_seq_le\n             return response_tokens\n \n \n-class Phi3ModelTester:\n-    def __init__(\n-        self,\n-        parent,\n-        batch_size=13,\n-        seq_length=7,\n-        is_training=True,\n-        use_input_mask=True,\n-        use_token_type_ids=False,\n-        use_labels=True,\n-        vocab_size=99,\n-        hidden_size=32,\n-        num_hidden_layers=2,\n-        num_attention_heads=4,\n-        intermediate_size=37,\n-        hidden_act=\"gelu\",\n-        hidden_dropout_prob=0.1,\n-        attention_probs_dropout_prob=0.1,\n-        max_position_embeddings=512,\n-        type_vocab_size=16,\n-        type_sequence_label_size=2,\n-        initializer_range=0.02,\n-        num_labels=3,\n-        num_choices=4,\n-        pad_token_id=0,\n-        scope=None,\n-    ):\n-        self.parent = parent\n-        self.batch_size = batch_size\n-        self.seq_length = seq_length\n-        self.is_training = is_training\n-        self.use_input_mask = use_input_mask\n-        self.use_token_type_ids = use_token_type_ids\n-        self.use_labels = use_labels\n-        self.vocab_size = vocab_size\n-        self.hidden_size = hidden_size\n-        self.num_hidden_layers = num_hidden_layers\n-        self.num_attention_heads = num_attention_heads\n-        self.intermediate_size = intermediate_size\n-        self.hidden_act = hidden_act\n-        self.hidden_dropout_prob = hidden_dropout_prob\n-        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n-        self.max_position_embeddings = max_position_embeddings\n-        self.type_vocab_size = type_vocab_size\n-        self.type_sequence_label_size = type_sequence_label_size\n-        self.initializer_range = initializer_range\n-        self.num_labels = num_labels\n-        self.num_choices = num_choices\n-        self.pad_token_id = pad_token_id\n-        self.scope = scope\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.prepare_config_and_inputs\n-    def prepare_config_and_inputs(self):\n-        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n-\n-        input_mask = None\n-        if self.use_input_mask:\n-            input_mask = torch.tril(torch.ones_like(input_ids).to(torch_device))\n-\n-        token_type_ids = None\n-        if self.use_token_type_ids:\n-            token_type_ids = ids_tensor([self.batch_size, self.seq_length], self.type_vocab_size)\n-\n-        sequence_labels = None\n-        token_labels = None\n-        choice_labels = None\n-        if self.use_labels:\n-            sequence_labels = ids_tensor([self.batch_size], self.type_sequence_label_size)\n-            token_labels = ids_tensor([self.batch_size, self.seq_length], self.num_labels)\n-            choice_labels = ids_tensor([self.batch_size], self.num_choices)\n-\n-        config = self.get_config()\n-\n-        return config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-\n-    def get_config(self):\n-        return Phi3Config(\n-            vocab_size=self.vocab_size,\n-            hidden_size=self.hidden_size,\n-            num_hidden_layers=self.num_hidden_layers,\n-            num_attention_heads=self.num_attention_heads,\n-            intermediate_size=self.intermediate_size,\n-            hidden_act=self.hidden_act,\n-            hidden_dropout_prob=self.hidden_dropout_prob,\n-            attention_probs_dropout_prob=self.attention_probs_dropout_prob,\n-            max_position_embeddings=self.max_position_embeddings,\n-            type_vocab_size=self.type_vocab_size,\n-            is_decoder=False,\n-            initializer_range=self.initializer_range,\n-            pad_token_id=self.pad_token_id,\n-        )\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.create_and_check_model with Llama->Phi3\n-    def create_and_check_model(\n-        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        model = Phi3Model(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=input_mask)\n-        result = model(input_ids)\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.prepare_config_and_inputs_for_common\n-    def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-        (\n-            config,\n-            input_ids,\n-            token_type_ids,\n-            input_mask,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-        ) = config_and_inputs\n-        inputs_dict = {\"input_ids\": input_ids, \"attention_mask\": input_mask}\n-        return config, inputs_dict\n+class Phi3ModelTester(CausalLMModelTester):\n+    config_class = Phi3Config\n+    if is_torch_available():\n+        base_model_class = Phi3Model\n+        causal_lm_class = Phi3ForCausalLM\n+        sequence_class = Phi3ForSequenceClassification\n+        token_class = Phi3ForTokenClassification\n \n \n @require_torch\n-class Phi3ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+class Phi3ModelTest(CausalLMModelTest, unittest.TestCase):\n     all_model_classes = (\n         (Phi3Model, Phi3ForCausalLM, Phi3ForSequenceClassification, Phi3ForTokenClassification)\n         if is_torch_available()\n@@ -223,160 +109,17 @@ class Phi3ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin\n         {\n             \"feature-extraction\": Phi3Model,\n             \"text-classification\": Phi3ForSequenceClassification,\n-            \"text-generation\": Phi3ForCausalLM,\n             \"token-classification\": Phi3ForTokenClassification,\n-            \"zero-shot\": Phi3ForSequenceClassification,\n+            \"text-generation\": Phi3ForCausalLM,\n         }\n         if is_torch_available()\n         else {}\n     )\n \n     test_headmasking = False\n     test_pruning = False\n-\n-    # TODO (ydshieh): Check this. See https://app.circleci.com/pipelines/github/huggingface/transformers/79292/workflows/fa2ba644-8953-44a6-8f67-ccd69ca6a476/jobs/1012905\n-    def is_pipeline_test_to_skip(\n-        self,\n-        pipeline_test_case_name,\n-        config_class,\n-        model_architecture,\n-        tokenizer_name,\n-        image_processor_name,\n-        feature_extractor_name,\n-        processor_name,\n-    ):\n-        return True\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTest.setUp with Llama->Phi3\n-    def setUp(self):\n-        self.model_tester = Phi3ModelTester(self)\n-        self.config_tester = ConfigTester(self, config_class=Phi3Config, hidden_size=37)\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTest.test_config\n-    def test_config(self):\n-        self.config_tester.run_common_tests()\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTest.test_model\n-    def test_model(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_model(*config_and_inputs)\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTest.test_llama_sequence_classification_model with Llama->Phi3,llama->phi3\n-    def test_phi3_sequence_classification_model(self):\n-        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.num_labels = 3\n-        input_ids = input_dict[\"input_ids\"]\n-        attention_mask = input_ids.ne(1).to(torch_device)\n-        sequence_labels = ids_tensor([self.model_tester.batch_size], self.model_tester.type_sequence_label_size)\n-        model = Phi3ForSequenceClassification(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=attention_mask, labels=sequence_labels)\n-        self.assertEqual(result.logits.shape, (self.model_tester.batch_size, self.model_tester.num_labels))\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTest.test_llama_sequence_classification_model_for_single_label with Llama->Phi3,llama->phi3\n-    def test_phi3_sequence_classification_model_for_single_label(self):\n-        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.num_labels = 3\n-        config.problem_type = \"single_label_classification\"\n-        input_ids = input_dict[\"input_ids\"]\n-        attention_mask = input_ids.ne(1).to(torch_device)\n-        sequence_labels = ids_tensor([self.model_tester.batch_size], self.model_tester.type_sequence_label_size)\n-        model = Phi3ForSequenceClassification(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=attention_mask, labels=sequence_labels)\n-        self.assertEqual(result.logits.shape, (self.model_tester.batch_size, self.model_tester.num_labels))\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTest.test_llama_sequence_classification_model_for_multi_label with Llama->Phi3,llama->phi3\n-    def test_phi3_sequence_classification_model_for_multi_label(self):\n-        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.num_labels = 3\n-        config.problem_type = \"multi_label_classification\"\n-        input_ids = input_dict[\"input_ids\"]\n-        attention_mask = input_ids.ne(1).to(torch_device)\n-        sequence_labels = ids_tensor(\n-            [self.model_tester.batch_size, config.num_labels], self.model_tester.type_sequence_label_size\n-        ).to(torch.float)\n-        model = Phi3ForSequenceClassification(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=attention_mask, labels=sequence_labels)\n-        self.assertEqual(result.logits.shape, (self.model_tester.batch_size, self.model_tester.num_labels))\n-\n-    @parameterized.expand([(\"longrope\",)])\n-    def test_model_rope_scaling_from_config(self, scaling_type):\n-        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-        short_input = ids_tensor([1, 10], config.vocab_size)\n-        long_input = ids_tensor([1, int(config.max_position_embeddings * 1.5)], config.vocab_size)\n-\n-        set_seed(42)  # Fixed seed at init time so the two models get the same random weights\n-        original_model = Phi3Model(config)\n-        original_model.to(torch_device)\n-        original_model.eval()\n-        original_short_output = original_model(short_input).last_hidden_state\n-        original_long_output = original_model(long_input).last_hidden_state\n-\n-        set_seed(42)  # Fixed seed at init time so the two models get the same random weights\n-        n_factors = config.hidden_size // config.num_attention_heads // 2\n-        config.rope_scaling = {\n-            \"type\": scaling_type,\n-            \"short_factor\": [5.0 for _ in range(n_factors)],\n-            \"long_factor\": [5.0 for _ in range(n_factors)],\n-        }\n-        scaled_model = Phi3Model(config)\n-        scaled_model.to(torch_device)\n-        scaled_model.eval()\n-        scaled_short_output = scaled_model(short_input).last_hidden_state\n-        scaled_long_output = scaled_model(long_input).last_hidden_state\n-\n-        # Scaling changes the RoPE embeddings, both for the short and long outputs\n-        self.assertFalse(torch.allclose(original_short_output, scaled_short_output, atol=1e-5))\n-        self.assertFalse(torch.allclose(original_long_output, scaled_long_output, atol=1e-5))\n-\n-    @parameterized.expand([(\"longrope\",)])\n-    def test_model_rope_scaling_short_long_factor(self, scaling_type):\n-        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-        n_factors = config.hidden_size // config.num_key_value_heads // 2\n-        config.rope_scaling = {\n-            \"type\": scaling_type,\n-            \"short_factor\": [3.0 for _ in range(n_factors)],\n-            \"long_factor\": [5.0 for _ in range(n_factors)],\n-        }\n-        input_tensor = ids_tensor([1, 4090], config.vocab_size)\n-        # Make sure we don't have padding tokens. If this is the case, then the actual number of \"true\" tokens may be shorter\n-        # than `config.original_max_position_embeddings + 5`, invalidating this test\n-        input_tensor[input_tensor == config.pad_token_id] += 1\n-        model = Phi3ForCausalLM(config)\n-        model.to(torch_device)\n-        model.eval()\n-        generation_args_short = {\n-            \"max_length\": config.original_max_position_embeddings,\n-            \"temperature\": 0.0,\n-            \"use_cache\": True,\n-            \"do_sample\": False,\n-            \"return_dict_in_generate\": True,\n-        }\n-        output_with_short_factor = model.generate(input_tensor, **generation_args_short)\n-        keys_with_short_factor = output_with_short_factor.past_key_values[0][0]\n-        generation_args_long = {\n-            \"max_length\": config.original_max_position_embeddings + 5,\n-            \"temperature\": 0.0,\n-            \"use_cache\": True,\n-            \"do_sample\": False,\n-            \"return_dict_in_generate\": True,\n-            \"output_logits\": True,\n-        }\n-        output_with_long_factor = model.generate(input_tensor, **generation_args_long)\n-        keys_with_long_factor = output_with_long_factor.past_key_values[0][0]\n-        last_token_logits = output_with_long_factor.logits[-1][-1]\n-        regenerated_last_token_logits = model(output_with_long_factor.sequences[:, :-1]).logits[0][-1]\n-        keys_with_long_factor = keys_with_long_factor[:, :, : config.original_max_position_embeddings - 1, :]\n-\n-        # KV cache is re-computed after reaching the (`config.original_max_position_embeddings`+1)th token position\n-        self.assertFalse(torch.allclose(keys_with_short_factor, keys_with_long_factor, atol=1e-2, rtol=1e-2))\n-        # Last token generated using long factor\n-        torch.testing.assert_close(last_token_logits, regenerated_last_token_logits, rtol=1e-2, atol=1e-2)\n+    model_tester_class = Phi3ModelTester\n+    rotary_embedding_layer = Phi3RotaryEmbedding\n \n \n @slow"
        },
        {
            "sha": "89bde307b6da87c7c6d3ab258a43c3c66ced36b6",
            "filename": "tests/models/phimoe/test_modeling_phimoe.py",
            "status": "modified",
            "additions": 13,
            "deletions": 272,
            "changes": 285,
            "blob_url": "https://github.com/huggingface/transformers/blob/53fb245eb60364c7377c5f37fc37807a00e9b2e2/tests%2Fmodels%2Fphimoe%2Ftest_modeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/53fb245eb60364c7377c5f37fc37807a00e9b2e2/tests%2Fmodels%2Fphimoe%2Ftest_modeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fphimoe%2Ftest_modeling_phimoe.py?ref=53fb245eb60364c7377c5f37fc37807a00e9b2e2",
            "patch": "@@ -16,20 +16,14 @@\n \n import unittest\n \n-from parameterized import parameterized\n-\n-from transformers import PhimoeConfig, StaticCache, is_torch_available, set_seed\n+from transformers import PhimoeConfig, StaticCache, is_torch_available\n from transformers.testing_utils import (\n-    is_flaky,\n     require_torch,\n     slow,\n     torch_device,\n )\n \n-from ...generation.test_utils import GenerationTesterMixin\n-from ...test_configuration_common import ConfigTester\n-from ...test_modeling_common import ModelTesterMixin, ids_tensor\n-from ...test_pipeline_mixin import PipelineTesterMixin\n+from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n \n \n if is_torch_available():\n@@ -92,138 +86,23 @@ def generate(model: PhimoeForCausalLM, prompt_tokens: torch.LongTensor, max_seq_\n             return response_tokens\n \n \n-class PhimoeModelTester:\n-    def __init__(\n-        self,\n-        parent,\n-        batch_size=13,\n-        seq_length=7,\n-        is_training=True,\n-        use_input_mask=True,\n-        use_token_type_ids=False,\n-        use_labels=True,\n-        vocab_size=99,\n-        hidden_size=32,\n-        num_hidden_layers=2,\n-        num_attention_heads=4,\n-        num_key_value_heads=4,\n-        intermediate_size=37,\n-        hidden_act=\"gelu\",\n-        hidden_dropout_prob=0.1,\n-        attention_probs_dropout_prob=0.1,\n-        max_position_embeddings=131072,\n-        type_vocab_size=16,\n-        type_sequence_label_size=2,\n-        initializer_range=0.02,\n-        num_labels=3,\n-        num_choices=4,\n-        pad_token_id=0,\n-        scope=None,\n-        original_max_position_embeddings=4096,\n-    ):\n-        self.parent = parent\n-        self.batch_size = batch_size\n-        self.seq_length = seq_length\n-        self.is_training = is_training\n-        self.use_input_mask = use_input_mask\n-        self.use_token_type_ids = use_token_type_ids\n-        self.use_labels = use_labels\n-        self.vocab_size = vocab_size\n-        self.hidden_size = hidden_size\n-        self.num_hidden_layers = num_hidden_layers\n-        self.num_attention_heads = num_attention_heads\n-        self.num_key_value_heads = num_key_value_heads\n-        self.intermediate_size = intermediate_size\n-        self.hidden_act = hidden_act\n-        self.hidden_dropout_prob = hidden_dropout_prob\n-        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n-        self.max_position_embeddings = max_position_embeddings\n-        self.type_vocab_size = type_vocab_size\n-        self.type_sequence_label_size = type_sequence_label_size\n-        self.initializer_range = initializer_range\n-        self.num_labels = num_labels\n-        self.num_choices = num_choices\n-        self.pad_token_id = pad_token_id\n-        self.scope = scope\n-        self.original_max_position_embeddings = original_max_position_embeddings\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.prepare_config_and_inputs\n-    def prepare_config_and_inputs(self):\n-        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n-\n-        input_mask = None\n-        if self.use_input_mask:\n-            input_mask = torch.tril(torch.ones_like(input_ids).to(torch_device))\n-\n-        token_type_ids = None\n-        if self.use_token_type_ids:\n-            token_type_ids = ids_tensor([self.batch_size, self.seq_length], self.type_vocab_size)\n-\n-        sequence_labels = None\n-        token_labels = None\n-        choice_labels = None\n-        if self.use_labels:\n-            sequence_labels = ids_tensor([self.batch_size], self.type_sequence_label_size)\n-            token_labels = ids_tensor([self.batch_size, self.seq_length], self.num_labels)\n-            choice_labels = ids_tensor([self.batch_size], self.num_choices)\n-\n-        config = self.get_config()\n-\n-        return config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-\n-    def get_config(self):\n-        return PhimoeConfig(\n-            vocab_size=self.vocab_size,\n-            hidden_size=self.hidden_size,\n-            num_hidden_layers=self.num_hidden_layers,\n-            num_attention_heads=self.num_attention_heads,\n-            num_key_value_heads=self.num_key_value_heads,\n-            intermediate_size=self.intermediate_size,\n-            hidden_act=self.hidden_act,\n-            hidden_dropout_prob=self.hidden_dropout_prob,\n-            attention_probs_dropout_prob=self.attention_probs_dropout_prob,\n-            max_position_embeddings=self.max_position_embeddings,\n-            type_vocab_size=self.type_vocab_size,\n-            is_decoder=False,\n-            initializer_range=self.initializer_range,\n-            pad_token_id=self.pad_token_id,\n-            num_experts_per_tok=2,\n-            num_local_experts=2,\n-            original_max_position_embeddings=self.original_max_position_embeddings,\n-        )\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.create_and_check_model with Llama->Phimoe\n-    def create_and_check_model(\n-        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        model = PhimoeModel(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=input_mask)\n-        result = model(input_ids)\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.prepare_config_and_inputs_for_common\n-    def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-        (\n-            config,\n-            input_ids,\n-            token_type_ids,\n-            input_mask,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-        ) = config_and_inputs\n-        inputs_dict = {\"input_ids\": input_ids, \"attention_mask\": input_mask}\n-        return config, inputs_dict\n+class PhimoeModelTester(CausalLMModelTester):\n+    if is_torch_available():\n+        config_class = PhimoeConfig\n+        base_model_class = PhimoeModel\n+        causal_lm_class = PhimoeForCausalLM\n+        sequence_class = PhimoeForSequenceClassification\n \n \n @require_torch\n-class PhimoeModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+class PhimoeModelTest(CausalLMModelTest, unittest.TestCase):\n     all_model_classes = (\n         (PhimoeModel, PhimoeForCausalLM, PhimoeForSequenceClassification) if is_torch_available() else ()\n     )\n+\n+    test_headmasking = False\n+    test_pruning = False\n+    model_tester_class = PhimoeModelTester\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": PhimoeModel,\n@@ -235,150 +114,12 @@ class PhimoeModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMix\n         else {}\n     )\n \n-    test_headmasking = False\n-    test_pruning = False\n-\n     # TODO (ydshieh): Check this. See https://app.circleci.com/pipelines/github/huggingface/transformers/79292/workflows/fa2ba644-8953-44a6-8f67-ccd69ca6a476/jobs/1012905\n     def is_pipeline_test_to_skip(\n         self, pipeline_test_casse_name, config_class, model_architecture, tokenizer_name, processor_name\n     ):\n         return True\n \n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTest.setUp with Llama->Phimoe\n-    def setUp(self):\n-        self.model_tester = PhimoeModelTester(self)\n-        self.config_tester = ConfigTester(self, config_class=PhimoeConfig, hidden_size=37)\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTest.test_config\n-    def test_config(self):\n-        self.config_tester.run_common_tests()\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTest.test_model\n-    def test_model(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_model(*config_and_inputs)\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTest.test_llama_sequence_classification_model with Llama->Phimoe,llama->phimoe\n-    def test_phimoe_sequence_classification_model(self):\n-        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.num_labels = 3\n-        input_ids = input_dict[\"input_ids\"]\n-        attention_mask = input_ids.ne(1).to(torch_device)\n-        sequence_labels = ids_tensor([self.model_tester.batch_size], self.model_tester.type_sequence_label_size)\n-        model = PhimoeForSequenceClassification(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=attention_mask, labels=sequence_labels)\n-        self.assertEqual(result.logits.shape, (self.model_tester.batch_size, self.model_tester.num_labels))\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTest.test_llama_sequence_classification_model_for_single_label with Llama->Phimoe,llama->phimoe\n-    def test_phimoe_sequence_classification_model_for_single_label(self):\n-        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.num_labels = 3\n-        config.problem_type = \"single_label_classification\"\n-        input_ids = input_dict[\"input_ids\"]\n-        attention_mask = input_ids.ne(1).to(torch_device)\n-        sequence_labels = ids_tensor([self.model_tester.batch_size], self.model_tester.type_sequence_label_size)\n-        model = PhimoeForSequenceClassification(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=attention_mask, labels=sequence_labels)\n-        self.assertEqual(result.logits.shape, (self.model_tester.batch_size, self.model_tester.num_labels))\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTest.test_llama_sequence_classification_model_for_multi_label with Llama->Phimoe,llama->phimoe\n-    def test_phimoe_sequence_classification_model_for_multi_label(self):\n-        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.num_labels = 3\n-        config.problem_type = \"multi_label_classification\"\n-        input_ids = input_dict[\"input_ids\"]\n-        attention_mask = input_ids.ne(1).to(torch_device)\n-        sequence_labels = ids_tensor(\n-            [self.model_tester.batch_size, config.num_labels], self.model_tester.type_sequence_label_size\n-        ).to(torch.float)\n-        model = PhimoeForSequenceClassification(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=attention_mask, labels=sequence_labels)\n-        self.assertEqual(result.logits.shape, (self.model_tester.batch_size, self.model_tester.num_labels))\n-\n-    @parameterized.expand([(\"longrope\",)])\n-    def test_model_rope_scaling_from_config(self, scaling_type):\n-        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-        short_input = ids_tensor([1, 10], config.vocab_size)\n-        long_input = ids_tensor([1, int(config.original_max_position_embeddings * 1.5)], config.vocab_size)\n-\n-        set_seed(42)  # Fixed seed at init time so the two models get the same random weights\n-        original_model = PhimoeModel(config)\n-        original_model.to(torch_device)\n-        original_model.eval()\n-        original_short_output = original_model(short_input).last_hidden_state\n-        original_long_output = original_model(long_input).last_hidden_state\n-\n-        set_seed(42)  # Fixed seed at init time so the two models get the same random weights\n-        n_factors = config.hidden_size // config.num_attention_heads // 2\n-        config.rope_scaling = {\n-            \"type\": scaling_type,\n-            \"short_factor\": [3.0 for _ in range(n_factors)],\n-            \"long_factor\": [5.0 for _ in range(n_factors)],\n-            \"short_mscale\": 1.243163121016122,\n-            \"long_mscale\": 1.243163121016122,\n-            \"original_max_position_embeddings\": 4096,\n-        }\n-        scaled_model = PhimoeModel(config)\n-        scaled_model.to(torch_device)\n-        scaled_model.eval()\n-        scaled_short_output = scaled_model(short_input).last_hidden_state\n-        scaled_long_output = scaled_model(long_input).last_hidden_state\n-\n-        # Scaling changes the RoPE embeddings, both for the short and long outputs\n-        self.assertFalse(torch.allclose(original_short_output, scaled_short_output, atol=1e-5))\n-        self.assertFalse(torch.allclose(original_long_output, scaled_long_output, atol=1e-5))\n-\n-    @parameterized.expand([(\"longrope\",)])\n-    @is_flaky()  # TODO (joao): unify rope tests in the mixin\n-    def test_model_rope_scaling_short_long_factor(self, scaling_type):\n-        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-        n_factors = config.hidden_size // config.num_key_value_heads // 2\n-        config.rope_scaling = {\n-            \"type\": scaling_type,\n-            \"short_factor\": [3.0 for _ in range(n_factors)],\n-            \"long_factor\": [5.0 for _ in range(n_factors)],\n-            \"short_mscale\": 1.243163121016122,\n-            \"long_mscale\": 1.243163121016122,\n-            \"original_max_position_embeddings\": 4096,\n-        }\n-        input_tensor = ids_tensor([1, 4090], config.vocab_size)\n-        model = PhimoeForCausalLM(config)\n-        model.to(torch_device)\n-        model.eval()\n-        generation_args_short = {\n-            \"max_length\": config.original_max_position_embeddings,\n-            \"temperature\": 0.0,\n-            \"use_cache\": True,\n-            \"do_sample\": False,\n-            \"return_dict_in_generate\": True,\n-        }\n-        output_with_short_factor = model.generate(input_tensor, **generation_args_short)\n-        keys_with_short_factor = output_with_short_factor.past_key_values[0][0]\n-        generation_args_long = {\n-            \"max_length\": config.original_max_position_embeddings + 5,\n-            \"temperature\": 0.0,\n-            \"use_cache\": True,\n-            \"do_sample\": False,\n-            \"return_dict_in_generate\": True,\n-            \"output_logits\": True,\n-        }\n-        output_with_long_factor = model.generate(input_tensor, **generation_args_long)\n-        keys_with_long_factor = output_with_long_factor.past_key_values[0][0]\n-        last_token_logits = output_with_long_factor.logits[-1][-1]\n-        regenerated_last_token_logits = model(output_with_long_factor.sequences[:, :-1]).logits[0][-1]\n-        keys_with_long_factor = keys_with_long_factor[:, :, : config.original_max_position_embeddings - 1, :]\n-\n-        # KV cache is re-computed after reaching the (`config.original_max_position_embeddings`+1)th token position\n-        self.assertFalse(torch.allclose(keys_with_short_factor, keys_with_long_factor, atol=1e-3, rtol=1e-3))\n-        # Last token generated using long factor\n-        torch.testing.assert_close(last_token_logits, regenerated_last_token_logits, rtol=1e-2, atol=1e-2)\n-\n \n @slow\n @require_torch"
        },
        {
            "sha": "a27695fa9d24c91911e471946118fae9a2bcc2c5",
            "filename": "tests/models/qwen2/test_modeling_qwen2.py",
            "status": "modified",
            "additions": 14,
            "deletions": 218,
            "changes": 232,
            "blob_url": "https://github.com/huggingface/transformers/blob/53fb245eb60364c7377c5f37fc37807a00e9b2e2/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/53fb245eb60364c7377c5f37fc37807a00e9b2e2/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py?ref=53fb245eb60364c7377c5f37fc37807a00e9b2e2",
            "patch": "@@ -33,11 +33,6 @@\n )\n from transformers.utils.import_utils import is_torch_greater_or_equal\n \n-from ...generation.test_utils import GenerationTesterMixin\n-from ...test_configuration_common import ConfigTester\n-from ...test_modeling_common import ModelTesterMixin, ids_tensor\n-from ...test_pipeline_mixin import PipelineTesterMixin\n-\n \n if is_torch_available():\n     import torch\n@@ -51,143 +46,21 @@\n     )\n \n \n-class Qwen2ModelTester:\n-    def __init__(\n-        self,\n-        parent,\n-        batch_size=13,\n-        seq_length=7,\n-        is_training=True,\n-        use_input_mask=True,\n-        use_token_type_ids=True,\n-        use_labels=True,\n-        vocab_size=99,\n-        hidden_size=32,\n-        num_hidden_layers=5,\n-        max_window_layers=3,\n-        use_sliding_window=True,\n-        sliding_window=50,\n-        num_attention_heads=4,\n-        num_key_value_heads=2,\n-        intermediate_size=37,\n-        hidden_act=\"gelu\",\n-        hidden_dropout_prob=0.1,\n-        attention_probs_dropout_prob=0.1,\n-        max_position_embeddings=512,\n-        type_vocab_size=16,\n-        type_sequence_label_size=2,\n-        initializer_range=0.02,\n-        num_labels=3,\n-        num_choices=4,\n-        pad_token_id=0,\n-        bos_token_id=1,\n-        scope=None,\n-    ):\n-        self.parent = parent\n-        self.batch_size = batch_size\n-        self.seq_length = seq_length\n-        self.is_training = is_training\n-        self.use_input_mask = use_input_mask\n-        self.use_token_type_ids = use_token_type_ids\n-        self.use_labels = use_labels\n-        self.vocab_size = vocab_size\n-        self.hidden_size = hidden_size\n-        self.num_hidden_layers = num_hidden_layers\n-        self.max_window_layers = max_window_layers\n-        self.use_sliding_window = use_sliding_window\n-        self.sliding_window = sliding_window\n-        self.num_attention_heads = num_attention_heads\n-        self.num_key_value_heads = num_key_value_heads\n-        self.intermediate_size = intermediate_size\n-        self.hidden_act = hidden_act\n-        self.hidden_dropout_prob = hidden_dropout_prob\n-        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n-        self.max_position_embeddings = max_position_embeddings\n-        self.type_vocab_size = type_vocab_size\n-        self.type_sequence_label_size = type_sequence_label_size\n-        self.initializer_range = initializer_range\n-        self.num_labels = num_labels\n-        self.num_choices = num_choices\n-        self.pad_token_id = pad_token_id\n-        self.bos_token_id = bos_token_id\n-        self.scope = scope\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.prepare_config_and_inputs\n-    def prepare_config_and_inputs(self):\n-        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n-\n-        input_mask = None\n-        if self.use_input_mask:\n-            input_mask = torch.tril(torch.ones_like(input_ids).to(torch_device))\n-\n-        token_type_ids = None\n-        if self.use_token_type_ids:\n-            token_type_ids = ids_tensor([self.batch_size, self.seq_length], self.type_vocab_size)\n-\n-        sequence_labels = None\n-        token_labels = None\n-        choice_labels = None\n-        if self.use_labels:\n-            sequence_labels = ids_tensor([self.batch_size], self.type_sequence_label_size)\n-            token_labels = ids_tensor([self.batch_size, self.seq_length], self.num_labels)\n-            choice_labels = ids_tensor([self.batch_size], self.num_choices)\n-\n-        config = self.get_config()\n-\n-        return config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-\n-    def get_config(self):\n-        return Qwen2Config(\n-            vocab_size=self.vocab_size,\n-            hidden_size=self.hidden_size,\n-            num_hidden_layers=self.num_hidden_layers,\n-            max_window_layers=self.max_window_layers,\n-            use_sliding_window=self.use_sliding_window,\n-            sliding_window=self.sliding_window,\n-            num_attention_heads=self.num_attention_heads,\n-            num_key_value_heads=self.num_key_value_heads,\n-            intermediate_size=self.intermediate_size,\n-            hidden_act=self.hidden_act,\n-            hidden_dropout_prob=self.hidden_dropout_prob,\n-            attention_probs_dropout_prob=self.attention_probs_dropout_prob,\n-            max_position_embeddings=self.max_position_embeddings,\n-            type_vocab_size=self.type_vocab_size,\n-            is_decoder=False,\n-            initializer_range=self.initializer_range,\n-            pad_token_id=self.pad_token_id,\n-            bos_token_id=self.bos_token_id,\n-        )\n+from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n \n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.create_and_check_model with Llama->Qwen2\n-    def create_and_check_model(\n-        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        model = Qwen2Model(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=input_mask)\n-        result = model(input_ids)\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.prepare_config_and_inputs_for_common\n-    def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-        (\n-            config,\n-            input_ids,\n-            token_type_ids,\n-            input_mask,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-        ) = config_and_inputs\n-        inputs_dict = {\"input_ids\": input_ids, \"attention_mask\": input_mask}\n-        return config, inputs_dict\n+\n+class Qwen2ModelTester(CausalLMModelTester):\n+    config_class = Qwen2Config\n+    if is_torch_available():\n+        base_model_class = Qwen2Model\n+        causal_lm_class = Qwen2ForCausalLM\n+        sequence_class = Qwen2ForSequenceClassification\n+        token_class = Qwen2ForTokenClassification\n+        question_answering_class = Qwen2ForQuestionAnswering\n \n \n @require_torch\n-# Copied from tests.models.mistral.test_modeling_mistral.MistralModelTest with Mistral->Qwen2\n-class Qwen2ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+class Qwen2ModelTest(CausalLMModelTest, unittest.TestCase):\n     all_model_classes = (\n         (\n             Qwen2Model,\n@@ -199,21 +72,20 @@ class Qwen2ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixi\n         if is_torch_available()\n         else ()\n     )\n+    test_headmasking = False\n+    test_pruning = False\n+    model_tester_class = Qwen2ModelTester\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": Qwen2Model,\n             \"text-classification\": Qwen2ForSequenceClassification,\n             \"token-classification\": Qwen2ForTokenClassification,\n             \"text-generation\": Qwen2ForCausalLM,\n-            \"zero-shot\": Qwen2ForSequenceClassification,\n             \"question-answering\": Qwen2ForQuestionAnswering,\n         }\n         if is_torch_available()\n         else {}\n     )\n-    test_headmasking = False\n-    test_pruning = False\n-    fx_compatible = False  # Broken by attention refactor cc @Cyrilvallez\n \n     # TODO (ydshieh): Check this. See https://app.circleci.com/pipelines/github/huggingface/transformers/79245/workflows/9490ef58-79c2-410d-8f51-e3495156cf9c/jobs/1012146\n     def is_pipeline_test_to_skip(\n@@ -228,82 +100,6 @@ def is_pipeline_test_to_skip(\n     ):\n         return True\n \n-    def setUp(self):\n-        self.model_tester = Qwen2ModelTester(self)\n-        self.config_tester = ConfigTester(self, config_class=Qwen2Config, hidden_size=37)\n-\n-    def test_config(self):\n-        self.config_tester.run_common_tests()\n-\n-    def test_model(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_model(*config_and_inputs)\n-\n-    def test_model_various_embeddings(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        for type in [\"absolute\", \"relative_key\", \"relative_key_query\"]:\n-            config_and_inputs[0].position_embedding_type = type\n-            self.model_tester.create_and_check_model(*config_and_inputs)\n-\n-    def test_torch_fx_output_loss(self):\n-        super().test_torch_fx_output_loss()\n-\n-    def test_Qwen2_sequence_classification_model(self):\n-        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.num_labels = 3\n-        input_ids = input_dict[\"input_ids\"]\n-        attention_mask = input_ids.ne(1).to(torch_device)\n-        sequence_labels = ids_tensor([self.model_tester.batch_size], self.model_tester.type_sequence_label_size)\n-        model = Qwen2ForSequenceClassification(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=attention_mask, labels=sequence_labels)\n-        self.assertEqual(result.logits.shape, (self.model_tester.batch_size, self.model_tester.num_labels))\n-\n-    def test_Qwen2_sequence_classification_model_for_single_label(self):\n-        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.num_labels = 3\n-        config.problem_type = \"single_label_classification\"\n-        input_ids = input_dict[\"input_ids\"]\n-        attention_mask = input_ids.ne(1).to(torch_device)\n-        sequence_labels = ids_tensor([self.model_tester.batch_size], self.model_tester.type_sequence_label_size)\n-        model = Qwen2ForSequenceClassification(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=attention_mask, labels=sequence_labels)\n-        self.assertEqual(result.logits.shape, (self.model_tester.batch_size, self.model_tester.num_labels))\n-\n-    def test_Qwen2_sequence_classification_model_for_multi_label(self):\n-        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.num_labels = 3\n-        config.problem_type = \"multi_label_classification\"\n-        input_ids = input_dict[\"input_ids\"]\n-        attention_mask = input_ids.ne(1).to(torch_device)\n-        sequence_labels = ids_tensor(\n-            [self.model_tester.batch_size, config.num_labels], self.model_tester.type_sequence_label_size\n-        ).to(torch.float)\n-        model = Qwen2ForSequenceClassification(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=attention_mask, labels=sequence_labels)\n-        self.assertEqual(result.logits.shape, (self.model_tester.batch_size, self.model_tester.num_labels))\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTest.test_llama_token_classification_model with Llama->Qwen2,llama->Qwen2\n-    def test_Qwen2_token_classification_model(self):\n-        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.num_labels = 3\n-        input_ids = input_dict[\"input_ids\"]\n-        attention_mask = input_ids.ne(1).to(torch_device)\n-        token_labels = ids_tensor([self.model_tester.batch_size, self.model_tester.seq_length], config.num_labels)\n-        model = Qwen2ForTokenClassification(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=attention_mask, labels=token_labels)\n-        self.assertEqual(\n-            result.logits.shape,\n-            (self.model_tester.batch_size, self.model_tester.seq_length, self.model_tester.num_labels),\n-        )\n-\n     @require_flash_attn\n     @require_torch_gpu\n     @pytest.mark.flash_attn_test"
        },
        {
            "sha": "dbfc7a1e6849d2a893c1d2f95ed74cb12fa1abaf",
            "filename": "tests/models/qwen2_moe/test_modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 12,
            "deletions": 245,
            "changes": 257,
            "blob_url": "https://github.com/huggingface/transformers/blob/53fb245eb60364c7377c5f37fc37807a00e9b2e2/tests%2Fmodels%2Fqwen2_moe%2Ftest_modeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/53fb245eb60364c7377c5f37fc37807a00e9b2e2/tests%2Fmodels%2Fqwen2_moe%2Ftest_modeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_moe%2Ftest_modeling_qwen2_moe.py?ref=53fb245eb60364c7377c5f37fc37807a00e9b2e2",
            "patch": "@@ -30,11 +30,6 @@\n     torch_device,\n )\n \n-from ...generation.test_utils import GenerationTesterMixin\n-from ...test_configuration_common import ConfigTester\n-from ...test_modeling_common import ModelTesterMixin, ids_tensor\n-from ...test_pipeline_mixin import PipelineTesterMixin\n-\n \n if is_torch_available():\n     import torch\n@@ -48,173 +43,21 @@\n     )\n \n \n-class Qwen2MoeModelTester:\n-    def __init__(\n-        self,\n-        parent,\n-        batch_size=13,\n-        seq_length=7,\n-        is_training=True,\n-        use_input_mask=True,\n-        use_token_type_ids=True,\n-        use_labels=True,\n-        vocab_size=99,\n-        hidden_size=32,\n-        num_hidden_layers=5,\n-        max_window_layers=3,\n-        use_sliding_window=True,\n-        sliding_window=50,\n-        num_attention_heads=4,\n-        num_key_value_heads=2,\n-        intermediate_size=37,\n-        hidden_act=\"gelu\",\n-        hidden_dropout_prob=0.1,\n-        attention_probs_dropout_prob=0.1,\n-        max_position_embeddings=512,\n-        expert_interval=1,\n-        moe_intermediate_size=12,\n-        shared_expert_intermediate_size=36,\n-        shared_expert_gate=True,\n-        num_experts_per_tok=2,\n-        num_experts=8,\n-        norm_topk_prob=False,\n-        output_router_logits=False,\n-        router_aux_loss_coef=0.001,\n-        type_vocab_size=16,\n-        type_sequence_label_size=2,\n-        initializer_range=0.02,\n-        num_labels=3,\n-        num_choices=4,\n-        pad_token_id=0,\n-        bos_token_id=1,\n-        scope=None,\n-        qkv_bias=False,\n-    ):\n-        self.parent = parent\n-        self.batch_size = batch_size\n-        self.seq_length = seq_length\n-        self.is_training = is_training\n-        self.use_input_mask = use_input_mask\n-        self.use_token_type_ids = use_token_type_ids\n-        self.use_labels = use_labels\n-        self.vocab_size = vocab_size\n-        self.hidden_size = hidden_size\n-        self.num_hidden_layers = num_hidden_layers\n-        self.max_window_layers = max_window_layers\n-        self.use_sliding_window = use_sliding_window\n-        self.sliding_window = sliding_window\n-        self.num_attention_heads = num_attention_heads\n-        self.num_key_value_heads = num_key_value_heads\n-        self.intermediate_size = intermediate_size\n-        self.hidden_act = hidden_act\n-        self.hidden_dropout_prob = hidden_dropout_prob\n-        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n-        self.max_position_embeddings = max_position_embeddings\n-        self.type_vocab_size = type_vocab_size\n-        self.type_sequence_label_size = type_sequence_label_size\n-        self.initializer_range = initializer_range\n-        self.num_labels = num_labels\n-        self.num_choices = num_choices\n-        self.pad_token_id = pad_token_id\n-        self.bos_token_id = bos_token_id\n-        self.scope = scope\n-        self.expert_interval = expert_interval\n-        self.moe_intermediate_size = moe_intermediate_size\n-        self.shared_expert_intermediate_size = shared_expert_intermediate_size\n-        self.shared_expert_gate = shared_expert_gate\n-        self.num_experts_per_tok = num_experts_per_tok\n-        self.num_experts = num_experts\n-        self.norm_topk_prob = norm_topk_prob\n-        self.output_router_logits = output_router_logits\n-        self.router_aux_loss_coef = router_aux_loss_coef\n-        self.qkv_bias = qkv_bias\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.prepare_config_and_inputs\n-    def prepare_config_and_inputs(self):\n-        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n-\n-        input_mask = None\n-        if self.use_input_mask:\n-            input_mask = torch.tril(torch.ones_like(input_ids).to(torch_device))\n-\n-        token_type_ids = None\n-        if self.use_token_type_ids:\n-            token_type_ids = ids_tensor([self.batch_size, self.seq_length], self.type_vocab_size)\n-\n-        sequence_labels = None\n-        token_labels = None\n-        choice_labels = None\n-        if self.use_labels:\n-            sequence_labels = ids_tensor([self.batch_size], self.type_sequence_label_size)\n-            token_labels = ids_tensor([self.batch_size, self.seq_length], self.num_labels)\n-            choice_labels = ids_tensor([self.batch_size], self.num_choices)\n-\n-        config = self.get_config()\n-\n-        return config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-\n-    def get_config(self):\n-        return Qwen2MoeConfig(\n-            vocab_size=self.vocab_size,\n-            hidden_size=self.hidden_size,\n-            num_hidden_layers=self.num_hidden_layers,\n-            max_window_layers=self.max_window_layers,\n-            use_sliding_window=self.use_sliding_window,\n-            sliding_window=self.sliding_window,\n-            num_attention_heads=self.num_attention_heads,\n-            num_key_value_heads=self.num_key_value_heads,\n-            intermediate_size=self.intermediate_size,\n-            hidden_act=self.hidden_act,\n-            hidden_dropout_prob=self.hidden_dropout_prob,\n-            attention_probs_dropout_prob=self.attention_probs_dropout_prob,\n-            max_position_embeddings=self.max_position_embeddings,\n-            expert_interval=self.expert_interval,\n-            moe_intermediate_size=self.moe_intermediate_size,\n-            shared_expert_intermediate_size=self.shared_expert_intermediate_size,\n-            shared_expert_gate=self.shared_expert_gate,\n-            num_experts_per_tok=self.num_experts_per_tok,\n-            num_experts=self.num_experts,\n-            norm_topk_prob=self.norm_topk_prob,\n-            output_router_logits=self.output_router_logits,\n-            router_aux_loss_coef=self.router_aux_loss_coef,\n-            type_vocab_size=self.type_vocab_size,\n-            is_decoder=False,\n-            initializer_range=self.initializer_range,\n-            pad_token_id=self.pad_token_id,\n-            bos_token_id=self.bos_token_id,\n-            qkv_bias=self.qkv_bias,\n-        )\n+from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n \n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.create_and_check_model with Llama->Qwen2Moe\n-    def create_and_check_model(\n-        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        model = Qwen2MoeModel(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=input_mask)\n-        result = model(input_ids)\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n \n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.prepare_config_and_inputs_for_common\n-    def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-        (\n-            config,\n-            input_ids,\n-            token_type_ids,\n-            input_mask,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-        ) = config_and_inputs\n-        inputs_dict = {\"input_ids\": input_ids, \"attention_mask\": input_mask}\n-        return config, inputs_dict\n+class Qwen2MoeModelTester(CausalLMModelTester):\n+    config_class = Qwen2MoeConfig\n+    if is_torch_available():\n+        base_model_class = Qwen2MoeModel\n+        causal_lm_class = Qwen2MoeForCausalLM\n+        sequence_class = Qwen2MoeForSequenceClassification\n+        token_class = Qwen2MoeForTokenClassification\n+        question_answering_class = Qwen2MoeForQuestionAnswering\n \n \n @require_torch\n-# Copied from tests.models.mistral.test_modeling_mistral.MistralModelTest with Mistral->Qwen2Moe\n-class Qwen2MoeModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+class Qwen2MoeModelTest(CausalLMModelTest, unittest.TestCase):\n     all_model_classes = (\n         (\n             Qwen2MoeModel,\n@@ -232,15 +75,15 @@ class Qwen2MoeModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterM\n             \"text-classification\": Qwen2MoeForSequenceClassification,\n             \"token-classification\": Qwen2MoeForTokenClassification,\n             \"text-generation\": Qwen2MoeForCausalLM,\n-            \"zero-shot\": Qwen2MoeForSequenceClassification,\n             \"question-answering\": Qwen2MoeForQuestionAnswering,\n         }\n         if is_torch_available()\n         else {}\n     )\n+\n     test_headmasking = False\n     test_pruning = False\n-    fx_compatible = False  # Broken by attention refactor cc @Cyrilvallez\n+    model_tester_class = Qwen2MoeModelTester\n \n     # TODO (ydshieh): Check this. See https://app.circleci.com/pipelines/github/huggingface/transformers/79245/workflows/9490ef58-79c2-410d-8f51-e3495156cf9c/jobs/1012146\n     def is_pipeline_test_to_skip(\n@@ -255,82 +98,6 @@ def is_pipeline_test_to_skip(\n     ):\n         return True\n \n-    def setUp(self):\n-        self.model_tester = Qwen2MoeModelTester(self)\n-        self.config_tester = ConfigTester(self, config_class=Qwen2MoeConfig, hidden_size=37)\n-\n-    def test_config(self):\n-        self.config_tester.run_common_tests()\n-\n-    def test_model(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_model(*config_and_inputs)\n-\n-    def test_model_various_embeddings(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        for type in [\"absolute\", \"relative_key\", \"relative_key_query\"]:\n-            config_and_inputs[0].position_embedding_type = type\n-            self.model_tester.create_and_check_model(*config_and_inputs)\n-\n-    def test_torch_fx_output_loss(self):\n-        super().test_torch_fx_output_loss()\n-\n-    def test_Qwen2Moe_sequence_classification_model(self):\n-        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.num_labels = 3\n-        input_ids = input_dict[\"input_ids\"]\n-        attention_mask = input_ids.ne(1).to(torch_device)\n-        sequence_labels = ids_tensor([self.model_tester.batch_size], self.model_tester.type_sequence_label_size)\n-        model = Qwen2MoeForSequenceClassification(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=attention_mask, labels=sequence_labels)\n-        self.assertEqual(result.logits.shape, (self.model_tester.batch_size, self.model_tester.num_labels))\n-\n-    def test_Qwen2Moe_sequence_classification_model_for_single_label(self):\n-        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.num_labels = 3\n-        config.problem_type = \"single_label_classification\"\n-        input_ids = input_dict[\"input_ids\"]\n-        attention_mask = input_ids.ne(1).to(torch_device)\n-        sequence_labels = ids_tensor([self.model_tester.batch_size], self.model_tester.type_sequence_label_size)\n-        model = Qwen2MoeForSequenceClassification(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=attention_mask, labels=sequence_labels)\n-        self.assertEqual(result.logits.shape, (self.model_tester.batch_size, self.model_tester.num_labels))\n-\n-    def test_Qwen2Moe_sequence_classification_model_for_multi_label(self):\n-        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.num_labels = 3\n-        config.problem_type = \"multi_label_classification\"\n-        input_ids = input_dict[\"input_ids\"]\n-        attention_mask = input_ids.ne(1).to(torch_device)\n-        sequence_labels = ids_tensor(\n-            [self.model_tester.batch_size, config.num_labels], self.model_tester.type_sequence_label_size\n-        ).to(torch.float)\n-        model = Qwen2MoeForSequenceClassification(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=attention_mask, labels=sequence_labels)\n-        self.assertEqual(result.logits.shape, (self.model_tester.batch_size, self.model_tester.num_labels))\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTest.test_llama_token_classification_model with Llama->Qwen2Moe,llama->Qwen2Moe\n-    def test_Qwen2Moe_token_classification_model(self):\n-        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.num_labels = 3\n-        input_ids = input_dict[\"input_ids\"]\n-        attention_mask = input_ids.ne(1).to(torch_device)\n-        token_labels = ids_tensor([self.model_tester.batch_size, self.model_tester.seq_length], config.num_labels)\n-        model = Qwen2MoeForTokenClassification(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=attention_mask, labels=token_labels)\n-        self.assertEqual(\n-            result.logits.shape,\n-            (self.model_tester.batch_size, self.model_tester.seq_length, self.model_tester.num_labels),\n-        )\n-\n     @require_flash_attn\n     @require_torch_gpu\n     @pytest.mark.flash_attn_test"
        },
        {
            "sha": "884c1ea077b8a5b06a995769aef7499178d446db",
            "filename": "tests/models/qwen3/test_modeling_qwen3.py",
            "status": "modified",
            "additions": 13,
            "deletions": 221,
            "changes": 234,
            "blob_url": "https://github.com/huggingface/transformers/blob/53fb245eb60364c7377c5f37fc37807a00e9b2e2/tests%2Fmodels%2Fqwen3%2Ftest_modeling_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/53fb245eb60364c7377c5f37fc37807a00e9b2e2/tests%2Fmodels%2Fqwen3%2Ftest_modeling_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen3%2Ftest_modeling_qwen3.py?ref=53fb245eb60364c7377c5f37fc37807a00e9b2e2",
            "patch": "@@ -33,11 +33,6 @@\n )\n from transformers.utils.import_utils import is_torch_greater_or_equal\n \n-from ...generation.test_utils import GenerationTesterMixin\n-from ...test_configuration_common import ConfigTester\n-from ...test_modeling_common import ModelTesterMixin, ids_tensor\n-from ...test_pipeline_mixin import PipelineTesterMixin\n-\n \n if is_torch_available():\n     import torch\n@@ -50,147 +45,21 @@\n         Qwen3Model,\n     )\n \n+from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n \n-class Qwen3ModelTester:\n-    def __init__(\n-        self,\n-        parent,\n-        batch_size=13,\n-        seq_length=7,\n-        is_training=True,\n-        use_input_mask=True,\n-        use_token_type_ids=True,\n-        use_labels=True,\n-        vocab_size=99,\n-        hidden_size=64,\n-        num_hidden_layers=5,\n-        max_window_layers=3,\n-        use_sliding_window=True,\n-        sliding_window=50,\n-        num_attention_heads=4,\n-        num_key_value_heads=2,\n-        head_dim=16,\n-        intermediate_size=37,\n-        hidden_act=\"gelu\",\n-        hidden_dropout_prob=0.1,\n-        attention_probs_dropout_prob=0.1,\n-        max_position_embeddings=512,\n-        type_vocab_size=16,\n-        type_sequence_label_size=2,\n-        initializer_range=0.02,\n-        num_labels=3,\n-        num_choices=4,\n-        pad_token_id=0,\n-        bos_token_id=1,\n-        scope=None,\n-    ):\n-        self.parent = parent\n-        self.batch_size = batch_size\n-        self.seq_length = seq_length\n-        self.is_training = is_training\n-        self.use_input_mask = use_input_mask\n-        self.use_token_type_ids = use_token_type_ids\n-        self.use_labels = use_labels\n-        self.vocab_size = vocab_size\n-        self.hidden_size = hidden_size\n-        self.num_hidden_layers = num_hidden_layers\n-        self.max_window_layers = max_window_layers\n-        self.use_sliding_window = use_sliding_window\n-        self.sliding_window = sliding_window\n-        self.num_attention_heads = num_attention_heads\n-        self.num_key_value_heads = num_key_value_heads\n-        self.head_dim = head_dim\n-        self.intermediate_size = intermediate_size\n-        self.hidden_act = hidden_act\n-        self.hidden_dropout_prob = hidden_dropout_prob\n-        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n-        self.max_position_embeddings = max_position_embeddings\n-        self.type_vocab_size = type_vocab_size\n-        self.type_sequence_label_size = type_sequence_label_size\n-        self.initializer_range = initializer_range\n-        self.num_labels = num_labels\n-        self.num_choices = num_choices\n-        self.pad_token_id = pad_token_id\n-        self.bos_token_id = bos_token_id\n-        self.scope = scope\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.prepare_config_and_inputs\n-    def prepare_config_and_inputs(self):\n-        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n-\n-        input_mask = None\n-        if self.use_input_mask:\n-            input_mask = torch.tril(torch.ones_like(input_ids).to(torch_device))\n-\n-        token_type_ids = None\n-        if self.use_token_type_ids:\n-            token_type_ids = ids_tensor([self.batch_size, self.seq_length], self.type_vocab_size)\n-\n-        sequence_labels = None\n-        token_labels = None\n-        choice_labels = None\n-        if self.use_labels:\n-            sequence_labels = ids_tensor([self.batch_size], self.type_sequence_label_size)\n-            token_labels = ids_tensor([self.batch_size, self.seq_length], self.num_labels)\n-            choice_labels = ids_tensor([self.batch_size], self.num_choices)\n-\n-        config = self.get_config()\n-\n-        return config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-\n-    def get_config(self):\n-        return Qwen3Config(\n-            vocab_size=self.vocab_size,\n-            hidden_size=self.hidden_size,\n-            num_hidden_layers=self.num_hidden_layers,\n-            max_window_layers=self.max_window_layers,\n-            use_sliding_window=self.use_sliding_window,\n-            sliding_window=self.sliding_window,\n-            num_attention_heads=self.num_attention_heads,\n-            num_key_value_heads=self.num_key_value_heads,\n-            head_dim=self.head_dim,\n-            intermediate_size=self.intermediate_size,\n-            hidden_act=self.hidden_act,\n-            hidden_dropout_prob=self.hidden_dropout_prob,\n-            attention_probs_dropout_prob=self.attention_probs_dropout_prob,\n-            max_position_embeddings=self.max_position_embeddings,\n-            type_vocab_size=self.type_vocab_size,\n-            is_decoder=False,\n-            initializer_range=self.initializer_range,\n-            pad_token_id=self.pad_token_id,\n-            bos_token_id=self.bos_token_id,\n-        )\n \n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.create_and_check_model with Llama->Qwen3\n-    def create_and_check_model(\n-        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        model = Qwen3Model(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=input_mask)\n-        result = model(input_ids)\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.prepare_config_and_inputs_for_common\n-    def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-        (\n-            config,\n-            input_ids,\n-            token_type_ids,\n-            input_mask,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-        ) = config_and_inputs\n-        inputs_dict = {\"input_ids\": input_ids, \"attention_mask\": input_mask}\n-        return config, inputs_dict\n+class Qwen3ModelTester(CausalLMModelTester):\n+    config_class = Qwen3Config\n+    if is_torch_available():\n+        base_model_class = Qwen3Model\n+        causal_lm_class = Qwen3ForCausalLM\n+        sequence_class = Qwen3ForSequenceClassification\n+        token_class = Qwen3ForTokenClassification\n+        question_answering_class = Qwen3ForQuestionAnswering\n \n \n @require_torch\n-# Copied from tests.models.mistral.test_modeling_mistral.MistralModelTest with Mistral->Qwen3\n-class Qwen3ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+class Qwen3ModelTest(CausalLMModelTest, unittest.TestCase):\n     all_model_classes = (\n         (\n             Qwen3Model,\n@@ -202,21 +71,20 @@ class Qwen3ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixi\n         if is_torch_available()\n         else ()\n     )\n+    test_headmasking = False\n+    test_pruning = False\n+    model_tester_class = Qwen3ModelTester\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": Qwen3Model,\n             \"text-classification\": Qwen3ForSequenceClassification,\n             \"token-classification\": Qwen3ForTokenClassification,\n             \"text-generation\": Qwen3ForCausalLM,\n-            \"zero-shot\": Qwen3ForSequenceClassification,\n             \"question-answering\": Qwen3ForQuestionAnswering,\n         }\n         if is_torch_available()\n         else {}\n     )\n-    test_headmasking = False\n-    test_pruning = False\n-    fx_compatible = False  # Broken by attention refactor cc @Cyrilvallez\n \n     # TODO (ydshieh): Check this. See https://app.circleci.com/pipelines/github/huggingface/transformers/79245/workflows/9490ef58-79c2-410d-8f51-e3495156cf9c/jobs/1012146\n     def is_pipeline_test_to_skip(\n@@ -231,82 +99,6 @@ def is_pipeline_test_to_skip(\n     ):\n         return True\n \n-    def setUp(self):\n-        self.model_tester = Qwen3ModelTester(self)\n-        self.config_tester = ConfigTester(self, config_class=Qwen3Config, hidden_size=37)\n-\n-    def test_config(self):\n-        self.config_tester.run_common_tests()\n-\n-    def test_model(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_model(*config_and_inputs)\n-\n-    def test_model_various_embeddings(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        for type in [\"absolute\", \"relative_key\", \"relative_key_query\"]:\n-            config_and_inputs[0].position_embedding_type = type\n-            self.model_tester.create_and_check_model(*config_and_inputs)\n-\n-    def test_torch_fx_output_loss(self):\n-        super().test_torch_fx_output_loss()\n-\n-    def test_Qwen3_sequence_classification_model(self):\n-        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.num_labels = 3\n-        input_ids = input_dict[\"input_ids\"]\n-        attention_mask = input_ids.ne(1).to(torch_device)\n-        sequence_labels = ids_tensor([self.model_tester.batch_size], self.model_tester.type_sequence_label_size)\n-        model = Qwen3ForSequenceClassification(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=attention_mask, labels=sequence_labels)\n-        self.assertEqual(result.logits.shape, (self.model_tester.batch_size, self.model_tester.num_labels))\n-\n-    def test_Qwen3_sequence_classification_model_for_single_label(self):\n-        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.num_labels = 3\n-        config.problem_type = \"single_label_classification\"\n-        input_ids = input_dict[\"input_ids\"]\n-        attention_mask = input_ids.ne(1).to(torch_device)\n-        sequence_labels = ids_tensor([self.model_tester.batch_size], self.model_tester.type_sequence_label_size)\n-        model = Qwen3ForSequenceClassification(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=attention_mask, labels=sequence_labels)\n-        self.assertEqual(result.logits.shape, (self.model_tester.batch_size, self.model_tester.num_labels))\n-\n-    def test_Qwen3_sequence_classification_model_for_multi_label(self):\n-        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.num_labels = 3\n-        config.problem_type = \"multi_label_classification\"\n-        input_ids = input_dict[\"input_ids\"]\n-        attention_mask = input_ids.ne(1).to(torch_device)\n-        sequence_labels = ids_tensor(\n-            [self.model_tester.batch_size, config.num_labels], self.model_tester.type_sequence_label_size\n-        ).to(torch.float)\n-        model = Qwen3ForSequenceClassification(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=attention_mask, labels=sequence_labels)\n-        self.assertEqual(result.logits.shape, (self.model_tester.batch_size, self.model_tester.num_labels))\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTest.test_llama_token_classification_model with Llama->Qwen3,llama->Qwen3\n-    def test_Qwen3_token_classification_model(self):\n-        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.num_labels = 3\n-        input_ids = input_dict[\"input_ids\"]\n-        attention_mask = input_ids.ne(1).to(torch_device)\n-        token_labels = ids_tensor([self.model_tester.batch_size, self.model_tester.seq_length], config.num_labels)\n-        model = Qwen3ForTokenClassification(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=attention_mask, labels=token_labels)\n-        self.assertEqual(\n-            result.logits.shape,\n-            (self.model_tester.batch_size, self.model_tester.seq_length, self.model_tester.num_labels),\n-        )\n-\n     @require_flash_attn\n     @require_torch_gpu\n     @pytest.mark.flash_attn_test"
        },
        {
            "sha": "0ffb74c6c244ffdef23c97f719912d0c7558aa28",
            "filename": "tests/models/qwen3_moe/test_modeling_qwen3_moe.py",
            "status": "modified",
            "additions": 14,
            "deletions": 242,
            "changes": 256,
            "blob_url": "https://github.com/huggingface/transformers/blob/53fb245eb60364c7377c5f37fc37807a00e9b2e2/tests%2Fmodels%2Fqwen3_moe%2Ftest_modeling_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/53fb245eb60364c7377c5f37fc37807a00e9b2e2/tests%2Fmodels%2Fqwen3_moe%2Ftest_modeling_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen3_moe%2Ftest_modeling_qwen3_moe.py?ref=53fb245eb60364c7377c5f37fc37807a00e9b2e2",
            "patch": "@@ -30,185 +30,33 @@\n     torch_device,\n )\n \n-from ...generation.test_utils import GenerationTesterMixin\n-from ...test_configuration_common import ConfigTester\n-from ...test_modeling_common import ModelTesterMixin, ids_tensor\n-from ...test_pipeline_mixin import PipelineTesterMixin\n-\n \n if is_torch_available():\n     import torch\n \n     from transformers import (\n+        Qwen3ForQuestionAnswering,\n         Qwen3MoeForCausalLM,\n         Qwen3MoeForQuestionAnswering,\n         Qwen3MoeForSequenceClassification,\n         Qwen3MoeForTokenClassification,\n         Qwen3MoeModel,\n     )\n+from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n \n \n-class Qwen3MoeModelTester:\n-    def __init__(\n-        self,\n-        parent,\n-        batch_size=13,\n-        seq_length=7,\n-        is_training=True,\n-        use_input_mask=True,\n-        use_token_type_ids=True,\n-        use_labels=True,\n-        vocab_size=99,\n-        hidden_size=64,\n-        num_hidden_layers=5,\n-        max_window_layers=3,\n-        use_sliding_window=True,\n-        sliding_window=50,\n-        num_attention_heads=4,\n-        num_key_value_heads=2,\n-        head_dim=16,\n-        intermediate_size=37,\n-        hidden_act=\"gelu\",\n-        hidden_dropout_prob=0.1,\n-        attention_probs_dropout_prob=0.1,\n-        max_position_embeddings=512,\n-        expert_interval=1,\n-        moe_intermediate_size=12,\n-        num_experts_per_tok=2,\n-        num_experts=8,\n-        norm_topk_prob=False,\n-        output_router_logits=False,\n-        router_aux_loss_coef=0.001,\n-        type_vocab_size=16,\n-        type_sequence_label_size=2,\n-        initializer_range=0.02,\n-        num_labels=3,\n-        num_choices=4,\n-        pad_token_id=0,\n-        bos_token_id=1,\n-        scope=None,\n-    ):\n-        self.parent = parent\n-        self.batch_size = batch_size\n-        self.seq_length = seq_length\n-        self.is_training = is_training\n-        self.use_input_mask = use_input_mask\n-        self.use_token_type_ids = use_token_type_ids\n-        self.use_labels = use_labels\n-        self.vocab_size = vocab_size\n-        self.hidden_size = hidden_size\n-        self.num_hidden_layers = num_hidden_layers\n-        self.max_window_layers = max_window_layers\n-        self.use_sliding_window = use_sliding_window\n-        self.sliding_window = sliding_window\n-        self.num_attention_heads = num_attention_heads\n-        self.num_key_value_heads = num_key_value_heads\n-        self.head_dim = head_dim\n-        self.intermediate_size = intermediate_size\n-        self.hidden_act = hidden_act\n-        self.hidden_dropout_prob = hidden_dropout_prob\n-        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n-        self.max_position_embeddings = max_position_embeddings\n-        self.type_vocab_size = type_vocab_size\n-        self.type_sequence_label_size = type_sequence_label_size\n-        self.initializer_range = initializer_range\n-        self.num_labels = num_labels\n-        self.num_choices = num_choices\n-        self.pad_token_id = pad_token_id\n-        self.bos_token_id = bos_token_id\n-        self.scope = scope\n-        self.expert_interval = expert_interval\n-        self.moe_intermediate_size = moe_intermediate_size\n-        self.num_experts_per_tok = num_experts_per_tok\n-        self.num_experts = num_experts\n-        self.norm_topk_prob = norm_topk_prob\n-        self.output_router_logits = output_router_logits\n-        self.router_aux_loss_coef = router_aux_loss_coef\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.prepare_config_and_inputs\n-    def prepare_config_and_inputs(self):\n-        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n-\n-        input_mask = None\n-        if self.use_input_mask:\n-            input_mask = torch.tril(torch.ones_like(input_ids).to(torch_device))\n-\n-        token_type_ids = None\n-        if self.use_token_type_ids:\n-            token_type_ids = ids_tensor([self.batch_size, self.seq_length], self.type_vocab_size)\n-\n-        sequence_labels = None\n-        token_labels = None\n-        choice_labels = None\n-        if self.use_labels:\n-            sequence_labels = ids_tensor([self.batch_size], self.type_sequence_label_size)\n-            token_labels = ids_tensor([self.batch_size, self.seq_length], self.num_labels)\n-            choice_labels = ids_tensor([self.batch_size], self.num_choices)\n-\n-        config = self.get_config()\n-\n-        return config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-\n-    def get_config(self):\n-        return Qwen3MoeConfig(\n-            vocab_size=self.vocab_size,\n-            hidden_size=self.hidden_size,\n-            num_hidden_layers=self.num_hidden_layers,\n-            max_window_layers=self.max_window_layers,\n-            use_sliding_window=self.use_sliding_window,\n-            sliding_window=self.sliding_window,\n-            num_attention_heads=self.num_attention_heads,\n-            num_key_value_heads=self.num_key_value_heads,\n-            head_dim=self.head_dim,\n-            intermediate_size=self.intermediate_size,\n-            hidden_act=self.hidden_act,\n-            hidden_dropout_prob=self.hidden_dropout_prob,\n-            attention_probs_dropout_prob=self.attention_probs_dropout_prob,\n-            max_position_embeddings=self.max_position_embeddings,\n-            expert_interval=self.expert_interval,\n-            moe_intermediate_size=self.moe_intermediate_size,\n-            num_experts_per_tok=self.num_experts_per_tok,\n-            num_experts=self.num_experts,\n-            norm_topk_prob=self.norm_topk_prob,\n-            output_router_logits=self.output_router_logits,\n-            router_aux_loss_coef=self.router_aux_loss_coef,\n-            type_vocab_size=self.type_vocab_size,\n-            is_decoder=False,\n-            initializer_range=self.initializer_range,\n-            pad_token_id=self.pad_token_id,\n-            bos_token_id=self.bos_token_id,\n-        )\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.create_and_check_model with Llama->Qwen3Moe\n-    def create_and_check_model(\n-        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        model = Qwen3MoeModel(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=input_mask)\n-        result = model(input_ids)\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.prepare_config_and_inputs_for_common\n-    def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-        (\n-            config,\n-            input_ids,\n-            token_type_ids,\n-            input_mask,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-        ) = config_and_inputs\n-        inputs_dict = {\"input_ids\": input_ids, \"attention_mask\": input_mask}\n-        return config, inputs_dict\n+class Qwen3MoeModelTester(CausalLMModelTester):\n+    config_class = Qwen3MoeConfig\n+    if is_torch_available():\n+        base_model_class = Qwen3MoeModel\n+        causal_lm_class = Qwen3MoeForCausalLM\n+        sequence_class = Qwen3MoeForSequenceClassification\n+        token_class = Qwen3MoeForTokenClassification\n+        question_answering_class = Qwen3MoeForQuestionAnswering\n \n \n @require_torch\n-# Copied from tests.models.mistral.test_modeling_mistral.MistralModelTest with Mistral->Qwen3Moe\n-class Qwen3MoeModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+class Qwen3MoeModelTest(CausalLMModelTest, unittest.TestCase):\n     all_model_classes = (\n         (\n             Qwen3MoeModel,\n@@ -226,15 +74,15 @@ class Qwen3MoeModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterM\n             \"text-classification\": Qwen3MoeForSequenceClassification,\n             \"token-classification\": Qwen3MoeForTokenClassification,\n             \"text-generation\": Qwen3MoeForCausalLM,\n-            \"zero-shot\": Qwen3MoeForSequenceClassification,\n-            \"question-answering\": Qwen3MoeForQuestionAnswering,\n+            \"question-answering\": Qwen3ForQuestionAnswering,\n         }\n         if is_torch_available()\n         else {}\n     )\n+\n     test_headmasking = False\n     test_pruning = False\n-    fx_compatible = False  # Broken by attention refactor cc @Cyrilvallez\n+    model_tester_class = Qwen3MoeModelTester\n \n     # TODO (ydshieh): Check this. See https://app.circleci.com/pipelines/github/huggingface/transformers/79245/workflows/9490ef58-79c2-410d-8f51-e3495156cf9c/jobs/1012146\n     def is_pipeline_test_to_skip(\n@@ -249,82 +97,6 @@ def is_pipeline_test_to_skip(\n     ):\n         return True\n \n-    def setUp(self):\n-        self.model_tester = Qwen3MoeModelTester(self)\n-        self.config_tester = ConfigTester(self, config_class=Qwen3MoeConfig, hidden_size=37)\n-\n-    def test_config(self):\n-        self.config_tester.run_common_tests()\n-\n-    def test_model(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_model(*config_and_inputs)\n-\n-    def test_model_various_embeddings(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        for type in [\"absolute\", \"relative_key\", \"relative_key_query\"]:\n-            config_and_inputs[0].position_embedding_type = type\n-            self.model_tester.create_and_check_model(*config_and_inputs)\n-\n-    def test_torch_fx_output_loss(self):\n-        super().test_torch_fx_output_loss()\n-\n-    def test_Qwen3Moe_sequence_classification_model(self):\n-        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.num_labels = 3\n-        input_ids = input_dict[\"input_ids\"]\n-        attention_mask = input_ids.ne(1).to(torch_device)\n-        sequence_labels = ids_tensor([self.model_tester.batch_size], self.model_tester.type_sequence_label_size)\n-        model = Qwen3MoeForSequenceClassification(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=attention_mask, labels=sequence_labels)\n-        self.assertEqual(result.logits.shape, (self.model_tester.batch_size, self.model_tester.num_labels))\n-\n-    def test_Qwen3Moe_sequence_classification_model_for_single_label(self):\n-        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.num_labels = 3\n-        config.problem_type = \"single_label_classification\"\n-        input_ids = input_dict[\"input_ids\"]\n-        attention_mask = input_ids.ne(1).to(torch_device)\n-        sequence_labels = ids_tensor([self.model_tester.batch_size], self.model_tester.type_sequence_label_size)\n-        model = Qwen3MoeForSequenceClassification(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=attention_mask, labels=sequence_labels)\n-        self.assertEqual(result.logits.shape, (self.model_tester.batch_size, self.model_tester.num_labels))\n-\n-    def test_Qwen3Moe_sequence_classification_model_for_multi_label(self):\n-        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.num_labels = 3\n-        config.problem_type = \"multi_label_classification\"\n-        input_ids = input_dict[\"input_ids\"]\n-        attention_mask = input_ids.ne(1).to(torch_device)\n-        sequence_labels = ids_tensor(\n-            [self.model_tester.batch_size, config.num_labels], self.model_tester.type_sequence_label_size\n-        ).to(torch.float)\n-        model = Qwen3MoeForSequenceClassification(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=attention_mask, labels=sequence_labels)\n-        self.assertEqual(result.logits.shape, (self.model_tester.batch_size, self.model_tester.num_labels))\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTest.test_llama_token_classification_model with Llama->Qwen3Moe,llama->Qwen3Moe\n-    def test_Qwen3Moe_token_classification_model(self):\n-        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.num_labels = 3\n-        input_ids = input_dict[\"input_ids\"]\n-        attention_mask = input_ids.ne(1).to(torch_device)\n-        token_labels = ids_tensor([self.model_tester.batch_size, self.model_tester.seq_length], config.num_labels)\n-        model = Qwen3MoeForTokenClassification(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=attention_mask, labels=token_labels)\n-        self.assertEqual(\n-            result.logits.shape,\n-            (self.model_tester.batch_size, self.model_tester.seq_length, self.model_tester.num_labels),\n-        )\n-\n     @require_flash_attn\n     @require_torch_gpu\n     @pytest.mark.flash_attn_test"
        },
        {
            "sha": "f3d8b15dde75b8bc4ba088dc84031f70b80287f4",
            "filename": "tests/models/recurrent_gemma/test_modeling_recurrent_gemma.py",
            "status": "modified",
            "additions": 76,
            "deletions": 178,
            "changes": 254,
            "blob_url": "https://github.com/huggingface/transformers/blob/53fb245eb60364c7377c5f37fc37807a00e9b2e2/tests%2Fmodels%2Frecurrent_gemma%2Ftest_modeling_recurrent_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/53fb245eb60364c7377c5f37fc37807a00e9b2e2/tests%2Fmodels%2Frecurrent_gemma%2Ftest_modeling_recurrent_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frecurrent_gemma%2Ftest_modeling_recurrent_gemma.py?ref=53fb245eb60364c7377c5f37fc37807a00e9b2e2",
            "patch": "@@ -16,6 +16,7 @@\n import unittest\n \n import pytest\n+from parameterized import parameterized\n \n from transformers import AutoModelForCausalLM, AutoTokenizer, RecurrentGemmaConfig, is_torch_available, set_seed\n from transformers.testing_utils import (\n@@ -27,151 +28,26 @@\n     torch_device,\n )\n \n-from ...test_configuration_common import ConfigTester\n-from ...test_modeling_common import ModelTesterMixin, ids_tensor\n-from ...test_pipeline_mixin import PipelineTesterMixin\n-\n \n if is_torch_available():\n     import torch\n \n-    from transformers import RecurrentGemmaForCausalLM, RecurrentGemmaModel\n-\n-\n-class RecurrentGemmaModelTester:\n-    def __init__(\n-        self,\n-        parent,\n-        batch_size=13,\n-        seq_length=12,\n-        is_training=True,\n-        use_input_mask=True,\n-        use_token_type_ids=False,\n-        use_labels=True,\n-        num_hidden_layers=3,\n-        vocab_size=99,\n-        hidden_size=32,\n-        intermediate_size=3 * 32,\n-        num_attention_heads=2,\n-        lru_width=2 * 32,\n-        embeddings_scale_by_sqrt_dim=True,\n-        attention_window_size=16,\n-        conv1d_width=4,\n-        logits_soft_cap=30.0,\n-        rms_norm_eps=1e-6,\n-        use_cache=True,\n-        rope_theta=10000.0,\n-        type_vocab_size=16,\n-        type_sequence_label_size=2,\n-        num_labels=3,\n-        num_choices=4,\n-        pad_token_id=0,\n-        scope=None,\n-    ):\n-        self.parent = parent\n-        self.batch_size = batch_size\n-        self.seq_length = seq_length\n-        self.is_training = is_training\n-        self.use_input_mask = use_input_mask\n-        self.use_token_type_ids = use_token_type_ids\n-        self.use_labels = use_labels\n-\n-        self.num_hidden_layers = num_hidden_layers\n-        self.vocab_size = vocab_size\n-        self.hidden_size = hidden_size\n-        self.intermediate_size = intermediate_size\n-        self.num_attention_heads = num_attention_heads\n-        self.lru_width = lru_width if lru_width is not None else hidden_size\n-        self.embeddings_scale_by_sqrt_dim = embeddings_scale_by_sqrt_dim\n-        self.attention_window_size = attention_window_size\n-        self.conv1d_width = conv1d_width\n-        self.logits_soft_cap = logits_soft_cap\n-        self.rms_norm_eps = rms_norm_eps\n-        self.use_cache = use_cache\n-        self.rope_theta = rope_theta\n-\n-        self.type_vocab_size = type_vocab_size\n-        self.type_sequence_label_size = type_sequence_label_size\n-        self.num_labels = num_labels\n-        self.num_choices = num_choices\n-        self.pad_token_id = pad_token_id\n-        self.scope = scope\n-\n-    # Copied from tests.models.mistral.test_modeling_mistral.MistralModelTester.prepare_config_and_inputs\n-    def prepare_config_and_inputs(self):\n-        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n-\n-        input_mask = None\n-        if self.use_input_mask:\n-            input_mask = torch.tril(torch.ones_like(input_ids).to(torch_device))\n-\n-        token_type_ids = None\n-        if self.use_token_type_ids:\n-            token_type_ids = ids_tensor([self.batch_size, self.seq_length], self.type_vocab_size)\n-\n-        sequence_labels = None\n-        token_labels = None\n-        choice_labels = None\n-        if self.use_labels:\n-            sequence_labels = ids_tensor([self.batch_size], self.type_sequence_label_size)\n-            token_labels = ids_tensor([self.batch_size, self.seq_length], self.num_labels)\n-            choice_labels = ids_tensor([self.batch_size], self.num_choices)\n-\n-        config = self.get_config()\n-\n-        return config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-\n-    def get_config(self):\n-        return RecurrentGemmaConfig(\n-            num_hidden_layers=self.num_hidden_layers,\n-            vocab_size=self.vocab_size,\n-            hidden_size=self.hidden_size,\n-            intermediate_size=self.intermediate_size,\n-            num_attention_heads=self.num_attention_heads,\n-            lru_width=self.lru_width,\n-            embeddings_scale_by_sqrt_dim=self.embeddings_scale_by_sqrt_dim,\n-            attention_window_size=self.attention_window_size,\n-            conv1d_width=self.conv1d_width,\n-            logits_soft_cap=self.logits_soft_cap,\n-            rms_norm_eps=self.rms_norm_eps,\n-            use_cache=self.use_cache,\n-            rope_theta=self.rope_theta,\n-            pad_token_id=self.pad_token_id,\n-            output_attentions=False,\n-        )\n+    from transformers import RecurrentGemmaConfig, RecurrentGemmaForCausalLM, RecurrentGemmaModel\n+\n+\n+from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n+\n \n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.create_and_check_model with Llama->RecurrentGemma\n-    def create_and_check_model(\n-        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        model = RecurrentGemmaModel(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=input_mask)\n-        result = model(input_ids)\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.prepare_config_and_inputs_for_common with Llama->RecurrentGemma\n-    def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-        (\n-            config,\n-            input_ids,\n-            token_type_ids,\n-            input_mask,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-        ) = config_and_inputs\n-        inputs_dict = {\"input_ids\": input_ids, \"attention_mask\": input_mask}\n-        return config, inputs_dict\n+class RecurrentGemmaModelTester(CausalLMModelTester):\n+    config_class = RecurrentGemmaConfig\n+    if is_torch_available():\n+        base_model_class = RecurrentGemmaModel\n+        causal_lm_class = RecurrentGemmaForCausalLM\n \n \n @require_torch\n-class RecurrentGemmaModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n-    all_model_classes = (RecurrentGemmaForCausalLM,) if is_torch_available() else ()\n-    # Doesn't run generation tests. TODO @gante not fully supported\n-    all_generative_model_classes = ()\n+class RecurrentGemmaModelTest(CausalLMModelTest, unittest.TestCase):\n+    all_model_classes = (RecurrentGemmaModel, RecurrentGemmaForCausalLM) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": RecurrentGemmaModel,\n@@ -180,48 +56,10 @@ class RecurrentGemmaModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.Te\n         if is_torch_available()\n         else {}\n     )\n-    fx_compatible = False  # FIXME let's try to support this @ArthurZucker\n-    test_torchscript = False  # FIXME let's try to support this @ArthurZucker\n-    test_missing_keys = False\n-    test_model_parallel = False\n+    test_headmasking = False\n     test_pruning = False\n-    test_head_masking = False  # RecurrentGemma does not have attention heads\n-\n-    # Need to remove 0.9 in `test_cpu_offload`\n-    # This is because we are hitting edge cases with the causal_mask buffer\n-    model_split_percents = [0.5, 0.6]\n-\n-    # TODO (ydshieh): Check this. See https://app.circleci.com/pipelines/github/huggingface/transformers/79245/workflows/9490ef58-79c2-410d-8f51-e3495156cf9c/jobs/1012146\n-    def is_pipeline_test_to_skip(\n-        self,\n-        pipeline_test_case_name,\n-        config_class,\n-        model_architecture,\n-        tokenizer_name,\n-        image_processor_name,\n-        feature_extractor_name,\n-        processor_name,\n-    ):\n-        return True\n-\n-    def setUp(self):\n-        # We don't output attentions\n-        self.has_attentions = False\n-        self.model_tester = RecurrentGemmaModelTester(self)\n-        self.config_tester = ConfigTester(self, config_class=RecurrentGemmaConfig, hidden_size=37)\n-\n-    def test_config(self):\n-        self.config_tester.run_common_tests()\n-\n-    def test_model(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_model(*config_and_inputs)\n-\n-    def test_model_various_embeddings(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        for type in [\"absolute\", \"relative_key\", \"relative_key_query\"]:\n-            config_and_inputs[0].position_embedding_type = type\n-            self.model_tester.create_and_check_model(*config_and_inputs)\n+    has_attentions = False\n+    model_tester_class = RecurrentGemmaModelTester\n \n     @unittest.skip(reason=\"RecurrentGemma only supports sdpa\")\n     def test_eager_matches_sdpa_generate(self):\n@@ -255,6 +93,7 @@ def test_model_parallelism(self):\n     def test_model_parallel_beam_search(self):\n         pass\n \n+    @parameterized.expand([(\"random\",), (\"same\",)])\n     @pytest.mark.generate\n     @unittest.skip(reason=\"Rely on `past_key_values` to crop the assistant pkv. Not supported\")\n     def test_assisted_decoding_matches_greedy_search(self):\n@@ -273,6 +112,65 @@ def test_assisted_decoding_sample(self):\n     def test_initialization(self):\n         pass\n \n+    @unittest.skip(reason=\"RecurrentGemma is unusual and fails a lot of generation tests\")\n+    @pytest.mark.generate\n+    def test_beam_sample_generate_dict_output(self):\n+        pass\n+\n+    @unittest.skip(reason=\"RecurrentGemma is unusual and fails a lot of generation tests\")\n+    @pytest.mark.generate\n+    def test_beam_search_generate_dict_output(self):\n+        pass\n+\n+    @unittest.skip(reason=\"RecurrentGemma is unusual and fails a lot of generation tests\")\n+    @pytest.mark.generate\n+    def test_beam_search_generate_dict_outputs_use_cache(self):\n+        pass\n+\n+    @unittest.skip(reason=\"RecurrentGemma is unusual and fails a lot of generation tests\")\n+    @pytest.mark.generate\n+    def test_constrained_beam_search_generate_dict_output(self):\n+        pass\n+\n+    @unittest.skip(reason=\"RecurrentGemma is unusual and fails a lot of generation tests\")\n+    @pytest.mark.generate\n+    def test_dola_decoding_sample(self):\n+        pass\n+\n+    @unittest.skip(reason=\"RecurrentGemma is unusual and fails a lot of generation tests\")\n+    @pytest.mark.generate\n+    def test_generate_without_input_ids(self):\n+        pass\n+\n+    @unittest.skip(reason=\"RecurrentGemma is unusual and fails a lot of generation tests\")\n+    @pytest.mark.generate\n+    def test_group_beam_search_generate(self):\n+        pass\n+\n+    @unittest.skip(reason=\"RecurrentGemma is unusual and fails a lot of generation tests\")\n+    @pytest.mark.generate\n+    def test_group_beam_search_generate_dict_output(self):\n+        pass\n+\n+    @unittest.skip(reason=\"RecurrentGemma is unusual and fails a lot of generation tests\")\n+    @pytest.mark.generate\n+    def test_constrained_beam_search_generate(self):\n+        pass\n+\n+    @unittest.skip(reason=\"RecurrentGemma is unusual and fails a lot of generation tests\")\n+    @pytest.mark.generate\n+    def test_greedy_generate_dict_outputs(self):\n+        pass\n+\n+    @unittest.skip(reason=\"RecurrentGemma is unusual and fails a lot of generation tests\")\n+    @pytest.mark.generate\n+    def test_greedy_generate_dict_outputs_use_cache(self):\n+        pass\n+\n+    @unittest.skip(reason=\"RecurrentGemma is unusual and fails a lot of generation tests\")\n+    def test_model_outputs_equivalence(self):\n+        pass\n+\n \n @require_torch_accelerator\n @slow"
        },
        {
            "sha": "87555a7d77406b64faf5cf595ad7c842b251ad01",
            "filename": "tests/models/stablelm/test_modeling_stablelm.py",
            "status": "modified",
            "additions": 21,
            "deletions": 282,
            "changes": 303,
            "blob_url": "https://github.com/huggingface/transformers/blob/53fb245eb60364c7377c5f37fc37807a00e9b2e2/tests%2Fmodels%2Fstablelm%2Ftest_modeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/53fb245eb60364c7377c5f37fc37807a00e9b2e2/tests%2Fmodels%2Fstablelm%2Ftest_modeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fstablelm%2Ftest_modeling_stablelm.py?ref=53fb245eb60364c7377c5f37fc37807a00e9b2e2",
            "patch": "@@ -16,9 +16,8 @@\n import unittest\n \n import pytest\n-from parameterized import parameterized\n \n-from transformers import StableLmConfig, is_torch_available, set_seed\n+from transformers import StableLmConfig, is_torch_available\n from transformers.testing_utils import (\n     require_bitsandbytes,\n     require_flash_attn,\n@@ -27,11 +26,6 @@\n     torch_device,\n )\n \n-from ...generation.test_utils import GenerationTesterMixin\n-from ...test_configuration_common import ConfigTester\n-from ...test_modeling_common import ModelTesterMixin, ids_tensor\n-from ...test_pipeline_mixin import PipelineTesterMixin\n-\n \n if is_torch_available():\n     import torch\n@@ -45,301 +39,46 @@\n     )\n     from transformers.models.stablelm.modeling_stablelm import StableLmRotaryEmbedding\n \n+from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n \n-# Copied from transformers.tests.models.persimmon.test_modeling_persimmon.PersimmonModelTester with Persimmon -> StableLm\n-class StableLmModelTester:\n-    # Ignore copy\n-    def __init__(\n-        self,\n-        parent,\n-        batch_size=13,\n-        seq_length=7,\n-        is_training=True,\n-        use_input_mask=True,\n-        use_token_type_ids=False,\n-        use_labels=True,\n-        vocab_size=99,\n-        hidden_size=64,\n-        num_hidden_layers=2,\n-        num_attention_heads=4,\n-        num_key_value_heads=4,\n-        intermediate_size=37,\n-        hidden_act=\"gelu\",\n-        hidden_dropout_prob=0.1,\n-        attention_probs_dropout_prob=0.1,\n-        max_position_embeddings=512,\n-        type_vocab_size=16,\n-        type_sequence_label_size=2,\n-        initializer_range=0.02,\n-        num_labels=3,\n-        num_choices=4,\n-        pad_token_id=0,\n-        scope=None,\n-    ):\n-        self.parent = parent\n-        self.batch_size = batch_size\n-        self.seq_length = seq_length\n-        self.is_training = is_training\n-        self.use_input_mask = use_input_mask\n-        self.use_token_type_ids = use_token_type_ids\n-        self.use_labels = use_labels\n-        self.vocab_size = vocab_size\n-        self.hidden_size = hidden_size\n-        self.num_hidden_layers = num_hidden_layers\n-        self.num_attention_heads = num_attention_heads\n-        self.num_key_value_heads = num_key_value_heads\n-        self.intermediate_size = intermediate_size\n-        self.hidden_act = hidden_act\n-        self.hidden_dropout_prob = hidden_dropout_prob\n-        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n-        self.max_position_embeddings = max_position_embeddings\n-        self.type_vocab_size = type_vocab_size\n-        self.type_sequence_label_size = type_sequence_label_size\n-        self.initializer_range = initializer_range\n-        self.num_labels = num_labels\n-        self.num_choices = num_choices\n-        self.pad_token_id = pad_token_id\n-        self.scope = scope\n-\n-    def prepare_config_and_inputs(self):\n-        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n-\n-        input_mask = None\n-        if self.use_input_mask:\n-            input_mask = torch.tril(torch.ones_like(input_ids).to(torch_device))\n-\n-        token_type_ids = None\n-        if self.use_token_type_ids:\n-            token_type_ids = ids_tensor([self.batch_size, self.seq_length], self.type_vocab_size)\n-\n-        sequence_labels = None\n-        token_labels = None\n-        choice_labels = None\n-        if self.use_labels:\n-            sequence_labels = ids_tensor([self.batch_size], self.type_sequence_label_size)\n-            token_labels = ids_tensor([self.batch_size, self.seq_length], self.num_labels)\n-            choice_labels = ids_tensor([self.batch_size], self.num_choices)\n \n-        config = self.get_config()\n-\n-        return config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-\n-    def get_config(self):\n-        return StableLmConfig(\n-            vocab_size=self.vocab_size,\n-            hidden_size=self.hidden_size,\n-            num_hidden_layers=self.num_hidden_layers,\n-            num_attention_heads=self.num_attention_heads,\n-            num_key_value_heads=self.num_key_value_heads,\n-            intermediate_size=self.intermediate_size,\n-            hidden_act=self.hidden_act,\n-            hidden_dropout_prob=self.hidden_dropout_prob,\n-            attention_probs_dropout_prob=self.attention_probs_dropout_prob,\n-            max_position_embeddings=self.max_position_embeddings,\n-            type_vocab_size=self.type_vocab_size,\n-            is_decoder=False,\n-            initializer_range=self.initializer_range,\n-            pad_token_id=self.pad_token_id,\n-        )\n-\n-    def create_and_check_model(\n-        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        model = StableLmModel(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=input_mask)\n-        result = model(input_ids)\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-\n-    def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-        (\n-            config,\n-            input_ids,\n-            token_type_ids,\n-            input_mask,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-        ) = config_and_inputs\n-        inputs_dict = {\"input_ids\": input_ids, \"attention_mask\": input_mask}\n-        return config, inputs_dict\n+class StableLmModelTester(CausalLMModelTester):\n+    if is_torch_available():\n+        config_class = StableLmConfig\n+        base_model_class = StableLmModel\n+        causal_lm_class = StableLmForCausalLM\n+        sequence_class = StableLmForSequenceClassification\n+        token_class = StableLmForTokenClassification\n \n \n @require_torch\n-# Copied from transformers.tests.persimmon.test_modeling_persimmon.PersimmonModelTest with Persimmon -> StableLm\n-class StableLmModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+class StableLmModelTest(CausalLMModelTest, unittest.TestCase):\n     all_model_classes = (\n-        (StableLmModel, StableLmForCausalLM, StableLmForSequenceClassification, StableLmForTokenClassification)\n+        (\n+            StableLmModel,\n+            StableLmForCausalLM,\n+            StableLmForSequenceClassification,\n+            StableLmForTokenClassification,\n+        )\n         if is_torch_available()\n         else ()\n     )\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": StableLmModel,\n             \"text-classification\": StableLmForSequenceClassification,\n+            \"text-generation\": StableLmForCausalLM,\n+            \"zero-shot\": StableLmForSequenceClassification,\n             \"token-classification\": StableLmForTokenClassification,\n-            # TODO (ydshieh): check why these two fail. Fix them or skip them in a better way.\n-            # \"text-generation\": StableLmForCausalLM,\n-            # \"zero-shot\": StableLmForSequenceClassification,\n         }\n         if is_torch_available()\n         else {}\n     )\n-\n     test_headmasking = False\n     test_pruning = False\n-\n-    def setUp(self):\n-        self.model_tester = StableLmModelTester(self)\n-        self.config_tester = ConfigTester(self, config_class=StableLmConfig, hidden_size=37)\n-\n-    def test_config(self):\n-        self.config_tester.run_common_tests()\n-\n-    def test_model(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_model(*config_and_inputs)\n-\n-    def test_stablelm_sequence_classification_model(self):\n-        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.num_labels = 3\n-        input_ids = input_dict[\"input_ids\"]\n-        attention_mask = input_ids.ne(1).to(torch_device)\n-        sequence_labels = ids_tensor([self.model_tester.batch_size], self.model_tester.type_sequence_label_size)\n-        model = StableLmForSequenceClassification(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=attention_mask, labels=sequence_labels)\n-        self.assertEqual(result.logits.shape, (self.model_tester.batch_size, self.model_tester.num_labels))\n-\n-    def test_stablelm_sequence_classification_model_for_single_label(self):\n-        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.num_labels = 3\n-        config.problem_type = \"single_label_classification\"\n-        input_ids = input_dict[\"input_ids\"]\n-        attention_mask = input_ids.ne(1).to(torch_device)\n-        sequence_labels = ids_tensor([self.model_tester.batch_size], self.model_tester.type_sequence_label_size)\n-        model = StableLmForSequenceClassification(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=attention_mask, labels=sequence_labels)\n-        self.assertEqual(result.logits.shape, (self.model_tester.batch_size, self.model_tester.num_labels))\n-\n-    def test_stablelm_sequence_classification_model_for_multi_label(self):\n-        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.num_labels = 3\n-        config.problem_type = \"multi_label_classification\"\n-        input_ids = input_dict[\"input_ids\"]\n-        attention_mask = input_ids.ne(1).to(torch_device)\n-        sequence_labels = ids_tensor(\n-            [self.model_tester.batch_size, config.num_labels], self.model_tester.type_sequence_label_size\n-        ).to(torch.float)\n-        model = StableLmForSequenceClassification(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=attention_mask, labels=sequence_labels)\n-        self.assertEqual(result.logits.shape, (self.model_tester.batch_size, self.model_tester.num_labels))\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTest.test_llama_token_classification_model with Llama->StableLm,llama->stablelm\n-    def test_stablelm_token_classification_model(self):\n-        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.num_labels = 3\n-        input_ids = input_dict[\"input_ids\"]\n-        attention_mask = input_ids.ne(1).to(torch_device)\n-        token_labels = ids_tensor([self.model_tester.batch_size, self.model_tester.seq_length], config.num_labels)\n-        model = StableLmForTokenClassification(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=attention_mask, labels=token_labels)\n-        self.assertEqual(\n-            result.logits.shape,\n-            (self.model_tester.batch_size, self.model_tester.seq_length, self.model_tester.num_labels),\n-        )\n-\n-    @parameterized.expand([(\"linear\",), (\"dynamic\",)])\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTest.test_model_rope_scaling_from_config with Llama->StableLm\n-    def test_model_rope_scaling_from_config(self, scaling_type):\n-        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-        short_input = ids_tensor([1, 10], config.vocab_size)\n-        long_input = ids_tensor([1, int(config.max_position_embeddings * 1.5)], config.vocab_size)\n-\n-        set_seed(42)  # Fixed seed at init time so the two models get the same random weights\n-        original_model = StableLmModel(config)\n-        original_model.to(torch_device)\n-        original_model.eval()\n-        original_short_output = original_model(short_input).last_hidden_state\n-        original_long_output = original_model(long_input).last_hidden_state\n-\n-        set_seed(42)  # Fixed seed at init time so the two models get the same random weights\n-        config.rope_scaling = {\"type\": scaling_type, \"factor\": 10.0}\n-        scaled_model = StableLmModel(config)\n-        scaled_model.to(torch_device)\n-        scaled_model.eval()\n-        scaled_short_output = scaled_model(short_input).last_hidden_state\n-        scaled_long_output = scaled_model(long_input).last_hidden_state\n-\n-        # Dynamic scaling does not change the RoPE embeddings until it receives an input longer than the original\n-        # maximum sequence length, so the outputs for the short input should match.\n-        if scaling_type == \"dynamic\":\n-            torch.testing.assert_close(original_short_output, scaled_short_output, rtol=1e-5, atol=1e-5)\n-        else:\n-            self.assertFalse(torch.allclose(original_short_output, scaled_short_output, atol=1e-5))\n-\n-        # The output should be different for long inputs\n-        self.assertFalse(torch.allclose(original_long_output, scaled_long_output, atol=1e-5))\n-\n-    # Copied from tests.models.gpt_neox.test_modeling_gpt_neox.GPTNeoXModelTest.test_model_rope_scaling with GPTNeoX->StableLm\n-    def test_model_rope_scaling(self):\n-        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-        scaling_factor = 10\n-        short_input_length = 10\n-        long_input_length = int(config.max_position_embeddings * 1.5)\n-\n-        # Inputs\n-        x = torch.randn(\n-            1, dtype=torch.float32, device=torch_device\n-        )  # used exclusively to get the dtype and the device\n-        position_ids_short = torch.arange(short_input_length, dtype=torch.long, device=torch_device)\n-        position_ids_short = position_ids_short.unsqueeze(0)\n-        position_ids_long = torch.arange(long_input_length, dtype=torch.long, device=torch_device)\n-        position_ids_long = position_ids_long.unsqueeze(0)\n-\n-        # Sanity check original RoPE\n-        original_rope = StableLmRotaryEmbedding(config).to(torch_device)\n-        original_cos_short, original_sin_short = original_rope(x, position_ids_short)\n-        original_cos_long, original_sin_long = original_rope(x, position_ids_long)\n-        torch.testing.assert_close(original_cos_short, original_cos_long[:, :short_input_length, :])\n-        torch.testing.assert_close(original_sin_short, original_sin_long[:, :short_input_length, :])\n-\n-        # Sanity check linear RoPE scaling\n-        # New position \"x\" should match original position with index \"x/scaling_factor\"\n-        config.rope_scaling = {\"type\": \"linear\", \"factor\": scaling_factor}\n-        linear_scaling_rope = StableLmRotaryEmbedding(config).to(torch_device)\n-        linear_cos_short, linear_sin_short = linear_scaling_rope(x, position_ids_short)\n-        linear_cos_long, linear_sin_long = linear_scaling_rope(x, position_ids_long)\n-        torch.testing.assert_close(linear_cos_short, linear_cos_long[:, :short_input_length, :])\n-        torch.testing.assert_close(linear_sin_short, linear_sin_long[:, :short_input_length, :])\n-        for new_position in range(0, long_input_length, scaling_factor):\n-            original_position = int(new_position // scaling_factor)\n-            torch.testing.assert_close(linear_cos_long[:, new_position, :], original_cos_long[:, original_position, :])\n-            torch.testing.assert_close(linear_sin_long[:, new_position, :], original_sin_long[:, original_position, :])\n-\n-        # Sanity check Dynamic NTK RoPE scaling\n-        # Scaling should only be observed after a long input is fed. We can observe that the frequencies increase\n-        # with scaling_factor (or that `inv_freq` decreases)\n-        config.rope_scaling = {\"type\": \"dynamic\", \"factor\": scaling_factor}\n-        ntk_scaling_rope = StableLmRotaryEmbedding(config).to(torch_device)\n-        ntk_cos_short, ntk_sin_short = ntk_scaling_rope(x, position_ids_short)\n-        ntk_cos_long, ntk_sin_long = ntk_scaling_rope(x, position_ids_long)\n-        torch.testing.assert_close(ntk_cos_short, original_cos_short)\n-        torch.testing.assert_close(ntk_sin_short, original_sin_short)\n-        with self.assertRaises(AssertionError):\n-            torch.testing.assert_close(ntk_cos_long, original_cos_long)\n-        with self.assertRaises(AssertionError):\n-            torch.testing.assert_close(ntk_sin_long, original_sin_long)\n-        self.assertTrue((ntk_scaling_rope.inv_freq <= original_rope.inv_freq).all())\n+    fx_compatible = False  # Broken by attention refactor cc @Cyrilvallez\n+    model_tester_class = StableLmModelTester\n+    rotary_embedding_layer = StableLmRotaryEmbedding  # Enables RoPE tests if set\n \n \n @require_torch"
        },
        {
            "sha": "956b210bae41276447257b0bbbf629a256883a11",
            "filename": "tests/models/starcoder2/test_modeling_starcoder2.py",
            "status": "modified",
            "additions": 12,
            "deletions": 220,
            "changes": 232,
            "blob_url": "https://github.com/huggingface/transformers/blob/53fb245eb60364c7377c5f37fc37807a00e9b2e2/tests%2Fmodels%2Fstarcoder2%2Ftest_modeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/53fb245eb60364c7377c5f37fc37807a00e9b2e2/tests%2Fmodels%2Fstarcoder2%2Ftest_modeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fstarcoder2%2Ftest_modeling_starcoder2.py?ref=53fb245eb60364c7377c5f37fc37807a00e9b2e2",
            "patch": "@@ -28,11 +28,6 @@\n     torch_device,\n )\n \n-from ...generation.test_utils import GenerationTesterMixin\n-from ...test_configuration_common import ConfigTester\n-from ...test_modeling_common import ModelTesterMixin, ids_tensor\n-from ...test_pipeline_mixin import PipelineTesterMixin\n-\n \n if is_torch_available():\n     import torch\n@@ -45,241 +40,38 @@\n         Starcoder2Model,\n     )\n \n+from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n \n-# Copied from transformers.tests.models.mistral.test_modeling_mistral.Starcoder2ModelTester with Mistral->Starcoder2\n-class Starcoder2ModelTester:\n-    def __init__(\n-        self,\n-        parent,\n-        batch_size=13,\n-        seq_length=7,\n-        is_training=True,\n-        use_input_mask=True,\n-        use_token_type_ids=False,\n-        use_labels=True,\n-        vocab_size=99,\n-        hidden_size=32,\n-        num_hidden_layers=2,\n-        num_attention_heads=4,\n-        num_key_value_heads=2,\n-        intermediate_size=37,\n-        hidden_act=\"gelu\",\n-        hidden_dropout_prob=0.1,\n-        attention_probs_dropout_prob=0.1,\n-        max_position_embeddings=512,\n-        type_vocab_size=16,\n-        type_sequence_label_size=2,\n-        initializer_range=0.02,\n-        num_labels=3,\n-        num_choices=4,\n-        pad_token_id=0,\n-        scope=None,\n-    ):\n-        self.parent = parent\n-        self.batch_size = batch_size\n-        self.seq_length = seq_length\n-        self.is_training = is_training\n-        self.use_input_mask = use_input_mask\n-        self.use_token_type_ids = use_token_type_ids\n-        self.use_labels = use_labels\n-        self.vocab_size = vocab_size\n-        self.hidden_size = hidden_size\n-        self.num_hidden_layers = num_hidden_layers\n-        self.num_attention_heads = num_attention_heads\n-        self.num_key_value_heads = num_key_value_heads\n-        self.intermediate_size = intermediate_size\n-        self.hidden_act = hidden_act\n-        self.hidden_dropout_prob = hidden_dropout_prob\n-        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n-        self.max_position_embeddings = max_position_embeddings\n-        self.type_vocab_size = type_vocab_size\n-        self.type_sequence_label_size = type_sequence_label_size\n-        self.initializer_range = initializer_range\n-        self.num_labels = num_labels\n-        self.num_choices = num_choices\n-        self.pad_token_id = pad_token_id\n-        self.scope = scope\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.prepare_config_and_inputs\n-    def prepare_config_and_inputs(self):\n-        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n-\n-        input_mask = None\n-        if self.use_input_mask:\n-            input_mask = torch.tril(torch.ones_like(input_ids).to(torch_device))\n-\n-        token_type_ids = None\n-        if self.use_token_type_ids:\n-            token_type_ids = ids_tensor([self.batch_size, self.seq_length], self.type_vocab_size)\n-\n-        sequence_labels = None\n-        token_labels = None\n-        choice_labels = None\n-        if self.use_labels:\n-            sequence_labels = ids_tensor([self.batch_size], self.type_sequence_label_size)\n-            token_labels = ids_tensor([self.batch_size, self.seq_length], self.num_labels)\n-            choice_labels = ids_tensor([self.batch_size], self.num_choices)\n-\n-        config = self.get_config()\n \n-        return config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-\n-    # Ignore copy\n-    def get_config(self):\n-        return Starcoder2Config(\n-            vocab_size=self.vocab_size,\n-            hidden_size=self.hidden_size,\n-            num_hidden_layers=self.num_hidden_layers,\n-            num_attention_heads=self.num_attention_heads,\n-            num_key_value_heads=self.num_key_value_heads,\n-            intermediate_size=self.intermediate_size,\n-            hidden_act=self.hidden_act,\n-            hidden_dropout_prob=self.hidden_dropout_prob,\n-            attention_probs_dropout_prob=self.attention_probs_dropout_prob,\n-            max_position_embeddings=self.max_position_embeddings,\n-            type_vocab_size=self.type_vocab_size,\n-            is_decoder=False,\n-            initializer_range=self.initializer_range,\n-            pad_token_id=self.pad_token_id,\n-            eos_token_id=self.pad_token_id,\n-            bos_token_id=self.pad_token_id,\n-        )\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.create_and_check_model with Llama->Starcoder2\n-    def create_and_check_model(\n-        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        model = Starcoder2Model(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=input_mask)\n-        result = model(input_ids)\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.prepare_config_and_inputs_for_common\n-    def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-        (\n-            config,\n-            input_ids,\n-            token_type_ids,\n-            input_mask,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-        ) = config_and_inputs\n-        inputs_dict = {\"input_ids\": input_ids, \"attention_mask\": input_mask}\n-        return config, inputs_dict\n+class Starcoder2ModelTester(CausalLMModelTester):\n+    config_class = Starcoder2Config\n+    if is_torch_available():\n+        base_model_class = Starcoder2Model\n+        causal_lm_class = Starcoder2ForCausalLM\n+        sequence_class = Starcoder2ForSequenceClassification\n+        token_class = Starcoder2ForTokenClassification\n \n \n @require_torch\n-# Copied from transformers.tests.models.mistral.test_modeling_mistral.MistralModelTest with Mistral->Starcoder2\n-class Starcoder2ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+class Starcoder2ModelTest(CausalLMModelTest, unittest.TestCase):\n     all_model_classes = (\n         (Starcoder2Model, Starcoder2ForCausalLM, Starcoder2ForSequenceClassification, Starcoder2ForTokenClassification)\n         if is_torch_available()\n         else ()\n     )\n+    test_headmasking = False\n+    test_pruning = False\n+    model_tester_class = Starcoder2ModelTester\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": Starcoder2Model,\n             \"text-classification\": Starcoder2ForSequenceClassification,\n             \"token-classification\": Starcoder2ForTokenClassification,\n             \"text-generation\": Starcoder2ForCausalLM,\n-            \"zero-shot\": Starcoder2ForSequenceClassification,\n         }\n         if is_torch_available()\n         else {}\n     )\n-    test_headmasking = False\n-    test_pruning = False\n-\n-    # TODO (ydshieh): Check this. See https://app.circleci.com/pipelines/github/huggingface/transformers/79245/workflows/9490ef58-79c2-410d-8f51-e3495156cf9c/jobs/1012146\n-    def is_pipeline_test_to_skip(\n-        self,\n-        pipeline_test_case_name,\n-        config_class,\n-        model_architecture,\n-        tokenizer_name,\n-        image_processor_name,\n-        feature_extractor_name,\n-        processor_name,\n-    ):\n-        return True\n-\n-    def setUp(self):\n-        self.model_tester = Starcoder2ModelTester(self)\n-        self.config_tester = ConfigTester(self, config_class=Starcoder2Config, hidden_size=37)\n-\n-    def test_config(self):\n-        self.config_tester.run_common_tests()\n-\n-    def test_model(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_model(*config_and_inputs)\n-\n-    def test_model_various_embeddings(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        for type in [\"absolute\", \"relative_key\", \"relative_key_query\"]:\n-            config_and_inputs[0].position_embedding_type = type\n-            self.model_tester.create_and_check_model(*config_and_inputs)\n-\n-    def test_Starcoder2_sequence_classification_model(self):\n-        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        print(config)\n-        config.num_labels = 3\n-        input_ids = input_dict[\"input_ids\"]\n-        attention_mask = input_ids.ne(1).to(torch_device)\n-        sequence_labels = ids_tensor([self.model_tester.batch_size], self.model_tester.type_sequence_label_size)\n-        model = Starcoder2ForSequenceClassification(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=attention_mask, labels=sequence_labels)\n-        self.assertEqual(result.logits.shape, (self.model_tester.batch_size, self.model_tester.num_labels))\n-\n-    def test_Starcoder2_sequence_classification_model_for_single_label(self):\n-        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.num_labels = 3\n-        config.problem_type = \"single_label_classification\"\n-        input_ids = input_dict[\"input_ids\"]\n-        attention_mask = input_ids.ne(1).to(torch_device)\n-        sequence_labels = ids_tensor([self.model_tester.batch_size], self.model_tester.type_sequence_label_size)\n-        model = Starcoder2ForSequenceClassification(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=attention_mask, labels=sequence_labels)\n-        self.assertEqual(result.logits.shape, (self.model_tester.batch_size, self.model_tester.num_labels))\n-\n-    def test_Starcoder2_sequence_classification_model_for_multi_label(self):\n-        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.num_labels = 3\n-        config.problem_type = \"multi_label_classification\"\n-        input_ids = input_dict[\"input_ids\"]\n-        attention_mask = input_ids.ne(1).to(torch_device)\n-        sequence_labels = ids_tensor(\n-            [self.model_tester.batch_size, config.num_labels], self.model_tester.type_sequence_label_size\n-        ).to(torch.float)\n-        model = Starcoder2ForSequenceClassification(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=attention_mask, labels=sequence_labels)\n-        self.assertEqual(result.logits.shape, (self.model_tester.batch_size, self.model_tester.num_labels))\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTest.test_llama_token_classification_model with Llama->Starcoder2,llama->Starcoder2\n-    def test_Starcoder2_token_classification_model(self):\n-        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.num_labels = 3\n-        input_ids = input_dict[\"input_ids\"]\n-        attention_mask = input_ids.ne(1).to(torch_device)\n-        token_labels = ids_tensor([self.model_tester.batch_size, self.model_tester.seq_length], config.num_labels)\n-        model = Starcoder2ForTokenClassification(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=attention_mask, labels=token_labels)\n-        self.assertEqual(\n-            result.logits.shape,\n-            (self.model_tester.batch_size, self.model_tester.seq_length, self.model_tester.num_labels),\n-        )\n \n     @require_flash_attn\n     @require_torch_gpu"
        },
        {
            "sha": "7bd494e452bbda323e0b29ad5d0126c206d52cb1",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/53fb245eb60364c7377c5f37fc37807a00e9b2e2/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/53fb245eb60364c7377c5f37fc37807a00e9b2e2/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=53fb245eb60364c7377c5f37fc37807a00e9b2e2",
            "patch": "@@ -4426,7 +4426,7 @@ def test_custom_4d_attention_mask(self):\n             # comparing softmax-normalized logits:\n             normalized_0 = F.softmax(out_last_tokens, dim=-1)\n             normalized_1 = F.softmax(out_shared_prefix_last_tokens, dim=-1)\n-            torch.testing.assert_close(normalized_0, normalized_1, rtol=1e-3, atol=1e-4)\n+            torch.testing.assert_close(normalized_0, normalized_1, rtol=1e-3, atol=1e-3)\n \n     @slow\n     @require_torch_accelerator"
        }
    ],
    "stats": {
        "total": 5238,
        "additions": 816,
        "deletions": 4422
    }
}