{
    "author": "Cyrilvallez",
    "message": "Detect and fix most `_init_weights()` issues - make it work for composite models (#37070)\n\n* Update test_modeling_common.py\n\n* Fix Llama and its modular children\n\n* Update test_modeling_common.py\n\n* qwen3\n\n* first try at prioritizing models\n\n* Update test_modeling_common.py\n\n* Update test_modeling_common.py\n\n* Update test_modeling_common.py\n\n* test\n\n* fix\n\n* fix\n\n* more models\n\n* more\n\n* more\n\n* more\n\n* smarter init for composite models!\n\n* fix post rebase\n\n* smol\n\n* fix missing args\n\n* more\n\n* typo\n\n* Super elegant and efficient init for submodels\n\n* Update modeling_utils.py\n\n* style\n\n* last fixes\n\n* cleanup\n\n* finalize cleanup\n\n* CIs\n\n* improve docstring\n\n* Update modeling_utils.py\n\n* llama4\n\n* style\n\n* CIs\n\n* style\n\n* add dpt\n\n* granite speech\n\n* qwen 2.5 omni\n\n* better fix\n\n* Parse the config file instead\n\n* CIs",
    "sha": "4e538409203402058d1ee75c57e03dd908eac508",
    "files": [
        {
            "sha": "5a8ebc6847d15481b27d1befc45a00a7fd21b9d4",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 34,
            "deletions": 3,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -2449,6 +2449,37 @@ def _initialize_weights(self, module):\n         self._init_weights(module)\n         module._is_hf_initialized = True\n \n+    @torch.no_grad()\n+    def initialize_weights(self):\n+        \"\"\"\n+        This is equivalent to calling `self.apply(self._initialize_weights)`, but correctly handles composite models.\n+        This function dynamically dispatches the correct `init_weights` function to the modules as we advance in the\n+        module graph along the recursion. It can handle an arbitrary number of sub-models. Without it, every composite\n+        model would have to recurse a second time on all sub-models explicitly in the outer-most `_init_weights`, which\n+        is extremely error prone and inefficient.\n+\n+        Note that the `torch.no_grad()` decorator is very important as well, as most of our `_init_weights` do not use\n+        `torch.nn.init` functions (which are all no_grad by default), but simply do in-place ops such as\n+        `module.weight.data.zero_()`.\n+        \"\"\"\n+        if not hasattr(torch.nn.Module, \"smart_apply\"):\n+            # This function is equivalent to `torch.nn.Module.apply`, except that it dynamically adjust the function\n+            # to apply as we go down the graph\n+            def smart_apply(self, fn):\n+                for module in self.children():\n+                    # We found a sub-model: recursively dispatch its own init function now!\n+                    if hasattr(module, \"_init_weights\"):\n+                        module.smart_apply(module._initialize_weights)\n+                    else:\n+                        module.smart_apply(fn)\n+                fn(self)\n+                return self\n+\n+            torch.nn.Module.smart_apply = smart_apply\n+\n+        # Let the magic happen with this simple call\n+        self.smart_apply(self._initialize_weights)\n+\n     def tie_weights(self):\n         \"\"\"\n         Tie the weights between the input embeddings and the output embeddings.\n@@ -3074,7 +3105,7 @@ def init_weights(self):\n \n         if _init_weights:\n             # Initialize weights\n-            self.apply(self._initialize_weights)\n+            self.initialize_weights()\n \n             # Tie weights should be skipped when not initializing all weights\n             # since from_pretrained(...) calls tie weights anyways\n@@ -5286,9 +5317,9 @@ def _initialize_missing_keys(\n                 )\n             )\n             with deepspeed.zero.GatheredParameters(not_initialized_parameters, modifier_rank=0):\n-                self.apply(self._initialize_weights)\n+                self.initialize_weights()\n         else:\n-            self.apply(self._initialize_weights)\n+            self.initialize_weights()\n \n     def get_parameter_or_buffer(self, target: str):\n         \"\"\""
        },
        {
            "sha": "867ad34b5513c27543a93e2faa33531cbb9d4eb6",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 9,
            "deletions": 8,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -679,12 +679,10 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, AriaTextRMSNorm):\n+            module.weight.data.fill_(1.0)\n         elif isinstance(module, AriaGroupedExpertsGemm):\n             module.weight.data.normal_(mean=0.0, std=std)\n-        elif isinstance(module, nn.Conv2d):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if hasattr(module, \"bias\") and module.bias is not None:\n-                module.bias.data.zero_()\n \n \n ARIA_TEXT_START_DOCSTRING = r\"\"\"\n@@ -724,14 +722,17 @@ class AriaPreTrainedModel(PreTrainedModel):\n \n     def _init_weights(self, module):\n         std = self.config.initializer_range\n+\n         if isinstance(module, nn.Linear):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n                 module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, nn.MultiheadAttention):\n+            # This uses torch's original init\n+            module._reset_parameters()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.weight.data.fill_(1.0)\n+            module.bias.data.zero_()\n         elif isinstance(module, AriaProjector):\n             nn.init.trunc_normal_(module.query, std=std)\n "
        },
        {
            "sha": "574ee053a954a575eb36aa91538594374ed3a839",
            "filename": "src/transformers/models/aria/modular_aria.py",
            "status": "modified",
            "additions": 9,
            "deletions": 8,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -1255,12 +1255,10 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, AriaTextRMSNorm):\n+            module.weight.data.fill_(1.0)\n         elif isinstance(module, AriaGroupedExpertsGemm):\n             module.weight.data.normal_(mean=0.0, std=std)\n-        elif isinstance(module, nn.Conv2d):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if hasattr(module, \"bias\") and module.bias is not None:\n-                module.bias.data.zero_()\n \n \n class AriaPreTrainedModel(LlamaPreTrainedModel):\n@@ -1269,14 +1267,17 @@ class AriaPreTrainedModel(LlamaPreTrainedModel):\n \n     def _init_weights(self, module):\n         std = self.config.initializer_range\n+\n         if isinstance(module, nn.Linear):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n                 module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, nn.MultiheadAttention):\n+            # This uses torch's original init\n+            module._reset_parameters()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.weight.data.fill_(1.0)\n+            module.bias.data.zero_()\n         elif isinstance(module, AriaProjector):\n             nn.init.trunc_normal_(module.query, std=std)\n "
        },
        {
            "sha": "8c1a3b23b9e307874a8d876e894d6d1e14fe014d",
            "filename": "src/transformers/models/aya_vision/modeling_aya_vision.py",
            "status": "modified",
            "additions": 4,
            "deletions": 11,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -127,26 +127,19 @@ class AyaVisionPreTrainedModel(PreTrainedModel):\n     _supports_static_cache = False\n \n     def _init_weights(self, module):\n-        # important: this ported version of AyaVision isn't meant for training from scratch - only\n-        # inference and fine-tuning - so the proper init weights code has been removed - the original codebase\n-        # https://github.com/haotian-liu/AyaVision/tree/main/aya_vision should serve for that purpose\n         std = (\n             self.config.initializer_range\n             if hasattr(self.config, \"initializer_range\")\n             else self.config.text_config.initializer_range\n         )\n \n-        if hasattr(module, \"class_embedding\"):\n-            module.class_embedding.data.normal_(mean=0.0, std=std)\n-\n-        if isinstance(module, (nn.Linear, nn.Conv2d)):\n+        if isinstance(module, nn.Linear):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n                 module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.weight.data.fill_(1.0)\n+            module.bias.data.zero_()\n \n \n @dataclass"
        },
        {
            "sha": "96f0de888dd793b22c4eaeba6e94684740b61c78",
            "filename": "src/transformers/models/aya_vision/modular_aya_vision.py",
            "status": "modified",
            "additions": 15,
            "deletions": 0,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodular_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodular_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodular_aya_vision.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -113,6 +113,21 @@ class AyaVisionPreTrainedModel(LlavaPreTrainedModel):\n     _supports_quantized_cache = False\n     _supports_static_cache = False\n \n+    def _init_weights(self, module):\n+        std = (\n+            self.config.initializer_range\n+            if hasattr(self.config, \"initializer_range\")\n+            else self.config.text_config.initializer_range\n+        )\n+\n+        if isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.weight.data.fill_(1.0)\n+            module.bias.data.zero_()\n+\n \n class AyaVisionCausalLMOutputWithPast(LlavaCausalLMOutputWithPast):\n     pass"
        },
        {
            "sha": "5d70cf0dda29956aff68f60497319e648238228c",
            "filename": "src/transformers/models/bamba/modeling_bamba.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -1052,10 +1052,16 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n                 module.bias.data.zero_()\n+        elif isinstance(module, (BambaRMSNormGated, BambaRMSNorm)):\n+            module.weight.data.fill_(1.0)\n         elif isinstance(module, nn.Embedding):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, BambaMixer):\n+            module.dt_bias.data.fill_(1.0)\n+            module.A_log.data = torch.log(torch.arange(1, module.num_heads + 1))\n+            module.D.data.fill_(1.0)\n \n \n BAMBA_INPUTS_DOCSTRING = r\"\"\""
        },
        {
            "sha": "9c52fb16ac63402d97bf77462dc2972bb5ed211f",
            "filename": "src/transformers/models/bamba/modular_bamba.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -820,10 +820,16 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n                 module.bias.data.zero_()\n+        elif isinstance(module, (BambaRMSNormGated, BambaRMSNorm)):\n+            module.weight.data.fill_(1.0)\n         elif isinstance(module, nn.Embedding):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, BambaMixer):\n+            module.dt_bias.data.fill_(1.0)\n+            module.A_log.data = torch.log(torch.arange(1, module.num_heads + 1))\n+            module.D.data.fill_(1.0)\n \n \n BAMBA_INPUTS_DOCSTRING = r\"\"\""
        },
        {
            "sha": "1ee48b00b81ff65d20e85b3857d89eba951a50d1",
            "filename": "src/transformers/models/blip_2/modeling_blip_2.py",
            "status": "modified",
            "additions": 19,
            "deletions": 11,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -423,22 +423,30 @@ class Blip2PreTrainedModel(PreTrainedModel):\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         factor = self.config.initializer_range\n-        if isinstance(module, nn.Conv2d) or isinstance(module, nn.Embedding) or isinstance(module, nn.Linear):\n+\n+        if isinstance(module, (nn.Linear, nn.Conv2d)):\n             module.weight.data.normal_(mean=0.0, std=factor)\n-            if hasattr(module, \"bias\") and module.bias is not None:\n+            if module.bias is not None:\n                 module.bias.data.zero_()\n-\n-        if isinstance(module, Blip2VisionEmbeddings):\n-            if hasattr(self.config, \"vision_config\") and not isinstance(self.config, Blip2VisionConfig):\n-                factor = self.config.vision_config.initializer_range\n-            nn.init.trunc_normal_(module.position_embedding, mean=0.0, std=factor)\n-            nn.init.trunc_normal_(module.class_embedding, mean=0.0, std=factor)\n-\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=factor)\n         elif isinstance(module, nn.LayerNorm):\n             module.bias.data.zero_()\n             module.weight.data.fill_(1.0)\n-        elif isinstance(module, nn.Linear) and module.bias is not None:\n-            module.bias.data.zero_()\n+        elif isinstance(module, Blip2VisionEmbeddings):\n+            nn.init.trunc_normal_(module.position_embedding, mean=0.0, std=factor)\n+            nn.init.trunc_normal_(module.class_embedding, mean=0.0, std=factor)\n+        elif isinstance(\n+            module,\n+            (\n+                Blip2Model,\n+                Blip2TextModelWithProjection,\n+                Blip2VisionModelWithProjection,\n+                Blip2ForConditionalGeneration,\n+                Blip2ForImageTextRetrieval,\n+            ),\n+        ):\n+            module.query_tokens.data.zero_()\n \n \n BLIP_2_START_DOCSTRING = r\"\"\""
        },
        {
            "sha": "ad0c25502350b4e3dbced9067d4b4beacf24b551",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 7,
            "deletions": 15,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -1056,12 +1056,16 @@ class ChameleonPreTrainedModel(PreTrainedModel):\n \n     def _init_weights(self, module):\n         std = self.config.initializer_range\n-        if isinstance(module, ChameleonVQVAE):\n-            module.apply(module._init_weights)\n-        elif isinstance(module, (nn.Linear, nn.Conv2d)):\n+\n+        if isinstance(module, (nn.Linear, nn.Conv2d)):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n                 module.bias.data.zero_()\n+        elif isinstance(module, (nn.GroupNorm, nn.LayerNorm)):\n+            module.bias.data.zero_()\n+            module.weight.data.fill_(1.0)\n+        elif isinstance(module, ChameleonRMSNorm):\n+            module.weight.data.fill_(1.0)\n         elif isinstance(module, nn.Embedding):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n@@ -1096,18 +1100,6 @@ class ChameleonVQVAE(ChameleonPreTrainedModel):\n     config_class = ChameleonVQVAEConfig\n     _no_split_modules = [\"ChameleonVQVAEVectorQuantizer\"]\n \n-    def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-        elif isinstance(module, nn.GroupNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n-        elif isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-\n     def __init__(self, config: ChameleonVQVAEConfig):\n         super().__init__(config)\n "
        },
        {
            "sha": "e8e6bba237050e0b6830b53df792b06f31e6c1a0",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -416,6 +416,8 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, CohereLayerNorm):\n+            module.weight.data.fill_(1.0)\n \n \n COHERE_INPUTS_DOCSTRING = r\"\"\""
        },
        {
            "sha": "c9f2d8ff2766faebf3b66314cd939f5c21a2d9e9",
            "filename": "src/transformers/models/cohere/modular_cohere.py",
            "status": "modified",
            "additions": 16,
            "deletions": 0,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodular_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodular_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodular_cohere.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -41,6 +41,7 @@\n     LlamaForCausalLM,\n     LlamaMLP,\n     LlamaModel,\n+    LlamaPreTrainedModel,\n     LlamaRotaryEmbedding,\n     eager_attention_forward,\n )\n@@ -277,6 +278,21 @@ def forward(\n         return outputs\n \n \n+class CoherePreTrainedModel(LlamaPreTrainedModel):\n+    def _init_weights(self, module):\n+        std = self.config.initializer_range\n+        if isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, CohereLayerNorm):\n+            module.weight.data.fill_(1.0)\n+\n+\n class CohereModel(LlamaModel):\n     def __init__(self, config: CohereConfig):\n         super().__init__(config)"
        },
        {
            "sha": "65e066f90c58d2eed9b15b810e36bb4569818eb0",
            "filename": "src/transformers/models/cohere2/modeling_cohere2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -424,6 +424,8 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, Cohere2LayerNorm):\n+            module.weight.data.fill_(1.0)\n \n \n COHERE2_INPUTS_DOCSTRING = r\"\"\""
        },
        {
            "sha": "586d5251b0d204e8cef08b5a1728cdd9bd6f1a6b",
            "filename": "src/transformers/models/deepseek_v3/modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -557,10 +557,10 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, DeepseekV3RMSNorm):\n+            module.weight.data.fill_(1.0)\n         elif isinstance(module, DeepseekV3TopkRouter):\n             module.weight.data.normal_(mean=0.0, std=std)\n-        elif isinstance(module, nn.Parameter):\n-            module.weight.data.normal_(mean=0.0, std=std)\n \n \n DEEPSEEK_V3_INPUTS_DOCSTRING = r\"\"\""
        },
        {
            "sha": "b4905c62011adc387eb03e890cb0a3378bc445c0",
            "filename": "src/transformers/models/deepseek_v3/modular_deepseek_v3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodular_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodular_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodular_deepseek_v3.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -347,10 +347,10 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, DeepseekV3RMSNorm):\n+            module.weight.data.fill_(1.0)\n         elif isinstance(module, DeepseekV3TopkRouter):\n             module.weight.data.normal_(mean=0.0, std=std)\n-        elif isinstance(module, nn.Parameter):\n-            module.weight.data.normal_(mean=0.0, std=std)\n \n \n class DeepseekV3Model(LlamaModel):"
        },
        {
            "sha": "f6ff0653349735d44c1e739cee751009c7091b43",
            "filename": "src/transformers/models/diffllama/modeling_diffllama.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -625,6 +625,13 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, DiffLlamaRMSNorm):  # noqa: F821\n+            module.weight.data.fill_(1.0)\n+        elif isinstance(module, DiffLlamaAttention):\n+            module.lambda_q1.data.normal_(0, self.config.lambda_std_dev)\n+            module.lambda_k1.data.normal_(0, self.config.lambda_std_dev)\n+            module.lambda_q2.data.normal_(0, self.config.lambda_std_dev)\n+            module.lambda_k2.data.normal_(0, self.config.lambda_std_dev)\n \n \n class DiffLlamaRotaryEmbedding(nn.Module):"
        },
        {
            "sha": "f7bc2d2c5ac1fbf98e7ffe9607d2dd8c4e5ae265",
            "filename": "src/transformers/models/diffllama/modular_diffllama.py",
            "status": "modified",
            "additions": 18,
            "deletions": 0,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodular_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodular_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodular_diffllama.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -431,6 +431,24 @@ class DiffLlamaPreTrainedModel(LlamaPreTrainedModel):\n     _supports_flex_attn = False\n     _supports_attention_backend = False\n \n+    def _init_weights(self, module):\n+        std = self.config.initializer_range\n+        if isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, DiffLlamaRMSNorm):  # noqa: F821\n+            module.weight.data.fill_(1.0)\n+        elif isinstance(module, DiffLlamaAttention):\n+            module.lambda_q1.data.normal_(0, self.config.lambda_std_dev)\n+            module.lambda_k1.data.normal_(0, self.config.lambda_std_dev)\n+            module.lambda_q2.data.normal_(0, self.config.lambda_std_dev)\n+            module.lambda_k2.data.normal_(0, self.config.lambda_std_dev)\n+\n \n class DiffLlamaModel(LlamaModel):\n     pass"
        },
        {
            "sha": "0dc1dcf2f87e072ed72b0892f0d88a9175203bd0",
            "filename": "src/transformers/models/dpt/modeling_dpt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodeling_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodeling_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodeling_dpt.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -852,7 +852,7 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n                 module.bias.data.zero_()\n-        elif isinstance(module, nn.LayerNorm):\n+        elif isinstance(module, (nn.LayerNorm, nn.BatchNorm2d)):\n             module.bias.data.zero_()\n             module.weight.data.fill_(1.0)\n         if isinstance(module, (DPTViTEmbeddings, DPTViTHybridEmbeddings)):"
        },
        {
            "sha": "1d85fcb639c9a7975c111c34933311832991d6f5",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 13,
            "deletions": 5,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -1020,15 +1020,23 @@ class Emu3VQVAE(PreTrainedModel):\n     def _init_weights(self, module):\n         if isinstance(module, (nn.Conv2d, nn.Conv3d)):\n             nn.init.kaiming_normal_(module.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n+            if module.bias is not None:\n+                fan_in, _ = nn.init._calculate_fan_in_and_fan_out(module.weight)\n+                bound = 1 / math.sqrt(fan_in)\n+                nn.init.uniform_(module.bias, -bound, bound)\n         elif isinstance(module, nn.Linear):\n             nn.init.kaiming_uniform_(module.weight, a=math.sqrt(5))\n             if module.bias is not None:\n                 fan_in, _ = nn.init._calculate_fan_in_and_fan_out(module.weight)\n                 bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n                 nn.init.uniform_(module.bias, -bound, bound)\n         elif isinstance(module, (nn.BatchNorm2d, nn.BatchNorm3d, nn.GroupNorm)):\n-            nn.init.constant_(module.weight, 1)\n-            nn.init.constant_(module.bias, 0)\n+            nn.init.constant_(module.weight, 1.0)\n+            nn.init.constant_(module.bias, 0.0)\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_()\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n \n     def __init__(self, config: Emu3VQVAEConfig):\n         super().__init__(config)\n@@ -1198,16 +1206,16 @@ class Emu3PreTrainedModel(PreTrainedModel):\n \n     def _init_weights(self, module):\n         std = self.config.get_text_config().initializer_range\n-        if isinstance(module, Emu3VQVAE):\n-            module.apply(module._init_weights)\n-        elif isinstance(module, (nn.Linear, nn.Conv2d)):\n+        if isinstance(module, (nn.Linear, nn.Conv2d)):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n                 module.bias.data.zero_()\n         elif isinstance(module, nn.Embedding):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, Emu3RMSNorm):  # noqa: F821\n+            module.weight.data.fill_(1.0)\n \n \n class Emu3RotaryEmbedding(nn.Module):"
        },
        {
            "sha": "62e95a7f731cf61bebd772dd68a0f9f5c17e8895",
            "filename": "src/transformers/models/emu3/modular_emu3.py",
            "status": "modified",
            "additions": 13,
            "deletions": 5,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -747,15 +747,23 @@ class Emu3VQVAE(PreTrainedModel):\n     def _init_weights(self, module):\n         if isinstance(module, (nn.Conv2d, nn.Conv3d)):\n             nn.init.kaiming_normal_(module.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n+            if module.bias is not None:\n+                fan_in, _ = nn.init._calculate_fan_in_and_fan_out(module.weight)\n+                bound = 1 / math.sqrt(fan_in)\n+                nn.init.uniform_(module.bias, -bound, bound)\n         elif isinstance(module, nn.Linear):\n             nn.init.kaiming_uniform_(module.weight, a=math.sqrt(5))\n             if module.bias is not None:\n                 fan_in, _ = nn.init._calculate_fan_in_and_fan_out(module.weight)\n                 bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n                 nn.init.uniform_(module.bias, -bound, bound)\n         elif isinstance(module, (nn.BatchNorm2d, nn.BatchNorm3d, nn.GroupNorm)):\n-            nn.init.constant_(module.weight, 1)\n-            nn.init.constant_(module.bias, 0)\n+            nn.init.constant_(module.weight, 1.0)\n+            nn.init.constant_(module.bias, 0.0)\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_()\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n \n     def __init__(self, config: Emu3VQVAEConfig):\n         super().__init__(config)\n@@ -894,16 +902,16 @@ class Emu3PreTrainedModel(ChameleonPreTrainedModel, Emu3VQVAE):\n \n     def _init_weights(self, module):\n         std = self.config.get_text_config().initializer_range\n-        if isinstance(module, Emu3VQVAE):\n-            module.apply(module._init_weights)\n-        elif isinstance(module, (nn.Linear, nn.Conv2d)):\n+        if isinstance(module, (nn.Linear, nn.Conv2d)):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n                 module.bias.data.zero_()\n         elif isinstance(module, nn.Embedding):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, Emu3RMSNorm):  # noqa: F821\n+            module.weight.data.fill_(1.0)\n \n \n EMU3_TEXT_INPUTS_DOCSTRING = r\"\"\""
        },
        {
            "sha": "f9bcf181c55f2f009621de7c713447e21fd0d23b",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -381,6 +381,8 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, GemmaRMSNorm):\n+            module.weight.data.fill_(1.0)\n \n \n GEMMA_INPUTS_DOCSTRING = r\"\"\""
        },
        {
            "sha": "ecbfedb2ad559cfbbcbe1480badf56ba23dc2745",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -426,6 +426,8 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, Gemma2RMSNorm):\n+            module.weight.data.fill_(1.0)\n \n \n GEMMA2_INPUTS_DOCSTRING = r\"\"\""
        },
        {
            "sha": "7d0447a54545af116a09d8ef4df3d5fcd4be90ce",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 5,
            "deletions": 7,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -486,13 +486,7 @@ class Gemma3PreTrainedModel(PreTrainedModel):\n     _supports_attention_backend = True\n \n     def _init_weights(self, module):\n-        # important: this ported version of Gemma2 isn't meant for training from scratch - only\n-        # inference and fine-tuning - so the proper init weights code has been removed\n-        std = (\n-            self.config.initializer_range\n-            if hasattr(self.config, \"initializer_range\")\n-            else self.config.text_config.initializer_range\n-        )\n+        std = self.config.initializer_range\n \n         if isinstance(module, (nn.Linear, nn.Conv2d)):\n             module.weight.data.normal_(mean=0.0, std=std)\n@@ -502,6 +496,10 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, Gemma3RMSNorm):\n+            module.weight.data.fill_(1.0)\n+        elif isinstance(module, Gemma3MultiModalProjector):\n+            module.mm_input_projection_weight.data.zero_()\n \n \n GEMMA3_INPUTS_DOCSTRING = r\"\"\""
        },
        {
            "sha": "afcb72c2029d3b07e188c033ce7340306f860bba",
            "filename": "src/transformers/models/gemma3/modular_gemma3.py",
            "status": "modified",
            "additions": 5,
            "deletions": 7,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -548,13 +548,7 @@ class Gemma3PreTrainedModel(Gemma2PreTrainedModel):\n     ]\n \n     def _init_weights(self, module):\n-        # important: this ported version of Gemma2 isn't meant for training from scratch - only\n-        # inference and fine-tuning - so the proper init weights code has been removed\n-        std = (\n-            self.config.initializer_range\n-            if hasattr(self.config, \"initializer_range\")\n-            else self.config.text_config.initializer_range\n-        )\n+        std = self.config.initializer_range\n \n         if isinstance(module, (nn.Linear, nn.Conv2d)):\n             module.weight.data.normal_(mean=0.0, std=std)\n@@ -564,6 +558,10 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, Gemma3RMSNorm):\n+            module.weight.data.fill_(1.0)\n+        elif isinstance(module, Gemma3MultiModalProjector):\n+            module.mm_input_projection_weight.data.zero_()\n \n \n class Gemma3TextModel(Gemma2Model):"
        },
        {
            "sha": "044a40140246cfb44362c5c7fb4776a80a7c8672",
            "filename": "src/transformers/models/glm/modeling_glm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -399,6 +399,8 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, GlmRMSNorm):\n+            module.weight.data.fill_(1.0)\n \n \n GLM_INPUTS_DOCSTRING = r\"\"\""
        },
        {
            "sha": "6356356f3e75ad0a4a874feeef4fa0030e99d359",
            "filename": "src/transformers/models/glm4/modeling_glm4.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -407,6 +407,8 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, Glm4RMSNorm):\n+            module.weight.data.fill_(1.0)\n \n \n GLM4_INPUTS_DOCSTRING = r\"\"\""
        },
        {
            "sha": "17db99b2a3e55d78e9435688abbec57dc37d2a76",
            "filename": "src/transformers/models/got_ocr2/modeling_got_ocr2.py",
            "status": "modified",
            "additions": 11,
            "deletions": 15,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -591,26 +591,22 @@ class GotOcr2PreTrainedModel(PreTrainedModel):\n     _supports_static_cache = True\n \n     def _init_weights(self, module):\n-        # important: this ported version of GotOcr2 isn't meant for training from scratch - only\n-        # inference and fine-tuning - so the proper init weights code has been removed - the original codebase\n-        # https://github.com/haotian-liu/GotOcr2/tree/main/got_ocr2 should serve for that purpose\n-        std = (\n-            self.config.initializer_range\n-            if hasattr(self.config, \"initializer_range\")\n-            else self.config.text_config.initializer_range\n-        )\n-\n-        if hasattr(module, \"class_embedding\"):\n-            module.class_embedding.data.normal_(mean=0.0, std=std)\n+        std = getattr(self.config, \"initializer_range\", self.config.get_text_config().initializer_range)\n \n         if isinstance(module, (nn.Linear, nn.Conv2d)):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n                 module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, (nn.LayerNorm, GotOcr2LayerNorm)):  # noqa: F821\n+            module.weight.data.fill_(1.0)\n+            module.bias.data.zero_()\n+        elif isinstance(module, GotOcr2VisionAttention):\n+            if module.use_rel_pos:\n+                module.rel_pos_h.data.zero_()\n+                module.rel_pos_w.data.zero_()\n+        elif isinstance(module, GotOcr2VisionEncoder):\n+            if module.pos_embed is not None:\n+                module.pos_embed.data.zero_()\n \n \n GOT_OCR2_INPUTS_DOCSTRING = r\"\"\""
        },
        {
            "sha": "e3daafd81cccb64bca2ce33a051ce1e7602341fe",
            "filename": "src/transformers/models/got_ocr2/modular_got_ocr2.py",
            "status": "modified",
            "additions": 17,
            "deletions": 1,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -276,7 +276,23 @@ class GotOcr2CausalLMOutputWithPast(LlavaCausalLMOutputWithPast):\n \n \n class GotOcr2PreTrainedModel(LlavaPreTrainedModel):\n-    pass\n+    def _init_weights(self, module):\n+        std = getattr(self.config, \"initializer_range\", self.config.get_text_config().initializer_range)\n+\n+        if isinstance(module, (nn.Linear, nn.Conv2d)):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, (nn.LayerNorm, GotOcr2LayerNorm)):  # noqa: F821\n+            module.weight.data.fill_(1.0)\n+            module.bias.data.zero_()\n+        elif isinstance(module, GotOcr2VisionAttention):\n+            if module.use_rel_pos:\n+                module.rel_pos_h.data.zero_()\n+                module.rel_pos_w.data.zero_()\n+        elif isinstance(module, GotOcr2VisionEncoder):\n+            if module.pos_embed is not None:\n+                module.pos_embed.data.zero_()\n \n \n GOT_OCR2_INPUTS_DOCSTRING = r\"\"\""
        },
        {
            "sha": "43e860a77571b81f431edf21b442cf9647ba77c1",
            "filename": "src/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -75,6 +75,9 @@ def _init_weights(self, module):\n         elif isinstance(module, nn.LayerNorm):\n             module.bias.data.zero_()\n             module.weight.data.fill_(1.0)\n+        elif isinstance(module, GPTNeoXJapaneseAttention):\n+            if module.dense_bias is not None:\n+                module.dense_bias.data.zero_()\n \n \n class GPTNeoXJapaneseAttention(nn.Module):"
        },
        {
            "sha": "445edab65b3d29557f46b70b82430a16785e4415",
            "filename": "src/transformers/models/granite/modeling_granite.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -366,6 +366,8 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, GraniteRMSNorm):\n+            module.weight.data.fill_(1.0)\n \n \n class GraniteRotaryEmbedding(nn.Module):"
        },
        {
            "sha": "6ae4e6c35ffc82e41e7723913d0a732d3427935b",
            "filename": "src/transformers/models/granite_speech/modeling_granite_speech.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fmodeling_granite_speech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fmodeling_granite_speech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fmodeling_granite_speech.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -330,11 +330,15 @@ def _init_weights(self, module: nn.Module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n                 module.bias.data.zero_()\n-\n         elif isinstance(module, nn.Embedding):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, (nn.LayerNorm, nn.BatchNorm1d)):\n+            module.weight.data.fill_(1.0)\n+            module.bias.data.zero_()\n+        elif isinstance(module, GraniteSpeechEncoderProjector):\n+            module.query.data.normal_()\n \n \n GRANITE_SPEECH_INPUTS_DOCSTRING = r\"\"\""
        },
        {
            "sha": "621788842461b482f7aad9b4d7ce63ad0f8791f0",
            "filename": "src/transformers/models/granitemoe/modeling_granitemoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -833,8 +833,7 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n+        elif isinstance(module, GraniteMoeRMSNorm):\n             module.weight.data.fill_(1.0)\n         elif isinstance(module, GraniteMoeParallelExperts):\n             module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)"
        },
        {
            "sha": "787b805ee87c8f1c4be7885a3b55d09e2e11ff87",
            "filename": "src/transformers/models/granitemoeshared/modeling_granitemoeshared.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -745,8 +745,7 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n+        elif isinstance(module, GraniteMoeSharedRMSNorm):\n             module.weight.data.fill_(1.0)\n         elif isinstance(module, GraniteMoeSharedParallelExperts):\n             module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)"
        },
        {
            "sha": "61bf2f2d09b59262a05c61b65b9925b556b6a67b",
            "filename": "src/transformers/models/helium/modeling_helium.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -384,6 +384,8 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, HeliumRMSNorm):\n+            module.weight.data.fill_(1.0)\n \n \n HELIUM_INPUTS_DOCSTRING = r\"\"\""
        },
        {
            "sha": "770eb90ab1a7bf489ca026f74fd48a71c20f26ff",
            "filename": "src/transformers/models/idefics/modeling_idefics.py",
            "status": "modified",
            "additions": 21,
            "deletions": 3,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -44,7 +44,7 @@\n )\n from .configuration_idefics import IdeficsConfig\n from .perceiver import IdeficsPerceiverResampler\n-from .vision import IdeficsVisionTransformer\n+from .vision import IdeficsVisionEmbeddings, IdeficsVisionTransformer\n \n \n if is_torch_flex_attn_available():\n@@ -934,14 +934,33 @@ def _init_weights(self, module):\n         # inference and fine-tuning - so the proper init weights code has been removed - the m4 code\n         # base should be used for training from scratch and it contains the correct code.\n         std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n+        if isinstance(module, (nn.Linear, nn.Conv2d)):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n                 module.bias.data.zero_()\n         elif isinstance(module, nn.Embedding):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.weight.data.fill_(1.0)\n+            module.bias.data.zero_()\n+        elif isinstance(module, IdeficsRMSNorm):\n+            module.weight.data.fill_(1.0)\n+        elif isinstance(module, IdeficsVisionEmbeddings):\n+            module.class_embedding.data.normal_()\n+        elif isinstance(module, IdeficsGatedCrossAttentionLayer):\n+            if self.config.alpha_initializer == \"zeros\":\n+                module.alpha_cross_attn.data.zero_()\n+                module.alpha_dense.data.zero_()\n+            elif self.config.alpha_initializer == \"ones\":\n+                module.alpha_cross_attn.data.fill_(1.0)\n+                module.alpha_dense.data.fill_(1.0)\n+            elif self.config.alpha_initializer in {\"normal\", \"gaussian\", \"random\"}:\n+                module.alpha_cross_attn.data.normal_(mean=0.0, std=self.config.alphas_initializer_range)\n+                module.alpha_dense.data.normal_(mean=0.0, std=self.config.alphas_initializer_range)\n+        elif isinstance(module, IdeficsPerceiverResampler):\n+            module.latents.data.normal_()\n \n \n LLAMA_INPUTS_DOCSTRING = r\"\"\"\n@@ -1495,7 +1514,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n \n \n class IdeficsForVisionText2Text(IdeficsPreTrainedModel, GenerationMixin):\n-    _keys_to_ignore_on_load_missing = [r\"lm_head.weight\"]\n     _tied_weights_keys = [\"model.embed_tokens.weight\", \"lm_head.weight\"]\n \n     def __init__(self, config, vision_model=None):"
        },
        {
            "sha": "31912d6ad92c28d4478ea39151288efb154531a6",
            "filename": "src/transformers/models/idefics2/configuration_idefics2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fidefics2%2Fconfiguration_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fidefics2%2Fconfiguration_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fconfiguration_idefics2.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -130,6 +130,8 @@ class Idefics2PerceiverConfig(PretrainedConfig):\n             Number of key-value heads in the perceiver attention block.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n             The dropout ratio for the attention probabilities.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation for initializing all weight matrices in the model.\n     \"\"\"\n \n     model_type = \"idefics2_perceiver\"\n@@ -145,6 +147,7 @@ def __init__(\n         resampler_head_dim=96,\n         num_key_value_heads=4,\n         attention_dropout=0.0,\n+        initializer_range=0.02,\n         **kwargs,\n     ):\n         self.hidden_act = hidden_act\n@@ -156,6 +159,7 @@ def __init__(\n         self.num_key_value_heads = num_key_value_heads\n         self.resampler_head_dim = resampler_head_dim\n         self.attention_dropout = attention_dropout\n+        self.initializer_range = initializer_range\n         if self.num_key_value_heads > self.resampler_n_heads:\n             raise ValueError(\n                 f\"num_key_value_heads={self.num_key_value_heads} must be less than or equal to\""
        },
        {
            "sha": "d23085bd3740f00f944a0b582455dccaa068c251",
            "filename": "src/transformers/models/idefics2/modeling_idefics2.py",
            "status": "modified",
            "additions": 12,
            "deletions": 8,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -517,14 +517,7 @@ class Idefics2PreTrainedModel(PreTrainedModel):\n     _supports_cache_class = True\n \n     def _init_weights(self, module):\n-        std = (\n-            self.config.initializer_range\n-            if hasattr(self.config, \"initializer_range\")\n-            else self.config.get_text_config().initializer_range\n-        )\n-\n-        if hasattr(module, \"class_embedding\"):\n-            module.class_embedding.data.normal_(mean=0.0, std=std)\n+        std = getattr(self.config, \"initializer_range\", self.config.get_text_config().initializer_range)\n \n         if isinstance(module, (nn.Linear, nn.Conv2d)):\n             module.weight.data.normal_(mean=0.0, std=std)\n@@ -534,6 +527,17 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.weight.data.fill_(1.0)\n+            module.bias.data.zero_()\n+        elif isinstance(module, Idefics2RMSNorm):\n+            module.weight.data.fill_(1.0)\n+        elif isinstance(module, nn.MultiheadAttention):\n+            module._reset_parameters()  # native torch init\n+        elif isinstance(module, Idefics2MultiheadAttentionPoolingHead):\n+            module.probe.data.normal_()\n+        elif isinstance(module, Idefics2PerceiverResampler):\n+            module.latents.data.fill_(1.0)\n \n \n IDEFICS2_INPUTS_DOCSTRING = r\"\"\""
        },
        {
            "sha": "5945fd71c5f16a1cb6858f7b947def221facf636",
            "filename": "src/transformers/models/idefics3/modeling_idefics3.py",
            "status": "modified",
            "additions": 6,
            "deletions": 9,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -533,16 +533,8 @@ class Idefics3PreTrainedModel(PreTrainedModel):\n     _supports_flex_attn = True\n     _supports_cache_class = True\n \n-    # Copied from transformers.models.idefics2.modeling_idefics2.Idefics2PreTrainedModel._init_weights\n     def _init_weights(self, module):\n-        std = (\n-            self.config.initializer_range\n-            if hasattr(self.config, \"initializer_range\")\n-            else self.config.get_text_config().initializer_range\n-        )\n-\n-        if hasattr(module, \"class_embedding\"):\n-            module.class_embedding.data.normal_(mean=0.0, std=std)\n+        std = getattr(self.config, \"initializer_range\", self.config.get_text_config().initializer_range)\n \n         if isinstance(module, (nn.Linear, nn.Conv2d)):\n             module.weight.data.normal_(mean=0.0, std=std)\n@@ -552,6 +544,11 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.weight.data.fill_(1.0)\n+            module.bias.data.zero_()\n+        elif isinstance(module, Idefics3RMSNorm):\n+            module.weight.data.fill_(1.0)\n \n \n IDEFICS3_VISION_START_DOCSTRING = r\"\"\""
        },
        {
            "sha": "a304353cc41129faf672257df14785e61602f5e4",
            "filename": "src/transformers/models/instructblip/modeling_instructblip.py",
            "status": "modified",
            "additions": 10,
            "deletions": 12,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -323,26 +323,24 @@ class InstructBlipPreTrainedModel(PreTrainedModel):\n         \"InstructBlipQFormerSelfOutput\",\n     ]\n \n-    # Copied from transformers.models.blip_2.modeling_blip_2.Blip2PreTrainedModel._init_weights with Blip2->InstructBlip\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         factor = self.config.initializer_range\n-        if isinstance(module, nn.Conv2d) or isinstance(module, nn.Embedding) or isinstance(module, nn.Linear):\n+\n+        if isinstance(module, (nn.Linear, nn.Conv2d)):\n             module.weight.data.normal_(mean=0.0, std=factor)\n-            if hasattr(module, \"bias\") and module.bias is not None:\n+            if module.bias is not None:\n                 module.bias.data.zero_()\n-\n-        if isinstance(module, InstructBlipVisionEmbeddings):\n-            if hasattr(self.config, \"vision_config\") and not isinstance(self.config, InstructBlipVisionConfig):\n-                factor = self.config.vision_config.initializer_range\n-            nn.init.trunc_normal_(module.position_embedding, mean=0.0, std=factor)\n-            nn.init.trunc_normal_(module.class_embedding, mean=0.0, std=factor)\n-\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=factor)\n         elif isinstance(module, nn.LayerNorm):\n             module.bias.data.zero_()\n             module.weight.data.fill_(1.0)\n-        elif isinstance(module, nn.Linear) and module.bias is not None:\n-            module.bias.data.zero_()\n+        elif isinstance(module, InstructBlipVisionEmbeddings):\n+            nn.init.trunc_normal_(module.position_embedding, mean=0.0, std=factor)\n+            nn.init.trunc_normal_(module.class_embedding, mean=0.0, std=factor)\n+        elif isinstance(module, InstructBlipForConditionalGeneration):\n+            module.query_tokens.data.zero_()\n \n \n INSTRUCTBLIP_START_DOCSTRING = r\"\"\""
        },
        {
            "sha": "d2a6c7b6f12045f4f1a05f9fd2804122932cbfbb",
            "filename": "src/transformers/models/instructblipvideo/modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 188,
            "deletions": 189,
            "changes": 377,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -130,44 +130,6 @@ def forward(self, pixel_values: torch.FloatTensor, interpolate_pos_encoding: boo\n         return embeddings\n \n \n-class InstructBlipVideoPreTrainedModel(PreTrainedModel):\n-    \"\"\"\n-    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n-    models.\n-    \"\"\"\n-\n-    config_class = InstructBlipVideoConfig\n-    base_model_prefix = \"blip\"\n-    supports_gradient_checkpointing = True\n-\n-    _no_split_modules = [\n-        \"InstructBlipVideoQFormerEmbeddings\",\n-        \"InstructBlipVideoAttention\",\n-        \"InstructBlipVideoQFormerMultiHeadAttention\",\n-        \"InstructBlipVideoQFormerSelfOutput\",\n-    ]\n-\n-    def _init_weights(self, module):\n-        \"\"\"Initialize the weights\"\"\"\n-        factor = self.config.initializer_range\n-        if isinstance(module, nn.Conv2d) or isinstance(module, nn.Embedding) or isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=factor)\n-            if hasattr(module, \"bias\") and module.bias is not None:\n-                module.bias.data.zero_()\n-\n-        if isinstance(module, InstructBlipVideoVisionEmbeddings):\n-            if hasattr(self.config, \"vision_config\") and not isinstance(self.config, InstructBlipVideoVisionConfig):\n-                factor = self.config.vision_config.initializer_range\n-            nn.init.trunc_normal_(module.position_embedding, mean=0.0, std=factor)\n-            nn.init.trunc_normal_(module.class_embedding, mean=0.0, std=factor)\n-\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n-        elif isinstance(module, nn.Linear) and module.bias is not None:\n-            module.bias.data.zero_()\n-\n-\n class InstructBlipVideoAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -416,73 +378,6 @@ def forward(\n \"\"\"\n \n \n-class InstructBlipVideoVisionModel(InstructBlipVideoPreTrainedModel):\n-    main_input_name = \"pixel_values\"\n-    config_class = InstructBlipVideoVisionConfig\n-\n-    def __init__(self, config: InstructBlipVideoVisionConfig):\n-        super().__init__(config)\n-        self.config = config\n-        embed_dim = config.hidden_size\n-\n-        self.embeddings = InstructBlipVideoVisionEmbeddings(config)\n-        self.encoder = InstructBlipVideoEncoder(config)\n-        self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n-\n-        self.post_init()\n-\n-    @add_start_docstrings_to_model_forward(INSTRUCTBLIPVIDEO_VISION_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=InstructBlipVideoVisionConfig)\n-    def forward(\n-        self,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        interpolate_pos_encoding: bool = False,\n-    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n-        r\"\"\"\n-        Returns:\n-\n-        \"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n-        if pixel_values is None:\n-            raise ValueError(\"You have to specify pixel_values\")\n-\n-        hidden_states = self.embeddings(pixel_values, interpolate_pos_encoding=interpolate_pos_encoding)\n-\n-        encoder_outputs = self.encoder(\n-            inputs_embeds=hidden_states,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-        )\n-\n-        last_hidden_state = encoder_outputs[0]\n-        last_hidden_state = self.post_layernorm(last_hidden_state)\n-\n-        pooled_output = last_hidden_state[:, 0, :]\n-        pooled_output = self.post_layernorm(pooled_output)\n-\n-        if not return_dict:\n-            return (last_hidden_state, pooled_output) + encoder_outputs[1:]\n-\n-        return BaseModelOutputWithPooling(\n-            last_hidden_state=last_hidden_state,\n-            pooler_output=pooled_output,\n-            hidden_states=encoder_outputs.hidden_states,\n-            attentions=encoder_outputs.attentions,\n-        )\n-\n-    def get_input_embeddings(self):\n-        return self.embeddings\n-\n-\n class InstructBlipVideoQFormerMultiHeadAttention(nn.Module):\n     def __init__(self, config, is_cross_attention=False):\n         super().__init__()\n@@ -957,6 +852,194 @@ def forward(\n         return embeddings\n \n \n+INSTRUCTBLIPVIDEO_START_DOCSTRING = r\"\"\"\n+    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n+    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n+    etc.)\n+\n+    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n+    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n+    and behavior.\n+\n+    Parameters:\n+        config ([`InstructBlipVideoConfig`]): Model configuration class with all the parameters of the model.\n+            Initializing with a config file does not load the weights associated with the model, only the\n+            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\"\"\"\n+\n+INSTRUCTBLIPVIDEO_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n+            Pixel values. Pixel values can be obtained using [`InstructBlipVideoProcessor`]. See\n+            [`InstructBlipVideoProcessor.__call__`] for details.\n+\n+        qformer_input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Indices of input sequence tokens in the vocabulary of the Q-Former. Input tokens can optionally be provided\n+            to serve as text prompt, which the Q-Former model will encode.\n+\n+            Indices can be obtained using [`InstructBlipVideoProcessor`]. See [`InstructBlipVideoProcessor.__call__`] for\n+            details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n+\n+        qformer_attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n+\n+            - 1 for tokens that are **not masked**,\n+            - 0 for tokens that are **masked**.\n+\n+            [What are attention masks?](../glossary#attention-mask)\n+\n+        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Indices of input sequence tokens in the vocabulary of the language model. Input tokens can optionally be\n+            provided to serve as text prompt, which the language model can continue.\n+\n+            Indices can be obtained using [`InstructBlipVideoProcessor`]. See [`InstructBlipVideoProcessor.__call__`] for\n+            details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n+\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n+\n+            - 1 for tokens that are **not masked**,\n+            - 0 for tokens that are **masked**.\n+\n+            [What are attention masks?](../glossary#attention-mask)\n+\n+        decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n+            Indices of decoder input sequence tokens in the vocabulary of the language model. Only relevant in case an\n+            encoder-decoder language model (like T5) is used.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details. [What are decoder input IDs?](../glossary#decoder-input-ids)\n+\n+        decoder_attention_mask (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n+            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n+            be used by default.\n+\n+            Only relevant in case an encoder-decoder language model (like T5) is used.\n+\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+        interpolate_pos_encoding (`bool`, *optional*, defaults to `False`):\n+            Whether to interpolate the pre-trained position encodings.\n+        use_cache (`bool`, *optional*):\n+            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n+            `past_key_values`).\n+\"\"\"\n+\n+\n+class InstructBlipVideoPreTrainedModel(PreTrainedModel):\n+    \"\"\"\n+    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n+    models.\n+    \"\"\"\n+\n+    config_class = InstructBlipVideoConfig\n+    base_model_prefix = \"blip\"\n+    supports_gradient_checkpointing = True\n+\n+    _no_split_modules = [\n+        \"InstructBlipVideoQFormerEmbeddings\",\n+        \"InstructBlipVideoAttention\",\n+        \"InstructBlipVideoQFormerMultiHeadAttention\",\n+        \"InstructBlipVideoQFormerSelfOutput\",\n+    ]\n+\n+    def _init_weights(self, module):\n+        \"\"\"Initialize the weights\"\"\"\n+        factor = self.config.initializer_range\n+\n+        if isinstance(module, (nn.Linear, nn.Conv2d)):\n+            module.weight.data.normal_(mean=0.0, std=factor)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=factor)\n+        elif isinstance(module, nn.LayerNorm):\n+            module.bias.data.zero_()\n+            module.weight.data.fill_(1.0)\n+        elif isinstance(module, InstructBlipVideoVisionEmbeddings):\n+            nn.init.trunc_normal_(module.position_embedding, mean=0.0, std=factor)\n+            nn.init.trunc_normal_(module.class_embedding, mean=0.0, std=factor)\n+        elif isinstance(module, InstructBlipVideoForConditionalGeneration):\n+            module.query_tokens.data.zero_()\n+\n+\n+class InstructBlipVideoVisionModel(InstructBlipVideoPreTrainedModel):\n+    main_input_name = \"pixel_values\"\n+    config_class = InstructBlipVideoVisionConfig\n+\n+    def __init__(self, config: InstructBlipVideoVisionConfig):\n+        super().__init__(config)\n+        self.config = config\n+        embed_dim = config.hidden_size\n+\n+        self.embeddings = InstructBlipVideoVisionEmbeddings(config)\n+        self.encoder = InstructBlipVideoEncoder(config)\n+        self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n+\n+        self.post_init()\n+\n+    @add_start_docstrings_to_model_forward(INSTRUCTBLIPVIDEO_VISION_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=InstructBlipVideoVisionConfig)\n+    def forward(\n+        self,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        interpolate_pos_encoding: bool = False,\n+    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n+        r\"\"\"\n+        Returns:\n+\n+        \"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        if pixel_values is None:\n+            raise ValueError(\"You have to specify pixel_values\")\n+\n+        hidden_states = self.embeddings(pixel_values, interpolate_pos_encoding=interpolate_pos_encoding)\n+\n+        encoder_outputs = self.encoder(\n+            inputs_embeds=hidden_states,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+\n+        last_hidden_state = encoder_outputs[0]\n+        last_hidden_state = self.post_layernorm(last_hidden_state)\n+\n+        pooled_output = last_hidden_state[:, 0, :]\n+        pooled_output = self.post_layernorm(pooled_output)\n+\n+        if not return_dict:\n+            return (last_hidden_state, pooled_output) + encoder_outputs[1:]\n+\n+        return BaseModelOutputWithPooling(\n+            last_hidden_state=last_hidden_state,\n+            pooler_output=pooled_output,\n+            hidden_states=encoder_outputs.hidden_states,\n+            attentions=encoder_outputs.attentions,\n+        )\n+\n+    def get_input_embeddings(self):\n+        return self.embeddings\n+\n+\n class InstructBlipVideoQFormerModel(InstructBlipVideoPreTrainedModel):\n     \"\"\"\n     Querying Transformer (Q-Former), used in InstructBlipVideo. Slightly modified from BLIP-2 as it also takes the\n@@ -1186,90 +1269,6 @@ def to_tuple(self) -> Tuple[Any]:\n         )\n \n \n-INSTRUCTBLIPVIDEO_START_DOCSTRING = r\"\"\"\n-    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n-    etc.)\n-\n-    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n-    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n-    and behavior.\n-\n-    Parameters:\n-        config ([`InstructBlipVideoConfig`]): Model configuration class with all the parameters of the model.\n-            Initializing with a config file does not load the weights associated with the model, only the\n-            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-INSTRUCTBLIPVIDEO_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n-            Pixel values. Pixel values can be obtained using [`InstructBlipVideoProcessor`]. See\n-            [`InstructBlipVideoProcessor.__call__`] for details.\n-\n-        qformer_input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Indices of input sequence tokens in the vocabulary of the Q-Former. Input tokens can optionally be provided\n-            to serve as text prompt, which the Q-Former model will encode.\n-\n-            Indices can be obtained using [`InstructBlipVideoProcessor`]. See [`InstructBlipVideoProcessor.__call__`] for\n-            details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-\n-        qformer_attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-\n-        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Indices of input sequence tokens in the vocabulary of the language model. Input tokens can optionally be\n-            provided to serve as text prompt, which the language model can continue.\n-\n-            Indices can be obtained using [`InstructBlipVideoProcessor`]. See [`InstructBlipVideoProcessor.__call__`] for\n-            details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-\n-        decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n-            Indices of decoder input sequence tokens in the vocabulary of the language model. Only relevant in case an\n-            encoder-decoder language model (like T5) is used.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details. [What are decoder input IDs?](../glossary#decoder-input-ids)\n-\n-        decoder_attention_mask (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n-            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n-            be used by default.\n-\n-            Only relevant in case an encoder-decoder language model (like T5) is used.\n-\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-        interpolate_pos_encoding (`bool`, *optional*, defaults to `False`):\n-            Whether to interpolate the pre-trained position encodings.\n-        use_cache (`bool`, *optional*):\n-            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n-            `past_key_values`).\n-\"\"\"\n-\n-\n @add_start_docstrings(\n     \"\"\"\n     InstructBlipVideo Model for generating text given an image and an optional text prompt. The model consists of a vision"
        },
        {
            "sha": "c91cc6e29985c56dfd1c5aa4a17d3d19d804eda5",
            "filename": "src/transformers/models/jamba/modeling_jamba.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -1115,6 +1115,13 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, JambaRMSNorm):\n+            module.weight.data.fill_(1.0)\n+        elif isinstance(module, JambaMambaMixer):\n+            A = torch.arange(1, module.ssm_state_size + 1)[None, :]\n+            A = A.expand(module.intermediate_size, -1).contiguous()\n+            module.A_log.data.copy_(torch.log(A))\n+            module.D.data.fill_(1.0)\n \n \n JAMBA_INPUTS_DOCSTRING = r\"\"\""
        },
        {
            "sha": "44e0e5b809582c31c01925ab02534f77d762404e",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -856,8 +856,7 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n+        elif isinstance(module, JetMoeRMSNorm):\n             module.weight.data.fill_(1.0)\n         elif isinstance(module, JetMoeParallelExperts):\n             module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)"
        },
        {
            "sha": "eec1ecfee32e46e90b3bcac26efb801c7b969582",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -389,6 +389,8 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, LlamaRMSNorm):\n+            module.weight.data.fill_(1.0)\n \n \n LLAMA_INPUTS_DOCSTRING = r\"\"\""
        },
        {
            "sha": "37614db7c23035223512eb66928d5df52b2bc0b4",
            "filename": "src/transformers/models/llama4/modeling_llama4.py",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -492,6 +492,17 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.weight.data.fill_(1.0)\n+            module.bias.data.zero_()\n+        elif isinstance(module, Llama4TextRMSNorm):\n+            module.weight.data.fill_(1.0)\n+        elif isinstance(module, Llama4TextExperts):\n+            module.gate_up_proj.data.normal_(mean=0.0, std=std)\n+            module.down_proj.data.normal_(mean=0.0, std=std)\n+        elif isinstance(module, Llama4VisionModel):\n+            module.class_embedding.data.normal_(std=module.scale)\n+            module.positional_embedding_vlm.data.normal_(std=module.scale)\n \n \n LLAMA4_INPUTS_DOCSTRING = r\"\"\""
        },
        {
            "sha": "a8b5f074d0c29890f9e1f374c07f5d3c7739f835",
            "filename": "src/transformers/models/llava/modeling_llava.py",
            "status": "modified",
            "additions": 2,
            "deletions": 13,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -144,23 +144,12 @@ def _init_weights(self, module):\n         # important: this ported version of Llava isn't meant for training from scratch - only\n         # inference and fine-tuning - so the proper init weights code has been removed - the original codebase\n         # https://github.com/haotian-liu/LLaVA/tree/main/llava should serve for that purpose\n-        std = (\n-            self.config.initializer_range\n-            if hasattr(self.config, \"initializer_range\")\n-            else self.config.text_config.initializer_range\n-        )\n-\n-        if hasattr(module, \"class_embedding\"):\n-            module.class_embedding.data.normal_(mean=0.0, std=std)\n+        std = getattr(self.config, \"initializer_range\", self.config.get_text_config().initializer_range)\n \n-        if isinstance(module, (nn.Linear, nn.Conv2d)):\n+        if isinstance(module, nn.Linear):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n                 module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n \n \n LLAVA_INPUTS_DOCSTRING = r\"\"\""
        },
        {
            "sha": "6301402e6e62d47c266a97ca9934a7edfdcc7e2c",
            "filename": "src/transformers/models/llava_next/modeling_llava_next.py",
            "status": "modified",
            "additions": 5,
            "deletions": 17,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -236,7 +236,6 @@ def forward(self, image_features):\n     \"The bare LLaMA Model outputting raw hidden-states without any specific head on top.\",\n     LLAVA_NEXT_START_DOCSTRING,\n )\n-# Copied from transformers.models.llava.modeling_llava.LlavaPreTrainedModel with Llava->LlavaNext,llava->llava_next\n class LlavaNextPreTrainedModel(PreTrainedModel):\n     config_class = LlavaNextConfig\n     base_model_prefix = \"model\"\n@@ -250,26 +249,15 @@ class LlavaNextPreTrainedModel(PreTrainedModel):\n     _supports_static_cache = True\n \n     def _init_weights(self, module):\n-        # important: this ported version of LlavaNext isn't meant for training from scratch - only\n-        # inference and fine-tuning - so the proper init weights code has been removed - the original codebase\n-        # https://github.com/haotian-liu/LLaVA/tree/main/llava_next should serve for that purpose\n-        std = (\n-            self.config.initializer_range\n-            if hasattr(self.config, \"initializer_range\")\n-            else self.config.text_config.initializer_range\n-        )\n-\n-        if hasattr(module, \"class_embedding\"):\n-            module.class_embedding.data.normal_(mean=0.0, std=std)\n+        std = getattr(self.config, \"initializer_range\", self.config.get_text_config().initializer_range)\n \n-        if isinstance(module, (nn.Linear, nn.Conv2d)):\n+        if isinstance(module, nn.Linear):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n                 module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, LlavaNextForConditionalGeneration):\n+            embed_std = 1 / math.sqrt(self.config.text_config.hidden_size)\n+            module.image_newline.data.normal_(mean=0.0, std=embed_std)\n \n \n LLAVA_NEXT_INPUTS_DOCSTRING = r\"\"\""
        },
        {
            "sha": "b4a9c899c9c39d2c9db68f2d8eb653aa51bf8f3c",
            "filename": "src/transformers/models/llava_next_video/modeling_llava_next_video.py",
            "status": "modified",
            "additions": 45,
            "deletions": 56,
            "changes": 101,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -129,62 +129,6 @@ def forward(self, image_features):\n         return image_features_spatial_pool.flatten(2).transpose(1, 2).contiguous()\n \n \n-LLAVA_NEXT_VIDEO_START_DOCSTRING = r\"\"\"\n-    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n-    etc.)\n-\n-    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n-    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n-    and behavior.\n-\n-    Parameters:\n-        config ([`LlavaNextVideoConfig`] or [`LlavaNextVideoVisionConfig`]):\n-            Model configuration class with all the parameters of the model. Initializing with a config file does not\n-            load the weights associated with the model, only the configuration. Check out the\n-            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"The bare LLaMA Model outputting raw hidden-states without any specific head on top.\",\n-    LLAVA_NEXT_VIDEO_START_DOCSTRING,\n-)\n-class LlavaNextVideoPreTrainedModel(PreTrainedModel):\n-    config_class = LlavaNextVideoConfig\n-    base_model_prefix = \"model\"\n-    supports_gradient_checkpointing = True\n-    _no_split_modules = [\"LlavaNextVideoVisionAttention\"]\n-    _skip_keys_device_placement = \"past_key_values\"\n-    _supports_cache_class = True\n-    _supports_flash_attn_2 = True\n-    _supports_sdpa = True\n-    _supports_quantized_cache = True\n-    _supports_static_cache = True\n-\n-    def _init_weights(self, module):\n-        # important: this ported version of LlavaNextVideo isn't meant for training from scratch - only\n-        # inference and fine-tuning - so the proper init weights code has been removed - the original codebase\n-        # https://github.com/haotian-liu/LLaVA/tree/main/llava_next_video should serve for that purpose\n-        std = (\n-            self.config.initializer_range\n-            if hasattr(self.config, \"initializer_range\")\n-            else self.config.text_config.initializer_range\n-        )\n-\n-        if hasattr(module, \"class_embedding\"):\n-            module.class_embedding.data.normal_(mean=0.0, std=std)\n-\n-        if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-\n-\n class LlavaNextVideoMultiModalProjector(nn.Module):\n     def __init__(self, config: LlavaNextVideoConfig):\n         super().__init__()\n@@ -207,6 +151,23 @@ def forward(self, image_features):\n         return hidden_states\n \n \n+LLAVA_NEXT_VIDEO_START_DOCSTRING = r\"\"\"\n+    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n+    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n+    etc.)\n+\n+    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n+    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n+    and behavior.\n+\n+    Parameters:\n+        config ([`LlavaNextVideoConfig`] or [`LlavaNextVideoVisionConfig`]):\n+            Model configuration class with all the parameters of the model. Initializing with a config file does not\n+            load the weights associated with the model, only the configuration. Check out the\n+            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\"\"\"\n+\n+\n def get_anyres_image_grid_shape(image_size, grid_pinpoints, patch_size):\n     \"\"\"\n     Calculate the shape of the image patch grid after the preprocessing for images of any resolution.\n@@ -394,6 +355,34 @@ def unpad_image(tensor, original_size):\n \"\"\"\n \n \n+@add_start_docstrings(\n+    \"The bare LLaMA Model outputting raw hidden-states without any specific head on top.\",\n+    LLAVA_NEXT_VIDEO_START_DOCSTRING,\n+)\n+class LlavaNextVideoPreTrainedModel(PreTrainedModel):\n+    config_class = LlavaNextVideoConfig\n+    base_model_prefix = \"model\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"LlavaNextVideoVisionAttention\"]\n+    _skip_keys_device_placement = \"past_key_values\"\n+    _supports_cache_class = True\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n+    _supports_quantized_cache = True\n+    _supports_static_cache = True\n+\n+    def _init_weights(self, module):\n+        std = getattr(self.config, \"initializer_range\", self.config.get_text_config().initializer_range)\n+\n+        if isinstance(module, (nn.Linear, nn.Conv2d)):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, LlavaNextVideoForConditionalGeneration):\n+            embed_std = 1 / math.sqrt(self.config.text_config.hidden_size)\n+            module.image_newline.data.normal_(mean=0.0, std=embed_std)\n+\n+\n @add_start_docstrings(\n     \"\"\"The LLAVA-NeXT model which consists of a vision backbone and a language model.\"\"\",\n     LLAVA_NEXT_VIDEO_START_DOCSTRING,"
        },
        {
            "sha": "0109082d8411e939a6381daaaed558e796cd7c7e",
            "filename": "src/transformers/models/llava_next_video/modular_llava_next_video.py",
            "status": "modified",
            "additions": 15,
            "deletions": 1,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -24,6 +24,7 @@\n from transformers.models.llava_next.modeling_llava_next import (\n     LlavaNextCausalLMOutputWithPast,\n     LlavaNextForConditionalGeneration,\n+    LlavaNextMultiModalProjector,\n     LlavaNextPreTrainedModel,\n     image_size_to_num_patches,\n )\n@@ -222,10 +223,23 @@ def forward(self, image_features):\n         return image_features_spatial_pool.flatten(2).transpose(1, 2).contiguous()\n \n \n-class LlavaNextVideoPreTrainedModel(LlavaNextPreTrainedModel):\n+class LlavaNextVideoMultiModalProjector(LlavaNextMultiModalProjector):\n     pass\n \n \n+class LlavaNextVideoPreTrainedModel(LlavaNextPreTrainedModel):\n+    def _init_weights(self, module):\n+        std = getattr(self.config, \"initializer_range\", self.config.get_text_config().initializer_range)\n+\n+        if isinstance(module, (nn.Linear, nn.Conv2d)):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, LlavaNextVideoForConditionalGeneration):\n+            embed_std = 1 / math.sqrt(self.config.text_config.hidden_size)\n+            module.image_newline.data.normal_(mean=0.0, std=embed_std)\n+\n+\n class LlavaNextVideoForConditionalGeneration(LlavaNextForConditionalGeneration):\n     def __init__(self, config: LlavaNextVideoConfig, **super_kwargs):\n         super().__init__(config, **super_kwargs)"
        },
        {
            "sha": "3f77d39c0221973b3c229c046e9639b7813b0086",
            "filename": "src/transformers/models/llava_onevision/modeling_llava_onevision.py",
            "status": "modified",
            "additions": 6,
            "deletions": 17,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -255,28 +255,17 @@ class LlavaOnevisionPreTrainedModel(PreTrainedModel):\n     _supports_quantized_cache = True\n     _supports_sdpa = True\n \n-    # Copied from transformers.models.llava_next.modeling_llava_next.LlavaNextPreTrainedModel._init_weights\n+    # Copied from transformers.models.llava_next.modeling_llava_next.LlavaNextPreTrainedModel._init_weights with LlavaNext->LlavaOnevision\n     def _init_weights(self, module):\n-        # important: this ported version of LlavaNext isn't meant for training from scratch - only\n-        # inference and fine-tuning - so the proper init weights code has been removed - the original codebase\n-        # https://github.com/haotian-liu/LLaVA/tree/main/llava_next should serve for that purpose\n-        std = (\n-            self.config.initializer_range\n-            if hasattr(self.config, \"initializer_range\")\n-            else self.config.text_config.initializer_range\n-        )\n-\n-        if hasattr(module, \"class_embedding\"):\n-            module.class_embedding.data.normal_(mean=0.0, std=std)\n+        std = getattr(self.config, \"initializer_range\", self.config.get_text_config().initializer_range)\n \n-        if isinstance(module, (nn.Linear, nn.Conv2d)):\n+        if isinstance(module, nn.Linear):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n                 module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, LlavaOnevisionForConditionalGeneration):\n+            embed_std = 1 / math.sqrt(self.config.text_config.hidden_size)\n+            module.image_newline.data.normal_(mean=0.0, std=embed_std)\n \n \n LLAVA_ONEVISION_INPUTS_DOCSTRING = r\"\"\""
        },
        {
            "sha": "02aedb8c0140a4e174b6670821f6451dd457982e",
            "filename": "src/transformers/models/mimi/modeling_mimi.py",
            "status": "modified",
            "additions": 4,
            "deletions": 13,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -1412,31 +1412,22 @@ class MimiPreTrainedModel(PreTrainedModel):\n     _supports_cache_class = True\n     _supports_static_cache = True\n \n-    # Copied from transformers.models.encodec.modeling_encodec.EncodecPreTrainedModel._init_weights\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, nn.Linear):\n             module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n                 module.bias.data.zero_()\n-        elif isinstance(module, (nn.LayerNorm, nn.GroupNorm)):\n+        elif isinstance(module, nn.LayerNorm):\n             module.bias.data.zero_()\n             module.weight.data.fill_(1.0)\n-        elif isinstance(module, nn.Conv1d):\n+        elif isinstance(module, (nn.Conv1d, nn.ConvTranspose1d)):\n             nn.init.kaiming_normal_(module.weight)\n             if module.bias is not None:\n                 k = math.sqrt(module.groups / (module.in_channels * module.kernel_size[0]))\n                 nn.init.uniform_(module.bias, a=-k, b=k)\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LSTM):\n-            for name, param in module.named_parameters():\n-                if \"weight\" in name:\n-                    nn.init.xavier_uniform_(param)\n-                elif \"bias\" in name:\n-                    nn.init.constant_(param, 0.0)\n+        elif isinstance(module, MimiLayerScale):\n+            module.scale.data.fill_(self.config.layer_scale_initial_scale)\n \n \n MIMI_START_DOCSTRING = r\"\"\""
        },
        {
            "sha": "7de6cad370c5e1bfd8771ea858dfbdf9fb9e60b8",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -318,6 +318,8 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, MistralRMSNorm):\n+            module.weight.data.fill_(1.0)\n \n \n class MistralRotaryEmbedding(nn.Module):"
        },
        {
            "sha": "08a8b7b315bac8e253467ff376ee4bc065fe5b5b",
            "filename": "src/transformers/models/mistral3/modeling_mistral3.py",
            "status": "modified",
            "additions": 4,
            "deletions": 16,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -203,26 +203,14 @@ class Mistral3PreTrainedModel(PreTrainedModel):\n     _supports_static_cache = True\n \n     def _init_weights(self, module):\n-        # important: this ported version of Mistral3 isn't meant for training from scratch - only\n-        # inference and fine-tuning - so the proper init weights code has been removed - the original codebase\n-        # https://github.com/haotian-liu/Mistral3/tree/main/mistral3 should serve for that purpose\n-        std = (\n-            self.config.initializer_range\n-            if hasattr(self.config, \"initializer_range\")\n-            else self.config.text_config.initializer_range\n-        )\n-\n-        if hasattr(module, \"class_embedding\"):\n-            module.class_embedding.data.normal_(mean=0.0, std=std)\n+        std = getattr(self.config, \"initializer_range\", self.config.get_text_config().initializer_range)\n \n-        if isinstance(module, (nn.Linear, nn.Conv2d)):\n+        if isinstance(module, nn.Linear):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n                 module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, Mistral3RMSNorm):\n+            module.weight.data.fill_(1.0)\n \n \n MISTRAL3_INPUTS_DOCSTRING = r\"\"\""
        },
        {
            "sha": "5eebcd8d560aaaf0599a3800c3e4775cee4cbe5b",
            "filename": "src/transformers/models/mistral3/modular_mistral3.py",
            "status": "modified",
            "additions": 13,
            "deletions": 1,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodular_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodular_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodular_mistral3.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -20,7 +20,7 @@\n \n from ...activations import ACT2FN\n from ...utils import is_torchdynamo_compiling, logging\n-from ..llava.modeling_llava import LlavaCausalLMOutputWithPast, LlavaForConditionalGeneration\n+from ..llava.modeling_llava import LlavaCausalLMOutputWithPast, LlavaForConditionalGeneration, LlavaPreTrainedModel\n from ..mistral.modeling_mistral import MistralRMSNorm\n from .configuration_mistral3 import Mistral3Config\n \n@@ -100,6 +100,18 @@ class Mistral3CausalLMOutputWithPast(LlavaCausalLMOutputWithPast):\n     pass\n \n \n+class Mistral3PreTrainedModel(LlavaPreTrainedModel):\n+    def _init_weights(self, module):\n+        std = getattr(self.config, \"initializer_range\", self.config.get_text_config().initializer_range)\n+\n+        if isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, Mistral3RMSNorm):\n+            module.weight.data.fill_(1.0)\n+\n+\n class Mistral3ForConditionalGeneration(LlavaForConditionalGeneration):\n     def get_image_features(\n         self,"
        },
        {
            "sha": "001692bd754b3346153fd066d21c361319069894",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -474,6 +474,8 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, MixtralRMSNorm):\n+            module.weight.data.fill_(1.0)\n \n \n MIXTRAL_INPUTS_DOCSTRING = r\"\"\""
        },
        {
            "sha": "a2911a46a07ab3f1dc246a1dc3f4944f59f73d95",
            "filename": "src/transformers/models/mllama/modeling_mllama.py",
            "status": "modified",
            "additions": 14,
            "deletions": 3,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -1029,7 +1029,8 @@ class MllamaPreTrainedModel(PreTrainedModel):\n     _supports_quantized_cache = True\n \n     def _init_weights(self, module):\n-        std = self.config.get_text_config().initializer_range\n+        std = getattr(self.config, \"initializer_range\", self.config.get_text_config().initializer_range)\n+\n         if isinstance(module, (nn.Linear, nn.Conv2d)):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n@@ -1038,15 +1039,25 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, nn.Parameter):\n-            module.data.normal_(mean=0.0, std=std)\n+        elif isinstance(module, nn.LayerNorm):\n+            module.weight.data.fill_(1.0)\n+            module.bias.data.zero_()\n+        elif isinstance(module, MllamaTextRMSNorm):\n+            module.weight.data.fill_(1.0)\n         elif isinstance(module, MllamaVisionModel):\n             nn.init.normal_(module.class_embedding.data, std=std)\n         elif isinstance(module, MllamaPrecomputedPositionEmbedding):\n             nn.init.normal_(module.embedding.data, std=std)\n+            nn.init.zeros_(module.gate.data)\n         elif isinstance(module, MllamaVisionEncoderLayer) and module.is_gated:\n             nn.init.normal_(module.gate_attn.data, std=std)\n             nn.init.normal_(module.gate_ffn.data, std=std)\n+        elif isinstance(module, MllamaCrossAttentionDecoderLayer):\n+            module.cross_attn_attn_gate.data.zero_()\n+            module.cross_attn_mlp_gate.data.zero_()\n+        elif isinstance(module, MllamaPrecomputedAspectRatioEmbedding):\n+            if module.is_gated:\n+                module.gate.data.zero_()\n \n     # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n     def _update_causal_mask("
        },
        {
            "sha": "aaeb405a0e088040ab819c5eef00a20ec9f10150",
            "filename": "src/transformers/models/moonshine/modeling_moonshine.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -536,6 +536,10 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n                 module.bias.data.zero_()\n+        elif isinstance(module, (nn.GroupNorm, nn.LayerNorm)):\n+            module.weight.data.fill_(1.0)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n         elif isinstance(module, nn.Embedding):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:"
        },
        {
            "sha": "d76941c8d6f065ebeb7e1561eacf6d1cf249bfb6",
            "filename": "src/transformers/models/moonshine/modular_moonshine.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -554,6 +554,10 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n                 module.bias.data.zero_()\n+        elif isinstance(module, (nn.GroupNorm, nn.LayerNorm)):\n+            module.weight.data.fill_(1.0)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n         elif isinstance(module, nn.Embedding):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:"
        },
        {
            "sha": "dc94efd35358a8c410c739b1769038fe45c8f9fb",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 6,
            "deletions": 9,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -849,22 +849,19 @@ class MoshiPreTrainedModel(PreTrainedModel):\n \n     def _init_weights(self, module):\n         std = self.config.initializer_range\n-        if isinstance(module, (nn.Linear, nn.Conv1d)):\n+\n+        if isinstance(module, nn.Linear):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n                 module.bias.data.zero_()\n-        elif isinstance(module, (nn.LayerNorm, nn.GroupNorm)):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n-        elif isinstance(module, nn.Conv1d):\n-            nn.init.kaiming_normal_(module.weight)\n-            if module.bias is not None:\n-                k = math.sqrt(module.groups / (module.in_channels * module.kernel_size[0]))\n-                nn.init.uniform_(module.bias, a=-k, b=k)\n+        elif isinstance(module, MoshiFlexibleLinear):\n+            module.weight.data.normal_()\n         elif isinstance(module, nn.Embedding):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, MoshiRMSNorm):\n+            module.weight.data.fill_(1.0)\n \n \n MOSHI_START_DOCSTRING = r\"\"\""
        },
        {
            "sha": "a17833912f1ef18c03c4831c0da3d164367d28b6",
            "filename": "src/transformers/models/nemotron/modeling_nemotron.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -623,6 +623,9 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, NemotronLayerNorm1P):\n+            module.weight.data.fill_(1.0)\n+            module.bias.data.zero_()\n \n \n NEMOTRON_INPUTS_DOCSTRING = r\"\"\""
        },
        {
            "sha": "bf5e80b839bbfc7d1521765b450af886d297d7f8",
            "filename": "src/transformers/models/olmo/modeling_olmo.py",
            "status": "modified",
            "additions": 34,
            "deletions": 34,
            "changes": 68,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -282,6 +282,40 @@ def forward(\n         return outputs\n \n \n+class OlmoRotaryEmbedding(nn.Module):\n+    def __init__(self, config: OlmoConfig, device=None):\n+        super().__init__()\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+        else:\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+    @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n+    def forward(self, x, position_ids):\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n OLMO_START_DOCSTRING = r\"\"\"\n     This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n     library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n@@ -329,40 +363,6 @@ def _init_weights(self, module):\n                 module.weight.data[module.padding_idx].zero_()\n \n \n-class OlmoRotaryEmbedding(nn.Module):\n-    def __init__(self, config: OlmoConfig, device=None):\n-        super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n-        self.max_seq_len_cached = config.max_position_embeddings\n-        self.original_max_seq_len = config.max_position_embeddings\n-\n-        self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n-\n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n-        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n-\n-    @torch.no_grad()\n-    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n-    def forward(self, x, position_ids):\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n-        position_ids_expanded = position_ids[:, None, :].float()\n-\n-        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n-            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos() * self.attention_scaling\n-            sin = emb.sin() * self.attention_scaling\n-\n-        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n-\n-\n OLMO_INPUTS_DOCSTRING = r\"\"\"\n     Args:\n         input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):"
        },
        {
            "sha": "a2af171557a659d9d5aace40927a724174a5fc3d",
            "filename": "src/transformers/models/olmo/modular_olmo.py",
            "status": "modified",
            "additions": 15,
            "deletions": 1,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Folmo%2Fmodular_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Folmo%2Fmodular_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodular_olmo.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -15,6 +15,7 @@\n     LlamaMLP,\n     LlamaModel,\n     LlamaPreTrainedModel,\n+    LlamaRotaryEmbedding,\n     apply_rotary_pos_emb,\n     eager_attention_forward,\n )\n@@ -114,10 +115,23 @@ def __init__(self, config: OlmoConfig, layer_idx: int):\n         self.self_attn = OlmoAttention(config=config, layer_idx=layer_idx)\n \n \n-class OlmoPreTrainedModel(LlamaPreTrainedModel):\n+class OlmoRotaryEmbedding(LlamaRotaryEmbedding):\n     pass\n \n \n+class OlmoPreTrainedModel(LlamaPreTrainedModel):\n+    def _init_weights(self, module):\n+        std = self.config.initializer_range\n+        if isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+\n+\n class OlmoModel(LlamaModel):\n     def __init__(self, config: OlmoConfig):\n         super().__init__(config)"
        },
        {
            "sha": "e44ea5f62b6a32c287029aeefbb3b95f4051b941",
            "filename": "src/transformers/models/olmo2/modeling_olmo2.py",
            "status": "modified",
            "additions": 36,
            "deletions": 34,
            "changes": 70,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -286,6 +286,40 @@ def forward(\n         return outputs\n \n \n+class Olmo2RotaryEmbedding(nn.Module):\n+    def __init__(self, config: Olmo2Config, device=None):\n+        super().__init__()\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+        else:\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+    @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n+    def forward(self, x, position_ids):\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n OLMO2_START_DOCSTRING = r\"\"\"\n     This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n     library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n@@ -331,40 +365,8 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n-\n-\n-class Olmo2RotaryEmbedding(nn.Module):\n-    def __init__(self, config: Olmo2Config, device=None):\n-        super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n-        self.max_seq_len_cached = config.max_position_embeddings\n-        self.original_max_seq_len = config.max_position_embeddings\n-\n-        self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n-\n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n-        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n-\n-    @torch.no_grad()\n-    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n-    def forward(self, x, position_ids):\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n-        position_ids_expanded = position_ids[:, None, :].float()\n-\n-        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n-            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos() * self.attention_scaling\n-            sin = emb.sin() * self.attention_scaling\n-\n-        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+        elif isinstance(module, Olmo2RMSNorm):\n+            module.weight.data.fill_(1.0)\n \n \n OLMO2_INPUTS_DOCSTRING = r\"\"\""
        },
        {
            "sha": "c43263e9545e2e1bc9cd78372770be011d1771cd",
            "filename": "src/transformers/models/olmo2/modular_olmo2.py",
            "status": "modified",
            "additions": 10,
            "deletions": 1,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodular_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodular_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodular_olmo2.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -7,13 +7,14 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...pytorch_utils import ALL_LAYERNORM_LAYERS\n from ...utils import logging\n-from ..llama.modeling_llama import LlamaRMSNorm, eager_attention_forward\n+from ..llama.modeling_llama import LlamaPreTrainedModel, LlamaRMSNorm, eager_attention_forward\n from ..olmo.configuration_olmo import OlmoConfig\n from ..olmo.modeling_olmo import (\n     OlmoAttention,\n     OlmoDecoderLayer,\n     OlmoForCausalLM,\n     OlmoModel,\n+    OlmoRotaryEmbedding,\n     apply_rotary_pos_emb,\n )\n \n@@ -287,6 +288,14 @@ def forward(\n         return outputs\n \n \n+class Olmo2RotaryEmbedding(OlmoRotaryEmbedding):\n+    pass\n+\n+\n+class Olmo2PreTrainedModel(LlamaPreTrainedModel):\n+    pass\n+\n+\n # The OLMo2 model is identical to the OLMo model, except RMSNorm is used instead of\n # standard layer norm for the output norm.\n class Olmo2Model(OlmoModel):"
        },
        {
            "sha": "007da568f053f95f298b21a95dcbcfdca892f743",
            "filename": "src/transformers/models/olmoe/modeling_olmoe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -747,6 +747,8 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n                 module.bias.data.zero_()\n+        elif isinstance(module, OlmoeRMSNorm):\n+            module.weight.data.fill_(1.0)\n         elif isinstance(module, nn.Embedding):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:"
        },
        {
            "sha": "d0175055cc92922a902b546b0e36ec63c1dc9b3b",
            "filename": "src/transformers/models/opt/modeling_opt.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -511,6 +511,9 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.weight.data.fill_(1.0)\n+            module.bias.data.zero_()\n \n \n OPT_INPUTS_DOCSTRING = r\"\"\""
        },
        {
            "sha": "f13bd0feefbf0710bd4600c47220d362ed104e11",
            "filename": "src/transformers/models/paligemma/modeling_paligemma.py",
            "status": "modified",
            "additions": 2,
            "deletions": 13,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -199,23 +199,12 @@ class PaliGemmaPreTrainedModel(PreTrainedModel):\n     def _init_weights(self, module):\n         # important: this ported version of PaliGemmaisn't meant for training from scratch - only\n         # inference and fine-tuning\n-        std = (\n-            self.config.initializer_range\n-            if hasattr(self.config, \"initializer_range\")\n-            else self.config.text_config.initializer_range\n-        )\n-\n-        if hasattr(module, \"class_embedding\"):\n-            module.class_embedding.data.normal_(mean=0.0, std=std)\n+        std = getattr(self.config, \"initializer_range\", self.config.get_text_config().initializer_range)\n \n-        if isinstance(module, (nn.Linear, nn.Conv2d)):\n+        if isinstance(module, nn.Linear):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n                 module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n \n \n PALIGEMMA_INPUTS_DOCSTRING = r\"\"\""
        },
        {
            "sha": "da8a8d29270af7e871c04382c982cd82e4f88ba4",
            "filename": "src/transformers/models/persimmon/modeling_persimmon.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -412,6 +412,9 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.weight.data.fill_(1.0)\n+            module.bias.data.zero_()\n \n \n PERSIMMON_INPUTS_DOCSTRING = r\"\"\""
        },
        {
            "sha": "8572b1546a8d6267dada203ef3826aadda2df17f",
            "filename": "src/transformers/models/phi/modeling_phi.py",
            "status": "modified",
            "additions": 37,
            "deletions": 34,
            "changes": 71,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -279,6 +279,40 @@ def forward(\n         return outputs\n \n \n+class PhiRotaryEmbedding(nn.Module):\n+    def __init__(self, config: PhiConfig, device=None):\n+        super().__init__()\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+        else:\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+    @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n+    def forward(self, x, position_ids):\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n PHI_START_DOCSTRING = r\"\"\"\n     This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n     library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n@@ -324,40 +358,9 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n-\n-\n-class PhiRotaryEmbedding(nn.Module):\n-    def __init__(self, config: PhiConfig, device=None):\n-        super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n-        self.max_seq_len_cached = config.max_position_embeddings\n-        self.original_max_seq_len = config.max_position_embeddings\n-\n-        self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n-\n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n-        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n-\n-    @torch.no_grad()\n-    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n-    def forward(self, x, position_ids):\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n-        position_ids_expanded = position_ids[:, None, :].float()\n-\n-        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n-            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos() * self.attention_scaling\n-            sin = emb.sin() * self.attention_scaling\n-\n-        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+        elif isinstance(module, nn.LayerNorm):\n+            module.weight.data.fill_(1.0)\n+            module.bias.data.zero_()\n \n \n PHI_INPUTS_DOCSTRING = r\"\"\""
        },
        {
            "sha": "5faee931e0a26e84876039f68e8c2da5890aff82",
            "filename": "src/transformers/models/phi/modular_phi.py",
            "status": "modified",
            "additions": 18,
            "deletions": 1,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fphi%2Fmodular_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fphi%2Fmodular_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodular_phi.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -20,6 +20,7 @@\n     LlamaForTokenClassification,\n     LlamaModel,\n     LlamaPreTrainedModel,\n+    LlamaRotaryEmbedding,\n     apply_rotary_pos_emb,\n     eager_attention_forward,  # copied from Llama\n )\n@@ -170,10 +171,26 @@ def forward(\n         return outputs\n \n \n-class PhiPreTrainedModel(LlamaPreTrainedModel):\n+class PhiRotaryEmbedding(LlamaRotaryEmbedding):\n     pass\n \n \n+class PhiPreTrainedModel(LlamaPreTrainedModel):\n+    def _init_weights(self, module):\n+        std = self.config.initializer_range\n+        if isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.weight.data.fill_(1.0)\n+            module.bias.data.zero_()\n+\n+\n class PhiModel(LlamaModel):\n     def __init__(self, config: PhiConfig):\n         super().__init__(config)"
        },
        {
            "sha": "1737d8c3df0697c4f7410b5f34d494cdec85a5db",
            "filename": "src/transformers/models/phi3/modeling_phi3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -373,6 +373,8 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, Phi3RMSNorm):\n+            module.weight.data.fill_(1.0)\n \n \n class Phi3RotaryEmbedding(nn.Module):"
        },
        {
            "sha": "8677ef9dc93d6fdf5d5ec1005f64f67511448c1a",
            "filename": "src/transformers/models/phi4_multimodal/modeling_phi4_multimodal.py",
            "status": "modified",
            "additions": 42,
            "deletions": 34,
            "changes": 76,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -1030,6 +1030,9 @@ def _init_weights(self, module):\n         elif isinstance(module, nn.LayerNorm):\n             module.bias.data.zero_()\n             module.weight.data.fill_(1.0)\n+        elif isinstance(module, Phi4MultimodalAudioGluPointWiseConv):\n+            module.b1.data.zero_()\n+            module.b2.data.zero_()\n \n \n def unfold_tensor(tensor, max_seq_len):\n@@ -1607,6 +1610,40 @@ def forward(\n         return inputs_embeds\n \n \n+class Phi4MultimodalRotaryEmbedding(nn.Module):\n+    def __init__(self, config: Phi4MultimodalConfig, device=None):\n+        super().__init__()\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+        else:\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+    @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n+    def forward(self, x, position_ids):\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n PHI4_MULTIMODAL_START_DOCSTRING = r\"\"\"\n     This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n     library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n@@ -1653,40 +1690,11 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n-\n-\n-class Phi4MultimodalRotaryEmbedding(nn.Module):\n-    def __init__(self, config: Phi4MultimodalConfig, device=None):\n-        super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n-        self.max_seq_len_cached = config.max_position_embeddings\n-        self.original_max_seq_len = config.max_position_embeddings\n-\n-        self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n-\n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n-        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n-\n-    @torch.no_grad()\n-    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n-    def forward(self, x, position_ids):\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n-        position_ids_expanded = position_ids[:, None, :].float()\n-\n-        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n-            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos() * self.attention_scaling\n-            sin = emb.sin() * self.attention_scaling\n-\n-        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+        elif isinstance(module, Phi4MultimodalRMSNorm):\n+            module.weight.data.fill_(1.0)\n+        elif isinstance(module, Phi4MultimodalImageEmbedding):\n+            module.global_img_feature_extensor.data.zero_()\n+            module.sub_img_feature_extensor.data.zero_()\n \n \n PHI4_MULTIMODAL_MODEL_INPUTS_DOCSTRING = r\"\"\""
        },
        {
            "sha": "d269b06037e9e67bd8aa69169dfa70ab75fe30c5",
            "filename": "src/transformers/models/phi4_multimodal/modular_phi4_multimodal.py",
            "status": "modified",
            "additions": 34,
            "deletions": 2,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -40,7 +40,14 @@\n     replace_return_docstrings,\n )\n from ..phi3.configuration_phi3 import Phi3Config\n-from ..phi3.modeling_phi3 import Phi3DecoderLayer, Phi3ForCausalLM, Phi3Model, Phi3RMSNorm\n+from ..phi3.modeling_phi3 import (\n+    Phi3DecoderLayer,\n+    Phi3ForCausalLM,\n+    Phi3Model,\n+    Phi3PreTrainedModel,\n+    Phi3RMSNorm,\n+    Phi3RotaryEmbedding,\n+)\n from ..siglip.configuration_siglip import SiglipVisionConfig\n from ..siglip.modeling_siglip import (\n     SiglipEncoder,\n@@ -1133,6 +1140,9 @@ def _init_weights(self, module):\n         elif isinstance(module, nn.LayerNorm):\n             module.bias.data.zero_()\n             module.weight.data.fill_(1.0)\n+        elif isinstance(module, Phi4MultimodalAudioGluPointWiseConv):\n+            module.b1.data.zero_()\n+            module.b2.data.zero_()\n \n \n class Phi4MultimodalAudioModel(Phi4MultimodalAudioPreTrainedModel):\n@@ -1519,6 +1529,28 @@ def forward(\n \"\"\"\n \n \n+class Phi4MultimodalRotaryEmbedding(Phi3RotaryEmbedding):\n+    pass\n+\n+\n+class Phi4MultimodalPreTrainedModel(Phi3PreTrainedModel):\n+    def _init_weights(self, module):\n+        std = self.config.initializer_range\n+        if isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, Phi4MultimodalRMSNorm):\n+            module.weight.data.fill_(1.0)\n+        elif isinstance(module, Phi4MultimodalImageEmbedding):\n+            module.global_img_feature_extensor.data.zero_()\n+            module.sub_img_feature_extensor.data.zero_()\n+\n+\n class Phi4MultimodalModel(Phi3Model, nn.Module):\n     \"\"\"\n     Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`Phi4MultimodalMMDecoderLayer`]\n@@ -1829,7 +1861,7 @@ def prepare_inputs_for_generation(\n     \"Phi4MultimodalAudioModel\",\n     \"Phi4MultimodalVisionPreTrainedModel\",\n     \"Phi4MultimodalVisionModel\",\n-    \"Phi4MultimodalPreTrainedModel\",  # noqa\n+    \"Phi4MultimodalPreTrainedModel\",\n     \"Phi4MultimodalModel\",\n     \"Phi4MultimodalForCausalLM\",\n     \"Phi4MultimodalVisionConfig\","
        },
        {
            "sha": "ab8370d8ec58bc5f2f35bcb507d0b37daae4ac5f",
            "filename": "src/transformers/models/phimoe/modeling_phimoe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -923,6 +923,9 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.bias.data.zero_()\n+            module.weight.data.fill_(1.0)\n \n \n PHIMOE_INPUTS_DOCSTRING = r\"\"\""
        },
        {
            "sha": "bc0ec918e997adaf3252c9b6fdc70eff1dff7ef6",
            "filename": "src/transformers/models/pixtral/modeling_pixtral.py",
            "status": "modified",
            "additions": 3,
            "deletions": 10,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -383,20 +383,13 @@ class PixtralPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"PixtralAttentionLayer\"]\n \n     def _init_weights(self, module):\n-        std = (\n-            self.config.initializer_range\n-            if hasattr(self.config, \"initializer_range\")\n-            else self.config.initializer_range\n-        )\n-\n+        std = self.config.initializer_range\n         if isinstance(module, (nn.Linear, nn.Conv2d)):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n                 module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, PixtralRMSNorm):\n+            module.weight.data.fill_(1.0)\n \n \n PIXTRAL_INPUTS_DOCSTRING = r\"\"\""
        },
        {
            "sha": "7a1e2c7e47fb96295974673d6c2598a462522a5f",
            "filename": "src/transformers/models/prompt_depth_anything/modeling_prompt_depth_anything.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fmodeling_prompt_depth_anything.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fmodeling_prompt_depth_anything.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fmodeling_prompt_depth_anything.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -254,15 +254,10 @@ class PromptDepthAnythingPreTrainedModel(PreTrainedModel):\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, (nn.Linear, nn.Conv2d, nn.ConvTranspose2d)):\n-            # Slightly different from the TF version which uses truncated_normal for initialization\n-            # cf https://github.com/pytorch/pytorch/pull/5617\n+        if isinstance(module, (nn.Conv2d, nn.ConvTranspose2d)):\n             module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n                 module.bias.data.zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n \n \n class PromptDepthAnythingReassembleLayer(nn.Module):"
        },
        {
            "sha": "aa834339ea9cf2fc916eb69a401bd4ceee80a056",
            "filename": "src/transformers/models/prompt_depth_anything/modular_prompt_depth_anything.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fmodular_prompt_depth_anything.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fmodular_prompt_depth_anything.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fmodular_prompt_depth_anything.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -210,15 +210,10 @@ class PromptDepthAnythingPreTrainedModel(PreTrainedModel):\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, (nn.Linear, nn.Conv2d, nn.ConvTranspose2d)):\n-            # Slightly different from the TF version which uses truncated_normal for initialization\n-            # cf https://github.com/pytorch/pytorch/pull/5617\n+        if isinstance(module, (nn.Conv2d, nn.ConvTranspose2d)):\n             module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n                 module.bias.data.zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n \n \n class PromptDepthAnythingReassembleLayer(nn.Module):"
        },
        {
            "sha": "147f6546527814029d7692f31be93653f339cdde",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -331,6 +331,8 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, Qwen2RMSNorm):\n+            module.weight.data.fill_(1.0)\n \n \n class Qwen2RotaryEmbedding(nn.Module):"
        },
        {
            "sha": "d079846708c773a3b3dda4d59f802f38356a1f8c",
            "filename": "src/transformers/models/qwen2_5_omni/configuration_qwen2_5_omni.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fconfiguration_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fconfiguration_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fconfiguration_qwen2_5_omni.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -92,6 +92,7 @@ def __init__(\n         window_size=112,\n         out_hidden_size=3584,\n         fullatt_block_indexes=[7, 15, 23, 31],\n+        initializer_range=0.02,\n         **kwargs,\n     ):\n         super().__init__(**kwargs)\n@@ -108,6 +109,7 @@ def __init__(\n         self.window_size = window_size\n         self.fullatt_block_indexes = fullatt_block_indexes\n         self.out_hidden_size = out_hidden_size\n+        self.initializer_range = initializer_range\n \n \n class Qwen2_5OmniAudioEncoderConfig(PretrainedConfig):"
        },
        {
            "sha": "4c71c431acd62b613876ffeac942b9f2197b0262",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 26,
            "deletions": 21,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -75,6 +75,26 @@\n logger = logging.get_logger(__name__)\n \n \n+class Qwen2RMSNorm(nn.Module):\n+    def __init__(self, hidden_size, eps=1e-6):\n+        \"\"\"\n+        Qwen2RMSNorm is equivalent to T5LayerNorm\n+        \"\"\"\n+        super().__init__()\n+        self.weight = nn.Parameter(torch.ones(hidden_size))\n+        self.variance_epsilon = eps\n+\n+    def forward(self, hidden_states):\n+        input_dtype = hidden_states.dtype\n+        hidden_states = hidden_states.to(torch.float32)\n+        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n+        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n+        return self.weight * hidden_states.to(input_dtype)\n+\n+    def extra_repr(self):\n+        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n+\n+\n Qwen2_5Omni_START_DOCSTRING = r\"\"\"\n     This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n     library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n@@ -112,14 +132,19 @@ def _init_weights(self, module):\n         # inference and fine-tuning - so the proper init weights code has been removed\n         std = self.config.initializer_range if hasattr(self.config, \"initializer_range\") else 0.02\n \n-        if isinstance(module, (nn.Linear, nn.Conv1d, nn.Conv3d)):\n+        if isinstance(module, (nn.Linear, nn.Conv1d, nn.Conv3d, nn.ConvTranspose1d)):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n                 module.bias.data.zero_()\n         elif isinstance(module, nn.Embedding):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.weight.data.fill_(1.0)\n+            module.bias.data.zero_()\n+        elif isinstance(module, Qwen2RMSNorm):\n+            module.weight.data.fill_(1.0)\n \n \n class Qwen2_5OmniPreTrainedModelForConditionalGeneration(Qwen2_5OmniPreTrainedModel):\n@@ -1102,26 +1127,6 @@ def forward(self, hidden_state):\n         return self.down_proj(self.act_fn(self.gate_proj(hidden_state)) * self.up_proj(hidden_state))\n \n \n-class Qwen2RMSNorm(nn.Module):\n-    def __init__(self, hidden_size, eps=1e-6):\n-        \"\"\"\n-        Qwen2RMSNorm is equivalent to T5LayerNorm\n-        \"\"\"\n-        super().__init__()\n-        self.weight = nn.Parameter(torch.ones(hidden_size))\n-        self.variance_epsilon = eps\n-\n-    def forward(self, hidden_states):\n-        input_dtype = hidden_states.dtype\n-        hidden_states = hidden_states.to(torch.float32)\n-        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n-        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n-        return self.weight * hidden_states.to(input_dtype)\n-\n-    def extra_repr(self):\n-        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n-\n-\n QWEN2_5_OMNI_VISION_ATTENTION_CLASSES = {\n     \"eager\": Qwen2_5OmniVisionAttention,\n     \"flash_attention_2\": Qwen2_5OmniVisionFlashAttention2,"
        },
        {
            "sha": "9bb81ddcc59a5c72cefc8c2cf84e87eb4f2100f3",
            "filename": "src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py",
            "status": "modified",
            "additions": 9,
            "deletions": 1,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -36,6 +36,7 @@\n     Qwen2_5_VLModel,\n     Qwen2_5_VLPreTrainedModel,\n     Qwen2_5_VLVisionBlock,\n+    Qwen2RMSNorm,\n )\n from transformers.models.qwen2_audio.configuration_qwen2_audio import Qwen2AudioEncoderConfig\n from transformers.models.qwen2_audio.modeling_qwen2_audio import Qwen2AudioEncoderLayer\n@@ -130,6 +131,7 @@ def __init__(\n         window_size=112,\n         out_hidden_size=3584,\n         fullatt_block_indexes=[7, 15, 23, 31],\n+        initializer_range=0.02,\n         **kwargs,\n     ):\n         super().__init__(\n@@ -145,6 +147,7 @@ def __init__(\n             window_size,\n             out_hidden_size,\n             fullatt_block_indexes,\n+            initializer_range=initializer_range,\n             **kwargs,\n         )\n         del self.tokens_per_second\n@@ -1027,14 +1030,19 @@ def _init_weights(self, module):\n         # inference and fine-tuning - so the proper init weights code has been removed\n         std = self.config.initializer_range if hasattr(self.config, \"initializer_range\") else 0.02\n \n-        if isinstance(module, (nn.Linear, nn.Conv1d, nn.Conv3d)):\n+        if isinstance(module, (nn.Linear, nn.Conv1d, nn.Conv3d, nn.ConvTranspose1d)):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n                 module.bias.data.zero_()\n         elif isinstance(module, nn.Embedding):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.weight.data.fill_(1.0)\n+            module.bias.data.zero_()\n+        elif isinstance(module, Qwen2RMSNorm):\n+            module.weight.data.fill_(1.0)\n \n \n class Qwen2_5OmniPreTrainedModelForConditionalGeneration(Qwen2_5OmniPreTrainedModel):"
        },
        {
            "sha": "63ca1c23592f1e4b1d293ecdc55c1afb9d223a64",
            "filename": "src/transformers/models/qwen2_5_vl/configuration_qwen2_5_vl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fconfiguration_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fconfiguration_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fconfiguration_qwen2_5_vl.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -46,6 +46,7 @@ def __init__(\n         window_size=112,\n         out_hidden_size=3584,\n         fullatt_block_indexes=[7, 15, 23, 31],\n+        initializer_range=0.02,\n         **kwargs,\n     ):\n         super().__init__(**kwargs)\n@@ -63,6 +64,7 @@ def __init__(\n         self.window_size = window_size\n         self.fullatt_block_indexes = fullatt_block_indexes\n         self.out_hidden_size = out_hidden_size\n+        self.initializer_range = initializer_range\n \n \n class Qwen2_5_VLConfig(PretrainedConfig):"
        },
        {
            "sha": "5f0a9d003f74483e9d71c504e0c72a8a50ce5108",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -388,6 +388,8 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, Qwen2RMSNorm):\n+            module.weight.data.fill_(1.0)\n \n \n class Qwen2_5_VisionTransformerPretrainedModel(Qwen2_5_VLPreTrainedModel):"
        },
        {
            "sha": "fa245c45f5a46ef9eed2750fcfd80bf11ca35daf",
            "filename": "src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py",
            "status": "modified",
            "additions": 14,
            "deletions": 1,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -89,6 +89,7 @@ def __init__(\n         window_size=112,\n         out_hidden_size=3584,\n         fullatt_block_indexes=[7, 15, 23, 31],\n+        initializer_range=0.02,\n         **kwargs,\n     ):\n         super().__init__(**kwargs)\n@@ -106,6 +107,7 @@ def __init__(\n         self.window_size = window_size\n         self.fullatt_block_indexes = fullatt_block_indexes\n         self.out_hidden_size = out_hidden_size\n+        self.initializer_range = initializer_range\n \n \n class Qwen2_5_VLConfig(Qwen2VLConfig):\n@@ -224,7 +226,18 @@ def forward(\n \n \n class Qwen2_5_VLPreTrainedModel(Qwen2VLPreTrainedModel):\n-    pass\n+    def _init_weights(self, module):\n+        std = self.config.initializer_range\n+        if isinstance(module, (nn.Linear, nn.Conv3d)):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, Qwen2RMSNorm):\n+            module.weight.data.fill_(1.0)\n \n \n class Qwen2_5_VisionTransformerPretrainedModel(Qwen2_5_VLPreTrainedModel):"
        },
        {
            "sha": "6c9cc40ad86b54ecb1afcf8267fc342f93970bcf",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -779,6 +779,8 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, Qwen2MoeRMSNorm):\n+            module.weight.data.fill_(1.0)\n \n \n QWEN2MOE_INPUTS_DOCSTRING = r\"\"\""
        },
        {
            "sha": "b03dbc8f0b670dc0819ae77516cb046ea669d705",
            "filename": "src/transformers/models/qwen2_vl/configuration_qwen2_vl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fconfiguration_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fconfiguration_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fconfiguration_qwen2_vl.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -38,6 +38,7 @@ def __init__(\n         patch_size=14,\n         spatial_merge_size=2,\n         temporal_patch_size=2,\n+        initializer_range=0.02,\n         **kwargs,\n     ):\n         super().__init__(**kwargs)\n@@ -52,6 +53,7 @@ def __init__(\n         self.patch_size = patch_size\n         self.spatial_merge_size = spatial_merge_size\n         self.temporal_patch_size = temporal_patch_size\n+        self.initializer_range = initializer_range\n \n \n class Qwen2VLConfig(PretrainedConfig):"
        },
        {
            "sha": "e172f092d7ca744a025c983d33f57dcd3eafe3a5",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -914,6 +914,7 @@ class Qwen2VLPreTrainedModel(PreTrainedModel):\n \n     def _init_weights(self, module):\n         std = self.config.initializer_range\n+\n         if isinstance(module, (nn.Linear, nn.Conv3d)):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n@@ -922,6 +923,11 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.weight.data.fill_(1.0)\n+            module.bias.data.zero_()\n+        elif isinstance(module, Qwen2RMSNorm):\n+            module.weight.data.fill_(1.0)\n \n \n class Qwen2VisionTransformerPretrainedModel(Qwen2VLPreTrainedModel):"
        },
        {
            "sha": "89f30e78f426ad9801be1a9a356b444af0404ddc",
            "filename": "src/transformers/models/qwen3/modeling_qwen3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -358,6 +358,8 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, Qwen3RMSNorm):\n+            module.weight.data.fill_(1.0)\n \n \n class Qwen3RotaryEmbedding(nn.Module):"
        },
        {
            "sha": "3462f565e216af31ee4567461afaf86a8baa26bf",
            "filename": "src/transformers/models/qwen3_moe/modeling_qwen3_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -488,6 +488,8 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, Qwen3MoeRMSNorm):\n+            module.weight.data.fill_(1.0)\n \n \n QWEN3_MOE_INPUTS_DOCSTRING = r\"\"\""
        },
        {
            "sha": "2e2afdbc015a0405de3a7be9700d051831839c2f",
            "filename": "src/transformers/models/recurrent_gemma/modeling_recurrent_gemma.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -581,6 +581,13 @@ def _init_weights(self, module):\n             torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n             if getattr(module, \"bias\", None) is not None:\n                 torch.nn.init.zeros_(module.bias)\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+\n+        elif isinstance(module, RecurrentGemmaRMSNorm):\n+            module.weight.data.fill_(1.0)\n \n     def _setup_cache(self, config, batch, device, dtype):\n         layers = getattr(self, \"model\", self).layers"
        },
        {
            "sha": "6ecf72d0a12cea00a552443316e9f4b48d310590",
            "filename": "src/transformers/models/rt_detr/modeling_rt_detr.py",
            "status": "modified",
            "additions": 7,
            "deletions": 5,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -1040,8 +1040,6 @@ class RTDetrPreTrainedModel(PreTrainedModel):\n \n     def _init_weights(self, module):\n         \"\"\"Initalize the weights\"\"\"\n-\n-        \"\"\"initialize linear layer bias value according to a given probability value.\"\"\"\n         if isinstance(module, (RTDetrForObjectDetection, RTDetrDecoder)):\n             if module.class_embed is not None:\n                 for layer in module.class_embed:\n@@ -1055,7 +1053,7 @@ def _init_weights(self, module):\n                     nn.init.constant_(layer.layers[-1].weight, 0)\n                     nn.init.constant_(layer.layers[-1].bias, 0)\n \n-        if isinstance(module, RTDetrMultiscaleDeformableAttention):\n+        elif isinstance(module, RTDetrMultiscaleDeformableAttention):\n             nn.init.constant_(module.sampling_offsets.weight.data, 0.0)\n             default_dtype = torch.get_default_dtype()\n             thetas = torch.arange(module.n_heads, dtype=torch.int64).to(default_dtype) * (\n@@ -1078,17 +1076,21 @@ def _init_weights(self, module):\n             nn.init.xavier_uniform_(module.output_proj.weight.data)\n             nn.init.constant_(module.output_proj.bias.data, 0.0)\n \n-        if isinstance(module, RTDetrModel):\n+        elif isinstance(module, RTDetrModel):\n             prior_prob = self.config.initializer_bias_prior_prob or 1 / (self.config.num_labels + 1)\n             bias = float(-math.log((1 - prior_prob) / prior_prob))\n             nn.init.xavier_uniform_(module.enc_score_head.weight)\n             nn.init.constant_(module.enc_score_head.bias, bias)\n \n-        if isinstance(module, (nn.Linear, nn.Conv2d, nn.BatchNorm2d)):\n+        elif isinstance(module, (nn.Linear, nn.Conv2d, nn.BatchNorm2d)):\n             module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n                 module.bias.data.zero_()\n \n+        elif isinstance(module, nn.LayerNorm):\n+            module.weight.data.fill_(1.0)\n+            module.bias.data.zero_()\n+\n         if hasattr(module, \"weight_embedding\") and self.config.learn_initial_query:\n             nn.init.xavier_uniform_(module.weight_embedding.weight)\n         if hasattr(module, \"denoising_class_embed\") and self.config.num_denoising > 0:"
        },
        {
            "sha": "9505c3fdd35b8fd6035b6d1db6364441b522fa2c",
            "filename": "src/transformers/models/rt_detr_v2/modeling_rt_detr_v2.py",
            "status": "modified",
            "additions": 7,
            "deletions": 5,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodeling_rt_detr_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodeling_rt_detr_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodeling_rt_detr_v2.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -1314,8 +1314,6 @@ class RTDetrV2PreTrainedModel(PreTrainedModel):\n \n     def _init_weights(self, module):\n         \"\"\"Initalize the weights\"\"\"\n-\n-        \"\"\"initialize linear layer bias value according to a given probability value.\"\"\"\n         if isinstance(module, (RTDetrV2ForObjectDetection, RTDetrV2Decoder)):\n             if module.class_embed is not None:\n                 for layer in module.class_embed:\n@@ -1329,7 +1327,7 @@ def _init_weights(self, module):\n                     nn.init.constant_(layer.layers[-1].weight, 0)\n                     nn.init.constant_(layer.layers[-1].bias, 0)\n \n-        if isinstance(module, RTDetrV2MultiscaleDeformableAttention):\n+        elif isinstance(module, RTDetrV2MultiscaleDeformableAttention):\n             nn.init.constant_(module.sampling_offsets.weight.data, 0.0)\n             default_dtype = torch.get_default_dtype()\n             thetas = torch.arange(module.n_heads, dtype=torch.int64).to(default_dtype) * (\n@@ -1352,17 +1350,21 @@ def _init_weights(self, module):\n             nn.init.xavier_uniform_(module.output_proj.weight.data)\n             nn.init.constant_(module.output_proj.bias.data, 0.0)\n \n-        if isinstance(module, RTDetrV2Model):\n+        elif isinstance(module, RTDetrV2Model):\n             prior_prob = self.config.initializer_bias_prior_prob or 1 / (self.config.num_labels + 1)\n             bias = float(-math.log((1 - prior_prob) / prior_prob))\n             nn.init.xavier_uniform_(module.enc_score_head.weight)\n             nn.init.constant_(module.enc_score_head.bias, bias)\n \n-        if isinstance(module, (nn.Linear, nn.Conv2d, nn.BatchNorm2d)):\n+        elif isinstance(module, (nn.Linear, nn.Conv2d, nn.BatchNorm2d)):\n             module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n                 module.bias.data.zero_()\n \n+        elif isinstance(module, nn.LayerNorm):\n+            module.weight.data.fill_(1.0)\n+            module.bias.data.zero_()\n+\n         if hasattr(module, \"weight_embedding\") and self.config.learn_initial_query:\n             nn.init.xavier_uniform_(module.weight_embedding.weight)\n         if hasattr(module, \"denoising_class_embed\") and self.config.num_denoising > 0:"
        },
        {
            "sha": "6c33479e9134949703fb9838dbcf669f2b348c79",
            "filename": "src/transformers/models/smolvlm/modeling_smolvlm.py",
            "status": "modified",
            "additions": 4,
            "deletions": 8,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -80,14 +80,7 @@ class SmolVLMPreTrainedModel(PreTrainedModel):\n     _supports_cache_class = True\n \n     def _init_weights(self, module):\n-        std = (\n-            self.config.initializer_range\n-            if hasattr(self.config, \"initializer_range\")\n-            else self.config.get_text_config().initializer_range\n-        )\n-\n-        if hasattr(module, \"class_embedding\"):\n-            module.class_embedding.data.normal_(mean=0.0, std=std)\n+        std = getattr(self.config, \"initializer_range\", self.config.get_text_config().initializer_range)\n \n         if isinstance(module, (nn.Linear, nn.Conv2d)):\n             module.weight.data.normal_(mean=0.0, std=std)\n@@ -97,6 +90,9 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.weight.data.fill_(1.0)\n+            module.bias.data.zero_()\n \n \n class SmolVLMVisionEmbeddings(nn.Module):"
        },
        {
            "sha": "4745fe30dad29b00b233311a71da3edece691820",
            "filename": "src/transformers/models/smolvlm/modular_smolvlm.py",
            "status": "modified",
            "additions": 14,
            "deletions": 1,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodular_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodular_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodular_smolvlm.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -94,7 +94,20 @@ class SmolVLMVisionConfig(Idefics3VisionConfig):\n \n \n class SmolVLMPreTrainedModel(Idefics3PreTrainedModel):\n-    pass\n+    def _init_weights(self, module):\n+        std = getattr(self.config, \"initializer_range\", self.config.get_text_config().initializer_range)\n+\n+        if isinstance(module, (nn.Linear, nn.Conv2d)):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.weight.data.fill_(1.0)\n+            module.bias.data.zero_()\n \n \n class SmolVLMVisionTransformer(Idefics3VisionTransformer):"
        },
        {
            "sha": "69beb543b466f432812de5c9f86c76c64d1a4657",
            "filename": "src/transformers/models/stablelm/modeling_stablelm.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -666,6 +666,9 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.weight.data.fill_(1.0)\n+            module.bias.data.zero_()\n \n \n STABLELM_INPUTS_DOCSTRING = r\"\"\""
        },
        {
            "sha": "569874aad1dd25e6d7a3c0feba4904dc8ecb4310",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 37,
            "deletions": 34,
            "changes": 71,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -275,6 +275,40 @@ def forward(\n         return outputs\n \n \n+class Starcoder2RotaryEmbedding(nn.Module):\n+    def __init__(self, config: Starcoder2Config, device=None):\n+        super().__init__()\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+        else:\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+    @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n+    def forward(self, x, position_ids):\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n STARCODER2_START_DOCSTRING = r\"\"\"\n     This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n     library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n@@ -320,40 +354,9 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n-\n-\n-class Starcoder2RotaryEmbedding(nn.Module):\n-    def __init__(self, config: Starcoder2Config, device=None):\n-        super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n-        self.max_seq_len_cached = config.max_position_embeddings\n-        self.original_max_seq_len = config.max_position_embeddings\n-\n-        self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n-\n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n-        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n-\n-    @torch.no_grad()\n-    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n-    def forward(self, x, position_ids):\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n-        position_ids_expanded = position_ids[:, None, :].float()\n-\n-        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n-            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos() * self.attention_scaling\n-            sin = emb.sin() * self.attention_scaling\n-\n-        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+        elif isinstance(module, nn.LayerNorm):\n+            module.weight.data.fill_(1.0)\n+            module.bias.data.zero_()\n \n \n STARCODER2_INPUTS_DOCSTRING = r\"\"\""
        },
        {
            "sha": "77b05d1dc1aa48e386104cb80ca6d4b4845d6cdf",
            "filename": "src/transformers/models/starcoder2/modular_starcoder2.py",
            "status": "modified",
            "additions": 22,
            "deletions": 0,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodular_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodular_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodular_starcoder2.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -41,6 +41,8 @@\n     MistralForSequenceClassification,\n     MistralForTokenClassification,\n     MistralModel,\n+    MistralPreTrainedModel,\n+    MistralRotaryEmbedding,\n     apply_rotary_pos_emb,\n     eager_attention_forward,\n )\n@@ -143,6 +145,26 @@ def __init__(self, config: Starcoder2Config, layer_idx: int):\n         self.post_attention_layernorm = nn.LayerNorm(config.hidden_size, eps=config.norm_epsilon)\n \n \n+class Starcoder2RotaryEmbedding(MistralRotaryEmbedding):\n+    pass\n+\n+\n+class Starcoder2PreTrainedModel(MistralPreTrainedModel):\n+    def _init_weights(self, module):\n+        std = self.config.initializer_range\n+        if isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.weight.data.fill_(1.0)\n+            module.bias.data.zero_()\n+\n+\n STARCODER2_INPUTS_DOCSTRING = None  # will be automatically redefined\n \n "
        },
        {
            "sha": "0196169c37763bb261d12692b8abf3dba9f3913d",
            "filename": "src/transformers/models/upernet/modeling_upernet.py",
            "status": "modified",
            "additions": 7,
            "deletions": 30,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fupernet%2Fmodeling_upernet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fupernet%2Fmodeling_upernet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fupernet%2Fmodeling_upernet.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -166,15 +166,6 @@ def __init__(self, config, in_channels):\n             padding=1,\n         )\n \n-    def init_weights(self):\n-        self.apply(self._init_weights)\n-\n-    def _init_weights(self, module):\n-        if isinstance(module, nn.Conv2d):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-\n     def psp_forward(self, inputs):\n         x = inputs[-1]\n         psp_outs = [x]\n@@ -266,15 +257,6 @@ def __init__(\n \n         self.classifier = nn.Conv2d(self.channels, config.num_labels, kernel_size=1)\n \n-    def init_weights(self):\n-        self.apply(self._init_weights)\n-\n-    def _init_weights(self, module):\n-        if isinstance(module, nn.Conv2d):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-\n     def forward(self, encoder_hidden_states: torch.Tensor) -> torch.Tensor:\n         # just take the relevant feature maps\n         hidden_states = encoder_hidden_states[self.in_index]\n@@ -296,18 +278,13 @@ class UperNetPreTrainedModel(PreTrainedModel):\n     _no_split_modules = []\n \n     def _init_weights(self, module):\n-        if isinstance(module, UperNetPreTrainedModel):\n-            module.backbone.init_weights()\n-            module.decode_head.init_weights()\n-            if module.auxiliary_head is not None:\n-                module.auxiliary_head.init_weights()\n-\n-    def init_weights(self):\n-        \"\"\"Initialize the weights\"\"\"\n-        self.backbone.init_weights()\n-        self.decode_head.init_weights()\n-        if self.auxiliary_head is not None:\n-            self.auxiliary_head.init_weights()\n+        if isinstance(module, nn.Conv2d):\n+            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.BatchNorm2d):\n+            module.weight.data.fill_(1.0)\n+            module.bias.data.zero_()\n \n \n UPERNET_START_DOCSTRING = r\"\"\""
        },
        {
            "sha": "71e5b9498b32a9103000ef39862d727523599137",
            "filename": "src/transformers/models/vipllava/modeling_vipllava.py",
            "status": "modified",
            "additions": 5,
            "deletions": 17,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -128,7 +128,6 @@ def forward(self, hidden_states):\n     \"The bare VipLlava Model outputting raw hidden-states without any specific head on top.\",\n     VIPLLAVA_START_DOCSTRING,\n )\n-# Copied from transformers.models.llava.modeling_llava.LlavaPreTrainedModel with Llava->VipLlava,llava->vipllava\n class VipLlavaPreTrainedModel(PreTrainedModel):\n     config_class = VipLlavaConfig\n     base_model_prefix = \"model\"\n@@ -142,26 +141,15 @@ class VipLlavaPreTrainedModel(PreTrainedModel):\n     _supports_static_cache = True\n \n     def _init_weights(self, module):\n-        # important: this ported version of VipLlava isn't meant for training from scratch - only\n-        # inference and fine-tuning - so the proper init weights code has been removed - the original codebase\n-        # https://github.com/haotian-liu/LLaVA/tree/main/vipllava should serve for that purpose\n-        std = (\n-            self.config.initializer_range\n-            if hasattr(self.config, \"initializer_range\")\n-            else self.config.text_config.initializer_range\n-        )\n-\n-        if hasattr(module, \"class_embedding\"):\n-            module.class_embedding.data.normal_(mean=0.0, std=std)\n+        std = getattr(self.config, \"initializer_range\", self.config.get_text_config().initializer_range)\n \n-        if isinstance(module, (nn.Linear, nn.Conv2d)):\n+        if isinstance(module, nn.Linear):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n                 module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.bias.data.zero_()\n+            module.weight.data.fill_(1.0)\n \n \n VIPLLAVA_INPUTS_DOCSTRING = r\"\"\""
        },
        {
            "sha": "2cb995586a71deec847818aa66762b1d6da8af64",
            "filename": "src/transformers/models/whisper/modeling_whisper.py",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -786,10 +786,14 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.weight.data.fill_(1.0)\n+            module.bias.data.zero_()\n         elif isinstance(module, WhisperEncoder):\n-            with torch.no_grad():\n-                embed_positions = module.embed_positions.weight\n-                embed_positions.copy_(sinusoids(*embed_positions.shape))\n+            module.embed_positions.weight.copy_(sinusoids(*module.embed_positions.weight.shape))\n+        elif isinstance(module, WhisperForAudioClassification):\n+            if self.config.use_weighted_layer_sum:\n+                module.layer_weights.data.fill_(1.0 / (self.config.num_hidden_layers + 1))\n \n     def _get_feat_extract_output_lengths(self, input_lengths: torch.LongTensor):\n         \"\"\""
        },
        {
            "sha": "2fe1720c424dd8879c345b68187ba6355dc75e9f",
            "filename": "src/transformers/models/zamba/modeling_zamba.py",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -850,10 +850,9 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, ZambaRMSNorm):\n+            module.weight.data.fill_(1.0)\n         elif isinstance(module, ZambaMambaMixer):\n-            module.A_log._no_weight_decay = True\n-            module.D._no_weight_decay = True\n-\n             module.x_proj_weight.data.normal_(mean=0.0, std=std)\n             dt_init_std = self.config.mamba_dt_rank**-0.5\n             nn.init.uniform_(module.dt_proj_weight, -dt_init_std, dt_init_std)\n@@ -866,10 +865,12 @@ def _init_weights(self, module):\n             ).clamp(min=self.config.time_step_floor)\n             # # Inverse of softplus: https://github.com/pytorch/pytorch/issues/72759\n             inv_dt = dt + torch.log(-torch.expm1(-dt))\n+            module.dt_proj_bias.data.copy_(inv_dt)\n \n-            with torch.no_grad():\n-                module.dt_proj_bias.copy_(inv_dt)\n-            module.dt_proj_bias._no_reinit = True\n+            A = torch.arange(1, module.ssm_state_size + 1, dtype=torch.float32)[None, :]\n+            A = A.expand(module.intermediate_size, -1).contiguous()\n+            module.A_log.data.copy_(torch.log(A).reshape(module.n_mamba_heads, module.mamba_head_dim, -1))\n+            module.D.data.fill_(1.0)\n \n     @classmethod\n     @classmethod"
        },
        {
            "sha": "a3735303ec1382c11b2d6c428eb32b6a6ac9ceec",
            "filename": "src/transformers/models/zamba2/modeling_zamba2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -1225,21 +1225,21 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, (Zamba2RMSNorm, Zamba2RMSNormGated)):\n+            module.weight.data.fill_(1.0)\n         elif isinstance(module, Zamba2MambaMixer):\n-            module.A_log._no_weight_decay = True\n-            module.D._no_weight_decay = True\n-\n             dt = torch.exp(\n                 torch.rand(self.config.n_mamba_heads)\n                 * (math.log(self.config.time_step_max) - math.log(self.config.time_step_min))\n                 + math.log(self.config.time_step_min)\n             ).clamp(min=self.config.time_step_floor)\n             # # Inverse of softplus: https://github.com/pytorch/pytorch/issues/72759\n             inv_dt = dt + torch.log(-torch.expm1(-dt))\n+            module.dt_bias.data.copy_(inv_dt)\n \n-            with torch.no_grad():\n-                module.dt_bias.copy_(inv_dt)\n-            module.dt_bias._no_reinit = True\n+            A = torch.arange(1, module.num_heads + 1)\n+            module.A_log.data.copy_(torch.log(A))\n+            module.D.data.fill_(1.0)\n \n \n ZAMBA2_START_DOCSTRING = r\"\"\""
        },
        {
            "sha": "2c672bba5ae6dcddf3c751a3f58c5328b911c900",
            "filename": "src/transformers/models/zamba2/modular_zamba2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodular_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodular_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodular_zamba2.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -930,21 +930,21 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, (Zamba2RMSNorm, Zamba2RMSNormGated)):\n+            module.weight.data.fill_(1.0)\n         elif isinstance(module, Zamba2MambaMixer):\n-            module.A_log._no_weight_decay = True\n-            module.D._no_weight_decay = True\n-\n             dt = torch.exp(\n                 torch.rand(self.config.n_mamba_heads)\n                 * (math.log(self.config.time_step_max) - math.log(self.config.time_step_min))\n                 + math.log(self.config.time_step_min)\n             ).clamp(min=self.config.time_step_floor)\n             # # Inverse of softplus: https://github.com/pytorch/pytorch/issues/72759\n             inv_dt = dt + torch.log(-torch.expm1(-dt))\n+            module.dt_bias.data.copy_(inv_dt)\n \n-            with torch.no_grad():\n-                module.dt_bias.copy_(inv_dt)\n-            module.dt_bias._no_reinit = True\n+            A = torch.arange(1, module.num_heads + 1)\n+            module.A_log.data.copy_(torch.log(A))\n+            module.D.data.fill_(1.0)\n \n \n class Zamba2Model(ZambaModel, Zamba2PreTrainedModel):"
        },
        {
            "sha": "ee1c4c5b71bfd0b7c67e9861918e46db666df6b0",
            "filename": "tests/models/phi4_multimodal/test_modeling_phi4_multimodal.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/tests%2Fmodels%2Fphi4_multimodal%2Ftest_modeling_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/tests%2Fmodels%2Fphi4_multimodal%2Ftest_modeling_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fphi4_multimodal%2Ftest_modeling_phi4_multimodal.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -85,13 +85,15 @@ def __init__(\n             intermediate_size=48,\n             depthwise_seperable_out_channel=128,\n             nemo_conv_channels=128,\n+            initializer_range=1e-5,\n         ),\n         vision_config=Phi4MultimodalVisionConfig(\n             num_hidden_layers=2,\n             hidden_size=32,\n             intermediate_size=64,\n             num_attention_heads=8,\n             crop_size=16,\n+            initializer_range=1e-5,\n         ),\n     ):\n         self.parent = parent"
        },
        {
            "sha": "7aa8134e5d75092f4fcc7642e2ccc5fdd2cfd8b1",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 70,
            "deletions": 0,
            "changes": 70,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e538409203402058d1ee75c57e03dd908eac508/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e538409203402058d1ee75c57e03dd908eac508/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=4e538409203402058d1ee75c57e03dd908eac508",
            "patch": "@@ -503,6 +503,76 @@ def test_peft_gradient_checkpointing_enable_disable(self):\n                         m.gradient_checkpointing, f\"Module {n} does not have gradient_checkpointing set to False\"\n                     )\n \n+    def test_can_init_all_missing_weights(self):\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        # This is used to get the addition year of the model\n+        filename = inspect.getfile(config.__class__)\n+        # No easy way to get model addition date -> check copyright year on top of file\n+        with open(filename) as file:\n+            source_code = file.read()\n+        addition_year = 0  # if we cannot find it, set it to 0 (i.e. oldest)\n+        if match_object := re.search(r\"^# Copyright (\\d{4})\", source_code, re.MULTILINE | re.IGNORECASE):\n+            addition_year = int(match_object.group(1))\n+\n+        for model_class in self.all_model_classes:\n+            # For now, skip everything older than 2025 and \"important models\" (too much models to patch otherwise)\n+            # Use `supports_cache_class` as a proxy to judge \"important\" models in order to prioritize them\n+            # TODO: relax this as we patch more and more models\n+            if addition_year < 2025 and not model_class._supports_cache_class:\n+                self.skipTest(reason=f\"{model_class} is not a priorited model for now.\")\n+\n+            # Monkey patch the method to add a seed (we do it on PreTrainedModel._initialize_weights, which wraps\n+            # `_init_weights` so that it can add the seed for composite models as well)\n+            original_initialize_weights = PreTrainedModel._initialize_weights\n+\n+            def seeded_initialize_weights(self, module):\n+                set_seed(0)\n+                original_initialize_weights(self, module)\n+\n+            PreTrainedModel._initialize_weights = seeded_initialize_weights\n+\n+            # First, initialize the model from config -> this ensure everything is correctly initialized, even if\n+            # _init_weights() does not take all weights into account correctly\n+            model_from_config = model_class(config)\n+            # Here, passing an empty state dict will force all weights to be moved from meta to cpu, then be initialized\n+            # by _init_weights()\n+            model_from_pretrained = model_class.from_pretrained(None, config=config, state_dict={})\n+\n+            # Back to original method to avoid issues if running several other tests\n+            PreTrainedModel._initialize_weights = original_initialize_weights\n+\n+            # First, check if any parameters are still on meta -> this is usually an issue with tied weights\n+            params_on_meta = []\n+            for k, v in model_from_pretrained.named_parameters():\n+                if v.device.type == \"meta\":\n+                    params_on_meta.append(k)\n+\n+            self.assertTrue(\n+                len(params_on_meta) == 0,\n+                f\"The following keys are still on the meta device, it probably comes from an issue in the tied weights:\\n{params_on_meta}\",\n+            )\n+\n+            # Everything must be exactly the same as we set the same seed for each init\n+            different_weights = []\n+            for (k1, v1), (k2, v2) in zip(\n+                model_from_config.state_dict().items(), model_from_pretrained.state_dict().items()\n+            ):\n+                self.assertEqual(k1, k2, \"The keys from each model should be the same\")\n+                # Since we added the seed, they should be exactly the same (i.e. using allclose maybe be wrong due\n+                # to very low std in init function)\n+                if not (v1 == v2).all():\n+                    different_weights.append(k1)\n+\n+            # Buffers that are initialized randomly are ignored as they are not initialized on meta device anyway\n+            buffer_names = {name for name, _ in model_from_config.named_buffers()}\n+            different_weights = [k for k in different_weights if k not in buffer_names]\n+\n+            self.assertTrue(\n+                len(different_weights) == 0,\n+                f\"The following keys are not properly handled by `_init_weights()`:\\n{different_weights}\",\n+            )\n+\n     @slow\n     @require_accelerate\n     @mark.accelerate_tests"
        }
    ],
    "stats": {
        "total": 1959,
        "additions": 1164,
        "deletions": 795
    }
}