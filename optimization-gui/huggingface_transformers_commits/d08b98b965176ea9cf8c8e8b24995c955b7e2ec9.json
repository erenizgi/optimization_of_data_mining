{
    "author": "Cyrilvallez",
    "message": "[loading/saving] Reverse all loading operations when saving (#42396)\n\n* first shot\n\n* default to reversing\n\n* oupso\n\n* oupsi 2\n\n* oupsi 3\n\n* fix renamed kwargs\n\n* fix timm_wrapper\n\n* remove fix_state_dict methods\n\n* can do it all the time, with __init__ as well\n\n* doc\n\n* oupsi\n\n* fix\n\n* create helper\n\n* fix annotation annoying isue\n\n* small fix\n\n* small fixes\n\n* alright commit all that already\n\n* oupsi\n\n* the fix\n\n* update quantizers\n\n* this works\n\n* the hardcoded regex got me hard....\n\n* style\n\n* the final one\n\n* cleanup a bit\n\n* better\n\n* style\n\n* oupsi readded it\n\n* do it inside the ops instead - no need for full names anymore\n\n* reverse quantizers and simplify signatures\n\n* small thingy\n\n* add no_grad decorator\n\n* utils to rename keys\n\n* oupssii again\n\n* add test\n\n* simplify nicely",
    "sha": "d08b98b965176ea9cf8c8e8b24995c955b7e2ec9",
    "files": [
        {
            "sha": "83d3790fc2fb078bdfb48a3eb792b385cd1c5c3a",
            "filename": "src/transformers/conversion_mapping.py",
            "status": "modified",
            "additions": 105,
            "deletions": 22,
            "changes": 127,
            "blob_url": "https://github.com/huggingface/transformers/blob/d08b98b965176ea9cf8c8e8b24995c955b7e2ec9/src%2Ftransformers%2Fconversion_mapping.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d08b98b965176ea9cf8c8e8b24995c955b7e2ec9/src%2Ftransformers%2Fconversion_mapping.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconversion_mapping.py?ref=d08b98b965176ea9cf8c8e8b24995c955b7e2ec9",
            "patch": "@@ -13,7 +13,10 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n+from __future__ import annotations\n+\n from copy import deepcopy\n+from typing import TYPE_CHECKING\n \n from .core_model_loading import Concatenate, MergeModulelist, WeightConverter, WeightRenaming\n from .utils import is_torch_available\n@@ -23,16 +26,21 @@\n     import torch\n \n \n+if TYPE_CHECKING:\n+    from .modeling_utils import PreTrainedModel\n+    from .quantizers import HfQuantizer\n+\n+\n def _build_checkpoint_conversion_mapping():\n     mapping = {\n         \"mixtral\": [\n             WeightRenaming(\".block_sparse_moe.gate\", \".mlp.gate\"),\n             WeightConverter(\n-                source_keys=[\n+                source_patterns=[\n                     \"block_sparse_moe.experts.*.w1.weight\",\n                     \"block_sparse_moe.experts.*.w3.weight\",\n                 ],  # you give me a list of 2 keys, I collect a list of a list of tensors\n-                target_keys=\"mlp.experts.gate_up_proj\",  # target key gets the list of two tensors\n+                target_patterns=\"mlp.experts.gate_up_proj\",  # target key gets the list of two tensors\n                 operations=[\n                     MergeModulelist(\n                         dim=0\n@@ -41,10 +49,10 @@ def _build_checkpoint_conversion_mapping():\n                 ],  # we want the loading to add this shard operation here. Though we can't shard after concats and merge, needs to be first\n             ),\n             WeightConverter(\n-                source_keys=[\n+                source_patterns=[\n                     \"block_sparse_moe.experts.*.w2.weight\",\n                 ],\n-                target_keys=\"mlp.experts.down_proj\",  # target key gets the list of two tensors\n+                target_patterns=\"mlp.experts.down_proj\",  # target key gets the list of two tensors\n                 operations=[\n                     MergeModulelist(\n                         dim=0\n@@ -54,50 +62,58 @@ def _build_checkpoint_conversion_mapping():\n         ],\n         \"qwen2_moe\": [\n             WeightConverter(\n-                source_keys=[\n+                source_patterns=[\n                     \"mlp.experts.*.gate_proj.weight\",\n                     \"mlp.experts.*.up_proj.weight\",\n                 ],\n-                target_keys=\"mlp.experts.gate_up_proj\",\n+                target_patterns=\"mlp.experts.gate_up_proj\",\n                 operations=[MergeModulelist(dim=0), Concatenate(dim=1)],\n             ),\n             WeightConverter(\n-                source_keys=[\"mlp.experts.*.down_proj.weight\"],\n-                target_keys=\"mlp.experts.down_proj\",\n+                source_patterns=[\"mlp.experts.*.down_proj.weight\"],\n+                target_patterns=\"mlp.experts.down_proj\",\n                 operations=[MergeModulelist(dim=0)],\n             ),\n         ],\n+        \"timm_wrapper\": [\n+            # Simply add the prefix `timm_model`\n+            # TODO: Would be probably much cleaner with a `add_prefix` argument in WeightRenaming\n+            WeightRenaming(\n+                source_patterns=r\"(.+)\",\n+                target_patterns=r\"timm_model.\\1\",\n+            )\n+        ],\n         \"legacy\": [\n             WeightRenaming(\n-                source_keys=\"LayerNorm.gamma\",\n-                target_keys=\"LayerNorm.weight\",\n+                source_patterns=\"LayerNorm.gamma\",\n+                target_patterns=\"LayerNorm.weight\",\n             ),\n             WeightRenaming(\n-                source_keys=\"LayerNorm.beta\",\n-                target_keys=\"LayerNorm.bias\",\n+                source_patterns=\"LayerNorm.beta\",\n+                target_patterns=\"LayerNorm.bias\",\n             ),\n         ],\n     }\n     if hasattr(torch.nn.utils.parametrizations, \"weight_norm\"):\n         mapping[\"legacy\"] += [\n             WeightRenaming(\n-                source_keys=\"weight_g\",\n-                target_keys=\"parametrizations.weight.original0\",\n+                source_patterns=\"weight_g\",\n+                target_patterns=\"parametrizations.weight.original0\",\n             ),\n             WeightRenaming(\n-                source_keys=\"weight_v\",\n-                target_keys=\"parametrizations.weight.original1\",\n+                source_patterns=\"weight_v\",\n+                target_patterns=\"parametrizations.weight.original1\",\n             ),\n         ]\n     else:\n         mapping[\"legacy\"] += [\n             WeightRenaming(\n-                source_keys=\"parametrizations.weight.original0\",\n-                target_keys=\"weight_g\",\n+                source_patterns=\"parametrizations.weight.original0\",\n+                target_patterns=\"weight_g\",\n             ),\n             WeightRenaming(\n-                source_keys=\"parametrizations.weight.original1\",\n-                target_keys=\"weight_v\",\n+                source_patterns=\"parametrizations.weight.original1\",\n+                target_patterns=\"weight_v\",\n             ),\n         ]\n \n@@ -127,5 +143,72 @@ def _build_checkpoint_conversion_mapping():\n def get_checkpoint_conversion_mapping(model_type):\n     global _checkpoint_conversion_mapping_cache\n     _checkpoint_conversion_mapping_cache = _build_checkpoint_conversion_mapping()\n-    globals()[\"_checkpoint_conversion_mapping\"] = _checkpoint_conversion_mapping_cache\n-    return deepcopy(_checkpoint_conversion_mapping_cache.get(model_type, None))\n+    return deepcopy(_checkpoint_conversion_mapping_cache.get(model_type))\n+\n+\n+# DO NOT MODIFY, KEPT FOR BC ONLY\n+VLMS = [\n+    \"aria\",\n+    \"ayavision\",\n+    \"colpali\",\n+    \"emu3\",\n+    \"fuyu\",\n+    \"gotocr2\",\n+    \"gemma3\",\n+    \"internvl\",\n+    \"llava\",  # all llava prefixed models fall under this check\n+    \"mistral3\",\n+    \"mllama\",\n+    \"paligemma\",\n+    \"shieldgemma2\",\n+    \"qwen2vl\",\n+    \"qwen2_5_vl\",\n+    \"videollava\",\n+    \"vipllava\",\n+    \"sam3_video\",\n+    \"sam3\",\n+    \"sam3_tracker\",\n+    \"sam3_tracker_video\",\n+]\n+\n+\n+def get_model_conversion_mapping(\n+    model: PreTrainedModel,\n+    key_mapping: dict[str, str] | None = None,\n+    hf_quantizer: HfQuantizer | None = None,\n+    add_legacy: bool = True,\n+) -> list[WeightConverter | WeightRenaming]:\n+    \"\"\"\n+    For a given `model`, obtain the weight conversion mapping if any are registered either as a simple renaming\n+    `_checkpoint_conversion_mapping` class argument, or in the general WeightConverter mapping.\n+    \"\"\"\n+    weight_conversions = []\n+\n+    # Load models with key mapping\n+    if key_mapping is not None:\n+        weight_conversions = [WeightRenaming(source_patterns=k, target_patterns=v) for k, v in key_mapping.items()]\n+    elif any(\n+        allowed_name in class_name.__name__.lower()\n+        for class_name in model.__class__.__mro__[:-1]\n+        for allowed_name in VLMS\n+    ):\n+        weight_conversions = [\n+            WeightRenaming(source_patterns=k, target_patterns=v)\n+            for k, v in model._checkpoint_conversion_mapping.items()\n+        ]\n+\n+    # TODO: should be checked recursively on submodels!!\n+    model_type = getattr(model.config, \"model_type\", None)\n+    if model_type is not None:\n+        model_specific_conversions = get_checkpoint_conversion_mapping(model_type)\n+        if model_specific_conversions is not None:\n+            weight_conversions.extend(model_specific_conversions)\n+\n+    if add_legacy:\n+        weight_conversions.extend(get_checkpoint_conversion_mapping(\"legacy\"))\n+\n+    # Add the ones from the quantizer as well if provided\n+    if hf_quantizer is not None:\n+        weight_conversions.extend(hf_quantizer.get_weight_conversions())\n+\n+    return weight_conversions"
        },
        {
            "sha": "f4ec3a7f38ca323ced6c090c38253ad340f158ce",
            "filename": "src/transformers/core_model_loading.py",
            "status": "modified",
            "additions": 336,
            "deletions": 176,
            "changes": 512,
            "blob_url": "https://github.com/huggingface/transformers/blob/d08b98b965176ea9cf8c8e8b24995c955b7e2ec9/src%2Ftransformers%2Fcore_model_loading.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d08b98b965176ea9cf8c8e8b24995c955b7e2ec9/src%2Ftransformers%2Fcore_model_loading.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcore_model_loading.py?ref=d08b98b965176ea9cf8c8e8b24995c955b7e2ec9",
            "patch": "@@ -19,7 +19,8 @@\n import os\n import re\n from abc import abstractmethod\n-from collections.abc import MutableMapping, MutableSet, Sequence\n+from collections import defaultdict\n+from collections.abc import MutableMapping, MutableSet\n from concurrent.futures import Future, ThreadPoolExecutor\n from contextlib import contextmanager\n from copy import deepcopy\n@@ -46,8 +47,6 @@\n \n logger = logging.get_logger(__name__)\n \n-logger = logging.get_logger(__name__)\n-\n \n def compile_glob_rule(source_glob: str, target_glob: str) -> tuple[re.Pattern, str]:\n     \"\"\"\n@@ -82,13 +81,13 @@ def build_glob_alternation(\n     i = 0\n     for glob in globs:\n         if isinstance(glob, (WeightRenaming, WeightConverter)):\n-            for src in glob.source_keys:\n+            for src in glob.source_patterns:\n                 group_name = f\"g{i}\"\n                 src_group_to_glob[group_name] = src\n                 i += 1\n                 body = src.replace(\"*\", r\".*\")\n                 branches.append(f\"(?P<{group_name}>{body})\")\n-                tgt_group_to_glob[group_name] = glob.target_keys[0]  # we index witht the first target\n+                tgt_group_to_glob[group_name] = glob.target_patterns[0]  # we index witht the first target\n         else:\n             group_name = f\"g{i}\"\n             src_group_to_glob[group_name] = glob\n@@ -105,144 +104,162 @@ def build_glob_alternation(\n class ConversionOps:\n     \"\"\"Base class for weight conversion operations.\"\"\"\n \n-    # The inverse operation class, will be used when saving the checkpoint\n-    reverse_op: type[ConversionOps]\n+    def __repr__(self):\n+        return f\"{self.__class__.__name__}(dim={self.dim})\"\n \n     @abstractmethod\n     def convert(\n-        self,\n-        value: dict[str, Any],\n-        source_keys: list[str],\n-        target_keys: list[str],\n-        full_layer_name: str,\n-        model,\n-        missing_keys,\n-        config,\n-        **kwargs,\n+        self, input_dict: dict[str, Any], source_patterns: list[str], target_patterns: list[str], **kwargs\n     ) -> dict[str, list[torch.Tensor]]:\n         raise NotImplementedError\n \n+    @property\n+    def reverse_op(self) -> ConversionOps:\n+        raise NotImplementedError\n \n-class Chunk(ConversionOps):\n-    \"\"\"Split a tensor along ``dim`` into equally sized chunks or using explicit ``sizes``.\"\"\"\n \n-    reverse_op: type[ConversionOps]\n+class Chunk(ConversionOps):\n+    \"\"\"Split a tensor along ``dim`` into equally sized chunks.\"\"\"\n \n-    def __init__(self, dim: int = 0, chunks: Optional[int] = None, sizes: Optional[Sequence[int]] = None):\n+    def __init__(self, dim: int = 0):\n         self.dim = dim\n-        self.chunks = chunks\n-        self.sizes = list(sizes) if sizes is not None else None\n-        self.reverse_op = Concatenate\n \n+    @torch.no_grad\n     def convert(\n-        self,\n-        value: dict[str, list[torch.Tensor]],\n-        source_keys: list[str],\n-        target_keys: list[str],\n-        full_layer_name: str,\n-        model,\n-        missing_keys,\n-        config,\n-    ) -> dict[str, list[torch.Tensor]]:\n-        tensors = next(iter(value.values()))\n-        tensor = tensors[0]\n-        sizes = len(target_keys)\n+        self, input_dict: dict[str, torch.Tensor], source_patterns: list[str], target_patterns: list[str], **kwargs\n+    ) -> dict[str, torch.Tensor]:\n+        tensors = next(iter(input_dict.values()))\n+        tensor = tensors[0] if isinstance(tensors, list) else tensors\n+        targets = self.get_target_pattern(input_dict, target_patterns)\n+        sizes = len(targets)\n         chunks = torch.chunk(tensor, sizes, dim=self.dim)\n-        return {full_layer_name.replace(target_keys[0], target): [chunk] for target, chunk in zip(target_keys, chunks)}\n+        return dict(zip(targets, chunks))\n \n+    def get_target_pattern(self, input_dict: dict, target_patterns: list[str]) -> list[str]:\n+        # Here we always return the target patterns\n+        if len(input_dict) > 1 or len(target_patterns) == 1:\n+            raise ValueError(\"Undefined Operation encountered!\")\n+        return target_patterns\n+\n+    @property\n+    def reverse_op(self) -> ConversionOps:\n+        return Concatenate(self.dim)\n \n-class Concatenate(ConversionOps):\n-    \"\"\"Concatenate tensors along `dim` using a reusable buffer.\"\"\"\n \n-    reverse_op: type[ConversionOps]\n+class Concatenate(ConversionOps):\n+    \"\"\"Concatenate tensors along `dim`.\"\"\"\n \n     def __init__(self, dim: int = 0):\n         self.dim = dim\n-        self.reverse_op = Chunk\n \n     @torch.no_grad\n     def convert(\n         self,\n-        value: dict[str, list[torch.Tensor]],\n-        source_keys: list[str],\n-        target_keys: list[str],\n-        full_layer_name: str,\n-        model,\n-        missing_keys,\n-        config,\n+        input_dict: dict[str, list[torch.Tensor]],\n+        source_patterns: list[str],\n+        target_patterns: list[str],\n+        **kwargs,\n     ) -> dict[str, torch.Tensor]:\n-        if len(target_keys) != 1:\n-            raise ValueError(\"Concatenate expects a single target key.\")\n-        if len(value) != len(source_keys):\n-            raise ValueError(\"Concatenate received an unexpected number of tensors compared to source keys.\")\n+        target_pattern = self.get_target_pattern(target_patterns)\n+        all_tensors = []\n+        # Very important to keep the relative order of the source patterms here, so we iterate over them not the\n+        # input directly as it's unordered!\n+        for source_pattern in source_patterns:\n+            tensors = input_dict[source_pattern]\n+            if isinstance(tensors, list):\n+                all_tensors.extend(tensors)\n+            else:\n+                all_tensors.append(tensors)\n+        return {target_pattern: torch.cat(all_tensors, dim=self.dim)}\n \n-        return {full_layer_name: torch.cat(tuple(value.values()), dim=self.dim)}\n+    def get_target_pattern(self, target_patterns: list[str]) -> str:\n+        # Here we always return the target pattern\n+        if len(target_patterns) > 1:\n+            raise ValueError(\"Undefined Operation encountered!\")\n+        return target_patterns[0]\n \n+    @property\n+    def reverse_op(self) -> ConversionOps:\n+        return Chunk(self.dim)\n \n-class MergeModulelist(Concatenate):\n+\n+class MergeModulelist(ConversionOps):\n     \"\"\"\n     Merge a list of tensors into a single tensor along the first dimension.\n     We explicitly define this because for EP or TP you want to make sure you know what you are doing!\n \n     \"\"\"\n \n     def __init__(self, dim: int = 0):\n-        super().__init__(dim=dim)\n-        self.reverse_op = SplitModulelist\n+        self.dim = dim\n \n     @torch.no_grad\n     def convert(\n         self,\n-        value: dict[str, list[torch.Tensor]],\n-        source_keys: list[str],\n-        target_keys: list[str],\n-        full_layer_name: str,\n-        model,\n-        missing_keys,\n-        config,\n+        input_dict: dict[str, list[torch.Tensor]],\n+        source_patterns: list[str],\n+        target_patterns: list[str],\n+        **kwargs,\n     ) -> dict[str, torch.Tensor]:\n         merged: dict[str, torch.Tensor] = {}\n-        for idx, key in enumerate(value.keys()):\n-            tensors = value.get(key, [])\n-            if len(source_keys) == 1:\n-                key = full_layer_name\n-            stacked = torch.stack(tensors, dim=self.dim)\n-            merged[key] = stacked\n+        for source_pattern, tensors in input_dict.items():\n+            target_pattern = self.get_target_pattern(input_dict, source_pattern, target_patterns)\n+            merged[target_pattern] = torch.stack(tensors, dim=self.dim)\n         return merged\n \n+    def get_target_pattern(self, input_dict: dict, source_pattern: str, target_patterns: list[str]) -> str:\n+        # Here it's a single operation, so we use the target\n+        if len(input_dict) == 1:\n+            if len(target_patterns) == 1:\n+                return target_patterns[0]\n+            else:\n+                raise ValueError(\"Undefined Operation encountered!\")\n+        #  Here it's the first operation in a chain, so we use the source as they were replaced before in the chain\n+        else:\n+            return source_pattern\n+\n+    @property\n+    def reverse_op(self) -> ConversionOps:\n+        return SplitModulelist(self.dim)\n+\n \n class SplitModulelist(ConversionOps):\n     \"\"\"Inverse of :class:`MergeModulelist` using explicit split sizes per group.\"\"\"\n \n-    def __init__(self, sizes: Sequence[Sequence[int]], dim: int = 0):\n-        if not isinstance(sizes, Sequence) or not all(isinstance(sub, Sequence) and sub for sub in sizes):\n-            raise ValueError(\"`sizes` must be a sequence of non-empty sequences of integers.\")\n-        self.sizes = [list(sub) for sub in sizes]\n+    def __init__(self, dim: int = 0):\n         self.dim = dim\n-        self.reverse_op = MergeModulelist\n \n     @torch.no_grad\n     def convert(\n-        self,\n-        value: dict[str, list[torch.Tensor]],\n-        source_keys: list[str],\n-        target_keys: list[str],\n-        full_layer_name: str,\n-        model,\n-        missing_keys,\n-        config,\n-    ) -> dict[str, list[torch.Tensor]]:\n-        if len(value) != len(self.sizes):\n-            raise ValueError(\"SplitModulelist received an unexpected number of tensors.\")\n-        result: dict[str, list[torch.Tensor]] = {}\n-        for (key, tensors), split_sizes in zip(value.items(), self.sizes):\n-            if len(tensors) != 1:\n-                raise ValueError(\"SplitModulelist expects exactly one tensor per key.\")\n-            current_tensor = tensors[0]\n-            if not isinstance(current_tensor, torch.Tensor):\n-                raise TypeError(\"SplitModulelist can only split torch.Tensor instances.\")\n-            result[key] = list(torch.split(current_tensor, split_sizes, dim=self.dim))\n-        return result\n+        self, input_dict: dict[str, torch.Tensor], source_patterns: list[str], target_patterns: list[str], **kwargs\n+    ) -> dict[str, torch.Tensor]:\n+        all_tensors = {}\n+        for source_pattern, tensors in input_dict.items():\n+            tensor = tensors[0] if isinstance(tensors, list) else tensors\n+            # We split in the number of tensors present in the given dim\n+            sizes = tensor.size(self.dim)\n+            targets = self.get_target_patterns(input_dict, source_pattern, target_patterns, sizes)\n+            chunks = torch.chunk(tensor, sizes, dim=self.dim)\n+            # We squeeze each chunk here as well to make sure to give them their original shape\n+            all_tensors.update({target: chunk.squeeze() for target, chunk in zip(targets, chunks)})\n+        return all_tensors\n+\n+    def get_target_patterns(\n+        self, input_dict: dict, source_pattern: str, target_patterns: list[str], sizes: int\n+    ) -> list[str]:\n+        # Here it's a single operation, so we use the target\n+        if len(input_dict) == 1:\n+            if len(target_patterns) == 1:\n+                return [target_patterns[0].replace(\"*\", f\"{i}\") for i in range(sizes)]\n+            else:\n+                raise ValueError(\"Undefined Operation encountered!\")\n+        # Here it's the last operation in a chain, so we use the source as they were replaced before in the chain\n+        else:\n+            return [source_pattern.replace(\"*\", f\"{i}\") for i in range(sizes)]\n+\n+    @property\n+    def reverse_op(self) -> ConversionOps:\n+        return MergeModulelist(self.dim)\n \n \n class PermuteForRope(ConversionOps):\n@@ -264,17 +281,15 @@ def _apply(self, tensor: torch.Tensor) -> torch.Tensor:\n     @torch.no_grad\n     def convert(\n         self,\n-        value: dict[str, list[torch.Tensor]],\n-        source_keys: list[str],\n-        target_keys: list[str],\n-        full_layer_name: str,\n-        model,\n-        missing_keys,\n+        input_dict: dict[str, list[torch.Tensor]],\n+        source_patterns: list[str],\n+        target_patterns: list[str],\n         config,\n+        **kwargs,\n     ) -> dict[str, list[torch.Tensor]]:\n         self.config = config\n         output: dict[str, list[torch.Tensor]] = {}\n-        for key, tensors in value.items():\n+        for key, tensors in input_dict.items():\n             if len(tensors) != 1:\n                 raise ValueError(\"PermuteForRope expects a single tensor per key.\")\n             output[key] = [self._apply(tensors[0])]\n@@ -283,27 +298,66 @@ def convert(\n \n @dataclass(slots=True)\n class WeightTransform:\n-    source_keys: Union[str, list[str]] = field(init=True)\n-    target_keys: Union[str, list[str]] = field(init=True)\n+    source_patterns: Union[str, list[str]] = field(init=True)\n+    target_patterns: Union[str, list[str]] = field(init=True)\n \n     distributed_operation: Optional[TensorParallelLayer] = None\n     quantization_operation: Optional[ConversionOps] = None\n \n-    collected_tensors: dict[str, list[Future]] = field(default_factory=dict, init=False)\n-    layer_targets: dict[str, set[str]] = field(default_factory=dict, init=False)\n+    collected_tensors: dict[str, list[Future]] = field(default_factory=lambda: defaultdict(list), init=False)\n+    layer_targets: dict[str, set[str]] = field(default_factory=lambda: defaultdict(set), init=False)\n \n     def __post_init__(self):\n-        if isinstance(self.source_keys, str):\n-            self.source_keys = [self.source_keys]\n-        if isinstance(self.target_keys, str):\n-            self.target_keys = [self.target_keys]\n+        if isinstance(self.source_patterns, str):\n+            self.source_patterns = [self.source_patterns]\n+        if isinstance(self.target_patterns, str):\n+            self.target_patterns = [self.target_patterns]\n+\n+        # Due to how our `_checkpoint_conversion_mapping` mappings are written, we need a few exceptions here\n+        # when instantiating the reverse mapping (i.e. the targets become sources, and sources become targets)\n+        # The issues lie in the sources usually, so here we need to check the targets for the reversed mapping\n+        for i, pattern in enumerate(self.target_patterns):\n+            # Some mapping contains `^` to notify start of string when matching -> remove it during reverse mapping\n+            pattern = pattern.removeprefix(\"^\")\n+            # This is ugly but needed for reverse mapping of Qwen2.5!\n+            if r\"(?!\\.(language_model|visual))\" in pattern:\n+                pattern = pattern.replace(r\"(?!\\.(language_model|visual))\", \"\")\n+            # Allow capturing groups in patterns, i.e. to add a prefix to all keys (e.g. timm_wrapper)\n+            if r\"(.+)\" in pattern:\n+                pattern = pattern.replace(r\"(.+)\", \"\")\n+            self.target_patterns[i] = pattern\n+\n+        # We also need to check capturing groups in the sources during reverse mapping (e.g. timm_wrapper)\n+        for i, pattern in enumerate(self.source_patterns):\n+            if r\"\\1\" in pattern:\n+                pattern = pattern.replace(r\"\\1\", \"\")\n+            self.source_patterns[i] = pattern\n \n     def add_tensor(self, target_key: str, source_key: str, source_pattern: str, future: Future):\n-        bucket = self.collected_tensors.setdefault(source_pattern, [])\n-        bucket += [future]\n+        self.collected_tensors[source_pattern].append(future)\n+        self.layer_targets[target_key].add(source_key)\n+\n+    def reset(self) -> None:\n+        \"\"\"Clean-up the collected tensors to make sure we don't keep references to past tensors in memory.\"\"\"\n+        self.collected_tensors = defaultdict(list)\n+\n+    def reverse_transform(self) -> WeightTransform:\n+        \"\"\"Reverse the current `WeightTransform` instance, to be able to save with the opposite weight transformations.\"\"\"\n+        # TODO: check this and relax when quantizer have `reverse_op`\n+        if self.quantization_operation is not None:\n+            raise ValueError(\"Cannot reverse the transform with TP or quantization\")\n+\n+        kwargs = {}\n+        # Add the reverse ops if applicable (it needs to be provided at __init__)\n+        if hasattr(self, \"operations\"):\n+            # All reverse ops, in reverse order\n+            kwargs[\"operations\"] = [op.reverse_op for op in self.operations[::-1]]\n+\n+        reverse_transform = self.__class__(\n+            source_patterns=self.target_patterns, target_patterns=self.source_patterns, **kwargs\n+        )\n \n-        bucket = self.layer_targets.setdefault(target_key, set())\n-        bucket.add(source_key)\n+        return reverse_transform\n \n \n @dataclass(slots=True)\n@@ -319,17 +373,24 @@ def convert(\n         missing_keys: Optional[MutableSet[str]] = None,\n         misc: Optional[MutableMapping[str, str]] = None,\n     ):\n+        # Collect the tensor if using threading\n         for pattern, futures in self.collected_tensors.items():\n-            self.collected_tensors[pattern] = [future.result() for future in futures]\n+            self.collected_tensors[pattern] = (\n+                futures if isinstance(futures[0], torch.Tensor) else [future.result() for future in futures]\n+            )\n+\n+        # Perform renaming op (for a simple WeightRenaming, `self.source_patterns` and `self.target_patterns` can\n+        # only be of length 1, and are actually the full key names - we also have only 1 single related tensor)\n+        target_key = self.target_patterns[0]\n+        collected_tensors = {target_key: self.collected_tensors[self.source_patterns[0]]}\n \n-        collected_tensors = self.collected_tensors\n         if hf_quantizer is not None and self.quantization_operation is not None:\n             with log_to_misc(layer_name, misc, (self.collected_tensors, layer_name), self.quantization_operation):\n                 collected_tensors = self.quantization_operation.convert(\n-                    self.collected_tensors,\n-                    source_keys=self.source_keys,\n-                    target_keys=self.target_keys,\n-                    full_layer_name=layer_name,\n+                    collected_tensors,\n+                    source_patterns=self.source_patterns,\n+                    target_patterns=self.target_patterns,\n+                    full_layer_name=target_key,\n                     model=model,\n                     config=config,\n                     missing_keys=missing_keys,\n@@ -344,9 +405,9 @@ class WeightConverter(WeightTransform):\n \n     def __post_init__(self):\n         WeightTransform.__post_init__(self)\n-        if bool(len(self.source_keys) - 1) + bool(len(self.target_keys) - 1) >= 2:\n+        if bool(len(self.source_patterns) - 1) + bool(len(self.target_patterns) - 1) >= 2:\n             raise ValueError(\n-                f\"source keys={self.source_keys}, target_keys={self.target_keys} but you can only have one to many, one to one or many to one.\"\n+                f\"source keys={self.source_patterns}, target_patterns={self.target_patterns} but you can only have one to many, one to one or many to one.\"\n             )\n         if not self.operations:\n             raise ValueError(\"WeightConverter requires at least one operation.\")\n@@ -360,27 +421,40 @@ def convert(\n         missing_keys: Optional[MutableSet[str]] = None,\n         misc: Optional[MutableMapping[str, str]] = None,\n     ):\n+        # Collect all tensors if using threading\n         for pattern, futures in self.collected_tensors.items():\n-            self.collected_tensors[pattern] = [future.result() for future in futures]\n+            self.collected_tensors[pattern] = (\n+                futures if isinstance(futures[0], torch.Tensor) else [future.result() for future in futures]\n+            )\n \n         collected_tensors = self.collected_tensors\n         for op in self.operations:\n             with log_to_misc(layer_name, misc, (collected_tensors, layer_name), op):\n                 collected_tensors = op.convert(\n                     collected_tensors,\n-                    source_keys=self.source_keys,\n-                    target_keys=self.target_keys,\n-                    full_layer_name=layer_name,\n+                    source_patterns=self.source_patterns,\n+                    target_patterns=self.target_patterns,\n+                    # Additional kwargs, ususally not used\n                     model=model,\n                     config=config,\n                     missing_keys=missing_keys,\n                 )\n+\n+        # Tensors are returned from ops with the target patterns, we need to expand them to full name.\n+        # This means we need to grab the prefix and suffix to add to every target key\n+        full_name = layer_name\n+        if \".*.\" in layer_name:\n+            full_name = layer_name.replace(\".*.\", \".0.\")\n+        prefix, _, suffix = next(full_name.partition(k) for k in collected_tensors.keys() if k in full_name)\n+        # Rename the tensors\n+        collected_tensors = {prefix + k + suffix: v for k, v in collected_tensors.items()}\n+\n         if hf_quantizer is not None and self.quantization_operation is not None:\n             with log_to_misc(layer_name, misc, (collected_tensors, layer_name), self.quantization_operation):\n                 collected_tensors = self.quantization_operation.convert(\n                     collected_tensors,\n-                    source_keys=self.source_keys,\n-                    target_keys=self.target_keys,\n+                    source_patterns=self.source_patterns,\n+                    target_patterns=self.target_patterns,\n                     full_layer_name=layer_name,\n                     config=config,\n                     model=model,\n@@ -552,7 +626,50 @@ def repl(m, repl_map: dict[str, str]) -> str:\n \n     # Exactly one match => return replacement\n     name = matched_groups[0]\n-    return repl_map[name]\n+    replacement = repl_map[name]\n+    # Allow capturing groups in patterns, i.e. to add a prefix to all keys (e.g. timm_wrapper)\n+    if r\"\\1\" in replacement and len(m.groups()) > 1:\n+        replacement = replacement.replace(r\"\\1\", m.group(1))\n+\n+    return replacement\n+\n+\n+def rename_source_key(\n+    source_key: str,\n+    rename_alternation: re.Pattern,\n+    rename_by_group: dict,\n+    weight_pattern_alternation: re.Pattern | None,\n+    weight_pattern_by_group: dict | None,\n+    prefix: str | None = None,\n+    meta_state_dict: dict | None = None,\n+) -> tuple[str, re.Match | None]:\n+    \"\"\"\n+    Rename a source key given all the renaming and weight conversion patterns we have. Also takes care of adding/removing\n+    the base model prefix during loading if necesary.\n+    \"\"\"\n+    # 1. apply all renamings\n+    renamed_key = rename_alternation.sub(lambda m: repl(m, rename_by_group), source_key).replace(\"\\\\\", \"\")\n+\n+    # 2. apply renaming through weight conversions on the key if we have any WeightConverter\n+    matched_converter_pattern = (\n+        weight_pattern_alternation.search(renamed_key) if weight_pattern_alternation is not None else None\n+    )\n+    if matched_converter_pattern is not None:\n+        renamed_key = weight_pattern_alternation.sub(lambda m: repl(m, weight_pattern_by_group), renamed_key).replace(\n+            \"\\\\\", \"\"\n+        )\n+\n+    # 3. check if we need to add or remove prefix if necesary (only during loading, not saving)\n+    if prefix is not None and meta_state_dict is not None:\n+        if (\n+            renamed_key.startswith(prefix)\n+            and meta_state_dict.get(re.sub(f\"^{prefix}.\", \"\", renamed_key, count=1)) is not None\n+        ):\n+            renamed_key = re.sub(f\"^{prefix}.\", \"\", renamed_key, count=1)\n+        elif meta_state_dict.get(f\"{prefix}.{renamed_key}\") is not None:\n+            renamed_key = f\"{prefix}.{renamed_key}\"\n+\n+    return renamed_key, matched_converter_pattern\n \n \n def convert_and_load_state_dict_in_model(\n@@ -576,8 +693,8 @@ def convert_and_load_state_dict_in_model(\n     {\n         \"model.layers.0.attention.q.weight\": # Notice here there is only the first key of the target keys\n             WeightConverter(\n-                source_keys=[\"qkv\"],\n-                target_keys=[\"q\", \"k\",\"v\"],\n+                source_patterns=[\"qkv\"],\n+                target_patterns=[\"q\", \"k\",\"v\"],\n                 operations=[Chunk(dim=0, chunks=3)]),\n                 collected_tensors={\n                     \"qkv\": [Future, Future, Future]},\n@@ -596,8 +713,8 @@ def convert_and_load_state_dict_in_model(\n     For example for:\n     ```python\n     WeightConverter(\n-        source_keys=[\"mlp.experts.*.gate_proj.weight\",\"mlp.experts.*.up_proj.weight\"],\n-        target_keys=\"mlp.experts.gate_up_proj\",\n+        source_patterns=[\"mlp.experts.*.gate_proj.weight\",\"mlp.experts.*.up_proj.weight\"],\n+        target_patterns=\"mlp.experts.gate_up_proj\",\n         operations=[MergeModulelist(dim=0), Concatenate(dim=1)],\n     )\n     ```\n@@ -633,17 +750,17 @@ def convert_and_load_state_dict_in_model(\n     ```python\n     # for \"medmekk/llama-3.2-1b-float8-torchao\"\n     WeightConverter(\n-        source_keys=[\":qdata\", \":scale\"],\n-        target_keys=\"\",\n+        source_patterns=[\":qdata\", \":scale\"],\n+        target_patterns=\"\",\n         operations=[TorchaoDeserialize()],\n     )\n     ```\n     This will collect all tensors that have the same prefix, but end with `:qdata` or `:scale`. This will give us:\n     ```python\n     all_weight_mapping = {\n         \"model.layers.13.self_attn.o_proj.weight\": WeightConverter(\n-            source_keys=[\":qdata\", \":scale\"],\n-            target_keys=\"\",\n+            source_patterns=[\":qdata\", \":scale\"],\n+            target_patterns=\"\",\n             operations=[TorchaoDeserialize()],\n             collected_tensors={\n                 \":qdata\": [Future],\n@@ -680,35 +797,30 @@ def convert_and_load_state_dict_in_model(\n     # build '(?P<g0>.*.*\\\\.block_sparse_moe\\\\..*)' and group to source {'g0': '*.block_sparse_moe.'}\n     # and target to source {'g0': '*.mlp.'}. This allows us to quickly find which pattern matched.\n     rename_alt, _, rename_by_group = build_glob_alternation(renamings)\n+    weight_pattern_alt, src_group_to_glob, tgt_group_to_glob = None, None, None\n     if converters != []:\n         weight_pattern_alt, src_group_to_glob, tgt_group_to_glob = build_glob_alternation(converters)\n     if tp_plan != {}:\n         tp_plan_alt, tp_plan_by_group_name, _ = build_glob_alternation(list(tp_plan.keys()))\n     if dtype_plan != {}:\n         dtype_policy_alt, dtype_policy_by_group_name, _ = build_glob_alternation(list(dtype_plan.keys()))\n \n-    pattern_to_converter = {k: converter for converter in converters for k in converter.source_keys}\n+    pattern_to_converter = {k: converter for converter in converters for k in converter.source_patterns}\n \n     state_dict = sorted(state_dict.items(), key=lambda kv: dot_natural_key(kv[0]))\n     for original_key, tensor in state_dict:\n-        # 1. apply all renamings\n-        renamed_key = rename_alt.sub(lambda m: repl(m, rename_by_group), original_key).replace(\"\\\\\", \"\")\n-\n-        # 2. apply 1 weight conversion on the key\n-        matched_pattern = weight_pattern_alt.search(renamed_key) if converters != [] else None\n-        if matched_pattern is not None:  # we have a converter to apply\n-            renamed_key = weight_pattern_alt.sub(lambda m: repl(m, tgt_group_to_glob), renamed_key).replace(\"\\\\\", \"\")\n-\n-        # 3. check if we need to add or remove prefix\n-        if (\n-            renamed_key.startswith(prefix)\n-            and meta_model_state_dict.get(re.sub(f\"^{prefix}.\", \"\", renamed_key, count=1)) is not None\n-        ):\n-            renamed_key = re.sub(f\"^{prefix}.\", \"\", renamed_key, count=1)\n-        elif meta_model_state_dict.get(f\"{prefix}.{renamed_key}\") is not None:\n-            renamed_key = f\"{prefix}.{renamed_key}\"\n+        # 1. Rename the key according to all renaming pattern and optional weight converter patterns\n+        renamed_key, matched_pattern = rename_source_key(\n+            original_key,\n+            rename_alt,\n+            rename_by_group,\n+            weight_pattern_alt,\n+            tgt_group_to_glob,\n+            prefix,\n+            meta_model_state_dict,\n+        )\n \n-        # 4. finally, collect the tensor into the proper converter\n+        # 2. finally, collect the tensor into the proper converter\n         if renamed_key in missing_keys:\n             empty_param = meta_model_state_dict.get(renamed_key)\n             if matched_pattern:\n@@ -717,10 +829,10 @@ def convert_and_load_state_dict_in_model(\n                 mapping = param_name_to_load.setdefault(renamed_key, new_converter)\n                 source_pattern = src_group_to_glob[matched_pattern.lastgroup]\n             else:\n-                mapping = param_name_to_load.setdefault(renamed_key, WeightRenaming(renamed_key, renamed_key))\n-                source_pattern = renamed_key\n+                mapping = param_name_to_load.setdefault(renamed_key, WeightRenaming(original_key, renamed_key))\n+                source_pattern = original_key\n \n-            # 5. Handle dtype casting\n+            # 3. Handle dtype casting\n             if (\n                 hf_quantizer\n                 and not hf_quantizer.pre_quantized\n@@ -740,7 +852,7 @@ def convert_and_load_state_dict_in_model(\n             elif empty_param is not None and empty_param.dtype != _dtype:\n                 _dtype = empty_param.dtype  # usually correct when initializing\n \n-            # 6. Handle TP sharding or device_map placement -> scheduled materialization\n+            # 4. Handle TP sharding or device_map placement -> scheduled materialization\n             future = None\n             if device_mesh:\n                 if matched_tp_pattern := tp_plan_alt.search(renamed_key):\n@@ -769,8 +881,8 @@ def convert_and_load_state_dict_in_model(\n             mapping.add_tensor(renamed_key, original_key, source_pattern, future)\n         elif matched_pattern:  # add all target keys as unexpected\n             mapping = pattern_to_converter[src_group_to_glob[matched_pattern.lastgroup]]\n-            for k in mapping.target_keys:\n-                unexpected_keys.add(renamed_key.replace(mapping.target_keys[0], k))\n+            for k in mapping.target_patterns:\n+                unexpected_keys.add(renamed_key.replace(mapping.target_patterns[0], k))\n         else:\n             unexpected_keys.add(renamed_key)\n \n@@ -810,27 +922,75 @@ def convert_and_load_state_dict_in_model(\n                             mapping.distributed_operation,\n                             hf_quantizer,\n                         )\n+\n+                # Cleanup the tensors\n+                mapping.reset()\n             except SkipLayer:\n                 continue\n \n+    # Keep the current weight conversion mapping for later saving (in case it was coming directly from the user)\n+    model._weight_conversions = weight_mapping\n     thread_pool.shutdown(wait=False)\n     return missing_keys, unexpected_keys, mismatch_keys, disk_offload_index, misc\n \n \n-# TODO this is not done yet!\n-def revert_weight_conversion(model, state_dict):\n-    mapping = getattr(model, \"_checkpoint_conversion_mapping\", {})  # IDK why but setting this will fail all llava.\n-    reverse_key_mapping = [(v, k) for k, v in mapping.items()]\n-    original_state_dict = {}\n-    for key, value in state_dict.items():\n-        for pattern, inverse_converter in reverse_key_mapping:\n-            # TODO FIXME you name it\n-            replacement = inverse_converter.lstrip(\"^\")  # strip off un-needed chars and patterns\n-            replacement = re.sub(r\"\\(.*\\)\", \"\", replacement)\n-            key, n_replace = re.subn(pattern, replacement, key)\n-            # Early exit of the loop\n-            if n_replace > 0:\n-                break\n-        original_state_dict[key] = value\n-    state_dict = original_state_dict\n-    return state_dict\n+def revert_weight_conversion(model: PreTrainedModel, state_dict: dict[str, torch.Tensor]):\n+    \"\"\"\n+    Revert the conversion mapping that was used to load the model with `from_pretrained`, or the default one\n+    if the model was created in another way and is part of the default mappings.\n+    \"\"\"\n+    weight_conversions = getattr(model, \"_weight_conversions\", None)\n+    # In this case, the model was not created with `from_pretrained` -> let's check if it's in the hardcoded\n+    # mappings, and recreate the mapping from there if it is\n+    if weight_conversions is None:\n+        from .conversion_mapping import get_model_conversion_mapping\n+\n+        # Do not resave with the legacy renaming, if present\n+        weight_conversions = get_model_conversion_mapping(model, add_legacy=False)\n+        weight_conversions = weight_conversions if len(weight_conversions) > 0 else None\n+\n+    # We did not find any operations to perform -> quick escape\n+    if weight_conversions is None:\n+        return state_dict\n+\n+    # Reverse all Transform to correctly match keys\n+    reverse_weight_conversion = [conversion.reverse_transform() for conversion in weight_conversions]\n+    # If we are still here, we need to create the (reverse) conversion mapping from scratch\n+    renamings = [entry for entry in reverse_weight_conversion if isinstance(entry, WeightRenaming)]\n+    converters = [entry for entry in reverse_weight_conversion if isinstance(entry, WeightConverter)]\n+    pattern_to_converter = {k: converter for converter in converters for k in converter.source_patterns}\n+    conversion_mapping = {}\n+\n+    # build '(?P<g0>.*.*\\\\.block_sparse_moe\\\\..*)' and group to source {'g0': '*.block_sparse_moe.'}\n+    # and target to source {'g0': '*.mlp.'}. This allows us to quickly find which pattern matched.\n+    rename_alt, _, rename_by_group = build_glob_alternation(renamings)\n+    weight_pattern_alt, src_group_to_glob, tgt_group_to_glob = None, None, None\n+    if converters != []:\n+        weight_pattern_alt, src_group_to_glob, tgt_group_to_glob = build_glob_alternation(converters)\n+\n+    state_dict = sorted(state_dict.items(), key=lambda kv: dot_natural_key(kv[0]))\n+    for original_key, tensor in state_dict:\n+        # Rename the key according to all renaming pattern and optional weight converter patterns\n+        renamed_key, matched_pattern = rename_source_key(\n+            original_key, rename_alt, rename_by_group, weight_pattern_alt, tgt_group_to_glob\n+        )\n+        if matched_pattern is not None:\n+            new_converter = deepcopy(pattern_to_converter[src_group_to_glob[matched_pattern.lastgroup]])\n+            # each target key gets its own converter instance\n+            mapping = conversion_mapping.setdefault(renamed_key, new_converter)\n+            source_pattern = src_group_to_glob[matched_pattern.lastgroup]\n+        else:\n+            mapping = conversion_mapping.setdefault(renamed_key, WeightRenaming(original_key, renamed_key))\n+            source_pattern = original_key\n+\n+        mapping.add_tensor(renamed_key, original_key, source_pattern, tensor)\n+\n+    new_state_dict = {}\n+    for first_param_name, reversed_converter in conversion_mapping.items():\n+        # Apply the reverse converter\n+        realized_value, misc = reversed_converter.convert(first_param_name, model=model, config=model.config)\n+        for target_name, param in realized_value.items():\n+            param = param[0] if isinstance(param, list) else param\n+            new_state_dict[target_name] = param\n+\n+    return new_state_dict"
        },
        {
            "sha": "8c2a8b9249cb267952ce6e560e4ce5e6bdb781bb",
            "filename": "src/transformers/integrations/finegrained_fp8.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d08b98b965176ea9cf8c8e8b24995c955b7e2ec9/src%2Ftransformers%2Fintegrations%2Ffinegrained_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d08b98b965176ea9cf8c8e8b24995c955b7e2ec9/src%2Ftransformers%2Fintegrations%2Ffinegrained_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ffinegrained_fp8.py?ref=d08b98b965176ea9cf8c8e8b24995c955b7e2ec9",
            "patch": "@@ -577,8 +577,6 @@ class Fp8Quantize(ConversionOps):\n     A quantization operation that creates two tensors, weight and scale out of a weight.\n     \"\"\"\n \n-    reverse_op: type[ConversionOps]\n-\n     def __init__(self, hf_quantizer):\n         self.hf_quantizer = hf_quantizer\n         self.reverse_op = Fp8Dequantize"
        },
        {
            "sha": "22a776a7ec741734076fd5b7846519cc21a33ad8",
            "filename": "src/transformers/integrations/torchao.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d08b98b965176ea9cf8c8e8b24995c955b7e2ec9/src%2Ftransformers%2Fintegrations%2Ftorchao.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d08b98b965176ea9cf8c8e8b24995c955b7e2ec9/src%2Ftransformers%2Fintegrations%2Ftorchao.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ftorchao.py?ref=d08b98b965176ea9cf8c8e8b24995c955b7e2ec9",
            "patch": "@@ -94,7 +94,7 @@ def convert(\n \n         module, tensor_name = get_module_from_name(model, full_layer_name)\n \n-        module._parameters[tensor_name] = torch.nn.Parameter(value, requires_grad=value.requires_grad).to(value.device)\n+        module._parameters[tensor_name] = torch.nn.Parameter(value, requires_grad=value.requires_grad)\n         # if we are quantizing tied parameters, to avoid tying the quantized weights\n         # the correct order to do it is\n         # 1. load the weight to model"
        },
        {
            "sha": "dcb1b821c34910d4bcf2c74a85e78040e53f3860",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 7,
            "deletions": 75,
            "changes": 82,
            "blob_url": "https://github.com/huggingface/transformers/blob/d08b98b965176ea9cf8c8e8b24995c955b7e2ec9/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d08b98b965176ea9cf8c8e8b24995c955b7e2ec9/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=d08b98b965176ea9cf8c8e8b24995c955b7e2ec9",
            "patch": "@@ -46,7 +46,7 @@\n \n from . import initialization as init\n from .configuration_utils import PreTrainedConfig\n-from .conversion_mapping import get_checkpoint_conversion_mapping\n+from .conversion_mapping import get_model_conversion_mapping\n from .core_model_loading import (\n     WeightConverter,\n     WeightRenaming,\n@@ -185,31 +185,6 @@ def is_local_dist_rank_0():\n     \"orthogonal_\": nn.init.orthogonal_,\n }\n \n-# DO NOT MODIFY, KEPT FOR BC ONLY\n-VLMS = [\n-    \"aria\",\n-    \"ayavision\",\n-    \"colpali\",\n-    \"emu3\",\n-    \"fuyu\",\n-    \"gotocr2\",\n-    \"gemma3\",\n-    \"internvl\",\n-    \"llava\",  # all llava prefixed models fall under this check\n-    \"mistral3\",\n-    \"mllama\",\n-    \"paligemma\",\n-    \"shieldgemma2\",\n-    \"qwen2vl\",\n-    \"qwen2_5_vl\",\n-    \"videollava\",\n-    \"vipllava\",\n-    \"sam3_video\",\n-    \"sam3\",\n-    \"sam3_tracker\",\n-    \"sam3_tracker_video\",\n-]\n-\n \n @contextmanager\n def no_init_weights():\n@@ -2957,7 +2932,7 @@ def save_pretrained(\n         variant: Optional[str] = None,\n         token: Optional[Union[str, bool]] = None,\n         save_peft_format: bool = True,\n-        save_original_format: bool = False,  # TODO next PR will make it go to True\n+        save_original_format: bool = True,\n         **kwargs,\n     ):\n         \"\"\"\n@@ -3145,9 +3120,6 @@ def save_pretrained(\n                 if ignore_key in state_dict:\n                     del state_dict[ignore_key]\n \n-        # Rename state_dict keys before saving to file. Do nothing unless overridden in a particular model.\n-        # (initially introduced with TimmWrapperModel to remove prefix and make checkpoints compatible with timm)\n-        state_dict = self._fix_state_dict_keys_on_save(state_dict)\n         # If model was sharded, we cannot properly determine sizes of tensors that `local_*` strategy was used,\n         # therefore we replace them with DTensors that are equivalently sharded\n         if self._tp_size is not None:\n@@ -3223,17 +3195,8 @@ def save_pretrained(\n                     \"This can also just mean that the module's tied weight keys are wrong vs the actual tied weights in the model.\",\n                 )\n \n-        if (\n-            any(\n-                allowed_name in class_name.__name__.lower()\n-                for class_name in self.__class__.__mro__[:-1]\n-                for allowed_name in VLMS\n-            )\n-            or save_original_format\n-        ):\n-            # MEGA BIG TODO HERE: self._conversion_ops needs to be used to save the final ckpt\n-            # using what was loaded. Actually self._conversion_ops wont work because we need it\n-            # even if the files are not legacy -> thus no conversion happened\n+        # Revert all renaming and/or weight operations\n+        if save_original_format:\n             state_dict = revert_weight_conversion(self, state_dict)\n \n         # Shard the model if it is too big.\n@@ -3783,13 +3746,7 @@ def from_pretrained(\n         trust_remote_code = kwargs.pop(\"trust_remote_code\", None)\n         use_kernels = kwargs.pop(\"use_kernels\", False)\n         kernel_config = kwargs.pop(\"kernel_config\", None)\n-\n         key_mapping = kwargs.pop(\"key_mapping\", None)\n-        # Load models with key mapping\n-        if key_mapping is None and any(\n-            allowed_name in class_name.__name__.lower() for class_name in cls.__mro__[:-1] for allowed_name in VLMS\n-        ):\n-            key_mapping = copy.copy(cls._checkpoint_conversion_mapping)\n \n         if distributed_config is not None and tp_plan is None:\n             tp_plan = \"auto\"\n@@ -3881,19 +3838,6 @@ def from_pretrained(\n             config, quantization_config, dtype, device_map, weights_only, user_agent\n         )\n \n-        weight_conversions: Optional[list[WeightConverter | WeightRenaming]] = None\n-        model_type = getattr(config, \"model_type\", None)\n-        if model_type is not None:\n-            weight_conversions = get_checkpoint_conversion_mapping(model_type)\n-            if weight_conversions is None:\n-                weight_conversions = get_checkpoint_conversion_mapping(\"legacy\")\n-            if key_mapping is not None:\n-                weight_conversions.extend(\n-                    [WeightRenaming(source_keys=k, target_keys=v) for k, v in key_mapping.items()]\n-                )\n-            if hf_quantizer is not None:\n-                weight_conversions.extend(hf_quantizer.get_weight_conversions())\n-\n         if gguf_file:\n             if hf_quantizer is not None:\n                 raise ValueError(\n@@ -3949,6 +3893,9 @@ def from_pretrained(\n             # Let's make sure we don't run the init function of buffer modules\n             model = cls(config, *model_args, **model_kwargs)\n \n+        # Obtain the weight conversion mapping for this model if any are registered\n+        weight_conversions = get_model_conversion_mapping(model, key_mapping, hf_quantizer)\n+\n         # make sure we use the model's config since the __init__ call might have copied it\n         config = model.config\n \n@@ -4033,21 +3980,6 @@ def from_pretrained(\n             return model, loading_info\n         return model\n \n-    @staticmethod\n-    def _fix_state_dict_key_on_save(key) -> tuple[str, bool]:\n-        \"\"\"\n-        Similar to `_fix_state_dict_key_on_load` allows to define hook for state dict key renaming on model save.\n-        Do nothing by default, but can be overridden in particular models.\n-        \"\"\"\n-        return key, False\n-\n-    def _fix_state_dict_keys_on_save(self, state_dict):\n-        \"\"\"\n-        Similar to `_fix_state_dict_keys_on_load` allows to define hook for state dict key renaming on model save.\n-        Apply `_fix_state_dict_key_on_save` to all keys in `state_dict`.\n-        \"\"\"\n-        return {self._fix_state_dict_key_on_save(key)[0]: value for key, value in state_dict.items()}\n-\n     @classmethod\n     def _load_pretrained_model(\n         cls,"
        },
        {
            "sha": "524da4b16f1d0b296d46a262dbdb6f9c432d1728",
            "filename": "src/transformers/models/granite_speech/modeling_granite_speech.py",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/d08b98b965176ea9cf8c8e8b24995c955b7e2ec9/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fmodeling_granite_speech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d08b98b965176ea9cf8c8e8b24995c955b7e2ec9/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fmodeling_granite_speech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fmodeling_granite_speech.py?ref=d08b98b965176ea9cf8c8e8b24995c955b7e2ec9",
            "patch": "@@ -540,20 +540,6 @@ def save_pretrained(self, save_directory, *args, **kwargs):\n         super().save_pretrained(save_directory, *args, **kwargs)\n         self._hf_peft_config_loaded = prev_val\n \n-    @staticmethod\n-    def _fix_state_dict_key_on_save(key) -> tuple[str, bool]:\n-        # save the model with the original weights format\n-        return key.replace(\".base_layer\", \"\"), False\n-\n-    def _fix_state_dict_keys_on_save(self, state_dict):\n-        if is_peft_available and self._hf_peft_config_loaded:\n-            # state dict is only adapter, should keep the same\n-            return state_dict\n-        # rename back the base model state dict\n-        return {\n-            self._fix_state_dict_key_on_save(key)[0]: value for key, value in state_dict.items() if \".lora_\" not in key\n-        }\n-\n     def _get_adapter_name(self):\n         return list(self.peft_config.keys())[0]\n "
        },
        {
            "sha": "e7dca5068ebcc6b6c05db0a6a2872cca296b67a8",
            "filename": "src/transformers/models/timm_wrapper/modeling_timm_wrapper.py",
            "status": "modified",
            "additions": 1,
            "deletions": 19,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/d08b98b965176ea9cf8c8e8b24995c955b7e2ec9/src%2Ftransformers%2Fmodels%2Ftimm_wrapper%2Fmodeling_timm_wrapper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d08b98b965176ea9cf8c8e8b24995c955b7e2ec9/src%2Ftransformers%2Fmodels%2Ftimm_wrapper%2Fmodeling_timm_wrapper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftimm_wrapper%2Fmodeling_timm_wrapper.py?ref=d08b98b965176ea9cf8c8e8b24995c955b7e2ec9",
            "patch": "@@ -98,30 +98,12 @@ def post_init(self):\n         self.supports_gradient_checkpointing = self._timm_model_supports_gradient_checkpointing()\n         super().post_init()\n \n-    @staticmethod\n-    def _fix_state_dict_key_on_load(key) -> tuple[str, bool]:\n-        \"\"\"\n-        Overrides original method that renames `gamma` and `beta` to `weight` and `bias`.\n-        We don't want this behavior for timm wrapped models. Instead, this method adds a\n-        \"timm_model.\" prefix to enable loading official timm Hub checkpoints.\n-        \"\"\"\n-        if \"timm_model.\" not in key:\n-            return f\"timm_model.{key}\", True\n-        return key, False\n-\n-    def _fix_state_dict_key_on_save(self, key):\n-        \"\"\"\n-        Overrides original method to remove \"timm_model.\" prefix from state_dict keys.\n-        Makes the saved checkpoint compatible with the `timm` library.\n-        \"\"\"\n-        return key.replace(\"timm_model.\", \"\"), True\n-\n     def load_state_dict(self, state_dict, *args, **kwargs):\n         \"\"\"\n         Override original method to fix state_dict keys on load for cases when weights are loaded\n         without using the `from_pretrained` method (e.g., in Trainer to resume from checkpoint).\n         \"\"\"\n-        state_dict = {self._fix_state_dict_key_on_load(k)[0]: v for k, v in state_dict.items()}\n+        state_dict = {f\"timm_model.{k}\" if \"timm_model.\" not in k else k: v for k, v in state_dict.items()}\n         return super().load_state_dict(state_dict, *args, **kwargs)\n \n     @torch.no_grad()"
        },
        {
            "sha": "e7919b7f81b7952ab6684d0d926b58ffb1177a47",
            "filename": "src/transformers/quantizers/quantizer_torchao.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d08b98b965176ea9cf8c8e8b24995c955b7e2ec9/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d08b98b965176ea9cf8c8e8b24995c955b7e2ec9/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py?ref=d08b98b965176ea9cf8c8e8b24995c955b7e2ec9",
            "patch": "@@ -548,13 +548,13 @@ def get_weight_conversions(self):\n         if self.pre_quantized:\n             return [\n                 WeightConverter(\n-                    source_keys=[\"weight:qdata\", \"weight:scale\", \"weight:zero_point\"],\n-                    target_keys=\"weight\",\n+                    source_patterns=[\"weight:qdata\", \"weight:scale\", \"weight:zero_point\"],\n+                    target_patterns=\"weight\",\n                     operations=[TorchAoDeserialize(self)],\n                 ),\n                 WeightConverter(\n-                    source_keys=[\"weight:_data\"],\n-                    target_keys=\"weight\",\n+                    source_patterns=[\"weight:_data\"],\n+                    target_patterns=\"weight\",\n                     operations=[TorchAoDeserialize(self)],\n                 ),\n                 # used for unsafe serialization"
        },
        {
            "sha": "c1f7c1b83ed0bd02741bbae41a9ed8bce864d754",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 122,
            "deletions": 0,
            "changes": 122,
            "blob_url": "https://github.com/huggingface/transformers/blob/d08b98b965176ea9cf8c8e8b24995c955b7e2ec9/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d08b98b965176ea9cf8c8e8b24995c955b7e2ec9/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=d08b98b965176ea9cf8c8e8b24995c955b7e2ec9",
            "patch": "@@ -31,6 +31,7 @@\n from packaging import version\n from parameterized import parameterized\n from pytest import mark\n+from safetensors.torch import load_file\n \n from transformers import (\n     AutoModel,\n@@ -42,6 +43,8 @@\n     logging,\n     set_seed,\n )\n+from transformers.conversion_mapping import get_model_conversion_mapping\n+from transformers.core_model_loading import WeightRenaming\n from transformers.integrations import HfDeepSpeedConfig\n from transformers.integrations.deepspeed import (\n     is_deepspeed_available,\n@@ -4055,10 +4058,129 @@ def test_tp_plan_matches_params(self):\n                 len(unused_entries) == 0, f\"The following entries of the TP-plan are not valid: {unused_entries}\"\n             )\n \n+    @unittest.skip(\"Some models have wrong mappings....\")\n+    def test_reverse_loading_mapping(self):\n+        \"\"\"Make sure we can load and save correctly the models having any weight renaming mapping or weight conversion\n+        mapping.\n+        Note that this test would be better if we could start from the serialized keys, and check that the model\n+        keys correspond to the weight converions. However, when instantiating a model, it already has the \"target\"\n+        keys (or modified keys after mapping) of the conversion mapping, so we have to do it the other way, i.e.\n+        reverse the conversion and then check that those converted keys match correctly the conversions.\n+\n+        However, all the checks performed here should ensure everything is going as it should.\n+        \"\"\"\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        #  Some MoE models alternate between a classic MLP and a MoE layer, in which case we want to have at\n+        # lest one MoE layer here to check the mapping\n+        config_to_set = config.get_text_config()\n+        config_to_set.first_k_dense_replace = 1  # means that the first layer (idx 0) will be MLP, then MoE\n+        config_to_set.moe_layer_start_index = 1  # same as above but for Ernie 4.5...\n+        config_to_set.mlp_only_layers = [0]  # same but for qwens\n+        config_to_set.num_dense_layers = 1  # lfm2_moe\n+\n+        for model_class in self.all_model_classes:\n+            # Each individual model is a subtest\n+            with self.subTest(model_class.__name__):\n+                model = model_class(copy.deepcopy(config))\n+                # Skip if no conversions\n+                conversions = get_model_conversion_mapping(model, add_legacy=False)\n+                if len(conversions) == 0:\n+                    self.skipTest(\"No conversion found for this model\")\n+\n+                # Find the model keys, so the targets according to the conversions\n+                model_keys = list(model.state_dict().keys())\n+\n+                with tempfile.TemporaryDirectory() as tmpdirname:\n+                    # Serialize with reverse mapping\n+                    model.save_pretrained(tmpdirname)\n+                    state_dict = load_file(os.path.join(tmpdirname, \"model.safetensors\"))\n+                    # Get all the serialized keys that we just saved according to the reverse mapping\n+                    serialized_keys = list(state_dict.keys())\n+\n+                # They should be different, otherwise we did not perform any mapping\n+                self.assertNotEqual(sorted(serialized_keys), sorted(model_keys), \"No key mapping was performed!\")\n+\n+                # Check that for each conversion entry, we at least map to one key\n+                for conversion in conversions:\n+                    for source_pattern in conversion.source_patterns:\n+                        # Sometimes the mappings specify keys that are tied, so absent from the saved state dict\n+                        if isinstance(conversion, WeightRenaming):\n+                            if any(\n+                                re.search(conversion.target_patterns[0], k) for k in model.all_tied_weights_keys.keys()\n+                            ):\n+                                continue\n+                        num_matches = sum(re.search(source_pattern, key) is not None for key in serialized_keys)\n+                        self.assertTrue(\n+                            num_matches > 0,\n+                            f\"`{source_pattern}` in `{conversion}` did not match any of the source keys. \"\n+                            \"This indicates whether that the pattern is not properly written, ot that it could not be reversed correctly\",\n+                        )\n+\n+                # If everything is still good at this point, let's test that we perform the same operations both when\n+                # reverting ops from `from_pretrained` and from `__init__`\n+                with tempfile.TemporaryDirectory() as tmpdirname:\n+                    # The model was instantiated from __init__ before being saved\n+                    model.save_pretrained(tmpdirname)\n+                    state_dict_saved_from_init = load_file(os.path.join(tmpdirname, \"model.safetensors\"))\n+\n+                    # Now reload it\n+                    model_reloaded = model_class.from_pretrained(tmpdirname)\n+\n+                    # Make sure both loaded state_dict are identical\n+                    self.assertTrue(compare_state_dicts(model_reloaded.state_dict(), model.state_dict()))\n+\n+                    # The model was instantiated from `from_pretrained` before being saved\n+                    model_reloaded.save_pretrained(tmpdirname)\n+                    state_dict_saved_from_pretrained = load_file(os.path.join(tmpdirname, \"model.safetensors\"))\n+\n+                    # Make sure both saved state_dict are identical\n+                    self.assertTrue(compare_state_dicts(state_dict_saved_from_init, state_dict_saved_from_pretrained))\n+\n+    @unittest.skip(\"Some models have wrong mappings....\")\n+    def test_can_load_from_already_mapped_keys(self):\n+        \"\"\"Test that we can correctly reload a model if we chose `save_original_format=False` in `save_pretrained`,\n+        i.e. we do not reapply weight conversions when reloading if it was saved correctly already.\n+        \"\"\"\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            # Each individual model is a subtest\n+            with self.subTest(model_class.__name__):\n+                model = model_class(copy.deepcopy(config))\n+\n+                # Skip if no conversions\n+                conversions = get_model_conversion_mapping(model, add_legacy=False)\n+                if len(conversions) == 0:\n+                    self.skipTest(\"No conversion found for this model\")\n+\n+                with tempfile.TemporaryDirectory() as tmpdirname:\n+                    # Serialize without reverting the mapping\n+                    model.save_pretrained(tmpdirname, save_original_format=False)\n+                    model_reloaded = model_class.from_pretrained(tmpdirname)\n+                    # Make sure both saved state_dict are identical\n+                    self.assertTrue(compare_state_dicts(model.state_dict(), model_reloaded.state_dict()))\n+\n \n global_rng = random.Random()\n \n \n+def compare_state_dicts(state_dict1, state_dict2) -> bool:\n+    \"\"\"Make sure 2 state dicts are the exact same\"\"\"\n+    # Make sure the keys are the exact same\n+    if sorted(state_dict1.keys()) != sorted(state_dict2.keys()):\n+        raise ValueError(\"The keys of both state dict are not the same\")\n+\n+    for k, v1 in state_dict1.items():\n+        v2 = state_dict2[k]\n+        try:\n+            torch.testing.assert_close(v1, v2)\n+        except Exception as e:\n+            raise AssertionError(f\"For key {k}: {e}\")\n+\n+    return True\n+\n+\n def ids_tensor(shape, vocab_size, rng=None, name=None):\n     #  Creates a random int32 tensor of the shape within the vocab size\n     if rng is None:"
        },
        {
            "sha": "88bdb27256bae802650bd76e319339f297881b2b",
            "filename": "tests/utils/test_core_model_loading.py",
            "status": "modified",
            "additions": 67,
            "deletions": 3,
            "changes": 70,
            "blob_url": "https://github.com/huggingface/transformers/blob/d08b98b965176ea9cf8c8e8b24995c955b7e2ec9/tests%2Futils%2Ftest_core_model_loading.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d08b98b965176ea9cf8c8e8b24995c955b7e2ec9/tests%2Futils%2Ftest_core_model_loading.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_core_model_loading.py?ref=d08b98b965176ea9cf8c8e8b24995c955b7e2ec9",
            "patch": "@@ -1,4 +1,4 @@\n-# Copyright 2024 HuggingFace Inc.\n+# Copyright 2025 HuggingFace Inc.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -28,9 +28,12 @@\n     build_glob_alternation,\n     convert_and_load_state_dict_in_model,\n     repl,\n+    revert_weight_conversion,\n )\n from transformers.utils.import_utils import is_triton_available\n \n+from ..test_modeling_common import compare_state_dicts\n+\n \n class TestWeightGlobMatching(unittest.TestCase):\n     def setUp(self):\n@@ -246,7 +249,7 @@ def test_moe_and_qkv_conversion(self):\n                     \"model.layers.0.self_attn.k_proj.weight\",\n                     \"model.layers.0.self_attn.v_proj.weight\",\n                 ],\n-                operations=[Chunk(dim=0, chunks=3)],\n+                operations=[Chunk(dim=0)],\n             ),\n             WeightRenaming(\"mlp.w2.weight\", \"mlp.down_proj.weight\"),\n         ]\n@@ -315,6 +318,67 @@ def stack_down(layer_prefix: str) -> torch.Tensor:\n \n         torch.testing.assert_close(model_state[\"mlp.down_proj.weight\"], raw_tensors[\"mlp.w2.weight\"])\n \n+    def test_moe_and_qkv_conversion_reversed(self):\n+        model = DummyRoot()\n+        model.config = PretrainedConfig()\n+\n+        raw_tensors = {\n+            \"model.layers.0.experts.0.w1.weight\": torch.tensor([[0.0, 1.0], [2.0, 3.0]]),\n+            \"model.layers.0.experts.1.w1.weight\": torch.tensor([[10.0, 11.0], [12.0, 13.0]]),\n+            \"model.layers.0.experts.0.w3.weight\": torch.tensor([[4.0, 5.0], [6.0, 7.0]]),\n+            \"model.layers.0.experts.1.w3.weight\": torch.tensor([[14.0, 15.0], [16.0, 17.0]]),\n+            \"model.layers.0.experts.0.w2.weight\": torch.tensor([[20.0, 21.0], [22.0, 23.0]]),\n+            \"model.layers.0.experts.1.w2.weight\": torch.tensor([[24.0, 25.0], [26.0, 27.0]]),\n+            \"model.layers.1.experts.0.w1.weight\": torch.tensor([[30.0, 31.0], [32.0, 33.0]]),\n+            \"model.layers.1.experts.1.w1.weight\": torch.tensor([[34.0, 35.0], [36.0, 37.0]]),\n+            \"model.layers.1.experts.0.w3.weight\": torch.tensor([[38.0, 39.0], [40.0, 41.0]]),\n+            \"model.layers.1.experts.1.w3.weight\": torch.tensor([[42.0, 43.0], [44.0, 45.0]]),\n+            \"model.layers.1.experts.0.w2.weight\": torch.tensor([[46.0, 47.0], [48.0, 49.0]]),\n+            \"model.layers.1.experts.1.w2.weight\": torch.tensor([[50.0, 51.0], [52.0, 53.0]]),\n+            \"model.layers.0.self_attn.qkv_proj.weight\": torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]),\n+            \"model.layers.1.self_attn.qkv_proj.weight\": torch.tensor([[7.0, 8.0], [9.0, 10.0], [11.0, 12.0]]),\n+            \"mlp.w2.weight\": torch.tensor([[60.0, 61.0], [62.0, 63.0]]),\n+        }\n+        state_dict = {k: v.clone() for k, v in raw_tensors.items()}\n+\n+        weight_mapping = [\n+            WeightConverter(\n+                [\"experts.*.w1.weight\", \"experts.*.w3.weight\"],\n+                \"experts.gate_up_proj.weight\",\n+                operations=[MergeModulelist(dim=0), Concatenate(dim=1)],\n+            ),\n+            WeightConverter(\n+                \"experts.*.w2.weight\",\n+                \"experts.down_proj.weight\",\n+                operations=[MergeModulelist(dim=0)],\n+            ),\n+            WeightConverter(\n+                \"self_attn.qkv_proj.weight\",\n+                [\n+                    \"self_attn.q_proj.weight\",\n+                    \"self_attn.k_proj.weight\",\n+                    \"self_attn.v_proj.weight\",\n+                ],\n+                operations=[Chunk(dim=0)],\n+            ),\n+            WeightRenaming(\"mlp.w2.weight\", \"mlp.down_proj.weight\"),\n+        ]\n+\n+        # Use the mapping to load\n+        missing, unexpected, mismatch, _, misc = convert_and_load_state_dict_in_model(\n+            model, state_dict, weight_mapping, tp_plan=None, hf_quantizer=None\n+        )\n+        self.assertTrue(len(missing) == 0)\n+        self.assertTrue(len(unexpected) == 0)\n+        self.assertTrue(len(mismatch) == 0)\n+        self.assertTrue(len(misc) == 0)\n+\n+        # Try to revert the mapping\n+        reversed_state_dict = revert_weight_conversion(model, model.state_dict())\n+\n+        # Make sure both saved state_dict are identical\n+        self.assertTrue(compare_state_dicts(reversed_state_dict, state_dict))\n+\n     def test_qkv_chunk_rope_permute_with_fp8_quantization(self):\n         if is_triton_available():\n             from transformers.integrations.finegrained_fp8 import Fp8Dequantize\n@@ -396,7 +460,7 @@ def __init__(self):\n                     \"model.layers.*.self_attn.k_proj.weight\",\n                     \"model.layers.*.self_attn.v_proj.weight\",\n                 ],\n-                operations=[Chunk(dim=0, chunks=3), PermuteForRope()],\n+                operations=[Chunk(dim=0), PermuteForRope()],\n             )\n         ]\n "
        }
    ],
    "stats": {
        "total": 959,
        "additions": 643,
        "deletions": 316
    }
}