{
    "author": "gante",
    "message": "[serve] Add speech to text (`/v1/audio/transcriptions`) (#39434)\n\n* Scaffolding\n\n* Explicit content\n\n* Naïve Responses API streaming implementation\n\n* Cleanup\n\n* Scaffolding\n\n* Explicit content\n\n* Naïve Responses API streaming implementation\n\n* Cleanup\n\n* use openai\n\n* validate request, including detecting unused fields\n\n* dict indexing\n\n* dict var access\n\n* tmp commit (tests failing)\n\n* add slow\n\n* use oai output type in completions\n\n* (little rebase errors)\n\n* working spec?\n\n* guard type hint\n\n* type hints. fix state (CB can now load different models)\n\n* type hints; fn names; error type\n\n* add docstrings\n\n* responses + kv cache\n\n* metadata support; fix kv cache; error event\n\n* add output_index and content_index\n\n* docstrings\n\n* add test_build_response_event\n\n* docs/comments\n\n* gate test requirements; terminate cb manager on model switch\n\n* nasty type hints\n\n* more type hints\n\n* disable validation by default; enable force models\n\n* todo\n\n* experiment: base model from typed dict\n\n* audio working\n\n* fix bad rebase\n\n* load audio with librosa\n\n* implement timed models\n\n* almost working\n\n* make fixup\n\n* fix tests\n\n* transcription request type\n\n* tokenizer -> processor\n\n* add example in docs\n\n---------\n\nCo-authored-by: Lysandre <hi@lysand.re>",
    "sha": "bf6c9976851627b545f071d4861c54cda7fdfe6b",
    "files": [
        {
            "sha": "cad4cbeb414ca72ae7f5b2ddd0bb92ac7a339000",
            "filename": "docs/source/en/serving.md",
            "status": "modified",
            "additions": 35,
            "deletions": 7,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf6c9976851627b545f071d4861c54cda7fdfe6b/docs%2Fsource%2Fen%2Fserving.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf6c9976851627b545f071d4861c54cda7fdfe6b/docs%2Fsource%2Fen%2Fserving.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fserving.md?ref=bf6c9976851627b545f071d4861c54cda7fdfe6b",
            "patch": "@@ -70,8 +70,13 @@ vllm serve Qwen/Qwen2.5-1.5B-Instruct \\\n > [!WARNING]\n > This section is experimental and subject to change in future versions\n \n-<!-- TODO: LLMs -> models, after we add audio/image input/output support -->\n-You can serve LLMs supported by `transformers` with the `transformers serve` CLI. It spawns a local server that offers a Chat Completion API or a Response API compatible with the OpenAI SDK, which are the _de facto_ standard for LLM conversations. This way, you can use the server from many third party applications, or test it using the `transformers chat` CLI ([docs](conversations.md#chat-cli)).\n+You can serve models of diverse modalities supported by `transformers` with the `transformers serve` CLI. It spawns a local server that offers compatibility with the OpenAI SDK, which is the _de facto_ standard for LLM conversations and other related tasks. This way, you can use the server from many third party applications, or test it using the `transformers chat` CLI ([docs](conversations.md#chat-cli)).\n+\n+The server supports the following REST APIs:\n+- `/v1/chat/completions`\n+- `/v1/responses`\n+- `/v1/audio/transcriptions`\n+- `/v1/models`\n \n To launch a server, simply use the `transformers serve` CLI command:\n \n@@ -109,7 +114,7 @@ The server is also an MCP client, so it can interact with MCP tools in agentic u\n <!-- TODO: example with a minimal python example, and explain that it is possible to pass a full generation config in the request -->\n \n \n-### Usage example 1: apps with local requests (feat. Jan)\n+### Usage example 1: chat with local requests (feat. Jan)\n \n This example shows how to use `transformers serve` as a local LLM provider for the [Jan](https://jan.ai/) app. Jan is a ChatGPT-alternative graphical interface, fully running on your machine. The requests to `transformers serve` come directly from the local app -- while this section focuses on Jan, you can extrapolate some instructions to other apps that make local requests.\n \n@@ -139,17 +144,17 @@ ssh -N -f -L 8000:localhost:8000 your_server_account@your_server_IP -p port_to_s\n Port forwarding is not Jan-specific: you can use it to connect `transformers serve` running in a different machine with an app of your choice.\n \n \n-### Usage example 2: apps with external requests (feat. Cursor)\n+### Usage example 2: chat with external requests (feat. Cursor)\n \n This example shows how to use `transformers serve` as a local LLM provider for [Cursor](https://cursor.com/), the popular IDE. Unlike in the previous example, requests to `transformers serve` will come from an external IP (Cursor's server IPs), which requires some additional setup. Furthermore, some of Cursor's requests require [CORS](https://developer.mozilla.org/en-US/docs/Web/HTTP/Guides/CORS), which is disabled by default for security reasons.\n \n-To launch our server with CORS enabled, run\n+To launch a server with CORS enabled, run\n \n ```shell\n transformers serve --enable-cors\n ```\n \n-We'll also need to expose our server to external IPs. A potential solution is to use [`ngrok`](https://ngrok.com/), which has a permissive free tier. After setting up your `ngrok` account and authenticating on your server machine, you run\n+You'll also need to expose your server to external IPs. A potential solution is to use [`ngrok`](https://ngrok.com/), which has a permissive free tier. After setting up your `ngrok` account and authenticating on your server machine, you run\n \n ```shell\n ngrok http [port]\n@@ -161,7 +166,7 @@ where `port` is the port used by `transformers serve` (`8000` by default). On th\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/transformers_serve_ngrok.png\"/>\n </h3>\n \n-We're now ready to set things up on the app side! In Cursor, while we can't set a new provider, we can change the endpoint for OpenAI requests in the model selection settings. First, navigate to \"Settings\" > \"Cursor Settings\", \"Models\" tab, and expand the \"API Keys\" collapsible. To set our `transformers serve` endpoint, follow this order:\n+You're now ready to set things up on the app side! In Cursor, while you can't set a new provider, you can change the endpoint for OpenAI requests in the model selection settings. First, navigate to \"Settings\" > \"Cursor Settings\", \"Models\" tab, and expand the \"API Keys\" collapsible. To set your `transformers serve` endpoint, follow this order:\n 1. Unselect ALL models in the list above (e.g. `gpt4`, ...);\n 2. Add and select the model you want to use (e.g. `Qwen/Qwen3-4B`)\n 3. Add some random text to OpenAI API Key. This field won't be used, but it can’t be empty;\n@@ -225,3 +230,26 @@ Image URL: https://evalstate-flux1-schnell.hf.space/gradio_api/file=/tmp/gradio/\n \n I have generated an image of a cat on the moon using the Flux 1 Schnell Image Generator. The image is 1024x1024 pixels and was created with 4 inference steps. Let me know if you would like to make any changes or need further assistance!\n ```\n+\n+### Usage example 4: speech to text transcription (feat. Open WebUI)\n+\n+This guide shows how to do audio transcription for chat purposes, using `transformers serve` and [Open WebUI](https://openwebui.com/). This guide assumes you have Open WebUI installed on your machine and ready to run. Please refer to the examples above to use the text functionalities of `transformer serve` with Open WebUI -- the instructions are the same.\n+\n+To start, let's launch the server. Some of Open WebUI's requests require [CORS](https://developer.mozilla.org/en-US/docs/Web/HTTP/Guides/CORS), which is disabled by default for security reasons, so you need to enable it:\n+\n+```shell\n+transformers serve --enable-cors\n+```\n+\n+Before you can speak into Open WebUI, you need to update its settings to use your server for speech to text (STT) tasks. Launch Open WebUI, and navigate to the audio tab inside the admin settings. If you're using Open WebUI with the default ports, [this link (default)](http://localhost:3000/admin/settings/audio) or [this link (python deployment)](http://localhost:8080/admin/settings/audio) will take you there. Do the following changes there:\n+1. Change the type of \"Speech-to-Text Engine\" to \"OpenAI\";\n+2. Update the address to your server's address -- `http://localhost:8000/v1` by default;\n+3. Type your model of choice into the \"STT Model\" field, e.g. `openai/whisper-large-v3` ([available models](https://huggingface.co/models?pipeline_tag=automatic-speech-recognition&sort=trending)).\n+\n+If you've done everything correctly, the audio tab should look like this\n+\n+<h3 align=\"center\">\n+    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/transformers_openwebui_stt_settings.png\"/>\n+</h3>\n+\n+You're now ready to speak! Open a new chat, utter a few words after hitting the microphone button, and you should see the corresponding text on the chat input after the model transcribes it."
        },
        {
            "sha": "3209f3c8aea1d0a385b111b3c36e1b0c9b642e94",
            "filename": "src/transformers/commands/serving.py",
            "status": "modified",
            "additions": 316,
            "deletions": 92,
            "changes": 408,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf6c9976851627b545f071d4861c54cda7fdfe6b/src%2Ftransformers%2Fcommands%2Fserving.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf6c9976851627b545f071d4861c54cda7fdfe6b/src%2Ftransformers%2Fcommands%2Fserving.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcommands%2Fserving.py?ref=bf6c9976851627b545f071d4861c54cda7fdfe6b",
            "patch": "@@ -14,24 +14,28 @@\n \n import copy\n import functools\n+import gc\n+import io\n import json\n import re\n+import threading\n import time\n from argparse import ArgumentParser, Namespace\n from dataclasses import dataclass, field\n from threading import Thread\n-from typing import Generator, Optional\n+from typing import Generator, Optional, Union\n \n from huggingface_hub import ModelInfo, model_info\n \n from transformers.utils.import_utils import (\n     is_fastapi_available,\n+    is_librosa_available,\n     is_openai_available,\n     is_pydantic_available,\n     is_uvicorn_available,\n )\n \n-from .. import LogitsProcessorList, PreTrainedTokenizerFast, TextIteratorStreamer\n+from .. import LogitsProcessorList, PreTrainedTokenizerFast, ProcessorMixin, TextIteratorStreamer\n from ..generation.continuous_batching import ContinuousBatchingManager, RequestStatus\n from ..utils import is_torch_available, logging\n from . import BaseTransformersCLICommand\n@@ -42,12 +46,16 @@\n \n     from transformers import (\n         AutoModelForCausalLM,\n-        AutoTokenizer,\n+        AutoModelForSpeechSeq2Seq,\n+        AutoProcessor,\n         BitsAndBytesConfig,\n         GenerationConfig,\n         PreTrainedModel,\n     )\n \n+if is_librosa_available():\n+    import librosa\n+\n serve_dependencies_available = (\n     is_pydantic_available() and is_fastapi_available() and is_uvicorn_available() and is_openai_available()\n )\n@@ -56,6 +64,8 @@\n     from fastapi import FastAPI, HTTPException\n     from fastapi.middleware.cors import CORSMiddleware\n     from fastapi.responses import JSONResponse, StreamingResponse\n+    from openai.types.audio.transcription import Transcription\n+    from openai.types.audio.transcription_create_params import TranscriptionCreateParamsBase\n     from openai.types.chat.chat_completion_chunk import (\n         ChatCompletionChunk,\n         Choice,\n@@ -90,20 +100,28 @@ class TransformersResponseCreateParamsStreaming(ResponseCreateParamsStreaming, t\n         OpenAI's ResponseCreateParamsStreaming with an additional field for the generation config (as a json string).\n         \"\"\"\n \n-        generation_config: Optional[str]\n+        generation_config: str\n \n     class TransformersCompletionCreateParamsStreaming(CompletionCreateParamsStreaming, total=False):\n         \"\"\"\n-        OpenAI's CompletionCreateParamsStreaming with additional fields for the generation config (as a json string)\n-        and the request ID to re-use the previous KV cache.\n+        OpenAI's CompletionCreateParamsStreaming with an additional field for the generation config (as a json string).\n         \"\"\"\n \n-        generation_config: Optional[str]\n-        request_id: Optional[str]\n+        generation_config: str\n+\n+    class TransformersTranscriptionCreateParams(TranscriptionCreateParamsBase, total=False):\n+        \"\"\"\n+        OpenAI's TranscriptionCreateParamsBase with an additional field for the generation config (as a json string).\n+        \"\"\"\n \n-    # Contrarily to OpenAI's output types, input types are `TypedDict`, which don't have validation\n+        file: bytes  # Overwritten -- pydantic isn't happy with `typing.IO[bytes]`, present in the original type\n+        generation_config: str\n+        stream: Optional[bool] = False\n+\n+    # Contrarily to OpenAI's output types, input types are `TypedDict`, which don't have built-in validation.\n     response_validator = TypeAdapter(TransformersResponseCreateParamsStreaming)\n     completion_validator = TypeAdapter(TransformersCompletionCreateParamsStreaming)\n+    transcription_validator = TypeAdapter(TransformersTranscriptionCreateParams)\n \n     # Define request fields that are not yet used in `transformers serve`. Receiving these fields will raise an\n     # HTTPException.\n@@ -146,6 +164,14 @@ class TransformersCompletionCreateParamsStreaming(CompletionCreateParamsStreamin\n         \"user\",\n         \"web_search_options\",\n     }\n+    UNUSED_TRANSCRIPTION_FIELDS = {\n+        \"chunking_strategy\",\n+        \"include\",\n+        \"language\",\n+        \"prompt\",\n+        \"response_format\",\n+        \"timestamp_granularities\",\n+    }\n \n \n logger = logging.get_logger(__name__)\n@@ -226,10 +252,6 @@ def create_generation_config_from_req(\n     if req.get(\"seed\") is not None:\n         torch.manual_seed(req[\"seed\"])\n \n-    # Sets server-specific defaults, if unset\n-    if generation_config.max_new_tokens is None:\n-        generation_config.max_new_tokens = 1024\n-\n     return generation_config\n \n \n@@ -247,6 +269,53 @@ def reset(self):\n         self.buffer = \"\"\n \n \n+class TimedModel:\n+    \"\"\"\n+    A class that holds a PreTrainedModel instance and its associated processor (tokenizer, audio processor, etc.).\n+    Automatically deletes the instances after a specified timeout.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        model: \"PreTrainedModel\",\n+        timeout_seconds: int,\n+        processor: Optional[Union[\"ProcessorMixin\", \"PreTrainedTokenizerFast\"]] = None,\n+    ):\n+        self.model = model\n+        self._name_or_path = str(model.name_or_path)\n+        self.processor = processor\n+        self.timeout_seconds = timeout_seconds\n+        self._timer = threading.Timer(self.timeout_seconds, self._delete_model)\n+        self._timer.start()\n+\n+    def reset_timer(self):\n+        \"\"\"Reset the timer for the deletion of the instances.\"\"\"\n+        self._timer.cancel()\n+        self._timer = threading.Timer(self.timeout_seconds, self._delete_model)\n+        self._timer.start()\n+\n+    def _delete_model(self):\n+        \"\"\"Delete the wrapped model and processor and clean up resources.\"\"\"\n+        if hasattr(self, \"model\") and self.model is not None:\n+            del self.model\n+            del self.processor\n+            self.model = None\n+            self.processor = None\n+            gc.collect()\n+\n+            # Clear CUDA cache if available\n+            if torch.cuda.is_available():\n+                torch.cuda.empty_cache()\n+\n+            logger.info(\n+                f\"{self._name_or_path} was removed from memory after {self.timeout_seconds} seconds of inactivity\"\n+            )\n+\n+    def is_deleted(self):\n+        \"\"\"Check if the instances have been deleted.\"\"\"\n+        return not hasattr(self, \"model\") or self.model is None\n+\n+\n @dataclass\n class ServeArguments:\n     r\"\"\"\n@@ -289,6 +358,10 @@ class ServeArguments:\n     # Serving settings\n     host: str = field(default=\"localhost\", metadata={\"help\": \"Interface the server will listen to.\"})\n     port: int = field(default=8000, metadata={\"help\": \"Port the server will listen to.\"})\n+    model_timeout: int = field(\n+        default=300,\n+        metadata={\"help\": \"Time in seconds after which a model will be removed from memory.\"},\n+    )\n \n     # Other settings\n     log_level: str = field(\n@@ -357,16 +430,15 @@ def __init__(self, args: ServeArguments):\n         cb_logger.setLevel(logging.log_levels[self.args.log_level.lower()])\n \n         # Internal state:\n-        # 1. Tracks the most recently used model, to prevent reloading the model unnecessarily\n-        self.loaded_model: Optional[str] = None\n+        # 1. Tracks models in memory, to prevent reloading the model unnecessarily\n+        self.loaded_models: dict[str, TimedModel] = {}\n         self.running_continuous_batching_manager: Optional[ContinuousBatchingManager] = None\n-        self.model: PreTrainedModel\n-        self.tokenizer: PreTrainedTokenizerFast\n \n         # 2. preserves information about the last call and last KV cache, to determine whether we can reuse the KV\n         # cache and avoid re-running prefil\n         self.last_messages = None\n         self.last_kv_cache = None\n+        self.last_text_model = None\n \n     def _validate_request(\n         self,\n@@ -433,10 +505,19 @@ def validate_chat_completion_request(self, request: dict):\n             unused_fields=UNUSED_CHAT_COMPLETION_FIELDS,\n         )\n \n+    def validate_transcription_request(self, request: dict):\n+        self._validate_request(\n+            request=request,\n+            schema=TransformersTranscriptionCreateParams,\n+            validator=transcription_validator,\n+            unused_fields=UNUSED_TRANSCRIPTION_FIELDS,\n+        )\n+\n     def build_chat_completion_chunk(\n         self,\n         request_id: Optional[str] = \"\",\n         content: Optional[str] = None,\n+        model: Optional[str] = None,\n         role: Optional[str] = None,\n         finish_reason: Optional[str] = None,\n         tool_calls: Optional[list[\"ChoiceDeltaToolCall\"]] = None,\n@@ -452,6 +533,8 @@ def build_chat_completion_chunk(\n                 The request ID.\n             content (`str`, *optional*):\n                 Content of the response from the model.\n+            model (`str`, *optional*):\n+                The model that generated the content.\n             role (`str`, *optional*):\n                 The role of the next content, until a new role is defined.\n             finish_reason (`str`, *optional*):\n@@ -465,7 +548,7 @@ def build_chat_completion_chunk(\n         chunk = ChatCompletionChunk(\n             id=request_id,\n             created=int(time.time()),\n-            model=self.loaded_model,\n+            model=model,\n             choices=[\n                 Choice(\n                     delta=ChoiceDelta(\n@@ -529,6 +612,26 @@ def responses(request: dict):\n             output = self.generate_response(request)\n             return StreamingResponse(output, media_type=\"text/event-stream\")\n \n+        from fastapi import Request\n+\n+        @app.post(\"/v1/audio/transcriptions\")\n+        async def audio_transcriptions(request: Request):\n+            # Parses the multipart/form-data request into the request format used by other endpoints\n+            async with request.form() as form:\n+                parsed_request = TransformersTranscriptionCreateParams(\n+                    file=await form[\"file\"].read(),\n+                    model=form[\"model\"],\n+                    # TODO: add other fields\n+                )\n+                logger.debug(\n+                    f\"Received file: {form['file'].filename}; MIME type: {form['file'].content_type}; \"\n+                    f\"size: {form['file'].size / 1024:.2f} KiB\"\n+                )\n+            self.validate_transcription_request(request=parsed_request)\n+\n+            output = self.generate_transcription(parsed_request)\n+            return StreamingResponse(output, media_type=\"text/event-stream\")\n+\n         @app.get(\"/v1/models\")\n         def get_all_models():\n             return JSONResponse(\n@@ -579,22 +682,22 @@ def continuous_batching_chat_completion(self, req: dict) -> Generator[str, None,\n         Returns:\n             `Generator[str, None, None]`: A generator that yields the OpenAI Chat Completion chunks.\n         \"\"\"\n-        if self.args.force_model is not None:\n-            req[\"model\"] = self.args.force_model\n \n-        update_model = self.canonicalized_model_name(req[\"model\"]) != self.loaded_model\n-        if update_model:\n+        model_id_and_revision = self.process_model_name(req[\"model\"])\n+        must_discard_cache = model_id_and_revision != self.last_text_model\n+        self.last_text_model = model_id_and_revision\n+        if must_discard_cache:\n             # When switching models, terminate a continuous batching manager if it is running.\n             if self.running_continuous_batching_manager is not None:\n                 self.running_continuous_batching_manager.stop(block=True, timeout=2)\n                 self.running_continuous_batching_manager = None\n-            self.load_model_and_tokenizer(req[\"model\"], self.args)\n+        model, tokenizer = self.load_text_model_and_tokenizer(model_id_and_revision)\n \n         generation_config = create_generation_config_from_req(\n             req,\n-            model_generation_config=self.model.generation_config,\n-            eos_token_id=self.tokenizer.eos_token_id,\n-            pad_token_id=self.tokenizer.pad_token_id,\n+            model_generation_config=model.generation_config,\n+            eos_token_id=tokenizer.eos_token_id,\n+            pad_token_id=tokenizer.pad_token_id,\n             use_cache=False,\n             num_blocks=1,\n             block_size=1024,\n@@ -604,7 +707,7 @@ def continuous_batching_chat_completion(self, req: dict) -> Generator[str, None,\n         )\n \n         if self.running_continuous_batching_manager is None:\n-            self.running_continuous_batching_manager = self.model.init_continuous_batching(\n+            self.running_continuous_batching_manager = model.init_continuous_batching(\n                 generation_config=generation_config, streaming=True\n             )\n \n@@ -614,9 +717,9 @@ def continuous_batching_chat_completion(self, req: dict) -> Generator[str, None,\n             self.running_continuous_batching_manager.start()\n \n         # TODO (Joao, Lysandre): this should also work with tool support\n-        inputs = self.tokenizer.apply_chat_template(\n-            req[\"messages\"], return_tensors=\"pt\", add_generation_prompt=True\n-        ).to(self.model.device)\n+        inputs = tokenizer.apply_chat_template(req[\"messages\"], return_tensors=\"pt\", add_generation_prompt=True).to(\n+            model.device\n+        )\n \n         def stream_chat_completion(_inputs):\n             try:\n@@ -628,7 +731,7 @@ def stream_chat_completion(_inputs):\n \n                 # Emit the assistant role to start the stream. Other chunks won't have a role, as it is implicit\n                 # they come from the assistant.\n-                yield self.build_chat_completion_chunk(request_id, role=\"assistant\")\n+                yield self.build_chat_completion_chunk(request_id, role=\"assistant\", model=model_id_and_revision)\n \n                 for result in self.running_continuous_batching_manager:\n                     if result.request_id != request_id:\n@@ -641,10 +744,14 @@ def stream_chat_completion(_inputs):\n \n                     finish_reason = \"stop\" if result.status == RequestStatus.FINISHED else None\n                     if result.status == RequestStatus.FINISHED:\n-                        yield self.build_chat_completion_chunk(request_id, finish_reason=finish_reason)\n+                        yield self.build_chat_completion_chunk(\n+                            request_id, finish_reason=finish_reason, model=model_id_and_revision\n+                        )\n                         break\n                     else:\n-                        yield self.build_chat_completion_chunk(request_id=request_id, content=result.next_token)\n+                        yield self.build_chat_completion_chunk(\n+                            request_id=request_id, content=result.next_token, model=model_id_and_revision\n+                        )\n \n             except Exception as e:\n                 logger.error(str(e))\n@@ -662,22 +769,20 @@ def generate_chat_completion(self, req: dict) -> Generator[str, None, None]:\n         Returns:\n             `Generator[str, None, None]`: A generator that yields the OpenAI Chat Completion chunks.\n         \"\"\"\n-        if self.args.force_model is not None:\n-            req[\"model\"] = self.args.force_model\n-\n-        update_model = self.canonicalized_model_name(req[\"model\"]) != self.loaded_model\n-        if update_model:\n-            self.load_model_and_tokenizer(req[\"model\"], self.args)\n-\n         # HACK for tiny-agents: it sends a request after the assistant message (???). Let's assume we can't have a\n         # request whose last message is from the assistant.\n         if req[\"messages\"][-1][\"role\"] == \"assistant\":\n             return\n \n+        model_id_and_revision = self.process_model_name(req[\"model\"])\n+        must_discard_cache = model_id_and_revision != self.last_text_model\n+        self.last_text_model = model_id_and_revision\n+        model, tokenizer = self.load_text_model_and_tokenizer(model_id_and_revision)\n+\n         # ====== TOOL PREPROCESSING LOGIC ======\n         tool_model_family = None\n         for supported_model_families in _MODELS_WITH_TOOL_SUPPORT:\n-            if supported_model_families in self.model.config.architectures[0].lower():\n+            if supported_model_families in model.config.architectures[0].lower():\n                 tool_model_family = supported_model_families\n                 break\n         # TODO: trigger 2 constrained generations after the tool call start token is emitted:\n@@ -686,21 +791,19 @@ def generate_chat_completion(self, req: dict) -> Generator[str, None, None]:\n         # ====== END OF TOOL PREPROCESSING LOGIC ======\n \n         if tool_model_family is not None:\n-            text = self.tokenizer.apply_chat_template(\n+            text = tokenizer.apply_chat_template(\n                 req[\"messages\"], add_generation_prompt=True, tokenize=False, tools=req.get(\"tools\")\n             )\n         else:\n-            text = self.tokenizer.apply_chat_template(req[\"messages\"], add_generation_prompt=True, tokenize=False)\n-        inputs = self.tokenizer(text, return_tensors=\"pt\").to(self.model.device)[\"input_ids\"]\n+            text = tokenizer.apply_chat_template(req[\"messages\"], add_generation_prompt=True, tokenize=False)\n+        inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)[\"input_ids\"]\n         request_id = req.get(\"request_id\", \"req_0\")\n \n-        generation_streamer = TextIteratorStreamer(self.tokenizer, skip_special_tokens=True, skip_prompt=True)\n-        generation_config = create_generation_config_from_req(\n-            req, model_generation_config=self.model.generation_config\n-        )\n+        generation_streamer = TextIteratorStreamer(tokenizer, skip_special_tokens=True, skip_prompt=True)\n+        generation_config = create_generation_config_from_req(req, model_generation_config=model.generation_config)\n \n         last_kv_cache = None\n-        if self.is_continuation(req) and not update_model:\n+        if self.is_continuation(req) and not must_discard_cache:\n             last_kv_cache = self.last_kv_cache\n \n         generation_kwargs = {\n@@ -715,7 +818,7 @@ def generate_chat_completion(self, req: dict) -> Generator[str, None, None]:\n         def stream_chat_completion(streamer, _request_id):\n             # Thin wrapper to save the KV cache after generation\n             def generate_with_cache(**kwargs):\n-                generate_output = self.model.generate(**kwargs)\n+                generate_output = model.generate(**kwargs)\n                 self.last_kv_cache = generate_output.past_key_values\n \n             thread = Thread(target=generate_with_cache, kwargs=generation_kwargs)\n@@ -726,7 +829,7 @@ def generate_with_cache(**kwargs):\n \n                 # Emit the assistant role to start the stream. Other chunks won't have a role, as it is implicit\n                 # they come from the assistant.\n-                yield self.build_chat_completion_chunk(request_id, role=\"assistant\")\n+                yield self.build_chat_completion_chunk(request_id, role=\"assistant\", model=model_id_and_revision)\n \n                 for result in streamer:\n                     # ====== TOOL CALL LOGIC ======\n@@ -740,10 +843,13 @@ def generate_with_cache(**kwargs):\n                         if result.strip() == _TOOL_CALL_TOKENS[tool_model_family][\"end\"]:\n                             tool_state.reset()\n                             yield self.build_chat_completion_chunk(\n-                                request_id=_request_id, role=None, finish_reason=\"tool_calls\"\n+                                request_id=_request_id,\n+                                role=None,\n+                                finish_reason=\"tool_calls\",\n+                                model=model_id_and_revision,\n                             )\n-                            continue\n \n+                            continue\n                         # Inside a tool call\n                         if tool_state.inside_tool_call:\n                             tool_state.buffer += result\n@@ -789,15 +895,17 @@ def generate_with_cache(**kwargs):\n                                 )\n \n                             yield self.build_chat_completion_chunk(\n-                                request_id=_request_id, role=None, tool_calls=[tool]\n+                                request_id=_request_id, role=None, tool_calls=[tool], model=model_id_and_revision\n                             )\n                             continue\n                     # ====== END OF TOOL CALL LOGIC ======\n \n                     # All non-tool related tokens are emitted as assistant messages. Empty text is skipped.\n                     if result != \"\":\n-                        yield self.build_chat_completion_chunk(_request_id, content=result)\n-                yield self.build_chat_completion_chunk(_request_id, finish_reason=\"stop\")\n+                        yield self.build_chat_completion_chunk(\n+                            _request_id, content=result, model=model_id_and_revision\n+                        )\n+                yield self.build_chat_completion_chunk(_request_id, finish_reason=\"stop\", model=model_id_and_revision)\n \n                 thread.join()\n             except Exception as e:\n@@ -820,24 +928,20 @@ def generate_response(self, req: dict) -> Generator[str, None, None]:\n             `Generator[str, None, None]`: A generator that yields the OpenAI Response events.\n         \"\"\"\n         # TODO -- Implement non-streaming mode\n-        if self.args.force_model is not None:\n-            req[\"model\"] = self.args.force_model\n-\n-        update_model = self.canonicalized_model_name(req[\"model\"]) != self.loaded_model\n-        if update_model:\n-            self.load_model_and_tokenizer(req[\"model\"], self.args)\n+        model_id_and_revision = self.process_model_name(req[\"model\"])\n+        must_discard_cache = model_id_and_revision != self.last_text_model\n+        self.last_text_model = model_id_and_revision\n+        model, tokenizer = self.load_text_model_and_tokenizer(model_id_and_revision)\n \n-        text = self.tokenizer.apply_chat_template(req[\"input\"], add_generation_prompt=True, tokenize=False)\n-        inputs = self.tokenizer(text, return_tensors=\"pt\").to(self.model.device)[\"input_ids\"]\n+        text = tokenizer.apply_chat_template(req[\"input\"], add_generation_prompt=True, tokenize=False)\n+        inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)[\"input_ids\"]\n         request_id = req.get(\"previous_response_id\", \"req_0\")\n \n-        generation_streamer = TextIteratorStreamer(self.tokenizer, skip_special_tokens=True, skip_prompt=True)\n-        generation_config = create_generation_config_from_req(\n-            req, model_generation_config=self.model.generation_config\n-        )\n+        generation_streamer = TextIteratorStreamer(tokenizer, skip_special_tokens=True, skip_prompt=True)\n+        generation_config = create_generation_config_from_req(req, model_generation_config=model.generation_config)\n \n         last_kv_cache = None\n-        if self.is_continuation(req) and not update_model:\n+        if self.is_continuation(req) and not must_discard_cache:\n             last_kv_cache = self.last_kv_cache\n \n         generation_kwargs = {\n@@ -850,7 +954,12 @@ def generate_response(self, req: dict) -> Generator[str, None, None]:\n         }\n \n         def stream_response(streamer, _request_id):\n-            thread = Thread(target=self.model.generate, kwargs=generation_kwargs)\n+            # Thin wrapper to save the KV cache after generation\n+            def generate_with_cache(**kwargs):\n+                generate_output = model.generate(**kwargs)\n+                self.last_kv_cache = generate_output.past_key_values\n+\n+            thread = Thread(target=generate_with_cache, kwargs=generation_kwargs)\n             sequence_number = 0\n             output_index = 0\n             content_index = 0\n@@ -868,7 +977,7 @@ def stream_response(streamer, _request_id):\n                         id=f\"resp_{request_id}\",\n                         created_at=created_at,\n                         status=\"queued\",\n-                        model=self.loaded_model,\n+                        model=model_id_and_revision,\n                         instructions=req.get(\"instructions\"),\n                         text={\"format\": {\"type\": \"text\"}},\n                         object=\"response\",\n@@ -889,7 +998,7 @@ def stream_response(streamer, _request_id):\n                         id=f\"resp_{request_id}\",\n                         created_at=created_at,\n                         status=\"in_progress\",\n-                        model=self.loaded_model,\n+                        model=model_id_and_revision,\n                         instructions=req.get(\"instructions\"),\n                         text={\"format\": {\"type\": \"text\"}},\n                         object=\"response\",\n@@ -994,7 +1103,7 @@ def stream_response(streamer, _request_id):\n                         id=f\"resp_{request_id}\",\n                         created_at=created_at,\n                         status=\"completed\",\n-                        model=self.loaded_model,\n+                        model=model_id_and_revision,\n                         instructions=req.get(\"instructions\"),\n                         text={\"format\": {\"type\": \"text\"}},\n                         output=[response_output_item_done.item],\n@@ -1026,7 +1135,7 @@ def stream_response(streamer, _request_id):\n                         id=f\"resp_{request_id}\",\n                         created_at=created_at,\n                         status=\"failed\",\n-                        model=self.loaded_model,\n+                        model=model_id_and_revision,\n                         instructions=req.get(\"instructions\"),\n                         text={\"format\": {\"type\": \"text\"}},\n                         output=[],\n@@ -1049,6 +1158,54 @@ def stream_response(streamer, _request_id):\n \n         return stream_response(generation_streamer, request_id)\n \n+    def generate_transcription(self, req: dict) -> Generator[str, None, None]:\n+        \"\"\"\n+        Generates an OpenAI Transcription using the audio file.\n+\n+        Args:\n+            req (`dict`): The request containing the audio file and model information.\n+\n+        Returns:\n+            `Generator[str, None, None]`: A generator that yields the transcription result.\n+        \"\"\"\n+        # TODO: implement streaming transcription (currently, it's not streaming)\n+        if not is_librosa_available():\n+            raise ImportError(\n+                \"Missing librosa dependency for audio transcription. Please install with `pip install librosa`\"\n+            )\n+        model_id_and_revision = self.process_model_name(req[\"model\"])\n+        audio_model, audio_processor = self.load_audio_model_and_processor(model_id_and_revision)\n+\n+        generation_streamer = TextIteratorStreamer(\n+            audio_processor.tokenizer, skip_special_tokens=True, skip_prompt=True\n+        )\n+        generation_config = create_generation_config_from_req(\n+            req, model_generation_config=audio_model.generation_config\n+        )\n+\n+        # Read the binary audio file using librosa\n+        model_sampling_rate = audio_processor.feature_extractor.sampling_rate\n+        audio_bytes = io.BytesIO(req[\"file\"])\n+        audio_array, _ = librosa.load(audio_bytes, sr=model_sampling_rate, mono=True)\n+        audio_inputs = audio_processor(audio_array, sampling_rate=model_sampling_rate, return_tensors=\"pt\").to(\n+            audio_model.device\n+        )\n+        audio_inputs[\"input_features\"] = audio_inputs[\"input_features\"].to(audio_model.dtype)\n+\n+        generation_kwargs = {\n+            \"streamer\": generation_streamer,\n+            \"generation_config\": generation_config,\n+            \"return_dict_in_generate\": True,\n+        }\n+\n+        def _generate_transcription():\n+            generated_ids = audio_model.generate(**audio_inputs, **generation_kwargs)\n+            transcription_text = audio_processor.batch_decode(generated_ids.sequences, skip_special_tokens=True)[0]\n+            transcription = Transcription(text=transcription_text)\n+            yield f\"{transcription.model_dump_json(exclude_none=True)}\"\n+\n+        return _generate_transcription()\n+\n     def is_continuation(self, req: dict) -> bool:\n         \"\"\"\n         Determines whether the current request is a continuation of the last request. In other words, if it is the\n@@ -1108,39 +1265,49 @@ def get_quantization_config(args: ServeArguments) -> Optional[\"BitsAndBytesConfi\n \n         return quantization_config\n \n-    def canonicalized_model_name(self, model_id: str) -> str:\n+    def process_model_name(self, model_id: str) -> str:\n         \"\"\"\n-        Canonicalizes the model name to the format \"model_id@revision\". If the model_id DOESN'T contain an @, it\n-        defaults to \"model_id@main\".\n+        Applies the `force_model` CLI argument and canonicalizes the model name to the format \"model_id@revision\".\n+        If the model_id DOESN'T contain an @, it defaults to \"model_id@main\".\n \n         Args:\n             model_id (`str`): The model ID.\n \n         Returns:\n-            `str`: The canonicalized model name.\n+            `str`: The canonicalized model name to be used\n         \"\"\"\n+        if self.args.force_model is not None:\n+            model_id = self.args.force_model\n         if \"@\" in model_id:\n             return model_id\n         return f\"{model_id}@main\"\n \n-    def load_model_and_tokenizer(self, model_id_and_revision: str, args: ServeArguments):\n+    def _load_model_and_data_processor(\n+        self, model_id_and_revision: str, model_cls: type[PreTrainedModel]\n+    ) -> tuple[PreTrainedModel, Union[ProcessorMixin, PreTrainedTokenizerFast]]:\n         \"\"\"\n-        Loads the model and tokenizer from the given model ID and revision into the ServeCommand instance.\n+        Generic method to load a model and a data processor from a model ID and revision, making use of the serve CLI\n+        arguments.\n \n         Args:\n             model_id_and_revision (`str`):\n                 The model ID and revision to load.\n-            args (`ServeArguments`):\n-                The serve arguments. May contain quantization settings, device, etc.\n+            model_cls (`type[PreTrainedModel]`):\n+                The model class to load.\n+\n+        Returns:\n+            `tuple[PreTrainedModel, Union[ProcessorMixin, PreTrainedTokenizerFast]]`: The loaded model and\n+            data processor (tokenizer, audio processor, etc.).\n         \"\"\"\n-        logger.warning(f\"Loading {model_id_and_revision}\")\n+        args = self.args\n+        logger.info(f\"Loading {model_id_and_revision}\")\n \n         if \"@\" in model_id_and_revision:\n             model_id, revision = model_id_and_revision.split(\"@\", 1)\n         else:\n             model_id, revision = model_id_and_revision, \"main\"\n \n-        tokenizer = AutoTokenizer.from_pretrained(\n+        data_processor = AutoProcessor.from_pretrained(\n             model_id,\n             revision=revision,\n             trust_remote_code=args.trust_remote_code,\n@@ -1158,19 +1325,76 @@ def load_model_and_tokenizer(self, model_id_and_revision: str, args: ServeArgume\n             \"trust_remote_code\": args.trust_remote_code,\n         }\n \n-        model = AutoModelForCausalLM.from_pretrained(model_id, **model_kwargs)\n-\n-        if model.generation_config.max_new_tokens is not None and model.generation_config.max_new_tokens < 1024:\n-            model.generation_config.max_new_tokens = 1024\n+        model = model_cls.from_pretrained(model_id, **model_kwargs)\n \n         if getattr(model, \"hf_device_map\", None) is None:\n             model = model.to(args.device)\n \n-        self.loaded_model = f\"{model_id}@{revision}\"\n+        has_default_max_length = (\n+            model.generation_config.max_new_tokens is None and model.generation_config.max_length == 20\n+        )\n+        has_short_max_new_tokens = (\n+            model.generation_config.max_new_tokens is not None and model.generation_config.max_new_tokens < 1024\n+        )\n+        if has_default_max_length or has_short_max_new_tokens:\n+            model.generation_config.max_new_tokens = 1024\n \n-        logger.warning(f\"Loaded model {self.loaded_model}\")\n-        self.model = model\n-        self.tokenizer = tokenizer\n+        logger.info(f\"Loaded model {model_id_and_revision}\")\n+        return model, data_processor\n+\n+    def load_text_model_and_tokenizer(\n+        self, model_id_and_revision: str\n+    ) -> tuple[PreTrainedModel, PreTrainedTokenizerFast]:\n+        \"\"\"\n+        Loads the text model and tokenizer from the given model ID and revision into the ServeCommand instance.\n+\n+        Args:\n+            model_id_and_revision (`str`):\n+                The model ID and revision to load.\n+\n+        Returns:\n+            `tuple[PreTrainedModel, PreTrainedTokenizerFast]`: The loaded text model and tokenizer.\n+        \"\"\"\n+        if model_id_and_revision not in self.loaded_models or self.loaded_models[model_id_and_revision].is_deleted():\n+            model, tokenizer = self._load_model_and_data_processor(model_id_and_revision, AutoModelForCausalLM)\n+            self.loaded_models[model_id_and_revision] = TimedModel(\n+                model,\n+                timeout_seconds=self.args.model_timeout,\n+                processor=tokenizer,\n+            )\n+        else:\n+            self.loaded_models[model_id_and_revision].reset_timer()\n+            model = self.loaded_models[model_id_and_revision].model\n+            tokenizer = self.loaded_models[model_id_and_revision].processor\n+\n+        return model, tokenizer\n+\n+    def load_audio_model_and_processor(self, model_id_and_revision: str) -> tuple[PreTrainedModel, ProcessorMixin]:\n+        \"\"\"\n+        Loads the audio model and processor from the given model ID and revision into the ServeCommand instance.\n+\n+        Args:\n+            model_id_and_revision (`str`):\n+                The model ID and revision to load.\n+\n+        Returns:\n+            `tuple[PreTrainedModel, ProcessorMixin]`: The loaded audio model and processor.\n+        \"\"\"\n+        if model_id_and_revision not in self.loaded_models or self.loaded_models[model_id_and_revision].is_deleted():\n+            audio_model, audio_processor = self._load_model_and_data_processor(\n+                model_id_and_revision, AutoModelForSpeechSeq2Seq\n+            )\n+            self.loaded_models[model_id_and_revision] = TimedModel(\n+                audio_model,\n+                timeout_seconds=self.args.model_timeout,\n+                processor=audio_processor,\n+            )\n+        else:\n+            self.loaded_models[model_id_and_revision].reset_timer()\n+            audio_model = self.loaded_models[model_id_and_revision].model\n+            audio_processor = self.loaded_models[model_id_and_revision].processor\n+\n+        return audio_model, audio_processor\n \n \n if __name__ == \"__main__\":"
        },
        {
            "sha": "ed344ef7ed061d3907d05204e949de13289f2907",
            "filename": "tests/commands/test_serving.py",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf6c9976851627b545f071d4861c54cda7fdfe6b/tests%2Fcommands%2Ftest_serving.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf6c9976851627b545f071d4861c54cda7fdfe6b/tests%2Fcommands%2Ftest_serving.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fcommands%2Ftest_serving.py?ref=bf6c9976851627b545f071d4861c54cda7fdfe6b",
            "patch": "@@ -63,14 +63,13 @@ def test_build_chat_completion_chunk(self):\n         \"\"\"\n         dummy = ServeCommand.__new__(ServeCommand)\n         dummy.args = type(\"Args\", (), {})()\n-        dummy.loaded_model = \"dummy_model@main\"\n \n         # The keys for these fields must be present in every chunk\n         MANDATORY_FIELDS = [\"data\", \"id\", \"choices\", \"created\", \"model\", \"object\", \"system_fingerprint\"]\n \n         # Case 1: most fields are provided\n         chunk = ServeCommand.build_chat_completion_chunk(\n-            dummy, request_id=\"req0\", content=\"hello\", finish_reason=\"stop\", role=\"user\"\n+            dummy, request_id=\"req0\", content=\"hello\", finish_reason=\"stop\", role=\"user\", model=\"dummy_model@main\"\n         )\n         for field in MANDATORY_FIELDS:\n             self.assertIn(field, chunk)\n@@ -79,13 +78,13 @@ def test_build_chat_completion_chunk(self):\n         )\n \n         # Case 2: only the role is provided -- other fields in 'choices' are omitted\n-        chunk = dummy.build_chat_completion_chunk(request_id=\"req0\", role=\"user\")\n+        chunk = dummy.build_chat_completion_chunk(request_id=\"req0\", role=\"user\", model=\"dummy_model@main\")\n         for field in MANDATORY_FIELDS:\n             self.assertIn(field, chunk)\n         self.assertIn('\"choices\":[{\"delta\":{\"role\":\"user\"},\"index\":0}]', chunk)\n \n         # Case 3: only the content is provided -- other fields in 'choices' are omitted\n-        chunk = dummy.build_chat_completion_chunk(request_id=\"req0\", content=\"hello\")\n+        chunk = dummy.build_chat_completion_chunk(request_id=\"req0\", content=\"hello\", model=\"dummy_model@main\")\n         for field in MANDATORY_FIELDS:\n             self.assertIn(field, chunk)\n         self.assertIn('\"choices\":[{\"delta\":{\"content\":\"hello\"},\"index\":0}]', chunk)\n@@ -96,7 +95,7 @@ def test_build_chat_completion_chunk(self):\n             function=ChoiceDeltaToolCallFunction(name=\"foo_bar\", arguments='{\"foo1\": \"bar1\", \"foo2\": \"bar2\"}'),\n             type=\"function\",\n         )\n-        chunk = dummy.build_chat_completion_chunk(request_id=\"req0\", tool_calls=[tool_call])\n+        chunk = dummy.build_chat_completion_chunk(request_id=\"req0\", tool_calls=[tool_call], model=\"dummy_model@main\")\n         for field in MANDATORY_FIELDS:\n             self.assertIn(field, chunk)\n         expected_choices_content = ("
        }
    ],
    "stats": {
        "total": 459,
        "additions": 355,
        "deletions": 104
    }
}