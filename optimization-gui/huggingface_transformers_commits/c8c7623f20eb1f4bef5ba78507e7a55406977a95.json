{
    "author": "GSNCodes",
    "message": "Update SegFormer model card (#40417)\n\n* Update SegFormer model card\n\n* Update docs/source/en/model_doc/segformer.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/segformer.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/segformer.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/segformer.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/segformer.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/segformer.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/segformer.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update the segformer model card\n\n* Remove quantization example\n\n---------\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "c8c7623f20eb1f4bef5ba78507e7a55406977a95",
    "files": [
        {
            "sha": "756c98d45f08b248b917708743a94bcf64a80b9b",
            "filename": "docs/source/en/model_doc/segformer.md",
            "status": "modified",
            "additions": 70,
            "deletions": 86,
            "changes": 156,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8c7623f20eb1f4bef5ba78507e7a55406977a95/docs%2Fsource%2Fen%2Fmodel_doc%2Fsegformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8c7623f20eb1f4bef5ba78507e7a55406977a95/docs%2Fsource%2Fen%2Fmodel_doc%2Fsegformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsegformer.md?ref=c8c7623f20eb1f4bef5ba78507e7a55406977a95",
            "patch": "@@ -1,113 +1,97 @@\n <!--Copyright 2021 The HuggingFace Team. All rights reserved.\n \n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n http://www.apache.org/licenses/LICENSE-2.0\n \n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n \n+âš ï¸ Note that this file is in Markdown but contains specific syntax\n+for our doc-builder (similar to MDX) that may not render properly\n+in your Markdown viewer.\n -->\n *This model was released on 2021-05-31 and added to Hugging Face Transformers on 2021-10-28.*\n \n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+           <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+    </div>\n+</div>\n+\n # SegFormer\n \n-<div class=\"flex flex-wrap space-x-1\">\n-<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-</div>\n+[SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers](https://huggingface.co/papers/2105.15203) is a semantic segmentation model that combines a hierarchical Transformer encoder (Mix Transformer, MiT) with a lightweight all-MLP decoder. It avoids positional encodings and complex decoders and achieves state-of-the-art performance on benchmarks like ADE20K and Cityscapes. This simple and lightweight design is more efficient and scalable.\n \n-## Overview\n+The figure below illustrates the architecture of SegFormer.\n \n-The SegFormer model was proposed in [SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers](https://huggingface.co/papers/2105.15203) by Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M. Alvarez, Ping\n-Luo. The model consists of a hierarchical Transformer encoder and a lightweight all-MLP decode head to achieve great\n-results on image segmentation benchmarks such as ADE20K and Cityscapes.\n+<img width=\"600\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/segformer_architecture.png\"/>\n \n-The abstract from the paper is the following:\n+You can find all the original SegFormer checkpoints under the [NVIDIA](https://huggingface.co/nvidia/models?search=segformer) organization.\n \n-*We present SegFormer, a simple, efficient yet powerful semantic segmentation framework which unifies Transformers with\n-lightweight multilayer perception (MLP) decoders. SegFormer has two appealing features: 1) SegFormer comprises a novel\n-hierarchically structured Transformer encoder which outputs multiscale features. It does not need positional encoding,\n-thereby avoiding the interpolation of positional codes which leads to decreased performance when the testing resolution\n-differs from training. 2) SegFormer avoids complex decoders. The proposed MLP decoder aggregates information from\n-different layers, and thus combining both local attention and global attention to render powerful representations. We\n-show that this simple and lightweight design is the key to efficient segmentation on Transformers. We scale our\n-approach up to obtain a series of models from SegFormer-B0 to SegFormer-B5, reaching significantly better performance\n-and efficiency than previous counterparts. For example, SegFormer-B4 achieves 50.3% mIoU on ADE20K with 64M parameters,\n-being 5x smaller and 2.2% better than the previous best method. Our best model, SegFormer-B5, achieves 84.0% mIoU on\n-Cityscapes validation set and shows excellent zero-shot robustness on Cityscapes-C.*\n+> [!TIP]\n+> This model was contributed by [nielsr](https://huggingface.co/nielsr).\n+>\n+> Click on the SegFormer models in the right sidebar for more examples of how to apply SegFormer to different vision tasks.\n \n-The figure below illustrates the architecture of SegFormer. Taken from the [original paper](https://huggingface.co/papers/2105.15203).\n+The example below demonstrates semantic segmentation with [`Pipeline`] or the [`AutoModel`] class.\n \n-<img width=\"600\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/segformer_architecture.png\"/>\n+<hfoptions id=\"usage\">\n+<hfoption id=\"Pipeline\">\n \n-This model was contributed by [nielsr](https://huggingface.co/nielsr). The original code can be found [here](https://github.com/NVlabs/SegFormer).\n-\n-## Usage tips\n-\n-- SegFormer consists of a hierarchical Transformer encoder, and a lightweight all-MLP decoder head.\n-  [`SegformerModel`] is the hierarchical Transformer encoder (which in the paper is also referred to\n-  as Mix Transformer or MiT). [`SegformerForSemanticSegmentation`] adds the all-MLP decoder head on\n-  top to perform semantic segmentation of images. In addition, there's\n-  [`SegformerForImageClassification`] which can be used to - you guessed it - classify images. The\n-  authors of SegFormer first pre-trained the Transformer encoder on ImageNet-1k to classify images. Next, they throw\n-  away the classification head, and replace it by the all-MLP decode head. Next, they fine-tune the model altogether on\n-  ADE20K, Cityscapes and COCO-stuff, which are important benchmarks for semantic segmentation. All checkpoints can be\n-  found on the [hub](https://huggingface.co/models?other=segformer).\n-- The quickest way to get started with SegFormer is by checking the [example notebooks](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/SegFormer) (which showcase both inference and\n-  fine-tuning on custom data). One can also check out the [blog post](https://huggingface.co/blog/fine-tune-segformer) introducing SegFormer and illustrating how it can be fine-tuned on custom data.\n-- One can also check out [this interactive demo on Hugging Face Spaces](https://huggingface.co/spaces/chansung/segformer-tf-transformers)\n-  to try out a SegFormer model on custom images.\n-- SegFormer works on any input size, as it pads the input to be divisible by `config.patch_sizes`.\n-- One can use [`SegformerImageProcessor`] to prepare images and corresponding segmentation maps\n-  for the model. Note that this image processor is fairly basic and does not include all data augmentations used in\n-  the original paper. The original preprocessing pipelines (for the ADE20k dataset for instance) can be found [here](https://github.com/NVlabs/SegFormer/blob/master/local_configs/_base_/datasets/ade20k_repeat.py). The most\n-  important preprocessing step is that images and segmentation maps are randomly cropped and padded to the same size,\n-  such as 512x512 or 640x640, after which they are normalized.\n-- One additional thing to keep in mind is that one can initialize [`SegformerImageProcessor`] with\n-  `do_reduce_labels` set to `True` or `False`. In some datasets (like ADE20k), the 0 index is used in the annotated\n-  segmentation maps for background. However, ADE20k doesn't include the \"background\" class in its 150 labels.\n-  Therefore, `do_reduce_labels` is used to reduce all labels by 1, and to make sure no loss is computed for the\n-  background class (i.e. it replaces 0 in the annotated maps by 255, which is the *ignore_index* of the loss function\n-  used by [`SegformerForSemanticSegmentation`]). However, other datasets use the 0 index as\n-  background class and include this class as part of all labels. In that case, `do_reduce_labels` should be set to\n-  `False`, as loss should also be computed for the background class.\n-- As most models, SegFormer comes in different sizes, the details of which can be found in the table below\n-  (taken from Table 7 of the [original paper](https://huggingface.co/papers/2105.15203)).\n-\n-| **Model variant** | **Depths**    | **Hidden sizes**    | **Decoder hidden size** | **Params (M)** | **ImageNet-1k Top 1** |\n-| :---------------: | ------------- | ------------------- | :---------------------: | :------------: | :-------------------: |\n-| MiT-b0            | [2, 2, 2, 2]  | [32, 64, 160, 256]  | 256                     | 3.7            | 70.5                  |\n-| MiT-b1            | [2, 2, 2, 2]  | [64, 128, 320, 512] | 256                     | 14.0           | 78.7                  |\n-| MiT-b2            | [3, 4, 6, 3]  | [64, 128, 320, 512] | 768                     | 25.4           | 81.6                  |\n-| MiT-b3            | [3, 4, 18, 3] | [64, 128, 320, 512] | 768                     | 45.2           | 83.1                  |\n-| MiT-b4            | [3, 8, 27, 3] | [64, 128, 320, 512] | 768                     | 62.6           | 83.6                  |\n-| MiT-b5            | [3, 6, 40, 3] | [64, 128, 320, 512] | 768                     | 82.0           | 83.8                  |\n-\n-Note that MiT in the above table refers to the Mix Transformer encoder backbone introduced in SegFormer. For\n-SegFormer's results on the segmentation datasets like ADE20k, refer to the [paper](https://huggingface.co/papers/2105.15203).\n+```python\n+import torch\n+from transformers import pipeline\n \n-## Resources\n+pipeline = pipeline(task=\"image-segmentation\", model=\"nvidia/segformer-b0-finetuned-ade-512-512\", torch_dtype=torch.float16)\n+pipeline(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\")\n+```\n+\n+</hfoption>\n+<hfoption id=\"AutoModel\">\n \n-A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you get started with SegFormer.\n+```python\n+import requests\n+from PIL import Image\n+from transformers import AutoProcessor, AutoModelForSemanticSegmentation\n \n-<PipelineTag pipeline=\"image-classification\"/>\n+url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n+image = Image.open(requests.get(url, stream=True).raw)\n \n-- [`SegformerForImageClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb).\n-- [Image classification task guide](../tasks/image_classification)\n+processor = AutoProcessor.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\")\n+model = AutoModelForSemanticSegmentation.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\")\n \n-Semantic segmentation:\n+inputs = processor(images=image, return_tensors=\"pt\")\n+outputs = model(**inputs)\n+logits = outputs.logits # shape [batch, num_labels, height, width]\n+```\n \n-- [`SegformerForSemanticSegmentation`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/semantic-segmentation).\n-- A blog on fine-tuning SegFormer on a custom dataset can be found [here](https://huggingface.co/blog/fine-tune-segformer).\n-- More demo notebooks on SegFormer (both inference + fine-tuning on a custom dataset) can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/SegFormer).\n-- [Semantic segmentation task guide](../tasks/semantic_segmentation)\n+</hfoption>\n+\n+</hfoptions>\n+\n+\n+\n+## Notes\n+\n+- SegFormer works with **any input size**, padding inputs to be divisible by `config.patch_sizes`.\n+- The most important preprocessing step is to randomly crop and pad all images to the same size (such as 512x512 or 640x640) and normalize afterwards.\n+- Some datasets (ADE20k) uses the `0` index in the annotated segmentation as the background, but doesn't include the \"background\" class in its labels. The `do_reduce_labels` argument in [`SegformerForImageProcessor`] is used to reduce all labels by `1`. To make sure no loss is computed for the background class, it replaces `0` in the annotated maps by `255`, which is the `ignore_index` of the loss function.\n+\n+   Other datasets may include a background class and label though, in which case, `do_reduce_labels` should be `False`.\n+\n+```python\n+from transformers import SegformerImageProcessor\n+processor = SegformerImageProcessor(do_reduce_labels=True)\n+```\n+\n+## Resources\n \n-If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n+- [Original SegFormer code (NVlabs)](https://github.com/NVlabs/SegFormer)  \n+- [Fine-tuning blog post](https://huggingface.co/blog/fine-tune-segformer)  \n+- [Tutorial notebooks (Niels Rogge)](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/SegFormer)  \n+- [Hugging Face demo space](https://huggingface.co/spaces/chansung/segformer-tf-transformers)  \n \n ## SegformerConfig\n "
        }
    ],
    "stats": {
        "total": 156,
        "additions": 70,
        "deletions": 86
    }
}