{
    "author": "August-murr",
    "message": "remove gemmas eager training warning (#40744)\n\n* removed warning\n\n* removed remaining warnings",
    "sha": "9ab60783239670ace6499ece0de5374b5d8ff30b",
    "files": [
        {
            "sha": "3d088cfc52cfe4fde868e0b0ed7aaccda35d880e",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ab60783239670ace6499ece0de5374b5d8ff30b/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ab60783239670ace6499ece0de5374b5d8ff30b/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=9ab60783239670ace6499ece0de5374b5d8ff30b",
            "patch": "@@ -534,11 +534,6 @@ def forward(\n         \"What is your favorite condiment?\"\n         ```\"\"\"\n \n-        if self.training and self.config._attn_implementation != \"eager\":\n-            logger.warning_once(\n-                \"It is strongly recommended to train Gemma2 models with the `eager` attention implementation \"\n-                f\"instead of `{self.config._attn_implementation}`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.\"\n-            )\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states"
        },
        {
            "sha": "2a3e05e4754e6651482f39a7e785f8be0e95742d",
            "filename": "src/transformers/models/gemma2/modular_gemma2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ab60783239670ace6499ece0de5374b5d8ff30b/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ab60783239670ace6499ece0de5374b5d8ff30b/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py?ref=9ab60783239670ace6499ece0de5374b5d8ff30b",
            "patch": "@@ -520,11 +520,6 @@ def forward(\n         \"What is your favorite condiment?\"\n         ```\"\"\"\n \n-        if self.training and self.config._attn_implementation != \"eager\":\n-            logger.warning_once(\n-                \"It is strongly recommended to train Gemma2 models with the `eager` attention implementation \"\n-                f\"instead of `{self.config._attn_implementation}`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.\"\n-            )\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states"
        },
        {
            "sha": "f8cbfcf1de77f30b44820facf21c96814ffed564",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ab60783239670ace6499ece0de5374b5d8ff30b/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ab60783239670ace6499ece0de5374b5d8ff30b/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=9ab60783239670ace6499ece0de5374b5d8ff30b",
            "patch": "@@ -654,11 +654,6 @@ def forward(\n         \"What is your favorite condiment?\"\n         ```\"\"\"\n \n-        if self.training and self.config._attn_implementation != \"eager\":\n-            logger.warning_once(\n-                \"It is strongly recommended to train Gemma3 models with the `eager` attention implementation \"\n-                f\"instead of `{self.config._attn_implementation}`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.\"\n-            )\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states"
        },
        {
            "sha": "198ed7c0fcce68fb6d3c15a75152ef14376dbc8f",
            "filename": "src/transformers/models/gemma3n/modeling_gemma3n.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ab60783239670ace6499ece0de5374b5d8ff30b/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ab60783239670ace6499ece0de5374b5d8ff30b/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py?ref=9ab60783239670ace6499ece0de5374b5d8ff30b",
            "patch": "@@ -1808,11 +1808,6 @@ def forward(\n         \"What is your favorite condiment?\"\n         ```\"\"\"\n \n-        if self.training and self.config._attn_implementation != \"eager\":\n-            logger.warning_once(\n-                \"It is strongly recommended to train Gemma3n models with the `eager` attention implementation \"\n-                f\"instead of `{self.config._attn_implementation}`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.\"\n-            )\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states"
        },
        {
            "sha": "4628614ba363a6d409a219081bbda580f8b0d829",
            "filename": "src/transformers/models/t5gemma/modeling_t5gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 10,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ab60783239670ace6499ece0de5374b5d8ff30b/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ab60783239670ace6499ece0de5374b5d8ff30b/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py?ref=9ab60783239670ace6499ece0de5374b5d8ff30b",
            "patch": "@@ -41,7 +41,7 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_torchdynamo_compiling, logging\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import OutputRecorder, check_model_inputs\n from .configuration_t5gemma import T5GemmaConfig, T5GemmaModuleConfig\n@@ -1064,15 +1064,6 @@ def forward(\n             config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n             (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n         \"\"\"\n-        if self.training and self.config._attn_implementation != \"eager\":\n-            msg = (\n-                \"It is strongly recommended to train T5Gemma models with the `eager` attention implementation \"\n-                f\"instead of `{self.config._attn_implementation}`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.\"\n-            )\n-            if is_torchdynamo_compiling():\n-                raise ValueError(msg)\n-            else:\n-                logger.warning_once(msg)\n \n         if labels is not None and decoder_input_ids is None and decoder_inputs_embeds is None:\n             # get decoder inputs from shifting lm labels to the right"
        },
        {
            "sha": "924ddaa6871d95f170a463e98ee20257b619c016",
            "filename": "src/transformers/models/t5gemma/modular_t5gemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ab60783239670ace6499ece0de5374b5d8ff30b/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ab60783239670ace6499ece0de5374b5d8ff30b/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py?ref=9ab60783239670ace6499ece0de5374b5d8ff30b",
            "patch": "@@ -37,7 +37,6 @@\n     TransformersKwargs,\n     auto_docstring,\n     can_return_tuple,\n-    is_torchdynamo_compiling,\n     logging,\n )\n from ...utils.deprecation import deprecate_kwarg\n@@ -921,15 +920,6 @@ def forward(\n             config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n             (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n         \"\"\"\n-        if self.training and self.config._attn_implementation != \"eager\":\n-            msg = (\n-                \"It is strongly recommended to train T5Gemma models with the `eager` attention implementation \"\n-                f\"instead of `{self.config._attn_implementation}`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.\"\n-            )\n-            if is_torchdynamo_compiling():\n-                raise ValueError(msg)\n-            else:\n-                logger.warning_once(msg)\n \n         if labels is not None and decoder_input_ids is None and decoder_inputs_embeds is None:\n             # get decoder inputs from shifting lm labels to the right"
        }
    ],
    "stats": {
        "total": 41,
        "additions": 1,
        "deletions": 40
    }
}