{
    "author": "yuanwu2017",
    "message": "Fix the fsdp config cannot work issue. (#37549)\n\n* Fix the fsdp config cannot work issue.\n\nSigned-off-by: yuanwu <yuan.wu@intel.com>\n\n* Check the fsdp_config type\n\nSigned-off-by: yuanwu <yuan.wu@intel.com>\n\n* Add the accelerate_fsdp_config test\n\nSigned-off-by: yuanwu <yuan.wu@intel.com>\n\n* fix error of make style\n\nSigned-off-by: yuanwu <yuan.wu@intel.com>\n\n* Add key check\n\nSigned-off-by: yuanwu <yuan.wu@intel.com>\n\n---------\n\nSigned-off-by: yuanwu <yuan.wu@intel.com>\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>",
    "sha": "a41b6d9b5c39e002670c1d852e822d0de4aca39b",
    "files": [
        {
            "sha": "3b1b8c58b5d5f3d03463a6928f13e15e609abbd7",
            "filename": "src/transformers/training_args.py",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/a41b6d9b5c39e002670c1d852e822d0de4aca39b/src%2Ftransformers%2Ftraining_args.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a41b6d9b5c39e002670c1d852e822d0de4aca39b/src%2Ftransformers%2Ftraining_args.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftraining_args.py?ref=a41b6d9b5c39e002670c1d852e822d0de4aca39b",
            "patch": "@@ -1872,10 +1872,12 @@ def __post_init__(self):\n                 warnings.warn(\"`--fsdp_config` is useful only when `--fsdp` is specified.\")\n             with open(self.fsdp_config, encoding=\"utf-8\") as f:\n                 self.fsdp_config = json.load(f)\n-                for k in list(self.fsdp_config.keys()):\n-                    if k.startswith(\"fsdp_\"):\n-                        v = self.fsdp_config.pop(k)\n-                        self.fsdp_config[k[5:]] = v\n+\n+        if self.fsdp_config is not None and isinstance(self.fsdp_config, dict):\n+            for k in list(self.fsdp_config.keys()):\n+                if k.startswith(\"fsdp_\"):\n+                    v = self.fsdp_config.pop(k)\n+                    self.fsdp_config[k[5:]] = v\n \n         if self.fsdp_min_num_params > 0:\n             warnings.warn(\"using `--fsdp_min_num_params` is deprecated. Use fsdp_config instead \", FutureWarning)"
        },
        {
            "sha": "781199747f3c948f8e8f9c0f60eeedfecd6383c1",
            "filename": "tests/fsdp/test_fsdp.py",
            "status": "modified",
            "additions": 36,
            "deletions": 0,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/a41b6d9b5c39e002670c1d852e822d0de4aca39b/tests%2Ffsdp%2Ftest_fsdp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a41b6d9b5c39e002670c1d852e822d0de4aca39b/tests%2Ffsdp%2Ftest_fsdp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ffsdp%2Ftest_fsdp.py?ref=a41b6d9b5c39e002670c1d852e822d0de4aca39b",
            "patch": "@@ -154,6 +154,20 @@ def setUp(self):\n             \"LOCAL_RANK\": \"0\",\n             \"WORLD_SIZE\": \"1\",\n         }\n+        self.accelerate_fsdp_config = {\n+            \"fsdp_activation_checkpointing\": False,\n+            \"fsdp_auto_wrap_policy\": \"TRANSFORMER_BASED_WRAP\",\n+            \"fsdp_backward_prefetch\": \"BACKWARD_PRE\",\n+            \"fsdp_cpu_ram_efficient_loading\": True,\n+            \"fsdp_forward_prefetch\": False,\n+            \"fsdp_offload_params\": False,\n+            \"fsdp_reshard_after_forward\": \"FULL_SHARD\",\n+            \"fsdp_state_dict_type\": \"FULL_STATE_DICT\",\n+            \"fsdp_sync_module_states\": True,\n+            \"fsdp_transformer_layer_cls_to_wrap\": \"LlamaDecoderLayer\",\n+            \"fsdp_use_orig_params\": True,\n+            \"fsdp_version\": 1,\n+        }\n \n         self.fsdp_config = {\n             \"backward_prefetch\": \"BACKWARD_PRE\",\n@@ -169,6 +183,28 @@ def setUp(self):\n     def tearDown(self):\n         super().tearDown()\n \n+    @parameterized.expand(params, name_func=_parameterized_custom_name_func)\n+    def test_accelerate_fsdp_config(self, sharding_strategy, dtype):\n+        output_dir = self.get_auto_remove_tmp_dir()\n+        kwargs = {\n+            \"output_dir\": output_dir,\n+            \"train_len\": 128,\n+            \"save_steps\": 5,\n+            \"learning_rate\": 0.1,\n+            \"fsdp\": f\"{sharding_strategy} offload auto_wrap\",\n+            \"fsdp_config\": self.accelerate_fsdp_config,\n+        }\n+        kwargs[dtype] = True\n+        with mockenv_context(**self.dist_env_1_gpu):\n+            trainer = get_regression_trainer(**kwargs)\n+            self.assertEqual(trainer.args.fsdp[0], sharding_strategy)\n+            self.assertEqual(trainer.args.fsdp[1], FSDPOption.OFFLOAD)\n+            self.assertEqual(trainer.args.fsdp[2], FSDPOption.AUTO_WRAP)\n+            for k, v in trainer.args.fsdp_config.items():\n+                self.assertTrue(k in self.accelerate_fsdp_config)\n+                self.assertEqual(v, self.accelerate_fsdp_config[k])\n+            self.assertEqual(os.environ.get(\"ACCELERATE_USE_FSDP\", \"false\"), \"true\")\n+\n     @parameterized.expand(params, name_func=_parameterized_custom_name_func)\n     def test_fsdp_config(self, sharding_strategy, dtype):\n         output_dir = self.get_auto_remove_tmp_dir()"
        }
    ],
    "stats": {
        "total": 46,
        "additions": 42,
        "deletions": 4
    }
}