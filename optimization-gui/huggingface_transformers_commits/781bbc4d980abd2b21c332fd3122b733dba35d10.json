{
    "author": "itazap",
    "message": "use diff internal model in tests (#33387)\n\n* use diff internal model in tests\r\n\r\n* use diff internal model in tests",
    "sha": "781bbc4d980abd2b21c332fd3122b733dba35d10",
    "files": [
        {
            "sha": "c7e8b5e86021e58b3b63ac130d3b8700043dd472",
            "filename": "tests/models/llama/test_tokenization_llama.py",
            "status": "modified",
            "additions": 14,
            "deletions": 2,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/781bbc4d980abd2b21c332fd3122b733dba35d10/tests%2Fmodels%2Fllama%2Ftest_tokenization_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/781bbc4d980abd2b21c332fd3122b733dba35d10/tests%2Fmodels%2Fllama%2Ftest_tokenization_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllama%2Ftest_tokenization_llama.py?ref=781bbc4d980abd2b21c332fd3122b733dba35d10",
            "patch": "@@ -846,7 +846,8 @@ class TikTokenIntegrationTests(unittest.TestCase):\n     \"\"\"\n \n     def test_tiktoken_llama(self):\n-        model_path = \"hf-internal-testing/Llama3-Instruct-Internal\"\n+        model_path = \"hf-internal-testing/llama-3-8b-internal\"\n+        subfolder = \"original\"\n         test_text = \"This is a test sentence.\"\n         test_tokens = [128000, 2028, 374, 264, 1296, 11914, 13, 128001]\n         num_reserved_special_tokens = 256\n@@ -866,6 +867,7 @@ def test_tiktoken_llama(self):\n \n         tiktoken_tokenizer = PreTrainedTokenizerFast.from_pretrained(\n             model_path,\n+            subfolder=subfolder,\n             additional_special_tokens=special_tokens,\n             bos_token=\"<|begin_of_text|>\",\n             eos_token=\"<|end_of_text|>\",\n@@ -874,7 +876,14 @@ def test_tiktoken_llama(self):\n         self.assertEqual(tokens[0], \"<|begin_of_text|>\")\n \n         tiktoken_tokenizer = AutoTokenizer.from_pretrained(\n-            model_path, legacy=False, additional_special_tokens=special_tokens, add_bos_token=True, add_eos_token=True\n+            model_path,\n+            subfolder=subfolder,\n+            legacy=False,\n+            additional_special_tokens=special_tokens,\n+            bos_token=\"<|begin_of_text|>\",\n+            eos_token=\"<|end_of_text|>\",\n+            add_bos_token=True,\n+            add_eos_token=True,\n         )\n         self.assertTrue(isinstance(tiktoken_tokenizer, PreTrainedTokenizerFast))\n \n@@ -892,7 +901,10 @@ def test_tiktoken_llama(self):\n \n         tiktoken_tokenizer = AutoTokenizer.from_pretrained(\n             model_path,\n+            subfolder=subfolder,\n             additional_special_tokens=special_tokens,\n+            bos_token=\"<|begin_of_text|>\",\n+            eos_token=\"<|end_of_text|>\",\n             from_slow=True,\n             add_bos_token=True,\n             add_eos_token=True,"
        }
    ],
    "stats": {
        "total": 16,
        "additions": 14,
        "deletions": 2
    }
}