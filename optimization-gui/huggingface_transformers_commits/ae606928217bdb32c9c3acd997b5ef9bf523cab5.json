{
    "author": "cyyever",
    "message": "Remove unused arguments (#40916)\n\n* Fix unused arguments\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* More fixes\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n---------\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>",
    "sha": "ae606928217bdb32c9c3acd997b5ef9bf523cab5",
    "files": [
        {
            "sha": "cd42288aebfaff699e74c2b1c04efccb4192e31b",
            "filename": "src/transformers/generation/candidate_generator.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ae606928217bdb32c9c3acd997b5ef9bf523cab5/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ae606928217bdb32c9c3acd997b5ef9bf523cab5/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py?ref=ae606928217bdb32c9c3acd997b5ef9bf523cab5",
            "patch": "@@ -524,7 +524,7 @@ def get_candidates(self, input_ids: torch.LongTensor) -> tuple[torch.LongTensor,\n         self.assistant_kwargs.pop(\"attention_mask\", None)\n \n         assistant_output = self.assistant_model.generate(**generation_args, **self.assistant_kwargs)\n-        new_target_ids = self._process_assistant_outputs(input_ids, assistant_output.sequences, assistant_input_ids)\n+        new_target_ids = self._process_assistant_outputs(input_ids, assistant_output.sequences)\n \n         # Update state\n         self.prev_target_ids_len = input_ids.shape[1]\n@@ -583,7 +583,7 @@ def _prepare_assistant_input_ids(self, input_ids: torch.LongTensor) -> tuple[tor\n         return assistant_input_ids, remove_from_pkv\n \n     def _process_assistant_outputs(\n-        self, input_ids: torch.LongTensor, assistant_sequences: torch.LongTensor, assistant_input_ids: torch.LongTensor\n+        self, input_ids: torch.LongTensor, assistant_sequences: torch.LongTensor\n     ) -> torch.LongTensor:\n         \"\"\"Processes assistant outputs to obtain target input IDs.\"\"\"\n         num_prev_assistant = self.prev_assistant_ids.shape[1]"
        },
        {
            "sha": "25cfa411321c0f98f476f473d6a3ac91cda2021a",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ae606928217bdb32c9c3acd997b5ef9bf523cab5/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ae606928217bdb32c9c3acd997b5ef9bf523cab5/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=ae606928217bdb32c9c3acd997b5ef9bf523cab5",
            "patch": "@@ -1412,7 +1412,6 @@ def _find_missing_and_unexpected_keys(\n     checkpoint_keys: list[str],\n     loading_base_model_from_task_state_dict: bool,\n     hf_quantizer: Optional[HfQuantizer],\n-    device_map: dict,\n ) -> tuple[list[str], list[str]]:\n     \"\"\"Find missing keys (keys that are part of the model parameters but were NOT found in the loaded state dict keys) and unexpected keys\n     (keys found in the loaded state dict keys, but that are NOT part of the model parameters)\n@@ -2713,7 +2712,7 @@ def _check_and_adjust_attn_implementation(\n                 try:\n                     self._sdpa_can_dispatch(is_init_check)\n                     applicable_attn_implementation = \"sdpa\"\n-                except (ValueError, ImportError) as e:\n+                except (ValueError, ImportError):\n                     applicable_attn_implementation = \"eager\"\n         else:\n             applicable_attn_implementation = self.get_correct_attn_implementation(\n@@ -5318,7 +5317,6 @@ def _load_pretrained_model(\n             checkpoint_keys,\n             loading_base_model_from_task_state_dict,\n             hf_quantizer,\n-            device_map,\n         )\n         # Find all the keys with shape mismatch (if we ignore the mismatch, the weights need to be newly initialized the\n         # same way as missing keys)"
        },
        {
            "sha": "e7535a8365f459023b31913226d334f3e5a1cbfc",
            "filename": "src/transformers/models/sew_d/modeling_sew_d.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ae606928217bdb32c9c3acd997b5ef9bf523cab5/src%2Ftransformers%2Fmodels%2Fsew_d%2Fmodeling_sew_d.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ae606928217bdb32c9c3acd997b5ef9bf523cab5/src%2Ftransformers%2Fmodels%2Fsew_d%2Fmodeling_sew_d.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsew_d%2Fmodeling_sew_d.py?ref=ae606928217bdb32c9c3acd997b5ef9bf523cab5",
            "patch": "@@ -509,7 +509,7 @@ def forward(ctx, input, mask, dim):\n     @staticmethod\n     def backward(ctx, grad_output):\n         (output,) = ctx.saved_tensors\n-        inputGrad = softmax_backward_data(ctx, grad_output, output, ctx.dim, output)\n+        inputGrad = softmax_backward_data(ctx, grad_output, output)\n         return inputGrad, None, None\n \n     @staticmethod"
        },
        {
            "sha": "11810bc2bea3a2c2f91a94115cf8d59b5d65e271",
            "filename": "src/transformers/pipelines/fill_mask.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ae606928217bdb32c9c3acd997b5ef9bf523cab5/src%2Ftransformers%2Fpipelines%2Ffill_mask.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ae606928217bdb32c9c3acd997b5ef9bf523cab5/src%2Ftransformers%2Fpipelines%2Ffill_mask.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Ffill_mask.py?ref=ae606928217bdb32c9c3acd997b5ef9bf523cab5",
            "patch": "@@ -163,7 +163,7 @@ def postprocess(self, model_outputs, top_k=5, target_ids=None):\n             return result[0]\n         return result\n \n-    def get_target_ids(self, targets, top_k=None):\n+    def get_target_ids(self, targets):\n         if isinstance(targets, str):\n             targets = [targets]\n         try:\n@@ -213,7 +213,7 @@ def _sanitize_parameters(self, top_k=None, targets=None, tokenizer_kwargs=None):\n         postprocess_params = {}\n \n         if targets is not None:\n-            target_ids = self.get_target_ids(targets, top_k)\n+            target_ids = self.get_target_ids(targets)\n             postprocess_params[\"target_ids\"] = target_ids\n \n         if top_k is not None:"
        },
        {
            "sha": "12a990766fc7e4e94c3a344ee489077f3dda0403",
            "filename": "src/transformers/pipelines/table_question_answering.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ae606928217bdb32c9c3acd997b5ef9bf523cab5/src%2Ftransformers%2Fpipelines%2Ftable_question_answering.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ae606928217bdb32c9c3acd997b5ef9bf523cab5/src%2Ftransformers%2Fpipelines%2Ftable_question_answering.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Ftable_question_answering.py?ref=ae606928217bdb32c9c3acd997b5ef9bf523cab5",
            "patch": "@@ -306,7 +306,7 @@ def _sanitize_parameters(self, sequential=None, padding=None, truncation=None, *\n \n         return preprocess_params, forward_params, {}\n \n-    def preprocess(self, pipeline_input, sequential=None, padding=True, truncation=None):\n+    def preprocess(self, pipeline_input, padding=True, truncation=None):\n         if truncation is None:\n             if self.type == \"tapas\":\n                 truncation = \"drop_rows_to_fit\""
        },
        {
            "sha": "b1f41117d4cfeb5242a83197ea4e2b04b2d8e9a7",
            "filename": "src/transformers/pytorch_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ae606928217bdb32c9c3acd997b5ef9bf523cab5/src%2Ftransformers%2Fpytorch_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ae606928217bdb32c9c3acd997b5ef9bf523cab5/src%2Ftransformers%2Fpytorch_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpytorch_utils.py?ref=ae606928217bdb32c9c3acd997b5ef9bf523cab5",
            "patch": "@@ -50,15 +50,15 @@\n _torch_distributed_available = torch.distributed.is_available()\n \n \n-def softmax_backward_data(parent, grad_output, output, dim, self):\n+def softmax_backward_data(parent, grad_output, output):\n     \"\"\"\n     A function that calls the internal `_softmax_backward_data` PyTorch method and that adjusts the arguments according\n     to the torch version detected.\n     \"\"\"\n \n     from torch import _softmax_backward_data\n \n-    return _softmax_backward_data(grad_output, output, parent.dim, self.dtype)\n+    return _softmax_backward_data(grad_output, output, parent.dim, output.dtype)\n \n \n def prune_linear_layer(layer: nn.Linear, index: torch.LongTensor, dim: int = 0) -> nn.Linear:"
        },
        {
            "sha": "f0e3149cd6cef65505845ac48e0130f186c8c629",
            "filename": "src/transformers/trainer_pt_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ae606928217bdb32c9c3acd997b5ef9bf523cab5/src%2Ftransformers%2Ftrainer_pt_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ae606928217bdb32c9c3acd997b5ef9bf523cab5/src%2Ftransformers%2Ftrainer_pt_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer_pt_utils.py?ref=ae606928217bdb32c9c3acd997b5ef9bf523cab5",
            "patch": "@@ -929,7 +929,7 @@ def _secs2timedelta(secs):\n     return f\"{datetime.timedelta(seconds=int(secs))}.{msec:02d}\"\n \n \n-def metrics_format(self, metrics: dict[str, float]) -> dict[str, float]:\n+def metrics_format(metrics: dict[str, float]) -> dict[str, float]:\n     \"\"\"\n     Reformat Trainer metrics values to a human-readable format.\n \n@@ -1038,7 +1038,7 @@ def log_metrics(self, split, metrics):\n         return\n \n     print(f\"***** {split} metrics *****\")\n-    metrics_formatted = self.metrics_format(metrics)\n+    metrics_formatted = metrics_format(metrics)\n     k_width = max(len(str(x)) for x in metrics_formatted)\n     v_width = max(len(str(x)) for x in metrics_formatted.values())\n     for key in sorted(metrics_formatted.keys()):"
        },
        {
            "sha": "e051057f33e2c258f93fbe2d431f19b47ce407e6",
            "filename": "src/transformers/utils/auto_docstring.py",
            "status": "modified",
            "additions": 5,
            "deletions": 10,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/ae606928217bdb32c9c3acd997b5ef9bf523cab5/src%2Ftransformers%2Futils%2Fauto_docstring.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ae606928217bdb32c9c3acd997b5ef9bf523cab5/src%2Ftransformers%2Futils%2Fauto_docstring.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fauto_docstring.py?ref=ae606928217bdb32c9c3acd997b5ef9bf523cab5",
            "patch": "@@ -1227,7 +1227,7 @@ def get_checkpoint_from_config_class(config_class):\n     return checkpoint\n \n \n-def add_intro_docstring(func, class_name, parent_class=None, indent_level=0):\n+def add_intro_docstring(func, class_name, indent_level=0):\n     intro_docstring = \"\"\n     if func.__name__ == \"forward\":\n         intro_docstring = rf\"\"\"The [`{class_name}`] forward method, overrides the `__call__` special method.\n@@ -1469,17 +1469,14 @@ def find_sig_line(lines, line_end):\n     return sig_line_end\n \n \n-def _process_kwargs_parameters(\n-    sig, func, parent_class, model_name_lowercase, documented_kwargs, indent_level, undocumented_parameters\n-):\n+def _process_kwargs_parameters(sig, func, parent_class, documented_kwargs, indent_level, undocumented_parameters):\n     \"\"\"\n     Process **kwargs parameters if needed.\n \n     Args:\n         sig (`inspect.Signature`): Function signature\n         func (`function`): Function the parameters belong to\n         parent_class (`class`): Parent class of the function\n-        model_name_lowercase (`str`): Lowercase model name\n         documented_kwargs (`dict`): Dictionary of kwargs that are already documented\n         indent_level (`int`): Indentation level\n         undocumented_parameters (`list`): List to append undocumented parameters to\n@@ -1510,7 +1507,7 @@ def _process_kwargs_parameters(\n             # Extract documentation for kwargs\n             kwargs_documentation = kwarg_param.annotation.__args__[0].__doc__\n             if kwargs_documentation is not None:\n-                documented_kwargs, _ = parse_docstring(kwargs_documentation)\n+                documented_kwargs = parse_docstring(kwargs_documentation)[0]\n \n             # Process each kwarg parameter\n             for param_name, param_type_annotation in kwarg_param.annotation.__args__[0].__annotations__.items():\n@@ -1597,7 +1594,7 @@ def _process_parameters_section(\n \n     # Process **kwargs parameters if needed\n     kwargs_docstring = _process_kwargs_parameters(\n-        sig, func, parent_class, model_name_lowercase, documented_kwargs, indent_level, undocumented_parameters\n+        sig, func, parent_class, documented_kwargs, indent_level, undocumented_parameters\n     )\n     docstring += kwargs_docstring\n \n@@ -1757,9 +1754,7 @@ def auto_method_docstring(\n         if not docstring.strip().endswith(\"\\n\"):\n             docstring += \"\\n\"\n     else:\n-        docstring = add_intro_docstring(\n-            func, class_name=class_name, parent_class=parent_class, indent_level=indent_level\n-        )\n+        docstring = add_intro_docstring(func, class_name=class_name, indent_level=indent_level)\n \n     # Process Parameters section\n     docstring += _process_parameters_section("
        },
        {
            "sha": "9c69addc94ffced53791958289c3a08284470446",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ae606928217bdb32c9c3acd997b5ef9bf523cab5/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ae606928217bdb32c9c3acd997b5ef9bf523cab5/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=ae606928217bdb32c9c3acd997b5ef9bf523cab5",
            "patch": "@@ -762,7 +762,7 @@ def is_torch_npu_available(check_device=False) -> bool:\n \n \n @lru_cache\n-def is_torch_mlu_available(check_device=False) -> bool:\n+def is_torch_mlu_available() -> bool:\n     \"\"\"\n     Checks if `mlu` is available via an `cndev-based` check which won't trigger the drivers and leave mlu\n     uninitialized."
        },
        {
            "sha": "1f6f79e2699470da5e2a1609dcd87c2de737490c",
            "filename": "src/transformers/video_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/ae606928217bdb32c9c3acd997b5ef9bf523cab5/src%2Ftransformers%2Fvideo_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ae606928217bdb32c9c3acd997b5ef9bf523cab5/src%2Ftransformers%2Fvideo_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fvideo_utils.py?ref=ae606928217bdb32c9c3acd997b5ef9bf523cab5",
            "patch": "@@ -714,7 +714,6 @@ def sample_indices_fn_func(metadata, **fn_kwargs):\n \n def convert_to_rgb(\n     video: np.ndarray,\n-    data_format: Optional[ChannelDimension] = None,\n     input_data_format: Optional[Union[str, ChannelDimension]] = None,\n ) -> np.ndarray:\n     \"\"\"\n@@ -723,15 +722,13 @@ def convert_to_rgb(\n     Args:\n         video (`np.ndarray`):\n             The video to convert.\n-        data_format (`ChannelDimension`, *optional*):\n-            The channel dimension format of the output video. If unset, will use the inferred format from the input.\n         input_data_format (`ChannelDimension`, *optional*):\n             The channel dimension format of the input video. If unset, will use the inferred format from the input.\n     \"\"\"\n     if not isinstance(video, np.ndarray):\n         raise TypeError(f\"Video has to be a numpy array to convert to RGB format, but found {type(video)}\")\n \n-    # np.array usually comes with ChannelDimension.LAST so leet's convert it\n+    # np.array usually comes with ChannelDimension.LAST so let's convert it\n     if input_data_format is None:\n         input_data_format = infer_channel_dimension_format(video)\n     video = to_channel_dimension_format(video, ChannelDimension.FIRST, input_channel_dim=input_data_format)"
        }
    ],
    "stats": {
        "total": 46,
        "additions": 18,
        "deletions": 28
    }
}