{
    "author": "ydshieh",
    "message": "Disable cache for `TokenizerTesterMixin` temporarily (#40611)\n\n* try no cache\n\n* try no cache\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "8a091cc07c9808196e3154ab8bd6252d919f929b",
    "files": [
        {
            "sha": "81f142a8a523da6f16a44fa597581492a2e5b6f2",
            "filename": "tests/models/bart/test_tokenization_bart.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Fbart%2Ftest_tokenization_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Fbart%2Ftest_tokenization_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbart%2Ftest_tokenization_bart.py?ref=8a091cc07c9808196e3154ab8bd6252d919f929b",
            "patch": "@@ -14,14 +14,13 @@\n import json\n import os\n import unittest\n-from functools import lru_cache\n \n from transformers import BartTokenizer, BartTokenizerFast, BatchEncoding\n from transformers.models.roberta.tokenization_roberta import VOCAB_FILES_NAMES\n from transformers.testing_utils import require_tokenizers, require_torch\n from transformers.utils import cached_property\n \n-from ...test_tokenization_common import TokenizerTesterMixin, filter_roberta_detectors, use_cache_if_possible\n+from ...test_tokenization_common import TokenizerTesterMixin, filter_roberta_detectors\n \n \n @require_tokenizers\n@@ -71,16 +70,12 @@ def setUpClass(cls):\n             fp.write(\"\\n\".join(merges))\n \n     @classmethod\n-    @use_cache_if_possible\n-    @lru_cache(maxsize=64)\n     def get_tokenizer(cls, pretrained_name=None, **kwargs):\n         kwargs.update(cls.special_tokens_map)\n         pretrained_name = pretrained_name or cls.tmpdirname\n         return cls.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n \n     @classmethod\n-    @use_cache_if_possible\n-    @lru_cache(maxsize=64)\n     def get_rust_tokenizer(cls, pretrained_name=None, **kwargs):\n         kwargs.update(cls.special_tokens_map)\n         pretrained_name = pretrained_name or cls.tmpdirname"
        },
        {
            "sha": "10e3ad5623cf8113aef47eb2bbaa5e34a5886962",
            "filename": "tests/models/bartpho/test_tokenization_bartpho.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Fbartpho%2Ftest_tokenization_bartpho.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Fbartpho%2Ftest_tokenization_bartpho.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbartpho%2Ftest_tokenization_bartpho.py?ref=8a091cc07c9808196e3154ab8bd6252d919f929b",
            "patch": "@@ -14,12 +14,11 @@\n \n import os\n import unittest\n-from functools import lru_cache\n \n from transformers.models.bartpho.tokenization_bartpho import VOCAB_FILES_NAMES, BartphoTokenizer\n from transformers.testing_utils import get_tests_dir\n \n-from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n+from ...test_tokenization_common import TokenizerTesterMixin\n \n \n SAMPLE_VOCAB = get_tests_dir(\"fixtures/test_sentencepiece_bpe.model\")\n@@ -48,8 +47,6 @@ def setUpClass(cls):\n         tokenizer.save_pretrained(cls.tmpdirname)\n \n     @classmethod\n-    @use_cache_if_possible\n-    @lru_cache(maxsize=64)\n     def get_tokenizer(cls, pretrained_name=None, **kwargs):\n         kwargs.update(cls.special_tokens_map)\n         pretrained_name = pretrained_name or cls.tmpdirname"
        },
        {
            "sha": "0876da773805837c0776b6c7bbfa804590429342",
            "filename": "tests/models/bert_japanese/test_tokenization_bert_japanese.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Fbert_japanese%2Ftest_tokenization_bert_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Fbert_japanese%2Ftest_tokenization_bert_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbert_japanese%2Ftest_tokenization_bert_japanese.py?ref=8a091cc07c9808196e3154ab8bd6252d919f929b",
            "patch": "@@ -16,7 +16,6 @@\n import os\n import pickle\n import unittest\n-from functools import lru_cache\n \n from transformers import AutoTokenizer\n from transformers.models.bert.tokenization_bert import BertTokenizer\n@@ -31,7 +30,7 @@\n )\n from transformers.testing_utils import custom_tokenizers, require_jumanpp, require_sudachi_projection\n \n-from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n+from ...test_tokenization_common import TokenizerTesterMixin\n \n \n @custom_tokenizers\n@@ -420,8 +419,6 @@ def setUpClass(cls):\n             vocab_writer.write(\"\".join([x + \"\\n\" for x in vocab_tokens]))\n \n     @classmethod\n-    @use_cache_if_possible\n-    @lru_cache(maxsize=64)\n     def get_tokenizer(cls, pretrained_name=None, **kwargs):\n         return BertJapaneseTokenizer.from_pretrained(cls.tmpdirname, subword_tokenizer_type=\"character\", **kwargs)\n "
        },
        {
            "sha": "c6bffb7e2e68b778a9c0b2b241beff493307fae1",
            "filename": "tests/models/bertweet/test_tokenization_bertweet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Fbertweet%2Ftest_tokenization_bertweet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Fbertweet%2Ftest_tokenization_bertweet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbertweet%2Ftest_tokenization_bertweet.py?ref=8a091cc07c9808196e3154ab8bd6252d919f929b",
            "patch": "@@ -14,11 +14,10 @@\n \n import os\n import unittest\n-from functools import lru_cache\n \n from transformers.models.bertweet.tokenization_bertweet import VOCAB_FILES_NAMES, BertweetTokenizer\n \n-from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n+from ...test_tokenization_common import TokenizerTesterMixin\n \n \n class BertweetTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n@@ -45,8 +44,6 @@ def setUpClass(cls):\n             fp.write(\"\\n\".join(merges))\n \n     @classmethod\n-    @use_cache_if_possible\n-    @lru_cache(maxsize=64)\n     def get_tokenizer(cls, pretrained_name=None, **kwargs):\n         kwargs.update(cls.special_tokens_map)\n         pretrained_name = pretrained_name or cls.tmpdirname"
        },
        {
            "sha": "5bd8622fa5a0bae2f63824044aff6738bb74738d",
            "filename": "tests/models/blenderbot_small/test_tokenization_blenderbot_small.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Fblenderbot_small%2Ftest_tokenization_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Fblenderbot_small%2Ftest_tokenization_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblenderbot_small%2Ftest_tokenization_blenderbot_small.py?ref=8a091cc07c9808196e3154ab8bd6252d919f929b",
            "patch": "@@ -17,14 +17,13 @@\n import json\n import os\n import unittest\n-from functools import lru_cache\n \n from transformers.models.blenderbot_small.tokenization_blenderbot_small import (\n     VOCAB_FILES_NAMES,\n     BlenderbotSmallTokenizer,\n )\n \n-from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n+from ...test_tokenization_common import TokenizerTesterMixin\n \n \n class BlenderbotSmallTokenizerTest(TokenizerTesterMixin, unittest.TestCase):\n@@ -50,8 +49,6 @@ def setUpClass(cls):\n             fp.write(\"\\n\".join(merges))\n \n     @classmethod\n-    @use_cache_if_possible\n-    @lru_cache(maxsize=64)\n     def get_tokenizer(cls, pretrained_name=None, **kwargs):\n         kwargs.update(cls.special_tokens_map)\n         pretrained_name = pretrained_name or cls.tmpdirname"
        },
        {
            "sha": "79d330e402777644a7b2e8d3cfb52ef08f36abd6",
            "filename": "tests/models/bloom/test_tokenization_bloom.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Fbloom%2Ftest_tokenization_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Fbloom%2Ftest_tokenization_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbloom%2Ftest_tokenization_bloom.py?ref=8a091cc07c9808196e3154ab8bd6252d919f929b",
            "patch": "@@ -14,14 +14,13 @@\n \n import copy\n import unittest\n-from functools import lru_cache\n \n from datasets import load_dataset\n \n from transformers import BloomTokenizerFast\n from transformers.testing_utils import require_jinja, require_tokenizers\n \n-from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n+from ...test_tokenization_common import TokenizerTesterMixin\n \n \n @require_tokenizers\n@@ -42,8 +41,6 @@ def setUpClass(cls):\n         tokenizer.save_pretrained(cls.tmpdirname)\n \n     @classmethod\n-    @use_cache_if_possible\n-    @lru_cache(maxsize=64)\n     def get_rust_tokenizer(cls, pretrained_name=None, **kwargs):\n         _kwargs = copy.deepcopy(cls.special_tokens_map)\n         _kwargs.update(kwargs)"
        },
        {
            "sha": "a6eba6231d595caece9e6f660e05e0c3402d1f49",
            "filename": "tests/models/byt5/test_tokenization_byt5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Fbyt5%2Ftest_tokenization_byt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Fbyt5%2Ftest_tokenization_byt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbyt5%2Ftest_tokenization_byt5.py?ref=8a091cc07c9808196e3154ab8bd6252d919f929b",
            "patch": "@@ -18,12 +18,11 @@\n import shutil\n import tempfile\n import unittest\n-from functools import lru_cache\n \n from transformers import AddedToken, BatchEncoding, ByT5Tokenizer\n from transformers.utils import cached_property, is_tf_available, is_torch_available\n \n-from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n+from ...test_tokenization_common import TokenizerTesterMixin\n \n \n if is_torch_available():\n@@ -49,8 +48,6 @@ def t5_base_tokenizer(self):\n         return ByT5Tokenizer.from_pretrained(\"google/byt5-small\")\n \n     @classmethod\n-    @use_cache_if_possible\n-    @lru_cache(maxsize=64)\n     def get_tokenizer(cls, pretrained_name=None, **kwargs) -> ByT5Tokenizer:\n         pretrained_name = pretrained_name or cls.tmpdirname\n         return cls.tokenizer_class.from_pretrained(pretrained_name, **kwargs)"
        },
        {
            "sha": "adcceb6b732bba070557c1d227c6ad271465b042",
            "filename": "tests/models/canine/test_tokenization_canine.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Fcanine%2Ftest_tokenization_canine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Fcanine%2Ftest_tokenization_canine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcanine%2Ftest_tokenization_canine.py?ref=8a091cc07c9808196e3154ab8bd6252d919f929b",
            "patch": "@@ -17,14 +17,13 @@\n import shutil\n import tempfile\n import unittest\n-from functools import lru_cache\n \n from transformers import BatchEncoding, CanineTokenizer\n from transformers.testing_utils import require_tokenizers, require_torch\n from transformers.tokenization_utils import AddedToken\n from transformers.utils import cached_property\n \n-from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n+from ...test_tokenization_common import TokenizerTesterMixin\n \n \n class CanineTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n@@ -43,8 +42,6 @@ def canine_tokenizer(self):\n         return CanineTokenizer.from_pretrained(\"google/canine-s\")\n \n     @classmethod\n-    @use_cache_if_possible\n-    @lru_cache(maxsize=64)\n     def get_tokenizer(cls, pretrained_name=None, **kwargs) -> CanineTokenizer:\n         pretrained_name = pretrained_name or cls.tmpdirname\n         tokenizer = cls.tokenizer_class.from_pretrained(pretrained_name, **kwargs)"
        },
        {
            "sha": "06651fc0f206cc62f9f78cec7c7c7e30b76a28ff",
            "filename": "tests/models/clip/test_tokenization_clip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Fclip%2Ftest_tokenization_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Fclip%2Ftest_tokenization_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclip%2Ftest_tokenization_clip.py?ref=8a091cc07c9808196e3154ab8bd6252d919f929b",
            "patch": "@@ -16,13 +16,12 @@\n import json\n import os\n import unittest\n-from functools import lru_cache\n \n from transformers import CLIPTokenizer, CLIPTokenizerFast\n from transformers.models.clip.tokenization_clip import VOCAB_FILES_NAMES\n from transformers.testing_utils import require_ftfy, require_tokenizers\n \n-from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n+from ...test_tokenization_common import TokenizerTesterMixin\n \n \n @require_tokenizers\n@@ -51,16 +50,12 @@ def setUpClass(cls):\n             fp.write(\"\\n\".join(merges))\n \n     @classmethod\n-    @use_cache_if_possible\n-    @lru_cache(maxsize=64)\n     def get_tokenizer(cls, pretrained_name=None, **kwargs):\n         kwargs.update(cls.special_tokens_map)\n         pretrained_name = pretrained_name or cls.tmpdirname\n         return CLIPTokenizer.from_pretrained(pretrained_name, **kwargs)\n \n     @classmethod\n-    @use_cache_if_possible\n-    @lru_cache(maxsize=64)\n     def get_rust_tokenizer(cls, pretrained_name=None, **kwargs):\n         kwargs.update(cls.special_tokens_map)\n         pretrained_name = pretrained_name or cls.tmpdirname"
        },
        {
            "sha": "e1c030272fed61d4a099e3b8b29b28113ba4c8d9",
            "filename": "tests/models/clvp/test_tokenization_clvp.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Fclvp%2Ftest_tokenization_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Fclvp%2Ftest_tokenization_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclvp%2Ftest_tokenization_clvp.py?ref=8a091cc07c9808196e3154ab8bd6252d919f929b",
            "patch": "@@ -16,11 +16,10 @@\n import json\n import os\n import unittest\n-from functools import lru_cache\n \n from transformers import ClvpTokenizer\n \n-from ...test_tokenization_common import TokenizerTesterMixin, slow, use_cache_if_possible\n+from ...test_tokenization_common import TokenizerTesterMixin, slow\n \n \n class ClvpTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n@@ -73,8 +72,6 @@ def setUpClass(cls):\n \n     # Copied from transformers.tests.models.gpt2.test_tokenization_gpt2.GPT2TokenizationTest.get_tokenizer with GPT2->Clvp\n     @classmethod\n-    @use_cache_if_possible\n-    @lru_cache(maxsize=64)\n     def get_tokenizer(cls, pretrained_name=None, **kwargs):\n         kwargs.update(cls.special_tokens_map)\n         pretrained_name = pretrained_name or cls.tmpdirname"
        },
        {
            "sha": "5f850211d7ca9ffb379dcb1f309d2a881dd9b4c7",
            "filename": "tests/models/codegen/test_tokenization_codegen.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Fcodegen%2Ftest_tokenization_codegen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Fcodegen%2Ftest_tokenization_codegen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcodegen%2Ftest_tokenization_codegen.py?ref=8a091cc07c9808196e3154ab8bd6252d919f929b",
            "patch": "@@ -17,13 +17,12 @@\n import os\n import re\n import unittest\n-from functools import lru_cache\n \n from transformers import CodeGenTokenizer, CodeGenTokenizerFast\n from transformers.models.codegen.tokenization_codegen import VOCAB_FILES_NAMES\n from transformers.testing_utils import require_tokenizers, slow\n \n-from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n+from ...test_tokenization_common import TokenizerTesterMixin\n \n \n @require_tokenizers\n@@ -75,16 +74,12 @@ def setUpClass(cls):\n             fp.write(\"\\n\".join(merges))\n \n     @classmethod\n-    @use_cache_if_possible\n-    @lru_cache(maxsize=64)\n     def get_tokenizer(cls, pretrained_name=None, **kwargs):\n         kwargs.update(cls.special_tokens_map)\n         pretrained_name = pretrained_name or cls.tmpdirname\n         return CodeGenTokenizer.from_pretrained(pretrained_name, **kwargs)\n \n     @classmethod\n-    @use_cache_if_possible\n-    @lru_cache(maxsize=64)\n     def get_rust_tokenizer(cls, pretrained_name=None, **kwargs):\n         kwargs.update(cls.special_tokens_map)\n         pretrained_name = pretrained_name or cls.tmpdirname"
        },
        {
            "sha": "73a0942522acaf6a7beffda0e8c15d4c29481f20",
            "filename": "tests/models/cohere/test_tokenization_cohere.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Fcohere%2Ftest_tokenization_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Fcohere%2Ftest_tokenization_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcohere%2Ftest_tokenization_cohere.py?ref=8a091cc07c9808196e3154ab8bd6252d919f929b",
            "patch": "@@ -14,7 +14,6 @@\n \n import copy\n import unittest\n-from functools import lru_cache\n \n from transformers import CohereTokenizerFast\n from transformers.testing_utils import (\n@@ -23,7 +22,7 @@\n     require_torch_multi_accelerator,\n )\n \n-from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n+from ...test_tokenization_common import TokenizerTesterMixin\n \n \n @require_tokenizers\n@@ -49,8 +48,6 @@ def setUpClass(cls):\n         tokenizer.save_pretrained(cls.tmpdirname)\n \n     @classmethod\n-    @use_cache_if_possible\n-    @lru_cache(maxsize=64)\n     def get_rust_tokenizer(cls, pretrained_name=None, **kwargs):\n         _kwargs = copy.deepcopy(cls.special_tokens_map)\n         _kwargs.update(kwargs)"
        },
        {
            "sha": "6b8030ab7f62d6d47a8da68064e1d63c1321c03a",
            "filename": "tests/models/ctrl/test_tokenization_ctrl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Fctrl%2Ftest_tokenization_ctrl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Fctrl%2Ftest_tokenization_ctrl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fctrl%2Ftest_tokenization_ctrl.py?ref=8a091cc07c9808196e3154ab8bd6252d919f929b",
            "patch": "@@ -15,11 +15,10 @@\n import json\n import os\n import unittest\n-from functools import lru_cache\n \n from transformers.models.ctrl.tokenization_ctrl import VOCAB_FILES_NAMES, CTRLTokenizer\n \n-from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n+from ...test_tokenization_common import TokenizerTesterMixin\n \n \n class CTRLTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n@@ -46,8 +45,6 @@ def setUpClass(cls):\n             fp.write(\"\\n\".join(merges))\n \n     @classmethod\n-    @use_cache_if_possible\n-    @lru_cache(maxsize=64)\n     def get_tokenizer(cls, pretrained_name=None, **kwargs):\n         kwargs.update(cls.special_tokens_map)\n         pretrained_name = pretrained_name or cls.tmpdirname"
        },
        {
            "sha": "c55e995ab500af48a5c8d61100f51b82db063df1",
            "filename": "tests/models/deberta/test_tokenization_deberta.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Fdeberta%2Ftest_tokenization_deberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Fdeberta%2Ftest_tokenization_deberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeberta%2Ftest_tokenization_deberta.py?ref=8a091cc07c9808196e3154ab8bd6252d919f929b",
            "patch": "@@ -16,13 +16,12 @@\n import json\n import os\n import unittest\n-from functools import lru_cache\n \n from transformers import DebertaTokenizer, DebertaTokenizerFast\n from transformers.models.deberta.tokenization_deberta import VOCAB_FILES_NAMES\n from transformers.testing_utils import slow\n \n-from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n+from ...test_tokenization_common import TokenizerTesterMixin\n \n \n class DebertaTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n@@ -70,8 +69,6 @@ def setUpClass(cls):\n             fp.write(\"\\n\".join(merges))\n \n     @classmethod\n-    @use_cache_if_possible\n-    @lru_cache(maxsize=64)\n     def get_tokenizer(cls, pretrained_name=None, **kwargs):\n         kwargs.update(cls.special_tokens_map)\n         pretrained_name = pretrained_name or cls.tmpdirname"
        },
        {
            "sha": "24a2353e3685eca47b35c04e21d9fe7c02a461dc",
            "filename": "tests/models/esm/test_tokenization_esm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Fesm%2Ftest_tokenization_esm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Fesm%2Ftest_tokenization_esm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fesm%2Ftest_tokenization_esm.py?ref=8a091cc07c9808196e3154ab8bd6252d919f929b",
            "patch": "@@ -16,15 +16,12 @@\n import os\n import tempfile\n import unittest\n-from functools import lru_cache\n \n from transformers.models.esm.tokenization_esm import VOCAB_FILES_NAMES, EsmTokenizer\n from transformers.testing_utils import require_tokenizers\n from transformers.tokenization_utils import PreTrainedTokenizer\n from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n \n-from ...test_tokenization_common import use_cache_if_possible\n-\n \n @require_tokenizers\n class ESMTokenizationTest(unittest.TestCase):\n@@ -44,8 +41,6 @@ def get_tokenizers(cls, **kwargs) -> list[PreTrainedTokenizerBase]:\n         return [cls.get_tokenizer(**kwargs)]\n \n     @classmethod\n-    @use_cache_if_possible\n-    @lru_cache(maxsize=64)\n     def get_tokenizer(cls, pretrained_name=None, **kwargs) -> PreTrainedTokenizer:\n         pretrained_name = pretrained_name or cls.tmpdirname\n         return cls.tokenizer_class.from_pretrained(pretrained_name, **kwargs)"
        },
        {
            "sha": "38b70b87625b861453ce2a80acbfe253b27c6f03",
            "filename": "tests/models/funnel/test_tokenization_funnel.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Ffunnel%2Ftest_tokenization_funnel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Ffunnel%2Ftest_tokenization_funnel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffunnel%2Ftest_tokenization_funnel.py?ref=8a091cc07c9808196e3154ab8bd6252d919f929b",
            "patch": "@@ -15,13 +15,12 @@\n \n import os\n import unittest\n-from functools import lru_cache\n \n from transformers import FunnelTokenizer, FunnelTokenizerFast\n from transformers.models.funnel.tokenization_funnel import VOCAB_FILES_NAMES\n from transformers.testing_utils import require_tokenizers\n \n-from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n+from ...test_tokenization_common import TokenizerTesterMixin\n \n \n @require_tokenizers\n@@ -56,15 +55,11 @@ def setUpClass(cls):\n             vocab_writer.write(\"\".join([x + \"\\n\" for x in vocab_tokens]))\n \n     @classmethod\n-    @use_cache_if_possible\n-    @lru_cache(maxsize=64)\n     def get_tokenizer(cls, pretrained_name=None, **kwargs):\n         pretrained_name = pretrained_name or cls.tmpdirname\n         return FunnelTokenizer.from_pretrained(pretrained_name, **kwargs)\n \n     @classmethod\n-    @use_cache_if_possible\n-    @lru_cache(maxsize=64)\n     def get_rust_tokenizer(cls, pretrained_name=None, **kwargs):\n         pretrained_name = pretrained_name or cls.tmpdirname\n         return FunnelTokenizerFast.from_pretrained(pretrained_name, **kwargs)"
        },
        {
            "sha": "c69e0b5210862c9d2fbd84cf1f3c0be15803859c",
            "filename": "tests/models/gpt2/test_tokenization_gpt2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Fgpt2%2Ftest_tokenization_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Fgpt2%2Ftest_tokenization_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt2%2Ftest_tokenization_gpt2.py?ref=8a091cc07c9808196e3154ab8bd6252d919f929b",
            "patch": "@@ -16,13 +16,12 @@\n import json\n import os\n import unittest\n-from functools import lru_cache\n \n from transformers import AutoTokenizer, GPT2Tokenizer, GPT2TokenizerFast\n from transformers.models.gpt2.tokenization_gpt2 import VOCAB_FILES_NAMES\n from transformers.testing_utils import require_jinja, require_tiktoken, require_tokenizers\n \n-from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n+from ...test_tokenization_common import TokenizerTesterMixin\n \n \n @require_tokenizers\n@@ -74,16 +73,12 @@ def setUpClass(cls):\n             fp.write(\"\\n\".join(merges))\n \n     @classmethod\n-    @use_cache_if_possible\n-    @lru_cache(maxsize=64)\n     def get_tokenizer(cls, pretrained_name=None, **kwargs):\n         kwargs.update(cls.special_tokens_map)\n         pretrained_name = pretrained_name or cls.tmpdirname\n         return GPT2Tokenizer.from_pretrained(pretrained_name, **kwargs)\n \n     @classmethod\n-    @use_cache_if_possible\n-    @lru_cache(maxsize=64)\n     def get_rust_tokenizer(cls, pretrained_name=None, **kwargs):\n         kwargs.update(cls.special_tokens_map)\n         pretrained_name = pretrained_name or cls.tmpdirname"
        },
        {
            "sha": "0e4732f270629ae9587ace02e644a8ba1d7963b1",
            "filename": "tests/models/gpt_neox_japanese/test_tokenization_gpt_neox_japanese.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Fgpt_neox_japanese%2Ftest_tokenization_gpt_neox_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Fgpt_neox_japanese%2Ftest_tokenization_gpt_neox_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt_neox_japanese%2Ftest_tokenization_gpt_neox_japanese.py?ref=8a091cc07c9808196e3154ab8bd6252d919f929b",
            "patch": "@@ -16,15 +16,14 @@\n import json\n import os\n import unittest\n-from functools import lru_cache\n \n from transformers.models.gpt_neox_japanese.tokenization_gpt_neox_japanese import (\n     VOCAB_FILES_NAMES,\n     GPTNeoXJapaneseTokenizer,\n )\n from transformers.testing_utils import require_tokenizers, slow\n \n-from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n+from ...test_tokenization_common import TokenizerTesterMixin\n \n \n @require_tokenizers\n@@ -73,8 +72,6 @@ def setUpClass(cls):\n             emoji_writer.write(json.dumps(emoji_tokens))\n \n     @classmethod\n-    @use_cache_if_possible\n-    @lru_cache(maxsize=64)\n     def get_tokenizer(cls, pretrained_name=None, **kwargs):\n         kwargs.update(cls.special_tokens_map)\n         pretrained_name = pretrained_name or cls.tmpdirname"
        },
        {
            "sha": "83df38a3b1536b4eaa8d1082505f506fe5b540f5",
            "filename": "tests/models/layoutlm/test_tokenization_layoutlm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Flayoutlm%2Ftest_tokenization_layoutlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Flayoutlm%2Ftest_tokenization_layoutlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flayoutlm%2Ftest_tokenization_layoutlm.py?ref=8a091cc07c9808196e3154ab8bd6252d919f929b",
            "patch": "@@ -15,13 +15,12 @@\n \n import os\n import unittest\n-from functools import lru_cache\n \n from transformers import LayoutLMTokenizer, LayoutLMTokenizerFast\n from transformers.models.layoutlm.tokenization_layoutlm import VOCAB_FILES_NAMES\n from transformers.testing_utils import require_tokenizers\n \n-from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n+from ...test_tokenization_common import TokenizerTesterMixin\n \n \n @require_tokenizers\n@@ -56,8 +55,6 @@ def setUpClass(cls):\n             vocab_writer.write(\"\".join([x + \"\\n\" for x in vocab_tokens]))\n \n     @classmethod\n-    @use_cache_if_possible\n-    @lru_cache(maxsize=64)\n     def get_tokenizer(cls, pretrained_name=None, **kwargs):\n         pretrained_name = pretrained_name or cls.tmpdirname\n         return LayoutLMTokenizer.from_pretrained(pretrained_name, **kwargs)"
        },
        {
            "sha": "b07768c65ac1e84d895fe2c0e17503ba9b947676",
            "filename": "tests/models/layoutlmv3/test_tokenization_layoutlmv3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Flayoutlmv3%2Ftest_tokenization_layoutlmv3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Flayoutlmv3%2Ftest_tokenization_layoutlmv3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flayoutlmv3%2Ftest_tokenization_layoutlmv3.py?ref=8a091cc07c9808196e3154ab8bd6252d919f929b",
            "patch": "@@ -19,7 +19,6 @@\n import shutil\n import tempfile\n import unittest\n-from functools import lru_cache\n \n from parameterized import parameterized\n \n@@ -43,7 +42,6 @@\n     SMALL_TRAINING_CORPUS,\n     TokenizerTesterMixin,\n     merge_model_tokenizer_mappings,\n-    use_cache_if_possible,\n )\n \n \n@@ -133,16 +131,12 @@ def setUpClass(cls):\n             fp.write(\"\\n\".join(merges))\n \n     @classmethod\n-    @use_cache_if_possible\n-    @lru_cache(maxsize=64)\n     def get_tokenizer(cls, pretrained_name=None, **kwargs):\n         kwargs.update(cls.special_tokens_map)\n         pretrained_name = pretrained_name or cls.tmpdirname\n         return cls.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n \n     @classmethod\n-    @use_cache_if_possible\n-    @lru_cache(maxsize=64)\n     def get_rust_tokenizer(cls, pretrained_name=None, **kwargs):\n         kwargs.update(cls.special_tokens_map)\n         pretrained_name = pretrained_name or cls.tmpdirname"
        },
        {
            "sha": "5290138d57306b393813873b66aee54ceec5609e",
            "filename": "tests/models/led/test_tokenization_led.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Fled%2Ftest_tokenization_led.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Fled%2Ftest_tokenization_led.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fled%2Ftest_tokenization_led.py?ref=8a091cc07c9808196e3154ab8bd6252d919f929b",
            "patch": "@@ -14,14 +14,13 @@\n import json\n import os\n import unittest\n-from functools import lru_cache\n \n from transformers import BatchEncoding, LEDTokenizer, LEDTokenizerFast\n from transformers.models.led.tokenization_led import VOCAB_FILES_NAMES\n from transformers.testing_utils import require_tokenizers, require_torch\n from transformers.utils import cached_property\n \n-from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n+from ...test_tokenization_common import TokenizerTesterMixin\n \n \n @require_tokenizers\n@@ -69,16 +68,12 @@ def setUpClass(cls):\n             fp.write(\"\\n\".join(merges))\n \n     @classmethod\n-    @use_cache_if_possible\n-    @lru_cache(maxsize=64)\n     def get_tokenizer(cls, pretrained_name=None, **kwargs):\n         kwargs.update(cls.special_tokens_map)\n         pretrained_name = pretrained_name or cls.tmpdirname\n         return cls.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n \n     @classmethod\n-    @use_cache_if_possible\n-    @lru_cache(maxsize=64)\n     def get_rust_tokenizer(cls, pretrained_name=None, **kwargs):\n         kwargs.update(cls.special_tokens_map)\n         pretrained_name = pretrained_name or cls.tmpdirname"
        },
        {
            "sha": "bad745e982bf728d8695df18247021fb5da1092c",
            "filename": "tests/models/longformer/test_tokenization_longformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Flongformer%2Ftest_tokenization_longformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Flongformer%2Ftest_tokenization_longformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flongformer%2Ftest_tokenization_longformer.py?ref=8a091cc07c9808196e3154ab8bd6252d919f929b",
            "patch": "@@ -17,13 +17,12 @@\n import json\n import os\n import unittest\n-from functools import lru_cache\n \n from transformers import AddedToken, LongformerTokenizer, LongformerTokenizerFast\n from transformers.models.longformer.tokenization_longformer import VOCAB_FILES_NAMES\n from transformers.testing_utils import require_tokenizers, slow\n \n-from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n+from ...test_tokenization_common import TokenizerTesterMixin\n \n \n @require_tokenizers\n@@ -75,16 +74,12 @@ def setUpClass(cls):\n             fp.write(\"\\n\".join(merges))\n \n     @classmethod\n-    @use_cache_if_possible\n-    @lru_cache(maxsize=64)\n     def get_tokenizer(cls, pretrained_name=None, **kwargs):\n         kwargs.update(cls.special_tokens_map)\n         pretrained_name = pretrained_name or cls.tmpdirname\n         return cls.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n \n     @classmethod\n-    @use_cache_if_possible\n-    @lru_cache(maxsize=64)\n     def get_rust_tokenizer(cls, pretrained_name=None, **kwargs):\n         kwargs.update(cls.special_tokens_map)\n         pretrained_name = pretrained_name or cls.tmpdirname"
        },
        {
            "sha": "8db058f882d0c816239bb24d2906a36fab3cefe3",
            "filename": "tests/models/luke/test_tokenization_luke.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Fluke%2Ftest_tokenization_luke.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Fluke%2Ftest_tokenization_luke.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fluke%2Ftest_tokenization_luke.py?ref=8a091cc07c9808196e3154ab8bd6252d919f929b",
            "patch": "@@ -13,12 +13,11 @@\n # limitations under the License.\n \n import unittest\n-from functools import lru_cache\n \n from transformers import AddedToken, LukeTokenizer\n from transformers.testing_utils import get_tests_dir, require_torch, slow\n \n-from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n+from ...test_tokenization_common import TokenizerTesterMixin\n \n \n SAMPLE_VOCAB = get_tests_dir(\"fixtures/vocab.json\")\n@@ -39,8 +38,6 @@ def setUpClass(cls):\n         cls.special_tokens_map = {\"entity_token_1\": \"<ent>\", \"entity_token_2\": \"<ent2>\"}\n \n     @classmethod\n-    @use_cache_if_possible\n-    @lru_cache(maxsize=64)\n     def get_tokenizer(cls, task=None, **kwargs):\n         kwargs.update(cls.special_tokens_map)\n         tokenizer = LukeTokenizer("
        },
        {
            "sha": "1e4d63f9b9a068512471043d57cb7a21b45818b9",
            "filename": "tests/models/m2m_100/test_tokenization_m2m_100.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Fm2m_100%2Ftest_tokenization_m2m_100.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Fm2m_100%2Ftest_tokenization_m2m_100.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fm2m_100%2Ftest_tokenization_m2m_100.py?ref=8a091cc07c9808196e3154ab8bd6252d919f929b",
            "patch": "@@ -14,7 +14,6 @@\n \n import tempfile\n import unittest\n-from functools import lru_cache\n from pathlib import Path\n from shutil import copyfile\n \n@@ -33,7 +32,7 @@\n if is_sentencepiece_available():\n     from transformers.models.m2m_100.tokenization_m2m_100 import VOCAB_FILES_NAMES, save_json\n \n-from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n+from ...test_tokenization_common import TokenizerTesterMixin\n \n \n if is_sentencepiece_available():\n@@ -70,8 +69,6 @@ def setUpClass(cls):\n         tokenizer.save_pretrained(cls.tmpdirname)\n \n     @classmethod\n-    @use_cache_if_possible\n-    @lru_cache(maxsize=64)\n     def get_tokenizer(cls, pretrained_name=None, **kwargs):\n         pretrained_name = pretrained_name or cls.tmpdirname\n         return M2M100Tokenizer.from_pretrained(pretrained_name, **kwargs)"
        },
        {
            "sha": "ebe26c5babb76f683f27590ae448d1b979c1285c",
            "filename": "tests/models/marian/test_tokenization_marian.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Fmarian%2Ftest_tokenization_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Fmarian%2Ftest_tokenization_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmarian%2Ftest_tokenization_marian.py?ref=8a091cc07c9808196e3154ab8bd6252d919f929b",
            "patch": "@@ -14,7 +14,6 @@\n \n import tempfile\n import unittest\n-from functools import lru_cache\n from pathlib import Path\n from shutil import copyfile\n \n@@ -26,7 +25,7 @@\n if is_sentencepiece_available():\n     from transformers.models.marian.tokenization_marian import VOCAB_FILES_NAMES, save_json\n \n-from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n+from ...test_tokenization_common import TokenizerTesterMixin\n \n \n SAMPLE_SP = get_tests_dir(\"fixtures/test_sentencepiece.model\")\n@@ -67,8 +66,6 @@ def setUpClass(cls):\n         tokenizer.save_pretrained(cls.tmpdirname)\n \n     @classmethod\n-    @use_cache_if_possible\n-    @lru_cache(maxsize=64)\n     def get_tokenizer(cls, pretrained_name=None, **kwargs) -> MarianTokenizer:\n         pretrained_name = pretrained_name or cls.tmpdirname\n         return MarianTokenizer.from_pretrained(pretrained_name, **kwargs)"
        },
        {
            "sha": "63936d91f9931efea3a2d2c8baa1fe75bf6c2d1b",
            "filename": "tests/models/mgp_str/test_tokenization_mgp_str.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Fmgp_str%2Ftest_tokenization_mgp_str.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Fmgp_str%2Ftest_tokenization_mgp_str.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmgp_str%2Ftest_tokenization_mgp_str.py?ref=8a091cc07c9808196e3154ab8bd6252d919f929b",
            "patch": "@@ -16,13 +16,12 @@\n import json\n import os\n import unittest\n-from functools import lru_cache\n \n from transformers import MgpstrTokenizer\n from transformers.models.mgp_str.tokenization_mgp_str import VOCAB_FILES_NAMES\n from transformers.testing_utils import require_tokenizers\n \n-from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n+from ...test_tokenization_common import TokenizerTesterMixin\n \n \n @require_tokenizers\n@@ -45,8 +44,6 @@ def setUpClass(cls):\n             fp.write(json.dumps(vocab_tokens) + \"\\n\")\n \n     @classmethod\n-    @use_cache_if_possible\n-    @lru_cache(maxsize=64)\n     def get_tokenizer(cls, pretrained_name=None, **kwargs):\n         pretrained_name = pretrained_name or cls.tmpdirname\n         return MgpstrTokenizer.from_pretrained(pretrained_name, **kwargs)"
        },
        {
            "sha": "262188e82204fde7e2357d1de5d0b834fd5adb51",
            "filename": "tests/models/mluke/test_tokenization_mluke.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Fmluke%2Ftest_tokenization_mluke.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Fmluke%2Ftest_tokenization_mluke.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmluke%2Ftest_tokenization_mluke.py?ref=8a091cc07c9808196e3154ab8bd6252d919f929b",
            "patch": "@@ -14,12 +14,11 @@\n \n \n import unittest\n-from functools import lru_cache\n \n from transformers.models.mluke.tokenization_mluke import MLukeTokenizer\n from transformers.testing_utils import get_tests_dir, require_torch, slow\n \n-from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n+from ...test_tokenization_common import TokenizerTesterMixin\n \n \n SAMPLE_VOCAB = get_tests_dir(\"fixtures/test_sentencepiece.model\")\n@@ -39,8 +38,6 @@ def setUpClass(cls):\n         cls.special_tokens_map = {\"entity_token_1\": \"<ent>\", \"entity_token_2\": \"<ent2>\"}\n \n     @classmethod\n-    @use_cache_if_possible\n-    @lru_cache(maxsize=64)\n     def get_tokenizer(cls, task=None, **kwargs):\n         kwargs.update(cls.special_tokens_map)\n         kwargs.update({\"task\": task})"
        },
        {
            "sha": "62906138f3c9c84e9bebbb4cfbc54c27c8db4c47",
            "filename": "tests/models/mvp/test_tokenization_mvp.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Fmvp%2Ftest_tokenization_mvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Fmvp%2Ftest_tokenization_mvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmvp%2Ftest_tokenization_mvp.py?ref=8a091cc07c9808196e3154ab8bd6252d919f929b",
            "patch": "@@ -14,14 +14,13 @@\n import json\n import os\n import unittest\n-from functools import lru_cache\n \n from transformers import BatchEncoding, MvpTokenizer, MvpTokenizerFast\n from transformers.models.roberta.tokenization_roberta import VOCAB_FILES_NAMES\n from transformers.testing_utils import require_tokenizers, require_torch\n from transformers.utils import cached_property\n \n-from ...test_tokenization_common import TokenizerTesterMixin, filter_roberta_detectors, use_cache_if_possible\n+from ...test_tokenization_common import TokenizerTesterMixin, filter_roberta_detectors\n \n \n @require_tokenizers\n@@ -71,16 +70,12 @@ def setUpClass(cls):\n             fp.write(\"\\n\".join(merges))\n \n     @classmethod\n-    @use_cache_if_possible\n-    @lru_cache(maxsize=64)\n     def get_tokenizer(cls, pretrained_name=None, **kwargs):\n         kwargs.update(cls.special_tokens_map)\n         pretrained_name = pretrained_name or cls.tmpdirname\n         return cls.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n \n     @classmethod\n-    @use_cache_if_possible\n-    @lru_cache(maxsize=64)\n     def get_rust_tokenizer(cls, pretrained_name=None, **kwargs):\n         kwargs.update(cls.special_tokens_map)\n         pretrained_name = pretrained_name or cls.tmpdirname"
        },
        {
            "sha": "af0990f515ced03fe359e15fc913144f0378d0cc",
            "filename": "tests/models/nougat/test_tokenization_nougat.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Fnougat%2Ftest_tokenization_nougat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Fnougat%2Ftest_tokenization_nougat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fnougat%2Ftest_tokenization_nougat.py?ref=8a091cc07c9808196e3154ab8bd6252d919f929b",
            "patch": "@@ -14,13 +14,12 @@\n \n import copy\n import unittest\n-from functools import lru_cache\n \n from transformers import NougatTokenizerFast\n from transformers.models.nougat.tokenization_nougat_fast import markdown_compatible, normalize_list_like_lines\n from transformers.testing_utils import require_levenshtein, require_nltk, require_tokenizers\n \n-from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n+from ...test_tokenization_common import TokenizerTesterMixin\n \n \n @require_tokenizers\n@@ -41,8 +40,6 @@ def setUpClass(cls):\n         tokenizer.save_pretrained(cls.tmpdirname)\n \n     @classmethod\n-    @use_cache_if_possible\n-    @lru_cache(maxsize=64)\n     def get_rust_tokenizer(cls, pretrained_name=None, **kwargs):\n         _kwargs = copy.deepcopy(cls.special_tokens_map)\n         _kwargs.update(kwargs)"
        },
        {
            "sha": "1a505b6f6e2313d03c96aee59aca4d2fc3e21954",
            "filename": "tests/models/pegasus/test_tokenization_pegasus.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Fpegasus%2Ftest_tokenization_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Fpegasus%2Ftest_tokenization_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpegasus%2Ftest_tokenization_pegasus.py?ref=8a091cc07c9808196e3154ab8bd6252d919f929b",
            "patch": "@@ -13,13 +13,12 @@\n # limitations under the License.\n \n import unittest\n-from functools import lru_cache\n \n from transformers import PegasusTokenizer, PegasusTokenizerFast\n from transformers.testing_utils import get_tests_dir, require_sentencepiece, require_tokenizers, require_torch, slow\n from transformers.utils import cached_property\n \n-from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n+from ...test_tokenization_common import TokenizerTesterMixin\n \n \n SAMPLE_VOCAB = get_tests_dir(\"fixtures/test_sentencepiece_no_bos.model\")\n@@ -47,8 +46,6 @@ def _large_tokenizer(self):\n         return PegasusTokenizer.from_pretrained(\"google/pegasus-large\")\n \n     @classmethod\n-    @use_cache_if_possible\n-    @lru_cache(maxsize=64)\n     def get_tokenizer(cls, pretrained_name=None, **kwargs) -> PegasusTokenizer:\n         pretrained_name = pretrained_name or cls.tmpdirname\n         return PegasusTokenizer.from_pretrained(pretrained_name, **kwargs)\n@@ -157,8 +154,6 @@ def _large_tokenizer(self):\n         return PegasusTokenizer.from_pretrained(\"google/bigbird-pegasus-large-arxiv\")\n \n     @classmethod\n-    @use_cache_if_possible\n-    @lru_cache(maxsize=64)\n     def get_tokenizer(cls, pretrained_name=None, **kwargs) -> PegasusTokenizer:\n         pretrained_name = pretrained_name or cls.tmpdirname\n         return PegasusTokenizer.from_pretrained(pretrained_name, **kwargs)"
        },
        {
            "sha": "ed6a3d30396a2fea018ab46ca821f67cc2a7488a",
            "filename": "tests/models/perceiver/test_tokenization_perceiver.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Fperceiver%2Ftest_tokenization_perceiver.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Fperceiver%2Ftest_tokenization_perceiver.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fperceiver%2Ftest_tokenization_perceiver.py?ref=8a091cc07c9808196e3154ab8bd6252d919f929b",
            "patch": "@@ -18,12 +18,11 @@\n import shutil\n import tempfile\n import unittest\n-from functools import lru_cache\n \n from transformers import AddedToken, BatchEncoding, PerceiverTokenizer\n from transformers.utils import cached_property, is_tf_available, is_torch_available\n \n-from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n+from ...test_tokenization_common import TokenizerTesterMixin\n \n \n if is_torch_available():\n@@ -50,8 +49,6 @@ def perceiver_tokenizer(self):\n         return PerceiverTokenizer.from_pretrained(\"deepmind/language-perceiver\")\n \n     @classmethod\n-    @use_cache_if_possible\n-    @lru_cache(maxsize=64)\n     def get_tokenizer(cls, pretrained_name=None, **kwargs) -> PerceiverTokenizer:\n         pretrained_name = pretrained_name or cls.tmpdirname\n         return cls.tokenizer_class.from_pretrained(pretrained_name, **kwargs)"
        },
        {
            "sha": "e459f013533b28823878e8ef0f35d3c8caf6d383",
            "filename": "tests/models/phobert/test_tokenization_phobert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Fphobert%2Ftest_tokenization_phobert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Fphobert%2Ftest_tokenization_phobert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fphobert%2Ftest_tokenization_phobert.py?ref=8a091cc07c9808196e3154ab8bd6252d919f929b",
            "patch": "@@ -14,11 +14,10 @@\n \n import os\n import unittest\n-from functools import lru_cache\n \n from transformers.models.phobert.tokenization_phobert import VOCAB_FILES_NAMES, PhobertTokenizer\n \n-from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n+from ...test_tokenization_common import TokenizerTesterMixin\n \n \n class PhobertTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n@@ -46,8 +45,6 @@ def setUpClass(cls):\n             fp.write(\"\\n\".join(merges))\n \n     @classmethod\n-    @use_cache_if_possible\n-    @lru_cache(maxsize=64)\n     def get_tokenizer(cls, pretrained_name=None, **kwargs):\n         kwargs.update(cls.special_tokens_map)\n         pretrained_name = pretrained_name or cls.tmpdirname"
        },
        {
            "sha": "49d879e2bd3a58d3e1a95c125b0146fe576c2eea",
            "filename": "tests/models/qwen2/test_tokenization_qwen2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Fqwen2%2Ftest_tokenization_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Fqwen2%2Ftest_tokenization_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2%2Ftest_tokenization_qwen2.py?ref=8a091cc07c9808196e3154ab8bd6252d919f929b",
            "patch": "@@ -17,13 +17,12 @@\n import json\n import os\n import unittest\n-from functools import lru_cache\n \n from transformers import AddedToken, Qwen2Tokenizer, Qwen2TokenizerFast\n from transformers.models.qwen2.tokenization_qwen2 import VOCAB_FILES_NAMES, bytes_to_unicode\n from transformers.testing_utils import require_tokenizers, slow\n \n-from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n+from ...test_tokenization_common import TokenizerTesterMixin\n \n \n @require_tokenizers\n@@ -93,8 +92,6 @@ def setUpClass(cls):\n             fp.write(\"\\n\".join(merges))\n \n     @classmethod\n-    @use_cache_if_possible\n-    @lru_cache(maxsize=64)\n     def get_tokenizer(cls, pretrained_name=None, **kwargs):\n         _kwargs = copy.deepcopy(cls.special_tokens_map)\n         _kwargs.update(kwargs)\n@@ -103,8 +100,6 @@ def get_tokenizer(cls, pretrained_name=None, **kwargs):\n         return Qwen2Tokenizer.from_pretrained(pretrained_name, **kwargs)\n \n     @classmethod\n-    @use_cache_if_possible\n-    @lru_cache(maxsize=64)\n     def get_rust_tokenizer(cls, pretrained_name=None, **kwargs):\n         _kwargs = copy.deepcopy(cls.special_tokens_map)\n         _kwargs.update(kwargs)"
        },
        {
            "sha": "0990dfd9c796fcd905a3e69c8abcc39656f62273",
            "filename": "tests/models/roberta/test_tokenization_roberta.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Froberta%2Ftest_tokenization_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Froberta%2Ftest_tokenization_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Froberta%2Ftest_tokenization_roberta.py?ref=8a091cc07c9808196e3154ab8bd6252d919f929b",
            "patch": "@@ -17,13 +17,12 @@\n import json\n import os\n import unittest\n-from functools import lru_cache\n \n from transformers import AddedToken, RobertaTokenizer, RobertaTokenizerFast\n from transformers.models.roberta.tokenization_roberta import VOCAB_FILES_NAMES\n from transformers.testing_utils import require_tokenizers, slow\n \n-from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n+from ...test_tokenization_common import TokenizerTesterMixin\n \n \n @require_tokenizers\n@@ -73,16 +72,12 @@ def setUpClass(cls):\n             fp.write(\"\\n\".join(merges))\n \n     @classmethod\n-    @use_cache_if_possible\n-    @lru_cache(maxsize=64)\n     def get_tokenizer(cls, pretrained_name=None, **kwargs):\n         kwargs.update(cls.special_tokens_map)\n         pretrained_name = pretrained_name or cls.tmpdirname\n         return cls.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n \n     @classmethod\n-    @use_cache_if_possible\n-    @lru_cache(maxsize=64)\n     def get_rust_tokenizer(cls, pretrained_name=None, **kwargs):\n         kwargs.update(cls.special_tokens_map)\n         pretrained_name = pretrained_name or cls.tmpdirname"
        },
        {
            "sha": "61b4030b0f97e46e8aaf6061c2bcb29cccc61bf2",
            "filename": "tests/models/roformer/test_tokenization_roformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Froformer%2Ftest_tokenization_roformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Froformer%2Ftest_tokenization_roformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Froformer%2Ftest_tokenization_roformer.py?ref=8a091cc07c9808196e3154ab8bd6252d919f929b",
            "patch": "@@ -14,12 +14,11 @@\n \n import tempfile\n import unittest\n-from functools import lru_cache\n \n from transformers import RoFormerTokenizer, RoFormerTokenizerFast\n from transformers.testing_utils import require_rjieba, require_tokenizers\n \n-from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n+from ...test_tokenization_common import TokenizerTesterMixin\n \n \n @require_rjieba\n@@ -38,15 +37,11 @@ def setUpClass(cls):\n         tokenizer.save_pretrained(cls.tmpdirname)\n \n     @classmethod\n-    @use_cache_if_possible\n-    @lru_cache(maxsize=64)\n     def get_tokenizer(cls, pretrained_name=None, **kwargs):\n         pretrained_name = pretrained_name or cls.tmpdirname\n         return cls.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n \n     @classmethod\n-    @use_cache_if_possible\n-    @lru_cache(maxsize=64)\n     def get_rust_tokenizer(cls, pretrained_name=None, **kwargs):\n         pretrained_name = pretrained_name or cls.tmpdirname\n         return cls.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)"
        },
        {
            "sha": "ab682bd9ed64aa5a71172f95a324d4c987f57c71",
            "filename": "tests/models/siglip/test_tokenization_siglip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Fsiglip%2Ftest_tokenization_siglip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Fsiglip%2Ftest_tokenization_siglip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsiglip%2Ftest_tokenization_siglip.py?ref=8a091cc07c9808196e3154ab8bd6252d919f929b",
            "patch": "@@ -16,13 +16,12 @@\n import os\n import tempfile\n import unittest\n-from functools import lru_cache\n \n from transformers import SPIECE_UNDERLINE, AddedToken, BatchEncoding, SiglipTokenizer\n from transformers.testing_utils import get_tests_dir, require_sentencepiece, require_tokenizers, slow\n from transformers.utils import cached_property, is_tf_available, is_torch_available\n \n-from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n+from ...test_tokenization_common import TokenizerTesterMixin\n \n \n SAMPLE_VOCAB = get_tests_dir(\"fixtures/test_sentencepiece.model\")\n@@ -136,8 +135,6 @@ def siglip_tokenizer(self):\n         return SiglipTokenizer.from_pretrained(\"google/siglip-base-patch16-224\")\n \n     @classmethod\n-    @use_cache_if_possible\n-    @lru_cache(maxsize=64)\n     def get_tokenizer(cls, pretrained_name=None, **kwargs) -> SiglipTokenizer:\n         pretrained_name = pretrained_name or cls.tmpdirname\n         return cls.tokenizer_class.from_pretrained(pretrained_name, **kwargs)"
        },
        {
            "sha": "8fdf5b75b00ed716383afb1acefc8502910ad61c",
            "filename": "tests/models/splinter/test_tokenization_splinter.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Fsplinter%2Ftest_tokenization_splinter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Fsplinter%2Ftest_tokenization_splinter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsplinter%2Ftest_tokenization_splinter.py?ref=8a091cc07c9808196e3154ab8bd6252d919f929b",
            "patch": "@@ -12,9 +12,8 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n import unittest\n-from functools import lru_cache\n \n-from tests.test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n+from tests.test_tokenization_common import TokenizerTesterMixin\n from transformers import SplinterTokenizerFast, is_tf_available, is_torch_available\n from transformers.models.splinter import SplinterTokenizer\n from transformers.testing_utils import get_tests_dir, slow\n@@ -51,15 +50,11 @@ def setUpClass(cls):\n         tokenizer.save_pretrained(cls.tmpdirname)\n \n     @classmethod\n-    @use_cache_if_possible\n-    @lru_cache(maxsize=64)\n     def get_tokenizer(cls, pretrained_name=None, **kwargs) -> SplinterTokenizer:\n         pretrained_name = pretrained_name or cls.tmpdirname\n         return cls.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n \n     @classmethod\n-    @use_cache_if_possible\n-    @lru_cache(maxsize=64)\n     def get_rust_tokenizer(cls, pretrained_name=None, **kwargs) -> SplinterTokenizerFast:\n         pretrained_name = pretrained_name or cls.tmpdirname\n         return cls.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)"
        },
        {
            "sha": "6bcd3ca2f50c75084cedb92df6da6fc4f1f78086",
            "filename": "tests/models/squeezebert/test_tokenization_squeezebert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Fsqueezebert%2Ftest_tokenization_squeezebert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Fsqueezebert%2Ftest_tokenization_squeezebert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsqueezebert%2Ftest_tokenization_squeezebert.py?ref=8a091cc07c9808196e3154ab8bd6252d919f929b",
            "patch": "@@ -12,13 +12,10 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from functools import lru_cache\n \n from transformers import SqueezeBertTokenizer, SqueezeBertTokenizerFast\n from transformers.testing_utils import require_tokenizers, slow\n \n-from ...test_tokenization_common import use_cache_if_possible\n-\n # Avoid import `BertTokenizationTest` directly as it will run as `test_tokenization_squeezebert.py::BertTokenizationTest`\n # together with `test_tokenization_bert.py::BertTokenizationTest`.\n from ..bert import test_tokenization_bert\n@@ -32,8 +29,6 @@ class SqueezeBertTokenizationTest(test_tokenization_bert.BertTokenizationTest):\n     from_pretrained_id = \"squeezebert/squeezebert-uncased\"\n \n     @classmethod\n-    @use_cache_if_possible\n-    @lru_cache(maxsize=64)\n     def get_rust_tokenizer(cls, pretrained_name=None, **kwargs):\n         pretrained_name = pretrained_name or cls.tmpdirname\n         return SqueezeBertTokenizerFast.from_pretrained(pretrained_name, **kwargs)"
        },
        {
            "sha": "31a5503ef1d2fcb7a08d91c95fdea8d2ab00dae1",
            "filename": "tests/models/t5/test_tokenization_t5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Ft5%2Ftest_tokenization_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Ft5%2Ftest_tokenization_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ft5%2Ftest_tokenization_t5.py?ref=8a091cc07c9808196e3154ab8bd6252d919f929b",
            "patch": "@@ -16,13 +16,12 @@\n import re\n import tempfile\n import unittest\n-from functools import lru_cache\n \n from transformers import SPIECE_UNDERLINE, AddedToken, BatchEncoding, T5Tokenizer, T5TokenizerFast\n from transformers.testing_utils import get_tests_dir, require_sentencepiece, require_seqio, require_tokenizers, slow\n from transformers.utils import cached_property, is_tf_available, is_torch_available\n \n-from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n+from ...test_tokenization_common import TokenizerTesterMixin\n \n \n SAMPLE_VOCAB = get_tests_dir(\"fixtures/test_sentencepiece.model\")\n@@ -147,15 +146,11 @@ def t5_base_tokenizer_fast(self):\n         return T5TokenizerFast.from_pretrained(\"google-t5/t5-base\")\n \n     @classmethod\n-    @use_cache_if_possible\n-    @lru_cache(maxsize=64)\n     def get_tokenizer(cls, pretrained_name=None, **kwargs) -> T5Tokenizer:\n         pretrained_name = pretrained_name or cls.tmpdirname\n         return cls.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n \n     @classmethod\n-    @use_cache_if_possible\n-    @lru_cache(maxsize=64)\n     def get_rust_tokenizer(cls, pretrained_name=None, **kwargs) -> T5TokenizerFast:\n         pretrained_name = pretrained_name or cls.tmpdirname\n         return cls.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)"
        },
        {
            "sha": "9473c1d22409889da9a79477e9937cb9edf284fd",
            "filename": "tests/models/vits/test_tokenization_vits.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Fvits%2Ftest_tokenization_vits.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Fvits%2Ftest_tokenization_vits.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvits%2Ftest_tokenization_vits.py?ref=8a091cc07c9808196e3154ab8bd6252d919f929b",
            "patch": "@@ -18,13 +18,12 @@\n import shutil\n import tempfile\n import unittest\n-from functools import lru_cache\n \n from transformers import VitsTokenizer\n from transformers.models.vits.tokenization_vits import VOCAB_FILES_NAMES\n from transformers.testing_utils import slow\n \n-from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n+from ...test_tokenization_common import TokenizerTesterMixin\n \n \n class VitsTokenizerTest(TokenizerTesterMixin, unittest.TestCase):\n@@ -53,8 +52,6 @@ def setUpClass(cls):\n             fp.write(json.dumps(vocab_tokens) + \"\\n\")\n \n     @classmethod\n-    @use_cache_if_possible\n-    @lru_cache(maxsize=64)\n     def get_tokenizer(cls, pretrained_name=None, **kwargs):\n         kwargs.update(cls.special_tokens_map)\n         kwargs[\"phonemize\"] = False"
        },
        {
            "sha": "8c3c77bb1ec549a41d5a0373bbf39860171a8fba",
            "filename": "tests/models/wav2vec2/test_tokenization_wav2vec2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Fwav2vec2%2Ftest_tokenization_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Fwav2vec2%2Ftest_tokenization_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwav2vec2%2Ftest_tokenization_wav2vec2.py?ref=8a091cc07c9808196e3154ab8bd6252d919f929b",
            "patch": "@@ -20,7 +20,6 @@\n import shutil\n import tempfile\n import unittest\n-from functools import lru_cache\n \n import numpy as np\n \n@@ -33,7 +32,7 @@\n from transformers.models.wav2vec2.tokenization_wav2vec2 import VOCAB_FILES_NAMES, Wav2Vec2CTCTokenizerOutput\n from transformers.testing_utils import require_torch, slow\n \n-from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n+from ...test_tokenization_common import TokenizerTesterMixin\n \n \n global_rng = random.Random()\n@@ -72,8 +71,6 @@ def setUpClass(cls):\n             fp.write(json.dumps(vocab_tokens) + \"\\n\")\n \n     @classmethod\n-    @use_cache_if_possible\n-    @lru_cache(maxsize=64)\n     def get_tokenizer(cls, pretrained_name=None, **kwargs):\n         kwargs.update(cls.special_tokens_map)\n         pretrained_name = pretrained_name or cls.tmpdirname\n@@ -393,8 +390,6 @@ def setUpClass(cls):\n             fp.write(json.dumps(vocab_tokens) + \"\\n\")\n \n     @classmethod\n-    @use_cache_if_possible\n-    @lru_cache(maxsize=64)\n     def get_tokenizer(cls, pretrained_name=None, **kwargs):\n         kwargs.update(cls.special_tokens_map)\n         pretrained_name = pretrained_name or cls.tmpdirname"
        },
        {
            "sha": "7be8646fc69c5773de4c8dd8d6f8736aa98313c6",
            "filename": "tests/models/wav2vec2_phoneme/test_tokenization_wav2vec2_phoneme.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Fwav2vec2_phoneme%2Ftest_tokenization_wav2vec2_phoneme.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Fmodels%2Fwav2vec2_phoneme%2Ftest_tokenization_wav2vec2_phoneme.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwav2vec2_phoneme%2Ftest_tokenization_wav2vec2_phoneme.py?ref=8a091cc07c9808196e3154ab8bd6252d919f929b",
            "patch": "@@ -16,14 +16,13 @@\n import json\n import os\n import unittest\n-from functools import lru_cache\n \n from transformers import Wav2Vec2PhonemeCTCTokenizer\n from transformers.models.wav2vec2.tokenization_wav2vec2 import VOCAB_FILES_NAMES\n from transformers.models.wav2vec2_phoneme.tokenization_wav2vec2_phoneme import Wav2Vec2PhonemeCTCTokenizerOutput\n from transformers.testing_utils import require_phonemizer\n \n-from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n+from ...test_tokenization_common import TokenizerTesterMixin\n \n \n @require_phonemizer\n@@ -85,8 +84,6 @@ def get_clean_sequence(self, tokenizer, with_prefix_space=False, max_length=20,\n         return output_txt, output_ids\n \n     @classmethod\n-    @use_cache_if_possible\n-    @lru_cache(maxsize=64)\n     def get_tokenizer(cls, pretrained_name=None, **kwargs):\n         kwargs.update(cls.special_tokens_map)\n         pretrained_name = pretrained_name or cls.tmpdirname"
        },
        {
            "sha": "251dab178af520b2f0718140113c4add448da258",
            "filename": "tests/test_tokenization_common.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Ftest_tokenization_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a091cc07c9808196e3154ab8bd6252d919f929b/tests%2Ftest_tokenization_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_tokenization_common.py?ref=8a091cc07c9808196e3154ab8bd6252d919f929b",
            "patch": "@@ -25,7 +25,6 @@\n import traceback\n import unittest\n from collections import OrderedDict\n-from functools import lru_cache\n from itertools import takewhile\n from pathlib import Path\n from typing import TYPE_CHECKING, Any, Optional, Union\n@@ -301,15 +300,11 @@ def get_tokenizers(self, fast=True, **kwargs) -> list[PreTrainedTokenizerBase]:\n             raise ValueError(\"This tokenizer class has no tokenizer to be tested.\")\n \n     @classmethod\n-    @use_cache_if_possible\n-    @lru_cache(maxsize=64)\n     def get_tokenizer(cls, pretrained_name=None, **kwargs) -> PreTrainedTokenizer:\n         pretrained_name = pretrained_name or cls.tmpdirname\n         return cls.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n \n     @classmethod\n-    @use_cache_if_possible\n-    @lru_cache(maxsize=64)\n     def get_rust_tokenizer(cls, pretrained_name=None, **kwargs) -> PreTrainedTokenizerFast:\n         pretrained_name = pretrained_name or cls.tmpdirname\n         return cls.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)"
        }
    ],
    "stats": {
        "total": 246,
        "additions": 39,
        "deletions": 207
    }
}