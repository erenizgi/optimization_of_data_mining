{
    "author": "molbap",
    "message": "Fix CI issues  (#35662)\n\n* make explicit gpu dep\n\n* [run-slow] bamba",
    "sha": "3a5c328fd846574535222dfb4b3eb31fb6c1102f",
    "files": [
        {
            "sha": "f7f9481e3edd49da210a60885417b61728b292c5",
            "filename": "tests/models/bamba/test_modeling_bamba.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a5c328fd846574535222dfb4b3eb31fb6c1102f/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a5c328fd846574535222dfb4b3eb31fb6c1102f/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py?ref=3a5c328fd846574535222dfb4b3eb31fb6c1102f",
            "patch": "@@ -22,6 +22,7 @@\n from transformers import AutoTokenizer, BambaConfig, is_torch_available\n from transformers.testing_utils import (\n     require_torch,\n+    require_torch_gpu,\n     slow,\n     torch_device,\n )\n@@ -487,6 +488,7 @@ def _prepare_model_kwargs(input_ids, attention_mask, signature):\n \n @slow\n @require_torch\n+@require_torch_gpu\n class BambaModelIntegrationTest(unittest.TestCase):\n     model = None\n     tokenizer = None"
        }
    ],
    "stats": {
        "total": 2,
        "additions": 2,
        "deletions": 0
    }
}