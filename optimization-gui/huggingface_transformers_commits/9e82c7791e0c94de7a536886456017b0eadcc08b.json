{
    "author": "vasqu",
    "message": "Fix Ernie Moe Test (#42595)\n\n* fix\n\n* fix\n\n* rm unnecessary config\n\n* remove references",
    "sha": "9e82c7791e0c94de7a536886456017b0eadcc08b",
    "files": [
        {
            "sha": "9e47ec145255f9dbf1830458a64d95302e7118f4",
            "filename": "tests/models/ernie4_5_moe/test_modeling_ernie4_5_moe.py",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e82c7791e0c94de7a536886456017b0eadcc08b/tests%2Fmodels%2Fernie4_5_moe%2Ftest_modeling_ernie4_5_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e82c7791e0c94de7a536886456017b0eadcc08b/tests%2Fmodels%2Fernie4_5_moe%2Ftest_modeling_ernie4_5_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fernie4_5_moe%2Ftest_modeling_ernie4_5_moe.py?ref=9e82c7791e0c94de7a536886456017b0eadcc08b",
            "patch": "@@ -97,24 +97,27 @@ def test_load_balancing_loss(self):\n         config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n         config.num_labels = 3\n         config.num_experts = 3\n-        config.expert_interval = 2\n         config.output_router_logits = True\n         input_ids = input_dict[\"input_ids\"]\n-        attention_mask = input_ids.ne(1).to(torch_device)\n+        attention_mask = input_ids.ne(config.pad_token_id).to(torch_device)\n         model = Ernie4_5_MoeForCausalLM(config)\n         model.to(torch_device)\n         model.eval()\n         result = model(input_ids, attention_mask=attention_mask)\n-        self.assertEqual(result.router_logits[0].shape, (91, config.num_experts))\n+        bs, seqlen = input_ids.shape\n+        self.assertEqual(result.router_logits[0].shape, (bs * seqlen, config.num_experts))\n         torch.testing.assert_close(result.aux_loss.cpu(), torch.tensor(2, dtype=torch.float32), rtol=1e-2, atol=1e-2)\n \n         # First, we make sure that adding padding tokens doesn't change the loss\n         # loss(input_ids, attention_mask=None) == loss(input_ids + padding, attention_mask=attention_mask_with_padding)\n+        # (This length is selected from experiments)\n         pad_length = input_ids.shape[1] * 4\n-        # Add padding tokens (assume that pad_token_id=1) to input_ids\n-        padding_block = torch.ones(input_ids.shape[0], pad_length, dtype=torch.int32).to(torch_device)\n+        # Add padding tokens to input_ids\n+        padding_block = config.pad_token_id * torch.ones(input_ids.shape[0], pad_length, dtype=torch.int32).to(\n+            torch_device\n+        )\n         padded_input_ids = torch.cat((padding_block, input_ids), dim=1)  # this is to simulate padding to the left\n-        padded_attention_mask = padded_input_ids.ne(1).to(torch_device)\n+        padded_attention_mask = padded_input_ids.ne(config.pad_token_id).to(torch_device)\n \n         padded_result = model(padded_input_ids, attention_mask=padded_attention_mask)\n         torch.testing.assert_close(result.aux_loss.cpu(), padded_result.aux_loss.cpu(), rtol=1e-4, atol=1e-4)"
        }
    ],
    "stats": {
        "total": 15,
        "additions": 9,
        "deletions": 6
    }
}