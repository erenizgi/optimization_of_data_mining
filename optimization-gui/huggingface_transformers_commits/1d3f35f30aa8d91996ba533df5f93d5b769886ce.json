{
    "author": "molbap",
    "message": "Add model visual debugger (#36798)\n\n* draft of model tracer visualiser\n\n* add context manager in addition to decorator\n\n* add debug utils to init\n\n* move model debugging utils to dedicated file\n\n* add documentation\n\n* protect some imports\n\n* format\n\n* move and protect imports\n\n* format\n\n* doc: improve errors in case of broken dummy imports.\n\n* format\n\n* use automatic torch backend\n\n* update doc\n\n* fix backend\n\n* (TEMP) move to dummies while backend wait\n\n* update documentation\n\n* doc",
    "sha": "1d3f35f30aa8d91996ba533df5f93d5b769886ce",
    "files": [
        {
            "sha": "c5708aa8e6530bf4cc87fc315373626b111b5826",
            "filename": "docs/source/en/internal/model_debugging_utils.md",
            "status": "added",
            "additions": 71,
            "deletions": 0,
            "changes": 71,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d3f35f30aa8d91996ba533df5f93d5b769886ce/docs%2Fsource%2Fen%2Finternal%2Fmodel_debugging_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d3f35f30aa8d91996ba533df5f93d5b769886ce/docs%2Fsource%2Fen%2Finternal%2Fmodel_debugging_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Finternal%2Fmodel_debugging_utils.md?ref=1d3f35f30aa8d91996ba533df5f93d5b769886ce",
            "patch": "@@ -0,0 +1,71 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# Model debugging toolboxes\n+\n+This page lists all the debugging and model adding tools used by the library, as well as the utility functions it provides for it.\n+\n+Most of those are only useful if you are adding new models in the library.\n+\n+\n+## Model addition debuggers\n+\n+\n+### Model addition debugger - context manager for model adders\n+\n+This context manager is a power user tool intended for model adders. \n+It tracks all forward calls within a model forward and logs a slice of each input and output on a nested Json.\n+To note, this context manager enforces `torch.inference_mode()`.\n+\n+### Rationale\n+\n+Because when porting models to transformers, even from python to python, model adders often have to do a lot of manual operations, involving saving and loading tensors, comparing dtypes, etc. This small tool can hopefully shave off some time.\n+\n+### Usage\n+\n+Add this context manager as follows to debug a model:\n+\n+```python\n+import torch\n+from PIL import Image\n+import requests\n+from transformers import LlavaProcessor, LlavaForConditionalGeneration\n+torch.random.manual_seed(673)\n+\n+# load pretrained model and processor\n+model_id = \"llava-hf/llava-1.5-7b-hf\"\n+processor = LlavaProcessor.from_pretrained(model_id)\n+model = LlavaForConditionalGeneration.from_pretrained(model_id, low_cpu_mem_usage=True)\n+\n+# create random image input\n+random_image = Image.fromarray(torch.randint(0, 256, (224, 224, 3), dtype=torch.uint8).numpy())\n+\n+# prompt\n+prompt = \"<image>Describe this image.\"\n+\n+# process inputs\n+inputs = processor(text=prompt, images=random_image, return_tensors=\"pt\")\n+\n+# call forward method (not .generate!)\n+with model_addition_debugger_context(model, \"optional_path_to_your_output_file.json\"):\n+    output = model.forward(**inputs)\n+\n+```\n+\n+\n+[[autodoc]] utils.model_addition_debugger\n+\n+[[autodoc]] utils.model_addition_debugger_context"
        },
        {
            "sha": "56bbcb76f4dd234b8505786f5c0a9466e9044e9b",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d3f35f30aa8d91996ba533df5f93d5b769886ce/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d3f35f30aa8d91996ba533df5f93d5b769886ce/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=1d3f35f30aa8d91996ba533df5f93d5b769886ce",
            "patch": "@@ -1376,6 +1376,10 @@\n \n     _import_structure[\"utils.dummy_pt_objects\"] = [name for name in dir(dummy_pt_objects) if not name.startswith(\"_\")]\n else:\n+    _import_structure[\"model_debugging_utils\"] = [\n+        \"model_addition_debugger\",\n+        \"model_addition_debugger_context\",\n+    ]\n     _import_structure[\"activations\"] = []\n     _import_structure[\"cache_utils\"] = [\n         \"Cache\",\n@@ -6605,6 +6609,7 @@\n     except OptionalDependencyNotAvailable:\n         from .utils.dummy_pt_objects import *\n     else:\n+        # Debugging\n         from .cache_utils import (\n             Cache,\n             CacheConfig,\n@@ -6690,6 +6695,10 @@\n             TorchExportableModuleWithStaticCache,\n             convert_and_export_with_cache,\n         )\n+        from .model_debugging_utils import (\n+            model_addition_debugger,\n+            model_addition_debugger_context,\n+        )\n         from .modeling_rope_utils import ROPE_INIT_FUNCTIONS\n         from .modeling_utils import PreTrainedModel\n         from .models.albert import ("
        },
        {
            "sha": "d45586aee1b12222f99b0aa3d5de256700eb0ae1",
            "filename": "src/transformers/model_debugging_utils.py",
            "status": "added",
            "additions": 329,
            "deletions": 0,
            "changes": 329,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d3f35f30aa8d91996ba533df5f93d5b769886ce/src%2Ftransformers%2Fmodel_debugging_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d3f35f30aa8d91996ba533df5f93d5b769886ce/src%2Ftransformers%2Fmodel_debugging_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodel_debugging_utils.py?ref=1d3f35f30aa8d91996ba533df5f93d5b769886ce",
            "patch": "@@ -0,0 +1,329 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team.\n+# All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import functools\n+import json\n+import os\n+import re\n+from contextlib import contextmanager\n+\n+from transformers.utils.import_utils import export\n+\n+from .utils import is_torch_available\n+\n+\n+if is_torch_available():\n+    import torch\n+    import torch.distributed.tensor\n+    from torch import nn\n+\n+    from .modeling_utils import PreTrainedModel\n+\n+from .utils import logging\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+# Note to code inspectors: this toolbox is intended for people who add models to `transformers`.\n+_torch_distributed_available = torch.distributed.is_available()\n+\n+\n+def _is_rank_zero():\n+    \"\"\"Return True if rank=0 or we aren't running distributed.\"\"\"\n+    if not (_torch_distributed_available and torch.distributed.is_initialized()):\n+        return True\n+    return torch.distributed.get_rank() == 0\n+\n+\n+MEMORY_ADDRESS_REGEX = re.compile(r\"object at 0x[0-9A-Fa-f]+\")\n+\n+\n+def _sanitize_repr_for_diff(x_str: str) -> str:\n+    \"\"\"\n+    Replace memory addresses in an object's repr with a stable placeholder\n+    so that beautiful JSON diffs won't be ruined by ephemeral addresses.\n+    \"\"\"\n+    return MEMORY_ADDRESS_REGEX.sub(\"object at 0xXXXXXXXX\", x_str)\n+\n+\n+def _dtensor_repr(x):\n+    \"\"\"Return a stable string representation for a DTensor-like object.\"\"\"\n+    if _is_rank_zero():\n+        return f\"DTensor (rank0) -> {repr(x._local_tensor)}\"\n+    return \"DTensor(non-rank0)\"\n+\n+\n+def _serialize_io(value):\n+    \"\"\"\n+    Recursively build a JSON-serializable Python structure from `value`.\n+    Tensors and DTensors become sanitized repr strings.\n+    Lists/tuples/dicts are recursed into.\n+    All memory addresses are replaced with a stable placeholder.\n+\n+    Args:\n+        value: Any Python object, often including torch Tensors, lists, dicts, etc.\n+\n+    Returns:\n+        A nested Python structure (list, dict, or sanitized string) that is safe to json.dump.\n+    \"\"\"\n+    if isinstance(value, (list, tuple)):\n+        return [_serialize_io(v) for v in value]\n+\n+    if isinstance(value, dict):\n+        return {k: _serialize_io(v) for k, v in value.items()}\n+\n+    if hasattr(value, \"_local_tensor\"):\n+        # DTensor-like handling, just use local tensor attribute\n+        return {\n+            \"shape\": repr(value._local_tensor.shape),\n+            \"dtype\": repr(value._local_tensor.dtype),\n+            \"value\": _sanitize_repr_for_diff(repr(value)),\n+        }\n+\n+    if isinstance(value, torch.Tensor):\n+        # standard PyTorch Tensor\n+        # return also the shape of such\n+        return {\"shape\": repr(value.shape), \"dtype\": repr(value.dtype), \"value\": _sanitize_repr_for_diff(repr(value))}\n+\n+    # fallback for everything else (bool, int, float, None, or custom class)\n+    return _sanitize_repr_for_diff(repr(value))\n+\n+\n+def prune_outputs_if_children(node):\n+    # if there are children, remove this node's \"outputs\"\n+    # so we only see outputs at the leaf level\n+    if node.get(\"children\"):\n+        node.pop(\"outputs\", None)\n+        for child in node[\"children\"]:\n+            prune_outputs_if_children(child)\n+\n+\n+def log_model_debug_trace(debug_path, model):\n+    if debug_path:\n+        try:\n+            os.makedirs(debug_path, exist_ok=False)\n+            output_path = os.path.join(debug_path, model._debugger_module_dump_name + \"_debug_tree.json\")\n+        except Exception as e:\n+            raise ValueError(f\"Unexpected or existing debug_path={debug_path}. {e}\")\n+    else:\n+        output_path = model._debugger_module_dump_name + \"_debug_tree.json\"\n+    logger.info(f\"Writing model trace at {output_path}\")\n+    with open(output_path, \"w\") as outfile:\n+        prune_outputs_if_children(model._call_tree)\n+        json.dump(model._call_tree, outfile, indent=2)\n+\n+\n+def _attach_debugger_logic(model, class_name, debug_path: str):\n+    # Prepare data structures on the model object\n+    model._call_tree = {\"module_path\": class_name, \"inputs\": None, \"outputs\": None, \"children\": []}\n+    model._debugger_model_call_stack = []\n+    model._debugger_module_dump_name = class_name  # used for final JSON filename\n+\n+    def wrap_forward(module, full_path):\n+        orig_forward = module.forward\n+\n+        @functools.wraps(orig_forward)\n+        def wrapped_forward(*inps, **kws):\n+            if _is_rank_zero():\n+                dict_inputs = {\"args\": inps, \"kwargs\": kws}\n+                dict_inputs = {k: dict_inputs[k] for k in dict_inputs if len(dict_inputs[k]) > 0}\n+                node = {\n+                    \"module_path\": full_path,\n+                    \"inputs\": _serialize_io(dict_inputs),\n+                    \"outputs\": None,\n+                    \"children\": [],\n+                }\n+                model._debugger_model_call_stack.append(node)\n+            with torch.inference_mode():\n+                out = orig_forward(*inps, **kws)\n+\n+            if _is_rank_zero():\n+                if sum(1 for _ in module.named_children()) > 0:\n+                    node[\"outputs\"] = None\n+                else:\n+                    node[\"outputs\"] = _serialize_io(out)\n+\n+                finished = model._debugger_model_call_stack.pop()\n+                # prune empty vertices here as well (mostly empty children nodes)\n+                if not finished[\"children\"]:\n+                    finished.pop(\"children\")\n+\n+                if model._debugger_model_call_stack:\n+                    model._debugger_model_call_stack[-1][\"children\"].append(finished)\n+            return out\n+\n+        module.forward = wrapped_forward\n+\n+    # wrap all submodules\n+    for name, submodule in model.named_modules():\n+        if name == \"\":\n+            continue\n+        wrap_forward(submodule, f\"{class_name}.{name}\")\n+\n+    # wrap top-level forward\n+    real_top_forward = model.forward\n+\n+    @functools.wraps(real_top_forward)\n+    def top_wrapped_forward(*inps, **kws):\n+        if _is_rank_zero():\n+            top_node = {\n+                \"module_path\": f\"{class_name} (top-level)\",\n+                \"inputs\": _serialize_io({\"args\": inps, \"kwargs\": kws}),\n+                \"outputs\": None,\n+                \"children\": [],\n+            }\n+            model._debugger_model_call_stack.append(top_node)\n+\n+        out = real_top_forward(*inps, **kws)\n+\n+        if _is_rank_zero() and model._debugger_model_call_stack:\n+            top_node[\"outputs\"] = _serialize_io(out)\n+            finished = model._debugger_model_call_stack.pop()\n+            model._call_tree[\"inputs\"] = finished[\"inputs\"]\n+            model._call_tree[\"outputs\"] = finished[\"outputs\"]\n+            model._call_tree[\"children\"] = finished[\"children\"]\n+            # prune empty stuff for visibility\n+            [model._call_tree.pop(k, None) for k in list(model._call_tree.keys()) if not model._call_tree[k]]\n+\n+        return out\n+\n+    model.forward = top_wrapped_forward\n+\n+    # Final hook for writing JSON on forward-end\n+    def final_hook(_, inputs, outputs):\n+        if _is_rank_zero() and model._debugger_model_call_stack:\n+            finished = model._debugger_model_call_stack.pop()\n+            model._call_tree[\"inputs\"] = finished[\"inputs\"]\n+            model._call_tree[\"outputs\"] = finished[\"outputs\"]\n+            model._call_tree[\"children\"] = finished[\"children\"]\n+\n+        if _is_rank_zero():\n+            log_model_debug_trace(debug_path=debug_path, model=model)\n+\n+    model.register_forward_hook(final_hook)\n+    # Optionally also for a couple possible hooks that have specific names. It should be just one.\n+    # This means modules that are not typically called \"forward\" within the model. But we should not need to recurse\n+    # through them.\n+    possible_model_calls = [\"language_model\", \"model\"]\n+    for model_call in possible_model_calls:\n+        this_model_call = getattr(model, model_call, None)\n+        if this_model_call and isinstance(this_model_call, (nn.Module, PreTrainedModel)):\n+            this_model_call.register_forward_hook(final_hook)\n+            break  # exit the loop after finding one (unsure, but should be just one call.)\n+\n+\n+@export(backends=(\"torch\",))\n+def model_addition_debugger(cls):\n+    \"\"\"\n+    # Model addition debugger - a model adder tracer\n+    This decorator is a power user tool intended for model adders.\n+    It tracks all forward calls within a model forward and logs a slice of each input and output on a nested Json.\n+    To note, this decorator enforces `torch.inference_mode()`.\n+    ## Usage\n+\n+    add decorator to your model class\n+    ```python\n+    from ...modeling_utils import model_addition_debugger\n+\n+    @model_addition_debugger\n+    class MyModel(nn.Module) # Can inherit from PreTrainedModel too\n+        # ... nothing else changes\n+    ```\n+    Then, in a separate script (example is for Llava)\n+\n+    ```python\n+    import torch\n+    from PIL import Image\n+    import requests\n+    from transformers import LlavaProcessor, LlavaForConditionalGeneration\n+    torch.random.manual_seed(673)\n+\n+    # load pretrained model and processor\n+    model_id = \"llava-hf/llava-1.5-7b-hf\"\n+    processor = LlavaProcessor.from_pretrained(model_id)\n+    model = LlavaForConditionalGeneration.from_pretrained(model_id, low_cpu_mem_usage=True)\n+\n+    # create random image input\n+    random_image = Image.fromarray(torch.randint(0, 256, (224, 224, 3), dtype=torch.uint8).numpy())\n+\n+    # prompt\n+    prompt = \"<image>Describe this image.\"\n+\n+    # process inputs\n+    inputs = processor(text=prompt, images=random_image, return_tensors=\"pt\")\n+\n+    # call forward method (not .generate!)\n+    with torch.no_grad():\n+        output = model.forward(**inputs)\n+    ```\n+\n+    \"\"\"\n+    orig_init = cls.__init__\n+\n+    @functools.wraps(cls.__init__)\n+    def wrapped_init(self, *args, **kwargs):\n+        orig_init(self, *args, **kwargs)\n+        _attach_debugger_logic(self, cls.__name__)\n+\n+    cls.__init__ = wrapped_init\n+    return cls\n+\n+\n+@export(backends=(\"torch\",))\n+@contextmanager\n+def model_addition_debugger_context(model, debug_path: str = None):\n+    \"\"\"\n+    # Model addition debugger - context manager for model adders\n+    This context manager is a power user tool intended for model adders.\n+    It tracks all forward calls within a model forward and logs a slice of each input and output on a nested Json.\n+    To note, this context manager enforces `torch.inference_mode()`.\n+\n+    ## Usage\n+\n+    add the context manager to a model to debug\n+\n+    ```python\n+    import torch\n+    from PIL import Image\n+    import requests\n+    from transformers import LlavaProcessor, LlavaForConditionalGeneration\n+    torch.random.manual_seed(673)\n+\n+    # load pretrained model and processor\n+    model_id = \"llava-hf/llava-1.5-7b-hf\"\n+    processor = LlavaProcessor.from_pretrained(model_id)\n+    model = LlavaForConditionalGeneration.from_pretrained(model_id, low_cpu_mem_usage=True)\n+\n+    # create random image input\n+    random_image = Image.fromarray(torch.randint(0, 256, (224, 224, 3), dtype=torch.uint8).numpy())\n+\n+    # prompt\n+    prompt = \"<image>Describe this image.\"\n+\n+    # process inputs\n+    inputs = processor(text=prompt, images=random_image, return_tensors=\"pt\")\n+\n+    # call forward method (not .generate!)\n+    with model_addition_debugger_context(model):\n+        output = model.forward(**inputs)\n+    ```\n+\n+    \"\"\"\n+    _attach_debugger_logic(model, model.__class__.__name__, debug_path)\n+    try:\n+        yield model\n+    finally:\n+        pass"
        },
        {
            "sha": "85eea3cb100ebe24039846624254c982e27cb8f9",
            "filename": "src/transformers/utils/dummy_pt_objects.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d3f35f30aa8d91996ba533df5f93d5b769886ce/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d3f35f30aa8d91996ba533df5f93d5b769886ce/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py?ref=1d3f35f30aa8d91996ba533df5f93d5b769886ce",
            "patch": "@@ -538,6 +538,14 @@ def convert_and_export_with_cache(*args, **kwargs):\n     requires_backends(convert_and_export_with_cache, [\"torch\"])\n \n \n+def model_addition_debugger(*args, **kwargs):\n+    requires_backends(model_addition_debugger, [\"torch\"])\n+\n+\n+def model_addition_debugger_context(*args, **kwargs):\n+    requires_backends(model_addition_debugger_context, [\"torch\"])\n+\n+\n ROPE_INIT_FUNCTIONS = None\n \n "
        },
        {
            "sha": "73d7ebbfd1d8badc944333344c0008c6cc63873c",
            "filename": "utils/check_dummies.py",
            "status": "modified",
            "additions": 15,
            "deletions": 2,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d3f35f30aa8d91996ba533df5f93d5b769886ce/utils%2Fcheck_dummies.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d3f35f30aa8d91996ba533df5f93d5b769886ce/utils%2Fcheck_dummies.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_dummies.py?ref=1d3f35f30aa8d91996ba533df5f93d5b769886ce",
            "patch": "@@ -222,10 +222,23 @@ def check_dummies(overwrite: bool = False):\n                 with open(dummy_file_paths[backend], \"w\", encoding=\"utf-8\", newline=\"\\n\") as f:\n                     f.write(dummy_files[backend])\n             else:\n+                # Temporary fix to help people identify which objects introduced are not correctly protected.\n+                for _actual, _dummy in zip(\n+                    actual_dummies[\"torch\"].split(\"class\"), dummy_files[\"torch\"].split(\"class\")\n+                ):\n+                    if _actual != _dummy:\n+                        actual_broken = _actual\n+                        dummy_broken = _dummy\n+                        break\n                 raise ValueError(\n                     \"The main __init__ has objects that are not present in \"\n-                    f\"transformers.utils.dummy_{short_names.get(backend, backend)}_objects.py. Run `make fix-copies` \"\n-                    \"to fix this.\"\n+                    f\"transformers.utils.dummy_{short_names.get(backend, backend)}_objects.py.\\n\"\n+                    f\" It is likely the following objects are responsible, see these excerpts: \\n\"\n+                    f\"---------------------------------- Actual -------------------------------------\\n\"\n+                    f\" \\n {actual_broken} \\n\"\n+                    f\"---------------------------------- Dummy -------------------------------------\\n\"\n+                    f\" \\n {dummy_broken} \\n\"\n+                    \"Run `make fix-copies` to fix this.\"\n                 )\n \n "
        }
    ],
    "stats": {
        "total": 434,
        "additions": 432,
        "deletions": 2
    }
}