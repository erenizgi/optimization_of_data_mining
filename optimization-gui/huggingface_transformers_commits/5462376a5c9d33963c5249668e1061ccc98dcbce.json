{
    "author": "Xqle",
    "message": "Fix invalid examples in QwenVL model docstrings and add Qwen3VL example (#41812)",
    "sha": "5462376a5c9d33963c5249668e1061ccc98dcbce",
    "files": [
        {
            "sha": "0e6e07ff54c1938498ce92aba5116a4194531a34",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 19,
            "deletions": 13,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/5462376a5c9d33963c5249668e1061ccc98dcbce/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5462376a5c9d33963c5249668e1061ccc98dcbce/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=5462376a5c9d33963c5249668e1061ccc98dcbce",
            "patch": "@@ -1453,8 +1453,6 @@ def forward(\n         Example:\n \n         ```python\n-        >>> from PIL import Image\n-        >>> import requests\n         >>> from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration\n \n         >>> model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\")\n@@ -1464,22 +1462,30 @@ def forward(\n             {\n                 \"role\": \"user\",\n                 \"content\": [\n-                    {\"type\": \"image\"},\n-                    {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n+                    {\n+                        \"type\": \"image\",\n+                        \"image\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\",\n+                    },\n+                    {\"type\": \"text\", \"text\": \"Describe the image.\"},\n                 ],\n-            },\n+            }\n         ]\n-        >>> url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n-        >>> image = Image.open(requests.get(url, stream=True).raw)\n \n-        >>> text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n-        >>> inputs = processor(text=[text], images=[image], vision_infos=[vision_infos])\n+        >>> inputs = processor.apply_chat_template(\n+            messages,\n+            tokenize=True,\n+            add_generation_prompt=True,\n+            return_dict=True,\n+            return_tensors=\"pt\"\n+        )\n \n         >>> # Generate\n-        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n-        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n-        \"The image shows a street scene with a red stop sign in the foreground. In the background, there is a large red gate with Chinese characters ...\"\n-        ```\"\"\"\n+        >>> generated_ids = model.generate(**inputs, max_new_tokens=1024)\n+        >>> generated_ids_trimmed = [out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]\n+        >>> output_text = processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+        >>> print(output_text)\n+        ```\n+        \"\"\"\n \n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = ("
        },
        {
            "sha": "cb3c713ae239800094796326a73859921a47956c",
            "filename": "src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py",
            "status": "modified",
            "additions": 19,
            "deletions": 13,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/5462376a5c9d33963c5249668e1061ccc98dcbce/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5462376a5c9d33963c5249668e1061ccc98dcbce/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py?ref=5462376a5c9d33963c5249668e1061ccc98dcbce",
            "patch": "@@ -684,8 +684,6 @@ def forward(\n         Example:\n \n         ```python\n-        >>> from PIL import Image\n-        >>> import requests\n         >>> from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration\n \n         >>> model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\")\n@@ -695,22 +693,30 @@ def forward(\n             {\n                 \"role\": \"user\",\n                 \"content\": [\n-                    {\"type\": \"image\"},\n-                    {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n+                    {\n+                        \"type\": \"image\",\n+                        \"image\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\",\n+                    },\n+                    {\"type\": \"text\", \"text\": \"Describe the image.\"},\n                 ],\n-            },\n+            }\n         ]\n-        >>> url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n-        >>> image = Image.open(requests.get(url, stream=True).raw)\n \n-        >>> text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n-        >>> inputs = processor(text=[text], images=[image], vision_infos=[vision_infos])\n+        >>> inputs = processor.apply_chat_template(\n+            messages,\n+            tokenize=True,\n+            add_generation_prompt=True,\n+            return_dict=True,\n+            return_tensors=\"pt\"\n+        )\n \n         >>> # Generate\n-        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n-        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n-        \"The image shows a street scene with a red stop sign in the foreground. In the background, there is a large red gate with Chinese characters ...\"\n-        ```\"\"\"\n+        >>> generated_ids = model.generate(**inputs, max_new_tokens=1024)\n+        >>> generated_ids_trimmed = [out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]\n+        >>> output_text = processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+        >>> print(output_text)\n+        ```\n+        \"\"\"\n \n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = ("
        },
        {
            "sha": "d0074b1662e6116ec7f53d364f9526d4307c2ca6",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 19,
            "deletions": 13,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/5462376a5c9d33963c5249668e1061ccc98dcbce/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5462376a5c9d33963c5249668e1061ccc98dcbce/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=5462376a5c9d33963c5249668e1061ccc98dcbce",
            "patch": "@@ -1348,8 +1348,6 @@ def forward(\n         Example:\n \n         ```python\n-        >>> from PIL import Image\n-        >>> import requests\n         >>> from transformers import AutoProcessor, Qwen2VLForConditionalGeneration\n \n         >>> model = Qwen2VLForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n@@ -1359,22 +1357,30 @@ def forward(\n             {\n                 \"role\": \"user\",\n                 \"content\": [\n-                    {\"type\": \"image\"},\n-                    {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n+                    {\n+                        \"type\": \"image\",\n+                        \"image\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\",\n+                    },\n+                    {\"type\": \"text\", \"text\": \"Describe the image.\"},\n                 ],\n-            },\n+            }\n         ]\n-        >>> url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n-        >>> image = Image.open(requests.get(url, stream=True).raw)\n \n-        >>> text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n-        >>> inputs = processor(text=[text], images=[image], vision_infos=[vision_infos])\n+        >>> inputs = processor.apply_chat_template(\n+            messages,\n+            tokenize=True,\n+            add_generation_prompt=True,\n+            return_dict=True,\n+            return_tensors=\"pt\"\n+        )\n \n         >>> # Generate\n-        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n-        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n-        \"The image shows a street scene with a red stop sign in the foreground. In the background, there is a large red gate with Chinese characters ...\"\n-        ```\"\"\"\n+        >>> generated_ids = model.generate(**inputs, max_new_tokens=1024)\n+        >>> generated_ids_trimmed = [out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]\n+        >>> output_text = processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+        >>> print(output_text)\n+        ```\n+        \"\"\"\n \n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = ("
        },
        {
            "sha": "37f6a5146053de727ce22204746af5833606871d",
            "filename": "src/transformers/models/qwen3_vl/modeling_qwen3_vl.py",
            "status": "modified",
            "additions": 35,
            "deletions": 1,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/5462376a5c9d33963c5249668e1061ccc98dcbce/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodeling_qwen3_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5462376a5c9d33963c5249668e1061ccc98dcbce/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodeling_qwen3_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodeling_qwen3_vl.py?ref=5462376a5c9d33963c5249668e1061ccc98dcbce",
            "patch": "@@ -1369,8 +1369,42 @@ def forward(\n             The temporal, height and width of feature shape of each video in LLM.\n \n         Example:\n-            TODO: Add example\n+\n+        ```python\n+        >>> from transformers import AutoProcessor, Qwen3VLForConditionalGeneration\n+\n+        >>> model = Qwen3VLForConditionalGeneration.from_pretrained(\"Qwen/Qwen3-VL-8B-Instruct\")\n+        >>> processor = AutoProcessor.from_pretrained(\"Qwen/Qwen3-VL-8B-Instruct\")\n+\n+        >>> messages = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\n+                        \"type\": \"image\",\n+                        \"image\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\",\n+                    },\n+                    {\"type\": \"text\", \"text\": \"Describe the image.\"},\n+                ],\n+            }\n+        ]\n+\n+        >>> inputs = processor.apply_chat_template(\n+            messages,\n+            tokenize=True,\n+            add_generation_prompt=True,\n+            return_dict=True,\n+            return_tensors=\"pt\"\n+        )\n+\n+        >>> # Generate\n+        >>> generated_ids = model.generate(**inputs, max_new_tokens=1024)\n+        >>> generated_ids_trimmed = [out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]\n+        >>> output_text = processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+        >>> print(output_text)\n+        ```\n         \"\"\"\n+\n         outputs = self.model(\n             input_ids=input_ids,\n             pixel_values=pixel_values,"
        },
        {
            "sha": "9216b99513981c2990284c7a433efefa74cf9115",
            "filename": "src/transformers/models/qwen3_vl/modular_qwen3_vl.py",
            "status": "modified",
            "additions": 35,
            "deletions": 1,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/5462376a5c9d33963c5249668e1061ccc98dcbce/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodular_qwen3_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5462376a5c9d33963c5249668e1061ccc98dcbce/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodular_qwen3_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodular_qwen3_vl.py?ref=5462376a5c9d33963c5249668e1061ccc98dcbce",
            "patch": "@@ -1134,8 +1134,42 @@ def forward(\n             The temporal, height and width of feature shape of each video in LLM.\n \n         Example:\n-            TODO: Add example\n+\n+        ```python\n+        >>> from transformers import AutoProcessor, Qwen3VLForConditionalGeneration\n+\n+        >>> model = Qwen3VLForConditionalGeneration.from_pretrained(\"Qwen/Qwen3-VL-8B-Instruct\")\n+        >>> processor = AutoProcessor.from_pretrained(\"Qwen/Qwen3-VL-8B-Instruct\")\n+\n+        >>> messages = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\n+                        \"type\": \"image\",\n+                        \"image\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\",\n+                    },\n+                    {\"type\": \"text\", \"text\": \"Describe the image.\"},\n+                ],\n+            }\n+        ]\n+\n+        >>> inputs = processor.apply_chat_template(\n+            messages,\n+            tokenize=True,\n+            add_generation_prompt=True,\n+            return_dict=True,\n+            return_tensors=\"pt\"\n+        )\n+\n+        >>> # Generate\n+        >>> generated_ids = model.generate(**inputs, max_new_tokens=1024)\n+        >>> generated_ids_trimmed = [out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]\n+        >>> output_text = processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+        >>> print(output_text)\n+        ```\n         \"\"\"\n+\n         outputs = self.model(\n             input_ids=input_ids,\n             pixel_values=pixel_values,"
        }
    ],
    "stats": {
        "total": 168,
        "additions": 127,
        "deletions": 41
    }
}