{
    "author": "vasqu",
    "message": "[`T5Gemma`] Fix cross attention cache (#41890)\n\n* fix\n\n* add test\n\n* style\n\n* added comment",
    "sha": "33c60a52540962d1d5efdbf29ef0b585649ff175",
    "files": [
        {
            "sha": "aadf014a4aa6716dc40e49bfb9e8b02556ec12a8",
            "filename": "src/transformers/models/t5gemma/modeling_t5gemma.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/33c60a52540962d1d5efdbf29ef0b585649ff175/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/33c60a52540962d1d5efdbf29ef0b585649ff175/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py?ref=33c60a52540962d1d5efdbf29ef0b585649ff175",
            "patch": "@@ -797,7 +797,9 @@ def forward(\n             inputs_embeds = self.embed_tokens(input_ids)\n \n         if not self.training and use_cache and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n+            # We do not pass the config to the cross attn cache to avoid initializing SWA\n+            # --> we use full attention between our cross attentions\n+            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache())\n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n             cache_position = torch.arange("
        },
        {
            "sha": "35b8ab1c60d6128126eb3aac5194e0afb2624c74",
            "filename": "src/transformers/models/t5gemma/modular_t5gemma.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/33c60a52540962d1d5efdbf29ef0b585649ff175/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/33c60a52540962d1d5efdbf29ef0b585649ff175/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py?ref=33c60a52540962d1d5efdbf29ef0b585649ff175",
            "patch": "@@ -835,7 +835,9 @@ def forward(\n             inputs_embeds = self.embed_tokens(input_ids)\n \n         if not self.training and use_cache and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n+            # We do not pass the config to the cross attn cache to avoid initializing SWA\n+            # --> we use full attention between our cross attentions\n+            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache())\n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n             cache_position = torch.arange("
        },
        {
            "sha": "c547fa6d02ae3e2c14217ae1614e40d159915bf9",
            "filename": "tests/models/t5gemma/test_modeling_t5gemma.py",
            "status": "modified",
            "additions": 16,
            "deletions": 0,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/33c60a52540962d1d5efdbf29ef0b585649ff175/tests%2Fmodels%2Ft5gemma%2Ftest_modeling_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/33c60a52540962d1d5efdbf29ef0b585649ff175/tests%2Fmodels%2Ft5gemma%2Ftest_modeling_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ft5gemma%2Ftest_modeling_t5gemma.py?ref=33c60a52540962d1d5efdbf29ef0b585649ff175",
            "patch": "@@ -19,11 +19,14 @@\n \n import pytest\n from parameterized import parameterized\n+from pytest import mark\n \n from transformers import T5GemmaConfig, T5GemmaModuleConfig, is_torch_available\n from transformers.testing_utils import (\n+    require_flash_attn,\n     require_torch,\n     require_torch_accelerator,\n+    require_torch_gpu,\n     torch_device,\n )\n \n@@ -1267,6 +1270,19 @@ def test_flex_attention_with_grads(self):\n             # If this does not raise an error, the test passes (see https://github.com/huggingface/transformers/pull/35605)\n             _ = model(**dummy_inputs)\n \n+    @require_flash_attn\n+    @require_torch_gpu\n+    @mark.flash_attn_test\n+    def test_generate_beyond_sliding_window_with_flash_attn(self):\n+        config, input_ids, _, attention_mask, _, _ = self.model_tester.prepare_config_and_inputs()\n+        config.decoder.sliding_window = 2  # arbitrary but less than seq_len\n+\n+        model = self.model_tester.causal_lm_class(config=config).to(dtype=torch.float16, device=torch_device).eval()\n+        model.set_attn_implementation(\"flash_attention_2\")\n+\n+        # Only generate beyond prefill, we don't care about the output as it only checks for crashes\n+        _ = model.generate(input_ids, attention_mask=attention_mask, max_new_tokens=2, use_cache=True)\n+\n \n class T5GemmaEncoderOnlyModelTester:\n     config_class = T5GemmaConfig"
        }
    ],
    "stats": {
        "total": 24,
        "additions": 22,
        "deletions": 2
    }
}