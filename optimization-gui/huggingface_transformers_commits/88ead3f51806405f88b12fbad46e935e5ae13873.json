{
    "author": "LysandreJik",
    "message": "Fix responses add tests (#39848)\n\n* Quick responses fix\n\n* [serve] Fix responses API and add tests\n\n* Remove typo\n\n* Remove typo\n\n* Tests",
    "sha": "88ead3f51806405f88b12fbad46e935e5ae13873",
    "files": [
        {
            "sha": "d8d7d1b22b21bb3b8f6ddcc883c7c0539ee65962",
            "filename": "src/transformers/commands/serving.py",
            "status": "modified",
            "additions": 26,
            "deletions": 1,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/88ead3f51806405f88b12fbad46e935e5ae13873/src%2Ftransformers%2Fcommands%2Fserving.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/88ead3f51806405f88b12fbad46e935e5ae13873/src%2Ftransformers%2Fcommands%2Fserving.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcommands%2Fserving.py?ref=88ead3f51806405f88b12fbad46e935e5ae13873",
            "patch": "@@ -396,6 +396,9 @@ class ServeArguments:\n     log_level: str = field(\n         default=\"info\", metadata={\"help\": \"Logging level as a string. Example: 'info' or 'warning'.\"}\n     )\n+    default_seed: Optional[int] = field(\n+        default=None, metadata={\"help\": \"The default seed for torch, should be an integer.\"}\n+    )\n     enable_cors: bool = field(\n         default=False,\n         metadata={\n@@ -451,6 +454,9 @@ def __init__(self, args: ServeArguments):\n         self.use_continuous_batching = self.args.attn_implementation == \"sdpa_paged\"\n         self.enable_cors = self.args.enable_cors\n \n+        if self.args.default_seed is not None:\n+            torch.manual_seed(self.args.default_seed)\n+\n         # Set up logging\n         transformers_logger = logging.get_logger(\"transformers\")\n         transformers_logger.setLevel(logging.log_levels[self.args.log_level.lower()])\n@@ -1032,7 +1038,26 @@ def generate_response(self, req: dict) -> Generator[str, None, None]:\n         self.last_model = model_id_and_revision\n         model, processor = self.load_model_and_processor(model_id_and_revision)\n \n-        inputs = processor.apply_chat_template(req[\"input\"], add_generation_prompt=True).to(model.device)\n+        if isinstance(req[\"input\"], str):\n+            inputs = [{\"role\": \"system\", \"content\": req[\"instructions\"]}] if \"instructions\" in req else []\n+            inputs.append({\"role\": \"user\", \"content\": req[\"input\"]})\n+        elif isinstance(req[\"input\"], list):\n+            if \"instructions\" in req:\n+                if req[\"input\"][0][\"role\"] != \"system\":\n+                    inputs = [{\"role\": \"system\", \"content\": req[\"instructions\"]}, *req[\"input\"]]\n+                else:\n+                    inputs = req[\"input\"]\n+                    inputs[0][\"content\"] = req[\"instructions\"]\n+            else:\n+                inputs = req[\"input\"]\n+        elif isinstance(req[\"input\"], dict):\n+            inputs = [{\"role\": \"system\", \"content\": req[\"instructions\"]}] if \"instructions\" in req else []\n+            inputs.append(req[\"input\"])\n+        else:\n+            raise ValueError(\"inputs should be a list, dict, or str\")\n+\n+        inputs = processor.apply_chat_template(inputs, add_generation_prompt=True, return_tensors=\"pt\")\n+        inputs = inputs.to(model.device)\n         request_id = req.get(\"previous_response_id\", \"req_0\")\n \n         generation_streamer = TextIteratorStreamer(processor, skip_special_tokens=True, skip_prompt=True)"
        },
        {
            "sha": "87b5f61e2cde5510c4543a53077226f5b26ff88e",
            "filename": "tests/commands/test_serving.py",
            "status": "modified",
            "additions": 105,
            "deletions": 3,
            "changes": 108,
            "blob_url": "https://github.com/huggingface/transformers/blob/88ead3f51806405f88b12fbad46e935e5ae13873/tests%2Fcommands%2Ftest_serving.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/88ead3f51806405f88b12fbad46e935e5ae13873/tests%2Fcommands%2Ftest_serving.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fcommands%2Ftest_serving.py?ref=88ead3f51806405f88b12fbad46e935e5ae13873",
            "patch": "@@ -30,8 +30,20 @@\n \n \n if is_openai_available():\n+    from openai import APIConnectionError, OpenAI\n     from openai.types.chat.chat_completion_chunk import ChoiceDeltaToolCall, ChoiceDeltaToolCallFunction\n-    from openai.types.responses import Response, ResponseCreatedEvent\n+    from openai.types.responses import (\n+        Response,\n+        ResponseCompletedEvent,\n+        ResponseContentPartAddedEvent,\n+        ResponseContentPartDoneEvent,\n+        ResponseCreatedEvent,\n+        ResponseInProgressEvent,\n+        ResponseOutputItemAddedEvent,\n+        ResponseOutputItemDoneEvent,\n+        ResponseTextDeltaEvent,\n+        ResponseTextDoneEvent,\n+    )\n \n \n @require_openai\n@@ -156,7 +168,7 @@ async def wrapper(*args, **kwargs):\n         for _ in range(max_attempts):\n             try:\n                 return await fn(*args, **kwargs)\n-            except aiohttp.client_exceptions.ClientConnectorError:\n+            except (aiohttp.client_exceptions.ClientConnectorError, APIConnectionError):\n                 time.sleep(delay)\n \n     return wrapper\n@@ -465,4 +477,94 @@ def setUpClass(cls):\n         thread.start()\n \n \n-# TODO: Response integration tests\n+@require_openai\n+class ServeResponsesMixin:\n+    \"\"\"\n+    Mixin class for the Completions API tests, to seamlessly replicate tests across the two versions of the API\n+    (`generate` and `continuous_batching`).\n+    \"\"\"\n+\n+    @async_retry\n+    async def run_server(self, request):\n+        client = OpenAI(base_url=f\"http://localhost:{self.port}/v1\", api_key=\"<KEY>\")\n+        stream = client.responses.create(**request)\n+\n+        all_payloads = []\n+        for payload in stream:\n+            all_payloads.append(payload)\n+\n+        return all_payloads\n+\n+    def test_request(self):\n+        \"\"\"Tests that an inference using the Responses API works\"\"\"\n+\n+        request = {\n+            \"model\": \"Qwen/Qwen2.5-0.5B-Instruct\",\n+            \"instructions\": \"You are a helpful assistant.\",\n+            \"input\": \"Hello!\",\n+            \"stream\": True,\n+            \"max_output_tokens\": 1,\n+        }\n+        all_payloads = asyncio.run(self.run_server(request))\n+        print(\"ok\")\n+\n+        order_of_payloads = [\n+            ResponseCreatedEvent,\n+            ResponseInProgressEvent,\n+            ResponseOutputItemAddedEvent,\n+            ResponseContentPartAddedEvent,\n+            ResponseTextDeltaEvent,\n+            ResponseTextDeltaEvent,\n+            ResponseTextDoneEvent,\n+            ResponseContentPartDoneEvent,\n+            ResponseOutputItemDoneEvent,\n+            ResponseCompletedEvent,\n+        ]\n+\n+        self.assertEqual(len(all_payloads), 10)\n+        for payload, payload_type in zip(all_payloads, order_of_payloads):\n+            self.assertIsInstance(payload, payload_type)\n+\n+    # TODO: one test for each request flag, to confirm it is working as expected\n+    # TODO: speed-based test to confirm that KV cache is working across requests\n+\n+\n+@slow  # server startup time is slow on our push CI\n+@require_openai\n+class ServeResponsesIntegrationTest(ServeResponsesMixin, unittest.TestCase):\n+    \"\"\"Tests the Responses API.\"\"\"\n+\n+    @classmethod\n+    def setUpClass(cls):\n+        \"\"\"Starts a server for tests to connect to.\"\"\"\n+        cls.port = 8003\n+        args = ServeArguments(port=cls.port, default_seed=42)\n+        serve_command = ServeCommand(args)\n+        thread = Thread(target=serve_command.run)\n+        thread.daemon = True\n+        thread.start()\n+\n+    @slow\n+    def test_full_request(self):\n+        \"\"\"Tests that an inference using the Responses API works\"\"\"\n+\n+        request = {\n+            \"model\": \"Qwen/Qwen2.5-0.5B-Instruct\",\n+            \"instructions\": \"You are a sports assistant designed to craft sports programs.\",\n+            \"input\": \"Tell me what you can do.\",\n+            \"stream\": True,\n+            \"max_output_tokens\": 30,\n+        }\n+        all_payloads = asyncio.run(self.run_server(request))\n+\n+        full_text = \"\"\n+        for token in all_payloads:\n+            if isinstance(token, ResponseTextDeltaEvent):\n+                full_text += token.delta\n+\n+        # Verify that the system prompt went through.\n+        self.assertTrue(\n+            full_text.startswith(\n+                \"As an AI language model, I am designed to assist with various tasks and provide information on different topics related to sports.\"\n+            )\n+        )"
        }
    ],
    "stats": {
        "total": 135,
        "additions": 131,
        "deletions": 4
    }
}