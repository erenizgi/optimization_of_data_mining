{
    "author": "saldanhad",
    "message": "[Doc]: Broken link in Kubernetes doc (#33879)\n\n* add relative path in .md and redirects to conf.py\r\n\r\n* add redirects to conf.py and update .md\r\n\r\n* modify links in .md",
    "sha": "b6a01df6e967bce0714e54ade3948be1c33cfc6f",
    "files": [
        {
            "sha": "4381def017ddc5a81d4127e5f0369421147dca1c",
            "filename": "docs/source/en/_config.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6a01df6e967bce0714e54ade3948be1c33cfc6f/docs%2Fsource%2Fen%2F_config.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6a01df6e967bce0714e54ade3948be1c33cfc6f/docs%2Fsource%2Fen%2F_config.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_config.py?ref=b6a01df6e967bce0714e54ade3948be1c33cfc6f",
            "patch": "@@ -11,4 +11,4 @@\n     \"{processor_class}\": \"FakeProcessorClass\",\n     \"{model_class}\": \"FakeModelClass\",\n     \"{object_class}\": \"FakeObjectClass\",\n-}\n+}\n\\ No newline at end of file"
        },
        {
            "sha": "f528378bd1b875ec3d8ace44f2837a90439182aa",
            "filename": "docs/source/en/perf_train_cpu_many.md",
            "status": "modified",
            "additions": 10,
            "deletions": 9,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6a01df6e967bce0714e54ade3948be1c33cfc6f/docs%2Fsource%2Fen%2Fperf_train_cpu_many.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6a01df6e967bce0714e54ade3948be1c33cfc6f/docs%2Fsource%2Fen%2Fperf_train_cpu_many.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_train_cpu_many.md?ref=b6a01df6e967bce0714e54ade3948be1c33cfc6f",
            "patch": "@@ -138,16 +138,16 @@ Now, run the following command in node0 and **4DDP** will be enabled in node0 an\n ## Usage with Kubernetes\n \n The same distributed training job from the previous section can be deployed to a Kubernetes cluster using the\n-[Kubeflow PyTorchJob training operator](https://www.kubeflow.org/docs/components/training/pytorch/).\n+[Kubeflow PyTorchJob training operator](https://www.kubeflow.org/docs/components/training/user-guides/pytorch).\n \n ### Setup\n \n This example assumes that you have:\n-* Access to a Kubernetes cluster with [Kubeflow installed](https://www.kubeflow.org/docs/started/installing-kubeflow/)\n-* [`kubectl`](https://kubernetes.io/docs/tasks/tools/) installed and configured to access the Kubernetes cluster\n-* A [Persistent Volume Claim (PVC)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) that can be used\n+* Access to a Kubernetes cluster with [Kubeflow installed](https://www.kubeflow.org/docs/started/installing-kubeflow)\n+* [`kubectl`](https://kubernetes.io/docs/tasks/tools) installed and configured to access the Kubernetes cluster\n+* A [Persistent Volume Claim (PVC)](https://kubernetes.io/docs/concepts/storage/persistent-volumes) that can be used\n   to store datasets and model files. There are multiple options for setting up the PVC including using an NFS\n-  [storage class](https://kubernetes.io/docs/concepts/storage/storage-classes/) or a cloud storage bucket.\n+  [storage class](https://kubernetes.io/docs/concepts/storage/storage-classes) or a cloud storage bucket.\n * A Docker container that includes your model training script and all the dependencies needed to run the script. For\n   distributed CPU training jobs, this typically includes PyTorch, Transformers, Intel Extension for PyTorch, Intel\n   oneCCL Bindings for PyTorch, and OpenSSH to communicate between the containers.\n@@ -176,7 +176,7 @@ PyTorchJob to the cluster.\n \n ### PyTorchJob Specification File\n \n-The [Kubeflow PyTorchJob](https://www.kubeflow.org/docs/components/training/pytorch/) is used to run the distributed\n+The [Kubeflow PyTorchJob](https://www.kubeflow.org/docs/components/training/user-guides/pytorch) is used to run the distributed\n training job on the cluster. The yaml file for the PyTorchJob defines parameters such as:\n  * The name of the PyTorchJob\n  * The number of replicas (workers)\n@@ -273,12 +273,13 @@ To run this example, update the yaml based on your training script and the nodes\n \n <Tip>\n \n-The CPU resource limits/requests in the yaml are defined in [cpu units](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#meaning-of-cpu)\n+The CPU resource limits/requests in the yaml are defined in \n+[cpu units](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#meaning-of-cpu)\n where 1 CPU unit is equivalent to 1 physical CPU core or 1 virtual core (depending on whether the node is a physical\n host or a VM). The amount of CPU and memory limits/requests defined in the yaml should be less than the amount of\n available CPU/memory capacity on a single machine. It is usually a good idea to not use the entire machine's capacity in\n order to leave some resources for the kubelet and OS. In order to get [\"guaranteed\"](https://kubernetes.io/docs/concepts/workloads/pods/pod-qos/#guaranteed)\n-[quality of service](https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/) for the worker pods,\n+[quality of service](https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod) for the worker pods,\n set the same CPU and memory amounts for both the resource limits and requests.\n \n </Tip>\n@@ -318,4 +319,4 @@ with the job, the PyTorchJob resource can be deleted from the cluster using `kub\n \n This guide covered running distributed PyTorch training jobs using multiple CPUs on bare metal and on a Kubernetes\n cluster. Both cases utilize Intel Extension for PyTorch and Intel oneCCL Bindings for PyTorch for optimal training\n-performance, and can be used as a template to run your own workload on multiple nodes.\n+performance, and can be used as a template to run your own workload on multiple nodes.\n\\ No newline at end of file"
        }
    ],
    "stats": {
        "total": 21,
        "additions": 11,
        "deletions": 10
    }
}