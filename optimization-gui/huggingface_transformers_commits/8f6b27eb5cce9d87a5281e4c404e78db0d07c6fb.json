{
    "author": "yao-matrix",
    "message": "enable `test_assisted_decoding_in_different_gpu` test on XPU (#37120)\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\nCo-authored-by: Yih-Dar <2521628+ydshieh@users.noreply.github.com>",
    "sha": "8f6b27eb5cce9d87a5281e4c404e78db0d07c6fb",
    "files": [
        {
            "sha": "3b43fddf548636b9538c0224111448e0345ba811",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8f6b27eb5cce9d87a5281e4c404e78db0d07c6fb/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8f6b27eb5cce9d87a5281e4c404e78db0d07c6fb/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=8f6b27eb5cce9d87a5281e4c404e78db0d07c6fb",
            "patch": "@@ -3748,11 +3748,13 @@ def test_return_unprocessed_logit_scores(self):\n         self.assertTrue(y_prob <= 1.0 and n_prob <= 1.0)\n \n     @slow\n-    @require_torch_multi_gpu\n+    @require_torch_multi_accelerator\n     def test_assisted_decoding_in_different_gpu(self):\n-        model = AutoModelForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-MistralForCausalLM\").to(\"cuda:0\")\n+        device_0 = f\"{torch_device}:0\" if torch_device != \"cpu\" else \"cpu\"\n+        device_1 = f\"{torch_device}:1\" if torch_device != \"cpu\" else \"cpu\"\n+        model = AutoModelForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-MistralForCausalLM\").to(device_0)\n         assistant = AutoModelForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-MistralForCausalLM\").to(\n-            \"cuda:1\"\n+            device_1\n         )\n         tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-MistralForCausalLM\")\n         model.config.pad_token_id = tokenizer.eos_token_id"
        }
    ],
    "stats": {
        "total": 8,
        "additions": 5,
        "deletions": 3
    }
}