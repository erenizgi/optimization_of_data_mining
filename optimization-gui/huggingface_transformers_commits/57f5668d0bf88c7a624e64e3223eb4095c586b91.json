{
    "author": "Rocketknight1",
    "message": "Fix Metaclip modular conversion (#40660)\n\n* Fix Metaclip modular conversion\n\n* manually run check_copies",
    "sha": "57f5668d0bf88c7a624e64e3223eb4095c586b91",
    "files": [
        {
            "sha": "9af55d5a759021255c84ad35e08172759247d7fe",
            "filename": "src/transformers/models/metaclip_2/modeling_metaclip_2.py",
            "status": "modified",
            "additions": 20,
            "deletions": 24,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/57f5668d0bf88c7a624e64e3223eb4095c586b91/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodeling_metaclip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/57f5668d0bf88c7a624e64e3223eb4095c586b91/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodeling_metaclip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodeling_metaclip_2.py?ref=57f5668d0bf88c7a624e64e3223eb4095c586b91",
            "patch": "@@ -715,16 +715,14 @@ def forward(\n         Examples:\n \n         ```python\n-        >>> import torch\n         >>> from transformers import AutoTokenizer, MetaClip2TextModelWithProjection\n \n         >>> model = MetaClip2TextModelWithProjection.from_pretrained(\"facebook/metaclip-2-worldwide-huge-quickgelu\")\n         >>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/metaclip-2-worldwide-huge-quickgelu\")\n \n         >>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n \n-        >>> with torch.inference_mode():\n-        ...     outputs = model(**inputs)\n+        >>> outputs = model(**inputs)\n         >>> text_embeds = outputs.text_embeds\n         ```\"\"\"\n \n@@ -887,9 +885,11 @@ def __init__(self, config: MetaClip2Config):\n     @auto_docstring\n     def get_text_features(\n         self,\n-        input_ids: torch.Tensor,\n+        input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n     ) -> torch.FloatTensor:\n         r\"\"\"\n         Returns:\n@@ -899,16 +899,13 @@ def get_text_features(\n         Examples:\n \n         ```python\n-        >>> import torch\n         >>> from transformers import AutoTokenizer, MetaClip2Model\n \n         >>> model = MetaClip2Model.from_pretrained(\"facebook/metaclip-2-worldwide-huge-quickgelu\")\n         >>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/metaclip-2-worldwide-huge-quickgelu\")\n \n         >>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n-\n-        >>> with torch.inference_mode():\n-        ...     text_features = model.get_text_features(**inputs)\n+        >>> text_features = model.get_text_features(**inputs)\n         ```\"\"\"\n         text_outputs: BaseModelOutputWithPooling = self.text_model(\n             input_ids=input_ids,\n@@ -924,7 +921,9 @@ def get_text_features(\n     @auto_docstring\n     def get_image_features(\n         self,\n-        pixel_values: torch.FloatTensor,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n     ) -> torch.FloatTensor:\n         r\"\"\"\n@@ -935,20 +934,19 @@ def get_image_features(\n         Examples:\n \n         ```python\n-        >>> import torch\n+        >>> from PIL import Image\n+        >>> import requests\n         >>> from transformers import AutoProcessor, MetaClip2Model\n-        >>> from transformers.image_utils import load_image\n \n         >>> model = MetaClip2Model.from_pretrained(\"facebook/metaclip-2-worldwide-huge-quickgelu\")\n         >>> processor = AutoProcessor.from_pretrained(\"facebook/metaclip-2-worldwide-huge-quickgelu\")\n \n         >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n-        >>> image = load_image(url)\n+        >>> image = Image.open(requests.get(url, stream=True).raw)\n \n         >>> inputs = processor(images=image, return_tensors=\"pt\")\n \n-        >>> with torch.inference_mode():\n-        ...     image_features = model.get_image_features(**inputs)\n+        >>> image_features = model.get_image_features(**inputs)\n         ```\"\"\"\n         vision_outputs: BaseModelOutputWithPooling = self.vision_model(\n             pixel_values=pixel_values,\n@@ -980,22 +978,21 @@ def forward(\n         Examples:\n \n         ```python\n-        >>> import torch\n+        >>> from PIL import Image\n+        >>> import requests\n         >>> from transformers import AutoProcessor, MetaClip2Model\n-        >>> from transformers.image_utils import load_image\n \n         >>> model = MetaClip2Model.from_pretrained(\"facebook/metaclip-2-worldwide-huge-quickgelu\")\n         >>> processor = AutoProcessor.from_pretrained(\"facebook/metaclip-2-worldwide-huge-quickgelu\")\n \n         >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n-        >>> image = load_image(url)\n+        >>> image = Image.open(requests.get(url, stream=True).raw)\n \n         >>> inputs = processor(\n         ...     text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True\n         ... )\n \n-        >>> with torch.inference_mode():\n-        ...     outputs = model(**inputs)\n+        >>> outputs = model(**inputs)\n         >>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n         >>> probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n         ```\"\"\"\n@@ -1277,20 +1274,19 @@ def forward(\n         Examples:\n \n         ```python\n-        >>> import torch\n+        >>> from PIL import Image\n+        >>> import requests\n         >>> from transformers import AutoProcessor, MetaClip2VisionModelWithProjection\n-        >>> from transformers.image_utils import load_image\n \n         >>> model = MetaClip2VisionModelWithProjection.from_pretrained(\"facebook/metaclip-2-worldwide-huge-quickgelu\")\n         >>> processor = AutoProcessor.from_pretrained(\"facebook/metaclip-2-worldwide-huge-quickgelu\")\n \n         >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n-        >>> image = load_image(url)\n+        >>> image = Image.open(requests.get(url, stream=True).raw)\n \n         >>> inputs = processor(images=image, return_tensors=\"pt\")\n \n-        >>> with torch.inference_mode():\n-        ...     outputs = model(**inputs)\n+        >>> outputs = model(**inputs)\n         >>> image_embeds = outputs.image_embeds\n         ```\"\"\"\n "
        }
    ],
    "stats": {
        "total": 44,
        "additions": 20,
        "deletions": 24
    }
}