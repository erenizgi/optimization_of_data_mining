{
    "author": "Cyrilvallez",
    "message": "[core] Large/full refactor of `from_pretrained` (#36033)\n\n* squash everything together\nstart to simplify inner logic\n\nUpdate modeling_utils.py\n\nUpdate modeling_utils.py\n\nUpdate modeling_utils.py\n\nUpdate modeling_utils.py\n\ncontinue refactor\n\nfix\n\nsmall fixes\n\nadd type hints/docstring\n\nUpdate modeling_utils.py\n\nremove _fast_init\n\nkeep improving\n\nUpdate modeling_utils.py\n\nUpdate modeling_utils.py\n\nnew first tp loading version\n\nstyle\n\nfix weird in-place op\n\ntrigger CIs\n\nUpdate modeling_utils.py\n\nmuch clearer renaming of keys\n\nfix\n\nupdate\n\nUpdate test_modeling_common.py\n\ntrigger CIs\n\nupdate\n\nupdate\n\nstyle\n\nUpdate modeling_utils.py\n\nUpdate modeling_utils.py\n\nUpdate modeling_utils.py\n\nfix\n\nfast download first prototype\n\nremove old function\n\nremove old functions\n\nRemove unused function and move back _get_tp_registry\n\nfix tp plan registry\n\nsimplify\n\nCIs\n\nUpdate hub.py\n\nUpdate modeling_utils.py\n\nsimplify\n\nsimplify renaming logic\n\nremove unused check\n\nadd sanity check back (a test depends on it)\n\nUpdate modeling_utils.py\n\nfinalize sound renaming logic\n\nstyle\n\nadd forgotten check\n\nUpdate modeling_utils.py\n\nadd key_mapping keyword\n\nstyle\n\nUpdate modeling_utils.py\n\nadd comment\n\nminor updates\n\nminor change for clarity\n\nfix small prefix issue and simplify\n\nstyle\n\ntrigger CIs\n\ntypo fix\n\nPost rebase fix\n\npost rebase cleanup\n\nsimplify tp\n\ntypo\n\noupsi\n\ntypo\n\ncorrectly escape\n\nimprovements based on Marc's review\n\nfinalize Marc's review comments\n\n squash everything\n\n* improve\n\n* Update modeling_utils.py\n\n* Update modeling_utils.py\n\n* fix\n\n* Update modeling_utils.py\n\n* Update modeling_utils.py\n\n* style\n\n* Update modeling_utils.py\n\n* simplify\n\n* style\n\n* Update modeling_utils.py\n\n* Update modeling_utils.py\n\n* Update modeling_utils.py\n\n* Update modeling_utils.py\n\n* Update modeling_utils.py\n\n* Update modeling_utils.py\n\n* fix dtype issue\n\n* Update modeling_utils.py\n\n* style\n\n* remove test that does not make sense\n\n* style\n\n* small fixes\n\n* style\n\n* fix\n\n* cleanup after rebase\n\n* style\n\n* typo\n\n* escape\n\n* tp for task specific top modules\n\n* Update modeling_utils.py\n\n* Update modeling_utils.py\n\n* fix allocation\n\n* CIs\n\n* CIs\n\n* CIs\n\n* improve docstring\n\n* CIs\n\n* Update modeling_utils.py\n\n* fix",
    "sha": "071a161d3e38f56dbda2743b979f0afeed2cd4f1",
    "files": [
        {
            "sha": "ac6b36d2dbcf7fef3621a3b35f083ae89a39b1b4",
            "filename": "src/transformers/file_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/071a161d3e38f56dbda2743b979f0afeed2cd4f1/src%2Ftransformers%2Ffile_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/071a161d3e38f56dbda2743b979f0afeed2cd4f1/src%2Ftransformers%2Ffile_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ffile_utils.py?ref=071a161d3e38f56dbda2743b979f0afeed2cd4f1",
            "patch": "@@ -71,7 +71,6 @@\n     copy_func,\n     default_cache_path,\n     define_sagemaker_information,\n-    get_file_from_repo,\n     get_torch_version,\n     has_file,\n     http_user_agent,"
        },
        {
            "sha": "c896a5aa865831bc161d481d5de4a6d6e1cc58ad",
            "filename": "src/transformers/integrations/deepspeed.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/071a161d3e38f56dbda2743b979f0afeed2cd4f1/src%2Ftransformers%2Fintegrations%2Fdeepspeed.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/071a161d3e38f56dbda2743b979f0afeed2cd4f1/src%2Ftransformers%2Fintegrations%2Fdeepspeed.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fdeepspeed.py?ref=071a161d3e38f56dbda2743b979f0afeed2cd4f1",
            "patch": "@@ -306,7 +306,7 @@ def deepspeed_config():\n         return None\n \n \n-def _load_state_dict_into_zero3_model(model_to_load, state_dict, start_prefix, assign_to_params_buffers=False):\n+def _load_state_dict_into_zero3_model(model_to_load, state_dict, assign_to_params_buffers=False):\n     \"\"\"\n     Loads state dict into a model specifically for Zero3, since DeepSpeed does not support the `transformers`\n     tensor parallelism API.\n@@ -349,7 +349,7 @@ def load(module: nn.Module, state_dict, prefix=\"\", assign_to_params_buffers=Fals\n             if child is not None:\n                 load(child, state_dict, prefix + name + \".\", assign_to_params_buffers)\n \n-    load(model_to_load, state_dict, prefix=start_prefix, assign_to_params_buffers=assign_to_params_buffers)\n+    load(model_to_load, state_dict, assign_to_params_buffers=assign_to_params_buffers)\n     # Delete `state_dict` so it could be collected by GC earlier. Note that `state_dict` is a copy of the argument, so\n     # it's safe to delete it.\n     del state_dict"
        },
        {
            "sha": "61c86cffd045fb9b8d9aee85fce88a0058031acb",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 1163,
            "deletions": 1284,
            "changes": 2447,
            "blob_url": "https://github.com/huggingface/transformers/blob/071a161d3e38f56dbda2743b979f0afeed2cd4f1/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/071a161d3e38f56dbda2743b979f0afeed2cd4f1/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=071a161d3e38f56dbda2743b979f0afeed2cd4f1"
        },
        {
            "sha": "134571014f90b7bcab1ab0a1d397f2772aa1e771",
            "filename": "src/transformers/models/auto/feature_extraction_auto.py",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/071a161d3e38f56dbda2743b979f0afeed2cd4f1/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/071a161d3e38f56dbda2743b979f0afeed2cd4f1/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py?ref=071a161d3e38f56dbda2743b979f0afeed2cd4f1",
            "patch": "@@ -25,7 +25,7 @@\n from ...configuration_utils import PretrainedConfig\n from ...dynamic_module_utils import get_class_from_dynamic_module, resolve_trust_remote_code\n from ...feature_extraction_utils import FeatureExtractionMixin\n-from ...utils import CONFIG_NAME, FEATURE_EXTRACTOR_NAME, get_file_from_repo, logging\n+from ...utils import CONFIG_NAME, FEATURE_EXTRACTOR_NAME, cached_file, logging\n from .auto_factory import _LazyAutoMapping\n from .configuration_auto import (\n     CONFIG_MAPPING_NAMES,\n@@ -220,7 +220,7 @@ def get_feature_extractor_config(\n             raise ValueError(\"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\")\n         token = use_auth_token\n \n-    resolved_config_file = get_file_from_repo(\n+    resolved_config_file = cached_file(\n         pretrained_model_name_or_path,\n         FEATURE_EXTRACTOR_NAME,\n         cache_dir=cache_dir,\n@@ -230,6 +230,9 @@ def get_feature_extractor_config(\n         token=token,\n         revision=revision,\n         local_files_only=local_files_only,\n+        _raise_exceptions_for_gated_repo=False,\n+        _raise_exceptions_for_missing_entries=False,\n+        _raise_exceptions_for_connection_errors=False,\n     )\n     if resolved_config_file is None:\n         logger.info("
        },
        {
            "sha": "40c45fa94b5b27cbc48a1e7537336f035a01883a",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/071a161d3e38f56dbda2743b979f0afeed2cd4f1/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/071a161d3e38f56dbda2743b979f0afeed2cd4f1/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=071a161d3e38f56dbda2743b979f0afeed2cd4f1",
            "patch": "@@ -29,7 +29,7 @@\n from ...utils import (\n     CONFIG_NAME,\n     IMAGE_PROCESSOR_NAME,\n-    get_file_from_repo,\n+    cached_file,\n     is_timm_config_dict,\n     is_timm_local_checkpoint,\n     is_torchvision_available,\n@@ -288,7 +288,7 @@ def get_image_processor_config(\n             raise ValueError(\"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\")\n         token = use_auth_token\n \n-    resolved_config_file = get_file_from_repo(\n+    resolved_config_file = cached_file(\n         pretrained_model_name_or_path,\n         IMAGE_PROCESSOR_NAME,\n         cache_dir=cache_dir,\n@@ -298,6 +298,9 @@ def get_image_processor_config(\n         token=token,\n         revision=revision,\n         local_files_only=local_files_only,\n+        _raise_exceptions_for_gated_repo=False,\n+        _raise_exceptions_for_missing_entries=False,\n+        _raise_exceptions_for_connection_errors=False,\n     )\n     if resolved_config_file is None:\n         logger.info("
        },
        {
            "sha": "53ddcd3d1e1f64ec5f4c58cf0ec8b5e0406b76f1",
            "filename": "src/transformers/models/auto/processing_auto.py",
            "status": "modified",
            "additions": 17,
            "deletions": 11,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/071a161d3e38f56dbda2743b979f0afeed2cd4f1/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/071a161d3e38f56dbda2743b979f0afeed2cd4f1/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py?ref=071a161d3e38f56dbda2743b979f0afeed2cd4f1",
            "patch": "@@ -28,7 +28,7 @@\n from ...image_processing_utils import ImageProcessingMixin\n from ...processing_utils import ProcessorMixin\n from ...tokenization_utils import TOKENIZER_CONFIG_FILE\n-from ...utils import FEATURE_EXTRACTOR_NAME, PROCESSOR_NAME, get_file_from_repo, logging\n+from ...utils import FEATURE_EXTRACTOR_NAME, PROCESSOR_NAME, cached_file, logging\n from .auto_factory import _LazyAutoMapping\n from .configuration_auto import (\n     CONFIG_MAPPING_NAMES,\n@@ -254,15 +254,21 @@ def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):\n         processor_auto_map = None\n \n         # First, let's see if we have a processor or preprocessor config.\n-        # Filter the kwargs for `get_file_from_repo`.\n-        get_file_from_repo_kwargs = {\n-            key: kwargs[key] for key in inspect.signature(get_file_from_repo).parameters.keys() if key in kwargs\n+        # Filter the kwargs for `cached_file`.\n+        cached_file_kwargs = {\n+            key: kwargs[key] for key in inspect.signature(cached_file).parameters.keys() if key in kwargs\n         }\n+        # We don't want to raise\n+        cached_file_kwargs.update(\n+            {\n+                \"_raise_exceptions_for_gated_repo\": False,\n+                \"_raise_exceptions_for_missing_entries\": False,\n+                \"_raise_exceptions_for_connection_errors\": False,\n+            }\n+        )\n \n         # Let's start by checking whether the processor class is saved in a processor config\n-        processor_config_file = get_file_from_repo(\n-            pretrained_model_name_or_path, PROCESSOR_NAME, **get_file_from_repo_kwargs\n-        )\n+        processor_config_file = cached_file(pretrained_model_name_or_path, PROCESSOR_NAME, **cached_file_kwargs)\n         if processor_config_file is not None:\n             config_dict, _ = ProcessorMixin.get_processor_dict(pretrained_model_name_or_path, **kwargs)\n             processor_class = config_dict.get(\"processor_class\", None)\n@@ -271,8 +277,8 @@ def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):\n \n         if processor_class is None:\n             # If not found, let's check whether the processor class is saved in an image processor config\n-            preprocessor_config_file = get_file_from_repo(\n-                pretrained_model_name_or_path, FEATURE_EXTRACTOR_NAME, **get_file_from_repo_kwargs\n+            preprocessor_config_file = cached_file(\n+                pretrained_model_name_or_path, FEATURE_EXTRACTOR_NAME, **cached_file_kwargs\n             )\n             if preprocessor_config_file is not None:\n                 config_dict, _ = ImageProcessingMixin.get_image_processor_dict(pretrained_model_name_or_path, **kwargs)\n@@ -291,8 +297,8 @@ def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):\n \n         if processor_class is None:\n             # Next, let's check whether the processor class is saved in a tokenizer\n-            tokenizer_config_file = get_file_from_repo(\n-                pretrained_model_name_or_path, TOKENIZER_CONFIG_FILE, **get_file_from_repo_kwargs\n+            tokenizer_config_file = cached_file(\n+                pretrained_model_name_or_path, TOKENIZER_CONFIG_FILE, **cached_file_kwargs\n             )\n             if tokenizer_config_file is not None:\n                 with open(tokenizer_config_file, encoding=\"utf-8\") as reader:"
        },
        {
            "sha": "5fa5cd19164d26ad01318947092fcc654eca567f",
            "filename": "src/transformers/models/bark/processing_bark.py",
            "status": "modified",
            "additions": 9,
            "deletions": 3,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/071a161d3e38f56dbda2743b979f0afeed2cd4f1/src%2Ftransformers%2Fmodels%2Fbark%2Fprocessing_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/071a161d3e38f56dbda2743b979f0afeed2cd4f1/src%2Ftransformers%2Fmodels%2Fbark%2Fprocessing_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2Fprocessing_bark.py?ref=071a161d3e38f56dbda2743b979f0afeed2cd4f1",
            "patch": "@@ -25,7 +25,7 @@\n from ...feature_extraction_utils import BatchFeature\n from ...processing_utils import ProcessorMixin\n from ...utils import logging\n-from ...utils.hub import get_file_from_repo\n+from ...utils.hub import cached_file\n from ..auto import AutoTokenizer\n \n \n@@ -86,7 +86,7 @@ def from_pretrained(\n         \"\"\"\n \n         if speaker_embeddings_dict_path is not None:\n-            speaker_embeddings_path = get_file_from_repo(\n+            speaker_embeddings_path = cached_file(\n                 pretrained_processor_name_or_path,\n                 speaker_embeddings_dict_path,\n                 subfolder=kwargs.pop(\"subfolder\", None),\n@@ -97,6 +97,9 @@ def from_pretrained(\n                 local_files_only=kwargs.pop(\"local_files_only\", False),\n                 token=kwargs.pop(\"use_auth_token\", None),\n                 revision=kwargs.pop(\"revision\", None),\n+                _raise_exceptions_for_gated_repo=False,\n+                _raise_exceptions_for_missing_entries=False,\n+                _raise_exceptions_for_connection_errors=False,\n             )\n             if speaker_embeddings_path is None:\n                 logger.warning(\n@@ -182,7 +185,7 @@ def _load_voice_preset(self, voice_preset: str = None, **kwargs):\n                     f\"Voice preset unrecognized, missing {key} as a key in self.speaker_embeddings[{voice_preset}].\"\n                 )\n \n-            path = get_file_from_repo(\n+            path = cached_file(\n                 self.speaker_embeddings.get(\"repo_or_path\", \"/\"),\n                 voice_preset_paths[key],\n                 subfolder=kwargs.pop(\"subfolder\", None),\n@@ -193,6 +196,9 @@ def _load_voice_preset(self, voice_preset: str = None, **kwargs):\n                 local_files_only=kwargs.pop(\"local_files_only\", False),\n                 token=kwargs.pop(\"use_auth_token\", None),\n                 revision=kwargs.pop(\"revision\", None),\n+                _raise_exceptions_for_gated_repo=False,\n+                _raise_exceptions_for_missing_entries=False,\n+                _raise_exceptions_for_connection_errors=False,\n             )\n             if path is None:\n                 raise ValueError("
        },
        {
            "sha": "45bb6aa49f39ae8e807a49d488bf9620e533efc7",
            "filename": "src/transformers/models/cvt/modeling_cvt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/071a161d3e38f56dbda2743b979f0afeed2cd4f1/src%2Ftransformers%2Fmodels%2Fcvt%2Fmodeling_cvt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/071a161d3e38f56dbda2743b979f0afeed2cd4f1/src%2Ftransformers%2Fmodels%2Fcvt%2Fmodeling_cvt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcvt%2Fmodeling_cvt.py?ref=071a161d3e38f56dbda2743b979f0afeed2cd4f1",
            "patch": "@@ -544,7 +544,7 @@ def _init_weights(self, module):\n         elif isinstance(module, CvtStage):\n             if self.config.cls_token[module.stage]:\n                 module.cls_token.data = nn.init.trunc_normal_(\n-                    torch.zeros(1, 1, self.config.embed_dim[-1]), mean=0.0, std=self.config.initializer_range\n+                    module.cls_token.data, mean=0.0, std=self.config.initializer_range\n                 )\n \n "
        },
        {
            "sha": "280b52a1b9f935c9f7d4375751363ba42eb23b68",
            "filename": "src/transformers/models/regnet/convert_regnet_seer_10b_to_pytorch.py",
            "status": "modified",
            "additions": 10,
            "deletions": 6,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/071a161d3e38f56dbda2743b979f0afeed2cd4f1/src%2Ftransformers%2Fmodels%2Fregnet%2Fconvert_regnet_seer_10b_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/071a161d3e38f56dbda2743b979f0afeed2cd4f1/src%2Ftransformers%2Fmodels%2Fregnet%2Fconvert_regnet_seer_10b_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fregnet%2Fconvert_regnet_seer_10b_to_pytorch.py?ref=071a161d3e38f56dbda2743b979f0afeed2cd4f1",
            "patch": "@@ -35,7 +35,7 @@\n from vissl.models.model_helpers import get_trunk_forward_outputs\n \n from transformers import AutoImageProcessor, RegNetConfig, RegNetForImageClassification, RegNetModel\n-from transformers.modeling_utils import PreTrainedModel\n+from transformers.modeling_utils import _load_state_dict_into_meta_model, load_state_dict\n from transformers.utils import logging\n \n \n@@ -244,14 +244,18 @@ def load_using_classy_vision(checkpoint_url: str) -> Tuple[Dict, Dict]:\n         our_model_func = RegNetModel\n         if \"in1k\" in model_name:\n             our_model_func = RegNetForImageClassification\n-        our_model = our_model_func(our_config)\n-        # place our model to the meta device (so remove all the weights)\n-        our_model.to(torch.device(\"meta\"))\n+        with torch.device(\"meta\"):\n+            our_model = our_model_func(our_config)\n         logger.info(\"Loading state_dict in our model.\")\n         # load state dict\n         state_dict_keys = our_model.state_dict().keys()\n-        PreTrainedModel._load_pretrained_model_low_mem(\n-            our_model, state_dict_keys, [save_directory / f\"{model_name}.pth\"]\n+        state_dict = load_state_dict(save_directory / f\"{model_name}.pth\", weights_only=True)\n+        fixed_state_dict = state_dict = {our_model._fix_state_dict_key_on_load(k)[0]: v for k, v in state_dict.items()}\n+        _load_state_dict_into_meta_model(\n+            our_model,\n+            fixed_state_dict,\n+            start_prefix=\"\",\n+            expected_keys=state_dict_keys,\n         )\n         logger.info(\"Finally, pushing!\")\n         # push it to hub"
        },
        {
            "sha": "48c1d85825335ba66f4328d1db9fa5e191f46ecc",
            "filename": "src/transformers/models/timm_wrapper/modeling_timm_wrapper.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/071a161d3e38f56dbda2743b979f0afeed2cd4f1/src%2Ftransformers%2Fmodels%2Ftimm_wrapper%2Fmodeling_timm_wrapper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/071a161d3e38f56dbda2743b979f0afeed2cd4f1/src%2Ftransformers%2Fmodels%2Ftimm_wrapper%2Fmodeling_timm_wrapper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftimm_wrapper%2Fmodeling_timm_wrapper.py?ref=071a161d3e38f56dbda2743b979f0afeed2cd4f1",
            "patch": "@@ -113,7 +113,7 @@ def load_state_dict(self, state_dict, *args, **kwargs):\n         Override original method to fix state_dict keys on load for cases when weights are loaded\n         without using the `from_pretrained` method (e.g., in Trainer to resume from checkpoint).\n         \"\"\"\n-        state_dict = self._fix_state_dict_keys_on_load(state_dict)\n+        state_dict = {self._fix_state_dict_key_on_load(k)[0]: v for k, v in state_dict.items()}\n         return super().load_state_dict(state_dict, *args, **kwargs)\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "bdcb273c7e922e66b7ffe0cd252696fbc5badfeb",
            "filename": "src/transformers/utils/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/071a161d3e38f56dbda2743b979f0afeed2cd4f1/src%2Ftransformers%2Futils%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/071a161d3e38f56dbda2743b979f0afeed2cd4f1/src%2Ftransformers%2Futils%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2F__init__.py?ref=071a161d3e38f56dbda2743b979f0afeed2cd4f1",
            "patch": "@@ -91,7 +91,6 @@\n     define_sagemaker_information,\n     download_url,\n     extract_commit_hash,\n-    get_file_from_repo,\n     has_file,\n     http_user_agent,\n     is_offline_mode,"
        },
        {
            "sha": "01d19c214053cb1d18db6949602d38130c3895ee",
            "filename": "src/transformers/utils/hub.py",
            "status": "modified",
            "additions": 246,
            "deletions": 180,
            "changes": 426,
            "blob_url": "https://github.com/huggingface/transformers/blob/071a161d3e38f56dbda2743b979f0afeed2cd4f1/src%2Ftransformers%2Futils%2Fhub.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/071a161d3e38f56dbda2743b979f0afeed2cd4f1/src%2Ftransformers%2Futils%2Fhub.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fhub.py?ref=071a161d3e38f56dbda2743b979f0afeed2cd4f1",
            "patch": "@@ -40,14 +40,14 @@\n     create_repo,\n     hf_hub_download,\n     hf_hub_url,\n+    snapshot_download,\n     try_to_load_from_cache,\n )\n from huggingface_hub.file_download import REGEX_COMMIT_HASH, http_get\n from huggingface_hub.utils import (\n     EntryNotFoundError,\n     GatedRepoError,\n     HfHubHTTPError,\n-    HFValidationError,\n     LocalEntryNotFoundError,\n     OfflineModeIsEnabled,\n     RepositoryNotFoundError,\n@@ -69,7 +69,6 @@\n     is_torch_available,\n     is_training_run_on_sagemaker,\n )\n-from .logging import tqdm\n \n \n logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n@@ -209,6 +208,69 @@ def extract_commit_hash(resolved_file: Optional[str], commit_hash: Optional[str]\n def cached_file(\n     path_or_repo_id: Union[str, os.PathLike],\n     filename: str,\n+    **kwargs,\n+) -> Optional[str]:\n+    \"\"\"\n+    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\n+\n+    Args:\n+        path_or_repo_id (`str` or `os.PathLike`):\n+            This can be either:\n+            - a string, the *model id* of a model repo on huggingface.co.\n+            - a path to a *directory* potentially containing the file.\n+        filename (`str`):\n+            The name of the file to locate in `path_or_repo`.\n+        cache_dir (`str` or `os.PathLike`, *optional*):\n+            Path to a directory in which a downloaded pretrained model configuration should be cached if the standard\n+            cache should not be used.\n+        force_download (`bool`, *optional*, defaults to `False`):\n+            Whether or not to force to (re-)download the configuration files and override the cached versions if they\n+            exist.\n+        resume_download:\n+            Deprecated and ignored. All downloads are now resumed by default when possible.\n+            Will be removed in v5 of Transformers.\n+        proxies (`Dict[str, str]`, *optional*):\n+            A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n+            'http://hostname': 'foo.bar:4012'}.` The proxies are used on each request.\n+        token (`str` or *bool*, *optional*):\n+            The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated\n+            when running `huggingface-cli login` (stored in `~/.huggingface`).\n+        revision (`str`, *optional*, defaults to `\"main\"`):\n+            The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n+            git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\n+            identifier allowed by git.\n+        local_files_only (`bool`, *optional*, defaults to `False`):\n+            If `True`, will only try to load the tokenizer configuration from local files.\n+        subfolder (`str`, *optional*, defaults to `\"\"`):\n+            In case the relevant files are located inside a subfolder of the model repo on huggingface.co, you can\n+            specify the folder name here.\n+        repo_type (`str`, *optional*):\n+            Specify the repo type (useful when downloading from a space for instance).\n+\n+    <Tip>\n+\n+    Passing `token=True` is required when you want to use a private model.\n+\n+    </Tip>\n+\n+    Returns:\n+        `Optional[str]`: Returns the resolved file (to the cache folder if downloaded from a repo).\n+\n+    Examples:\n+\n+    ```python\n+    # Download a model weight from the Hub and cache it.\n+    model_weights_file = cached_file(\"google-bert/bert-base-uncased\", \"pytorch_model.bin\")\n+    ```\n+    \"\"\"\n+    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)\n+    file = file[0] if file is not None else file\n+    return file\n+\n+\n+def cached_files(\n+    path_or_repo_id: Union[str, os.PathLike],\n+    filenames: List[str],\n     cache_dir: Optional[Union[str, os.PathLike]] = None,\n     force_download: bool = False,\n     resume_download: Optional[bool] = None,\n@@ -226,16 +288,15 @@ def cached_file(\n     **deprecated_kwargs,\n ) -> Optional[str]:\n     \"\"\"\n-    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\n+    Tries to locate several files in a local folder and repo, downloads and cache them if necessary.\n \n     Args:\n         path_or_repo_id (`str` or `os.PathLike`):\n             This can be either:\n-\n             - a string, the *model id* of a model repo on huggingface.co.\n             - a path to a *directory* potentially containing the file.\n-        filename (`str`):\n-            The name of the file to locate in `path_or_repo`.\n+        filenames (`List[str]`):\n+            The name of all the files to locate in `path_or_repo`.\n         cache_dir (`str` or `os.PathLike`, *optional*):\n             Path to a directory in which a downloaded pretrained model configuration should be cached if the standard\n             cache should not be used.\n@@ -263,6 +324,17 @@ def cached_file(\n         repo_type (`str`, *optional*):\n             Specify the repo type (useful when downloading from a space for instance).\n \n+    Private args:\n+        _raise_exceptions_for_gated_repo (`bool`):\n+            if False, do not raise an exception for gated repo error but return None.\n+        _raise_exceptions_for_missing_entries (`bool`):\n+            if False, do not raise an exception for missing entries but return None.\n+        _raise_exceptions_for_connection_errors (`bool`):\n+            if False, do not raise an exception for connection errors but return None.\n+        _commit_hash (`str`, *optional*):\n+            passed when we are chaining several calls to various files (e.g. when loading a tokenizer or\n+            a pipeline). If files are cached for this commit hash, avoid calls to head and get from the cache.\n+\n     <Tip>\n \n     Passing `token=True` is required when you want to use a private model.\n@@ -289,144 +361,176 @@ def cached_file(\n             raise ValueError(\"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\")\n         token = use_auth_token\n \n-    # Private arguments\n-    #     _raise_exceptions_for_gated_repo: if False, do not raise an exception for gated repo error but return\n-    #         None.\n-    #     _raise_exceptions_for_missing_entries: if False, do not raise an exception for missing entries but return\n-    #         None.\n-    #     _raise_exceptions_for_connection_errors: if False, do not raise an exception for connection errors but return\n-    #         None.\n-    #     _commit_hash: passed when we are chaining several calls to various files (e.g. when loading a tokenizer or\n-    #         a pipeline). If files are cached for this commit hash, avoid calls to head and get from the cache.\n     if is_offline_mode() and not local_files_only:\n         logger.info(\"Offline mode: forcing local_files_only=True\")\n         local_files_only = True\n     if subfolder is None:\n         subfolder = \"\"\n \n+    # Add folder to filenames\n+    full_filenames = [os.path.join(subfolder, file) for file in filenames]\n+\n     path_or_repo_id = str(path_or_repo_id)\n-    full_filename = os.path.join(subfolder, filename)\n-    if os.path.isdir(path_or_repo_id):\n-        resolved_file = os.path.join(os.path.join(path_or_repo_id, subfolder), filename)\n-        if not os.path.isfile(resolved_file):\n-            if _raise_exceptions_for_missing_entries and filename not in [\"config.json\", f\"{subfolder}/config.json\"]:\n-                raise EnvironmentError(\n-                    f\"{path_or_repo_id} does not appear to have a file named {full_filename}. Checkout \"\n-                    f\"'https://huggingface.co/{path_or_repo_id}/tree/{revision}' for available files.\"\n-                )\n-            else:\n-                return None\n-        return resolved_file\n+    existing_files = []\n+    for filename in full_filenames:\n+        if os.path.isdir(path_or_repo_id):\n+            resolved_file = os.path.join(path_or_repo_id, filename)\n+            if not os.path.isfile(resolved_file):\n+                if _raise_exceptions_for_missing_entries and filename != os.path.join(subfolder, \"config.json\"):\n+                    revision_ = \"main\" if revision is None else revision\n+                    raise EnvironmentError(\n+                        f\"{path_or_repo_id} does not appear to have a file named {filename}. Checkout \"\n+                        f\"'https://huggingface.co/{path_or_repo_id}/tree/{revision_}' for available files.\"\n+                    )\n+                else:\n+                    return None\n+            existing_files.append(resolved_file)\n+\n+    # All files exist\n+    if len(existing_files) == len(full_filenames):\n+        return existing_files\n \n     if cache_dir is None:\n         cache_dir = TRANSFORMERS_CACHE\n     if isinstance(cache_dir, Path):\n         cache_dir = str(cache_dir)\n \n+    existing_files = []\n+    file_counter = 0\n     if _commit_hash is not None and not force_download:\n-        # If the file is cached under that commit hash, we return it directly.\n-        resolved_file = try_to_load_from_cache(\n-            path_or_repo_id, full_filename, cache_dir=cache_dir, revision=_commit_hash, repo_type=repo_type\n-        )\n-        if resolved_file is not None:\n-            if resolved_file is not _CACHED_NO_EXIST:\n-                return resolved_file\n-            elif not _raise_exceptions_for_missing_entries:\n-                return None\n-            else:\n-                raise EnvironmentError(f\"Could not locate {full_filename} inside {path_or_repo_id}.\")\n+        for filename in full_filenames:\n+            # If the file is cached under that commit hash, we return it directly.\n+            resolved_file = try_to_load_from_cache(\n+                path_or_repo_id, filename, cache_dir=cache_dir, revision=_commit_hash, repo_type=repo_type\n+            )\n+            if resolved_file is not None:\n+                if resolved_file is not _CACHED_NO_EXIST:\n+                    file_counter += 1\n+                    existing_files.append(resolved_file)\n+                elif not _raise_exceptions_for_missing_entries:\n+                    file_counter += 1\n+                else:\n+                    raise EnvironmentError(f\"Could not locate {filename} inside {path_or_repo_id}.\")\n+\n+    # Either all the files were found, or some were _CACHED_NO_EXIST but we do not raise for missing entries\n+    if file_counter == len(full_filenames):\n+        return existing_files if len(existing_files) > 0 else None\n \n     user_agent = http_user_agent(user_agent)\n+    # download the files if needed\n     try:\n-        # Load from URL or cache if already cached\n-        resolved_file = hf_hub_download(\n-            path_or_repo_id,\n-            filename,\n-            subfolder=None if len(subfolder) == 0 else subfolder,\n-            repo_type=repo_type,\n-            revision=revision,\n-            cache_dir=cache_dir,\n-            user_agent=user_agent,\n-            force_download=force_download,\n-            proxies=proxies,\n-            resume_download=resume_download,\n-            token=token,\n-            local_files_only=local_files_only,\n-        )\n-    except GatedRepoError as e:\n-        resolved_file = _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n-        if resolved_file is not None or not _raise_exceptions_for_gated_repo:\n-            return resolved_file\n-        raise EnvironmentError(\n-            \"You are trying to access a gated repo.\\nMake sure to have access to it at \"\n-            f\"https://huggingface.co/{path_or_repo_id}.\\n{str(e)}\"\n-        ) from e\n-    except RepositoryNotFoundError as e:\n-        raise EnvironmentError(\n-            f\"{path_or_repo_id} is not a local folder and is not a valid model identifier \"\n-            \"listed on 'https://huggingface.co/models'\\nIf this is a private repository, make sure to pass a token \"\n-            \"having permission to this repo either by logging in with `huggingface-cli login` or by passing \"\n-            \"`token=<your_token>`\"\n-        ) from e\n-    except RevisionNotFoundError as e:\n-        raise EnvironmentError(\n-            f\"{revision} is not a valid git identifier (branch name, tag name or commit id) that exists \"\n-            \"for this model name. Check the model page at \"\n-            f\"'https://huggingface.co/{path_or_repo_id}' for available revisions.\"\n-        ) from e\n-    except LocalEntryNotFoundError as e:\n-        resolved_file = _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n-        if (\n-            resolved_file is not None\n-            or not _raise_exceptions_for_missing_entries\n-            or not _raise_exceptions_for_connection_errors\n-        ):\n-            return resolved_file\n-        raise EnvironmentError(\n-            f\"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this file, couldn't find it in the\"\n-            f\" cached files and it looks like {path_or_repo_id} is not the path to a directory containing a file named\"\n-            f\" {full_filename}.\\nCheckout your internet connection or see how to run the library in offline mode at\"\n-            \" 'https://huggingface.co/docs/transformers/installation#offline-mode'.\"\n-        ) from e\n-    except EntryNotFoundError as e:\n-        if not _raise_exceptions_for_missing_entries:\n-            return None\n-        if revision is None:\n-            revision = \"main\"\n-        if filename in [\"config.json\", f\"{subfolder}/config.json\"]:\n+        if len(full_filenames) == 1:\n+            # This is slightly better for only 1 file\n+            hf_hub_download(\n+                path_or_repo_id,\n+                filenames[0],\n+                subfolder=None if len(subfolder) == 0 else subfolder,\n+                repo_type=repo_type,\n+                revision=revision,\n+                cache_dir=cache_dir,\n+                user_agent=user_agent,\n+                force_download=force_download,\n+                proxies=proxies,\n+                resume_download=resume_download,\n+                token=token,\n+                local_files_only=local_files_only,\n+            )\n+        else:\n+            snapshot_download(\n+                path_or_repo_id,\n+                allow_patterns=full_filenames,\n+                repo_type=repo_type,\n+                revision=revision,\n+                cache_dir=cache_dir,\n+                user_agent=user_agent,\n+                force_download=force_download,\n+                proxies=proxies,\n+                resume_download=resume_download,\n+                token=token,\n+                local_files_only=local_files_only,\n+            )\n+\n+    except Exception as e:\n+        # We cannot recover from them\n+        if isinstance(e, RepositoryNotFoundError) and not isinstance(e, GatedRepoError):\n+            raise EnvironmentError(\n+                f\"{path_or_repo_id} is not a local folder and is not a valid model identifier \"\n+                \"listed on 'https://huggingface.co/models'\\nIf this is a private repository, make sure to pass a token \"\n+                \"having permission to this repo either by logging in with `huggingface-cli login` or by passing \"\n+                \"`token=<your_token>`\"\n+            ) from e\n+        elif isinstance(e, RevisionNotFoundError):\n+            raise EnvironmentError(\n+                f\"{revision} is not a valid git identifier (branch name, tag name or commit id) that exists \"\n+                \"for this model name. Check the model page at \"\n+                f\"'https://huggingface.co/{path_or_repo_id}' for available revisions.\"\n+            ) from e\n+\n+        # Now we try to recover if we can find all files correctly in the cache\n+        resolved_files = [\n+            _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision) for filename in full_filenames\n+        ]\n+        if all(file is not None for file in resolved_files):\n+            return resolved_files\n+\n+        # Raise based on the flags. Note that we will raise for missing entries at the very end, even when\n+        # not entering this Except block, as it may also happen when `snapshot_download` does not raise\n+        if isinstance(e, GatedRepoError):\n+            if not _raise_exceptions_for_gated_repo:\n+                return None\n+            raise EnvironmentError(\n+                \"You are trying to access a gated repo.\\nMake sure to have access to it at \"\n+                f\"https://huggingface.co/{path_or_repo_id}.\\n{str(e)}\"\n+            ) from e\n+        elif isinstance(e, LocalEntryNotFoundError):\n+            if not _raise_exceptions_for_connection_errors:\n+                return None\n+            # Here we only raise if both flags for missing entry and connection errors are True (because it can be raised\n+            # even when `local_files_only` is True, in which case raising for connections errors only would not make sense)\n+            elif _raise_exceptions_for_missing_entries:\n+                raise EnvironmentError(\n+                    f\"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load the files, and couldn't find them in the\"\n+                    f\" cached files.\\nCheckout your internet connection or see how to run the library in offline mode at\"\n+                    \" 'https://huggingface.co/docs/transformers/installation#offline-mode'.\"\n+                ) from e\n+        # snapshot_download will not raise EntryNotFoundError, but hf_hub_download can. If this is the case, it will be treated\n+        # later on anyway and re-raised if needed\n+        elif isinstance(e, HTTPError) and not isinstance(e, EntryNotFoundError):\n+            if not _raise_exceptions_for_connection_errors:\n+                return None\n+            raise EnvironmentError(\n+                f\"There was a specific connection error when trying to load {path_or_repo_id}:\\n{e}\"\n+            )\n+\n+    resolved_files = [\n+        _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision) for filename in full_filenames\n+    ]\n+    # If there are any missing file and the flag is active, raise\n+    if any(file is None for file in resolved_files) and _raise_exceptions_for_missing_entries:\n+        missing_entries = [original for original, resolved in zip(full_filenames, resolved_files) if resolved is None]\n+        # Last escape\n+        if len(resolved_files) == 1 and missing_entries[0] == os.path.join(subfolder, \"config.json\"):\n             return None\n+        # Now we raise for missing entries\n+        revision_ = \"main\" if revision is None else revision\n+        msg = f\"a file named {missing_entries[0]}\" if len(missing_entries) == 1 else f\"files named {*missing_entries,}\"\n         raise EnvironmentError(\n-            f\"{path_or_repo_id} does not appear to have a file named {full_filename}. Checkout \"\n-            f\"'https://huggingface.co/{path_or_repo_id}/tree/{revision}' for available files.\"\n-        ) from e\n-    except HTTPError as err:\n-        resolved_file = _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n-        if resolved_file is not None or not _raise_exceptions_for_connection_errors:\n-            return resolved_file\n-        raise EnvironmentError(f\"There was a specific connection error when trying to load {path_or_repo_id}:\\n{err}\")\n-    except HFValidationError as e:\n-        raise EnvironmentError(\n-            f\"Incorrect path_or_model_id: '{path_or_repo_id}'. Please provide either the path to a local folder or the repo_id of a model on the Hub.\"\n-        ) from e\n-    return resolved_file\n+            f\"{path_or_repo_id} does not appear to have {msg}. Checkout 'https://huggingface.co/{path_or_repo_id}/tree/{revision_}'\"\n+            \"for available files.\"\n+        )\n+\n+    # Remove potential missing entries (we can silently remove them at this point based on the flags)\n+    resolved_files = [file for file in resolved_files if file is not None]\n+    # Return `None` if the list is empty, coherent with other Exception when the flag is not active\n+    resolved_files = None if len(resolved_files) == 0 else resolved_files\n \n+    return resolved_files\n \n-# TODO: deprecate `get_file_from_repo` or document it differently?\n-#       Docstring is exactly the same as `cached_repo` but behavior is slightly different. If file is missing or if\n-#       there is a connection error, `cached_repo` will return None while `get_file_from_repo` will raise an error.\n-#       IMO we should keep only 1 method and have a single `raise_error` argument (to be discussed).\n+\n+# TODO cyril: Deprecated and should be removed in 4.51\n def get_file_from_repo(\n-    path_or_repo: Union[str, os.PathLike],\n-    filename: str,\n-    cache_dir: Optional[Union[str, os.PathLike]] = None,\n-    force_download: bool = False,\n-    resume_download: Optional[bool] = None,\n-    proxies: Optional[Dict[str, str]] = None,\n-    token: Optional[Union[bool, str]] = None,\n-    revision: Optional[str] = None,\n-    local_files_only: bool = False,\n-    subfolder: str = \"\",\n-    **deprecated_kwargs,\n+    *args,\n+    **kwargs,\n ):\n     \"\"\"\n     Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\n@@ -483,30 +587,15 @@ def get_file_from_repo(\n     tokenizer_config = get_file_from_repo(\"FacebookAI/xlm-roberta-base\", \"tokenizer_config.json\")\n     ```\n     \"\"\"\n-    use_auth_token = deprecated_kwargs.pop(\"use_auth_token\", None)\n-    if use_auth_token is not None:\n-        warnings.warn(\n-            \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n-            FutureWarning,\n-        )\n-        if token is not None:\n-            raise ValueError(\"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\")\n-        token = use_auth_token\n-\n+    logger.warning(\n+        \"`get_file_from_repo` is deprecated and will be removed in version 4.51. Use `cached_file` instead.\"\n+    )\n     return cached_file(\n-        path_or_repo_id=path_or_repo,\n-        filename=filename,\n-        cache_dir=cache_dir,\n-        force_download=force_download,\n-        resume_download=resume_download,\n-        proxies=proxies,\n-        token=token,\n-        revision=revision,\n-        local_files_only=local_files_only,\n-        subfolder=subfolder,\n+        *args,\n         _raise_exceptions_for_gated_repo=False,\n         _raise_exceptions_for_missing_entries=False,\n         _raise_exceptions_for_connection_errors=False,\n+        **kwargs,\n     )\n \n \n@@ -1023,45 +1112,22 @@ def get_checkpoint_shard_files(\n         shard_filenames = [os.path.join(pretrained_model_name_or_path, subfolder, f) for f in shard_filenames]\n         return shard_filenames, sharded_metadata\n \n-    # At this stage pretrained_model_name_or_path is a model identifier on the Hub\n-    cached_filenames = []\n-    # Check if the model is already cached or not. We only try the last checkpoint, this should cover most cases of\n-    # downloaded (if interrupted).\n-    last_shard = try_to_load_from_cache(\n-        pretrained_model_name_or_path, shard_filenames[-1], cache_dir=cache_dir, revision=_commit_hash\n+    # At this stage pretrained_model_name_or_path is a model identifier on the Hub. Try to get everything from cache,\n+    # or download the files\n+    cached_filenames = cached_files(\n+        pretrained_model_name_or_path,\n+        shard_filenames,\n+        cache_dir=cache_dir,\n+        force_download=force_download,\n+        proxies=proxies,\n+        resume_download=resume_download,\n+        local_files_only=local_files_only,\n+        token=token,\n+        user_agent=user_agent,\n+        revision=revision,\n+        subfolder=subfolder,\n+        _commit_hash=_commit_hash,\n     )\n-    show_progress_bar = last_shard is None or force_download\n-    for shard_filename in tqdm(shard_filenames, desc=\"Downloading shards\", disable=not show_progress_bar):\n-        try:\n-            # Load from URL\n-            cached_filename = cached_file(\n-                pretrained_model_name_or_path,\n-                shard_filename,\n-                cache_dir=cache_dir,\n-                force_download=force_download,\n-                proxies=proxies,\n-                resume_download=resume_download,\n-                local_files_only=local_files_only,\n-                token=token,\n-                user_agent=user_agent,\n-                revision=revision,\n-                subfolder=subfolder,\n-                _commit_hash=_commit_hash,\n-            )\n-        # We have already dealt with RepositoryNotFoundError and RevisionNotFoundError when getting the index, so\n-        # we don't have to catch them here.\n-        except EntryNotFoundError:\n-            raise EnvironmentError(\n-                f\"{pretrained_model_name_or_path} does not appear to have a file named {shard_filename} which is \"\n-                \"required according to the checkpoint index.\"\n-            )\n-        except HTTPError:\n-            raise EnvironmentError(\n-                f\"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load {shard_filename}. You should try\"\n-                \" again after checking your internet connection.\"\n-            )\n-\n-        cached_filenames.append(cached_filename)\n \n     return cached_filenames, sharded_metadata\n "
        },
        {
            "sha": "c29f58f33ff0b3c38010978fb64f4df31ab78d95",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 5,
            "deletions": 8,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/071a161d3e38f56dbda2743b979f0afeed2cd4f1/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/071a161d3e38f56dbda2743b979f0afeed2cd4f1/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=071a161d3e38f56dbda2743b979f0afeed2cd4f1",
            "patch": "@@ -2368,10 +2368,9 @@ def test_model_weights_reload_no_missing_tied_weights(self):\n                 safe_save_file(placeholder_dict, os.path.join(tmp_dir, \"model.safetensors\"), metadata={\"format\": \"pt\"})\n                 model_reloaded, infos = model_class.from_pretrained(tmp_dir, output_loading_info=True)\n \n-                prefix = f\"{model_reloaded.base_model_prefix}.\"\n                 params = dict(model_reloaded.named_parameters())\n                 params.update(dict(model_reloaded.named_buffers()))\n-                param_names = {k[len(prefix) :] if k.startswith(prefix) else k for k in params.keys()}\n+                param_names = set(params.keys())\n \n                 missing_keys = set(infos[\"missing_keys\"])\n \n@@ -2383,9 +2382,8 @@ def test_model_weights_reload_no_missing_tied_weights(self):\n                     ptrs[id_tensor_storage(tensor)].append(name)\n                 tied_params = [names for _, names in ptrs.items() if len(names) > 1]\n                 for group in tied_params:\n-                    group = {k[len(prefix) :] if k.startswith(prefix) else k for k in group}\n                     # We remove the group from extra_missing if not all weights from group are in it\n-                    if len(group - extra_missing) > 0:\n+                    if len(set(group) - extra_missing) > 0:\n                         extra_missing = extra_missing - set(group)\n \n                 self.assertEqual(\n@@ -2399,15 +2397,14 @@ def test_model_weights_reload_no_missing_tied_weights(self):\n                 # Remove nonpersistent buffers from missed_missing\n                 buffers = [n for n, _ in model_reloaded.named_buffers()]\n                 nonpersistent_buffers = {n for n in buffers if n not in model_reloaded.state_dict()}\n-                nonpersistent_buffers = {\n-                    k[len(prefix) :] if k.startswith(prefix) else k for k in nonpersistent_buffers\n-                }\n                 missed_missing = missed_missing - nonpersistent_buffers\n \n                 if model_reloaded._keys_to_ignore_on_load_missing is None:\n                     expected_missing = set()\n                 else:\n-                    expected_missing = set(model_reloaded._keys_to_ignore_on_load_missing)\n+                    expected_missing = set()\n+                    for pattern in model_reloaded._keys_to_ignore_on_load_missing:\n+                        expected_missing.update({k for k in param_names if re.search(pattern, k) is not None})\n                 self.assertEqual(\n                     missed_missing,\n                     expected_missing,"
        },
        {
            "sha": "ec5887bd16c145fb20766619631698d981440074",
            "filename": "tests/utils/test_hub_utils.py",
            "status": "modified",
            "additions": 55,
            "deletions": 18,
            "changes": 73,
            "blob_url": "https://github.com/huggingface/transformers/blob/071a161d3e38f56dbda2743b979f0afeed2cd4f1/tests%2Futils%2Ftest_hub_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/071a161d3e38f56dbda2743b979f0afeed2cd4f1/tests%2Futils%2Ftest_hub_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_hub_utils.py?ref=071a161d3e38f56dbda2743b979f0afeed2cd4f1",
            "patch": "@@ -28,7 +28,6 @@\n     TRANSFORMERS_CACHE,\n     WEIGHTS_NAME,\n     cached_file,\n-    get_file_from_repo,\n     has_file,\n )\n \n@@ -87,14 +86,8 @@ def test_non_existence_is_cached(self):\n         path = cached_file(RANDOM_BERT, \"conf\", local_files_only=True, _raise_exceptions_for_missing_entries=False)\n         self.assertIsNone(path)\n \n-        response_mock = mock.Mock()\n-        response_mock.status_code = 500\n-        response_mock.headers = {}\n-        response_mock.raise_for_status.side_effect = HTTPError\n-        response_mock.json.return_value = {}\n-\n-        # Under the mock environment we get a 500 error when trying to reach the tokenizer.\n-        with mock.patch(\"requests.Session.request\", return_value=response_mock) as mock_head:\n+        # Under the mock environment, hf_hub_download will always raise an HTTPError\n+        with mock.patch(\"transformers.utils.hub.hf_hub_download\", side_effect=HTTPError) as mock_head:\n             path = cached_file(RANDOM_BERT, \"conf\", _raise_exceptions_for_connection_errors=False)\n             self.assertIsNone(path)\n             # This check we did call the fake head request\n@@ -117,18 +110,45 @@ def test_has_file_in_cache(self):\n             assert has_file(TINY_BERT_PT_ONLY, WEIGHTS_NAME, local_files_only=True, cache_dir=tmp_dir)\n \n     def test_get_file_from_repo_distant(self):\n-        # `get_file_from_repo` returns None if the file does not exist\n-        self.assertIsNone(get_file_from_repo(\"google-bert/bert-base-cased\", \"ahah.txt\"))\n+        # should return None if the file does not exist\n+        self.assertIsNone(\n+            cached_file(\n+                \"google-bert/bert-base-cased\",\n+                \"ahah.txt\",\n+                _raise_exceptions_for_gated_repo=False,\n+                _raise_exceptions_for_missing_entries=False,\n+                _raise_exceptions_for_connection_errors=False,\n+            )\n+        )\n \n         # The function raises if the repository does not exist.\n         with self.assertRaisesRegex(EnvironmentError, \"is not a valid model identifier\"):\n-            get_file_from_repo(\"bert-base-case\", CONFIG_NAME)\n+            cached_file(\n+                \"bert-base-case\",\n+                CONFIG_NAME,\n+                _raise_exceptions_for_gated_repo=False,\n+                _raise_exceptions_for_missing_entries=False,\n+                _raise_exceptions_for_connection_errors=False,\n+            )\n \n         # The function raises if the revision does not exist.\n         with self.assertRaisesRegex(EnvironmentError, \"is not a valid git identifier\"):\n-            get_file_from_repo(\"google-bert/bert-base-cased\", CONFIG_NAME, revision=\"ahaha\")\n-\n-        resolved_file = get_file_from_repo(\"google-bert/bert-base-cased\", CONFIG_NAME)\n+            cached_file(\n+                \"google-bert/bert-base-cased\",\n+                CONFIG_NAME,\n+                revision=\"ahaha\",\n+                _raise_exceptions_for_gated_repo=False,\n+                _raise_exceptions_for_missing_entries=False,\n+                _raise_exceptions_for_connection_errors=False,\n+            )\n+\n+        resolved_file = cached_file(\n+            \"google-bert/bert-base-cased\",\n+            CONFIG_NAME,\n+            _raise_exceptions_for_gated_repo=False,\n+            _raise_exceptions_for_missing_entries=False,\n+            _raise_exceptions_for_connection_errors=False,\n+        )\n         # The name is the cached name which is not very easy to test, so instead we load the content.\n         config = json.loads(open(resolved_file, \"r\").read())\n         self.assertEqual(config[\"hidden_size\"], 768)\n@@ -137,9 +157,26 @@ def test_get_file_from_repo_local(self):\n         with tempfile.TemporaryDirectory() as tmp_dir:\n             filename = Path(tmp_dir) / \"a.txt\"\n             filename.touch()\n-            self.assertEqual(get_file_from_repo(tmp_dir, \"a.txt\"), str(filename))\n-\n-            self.assertIsNone(get_file_from_repo(tmp_dir, \"b.txt\"))\n+            self.assertEqual(\n+                cached_file(\n+                    tmp_dir,\n+                    \"a.txt\",\n+                    _raise_exceptions_for_gated_repo=False,\n+                    _raise_exceptions_for_missing_entries=False,\n+                    _raise_exceptions_for_connection_errors=False,\n+                ),\n+                str(filename),\n+            )\n+\n+            self.assertIsNone(\n+                cached_file(\n+                    tmp_dir,\n+                    \"b.txt\",\n+                    _raise_exceptions_for_gated_repo=False,\n+                    _raise_exceptions_for_missing_entries=False,\n+                    _raise_exceptions_for_connection_errors=False,\n+                )\n+            )\n \n     def test_get_file_gated_repo(self):\n         \"\"\"Test download file from a gated repo fails with correct message when not authenticated.\"\"\""
        },
        {
            "sha": "a85b598c0845d3b00478fe433e69ccb85d188415",
            "filename": "tests/utils/test_modeling_utils.py",
            "status": "modified",
            "additions": 5,
            "deletions": 21,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/071a161d3e38f56dbda2743b979f0afeed2cd4f1/tests%2Futils%2Ftest_modeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/071a161d3e38f56dbda2743b979f0afeed2cd4f1/tests%2Futils%2Ftest_modeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_utils.py?ref=071a161d3e38f56dbda2743b979f0afeed2cd4f1",
            "patch": "@@ -14,7 +14,6 @@\n # limitations under the License.\n import copy\n import glob\n-import itertools\n import json\n import os\n import os.path\n@@ -525,13 +524,12 @@ def test_model_from_config_torch_dtype_composite(self):\n         self.assertEqual(model.vision_tower.dtype, torch.bfloat16)\n         self.assertEqual(model.multi_modal_projector.linear_1.weight.dtype, torch.float16)\n \n-        # TODO @ARTHURZUCKER FIX THIS\n         # but if the model has `_keep_in_fp32_modules` then those modules should be in fp32 no matter what\n-        # LlavaForConditionalGeneration._keep_in_fp32_modules = [\"multi_modal_projector\"]\n-        # model = LlavaForConditionalGeneration.from_pretrained(TINY_LLAVA, config=config, torch_dtype=\"auto\")\n-        # self.assertEqual(model.language_model.dtype, torch.float32)\n-        # self.assertEqual(model.vision_tower.dtype, torch.bfloat16)\n-        # self.assertEqual(model.multi_modal_projector.linear_1.weight.dtype, torch.float32)\n+        LlavaForConditionalGeneration._keep_in_fp32_modules = [\"multi_modal_projector\"]\n+        model = LlavaForConditionalGeneration.from_pretrained(TINY_LLAVA, config=config, torch_dtype=\"auto\")\n+        self.assertEqual(model.language_model.dtype, torch.float32)\n+        self.assertEqual(model.vision_tower.dtype, torch.bfloat16)\n+        self.assertEqual(model.multi_modal_projector.linear_1.weight.dtype, torch.float32)\n \n         # torch.set_default_dtype() supports only float dtypes, so will fail with non-float type\n         with self.assertRaises(ValueError):\n@@ -540,20 +538,6 @@ def test_model_from_config_torch_dtype_composite(self):\n                 TINY_LLAVA, torch_dtype={\"text_config\": \"float32\", \"vision_config\": \"int64\", \"\": \"float16\"}\n             )\n \n-    @require_torch\n-    @unittest.skip(\"Broken by @arthurzucker because the fix was not correct. Knowing the context is super hard\")\n-    def test_model_from_pretrained_meta_device(self):\n-        def is_on_meta(model_id, dtype):\n-            with torch.device(\"meta\"):\n-                model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=dtype)\n-                return all(value.device.type == \"meta\" for value in model.state_dict().values())\n-\n-        model_ids = (\"fxmarty/tiny-llama-fast-tokenizer\", \"fxmarty/small-llama-testing\")\n-        dtypes = (None, \"auto\", torch.float16)\n-\n-        for model_id, dtype in itertools.product(model_ids, dtypes):\n-            self.assertTrue(is_on_meta(model_id, dtype))\n-\n     def test_model_from_pretrained_torch_dtype(self):\n         # test that the model can be instantiated with dtype of either\n         # 1. explicit from_pretrained's torch_dtype argument"
        }
    ],
    "stats": {
        "total": 3065,
        "additions": 1524,
        "deletions": 1541
    }
}