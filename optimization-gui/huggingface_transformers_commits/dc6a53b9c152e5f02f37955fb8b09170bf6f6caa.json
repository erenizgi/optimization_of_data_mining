{
    "author": "JuanFKurucz",
    "message": "Fix code examples to load gpt 1 openai community model (#42347)\n\n* Fix code examples to load gpt 1 openai community model\n\n* Remove dtypes redundant declaration",
    "sha": "dc6a53b9c152e5f02f37955fb8b09170bf6f6caa",
    "files": [
        {
            "sha": "75798072bd670eff747cb66e2cf32e5ecc188c92",
            "filename": "docs/source/en/model_doc/openai-gpt.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/dc6a53b9c152e5f02f37955fb8b09170bf6f6caa/docs%2Fsource%2Fen%2Fmodel_doc%2Fopenai-gpt.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/dc6a53b9c152e5f02f37955fb8b09170bf6f6caa/docs%2Fsource%2Fen%2Fmodel_doc%2Fopenai-gpt.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fopenai-gpt.md?ref=dc6a53b9c152e5f02f37955fb8b09170bf6f6caa",
            "patch": "@@ -43,7 +43,7 @@ The example below demonstrates how to generate text with [`Pipeline`], [`AutoMod\n import torch\n from transformers import pipeline\n \n-generator = pipeline(task=\"text-generation\", model=\"openai-community/gpt\", dtype=torch.float16, device=0)\n+generator = pipeline(task=\"text-generation\", model=\"openai-community/openai-gpt\", device=0)\n output = generator(\"The future of AI is\", max_length=50, do_sample=True)\n print(output[0][\"generated_text\"])\n ```\n@@ -54,8 +54,8 @@ print(output[0][\"generated_text\"])\n ```python\n from transformers import AutoModelForCausalLM, AutoTokenizer\n \n-tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt\")\n-model = AutoModelForCausalLM.from_pretrained(\"openai-community/openai-gpt\", dtype=torch.float16)\n+tokenizer = AutoTokenizer.from_pretrained(\"openai-community/openai-gpt\")\n+model = AutoModelForCausalLM.from_pretrained(\"openai-community/openai-gpt\")\n \n inputs = tokenizer(\"The future of AI is\", return_tensors=\"pt\")\n outputs = model.generate(**inputs, max_length=50)"
        }
    ],
    "stats": {
        "total": 6,
        "additions": 3,
        "deletions": 3
    }
}