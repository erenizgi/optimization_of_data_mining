{
    "author": "zucchini-nlp",
    "message": "Chat template: update for processor (#35953)\n\n* update\r\n\r\n* we need batched nested input to always process correctly\r\n\r\n* update a bit\r\n\r\n* fix copies",
    "sha": "eebd2c972c0f4c9dec9129092393bf295a4516d9",
    "files": [
        {
            "sha": "2cec08ae9c20c326fec242b357fc337a3198619a",
            "filename": "src/transformers/image_utils.py",
            "status": "modified",
            "additions": 73,
            "deletions": 12,
            "changes": 85,
            "blob_url": "https://github.com/huggingface/transformers/blob/eebd2c972c0f4c9dec9129092393bf295a4516d9/src%2Ftransformers%2Fimage_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eebd2c972c0f4c9dec9129092393bf295a4516d9/src%2Ftransformers%2Fimage_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_utils.py?ref=eebd2c972c0f4c9dec9129092393bf295a4516d9",
            "patch": "@@ -562,21 +562,33 @@ def get_uniform_frame_indices(total_num_frames: int, num_frames: Optional[int] =\n     return indices\n \n \n-def read_video_opencv(video_path: str, num_frames: Optional[int] = None):\n+def read_video_opencv(video_path: str, num_frames: Optional[int] = None, fps: Optional[int] = None):\n     \"\"\"\n     Decode the video with open-cv decoder.\n \n     Args:\n         video_path (`str`):\n             Path to the video file.\n         num_frames (`int`, *optional*):\n-            Number of frames to sample uniformly. If not specified, all frames are sampled.\n+            Number of frames to sample uniformly. Should be passed only when `fps=None`.\n+            If not specified and `fps==None`, all frames are sampled.\n+        fps (`int`, *optional*):\n+            Number of frames to sample per second. Should be passed only when `num_frames=None`.\n+            If not specified and `num_frames==None`, all frames are sampled.\n \n     Returns:\n         np.ndarray: np array of decoded frames of shape (num_frames, height, width, 3).\n     \"\"\"\n     video = cv2.VideoCapture(video_path)\n     total_num_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n+    video_fps = video.get(cv2.CAP_PROP_FPS)\n+    if num_frames is None and fps is not None:\n+        num_frames = int(total_num_frames / video_fps * fps)\n+        if num_frames > total_num_frames:\n+            raise ValueError(\n+                f\"When loading the video with fps={fps}, we identified that num_frames ({num_frames}) > total_frames ({total_num_frames}) .\"\n+                f\"Make sure that fps of a video is less than the requested fps for loading. Detected video_fps={video_fps}\"\n+            )\n     indices = get_uniform_frame_indices(total_num_frames, num_frames=num_frames)\n \n     index = 0\n@@ -595,42 +607,66 @@ def read_video_opencv(video_path: str, num_frames: Optional[int] = None):\n     return np.stack(frames)\n \n \n-def read_video_decord(video_path: str, num_frames: Optional[int] = None):\n+def read_video_decord(video_path: str, num_frames: Optional[int] = None, fps: Optional[int] = None):\n     \"\"\"\n     Decode the video with Decord decoder.\n \n     Args:\n         video_path (`str`):\n             Path to the video file.\n         num_frames (`int`, *optional*):\n-            Number of frames to sample uniformly. If not specified, all frames are sampled.\n+            Number of frames to sample uniformly. Should be passed only when `fps=None`.\n+            If not specified and `fps==None`, all frames are sampled.\n+        fps (`int`, *optional*):\n+            Number of frames to sample per second. Should be passed only when `num_frames=None`.\n+            If not specified and `num_frames==None`, all frames are sampled.\n \n     Returns:\n         np.ndarray: np array of decoded frames of shape (num_frames, height, width, 3).\n     \"\"\"\n     vr = VideoReader(uri=video_path, ctx=cpu(0))  # decord has problems with gpu\n-    indices = get_uniform_frame_indices(total_num_frames=len(vr), num_frames=num_frames)\n+    video_fps = vr.get_avg_fps()\n+    total_num_frames = len(vr)\n+    if num_frames is None and fps is not None:\n+        num_frames = int(total_num_frames / video_fps * fps)\n+        if num_frames > total_num_frames:\n+            raise ValueError(\n+                f\"When loading the video with fps={fps}, we identified that num_frames ({num_frames}) > total_frames ({total_num_frames}) .\"\n+                f\"Make sure that fps of a video is less than the requested fps for loading. Detected video_fps={video_fps}\"\n+            )\n+    indices = get_uniform_frame_indices(total_num_frames=total_num_frames, num_frames=num_frames)\n     frames = vr.get_batch(indices).asnumpy()\n     return frames\n \n \n-def read_video_pyav(video_path: str, num_frames: Optional[int] = None):\n+def read_video_pyav(video_path: str, num_frames: Optional[int] = None, fps: Optional[int] = None):\n     \"\"\"\n     Decode the video with PyAV decoder.\n \n     Args:\n         video_path (`str`):\n             Path to the video file.\n         num_frames (`int`, *optional*):\n-            Number of frames to sample uniformly. If not specified, all frames are sampled.\n+            Number of frames to sample uniformly. Should be passed only when `fps=None`.\n+            If not specified and `fps==None`, all frames are sampled.\n+        fps (`int`, *optional*):\n+            Number of frames to sample per second. Should be passed only when `num_frames=None`.\n+            If not specified and `num_frames==None`, all frames are sampled.\n \n     Returns:\n         np.ndarray: np array of decoded frames of shape (num_frames, height, width, 3).\n     \"\"\"\n     container = av.open(video_path)\n \n-    # sample uniformly \"num_frames\" frames from the video\n     total_num_frames = container.streams.video[0].frames\n+    video_fps = container.streams.video[0].average_rate  # should we better use `av_guess_frame_rate`?\n+    if num_frames is None and fps is not None:\n+        num_frames = int(total_num_frames / video_fps * fps)\n+        if num_frames > total_num_frames:\n+            raise ValueError(\n+                f\"When loading the video with fps={fps}, we identified that num_frames ({num_frames}) > total_frames ({total_num_frames}) .\"\n+                f\"Make sure that fps of a video is less than the requested fps for loading. Detected video_fps={video_fps}\"\n+            )\n     indices = get_uniform_frame_indices(total_num_frames, num_frames=num_frames)\n \n     frames = []\n@@ -644,15 +680,19 @@ def read_video_pyav(video_path: str, num_frames: Optional[int] = None):\n     return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n \n \n-def read_video_torchvision(video_path: str, num_frames: Optional[int] = None):\n+def read_video_torchvision(video_path: str, num_frames: Optional[int] = None, fps: Optional[int] = None):\n     \"\"\"\n     Decode the video with torchvision decoder.\n \n     Args:\n         video_path (`str`):\n             Path to the video file.\n         num_frames (`int`, *optional*):\n-            Number of frames to sample uniformly. If not specified, all frames are sampled.\n+            Number of frames to sample uniformly. Should be passed only when `fps=None`.\n+            If not specified and `fps==None`, all frames are sampled.\n+        fps (`int`, *optional*):\n+            Number of frames to sample per second. Should be passed only when `num_frames=None`.\n+            If not specified and `num_frames==None`, all frames are sampled.\n \n     Returns:\n         np.ndarray: np array of decoded frames of shape (num_frames, height, width, 3).\n@@ -664,6 +704,15 @@ def read_video_torchvision(video_path: str, num_frames: Optional[int] = None):\n         pts_unit=\"sec\",\n         output_format=\"TCHW\",\n     )\n+    video_fps = info[\"video_fps\"]\n+    total_num_frames = video.size(0) - 1\n+    if num_frames is None and fps is not None:\n+        num_frames = int(total_num_frames / video_fps * fps)\n+        if num_frames > total_num_frames:\n+            raise ValueError(\n+                f\"When loading the video with fps={fps}, we identified that num_frames ({num_frames}) > total_frames ({total_num_frames}) .\"\n+                f\"Make sure that fps of a video is less than the requested fps for loading. Detected video_fps={video_fps}\"\n+            )\n \n     if num_frames is not None:\n         idx = torch.linspace(0, video.size(0) - 1, num_frames, dtype=torch.int64)\n@@ -680,7 +729,12 @@ def read_video_torchvision(video_path: str, num_frames: Optional[int] = None):\n }\n \n \n-def load_video(video: Union[str, \"VideoInput\"], num_frames: Optional[int] = None, backend: str = \"opencv\") -> np.array:\n+def load_video(\n+    video: Union[str, \"VideoInput\"],\n+    num_frames: Optional[int] = None,\n+    fps: Optional[int] = None,\n+    backend: str = \"opencv\",\n+) -> np.array:\n     \"\"\"\n     Loads `video` to a numpy array.\n \n@@ -689,12 +743,19 @@ def load_video(video: Union[str, \"VideoInput\"], num_frames: Optional[int] = None\n             The video to convert to the numpy array format. Can be a link to video or local path.\n         num_frames (`int`, *optional*):\n             Number of frames to sample uniformly. If not passed, the whole video is loaded.\n+        fps (`int`, *optional*):\n+            Number of frames to sample per second. Should be passed only when `num_frames=None`.\n+            If not specified and `num_frames==None`, all frames are sampled.\n         backend (`str`, *optional*, defaults to `\"opencv\"`):\n             The backend to use when loading the video. Can be any of [\"decord\", \"pyav\", \"opencv\", \"torchvision\"]. Defaults to \"opencv\".\n \n     Returns:\n         `np.array`: A numpy array of shape (num_frames, channels, height, width).\n     \"\"\"\n+\n+    if fps is not None and num_frames is not None:\n+        raise ValueError(\"`num_frames` and `fps` are mutually exclusive arguments, please use only one!\")\n+\n     if video.startswith(\"https://www.youtube.com\") or video.startswith(\"http://www.youtube.com\"):\n         if not is_yt_dlp_available():\n             raise ImportError(\"To load a video from YouTube url you have  to install `yt_dlp` first.\")\n@@ -735,7 +796,7 @@ def load_video(video: Union[str, \"VideoInput\"], num_frames: Optional[int] = None\n         )\n \n     video_decoder = VIDEO_DECODERS[backend]\n-    video = video_decoder(file_obj)\n+    video = video_decoder(file_obj, num_frames=num_frames, fps=fps)\n     return video\n \n "
        },
        {
            "sha": "0a04d8117dd273bbde0a20c1826a63e318916e55",
            "filename": "src/transformers/models/aria/image_processing_aria.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eebd2c972c0f4c9dec9129092393bf295a4516d9/src%2Ftransformers%2Fmodels%2Faria%2Fimage_processing_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eebd2c972c0f4c9dec9129092393bf295a4516d9/src%2Ftransformers%2Fmodels%2Faria%2Fimage_processing_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fimage_processing_aria.py?ref=eebd2c972c0f4c9dec9129092393bf295a4516d9",
            "patch": "@@ -110,6 +110,8 @@ class AriaImageProcessor(BaseImageProcessor):\n             The resampling filter to use if resizing the image.\n     \"\"\"\n \n+    model_input_names = [\"pixel_values\", \"pixel_mask\", \"num_crops\"]\n+\n     def __init__(\n         self,\n         image_mean: List[float] = None,"
        },
        {
            "sha": "7d579d6e37f3f0da68a18215391eb4f812d6fe56",
            "filename": "src/transformers/models/aria/modular_aria.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/eebd2c972c0f4c9dec9129092393bf295a4516d9/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eebd2c972c0f4c9dec9129092393bf295a4516d9/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py?ref=eebd2c972c0f4c9dec9129092393bf295a4516d9",
            "patch": "@@ -499,6 +499,8 @@ class AriaImageProcessor(BaseImageProcessor):\n             The resampling filter to use if resizing the image.\n     \"\"\"\n \n+    model_input_names = [\"pixel_values\", \"pixel_mask\", \"num_crops\"]\n+\n     def __init__(\n         self,\n         image_mean: List[float] = None,\n@@ -997,6 +999,10 @@ def decode(self, *args, **kwargs):\n     def model_input_names(self):\n         tokenizer_input_names = self.tokenizer.model_input_names\n         image_processor_input_names = self.image_processor.model_input_names\n+\n+        # Remove `num_crops`, it is popped and used only when processing. Make a copy of list when remocing\n+        # otherwise `self.image_processor.model_input_names` is also modified\n+        image_processor_input_names = [name for name in image_processor_input_names if name != \"num_crops\"]\n         return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n \n "
        },
        {
            "sha": "4b7163db8fd342b4c35606dd92e66311e3de2d86",
            "filename": "src/transformers/models/aria/processing_aria.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/eebd2c972c0f4c9dec9129092393bf295a4516d9/src%2Ftransformers%2Fmodels%2Faria%2Fprocessing_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eebd2c972c0f4c9dec9129092393bf295a4516d9/src%2Ftransformers%2Fmodels%2Faria%2Fprocessing_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fprocessing_aria.py?ref=eebd2c972c0f4c9dec9129092393bf295a4516d9",
            "patch": "@@ -158,6 +158,10 @@ def decode(self, *args, **kwargs):\n     def model_input_names(self):\n         tokenizer_input_names = self.tokenizer.model_input_names\n         image_processor_input_names = self.image_processor.model_input_names\n+\n+        # Remove `num_crops`, it is popped and used only when processing. Make a copy of list when remocing\n+        # otherwise `self.image_processor.model_input_names` is also modified\n+        image_processor_input_names = [name for name in image_processor_input_names if name != \"num_crops\"]\n         return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n \n "
        },
        {
            "sha": "1cc02f58ddceebb1e463f16d90e921e8e814caf3",
            "filename": "src/transformers/models/emu3/image_processing_emu3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eebd2c972c0f4c9dec9129092393bf295a4516d9/src%2Ftransformers%2Fmodels%2Femu3%2Fimage_processing_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eebd2c972c0f4c9dec9129092393bf295a4516d9/src%2Ftransformers%2Fmodels%2Femu3%2Fimage_processing_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fimage_processing_emu3.py?ref=eebd2c972c0f4c9dec9129092393bf295a4516d9",
            "patch": "@@ -132,7 +132,7 @@ class Emu3ImageProcessor(BaseImageProcessor):\n             The spatial downsample factor the image will be downsampled in feature extracting phase\n     \"\"\"\n \n-    model_input_names = [\"pixel_values\"]\n+    model_input_names = [\"pixel_values\", \"image_sizes\"]\n \n     def __init__(\n         self,"
        },
        {
            "sha": "01966e470bdf348b8ddfaca4cb3af25832293cce",
            "filename": "src/transformers/models/emu3/processing_emu3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/eebd2c972c0f4c9dec9129092393bf295a4516d9/src%2Ftransformers%2Fmodels%2Femu3%2Fprocessing_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eebd2c972c0f4c9dec9129092393bf295a4516d9/src%2Ftransformers%2Fmodels%2Femu3%2Fprocessing_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fprocessing_emu3.py?ref=eebd2c972c0f4c9dec9129092393bf295a4516d9",
            "patch": "@@ -63,6 +63,7 @@ class Emu3Processor(ProcessorMixin):\n     \"\"\"\n \n     attributes = [\"image_processor\", \"tokenizer\"]\n+    valid_kwargs = [\"chat_template\"]\n     tokenizer_class = (\"GPT2Tokenizer\", \"GPT2TokenizerFast\")\n     image_processor_class = \"Emu3ImageProcessor\"\n \n@@ -179,7 +180,7 @@ def __call__(\n         data = self.tokenizer(text, **output_kwargs[\"text_kwargs\"])\n         data.update(**image_features)\n \n-        return BatchFeature(data=data, tensor_type=output_kwargs[\"common_kwargs\"][\"return_tensors\"])\n+        return BatchFeature(data=data, tensor_type=output_kwargs[\"common_kwargs\"].pop(\"return_tensors\", None))\n \n     def calculate_generate_size(self, ratio, image_area, spatial_factor):\n         width, height = map(int, ratio.split(\":\"))"
        },
        {
            "sha": "2a853fc02e7a547db3ce8f9eb70a016278779b6a",
            "filename": "src/transformers/models/idefics2/image_processing_idefics2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eebd2c972c0f4c9dec9129092393bf295a4516d9/src%2Ftransformers%2Fmodels%2Fidefics2%2Fimage_processing_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eebd2c972c0f4c9dec9129092393bf295a4516d9/src%2Ftransformers%2Fmodels%2Fidefics2%2Fimage_processing_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fimage_processing_idefics2.py?ref=eebd2c972c0f4c9dec9129092393bf295a4516d9",
            "patch": "@@ -184,7 +184,7 @@ class Idefics2ImageProcessor(BaseImageProcessor):\n             strategy was first introduced in https://arxiv.org/abs/2311.06607.\n     \"\"\"\n \n-    model_input_names = [\"pixel_values\"]\n+    model_input_names = [\"pixel_values\", \"pixel_attention_mask\"]\n \n     def __init__(\n         self,"
        },
        {
            "sha": "bf21f06e8c5ae15d9b3d47c93f1706ded8b302ba",
            "filename": "src/transformers/models/idefics3/image_processing_idefics3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eebd2c972c0f4c9dec9129092393bf295a4516d9/src%2Ftransformers%2Fmodels%2Fidefics3%2Fimage_processing_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eebd2c972c0f4c9dec9129092393bf295a4516d9/src%2Ftransformers%2Fmodels%2Fidefics3%2Fimage_processing_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fimage_processing_idefics3.py?ref=eebd2c972c0f4c9dec9129092393bf295a4516d9",
            "patch": "@@ -289,7 +289,7 @@ class Idefics3ImageProcessor(BaseImageProcessor):\n             sample in the batch, such that the returned tensor is of shape (batch_size, max_num_images, num_channels, max_height, max_width).\n     \"\"\"\n \n-    model_input_names = [\"pixel_values\"]\n+    model_input_names = [\"pixel_values\", \"pixel_attention_mask\"]\n \n     def __init__(\n         self,"
        },
        {
            "sha": "6501fca6b68485b386e36f8b1b82cd5e88073d01",
            "filename": "src/transformers/models/idefics3/processing_idefics3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/eebd2c972c0f4c9dec9129092393bf295a4516d9/src%2Ftransformers%2Fmodels%2Fidefics3%2Fprocessing_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eebd2c972c0f4c9dec9129092393bf295a4516d9/src%2Ftransformers%2Fmodels%2Fidefics3%2Fprocessing_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fprocessing_idefics3.py?ref=eebd2c972c0f4c9dec9129092393bf295a4516d9",
            "patch": "@@ -129,6 +129,7 @@ class Idefics3Processor(ProcessorMixin):\n     \"\"\"\n \n     attributes = [\"image_processor\", \"tokenizer\"]\n+    valid_kwargs = [\"image_seq_len\", \"chat_template\"]\n     image_processor_class = \"Idefics3ImageProcessor\"\n     tokenizer_class = \"AutoTokenizer\"\n \n@@ -354,7 +355,7 @@ def decode(self, *args, **kwargs):\n     def model_input_names(self):\n         tokenizer_input_names = self.tokenizer.model_input_names\n         image_processor_input_names = self.image_processor.model_input_names\n-        return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n+        return list(dict.fromkeys(image_processor_input_names + tokenizer_input_names))\n \n \n __all__ = [\"Idefics3Processor\"]"
        },
        {
            "sha": "d338775df644e601feda6ab61f7dc2695f2f9a25",
            "filename": "src/transformers/models/llava_next/image_processing_llava_next.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eebd2c972c0f4c9dec9129092393bf295a4516d9/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eebd2c972c0f4c9dec9129092393bf295a4516d9/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next.py?ref=eebd2c972c0f4c9dec9129092393bf295a4516d9",
            "patch": "@@ -163,7 +163,7 @@ class LlavaNextImageProcessor(BaseImageProcessor):\n             Whether to convert the image to RGB.\n     \"\"\"\n \n-    model_input_names = [\"pixel_values\"]\n+    model_input_names = [\"pixel_values\", \"image_sizes\"]\n \n     def __init__(\n         self,"
        },
        {
            "sha": "5ed398e928faebc7457a6d68cbef734aaccaf550",
            "filename": "src/transformers/models/mllama/processing_mllama.py",
            "status": "modified",
            "additions": 9,
            "deletions": 3,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/eebd2c972c0f4c9dec9129092393bf295a4516d9/src%2Ftransformers%2Fmodels%2Fmllama%2Fprocessing_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eebd2c972c0f4c9dec9129092393bf295a4516d9/src%2Ftransformers%2Fmodels%2Fmllama%2Fprocessing_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fprocessing_mllama.py?ref=eebd2c972c0f4c9dec9129092393bf295a4516d9",
            "patch": "@@ -202,14 +202,17 @@ class MllamaProcessor(ProcessorMixin):\n             The image processor is a required input.\n         tokenizer ([`PreTrainedTokenizer`, `PreTrainedTokenizerFast`]):\n             The tokenizer is a required input.\n+        chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n+            in a chat into a tokenizable string.\n \n     \"\"\"\n \n     attributes = [\"image_processor\", \"tokenizer\"]\n+    valid_kwargs = [\"chat_template\"]\n     image_processor_class = \"MllamaImageProcessor\"\n     tokenizer_class = \"PreTrainedTokenizerFast\"\n \n-    def __init__(self, image_processor, tokenizer):\n+    def __init__(self, image_processor, tokenizer, chat_template=None):\n         if not hasattr(tokenizer, \"image_token\"):\n             self.image_token = \"<|image|>\"\n             self.image_token_id = tokenizer.convert_tokens_to_ids(self.image_token)\n@@ -220,8 +223,7 @@ def __init__(self, image_processor, tokenizer):\n         self.python_token = \"<|python_tag|>\"\n         self.python_token_id = tokenizer.convert_tokens_to_ids(self.python_token)\n         self.bos_token = tokenizer.bos_token\n-        self.chat_template = tokenizer.chat_template\n-        super().__init__(image_processor, tokenizer)\n+        super().__init__(image_processor, tokenizer, chat_template=chat_template)\n \n     def __call__(\n         self,\n@@ -364,6 +366,10 @@ def post_process_image_text_to_text(self, generated_outputs):\n     def model_input_names(self):\n         tokenizer_input_names = self.tokenizer.model_input_names\n         image_processor_input_names = self.image_processor.model_input_names\n+\n+        # Remove `num_tiles`, it is popped and used only when processing. Make a copy of list when remocing\n+        # otherwise `self.image_processor.model_input_names` is also modified\n+        image_processor_input_names = [name for name in image_processor_input_names if name != \"num_tiles\"]\n         return list(tokenizer_input_names + image_processor_input_names + [\"cross_attention_mask\"])\n \n "
        },
        {
            "sha": "f9a0c4c5e8c9aa3dd8b1ffc2067579114d89214a",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 68,
            "deletions": 28,
            "changes": 96,
            "blob_url": "https://github.com/huggingface/transformers/blob/eebd2c972c0f4c9dec9129092393bf295a4516d9/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eebd2c972c0f4c9dec9129092393bf295a4516d9/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=eebd2c972c0f4c9dec9129092393bf295a4516d9",
            "patch": "@@ -379,6 +379,9 @@ class ChatTemplateKwargs(TypedDict, total=False):\n         The backend to use when loading the video which will be used only when there are videos in the conversation.\n         Can be any of [\"decord\", \"pyav\", \"opencv\", \"torchvision\"]. Defaults to \"pyav\" because it is the only backend\n         that supports all types of sources to load from.\n+    video_fps (`int`, *optional*):\n+        Number of frames to sample per second. Should be passed only when `num_frames=None`.\n+        If not specified and `num_frames==None`, all frames are sampled.\n     \"\"\"\n \n     tokenize: Optional[bool] = False\n@@ -390,6 +393,7 @@ class ChatTemplateKwargs(TypedDict, total=False):\n     return_assistant_tokens_mask: Optional[bool] = False\n     num_frames: Optional[int] = None\n     video_load_backend: Optional[str] = \"pyav\"\n+    video_fps: Optional[int] = None\n \n \n class AllKwargsForChatTemplate(\n@@ -762,7 +766,11 @@ def get_processor_dict(\n         # (`cached_file` called using `_raise_exceptions_for_missing_entries=False` to avoid exception)\n         # However, for models added in the future, we won't get the expected error if this file is missing.\n         if resolved_processor_file is None:\n-            return {}, kwargs\n+            # In any case we need to pass `chat_template` if it is available\n+            processor_dict = {}\n+            if \"chat_template\" in kwargs:\n+                processor_dict = {\"chat_template\": kwargs.pop(\"chat_template\")}\n+            return processor_dict, kwargs\n \n         try:\n             # Load processor dict\n@@ -786,6 +794,9 @@ def get_processor_dict(\n                 \"in the processor's config. Make sure to move your template to its own file.\"\n             )\n \n+        if \"chat_template\" in kwargs:\n+            processor_dict[\"chat_template\"] = kwargs.pop(\"chat_template\")\n+\n         if not is_local:\n             if \"auto_map\" in processor_dict:\n                 processor_dict[\"auto_map\"] = add_model_info_to_auto_map(\n@@ -817,7 +828,6 @@ def from_args_and_dict(cls, args, processor_dict: Dict[str, Any], **kwargs):\n         \"\"\"\n         processor_dict = processor_dict.copy()\n         return_unused_kwargs = kwargs.pop(\"return_unused_kwargs\", False)\n-        chat_template = kwargs.pop(\"chat_template\", None)\n \n         # We have to pop up some unused (but specific) kwargs and then validate that it doesn't contain unused kwargs\n         # If we don't pop, some specific kwargs will raise a warning\n@@ -829,8 +839,6 @@ def from_args_and_dict(cls, args, processor_dict: Dict[str, Any], **kwargs):\n \n         unused_kwargs = cls.validate_init_kwargs(processor_config=processor_dict, valid_kwargs=cls.valid_kwargs)\n         processor = cls(*args, **processor_dict)\n-        if chat_template is not None:\n-            setattr(processor, \"chat_template\", chat_template)\n \n         # Update processor with kwargs if needed\n         for key in set(kwargs.keys()):\n@@ -1199,12 +1207,6 @@ def apply_chat_template(\n                     \"https://huggingface.co/docs/transformers/main/en/chat_templating for more information.\"\n                 )\n \n-        text_kwargs = {}\n-        for key in TextKwargs.__annotations__.keys():\n-            value = kwargs.pop(key, None)\n-            if value is not None:\n-                text_kwargs[key] = value\n-\n         chat_template_kwargs = {}\n         for key in ChatTemplateKwargs.__annotations__.keys():\n             value = kwargs.pop(key, getattr(ChatTemplateKwargs, key))\n@@ -1214,38 +1216,76 @@ def apply_chat_template(\n         tokenize = chat_template_kwargs.pop(\"tokenize\")\n         return_dict = chat_template_kwargs.pop(\"return_dict\")\n         num_frames = chat_template_kwargs.pop(\"num_frames\")\n+        video_fps = chat_template_kwargs.pop(\"video_fps\")\n         video_load_backend = chat_template_kwargs.pop(\"video_load_backend\")\n \n         prompt = self.tokenizer.apply_chat_template(\n             conversation,\n             chat_template=chat_template,\n             tokenize=False,\n             return_dict=False,\n-            **text_kwargs,\n             **chat_template_kwargs,\n         )\n \n-        # we will have to return all processed inputs in a dict\n+        if isinstance(conversation, (list, tuple)) and (\n+            isinstance(conversation[0], (list, tuple)) or hasattr(conversation[0], \"content\")\n+        ):\n+            conversations = conversation\n+            is_batched = True\n+        else:\n+            conversations = [conversation]\n+            is_batched = False\n+\n         if tokenize:\n-            images, videos = [], []\n-            for message in conversation:\n-                visuals = [content for content in message[\"content\"] if content[\"type\"] in [\"image\", \"video\"]]\n-                for vision_info in visuals:\n-                    if vision_info[\"type\"] == \"image\":\n-                        for key in [\"image\", \"url\", \"path\", \"base64\"]:\n-                            if key in vision_info:\n-                                images.append(load_image(vision_info[key]))\n-                    elif vision_info[\"type\"] == \"video\":\n-                        for key in [\"video\", \"url\", \"path\"]:\n-                            if key in vision_info:\n-                                videos.append(\n-                                    load_video(vision_info[key], num_frames=num_frames, backend=video_load_backend)\n-                                )\n+            batch_images, batch_videos = [], []\n+            for conversation in conversations:\n+                images, videos = [], []\n+                for message in conversation:\n+                    visuals = [content for content in message[\"content\"] if content[\"type\"] in [\"image\", \"video\"]]\n+                    image_fnames = [\n+                        vision_info[key]\n+                        for vision_info in visuals\n+                        for key in [\"image\", \"url\", \"path\", \"base64\"]\n+                        if key in vision_info and vision_info[\"type\"] == \"image\"\n+                    ]\n+                    video_fnames = [\n+                        vision_info[key]\n+                        for vision_info in visuals\n+                        for key in [\"video\", \"url\", \"path\"]\n+                        if key in vision_info and vision_info[\"type\"] == \"video\"\n+                    ]\n+                    for fname in image_fnames:\n+                        images.append(load_image(fname))\n+                    for fname in video_fnames:\n+                        if isinstance(fname, (list, tuple)) and isinstance(fname[0], str):\n+                            video = [np.array(load_image(image_fname)).T for image_fname in fname]\n+                            # create a 4D video because `load_video` always returns a 4D array\n+                            video = np.stack(video)\n+                        else:\n+                            video = load_video(fname, num_frames=num_frames, fps=video_fps, backend=video_load_backend)\n+                        videos.append(video)\n+\n+                # Currently all processors can accept accept nested list of batches, but not flat list of visuals\n+                # So we'll make a batched list of images and let the processor handle it\n+                if images:\n+                    batch_images.append(images)\n+                if videos:\n+                    batch_videos.append(videos)\n+\n+            # Tokenizer's `apply_chat_template` never adds special tokens when tokenizing\n+            # But processor's `apply_chat_template` didn't have an option to tokenize, so users had to format the prompt\n+            # and pass it to the processor. Users thus never worried about special tokens relying on processor hadnling\n+            # everything internally. The below line is to keep BC for that and be able to work with model that have\n+            # special tokens in the template (consistent with tokenizers). We dont want to raise warning, it will flood command line\n+            # without actionable solution for users\n+            single_prompt = prompt[0] if is_batched else prompt\n+            if self.tokenizer.bos_token is not None and single_prompt.startswith(self.tokenizer.bos_token):\n+                kwargs[\"add_special_tokens\"] = False\n \n             out = self(\n                 text=prompt,\n-                images=images if images else None,\n-                videos=videos if videos else None,\n+                images=batch_images if batch_images else None,\n+                videos=batch_videos if batch_videos else None,\n                 **kwargs,\n             )\n             if return_dict:"
        },
        {
            "sha": "623153b6798adf06456b03ca27b6c1f9ffd78e10",
            "filename": "tests/models/aria/test_processor_aria.py",
            "status": "modified",
            "additions": 49,
            "deletions": 0,
            "changes": 49,
            "blob_url": "https://github.com/huggingface/transformers/blob/eebd2c972c0f4c9dec9129092393bf295a4516d9/tests%2Fmodels%2Faria%2Ftest_processor_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eebd2c972c0f4c9dec9129092393bf295a4516d9/tests%2Fmodels%2Faria%2Ftest_processor_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faria%2Ftest_processor_aria.py?ref=eebd2c972c0f4c9dec9129092393bf295a4516d9",
            "patch": "@@ -237,6 +237,55 @@ def test_apply_chat_template(self):\n \"\"\"\n         self.assertEqual(rendered, expected_rendered)\n \n+    # Override as AriaImageProcessor doesn't accept `do_rescale`\n+    def test_chat_template_accepts_processing_kwargs(self):\n+        processor = self.get_processor()\n+        if processor.chat_template is None:\n+            self.skipTest(\"Processor has no chat template\")\n+\n+        messages = [\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n+                    ],\n+                },\n+            ]\n+        ]\n+\n+        formatted_prompt_tokenized = processor.apply_chat_template(\n+            messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            padding=\"max_length\",\n+            max_length=50,\n+        )\n+        self.assertEqual(len(formatted_prompt_tokenized[0]), 50)\n+\n+        formatted_prompt_tokenized = processor.apply_chat_template(\n+            messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            truncation=True,\n+            max_length=5,\n+        )\n+        self.assertEqual(len(formatted_prompt_tokenized[0]), 5)\n+\n+        # Now test the ability to return dict\n+        messages[0][0][\"content\"].append(\n+            {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"}\n+        )\n+        out_dict = processor.apply_chat_template(\n+            messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+            max_image_size=980,\n+            return_tensors=\"np\",\n+        )\n+        self.assertListEqual(list(out_dict[self.images_input_name].shape), [1, 3, 980, 980])\n+\n     # Override as AriaProcessor needs image tokens in prompts\n     def prepare_text_inputs(self, batch_size: Optional[int] = None):\n         if batch_size is None:"
        },
        {
            "sha": "c7b792c2a391a7898ff4998862e6e82ad3e4d4a1",
            "filename": "tests/models/emu3/test_processor_emu3.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/eebd2c972c0f4c9dec9129092393bf295a4516d9/tests%2Fmodels%2Femu3%2Ftest_processor_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eebd2c972c0f4c9dec9129092393bf295a4516d9/tests%2Fmodels%2Femu3%2Ftest_processor_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Femu3%2Ftest_processor_emu3.py?ref=eebd2c972c0f4c9dec9129092393bf295a4516d9",
            "patch": "@@ -52,6 +52,11 @@ def setUp(self):\n         )\n         processor.save_pretrained(self.tmpdirname)\n \n+    def prepare_processor_dict(self):\n+        return {\n+            \"chat_template\": \"{% for message in messages %}{% if message['role'] != 'system' %}{{ message['role'].upper() + ': '}}{% endif %}{# Render all images first #}{% for content in message['content'] | selectattr('type', 'equalto', 'image') %}{{ '<image>' }}{% endfor %}{# Render all text next #}{% if message['role'] != 'assistant' %}{% for content in message['content'] | selectattr('type', 'equalto', 'text') %}{{ content['text'] + ' '}}{% endfor %}{% else %}{% for content in message['content'] | selectattr('type', 'equalto', 'text') %}{% generation %}{{ content['text'] + ' '}}{% endgeneration %}{% endfor %}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ 'ASSISTANT:' }}{% endif %}\",\n+        }  # fmt: skip\n+\n     def test_processor_for_generation(self):\n         processor_components = self.prepare_components()\n         processor = self.processor_class(**processor_components)"
        },
        {
            "sha": "a411430a1e8af830cabd94d4b57a467ec29bb382",
            "filename": "tests/models/llava/test_processor_llava.py",
            "status": "modified",
            "additions": 7,
            "deletions": 26,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/eebd2c972c0f4c9dec9129092393bf295a4516d9/tests%2Fmodels%2Fllava%2Ftest_processor_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eebd2c972c0f4c9dec9129092393bf295a4516d9/tests%2Fmodels%2Fllava%2Ftest_processor_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava%2Ftest_processor_llava.py?ref=eebd2c972c0f4c9dec9129092393bf295a4516d9",
            "patch": "@@ -17,7 +17,7 @@\n import unittest\n \n from transformers import AutoProcessor, AutoTokenizer, LlamaTokenizerFast, LlavaProcessor\n-from transformers.testing_utils import require_torch, require_vision\n+from transformers.testing_utils import require_vision\n from transformers.utils import is_torch_available, is_vision_available\n \n from ...test_processing_common import ProcessorTesterMixin\n@@ -27,7 +27,7 @@\n     from transformers import CLIPImageProcessor\n \n if is_torch_available:\n-    import torch\n+    pass\n \n \n @require_vision\n@@ -53,7 +53,11 @@ def tearDown(self):\n         shutil.rmtree(self.tmpdirname)\n \n     def prepare_processor_dict(self):\n-        return {\"chat_template\": \"dummy_template\", \"patch_size\": 3, \"vision_feature_select_strategy\": \"default\"}\n+        return {\n+            \"chat_template\": \"{% for message in messages %}{% if message['role'] != 'system' %}{{ message['role'].upper() + ': '}}{% endif %}{# Render all images first #}{% for content in message['content'] | selectattr('type', 'equalto', 'image') %}{{ '<image>\\n' }}{% endfor %}{# Render all text next #}{% if message['role'] != 'assistant' %}{% for content in message['content'] | selectattr('type', 'equalto', 'text') %}{{ content['text'] + ' '}}{% endfor %}{% else %}{% for content in message['content'] | selectattr('type', 'equalto', 'text') %}{% generation %}{{ content['text'] + ' '}}{% endgeneration %}{% endfor %}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ 'ASSISTANT:' }}{% endif %}\",\n+            \"patch_size\": 3,\n+            \"vision_feature_select_strategy\": \"default\"\n+        }  # fmt: skip\n \n     @unittest.skip(\n         \"Skip because the model has no processor kwargs except for chat template and\"\n@@ -123,29 +127,6 @@ def test_chat_template_dict(self):\n         )\n         self.assertListEqual(list(out_dict_with_image.keys()), [\"input_ids\", \"attention_mask\", \"pixel_values\"])\n \n-    @require_torch\n-    def test_chat_template_dict_torch(self):\n-        processor = LlavaProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n-        messages = [\n-            {\n-                \"role\": \"user\",\n-                \"content\": [\n-                    {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"},\n-                    {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n-                ],\n-            },\n-        ]\n-\n-        out_dict_tensors = processor.apply_chat_template(\n-            messages,\n-            add_generation_prompt=True,\n-            tokenize=True,\n-            return_dict=True,\n-            return_tensors=\"pt\",\n-        )\n-        self.assertListEqual(list(out_dict_tensors.keys()), [\"input_ids\", \"attention_mask\", \"pixel_values\"])\n-        self.assertTrue(isinstance(out_dict_tensors[\"input_ids\"], torch.Tensor))\n-\n     def test_chat_template_with_continue_final_message(self):\n         processor = LlavaProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n         expected_prompt = \"USER: <image>\\nDescribe this image. ASSISTANT: There is a dog and\""
        },
        {
            "sha": "af14578411636149cac8f30f834a23ec5eafd994",
            "filename": "tests/models/llava_next/test_processor_llava_next.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/eebd2c972c0f4c9dec9129092393bf295a4516d9/tests%2Fmodels%2Fllava_next%2Ftest_processor_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eebd2c972c0f4c9dec9129092393bf295a4516d9/tests%2Fmodels%2Fllava_next%2Ftest_processor_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next%2Ftest_processor_llava_next.py?ref=eebd2c972c0f4c9dec9129092393bf295a4516d9",
            "patch": "@@ -50,7 +50,11 @@ def get_image_processor(self, **kwargs):\n         return LlavaNextProcessor.from_pretrained(self.tmpdirname, **kwargs).image_processor\n \n     def prepare_processor_dict(self):\n-        return {\"chat_template\": \"dummy_template\", \"patch_size\": 3, \"vision_feature_select_strategy\": \"default\"}\n+        return {\n+            \"chat_template\": \"{% for message in messages %}{% if message['role'] != 'system' %}{{ message['role'].upper() + ': '}}{% endif %}{# Render all images first #}{% for content in message['content'] | selectattr('type', 'equalto', 'image') %}{{ '<image>\\n' }}{% endfor %}{# Render all text next #}{% if message['role'] != 'assistant' %}{% for content in message['content'] | selectattr('type', 'equalto', 'text') %}{{ content['text'] + ' '}}{% endfor %}{% else %}{% for content in message['content'] | selectattr('type', 'equalto', 'text') %}{% generation %}{{ content['text'] + ' '}}{% endgeneration %}{% endfor %}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ 'ASSISTANT:' }}{% endif %}\",\n+            \"patch_size\": 3,\n+            \"vision_feature_select_strategy\": \"default\"\n+        }  # fmt: skip\n \n     @unittest.skip(\n         \"Skip because the model has no processor kwargs except for chat template and\""
        },
        {
            "sha": "3f5db8c9c3ac98fc9a9aa4461ba8228b49e99bff",
            "filename": "tests/models/llava_onevision/test_processor_llava_onevision.py",
            "status": "modified",
            "additions": 7,
            "deletions": 30,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/eebd2c972c0f4c9dec9129092393bf295a4516d9/tests%2Fmodels%2Fllava_onevision%2Ftest_processor_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eebd2c972c0f4c9dec9129092393bf295a4516d9/tests%2Fmodels%2Fllava_onevision%2Ftest_processor_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_onevision%2Ftest_processor_llava_onevision.py?ref=eebd2c972c0f4c9dec9129092393bf295a4516d9",
            "patch": "@@ -16,7 +16,7 @@\n import tempfile\n import unittest\n \n-from transformers.testing_utils import require_av, require_torch, require_vision\n+from transformers.testing_utils import require_av, require_vision\n from transformers.utils import is_torch_available, is_vision_available\n \n from ...test_processing_common import ProcessorTesterMixin\n@@ -32,7 +32,7 @@\n     )\n \n if is_torch_available:\n-    import torch\n+    pass\n \n \n @require_vision\n@@ -61,7 +61,11 @@ def get_video_processor(self, **kwargs):\n         return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).video_processor\n \n     def prepare_processor_dict(self):\n-        return {\"chat_template\": \"dummy_template\", \"num_image_tokens\": 6, \"vision_feature_select_strategy\": \"default\"}\n+        return {\n+            \"chat_template\": \"{% for message in messages %}{{'<|im_start|>' + message['role'] + ' '}}{# Render all images first #}{% for content in message['content'] | selectattr('type', 'equalto', 'image') %}{{ '<image>' }}{% endfor %}{# Render all video then #}{% for content in message['content'] | selectattr('type', 'equalto', 'video') %}{{ '<video>' }}{% endfor %}{# Render all text next #}{% if message['role'] != 'assistant' %}{% for content in message['content'] | selectattr('type', 'equalto', 'text') %}{{ '\\n' + content['text'] }}{% endfor %}{% else %}{% for content in message['content'] | selectattr('type', 'equalto', 'text') %}{% generation %}{{ '\\n' + content['text'] }}{% endgeneration %}{% endfor %}{% endif %}{{'<|im_end|>'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\",\n+            \"num_image_tokens\": 6,\n+            \"vision_feature_select_strategy\": \"default\"\n+        }  # fmt: skip\n \n     def test_processor_to_json_string(self):\n         processor = self.get_processor()\n@@ -133,30 +137,3 @@ def test_chat_template_dict(self):\n             messages, add_generation_prompt=True, tokenize=True, return_dict=True\n         )\n         self.assertListEqual(list(out_dict_with_video.keys()), [\"input_ids\", \"attention_mask\", \"pixel_values_videos\"])\n-\n-    @require_torch\n-    @require_av\n-    def test_chat_template_dict_torch(self):\n-        processor = AutoProcessor.from_pretrained(\"llava-hf/llava-onevision-qwen2-7b-ov-hf\")\n-        messages = [\n-            {\n-                \"role\": \"user\",\n-                \"content\": [\n-                    {\n-                        \"type\": \"video\",\n-                        \"url\": \"https://test-videos.co.uk/vids/bigbuckbunny/mp4/h264/720/Big_Buck_Bunny_720_10s_10MB.mp4\",\n-                    },\n-                    {\"type\": \"text\", \"text\": \"What is shown in this video?\"},\n-                ],\n-            },\n-        ]\n-\n-        out_dict_tensors = processor.apply_chat_template(\n-            messages,\n-            add_generation_prompt=True,\n-            tokenize=True,\n-            return_dict=True,\n-            return_tensors=\"pt\",\n-        )\n-        self.assertListEqual(list(out_dict_tensors.keys()), [\"input_ids\", \"attention_mask\", \"pixel_values_videos\"])\n-        self.assertTrue(isinstance(out_dict_tensors[\"input_ids\"], torch.Tensor))"
        },
        {
            "sha": "6d5db2f677cfddd5bae48ca5a72a62dc06f91000",
            "filename": "tests/models/mllama/test_processor_mllama.py",
            "status": "modified",
            "additions": 15,
            "deletions": 0,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/eebd2c972c0f4c9dec9129092393bf295a4516d9/tests%2Fmodels%2Fmllama%2Ftest_processor_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eebd2c972c0f4c9dec9129092393bf295a4516d9/tests%2Fmodels%2Fmllama%2Ftest_processor_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmllama%2Ftest_processor_mllama.py?ref=eebd2c972c0f4c9dec9129092393bf295a4516d9",
            "patch": "@@ -13,6 +13,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n+import json\n import shutil\n import tempfile\n import unittest\n@@ -52,6 +53,20 @@ def setUp(self):\n     def tearDown(self):\n         shutil.rmtree(self.tmpdirname)\n \n+    def prepare_processor_dict(self):\n+        return {\"chat_template\": \"{% for message in messages %}{% if loop.index0 == 0 %}{{ bos_token }}{% endif %}{{ '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' }}{% if message['content'] is string %}{{ message['content'] }}{% else %}{% for content in message['content'] %}{% if content['type'] == 'image' %}{{ '<|image|>' }}{% elif content['type'] == 'text' %}{{ content['text'] }}{% endif %}{% endfor %}{% endif %}{{ '<|eot_id|>' }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\"}  # fmt: skip\n+\n+    def test_chat_template_is_saved(self):\n+        processor_loaded = self.processor_class.from_pretrained(self.tmpdirname)\n+        processor_dict_loaded = json.loads(processor_loaded.to_json_string())\n+        # chat templates aren't serialized to json in processors\n+        self.assertFalse(\"chat_template\" in processor_dict_loaded.keys())\n+\n+        # they have to be saved as separate file and loaded back from that file\n+        # so we check if the same template is loaded\n+        processor_dict = self.prepare_processor_dict()\n+        self.assertTrue(processor_loaded.chat_template == processor_dict.get(\"chat_template\", None))\n+\n     def test_apply_chat_template(self):\n         # Message contains content which a mix of lists with images and image urls and string\n         messages = ["
        },
        {
            "sha": "c85389c07306bbb9b30c3b4d1ee6694978c58590",
            "filename": "tests/models/qwen2_5_vl/test_processor_qwen2_5_vl.py",
            "status": "modified",
            "additions": 200,
            "deletions": 1,
            "changes": 201,
            "blob_url": "https://github.com/huggingface/transformers/blob/eebd2c972c0f4c9dec9129092393bf295a4516d9/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_processor_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eebd2c972c0f4c9dec9129092393bf295a4516d9/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_processor_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_processor_qwen2_5_vl.py?ref=eebd2c972c0f4c9dec9129092393bf295a4516d9",
            "patch": "@@ -12,14 +12,15 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n+import inspect\n import shutil\n import tempfile\n import unittest\n \n import pytest\n \n from transformers import AutoProcessor, Qwen2Tokenizer\n-from transformers.testing_utils import require_torch, require_vision\n+from transformers.testing_utils import require_av, require_torch, require_vision\n from transformers.utils import is_vision_available\n \n from ...test_processing_common import ProcessorTesterMixin\n@@ -45,6 +46,9 @@ def get_tokenizer(self, **kwargs):\n     def get_image_processor(self, **kwargs):\n         return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).image_processor\n \n+    def prepare_processor_dict(self):\n+        return {\"chat_template\": \"{% set image_count = namespace(value=0) %}{% set video_count = namespace(value=0) %}{% for message in messages %}{% if loop.first and message['role'] != 'system' %}<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n{% endif %}<|im_start|>{{ message['role'] }}\\n{% if message['content'] is string %}{{ message['content'] }}<|im_end|>\\n{% else %}{% for content in message['content'] %}{% if content['type'] == 'image' or 'image' in content or 'image_url' in content %}{% set image_count.value = image_count.value + 1 %}{% if add_vision_id %}Picture {{ image_count.value }}: {% endif %}<|vision_start|><|image_pad|><|vision_end|>{% elif content['type'] == 'video' or 'video' in content %}{% set video_count.value = video_count.value + 1 %}{% if add_vision_id %}Video {{ video_count.value }}: {% endif %}<|vision_start|><|video_pad|><|vision_end|>{% elif 'text' in content %}{{ content['text'] }}{% endif %}{% endfor %}<|im_end|>\\n{% endif %}{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\\n{% endif %}\"}  # fmt: skip\n+\n     def tearDown(self):\n         shutil.rmtree(self.tmpdirname)\n \n@@ -111,3 +115,198 @@ def test_model_input_names(self):\n         inputs = processor(text=input_str, images=image_input, videos=video_inputs)\n \n         self.assertListEqual(list(inputs.keys()), processor.model_input_names)\n+\n+    def test_chat_template_single(self):\n+        processor = self.get_processor()\n+        if processor.chat_template is None:\n+            self.skipTest(\"Processor has no chat template\")\n+\n+        messages = [\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n+                    ],\n+                },\n+            ]\n+        ]\n+\n+        formatted_prompt = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n+        self.assertEqual(len(formatted_prompt), 1)\n+\n+        formatted_prompt_tokenized = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True)\n+        expected_output = processor.tokenizer(formatted_prompt, return_tensors=None).input_ids\n+        self.assertListEqual(expected_output, formatted_prompt_tokenized)\n+\n+        out_dict = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True)\n+        self.assertListEqual(list(out_dict.keys()), [\"input_ids\", \"attention_mask\"])\n+\n+        # Now test the ability to return dict\n+        messages[0][0][\"content\"].append(\n+            {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"}\n+        )\n+        out_dict = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True)\n+        self.assertTrue(self.images_input_name in out_dict)\n+\n+        # should always have input_ids and attention_mask\n+        self.assertEqual(len(out_dict[\"input_ids\"]), 1)\n+        self.assertEqual(len(out_dict[\"attention_mask\"]), 1)\n+        self.assertEqual(len(out_dict[self.images_input_name]), 71280)\n+\n+    def test_chat_template_batched(self):\n+        processor = self.get_processor()\n+        if processor.chat_template is None:\n+            self.skipTest(\"Processor has no chat template\")\n+\n+        batched_messages = [\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n+                    ],\n+                },\n+            ],\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\"type\": \"text\", \"text\": \"What do you see?\"},\n+                    ],\n+                },\n+            ],\n+        ]\n+\n+        formatted_prompt = processor.apply_chat_template(batched_messages, add_generation_prompt=True, tokenize=False)\n+        self.assertEqual(len(formatted_prompt), 2)\n+\n+        formatted_prompt_tokenized = processor.apply_chat_template(\n+            batched_messages, add_generation_prompt=True, tokenize=True, padding=True\n+        )\n+        expected_output = processor.tokenizer(formatted_prompt, return_tensors=None, padding=True).input_ids\n+        self.assertListEqual(expected_output, formatted_prompt_tokenized)\n+\n+        out_dict = processor.apply_chat_template(\n+            batched_messages, add_generation_prompt=True, tokenize=True, return_dict=True, padding=True\n+        )\n+        self.assertListEqual(list(out_dict.keys()), [\"input_ids\", \"attention_mask\"])\n+\n+        # Now test the ability to return dict\n+        batched_messages[0][0][\"content\"].append(\n+            {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"}\n+        )\n+        batched_messages[1][0][\"content\"].append(\n+            {\"type\": \"image\", \"url\": \"http://images.cocodataset.org/val2017/000000039769.jpg\"}\n+        )\n+        out_dict = processor.apply_chat_template(\n+            batched_messages, add_generation_prompt=True, tokenize=True, return_dict=True, padding=True\n+        )\n+        self.assertTrue(self.images_input_name in out_dict)\n+\n+        # should always have input_ids and attention_mask\n+        self.assertEqual(len(out_dict[\"input_ids\"]), 2)\n+        self.assertEqual(len(out_dict[\"attention_mask\"]), 2)\n+        self.assertEqual(len(out_dict[self.images_input_name]), 90480)\n+\n+    @require_av\n+    def test_chat_template_video(self):\n+        processor = self.get_processor()\n+        if processor.chat_template is None:\n+            self.skipTest(\"Processor has no chat template\")\n+\n+        signature = inspect.signature(processor.__call__)\n+        if \"videos\" not in {*signature.parameters.keys()} or (\n+            signature.parameters.get(\"videos\") is not None\n+            and signature.parameters[\"videos\"].annotation == inspect._empty\n+        ):\n+            self.skipTest(\"Processor doesn't accept videos at input\")\n+\n+        messages = [\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\"type\": \"video\"},\n+                        {\"type\": \"text\", \"text\": \"What is shown in this video?\"},\n+                    ],\n+                },\n+            ]\n+        ]\n+\n+        formatted_prompt = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n+        self.assertEqual(len(formatted_prompt), 1)\n+\n+        formatted_prompt_tokenized = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True)\n+        expected_output = processor.tokenizer(formatted_prompt, return_tensors=None).input_ids\n+        self.assertListEqual(expected_output, formatted_prompt_tokenized)\n+\n+        out_dict = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True)\n+        self.assertListEqual(list(out_dict.keys()), [\"input_ids\", \"attention_mask\"])\n+\n+        # Add video URL for return dict and load with `num_frames` arg\n+        messages[0][0][\"content\"][0] = {\n+            \"type\": \"video\",\n+            \"url\": \"https://test-videos.co.uk/vids/bigbuckbunny/mp4/h264/720/Big_Buck_Bunny_720_10s_10MB.mp4\",\n+        }\n+        num_frames = 3\n+        out_dict_with_video = processor.apply_chat_template(\n+            messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+            num_frames=num_frames,\n+        )\n+        self.assertTrue(self.videos_input_name in out_dict_with_video)\n+        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 115200)\n+\n+        # Load with `video_fps` arg\n+        video_fps = 1\n+        out_dict_with_video = processor.apply_chat_template(\n+            messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+            video_fps=video_fps,\n+        )\n+        self.assertTrue(self.videos_input_name in out_dict_with_video)\n+        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 288000)\n+\n+        # Load with `video_fps` and `num_frames` args, should raise an error\n+        with self.assertRaises(ValueError):\n+            out_dict_with_video = processor.apply_chat_template(\n+                messages,\n+                add_generation_prompt=True,\n+                tokenize=True,\n+                return_dict=True,\n+                video_fps=video_fps,\n+                num_frames=num_frames,\n+            )\n+\n+        # Load without any arg should load the whole video\n+        out_dict_with_video = processor.apply_chat_template(\n+            messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+        )\n+        self.assertTrue(self.videos_input_name in out_dict_with_video)\n+        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 8640000)\n+\n+        # Load video as a list of frames (i.e. images). NOTE: each frame should have same size\n+        # because we assume they come from one video\n+        messages[0][0][\"content\"][0] = {\n+            \"type\": \"video\",\n+            \"url\": [\n+                \"https://www.ilankelman.org/stopsigns/australia.jpg\",\n+                \"https://www.ilankelman.org/stopsigns/australia.jpg\",\n+            ],\n+        }\n+        out_dict_with_video = processor.apply_chat_template(\n+            messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+        )\n+        self.assertTrue(self.videos_input_name in out_dict_with_video)\n+        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 71280)"
        },
        {
            "sha": "59546485c59717a161c4e884a88bebc1f7e68f5f",
            "filename": "tests/models/qwen2_vl/test_processor_qwen2_vl.py",
            "status": "modified",
            "additions": 200,
            "deletions": 1,
            "changes": 201,
            "blob_url": "https://github.com/huggingface/transformers/blob/eebd2c972c0f4c9dec9129092393bf295a4516d9/tests%2Fmodels%2Fqwen2_vl%2Ftest_processor_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eebd2c972c0f4c9dec9129092393bf295a4516d9/tests%2Fmodels%2Fqwen2_vl%2Ftest_processor_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_processor_qwen2_vl.py?ref=eebd2c972c0f4c9dec9129092393bf295a4516d9",
            "patch": "@@ -12,14 +12,15 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n+import inspect\n import shutil\n import tempfile\n import unittest\n \n import pytest\n \n from transformers import AutoProcessor, Qwen2Tokenizer\n-from transformers.testing_utils import require_torch, require_vision\n+from transformers.testing_utils import require_av, require_torch, require_vision\n from transformers.utils import is_vision_available\n \n from ...test_processing_common import ProcessorTesterMixin\n@@ -45,6 +46,9 @@ def get_tokenizer(self, **kwargs):\n     def get_image_processor(self, **kwargs):\n         return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).image_processor\n \n+    def prepare_processor_dict(self):\n+        return {\"chat_template\": \"{% set image_count = namespace(value=0) %}{% set video_count = namespace(value=0) %}{% for message in messages %}{% if loop.first and message['role'] != 'system' %}<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n{% endif %}<|im_start|>{{ message['role'] }}\\n{% if message['content'] is string %}{{ message['content'] }}<|im_end|>\\n{% else %}{% for content in message['content'] %}{% if content['type'] == 'image' or 'image' in content or 'image_url' in content %}{% set image_count.value = image_count.value + 1 %}{% if add_vision_id %}Picture {{ image_count.value }}: {% endif %}<|vision_start|><|image_pad|><|vision_end|>{% elif content['type'] == 'video' or 'video' in content %}{% set video_count.value = video_count.value + 1 %}{% if add_vision_id %}Video {{ video_count.value }}: {% endif %}<|vision_start|><|video_pad|><|vision_end|>{% elif 'text' in content %}{{ content['text'] }}{% endif %}{% endfor %}<|im_end|>\\n{% endif %}{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\\n{% endif %}\"}  # fmt: skip\n+\n     def tearDown(self):\n         shutil.rmtree(self.tmpdirname)\n \n@@ -108,3 +112,198 @@ def test_model_input_names(self):\n         inputs = processor(text=input_str, images=image_input, videos=video_inputs)\n \n         self.assertListEqual(list(inputs.keys()), processor.model_input_names)\n+\n+    def test_chat_template_single(self):\n+        processor = self.get_processor()\n+        if processor.chat_template is None:\n+            self.skipTest(\"Processor has no chat template\")\n+\n+        messages = [\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n+                    ],\n+                },\n+            ]\n+        ]\n+\n+        formatted_prompt = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n+        self.assertEqual(len(formatted_prompt), 1)\n+\n+        formatted_prompt_tokenized = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True)\n+        expected_output = processor.tokenizer(formatted_prompt, return_tensors=None).input_ids\n+        self.assertListEqual(expected_output, formatted_prompt_tokenized)\n+\n+        out_dict = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True)\n+        self.assertListEqual(list(out_dict.keys()), [\"input_ids\", \"attention_mask\"])\n+\n+        # Now test the ability to return dict\n+        messages[0][0][\"content\"].append(\n+            {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"}\n+        )\n+        out_dict = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True)\n+        self.assertTrue(self.images_input_name in out_dict)\n+\n+        # should always have input_ids and attention_mask\n+        self.assertEqual(len(out_dict[\"input_ids\"]), 1)\n+        self.assertEqual(len(out_dict[\"attention_mask\"]), 1)\n+        self.assertEqual(len(out_dict[self.images_input_name]), 71280)\n+\n+    def test_chat_template_batched(self):\n+        processor = self.get_processor()\n+        if processor.chat_template is None:\n+            self.skipTest(\"Processor has no chat template\")\n+\n+        batched_messages = [\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n+                    ],\n+                },\n+            ],\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\"type\": \"text\", \"text\": \"What do you see?\"},\n+                    ],\n+                },\n+            ],\n+        ]\n+\n+        formatted_prompt = processor.apply_chat_template(batched_messages, add_generation_prompt=True, tokenize=False)\n+        self.assertEqual(len(formatted_prompt), 2)\n+\n+        formatted_prompt_tokenized = processor.apply_chat_template(\n+            batched_messages, add_generation_prompt=True, tokenize=True, padding=True\n+        )\n+        expected_output = processor.tokenizer(formatted_prompt, return_tensors=None, padding=True).input_ids\n+        self.assertListEqual(expected_output, formatted_prompt_tokenized)\n+\n+        out_dict = processor.apply_chat_template(\n+            batched_messages, add_generation_prompt=True, tokenize=True, return_dict=True, padding=True\n+        )\n+        self.assertListEqual(list(out_dict.keys()), [\"input_ids\", \"attention_mask\"])\n+\n+        # Now test the ability to return dict\n+        batched_messages[0][0][\"content\"].append(\n+            {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"}\n+        )\n+        batched_messages[1][0][\"content\"].append(\n+            {\"type\": \"image\", \"url\": \"http://images.cocodataset.org/val2017/000000039769.jpg\"}\n+        )\n+        out_dict = processor.apply_chat_template(\n+            batched_messages, add_generation_prompt=True, tokenize=True, return_dict=True, padding=True\n+        )\n+        self.assertTrue(self.images_input_name in out_dict)\n+\n+        # should always have input_ids and attention_mask\n+        self.assertEqual(len(out_dict[\"input_ids\"]), 2)\n+        self.assertEqual(len(out_dict[\"attention_mask\"]), 2)\n+        self.assertEqual(len(out_dict[self.images_input_name]), 90480)\n+\n+    @require_av\n+    def test_chat_template_video(self):\n+        processor = self.get_processor()\n+        if processor.chat_template is None:\n+            self.skipTest(\"Processor has no chat template\")\n+\n+        signature = inspect.signature(processor.__call__)\n+        if \"videos\" not in {*signature.parameters.keys()} or (\n+            signature.parameters.get(\"videos\") is not None\n+            and signature.parameters[\"videos\"].annotation == inspect._empty\n+        ):\n+            self.skipTest(\"Processor doesn't accept videos at input\")\n+\n+        messages = [\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\"type\": \"video\"},\n+                        {\"type\": \"text\", \"text\": \"What is shown in this video?\"},\n+                    ],\n+                },\n+            ]\n+        ]\n+\n+        formatted_prompt = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n+        self.assertEqual(len(formatted_prompt), 1)\n+\n+        formatted_prompt_tokenized = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True)\n+        expected_output = processor.tokenizer(formatted_prompt, return_tensors=None).input_ids\n+        self.assertListEqual(expected_output, formatted_prompt_tokenized)\n+\n+        out_dict = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True)\n+        self.assertListEqual(list(out_dict.keys()), [\"input_ids\", \"attention_mask\"])\n+\n+        # Add video URL for return dict and load with `num_frames` arg\n+        messages[0][0][\"content\"][0] = {\n+            \"type\": \"video\",\n+            \"url\": \"https://test-videos.co.uk/vids/bigbuckbunny/mp4/h264/720/Big_Buck_Bunny_720_10s_10MB.mp4\",\n+        }\n+        num_frames = 3\n+        out_dict_with_video = processor.apply_chat_template(\n+            messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+            num_frames=num_frames,\n+        )\n+        self.assertTrue(self.videos_input_name in out_dict_with_video)\n+        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 115200)\n+\n+        # Load with `video_fps` arg\n+        video_fps = 1\n+        out_dict_with_video = processor.apply_chat_template(\n+            messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+            video_fps=video_fps,\n+        )\n+        self.assertTrue(self.videos_input_name in out_dict_with_video)\n+        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 288000)\n+\n+        # Load with `video_fps` and `num_frames` args, should raise an error\n+        with self.assertRaises(ValueError):\n+            out_dict_with_video = processor.apply_chat_template(\n+                messages,\n+                add_generation_prompt=True,\n+                tokenize=True,\n+                return_dict=True,\n+                video_fps=video_fps,\n+                num_frames=num_frames,\n+            )\n+\n+        # Load without any arg should load the whole video\n+        out_dict_with_video = processor.apply_chat_template(\n+            messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+        )\n+        self.assertTrue(self.videos_input_name in out_dict_with_video)\n+        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 8640000)\n+\n+        # Load video as a list of frames (i.e. images). NOTE: each frame should have same size\n+        # because we assume they come from one video\n+        messages[0][0][\"content\"][0] = {\n+            \"type\": \"video\",\n+            \"url\": [\n+                \"https://www.ilankelman.org/stopsigns/australia.jpg\",\n+                \"https://www.ilankelman.org/stopsigns/australia.jpg\",\n+            ],\n+        }\n+        out_dict_with_video = processor.apply_chat_template(\n+            messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+        )\n+        self.assertTrue(self.videos_input_name in out_dict_with_video)\n+        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 71280)"
        },
        {
            "sha": "fd3345cbbd3325f9b9164e504749bd7a74b42b9d",
            "filename": "tests/test_processing_common.py",
            "status": "modified",
            "additions": 308,
            "deletions": 3,
            "changes": 311,
            "blob_url": "https://github.com/huggingface/transformers/blob/eebd2c972c0f4c9dec9129092393bf295a4516d9/tests%2Ftest_processing_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eebd2c972c0f4c9dec9129092393bf295a4516d9/tests%2Ftest_processing_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_processing_common.py?ref=eebd2c972c0f4c9dec9129092393bf295a4516d9",
            "patch": "@@ -27,17 +27,21 @@\n from transformers.processing_utils import Unpack\n from transformers.testing_utils import (\n     check_json_file_has_correct_format,\n+    require_av,\n     require_torch,\n     require_vision,\n )\n-from transformers.utils import is_vision_available\n+from transformers.utils import is_torch_available, is_vision_available\n \n \n global_rng = random.Random()\n \n if is_vision_available():\n     from PIL import Image\n \n+if is_torch_available():\n+    import torch\n+\n \n def prepare_image_inputs():\n     \"\"\"This function prepares a list of PIL images\"\"\"\n@@ -131,8 +135,10 @@ def test_processor_to_json_string(self):\n         processor = self.get_processor()\n         obj = json.loads(processor.to_json_string())\n         for key, value in self.prepare_processor_dict().items():\n-            self.assertEqual(obj[key], value)\n-            self.assertEqual(getattr(processor, key, None), value)\n+            # Chat template is saved as a separate file\n+            if key not in \"chat_template\":\n+                self.assertEqual(obj[key], value)\n+                self.assertEqual(getattr(processor, key, None), value)\n \n     def test_processor_from_and_save_pretrained(self):\n         processor_first = self.get_processor()\n@@ -532,6 +538,10 @@ def test_prepare_and_validate_optional_call_args(self):\n \n     def test_chat_template_save_loading(self):\n         processor = self.get_processor()\n+        signature = inspect.signature(processor.__call__)\n+        if \"chat_template\" not in {*signature.parameters.keys()}:\n+            self.skipTest(\"Processor doesn't accept chat templates at input\")\n+\n         existing_tokenizer_template = getattr(processor.tokenizer, \"chat_template\", None)\n         processor.chat_template = \"test template\"\n         with tempfile.TemporaryDirectory() as tmpdirname:\n@@ -553,3 +563,298 @@ def test_chat_template_save_loading(self):\n             # When we save as single files, tokenizers and processors share a chat template, which means\n             # the reloaded tokenizer should get the chat template as well\n             self.assertEqual(reloaded_processor.chat_template, reloaded_processor.tokenizer.chat_template)\n+\n+    def test_chat_template_single(self):\n+        processor = self.get_processor()\n+        if processor.chat_template is None:\n+            self.skipTest(\"Processor has no chat template\")\n+\n+        messages = [\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n+                    ],\n+                },\n+            ]\n+        ]\n+\n+        formatted_prompt = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n+        self.assertEqual(len(formatted_prompt), 1)\n+\n+        formatted_prompt_tokenized = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True)\n+        add_special_tokens = True\n+        if processor.tokenizer.bos_token is not None and formatted_prompt[0].startswith(processor.tokenizer.bos_token):\n+            add_special_tokens = False\n+        expected_output = processor.tokenizer(\n+            formatted_prompt, return_tensors=None, add_special_tokens=add_special_tokens\n+        ).input_ids\n+        self.assertListEqual(expected_output, formatted_prompt_tokenized)\n+\n+        out_dict = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True)\n+        self.assertListEqual(list(out_dict.keys()), [\"input_ids\", \"attention_mask\"])\n+\n+        # Now test the ability to return dict\n+        messages[0][0][\"content\"].append(\n+            {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"}\n+        )\n+        out_dict = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True)\n+        self.assertTrue(self.images_input_name in out_dict)\n+\n+        # should always have input_ids and attention_mask\n+        self.assertEqual(len(out_dict[\"input_ids\"]), 1)\n+        self.assertEqual(len(out_dict[\"attention_mask\"]), 1)\n+        self.assertEqual(len(out_dict[self.images_input_name]), 1)\n+\n+    def test_chat_template_batched(self):\n+        processor = self.get_processor()\n+        if processor.chat_template is None:\n+            self.skipTest(\"Processor has no chat template\")\n+\n+        batched_messages = [\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n+                    ],\n+                },\n+            ],\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\"type\": \"text\", \"text\": \"What do you see?\"},\n+                    ],\n+                },\n+            ],\n+        ]\n+\n+        formatted_prompt = processor.apply_chat_template(batched_messages, add_generation_prompt=True, tokenize=False)\n+        self.assertEqual(len(formatted_prompt), 2)\n+\n+        formatted_prompt_tokenized = processor.apply_chat_template(\n+            batched_messages, add_generation_prompt=True, tokenize=True, padding=True\n+        )\n+        add_special_tokens = True\n+        if processor.tokenizer.bos_token is not None and formatted_prompt[0].startswith(processor.tokenizer.bos_token):\n+            add_special_tokens = False\n+        expected_output = processor.tokenizer(\n+            formatted_prompt,\n+            return_tensors=None,\n+            padding=True,\n+            add_special_tokens=add_special_tokens,\n+        ).input_ids\n+        self.assertListEqual(expected_output, formatted_prompt_tokenized)\n+\n+        out_dict = processor.apply_chat_template(\n+            batched_messages, add_generation_prompt=True, tokenize=True, return_dict=True, padding=True\n+        )\n+        self.assertListEqual(list(out_dict.keys()), [\"input_ids\", \"attention_mask\"])\n+\n+        # Now test the ability to return dict\n+        batched_messages[0][0][\"content\"].append(\n+            {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"}\n+        )\n+        batched_messages[1][0][\"content\"].append(\n+            {\"type\": \"image\", \"url\": \"http://images.cocodataset.org/val2017/000000039769.jpg\"}\n+        )\n+        out_dict = processor.apply_chat_template(\n+            batched_messages, add_generation_prompt=True, tokenize=True, return_dict=True, padding=True\n+        )\n+        self.assertTrue(self.images_input_name in out_dict)\n+\n+        # should always have input_ids and attention_mask\n+        self.assertEqual(len(out_dict[\"input_ids\"]), 2)\n+        self.assertEqual(len(out_dict[\"attention_mask\"]), 2)\n+        self.assertEqual(len(out_dict[self.images_input_name]), 2)\n+\n+    def test_chat_template_accepts_processing_kwargs(self):\n+        processor = self.get_processor()\n+        if processor.chat_template is None:\n+            self.skipTest(\"Processor has no chat template\")\n+\n+        messages = [\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n+                    ],\n+                },\n+            ]\n+        ]\n+\n+        formatted_prompt_tokenized = processor.apply_chat_template(\n+            messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            padding=\"max_length\",\n+            max_length=50,\n+        )\n+        self.assertEqual(len(formatted_prompt_tokenized[0]), 50)\n+\n+        formatted_prompt_tokenized = processor.apply_chat_template(\n+            messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            truncation=True,\n+            max_length=5,\n+        )\n+        self.assertEqual(len(formatted_prompt_tokenized[0]), 5)\n+\n+        # Now test the ability to return dict\n+        messages[0][0][\"content\"].append(\n+            {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"}\n+        )\n+        out_dict = processor.apply_chat_template(\n+            messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+            do_rescale=True,\n+            rescale_factor=-1,\n+            return_tensors=\"np\",\n+        )\n+        self.assertLessEqual(out_dict[self.images_input_name][0][0].mean(), 0)\n+\n+    @require_torch\n+    def test_chat_template_dict_torch(self):\n+        processor = self.get_processor()\n+        if processor.chat_template is None:\n+            self.skipTest(\"Processor has no chat template\")\n+\n+        messages = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"},\n+                    {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n+                ],\n+            },\n+        ]\n+\n+        out_dict_tensors = processor.apply_chat_template(\n+            messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+            return_tensors=\"pt\",\n+        )\n+        self.assertTrue(self.images_input_name in out_dict_tensors)\n+        for k in out_dict_tensors:\n+            self.assertIsInstance(out_dict_tensors[k], torch.Tensor)\n+\n+    @require_av\n+    def test_chat_template_video(self):\n+        processor = self.get_processor()\n+        if processor.chat_template is None:\n+            self.skipTest(\"Processor has no chat template\")\n+\n+        signature = inspect.signature(processor.__call__)\n+        if \"videos\" not in {*signature.parameters.keys()} or (\n+            signature.parameters.get(\"videos\") is not None\n+            and signature.parameters[\"videos\"].annotation == inspect._empty\n+        ):\n+            self.skipTest(\"Processor doesn't accept videos at input\")\n+\n+        messages = [\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\"type\": \"video\"},\n+                        {\"type\": \"text\", \"text\": \"What is shown in this video?\"},\n+                    ],\n+                },\n+            ]\n+        ]\n+\n+        formatted_prompt = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n+        self.assertEqual(len(formatted_prompt), 1)\n+\n+        formatted_prompt_tokenized = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True)\n+        add_special_tokens = True\n+        if processor.tokenizer.bos_token is not None and formatted_prompt[0].startswith(processor.tokenizer.bos_token):\n+            add_special_tokens = False\n+        expected_output = processor.tokenizer(\n+            formatted_prompt,\n+            return_tensors=None,\n+            add_special_tokens=add_special_tokens,\n+        ).input_ids\n+        self.assertListEqual(expected_output, formatted_prompt_tokenized)\n+\n+        out_dict = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True)\n+        self.assertListEqual(list(out_dict.keys()), [\"input_ids\", \"attention_mask\"])\n+\n+        # Add video URL for return dict and load with `num_frames` arg\n+        messages[0][0][\"content\"][0] = {\n+            \"type\": \"video\",\n+            \"url\": \"https://test-videos.co.uk/vids/bigbuckbunny/mp4/h264/720/Big_Buck_Bunny_720_10s_10MB.mp4\",\n+        }\n+        num_frames = 3\n+        out_dict_with_video = processor.apply_chat_template(\n+            messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+            num_frames=num_frames,\n+        )\n+        self.assertTrue(self.videos_input_name in out_dict_with_video)\n+        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 1)\n+        self.assertEqual(len(out_dict_with_video[self.videos_input_name][0]), num_frames)\n+\n+        # Load with `video_fps` arg\n+        video_fps = 1\n+        out_dict_with_video = processor.apply_chat_template(\n+            messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+            video_fps=video_fps,\n+        )\n+        self.assertTrue(self.videos_input_name in out_dict_with_video)\n+        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 1)\n+        self.assertEqual(len(out_dict_with_video[self.videos_input_name][0]), video_fps * 10)\n+\n+        # Load with `video_fps` and `num_frames` args, should raise an error\n+        with self.assertRaises(ValueError):\n+            out_dict_with_video = processor.apply_chat_template(\n+                messages,\n+                add_generation_prompt=True,\n+                tokenize=True,\n+                return_dict=True,\n+                video_fps=video_fps,\n+                num_frames=num_frames,\n+            )\n+\n+        # Load without any arg should load the whole video\n+        out_dict_with_video = processor.apply_chat_template(\n+            messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+        )\n+        self.assertTrue(self.videos_input_name in out_dict_with_video)\n+        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 1)\n+        self.assertEqual(len(out_dict_with_video[self.videos_input_name][0]), 300)\n+\n+        # Load video as a list of frames (i.e. images). NOTE: each frame should have same size\n+        # because we assume they come from one video\n+        messages[0][0][\"content\"][0] = {\n+            \"type\": \"video\",\n+            \"url\": [\n+                \"https://www.ilankelman.org/stopsigns/australia.jpg\",\n+                \"https://www.ilankelman.org/stopsigns/australia.jpg\",\n+            ],\n+        }\n+        out_dict_with_video = processor.apply_chat_template(\n+            messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+        )\n+        self.assertTrue(self.videos_input_name in out_dict_with_video)\n+        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 1)\n+        self.assertEqual(len(out_dict_with_video[self.videos_input_name][0]), 2)"
        }
    ],
    "stats": {
        "total": 1077,
        "additions": 966,
        "deletions": 111
    }
}