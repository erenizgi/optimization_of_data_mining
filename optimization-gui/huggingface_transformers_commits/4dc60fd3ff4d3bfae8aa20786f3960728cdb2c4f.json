{
    "author": "MekkCyber",
    "message": "[Quantization] Removing misleading int8 quantization in Finegrained FP8 (#42945)\n\n* rm misleading\n\n* add comment",
    "sha": "4dc60fd3ff4d3bfae8aa20786f3960728cdb2c4f",
    "files": [
        {
            "sha": "a571f53ea78ec7f36d013ae24d04831430173644",
            "filename": "src/transformers/integrations/finegrained_fp8.py",
            "status": "modified",
            "additions": 4,
            "deletions": 11,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/4dc60fd3ff4d3bfae8aa20786f3960728cdb2c4f/src%2Ftransformers%2Fintegrations%2Ffinegrained_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4dc60fd3ff4d3bfae8aa20786f3960728cdb2c4f/src%2Ftransformers%2Fintegrations%2Ffinegrained_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ffinegrained_fp8.py?ref=4dc60fd3ff4d3bfae8aa20786f3960728cdb2c4f",
            "patch": "@@ -34,14 +34,10 @@\n     _FP8_DTYPE = torch.float8_e4m3fn\n     _FP8_MIN = torch.finfo(_FP8_DTYPE).min\n     _FP8_MAX = torch.finfo(_FP8_DTYPE).max\n-    _FP8_IS_INT = False\n except AttributeError:\n-    _FP8_DTYPE = torch.int8\n-    _FP8_MIN, _FP8_MAX = -127, 127\n-    _FP8_IS_INT = True\n-    logger.warning_once(\n-        \"torch.float8_e4m3fn not available; falling back to int8 emulation for Fp8Quantize operations.\"\n-    )\n+    _FP8_DTYPE = None\n+    _FP8_MIN, _FP8_MAX = -448, 448\n+    logger.warning_once(\"torch.float8_e4m3fn not available\")\n \n \n # Copied from https://huggingface.co/deepseek-ai/DeepSeek-V3/blob/main/inference/kernel.py\n@@ -701,10 +697,7 @@ def convert(self, input_dict: torch.Tensor, **kwargs) -> dict[str, torch.Tensor]\n         scales_broadcast = scales.unsqueeze(-1).unsqueeze(-3)  # -> (..., rows_tiles, 1, cols_tiles, 1)\n         scaled = reshaped * scales_broadcast\n \n-        if _FP8_IS_INT:\n-            quantized = torch.clamp(scaled.round(), min=_FP8_MIN, max=_FP8_MAX).to(_FP8_DTYPE)\n-        else:\n-            quantized = torch.clamp(scaled, min=_FP8_MIN, max=_FP8_MAX).to(_FP8_DTYPE)\n+        quantized = torch.clamp(scaled, min=_FP8_MIN, max=_FP8_MAX).to(_FP8_DTYPE)\n \n         quantized = quantized.reshape(original_shape)\n "
        },
        {
            "sha": "f3f2aef53e928998dbe82becb40263a39f0b00ca",
            "filename": "src/transformers/quantizers/quantizer_finegrained_fp8.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/4dc60fd3ff4d3bfae8aa20786f3960728cdb2c4f/src%2Ftransformers%2Fquantizers%2Fquantizer_finegrained_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4dc60fd3ff4d3bfae8aa20786f3960728cdb2c4f/src%2Ftransformers%2Fquantizers%2Fquantizer_finegrained_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_finegrained_fp8.py?ref=4dc60fd3ff4d3bfae8aa20786f3960728cdb2c4f",
            "patch": "@@ -48,7 +48,8 @@ def validate_environment(self, *args, **kwargs):\n             if (major < 8) or (major == 8 and minor < 9):\n                 logger.warning_once(\n                     \"FP8 quantized models is only supported on GPUs with compute capability >= 8.9 (e.g 4090/H100)\"\n-                    f\", actual = `{major}.{minor}`. We will default to dequantizing the model to bf16 \"\n+                    f\", actual = `{major}.{minor}`. We will default to dequantizing the model to bf16. Feel free \"\n+                    f\"to use a different quantization method like bitsandbytes or torchao\"\n                 )\n                 self.quantization_config.dequantize = True\n                 return"
        }
    ],
    "stats": {
        "total": 18,
        "additions": 6,
        "deletions": 12
    }
}