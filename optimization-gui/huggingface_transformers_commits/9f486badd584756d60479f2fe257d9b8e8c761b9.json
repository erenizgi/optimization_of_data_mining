{
    "author": "SunMarc",
    "message": "Display warning for unknown quants config instead of an error (#35963)\n\n* add supports_quant_method check\r\n\r\n* fix\r\n\r\n* add test and fix suggestions\r\n\r\n* change logic slightly\r\n\r\n---------\r\n\r\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>",
    "sha": "9f486badd584756d60479f2fe257d9b8e8c761b9",
    "files": [
        {
            "sha": "1c67ee1f8997badf3269567052789ec7b6479e40",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9f486badd584756d60479f2fe257d9b8e8c761b9/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9f486badd584756d60479f2fe257d9b8e8c761b9/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=9f486badd584756d60479f2fe257d9b8e8c761b9",
            "patch": "@@ -3634,7 +3634,10 @@ def from_pretrained(\n \n             model_kwargs = kwargs\n \n-        pre_quantized = getattr(config, \"quantization_config\", None) is not None\n+        pre_quantized = hasattr(config, \"quantization_config\")\n+        if pre_quantized and not AutoHfQuantizer.supports_quant_method(config.quantization_config):\n+            pre_quantized = False\n+\n         if pre_quantized or quantization_config is not None:\n             if pre_quantized:\n                 config.quantization_config = AutoHfQuantizer.merge_quantization_configs(\n@@ -3647,7 +3650,6 @@ def from_pretrained(\n                 config.quantization_config,\n                 pre_quantized=pre_quantized,\n             )\n-\n         else:\n             hf_quantizer = None\n "
        },
        {
            "sha": "cdb569dd1c8400ab99ca3f36ae019a86b4ad64b5",
            "filename": "src/transformers/quantizers/auto.py",
            "status": "modified",
            "additions": 23,
            "deletions": 0,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/9f486badd584756d60479f2fe257d9b8e8c761b9/src%2Ftransformers%2Fquantizers%2Fauto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9f486badd584756d60479f2fe257d9b8e8c761b9/src%2Ftransformers%2Fquantizers%2Fauto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fauto.py?ref=9f486badd584756d60479f2fe257d9b8e8c761b9",
            "patch": "@@ -15,6 +15,7 @@\n from typing import Dict, Optional, Union\n \n from ..models.auto.configuration_auto import AutoConfig\n+from ..utils import logging\n from ..utils.quantization_config import (\n     AqlmConfig,\n     AwqConfig,\n@@ -82,6 +83,8 @@\n     \"vptq\": VptqConfig,\n }\n \n+logger = logging.get_logger(__name__)\n+\n \n class AutoQuantizationConfig:\n     \"\"\"\n@@ -195,3 +198,23 @@ def merge_quantization_configs(\n             warnings.warn(warning_msg)\n \n         return quantization_config\n+\n+    @staticmethod\n+    def supports_quant_method(quantization_config_dict):\n+        quant_method = quantization_config_dict.get(\"quant_method\", None)\n+        if quantization_config_dict.get(\"load_in_8bit\", False) or quantization_config_dict.get(\"load_in_4bit\", False):\n+            suffix = \"_4bit\" if quantization_config_dict.get(\"load_in_4bit\", False) else \"_8bit\"\n+            quant_method = QuantizationMethod.BITS_AND_BYTES + suffix\n+        elif quant_method is None:\n+            raise ValueError(\n+                \"The model's quantization config from the arguments has no `quant_method` attribute. Make sure that the model has been correctly quantized\"\n+            )\n+\n+        if quant_method not in AUTO_QUANTIZATION_CONFIG_MAPPING.keys():\n+            logger.warning(\n+                f\"Unknown quantization type, got {quant_method} - supported types are:\"\n+                f\" {list(AUTO_QUANTIZER_MAPPING.keys())}. Hence, we will skip the quantization. \"\n+                \"To remove the warning, you can delete the quantization_config attribute in config.json\"\n+            )\n+            return False\n+        return True"
        },
        {
            "sha": "2179c4be570e676fc3b8b8b2c02bd0abb6b71953",
            "filename": "tests/utils/test_modeling_utils.py",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/9f486badd584756d60479f2fe257d9b8e8c761b9/tests%2Futils%2Ftest_modeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9f486badd584756d60479f2fe257d9b8e8c761b9/tests%2Futils%2Ftest_modeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_utils.py?ref=9f486badd584756d60479f2fe257d9b8e8c761b9",
            "patch": "@@ -1819,6 +1819,19 @@ def test_cache_when_needed_at_train_time(self):\n         self.assertIsNone(model_outputs.past_key_values)\n         self.assertTrue(model.training)\n \n+    def test_unknown_quantization_config(self):\n+        with tempfile.TemporaryDirectory() as tmpdir:\n+            config = BertConfig(\n+                vocab_size=99, hidden_size=32, num_hidden_layers=5, num_attention_heads=4, intermediate_size=37\n+            )\n+            model = BertModel(config)\n+            config.quantization_config = {\"quant_method\": \"unknown\"}\n+            model.save_pretrained(tmpdir)\n+            with self.assertLogs(\"transformers\", level=\"WARNING\") as cm:\n+                BertModel.from_pretrained(tmpdir)\n+            self.assertEqual(len(cm.records), 1)\n+            self.assertTrue(cm.records[0].message.startswith(\"Unknown quantization type, got\"))\n+\n \n @slow\n @require_torch"
        }
    ],
    "stats": {
        "total": 42,
        "additions": 40,
        "deletions": 2
    }
}