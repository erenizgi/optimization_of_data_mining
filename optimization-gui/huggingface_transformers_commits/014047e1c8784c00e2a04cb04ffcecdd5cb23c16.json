{
    "author": "DeepWaved",
    "message": "Fix bug in apply_rotary_pos_emb_flashatt: in Qwen2-5-VL (#36065)",
    "sha": "014047e1c8784c00e2a04cb04ffcecdd5cb23c16",
    "files": [
        {
            "sha": "20b61ddf8785f5a908b439556d2c187492aef755",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/014047e1c8784c00e2a04cb04ffcecdd5cb23c16/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/014047e1c8784c00e2a04cb04ffcecdd5cb23c16/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=014047e1c8784c00e2a04cb04ffcecdd5cb23c16",
            "patch": "@@ -162,8 +162,8 @@ def forward(self, x: torch.Tensor) -> torch.Tensor:\n \n def apply_rotary_pos_emb_flashatt(tensor: torch.Tensor, freqs: torch.Tensor) -> torch.Tensor:\n     tensor_ = tensor.float()\n-    cos = freqs.cos()\n-    sin = freqs.sin()\n+    cos = freqs.cos().float()\n+    sin = freqs.sin().float()\n     output = apply_rotary_emb(tensor_, cos, sin).type_as(tensor)\n     return output\n "
        },
        {
            "sha": "7646bb6e34ec93a7a709910ce33265aa693a214a",
            "filename": "src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/014047e1c8784c00e2a04cb04ffcecdd5cb23c16/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/014047e1c8784c00e2a04cb04ffcecdd5cb23c16/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py?ref=014047e1c8784c00e2a04cb04ffcecdd5cb23c16",
            "patch": "@@ -65,8 +65,8 @@\n \n def apply_rotary_pos_emb_flashatt(tensor: torch.Tensor, freqs: torch.Tensor) -> torch.Tensor:\n     tensor_ = tensor.float()\n-    cos = freqs.cos()\n-    sin = freqs.sin()\n+    cos = freqs.cos().float()\n+    sin = freqs.sin().float()\n     output = apply_rotary_emb(tensor_, cos, sin).type_as(tensor)\n     return output\n "
        }
    ],
    "stats": {
        "total": 8,
        "additions": 4,
        "deletions": 4
    }
}