{
    "author": "molbap",
    "message": "Add vision contribution guide (#41456)\n\n* vision contrib guide\n\n* Update CONTRIBUTING.md\n\nCo-authored-by: Aritra Roy Gosthipaty <aritra.born2fly@gmail.com>\n\n* Update CONTRIBUTING.md\n\nCo-authored-by: Aritra Roy Gosthipaty <aritra.born2fly@gmail.com>\n\n* Update CONTRIBUTING.md\n\nCo-authored-by: Aritra Roy Gosthipaty <aritra.born2fly@gmail.com>\n\n* Update CONTRIBUTING.md\n\nCo-authored-by: Aritra Roy Gosthipaty <aritra.born2fly@gmail.com>\n\n* Update CONTRIBUTING.md\n\nCo-authored-by: Aritra Roy Gosthipaty <aritra.born2fly@gmail.com>\n\n* Update CONTRIBUTING.md\n\nCo-authored-by: Aritra Roy Gosthipaty <aritra.born2fly@gmail.com>\n\n* update tiny things\n\n---------\n\nCo-authored-by: Aritra Roy Gosthipaty <aritra.born2fly@gmail.com>",
    "sha": "9aab965b1e61d92d402809bd467c317ec464e560",
    "files": [
        {
            "sha": "5ad1eab8010646f0dee4e1d55d7f42737b588e04",
            "filename": "CONTRIBUTING.md",
            "status": "modified",
            "additions": 119,
            "deletions": 1,
            "changes": 120,
            "blob_url": "https://github.com/huggingface/transformers/blob/9aab965b1e61d92d402809bd467c317ec464e560/CONTRIBUTING.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/9aab965b1e61d92d402809bd467c317ec464e560/CONTRIBUTING.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/CONTRIBUTING.md?ref=9aab965b1e61d92d402809bd467c317ec464e560",
            "patch": "@@ -112,7 +112,125 @@ New models are constantly released and if you want to implement a new model, ple\n \n If you are willing to contribute the model yourself, let us know so we can help you add it to ü§ó Transformers!\n \n-We have a technical guide for [how to add a model to ü§ó Transformers](https://huggingface.co/docs/transformers/add_new_model).\n+We have a technical guide for [how to add a model to ü§ó Transformers](https://huggingface.co/docs/transformers/modular_transformers).\n+\n+### Vision-Language Model Contribution Checklist\n+\n+If you're contributing a **vision-language model** (or any multimodal model that processes images/videos), please follow this checklist. Maintainers will use this to review your PR, and completing these steps will significantly increase the likelihood of your PR being merged quickly.\n+\n+**Required checklist for all vision-language model contributions:**\n+\n+‚òê **1. Implement a modular file**\n+\n+All new models should use the modular architecture pattern. Create a `modular_<model_name>.py` file using the modular model converter:\n+\n+- Use the CLI, [`transformers add-new-model-like`](https://github.com/huggingface/transformers/blob/main/src/transformers/cli/add_new_model_like.py) to generate a modular skeleton and get started\n+- All code should be in the modular file if possible. Modeling must be in it, it's better if configuration is in it as well. \n+- Reuse existing patterns from similar models as much as possible\n+\n+To verify your modular file is correct, run:\n+\n+```bash\n+python utils/modular_model_converter.py <model_name>\n+```\n+\n+This will generate the separate files (`modeling_*.py`, `configuration_*.py`, etc.) from your modular file. The CI will enforce that these generated files match your modular file.\n+\n+‚òê **2. Add a fast image processor (for image models)**\n+\n+If your model processes images, implement a fast image processor that uses `torch` and `torchvision` instead of PIL/numpy for better inference performance:\n+\n+- See the detailed guide in [#36978](https://github.com/huggingface/transformers/issues/36978)\n+- Fast processors inherit from `BaseImageProcessorFast`\n+- Examples: `LlavaOnevisionImageProcessorFast`, `Idefics2ImageProcessorFast`\n+\n+‚òê **3. Create a weight conversion script**\n+\n+Add a `convert_<model_name>_to_hf.py` script that converts the original model weights to the HuggingFace format:\n+\n+- Script should handle checkpoint loading, key mapping, and saving in HF format\n+- Include usage examples and documentation in the script\n+- Examples: [`convert_llava_onevision_weights_to_hf.py`](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llava_onevision/convert_llava_onevision_weights_to_hf.py), [`convert_idefics2_weights_to_hf.py`](https://github.com/huggingface/transformers/blob/main/src/transformers/models/idefics2/convert_idefics2_weights_to_hf.py)\n+\n+‚òê **4. Add integration tests with exact output matching**\n+\n+At minimum, add an `IntegrationTest` class that tests end-to-end generation (processing and modelling) with **exact** output matching:\n+\n+- For generative models: test that generated text matches expected output exactly\n+- For non-generative models: test that output logits match expected values\n+- Tests should use real checkpoints (load in 4-bit or half precision if the checkpoint is too big to fit in our CI runners) and real inputs\n+- Example pattern:\n+\n+```python\n+class MyModelIntegrationTest(unittest.TestCase):\n+    @slow\n+    def test_model_integration(self):\n+        model = MyModelForConditionalGeneration.from_pretrained(\"org/model-name\")\n+        processor = AutoProcessor.from_pretrained(\"org/model-name\")\n+\n+        inputs = processor(images=image, text=prompt, return_tensors=\"pt\")\n+        output = model.generate(**inputs, max_new_tokens=20)\n+\n+        EXPECTED_TEXT = \"exact expected output\"\n+        self.assertEqual(processor.decode(output[0]), EXPECTED_TEXT)\n+```\n+\n+See `tests/models/llava_onevision/test_modeling_llava_onevision.py` for complete examples.\n+\n+‚òê **5. Update documentation**\n+\n+Add or update model documentation:\n+\n+- Create if the cli hasn't `docs/source/en/model_doc/<model_name>.md` with usage examples\n+- Include model description, paper link, and basic usage with `Pipeline` and `AutoModel`\n+- Add the model to the appropriate TOC files\n+\n+‚òê **6. Look for reusable patterns**\n+\n+The library has 400+ models with many established patterns:\n+\n+- Search for similar models (e.g., other vision-language models)\n+- Reuse attention mechanisms, layer implementations, and processing patterns\n+- Check models like LLaVA, Idefics2, Fuyu for vision-language patterns\n+- Use provided decorators like (`auto_docstring`, `can_return_tuple`, `check_model_inputs` and `_can_record_outputs`) where relevant. \n+- Don't reinvent the wheel\n+\n+‚òê **7. Run quality checks and read the output**\n+\n+Before submitting your PR, install quality dependencies and run the full check suite:\n+\n+```bash\n+pip install -e \".[quality]\"\n+make fixup\n+```\n+\n+**Important**: Take time to read the output of `make fixup`. It will:\n+- Lint and format your code automatically\n+- Run consistency checks (imports, docstrings, etc.)\n+- Show any remaining issues that need manual fixes\n+\n+All checks must pass before your PR can be merged.\n+\n+**If this checklist is complete, your PR has a very high likelihood of being merged!** Following these steps makes the maintainers' work much easier and will reduce the number of review iterations, getting your important work out there faster.\n+\n+#### Copy-pastable checklist for maintainers\n+\n+Here's a condensed version maintainers can copy into PRs:\n+\n+```markdown\n+## Multimodal Model Addition Checklist\n+\n+Please ensure your PR completes all following items. See the [full checklist](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#vision-language-model-contribution-checklist) for details.\n+\n+- [ ] **Modular file**: `modular_<model_name>.py` implemented and verified with `python utils/modular_model_converter.py <model_name>`\n+- [ ] **Fast image processor**: Implemented using `BaseImageProcessorFast` (see [#36978](https://github.com/huggingface/transformers/issues/36978))\n+- [ ] **Conversion script**: `convert_<model_name>_to_hf.py` added with usage examples\n+- [ ] **Integration tests**: End-to-end tests with exact output matching (text or logits)\n+- [ ] **Documentation**: Model docs added/updated in `docs/source/en/model_doc/`\n+- [ ] **Pattern reuse**: Verified against similar models (LLaVA, Idefics2, etc.)\n+- [ ] **Quality checks**: `make fixup` passes with no errors\n+\n+```\n \n ## Do you want to add documentation?\n "
        }
    ],
    "stats": {
        "total": 120,
        "additions": 119,
        "deletions": 1
    }
}