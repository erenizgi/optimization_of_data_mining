{
    "author": "stevhliu",
    "message": "[docs] ViTPose (#38630)\n\n* vitpose\n\n* fix?\n\n* fix?\n\n* feedback\n\n* fix\n\n* feedback\n\n* feedback\n\n* update sample image",
    "sha": "df12d87d184db59aed00b6b22c2daff7bac95204",
    "files": [
        {
            "sha": "f9ed726593446dbd13d1f1d5291a48c3834764c5",
            "filename": "docs/source/en/model_doc/vitpose.md",
            "status": "modified",
            "additions": 187,
            "deletions": 177,
            "changes": 364,
            "blob_url": "https://github.com/huggingface/transformers/blob/df12d87d184db59aed00b6b22c2daff7bac95204/docs%2Fsource%2Fen%2Fmodel_doc%2Fvitpose.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/df12d87d184db59aed00b6b22c2daff7bac95204/docs%2Fsource%2Fen%2Fmodel_doc%2Fvitpose.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvitpose.md?ref=df12d87d184db59aed00b6b22c2daff7bac95204",
            "patch": "@@ -10,52 +10,39 @@ an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express o\n specific language governing permissions and limitations under the License.\n -->\n \n-# ViTPose\n-\n-<div class=\"flex flex-wrap space-x-1\">\n-<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<div style=\"float: right;\">\n+  <div class=\"flex flex-wrap space-x-1\">\n+    <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+  </div>\n </div>\n \n-## Overview\n-\n-The ViTPose model was proposed in [ViTPose: Simple Vision Transformer Baselines for Human Pose Estimation](https://huggingface.co/papers/2204.12484) by Yufei Xu, Jing Zhang, Qiming Zhang, Dacheng Tao. ViTPose employs a standard, non-hierarchical [Vision Transformer](vit) as backbone for the task of keypoint estimation. A simple decoder head is added on top to predict the heatmaps from a given image. Despite its simplicity, the model gets state-of-the-art results on the challenging MS COCO Keypoint Detection benchmark. The model was further improved in [ViTPose++: Vision Transformer for Generic Body Pose Estimation](https://huggingface.co/papers/2212.04246) where the authors employ\n-a mixture-of-experts (MoE) module in the ViT backbone along with pre-training on more data, which further enhances the performance.\n+# ViTPose\n \n-The abstract from the paper is the following:\n+[ViTPose](https://huggingface.co/papers/2204.12484) is a vision transformer-based model for keypoint (pose) estimation. It uses a simple, non-hierarchical [ViT](./vit) backbone and a lightweight decoder head. This architecture simplifies model design, takes advantage of transformer scalability, and can be adapted to different training strategies.\n \n-*Although no specific domain knowledge is considered in the design, plain vision transformers have shown excellent performance in visual recognition tasks. However, little effort has been made to reveal the potential of such simple structures for pose estimation tasks. In this paper, we show the surprisingly good capabilities of plain vision transformers for pose estimation from various aspects, namely simplicity in model structure, scalability in model size, flexibility in training paradigm, and transferability of knowledge between models, through a simple baseline model called ViTPose. Specifically, ViTPose employs plain and non-hierarchical vision transformers as backbones to extract features for a given person instance and a lightweight decoder for pose estimation. It can be scaled up from 100M to 1B parameters by taking the advantages of the scalable model capacity and high parallelism of transformers, setting a new Pareto front between throughput and performance. Besides, ViTPose is very flexible regarding the attention type, input resolution, pre-training and finetuning strategy, as well as dealing with multiple pose tasks. We also empirically demonstrate that the knowledge of large ViTPose models can be easily transferred to small ones via a simple knowledge token. Experimental results show that our basic ViTPose model outperforms representative methods on the challenging MS COCO Keypoint Detection benchmark, while the largest model sets a new state-of-the-art.*\n+[ViTPose++](https://huggingface.co/papers/2212.04246) improves on ViTPose by incorporating a mixture-of-experts (MoE) module in the backbone and using more diverse pretraining data.\n \n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/vitpose-architecture.png\"\n alt=\"drawing\" width=\"600\"/>\n \n-<small> ViTPose architecture. Taken from the <a href=\"https://huggingface.co/papers/2204.12484\">original paper.</a> </small>\n-\n-This model was contributed by [nielsr](https://huggingface.co/nielsr) and [sangbumchoi](https://github.com/SangbumChoi).\n-The original code can be found [here](https://github.com/ViTAE-Transformer/ViTPose).\n+You can find all ViTPose and ViTPose++ checkpoints under the [ViTPose collection](https://huggingface.co/collections/usyd-community/vitpose-677fcfd0a0b2b5c8f79c4335).\n \n-## Usage Tips\n-\n-ViTPose is a so-called top-down keypoint detection model. This means that one first uses an object detector, like [RT-DETR](rt_detr.md), to detect people (or other instances) in an image. Next, ViTPose takes the cropped images as input and predicts the keypoints for each of them.\n+The example below demonstrates pose estimation with the [`VitPoseForPoseEstimation`] class.\n \n ```py\n import torch\n import requests\n import numpy as np\n-\n+import supervision as sv\n from PIL import Image\n-\n from transformers import AutoProcessor, RTDetrForObjectDetection, VitPoseForPoseEstimation\n \n device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n \n-url = \"http://images.cocodataset.org/val2017/000000000139.jpg\"\n+url = \"https://www.fcbarcelona.com/fcbarcelona/photo/2021/01/31/3c55a19f-dfc1-4451-885e-afd14e890a11/mini_2021-01-31-BARCELONA-ATHLETIC-BILBAOI-30.JPG\"\n image = Image.open(requests.get(url, stream=True).raw)\n \n-# ------------------------------------------------------------------------\n-# Stage 1. Detect humans on the image\n-# ------------------------------------------------------------------------\n-\n-# You can choose any detector of your choice\n+# Detect humans in the image\n person_image_processor = AutoProcessor.from_pretrained(\"PekingU/rtdetr_r50vd_coco_o365\")\n person_model = RTDetrForObjectDetection.from_pretrained(\"PekingU/rtdetr_r50vd_coco_o365\", device_map=device)\n \n@@ -67,7 +54,7 @@ with torch.no_grad():\n results = person_image_processor.post_process_object_detection(\n     outputs, target_sizes=torch.tensor([(image.height, image.width)]), threshold=0.3\n )\n-result = results[0]  # take first image results\n+result = results[0]\n \n # Human label refers 0 index in COCO dataset\n person_boxes = result[\"boxes\"][result[\"labels\"] == 0]\n@@ -77,10 +64,7 @@ person_boxes = person_boxes.cpu().numpy()\n person_boxes[:, 2] = person_boxes[:, 2] - person_boxes[:, 0]\n person_boxes[:, 3] = person_boxes[:, 3] - person_boxes[:, 1]\n \n-# ------------------------------------------------------------------------\n-# Stage 2. Detect keypoints for each person found\n-# ------------------------------------------------------------------------\n-\n+# Detect keypoints for each person found\n image_processor = AutoProcessor.from_pretrained(\"usyd-community/vitpose-base-simple\")\n model = VitPoseForPoseEstimation.from_pretrained(\"usyd-community/vitpose-base-simple\", device_map=device)\n \n@@ -90,54 +74,7 @@ with torch.no_grad():\n     outputs = model(**inputs)\n \n pose_results = image_processor.post_process_pose_estimation(outputs, boxes=[person_boxes])\n-image_pose_result = pose_results[0]  # results for first image\n-```\n-\n-### ViTPose++ models\n-\n-The best [checkpoints](https://huggingface.co/collections/usyd-community/vitpose-677fcfd0a0b2b5c8f79c4335) are those of the [ViTPose++ paper](https://huggingface.co/papers/2212.04246). ViTPose++ models employ a so-called [Mixture-of-Experts (MoE)](https://huggingface.co/blog/moe) architecture for the ViT backbone, resulting in better performance.\n-\n-The ViTPose+ checkpoints use 6 experts, hence 6 different dataset indices can be passed. \n-An overview of the various dataset indices is provided below:\n-\n-- 0: [COCO validation 2017](https://cocodataset.org/#overview) dataset, using an object detector that gets 56 AP on the \"person\" class\n-- 1: [AiC](https://github.com/fabbrimatteo/AiC-Dataset) dataset\n-- 2: [MPII](https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/software-and-datasets/mpii-human-pose-dataset) dataset\n-- 3: [AP-10K](https://github.com/AlexTheBad/AP-10K) dataset\n-- 4: [APT-36K](https://github.com/pandorgan/APT-36K) dataset\n-- 5: [COCO-WholeBody](https://github.com/jin-s13/COCO-WholeBody) dataset\n-\n-Pass the `dataset_index` argument in the forward of the model to indicate which experts to use for each example in the batch. Example usage is shown below:\n-\n-```python\n-image_processor = AutoProcessor.from_pretrained(\"usyd-community/vitpose-plus-base\")\n-model = VitPoseForPoseEstimation.from_pretrained(\"usyd-community/vitpose-plus-base\", device=device)\n-\n-inputs = image_processor(image, boxes=[person_boxes], return_tensors=\"pt\").to(device)\n-\n-dataset_index = torch.tensor([0], device=device) # must be a tensor of shape (batch_size,)\n-\n-with torch.no_grad():\n-    outputs = model(**inputs, dataset_index=dataset_index)\n-```\n-\n-The ViTPose+ checkpoints use 6 experts, hence 6 different dataset indices can be passed. \n-An overview of the various dataset indices is provided below:\n-\n-- 0: [COCO validation 2017](https://cocodataset.org/#overview) dataset, using an object detector that gets 56 AP on the \"person\" class\n-- 1: [AiC](https://github.com/fabbrimatteo/AiC-Dataset) dataset\n-- 2: [MPII](https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/software-and-datasets/mpii-human-pose-dataset) dataset\n-- 3: [AP-10K](https://github.com/AlexTheBad/AP-10K) dataset\n-- 4: [APT-36K](https://github.com/pandorgan/APT-36K) dataset\n-- 5: [COCO-WholeBody](https://github.com/jin-s13/COCO-WholeBody) dataset\n-\n-\n-### Visualization\n-\n-To visualize the various keypoints, one can either leverage the `supervision` [library](https://github.com/roboflow/supervision (requires `pip install supervision`):\n-\n-```python\n-import supervision as sv\n+image_pose_result = pose_results[0]\n \n xy = torch.stack([pose_result['keypoints'] for pose_result in image_pose_result]).cpu().numpy()\n scores = torch.stack([pose_result['scores'] for pose_result in image_pose_result]).cpu().numpy()\n@@ -162,119 +99,192 @@ annotated_frame = vertex_annotator.annotate(\n     scene=annotated_frame,\n     key_points=key_points\n )\n+annotated_frame\n ```\n \n-Alternatively, one can also visualize the keypoints using [OpenCV](https://opencv.org/) (requires `pip install opencv-python`):\n-\n-```python\n-import math\n-import cv2\n-\n-def draw_points(image, keypoints, scores, pose_keypoint_color, keypoint_score_threshold, radius, show_keypoint_weight):\n-    if pose_keypoint_color is not None:\n-        assert len(pose_keypoint_color) == len(keypoints)\n-    for kid, (kpt, kpt_score) in enumerate(zip(keypoints, scores)):\n-        x_coord, y_coord = int(kpt[0]), int(kpt[1])\n-        if kpt_score > keypoint_score_threshold:\n-            color = tuple(int(c) for c in pose_keypoint_color[kid])\n-            if show_keypoint_weight:\n-                cv2.circle(image, (int(x_coord), int(y_coord)), radius, color, -1)\n-                transparency = max(0, min(1, kpt_score))\n-                cv2.addWeighted(image, transparency, image, 1 - transparency, 0, dst=image)\n-            else:\n-                cv2.circle(image, (int(x_coord), int(y_coord)), radius, color, -1)\n-\n-def draw_links(image, keypoints, scores, keypoint_edges, link_colors, keypoint_score_threshold, thickness, show_keypoint_weight, stick_width = 2):\n-    height, width, _ = image.shape\n-    if keypoint_edges is not None and link_colors is not None:\n-        assert len(link_colors) == len(keypoint_edges)\n-        for sk_id, sk in enumerate(keypoint_edges):\n-            x1, y1, score1 = (int(keypoints[sk[0], 0]), int(keypoints[sk[0], 1]), scores[sk[0]])\n-            x2, y2, score2 = (int(keypoints[sk[1], 0]), int(keypoints[sk[1], 1]), scores[sk[1]])\n-            if (\n-                x1 > 0\n-                and x1 < width\n-                and y1 > 0\n-                and y1 < height\n-                and x2 > 0\n-                and x2 < width\n-                and y2 > 0\n-                and y2 < height\n-                and score1 > keypoint_score_threshold\n-                and score2 > keypoint_score_threshold\n-            ):\n-                color = tuple(int(c) for c in link_colors[sk_id])\n-                if show_keypoint_weight:\n-                    X = (x1, x2)\n-                    Y = (y1, y2)\n-                    mean_x = np.mean(X)\n-                    mean_y = np.mean(Y)\n-                    length = ((Y[0] - Y[1]) ** 2 + (X[0] - X[1]) ** 2) ** 0.5\n-                    angle = math.degrees(math.atan2(Y[0] - Y[1], X[0] - X[1]))\n-                    polygon = cv2.ellipse2Poly(\n-                        (int(mean_x), int(mean_y)), (int(length / 2), int(stick_width)), int(angle), 0, 360, 1\n-                    )\n-                    cv2.fillConvexPoly(image, polygon, color)\n-                    transparency = max(0, min(1, 0.5 * (keypoints[sk[0], 2] + keypoints[sk[1], 2])))\n-                    cv2.addWeighted(image, transparency, image, 1 - transparency, 0, dst=image)\n-                else:\n-                    cv2.line(image, (x1, y1), (x2, y2), color, thickness=thickness)\n-\n-\n-# Note: keypoint_edges and color palette are dataset-specific\n-keypoint_edges = model.config.edges\n-\n-palette = np.array(\n-    [\n-        [255, 128, 0],\n-        [255, 153, 51],\n-        [255, 178, 102],\n-        [230, 230, 0],\n-        [255, 153, 255],\n-        [153, 204, 255],\n-        [255, 102, 255],\n-        [255, 51, 255],\n-        [102, 178, 255],\n-        [51, 153, 255],\n-        [255, 153, 153],\n-        [255, 102, 102],\n-        [255, 51, 51],\n-        [153, 255, 153],\n-        [102, 255, 102],\n-        [51, 255, 51],\n-        [0, 255, 0],\n-        [0, 0, 255],\n-        [255, 0, 0],\n-        [255, 255, 255],\n-    ]\n+<div class=\"flex justify-center\">\n+    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/vitpose.png\"/>\n+</div>\n+\n+Quantization reduces the memory burden of large models by representing the weights in a lower precision. Refer to the [Quantization](../quantization/overview) overview for more available quantization backends.\n+\n+The example below uses [torchao](../quantization/torchao) to only quantize the weights to int4.\n+\n+```py\n+# pip install torchao\n+import torch\n+import requests\n+import numpy as np\n+from PIL import Image\n+from transformers import AutoProcessor, RTDetrForObjectDetection, VitPoseForPoseEstimation, TorchAoConfig\n+\n+url = \"https://www.fcbarcelona.com/fcbarcelona/photo/2021/01/31/3c55a19f-dfc1-4451-885e-afd14e890a11/mini_2021-01-31-BARCELONA-ATHLETIC-BILBAOI-30.JPG\"\n+image = Image.open(requests.get(url, stream=True).raw)\n+\n+person_image_processor = AutoProcessor.from_pretrained(\"PekingU/rtdetr_r50vd_coco_o365\")\n+person_model = RTDetrForObjectDetection.from_pretrained(\"PekingU/rtdetr_r50vd_coco_o365\", device_map=device)\n+\n+inputs = person_image_processor(images=image, return_tensors=\"pt\").to(device)\n+\n+with torch.no_grad():\n+    outputs = person_model(**inputs)\n+\n+results = person_image_processor.post_process_object_detection(\n+    outputs, target_sizes=torch.tensor([(image.height, image.width)]), threshold=0.3\n )\n+result = results[0]\n \n-link_colors = palette[[0, 0, 0, 0, 7, 7, 7, 9, 9, 9, 9, 9, 16, 16, 16, 16, 16, 16, 16]]\n-keypoint_colors = palette[[16, 16, 16, 16, 16, 9, 9, 9, 9, 9, 9, 0, 0, 0, 0, 0, 0]]\n+person_boxes = result[\"boxes\"][result[\"labels\"] == 0]\n+person_boxes = person_boxes.cpu().numpy()\n \n-numpy_image = np.array(image)\n+person_boxes[:, 2] = person_boxes[:, 2] - person_boxes[:, 0]\n+person_boxes[:, 3] = person_boxes[:, 3] - person_boxes[:, 1]\n+\n+quantization_config = TorchAoConfig(\"int4_weight_only\", group_size=128)\n \n-for pose_result in image_pose_result:\n-    scores = np.array(pose_result[\"scores\"])\n-    keypoints = np.array(pose_result[\"keypoints\"])\n+image_processor = AutoProcessor.from_pretrained(\"usyd-community/vitpose-plus-huge\")\n+model = VitPoseForPoseEstimation.from_pretrained(\"usyd-community/vitpose-plus-huge\", device_map=device, quantization_config=quantization_config)\n \n-    # draw each point on image\n-    draw_points(numpy_image, keypoints, scores, keypoint_colors, keypoint_score_threshold=0.3, radius=4, show_keypoint_weight=False)\n+inputs = image_processor(image, boxes=[person_boxes], return_tensors=\"pt\").to(device)\n \n-    # draw links\n-    draw_links(numpy_image, keypoints, scores, keypoint_edges, link_colors, keypoint_score_threshold=0.3, thickness=1, show_keypoint_weight=False)\n+with torch.no_grad():\n+    outputs = model(**inputs)\n \n-pose_image = Image.fromarray(numpy_image)\n-pose_image\n+pose_results = image_processor.post_process_pose_estimation(outputs, boxes=[person_boxes])\n+image_pose_result = pose_results[0]\n ```\n-<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/vitpose-coco.jpg\" alt=\"drawing\" width=\"600\"/>\n+\n+## Notes\n+\n+- Use [`AutoProcessor`] to automatically prepare bounding box and image inputs.\n+- ViTPose is a top-down pose estimator. It uses a object detector to detect individuals first before keypoint prediction.\n+- ViTPose++ has 6 different MoE expert heads (COCO validation `0`, AiC `1`, MPII `2`, AP-10K `3`, APT-36K `4`, COCO-WholeBody `5`) which supports 6 different datasets. Pass a specific value corresponding to the dataset to the `dataset_index` to indicate which expert to use.\n+\n+    ```py\n+    from transformers import AutoProcessor, VitPoseForPoseEstimation\n+\n+    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+\n+    image_processor = AutoProcessor.from_pretrained(\"usyd-community/vitpose-plus-base\")\n+    model = VitPoseForPoseEstimation.from_pretrained(\"usyd-community/vitpose-plus-base\", device=device)\n+\n+    inputs = image_processor(image, boxes=[person_boxes], return_tensors=\"pt\").to(device)\n+    dataset_index = torch.tensor([0], device=device) # must be a tensor of shape (batch_size,)\n+\n+    with torch.no_grad():\n+        outputs = model(**inputs, dataset_index=dataset_index)\n+    ```\n+\n+- [OpenCV](https://opencv.org/) is an alternative option for visualizing the estimated pose.\n+\n+    ```py\n+    # pip install opencv-python\n+    import math\n+    import cv2\n+\n+    def draw_points(image, keypoints, scores, pose_keypoint_color, keypoint_score_threshold, radius, show_keypoint_weight):\n+        if pose_keypoint_color is not None:\n+            assert len(pose_keypoint_color) == len(keypoints)\n+        for kid, (kpt, kpt_score) in enumerate(zip(keypoints, scores)):\n+            x_coord, y_coord = int(kpt[0]), int(kpt[1])\n+            if kpt_score > keypoint_score_threshold:\n+                color = tuple(int(c) for c in pose_keypoint_color[kid])\n+                if show_keypoint_weight:\n+                    cv2.circle(image, (int(x_coord), int(y_coord)), radius, color, -1)\n+                    transparency = max(0, min(1, kpt_score))\n+                    cv2.addWeighted(image, transparency, image, 1 - transparency, 0, dst=image)\n+                else:\n+                    cv2.circle(image, (int(x_coord), int(y_coord)), radius, color, -1)\n+\n+    def draw_links(image, keypoints, scores, keypoint_edges, link_colors, keypoint_score_threshold, thickness, show_keypoint_weight, stick_width = 2):\n+        height, width, _ = image.shape\n+        if keypoint_edges is not None and link_colors is not None:\n+            assert len(link_colors) == len(keypoint_edges)\n+            for sk_id, sk in enumerate(keypoint_edges):\n+                x1, y1, score1 = (int(keypoints[sk[0], 0]), int(keypoints[sk[0], 1]), scores[sk[0]])\n+                x2, y2, score2 = (int(keypoints[sk[1], 0]), int(keypoints[sk[1], 1]), scores[sk[1]])\n+                if (\n+                    x1 > 0\n+                    and x1 < width\n+                    and y1 > 0\n+                    and y1 < height\n+                    and x2 > 0\n+                    and x2 < width\n+                    and y2 > 0\n+                    and y2 < height\n+                    and score1 > keypoint_score_threshold\n+                    and score2 > keypoint_score_threshold\n+                ):\n+                    color = tuple(int(c) for c in link_colors[sk_id])\n+                    if show_keypoint_weight:\n+                        X = (x1, x2)\n+                        Y = (y1, y2)\n+                        mean_x = np.mean(X)\n+                        mean_y = np.mean(Y)\n+                        length = ((Y[0] - Y[1]) ** 2 + (X[0] - X[1]) ** 2) ** 0.5\n+                        angle = math.degrees(math.atan2(Y[0] - Y[1], X[0] - X[1]))\n+                        polygon = cv2.ellipse2Poly(\n+                            (int(mean_x), int(mean_y)), (int(length / 2), int(stick_width)), int(angle), 0, 360, 1\n+                        )\n+                        cv2.fillConvexPoly(image, polygon, color)\n+                        transparency = max(0, min(1, 0.5 * (keypoints[sk[0], 2] + keypoints[sk[1], 2])))\n+                        cv2.addWeighted(image, transparency, image, 1 - transparency, 0, dst=image)\n+                    else:\n+                        cv2.line(image, (x1, y1), (x2, y2), color, thickness=thickness)\n+\n+    # Note: keypoint_edges and color palette are dataset-specific\n+    keypoint_edges = model.config.edges\n+\n+    palette = np.array(\n+        [\n+            [255, 128, 0],\n+            [255, 153, 51],\n+            [255, 178, 102],\n+            [230, 230, 0],\n+            [255, 153, 255],\n+            [153, 204, 255],\n+            [255, 102, 255],\n+            [255, 51, 255],\n+            [102, 178, 255],\n+            [51, 153, 255],\n+            [255, 153, 153],\n+            [255, 102, 102],\n+            [255, 51, 51],\n+            [153, 255, 153],\n+            [102, 255, 102],\n+            [51, 255, 51],\n+            [0, 255, 0],\n+            [0, 0, 255],\n+            [255, 0, 0],\n+            [255, 255, 255],\n+        ]\n+    )\n+\n+    link_colors = palette[[0, 0, 0, 0, 7, 7, 7, 9, 9, 9, 9, 9, 16, 16, 16, 16, 16, 16, 16]]\n+    keypoint_colors = palette[[16, 16, 16, 16, 16, 9, 9, 9, 9, 9, 9, 0, 0, 0, 0, 0, 0]]\n+\n+    numpy_image = np.array(image)\n+\n+    for pose_result in image_pose_result:\n+        scores = np.array(pose_result[\"scores\"])\n+        keypoints = np.array(pose_result[\"keypoints\"])\n+\n+        # draw each point on image\n+        draw_points(numpy_image, keypoints, scores, keypoint_colors, keypoint_score_threshold=0.3, radius=4, show_keypoint_weight=False)\n+\n+        # draw links\n+        draw_links(numpy_image, keypoints, scores, keypoint_edges, link_colors, keypoint_score_threshold=0.3, thickness=1, show_keypoint_weight=False)\n+\n+    pose_image = Image.fromarray(numpy_image)\n+    pose_image\n+    ```\n \n ## Resources\n \n-A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you get started with ViTPose. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n+Refer to resources below to learn more about using ViTPose.\n \n-- A demo of ViTPose on images and video can be found [here](https://huggingface.co/spaces/hysts/ViTPose-transformers).\n-- A notebook illustrating inference and visualization can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/ViTPose/Inference_with_ViTPose_for_human_pose_estimation.ipynb).\n+- This [notebook](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/ViTPose/Inference_with_ViTPose_for_body_pose_estimation.ipynb) demonstrates inference and visualization.\n+- This [Space](https://huggingface.co/spaces/hysts/ViTPose-transformers) demonstrates ViTPose on images and video.\n \n ## VitPoseImageProcessor\n "
        }
    ],
    "stats": {
        "total": 364,
        "additions": 187,
        "deletions": 177
    }
}