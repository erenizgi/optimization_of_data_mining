{
    "author": "ArthurZucker",
    "message": "ðŸš¨Early-errorðŸš¨ config will error out if `output_attentions=True` and the attn implementation is wrong (#38288)\n\n* Protect ParallelInterface\n\n* early error out on output attention setting for no wraning in modeling\n\n* modular update\n\n* fixup\n\n* update model tests\n\n* update\n\n* oups\n\n* set model's config\n\n* more cases\n\n* ??\n\n* properly fix\n\n* fixup\n\n* update\n\n* last onces\n\n* update\n\n* fix?\n\n* fix wrong merge commit\n\n* fix hub test\n\n* nits\n\n* wow I am tired\n\n* updates\n\n* fix pipeline!\n\n---------\n\nCo-authored-by: Lysandre <hi@lysand.re>",
    "sha": "f5d45d89c4f1d3282fc216a96dc6830b88d41249",
    "files": [
        {
            "sha": "5dad796f260fb6fbd74bdef5d1856a9a49fc9b7b",
            "filename": "docs/source/en/model_doc/jamba.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/docs%2Fsource%2Fen%2Fmodel_doc%2Fjamba.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/docs%2Fsource%2Fen%2Fmodel_doc%2Fjamba.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fjamba.md?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -99,7 +99,7 @@ quantization_config = BitsAndBytesConfig(load_in_8bit=True,\n device_map = {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 1, 'model.layers.17': 1, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 2, 'model.layers.26': 2, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.layers.32': 3, 'model.layers.33': 3, 'model.layers.34': 3, 'model.layers.35': 3, 'model.layers.36': 4, 'model.layers.37': 4, 'model.layers.38': 4, 'model.layers.39': 4, 'model.layers.40': 4, 'model.layers.41': 4, 'model.layers.42': 4, 'model.layers.43': 4, 'model.layers.44': 4, 'model.layers.45': 5, 'model.layers.46': 5, 'model.layers.47': 5, 'model.layers.48': 5, 'model.layers.49': 5, 'model.layers.50': 5, 'model.layers.51': 5, 'model.layers.52': 5, 'model.layers.53': 5, 'model.layers.54': 6, 'model.layers.55': 6, 'model.layers.56': 6, 'model.layers.57': 6, 'model.layers.58': 6, 'model.layers.59': 6, 'model.layers.60': 6, 'model.layers.61': 6, 'model.layers.62': 6, 'model.layers.63': 7, 'model.layers.64': 7, 'model.layers.65': 7, 'model.layers.66': 7, 'model.layers.67': 7, 'model.layers.68': 7, 'model.layers.69': 7, 'model.layers.70': 7, 'model.layers.71': 7, 'model.final_layernorm': 7, 'lm_head': 7}\n model = AutoModelForCausalLM.from_pretrained(\"ai21labs/AI21-Jamba-Large-1.6\",\n                                              torch_dtype=torch.bfloat16,\n-                                             attn_implementation=\"flash_attention_2\",\n+                    attn_implementation=\"flash_attention_2\",\n                                              quantization_config=quantization_config,\n                                              device_map=device_map)\n "
        },
        {
            "sha": "28e3b41f7e171beef744b9617683ecfc9d16eebc",
            "filename": "src/transformers/configuration_utils.py",
            "status": "modified",
            "additions": 19,
            "deletions": 1,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/src%2Ftransformers%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/src%2Ftransformers%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconfiguration_utils.py?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -214,7 +214,7 @@ def __init__(self, **kwargs):\n         # Attributes with defaults\n         self.return_dict = kwargs.pop(\"return_dict\", True)\n         self.output_hidden_states = kwargs.pop(\"output_hidden_states\", False)\n-        self.output_attentions = kwargs.pop(\"output_attentions\", False)\n+        self._output_attentions = kwargs.pop(\"output_attentions\", False)\n         self.torchscript = kwargs.pop(\"torchscript\", False)  # Only used by PyTorch models\n         self.torch_dtype = kwargs.pop(\"torch_dtype\", None)  # Only used by PyTorch models\n         self.use_bfloat16 = kwargs.pop(\"use_bfloat16\", False)\n@@ -331,6 +331,22 @@ def name_or_path(self) -> str:\n     def name_or_path(self, value):\n         self._name_or_path = str(value)  # Make sure that name_or_path is a string (for JSON encoding)\n \n+    @property\n+    def output_attentions(self):\n+        \"\"\"\n+        `bool`: Whether or not the model should returns all attentions.\n+        \"\"\"\n+        return self._output_attentions\n+\n+    @output_attentions.setter\n+    def output_attentions(self, value):\n+        if self._attn_implementation != \"eager\":\n+            raise ValueError(\n+                \"The `output_attentions` attribute is not supported when using the `attn_implementation` set to \"\n+                f\"{self._attn_implementation}. Please set it to 'eager' instead.\"\n+            )\n+        self._output_attentions = value\n+\n     @property\n     def use_return_dict(self) -> bool:\n         \"\"\"\n@@ -1004,6 +1020,8 @@ def _remove_keys_not_serialized(self, d: dict[str, Any]) -> None:\n \n         if \"_auto_class\" in d:\n             del d[\"_auto_class\"]\n+        if \"_output_attentions\" in d:\n+            d[\"output_attentions\"] = d.pop(\"_output_attentions\")\n         if \"_commit_hash\" in d:\n             del d[\"_commit_hash\"]\n         if \"_attn_implementation_internal\" in d:"
        },
        {
            "sha": "8f80a7ff08b2e61701df6e721ae2d27b42d82a0e",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 8,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -549,15 +549,8 @@ def forward(\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n-\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,"
        },
        {
            "sha": "918782826ddf30d4da65264cff85a5ff1bb3f475",
            "filename": "src/transformers/models/bamba/modeling_bamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 8,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -313,15 +313,8 @@ def forward(\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n-\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,"
        },
        {
            "sha": "34abf4f15d74fdc1d491385ebd43050242752594",
            "filename": "src/transformers/models/csm/modeling_csm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 8,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -337,15 +337,8 @@ def forward(\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n-\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,"
        },
        {
            "sha": "cf7004c97f7685cb55bd8d1375007e569c4a97a9",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 8,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -206,15 +206,8 @@ def forward(\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n-\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,"
        },
        {
            "sha": "b7990e9660c0b3de65c50aa162b84a4df92a27e8",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 8,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -239,15 +239,8 @@ def forward(\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n-\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,"
        },
        {
            "sha": "236b0ed5c4c2c611cb71963e895c00b06bc6f9b4",
            "filename": "src/transformers/models/glm/modeling_glm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 8,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -201,15 +201,8 @@ def forward(\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n-\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,"
        },
        {
            "sha": "48d8522502ee04e7bd4cf24e5fd5778970f8f741",
            "filename": "src/transformers/models/glm4/modeling_glm4.py",
            "status": "modified",
            "additions": 1,
            "deletions": 8,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -259,15 +259,8 @@ def forward(\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n-\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,"
        },
        {
            "sha": "639db777012d03c45ea01af3a01951371baf170d",
            "filename": "src/transformers/models/granite/modeling_granite.py",
            "status": "modified",
            "additions": 1,
            "deletions": 8,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -165,15 +165,8 @@ def forward(\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n-\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,"
        },
        {
            "sha": "eea73341d05676c031a5649caa2f314fe0684edf",
            "filename": "src/transformers/models/helium/modeling_helium.py",
            "status": "modified",
            "additions": 1,
            "deletions": 8,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -241,15 +241,8 @@ def forward(\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n-\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,"
        },
        {
            "sha": "7b0416ec19f82612fd22db3e9252ac9094c38669",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 8,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -245,15 +245,8 @@ def forward(\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n-\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,"
        },
        {
            "sha": "617ba23ebd6b01dfba201d275bf9b48badfc9a2d",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -1157,7 +1157,8 @@ def test_assisted_decoding_matches_greedy_search(self, assistant_type):\n                 self.skipTest(reason=f\"{model_class.__name__} doesn't support caching\")\n \n             config.is_decoder = True\n-            model = model_class(config).to(torch_device).eval()\n+            model = model_class._from_config(config, attn_implementation=\"eager\").to(torch_device).eval()\n+            config = model.config\n             # Sets assisted generation arguments such that:\n             # a) no EOS is generated, to ensure generation doesn't break early\n             # b) the assistant model always generates two tokens when it is called, to ensure the input preparation of\n@@ -1187,6 +1188,7 @@ def test_assisted_decoding_matches_greedy_search(self, assistant_type):\n                 assistant_model = model_class(config).to(torch_device).eval()\n             else:\n                 assistant_model = model\n+            assistant_model.config._attn_implementation = \"eager\"\n             assistant_model.generation_config.num_assistant_tokens = 2  # see b)\n             assistant_model.generation_config.num_assistant_tokens_schedule = \"constant\"  # see b)\n             generation_kwargs.update({\"assistant_model\": assistant_model})\n@@ -1367,7 +1369,8 @@ def test_assisted_decoding_sample(self):\n                 self.skipTest(reason=f\"{model_class.__name__} doesn't support caching\")\n \n             config.is_decoder = True\n-            model = model_class(config).to(torch_device).eval()\n+            model = model_class._from_config(config, attn_implementation=\"eager\").to(torch_device).eval()\n+            config = model.config\n             # Sets assisted generation arguments such that:\n             # a) no EOS is generated, to ensure generation doesn't break early\n             # b) the assistant model always generates two tokens when it is called, to ensure the input preparation of"
        },
        {
            "sha": "954f9f16622bf934444f57eca2fd346d01d020da",
            "filename": "tests/models/autoformer/test_modeling_autoformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fautoformer%2Ftest_modeling_autoformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fautoformer%2Ftest_modeling_autoformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fautoformer%2Ftest_modeling_autoformer.py?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -323,7 +323,8 @@ def test_attention_outputs(self):\n             inputs_dict[\"output_attentions\"] = True\n             inputs_dict[\"output_hidden_states\"] = False\n             config.return_dict = True\n-            model = model_class(config)\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            config = model.config\n             model.to(torch_device)\n             model.eval()\n             with torch.no_grad():"
        },
        {
            "sha": "7c00a7a030d2fd5a8c3f28d788f5a7e65a74a81a",
            "filename": "tests/models/bamba/test_modeling_bamba.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -362,7 +362,8 @@ def test_attention_outputs(self):\n             inputs_dict[\"output_attentions\"] = True\n             inputs_dict[\"output_hidden_states\"] = False\n             config.return_dict = True\n-            model = model_class(config)\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            config = model.config\n             model.to(torch_device)\n             model.eval()\n "
        },
        {
            "sha": "4043a8ea908031db16d849ada5d5a0cfb723788d",
            "filename": "tests/models/canine/test_modeling_canine.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fcanine%2Ftest_modeling_canine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fcanine%2Ftest_modeling_canine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcanine%2Ftest_modeling_canine.py?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -318,7 +318,8 @@ def test_attention_outputs(self):\n             inputs_dict[\"output_attentions\"] = True\n             inputs_dict[\"output_hidden_states\"] = False\n             config.return_dict = True\n-            model = model_class(config)\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            config = model.config\n             model.to(torch_device)\n             model.eval()\n             with torch.no_grad():"
        },
        {
            "sha": "f752e58c6af32d324e42f3378f247f6ac123cd6e",
            "filename": "tests/models/conditional_detr/test_modeling_conditional_detr.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fconditional_detr%2Ftest_modeling_conditional_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fconditional_detr%2Ftest_modeling_conditional_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fconditional_detr%2Ftest_modeling_conditional_detr.py?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -279,7 +279,8 @@ def test_attention_outputs(self):\n             inputs_dict[\"output_attentions\"] = True\n             inputs_dict[\"output_hidden_states\"] = False\n             config.return_dict = True\n-            model = model_class(config)\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            config = model.config\n             model.to(torch_device)\n             model.eval()\n             with torch.no_grad():"
        },
        {
            "sha": "908c0920389e42c708706f26784d9e75fb59397f",
            "filename": "tests/models/convbert/test_modeling_convbert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fconvbert%2Ftest_modeling_convbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fconvbert%2Ftest_modeling_convbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fconvbert%2Ftest_modeling_convbert.py?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -327,7 +327,8 @@ def test_attention_outputs(self):\n             inputs_dict[\"output_attentions\"] = True\n             inputs_dict[\"output_hidden_states\"] = False\n             config.return_dict = True\n-            model = model_class(config)\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            config = model.config\n             model.to(torch_device)\n             model.eval()\n             with torch.no_grad():"
        },
        {
            "sha": "741fbd0ca5462f6bbd61c402f6f8cacfa3946ba5",
            "filename": "tests/models/d_fine/test_modeling_d_fine.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fd_fine%2Ftest_modeling_d_fine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fd_fine%2Ftest_modeling_d_fine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fd_fine%2Ftest_modeling_d_fine.py?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -371,7 +371,8 @@ def test_attention_outputs(self):\n             inputs_dict[\"output_attentions\"] = True\n             inputs_dict[\"output_hidden_states\"] = False\n             config.return_dict = True\n-            model = model_class(config)\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            config = model.config\n             model.to(torch_device)\n             model.eval()\n             with torch.no_grad():"
        },
        {
            "sha": "8b4d8c139dcc9c6acf5616419e49ec534d7559ec",
            "filename": "tests/models/dab_detr/test_modeling_dab_detr.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fdab_detr%2Ftest_modeling_dab_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fdab_detr%2Ftest_modeling_dab_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdab_detr%2Ftest_modeling_dab_detr.py?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -499,7 +499,8 @@ def test_attention_outputs(self):\n             inputs_dict[\"output_attentions\"] = True\n             inputs_dict[\"output_hidden_states\"] = False\n             config.return_dict = True\n-            model = model_class(config)\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            config = model.config\n             model.to(torch_device)\n             model.eval()\n             with torch.no_grad():"
        },
        {
            "sha": "d069a711bf9e50b4111b0d32056fd7b2af84ac34",
            "filename": "tests/models/deformable_detr/test_modeling_deformable_detr.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fdeformable_detr%2Ftest_modeling_deformable_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fdeformable_detr%2Ftest_modeling_deformable_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeformable_detr%2Ftest_modeling_deformable_detr.py?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -278,7 +278,8 @@ def test_attention_outputs(self):\n             inputs_dict[\"output_attentions\"] = True\n             inputs_dict[\"output_hidden_states\"] = False\n             config.return_dict = True\n-            model = model_class(config)\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            config = model.config\n             model.to(torch_device)\n             model.eval()\n             with torch.no_grad():"
        },
        {
            "sha": "b626f74c5c572bd9cfc145e3e916b077cf012b23",
            "filename": "tests/models/detr/test_modeling_detr.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fdetr%2Ftest_modeling_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fdetr%2Ftest_modeling_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdetr%2Ftest_modeling_detr.py?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -279,7 +279,8 @@ def test_attention_outputs(self):\n             inputs_dict[\"output_attentions\"] = True\n             inputs_dict[\"output_hidden_states\"] = False\n             config.return_dict = True\n-            model = model_class(config)\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            config = model.config\n             model.to(torch_device)\n             model.eval()\n             with torch.no_grad():"
        },
        {
            "sha": "456da85007990530d7ba7038c9d93ffecaa103b8",
            "filename": "tests/models/donut/test_modeling_donut_swin.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fdonut%2Ftest_modeling_donut_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fdonut%2Ftest_modeling_donut_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdonut%2Ftest_modeling_donut_swin.py?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -214,7 +214,8 @@ def test_attention_outputs(self):\n             inputs_dict[\"output_attentions\"] = True\n             inputs_dict[\"output_hidden_states\"] = False\n             config.return_dict = True\n-            model = model_class(config)\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            config = model.config\n             model.to(torch_device)\n             model.eval()\n             with torch.no_grad():"
        },
        {
            "sha": "e9eddf865678d75dcce1f6df185a9ee96c6be195",
            "filename": "tests/models/encoder_decoder/test_modeling_encoder_decoder.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fencoder_decoder%2Ftest_modeling_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fencoder_decoder%2Ftest_modeling_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fencoder_decoder%2Ftest_modeling_encoder_decoder.py?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -457,6 +457,7 @@ def check_encoder_decoder_model_output_attentions_from_config(\n         decoder_attention_mask = decoder_attention_mask[:, :-1]\n         encoder_model, decoder_model = self.get_encoder_decoder_model(config, decoder_config)\n         enc_dec_model = EncoderDecoderModel(encoder=encoder_model, decoder=decoder_model)\n+        enc_dec_model.config._attn_implementation = \"eager\"  # model config -> won't work\n         enc_dec_model.config.output_attentions = True  # model config -> won't work\n         enc_dec_model.to(torch_device)\n         outputs_encoder_decoder = enc_dec_model("
        },
        {
            "sha": "e8432c477ce12bf9abec02706e462f4f84e67667",
            "filename": "tests/models/falcon_h1/test_modeling_falcon_h1.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Ffalcon_h1%2Ftest_modeling_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Ffalcon_h1%2Ftest_modeling_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffalcon_h1%2Ftest_modeling_falcon_h1.py?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -337,7 +337,8 @@ def test_attention_outputs(self):\n             inputs_dict[\"output_attentions\"] = True\n             inputs_dict[\"output_hidden_states\"] = False\n             config.return_dict = True\n-            model = model_class(config)\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            config = model.config\n             model.to(torch_device)\n             model.eval()\n "
        },
        {
            "sha": "22201f42b0ea7f1a6e389292b0e758587a268519",
            "filename": "tests/models/fastspeech2_conformer/test_modeling_fastspeech2_conformer.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Ffastspeech2_conformer%2Ftest_modeling_fastspeech2_conformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Ffastspeech2_conformer%2Ftest_modeling_fastspeech2_conformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffastspeech2_conformer%2Ftest_modeling_fastspeech2_conformer.py?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -287,7 +287,8 @@ def test_attention_outputs(self):\n             inputs_dict[\"output_attentions\"] = True\n             inputs_dict[\"output_hidden_states\"] = False\n             config.return_dict = True\n-            model = model_class(config)\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            config = model.config\n             model.to(torch_device)\n             model.eval()\n             with torch.no_grad():\n@@ -709,7 +710,8 @@ def test_attention_outputs(self):\n             inputs_dict[\"output_attentions\"] = True\n             inputs_dict[\"output_hidden_states\"] = False\n             config.model_config.return_dict = True\n-            model = model_class(config)\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            config = model.config\n             model.to(torch_device)\n             model.eval()\n             with torch.no_grad():"
        },
        {
            "sha": "f5cafdc95780f306e45f5349fe4a864d2b17c3e6",
            "filename": "tests/models/flava/test_modeling_flava.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fflava%2Ftest_modeling_flava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fflava%2Ftest_modeling_flava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fflava%2Ftest_modeling_flava.py?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -218,7 +218,8 @@ def test_attention_outputs(self):\n             inputs_dict[\"output_attentions\"] = True\n             inputs_dict[\"output_hidden_states\"] = False\n             config.return_dict = True\n-            model = model_class(config)\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            config = model.config\n             model.to(torch_device)\n             model.eval()\n             with torch.no_grad():"
        },
        {
            "sha": "b3e1852373a8c5e54ef1e5830b23e5380898cb28",
            "filename": "tests/models/glpn/test_modeling_glpn.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fglpn%2Ftest_modeling_glpn.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fglpn%2Ftest_modeling_glpn.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fglpn%2Ftest_modeling_glpn.py?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -184,7 +184,8 @@ def test_attention_outputs(self):\n             inputs_dict[\"output_attentions\"] = True\n             inputs_dict[\"output_hidden_states\"] = False\n             config.return_dict = True\n-            model = model_class(config)\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            config = model.config\n             model.to(torch_device)\n             model.eval()\n             with torch.no_grad():"
        },
        {
            "sha": "80023aa5b7918514d80ff19c816183c19f950166",
            "filename": "tests/models/grounding_dino/test_modeling_grounding_dino.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fgrounding_dino%2Ftest_modeling_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fgrounding_dino%2Ftest_modeling_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgrounding_dino%2Ftest_modeling_grounding_dino.py?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -327,7 +327,8 @@ def test_attention_outputs(self):\n             inputs_dict[\"output_attentions\"] = True\n             inputs_dict[\"output_hidden_states\"] = False\n             config.return_dict = True\n-            model = model_class(config)\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            config = model.config\n             model.to(torch_device)\n             model.eval()\n             with torch.no_grad():"
        },
        {
            "sha": "24e4328ac7ed45ae4c371fb6ffd4300e03881290",
            "filename": "tests/models/groupvit/test_modeling_groupvit.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fgroupvit%2Ftest_modeling_groupvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fgroupvit%2Ftest_modeling_groupvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgroupvit%2Ftest_modeling_groupvit.py?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -202,7 +202,8 @@ def test_attention_outputs(self):\n             inputs_dict[\"output_attentions\"] = True\n             inputs_dict[\"output_hidden_states\"] = False\n             config.return_dict = True\n-            model = model_class(config)\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            config = model.config\n             model.to(torch_device)\n             model.eval()\n             with torch.no_grad():"
        },
        {
            "sha": "dfbec4a4b8aec5ea045a8601d020bf1411c5dcff",
            "filename": "tests/models/hiera/test_modeling_hiera.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fhiera%2Ftest_modeling_hiera.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fhiera%2Ftest_modeling_hiera.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fhiera%2Ftest_modeling_hiera.py?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -281,7 +281,8 @@ def test_attention_outputs(self):\n             inputs_dict[\"output_attentions\"] = True\n             inputs_dict[\"output_hidden_states\"] = False\n             config.return_dict = True\n-            model = model_class(config)\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            config = model.config\n             model.to(torch_device)\n             model.eval()\n             with torch.no_grad():"
        },
        {
            "sha": "c76ece0d959096bd99add0273a8e87931aa8373a",
            "filename": "tests/models/idefics/test_modeling_idefics.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -502,7 +502,8 @@ def test_attention_outputs(self):\n             inputs_dict[\"output_attentions\"] = True\n             inputs_dict[\"output_hidden_states\"] = False\n             config.return_dict = True\n-            model = model_class(config)\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            config = model.config\n             model.to(torch_device)\n             model.eval()\n             with torch.no_grad():"
        },
        {
            "sha": "22e6217c72c1ec7e151e8438d6175090d19947dc",
            "filename": "tests/models/informer/test_modeling_informer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Finformer%2Ftest_modeling_informer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Finformer%2Ftest_modeling_informer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finformer%2Ftest_modeling_informer.py?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -384,7 +384,8 @@ def test_attention_outputs(self):\n             inputs_dict[\"output_attentions\"] = True\n             inputs_dict[\"output_hidden_states\"] = False\n             config.return_dict = True\n-            model = model_class(config)\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            config = model.config\n             model.to(torch_device)\n             model.eval()\n             with torch.no_grad():"
        },
        {
            "sha": "993db32378b058ea795cd16aec847273fbb5be70",
            "filename": "tests/models/jamba/test_modeling_jamba.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fjamba%2Ftest_modeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fjamba%2Ftest_modeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fjamba%2Ftest_modeling_jamba.py?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -452,7 +452,8 @@ def test_attention_outputs(self):\n             inputs_dict[\"output_attentions\"] = True\n             inputs_dict[\"output_hidden_states\"] = False\n             config.return_dict = True\n-            model = model_class(config)\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            config = model.config\n             model.to(torch_device)\n             model.eval()\n "
        },
        {
            "sha": "00cf7e59b6ea71b78d77869d5a3fda6b60e3ad6a",
            "filename": "tests/models/layoutlmv2/test_modeling_layoutlmv2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Flayoutlmv2%2Ftest_modeling_layoutlmv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Flayoutlmv2%2Ftest_modeling_layoutlmv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flayoutlmv2%2Ftest_modeling_layoutlmv2.py?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -334,7 +334,8 @@ def test_attention_outputs(self):\n             inputs_dict[\"output_attentions\"] = True\n             inputs_dict[\"output_hidden_states\"] = False\n             config.return_dict = True\n-            model = model_class(config)\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            config = model.config\n             model.to(torch_device)\n             model.eval()\n             with torch.no_grad():"
        },
        {
            "sha": "60a10eb860b2e3b7b5b6724d8a675d640448e23f",
            "filename": "tests/models/led/test_modeling_led.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fled%2Ftest_modeling_led.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fled%2Ftest_modeling_led.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fled%2Ftest_modeling_led.py?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -405,7 +405,8 @@ def test_attention_outputs(self):\n             inputs_dict[\"output_attentions\"] = True\n             inputs_dict[\"output_hidden_states\"] = False\n             config.return_dict = True\n-            model = model_class(config)\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            config = model.config\n             model.to(torch_device)\n             model.eval()\n             with torch.no_grad():"
        },
        {
            "sha": "dd51475540c75d6c2c3902c8ea15bbc78b6cbf8e",
            "filename": "tests/models/luke/test_modeling_luke.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fluke%2Ftest_modeling_luke.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fluke%2Ftest_modeling_luke.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fluke%2Ftest_modeling_luke.py?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -758,7 +758,8 @@ def test_attention_outputs(self):\n             inputs_dict[\"output_attentions\"] = True\n             inputs_dict[\"output_hidden_states\"] = False\n             config.return_dict = True\n-            model = model_class(config)\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            config = model.config\n             model.to(torch_device)\n             model.eval()\n             with torch.no_grad():"
        },
        {
            "sha": "2f30d4dc3c6c7ceb021efaccb48b920da02c5a4a",
            "filename": "tests/models/maskformer/test_modeling_maskformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fmaskformer%2Ftest_modeling_maskformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fmaskformer%2Ftest_modeling_maskformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmaskformer%2Ftest_modeling_maskformer.py?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -299,7 +299,8 @@ def test_attention_outputs(self):\n             inputs_dict[\"output_attentions\"] = True\n             inputs_dict[\"output_hidden_states\"] = False\n             config.return_dict = True\n-            model = model_class(config)\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            config = model.config\n             model.to(torch_device)\n             model.eval()\n             with torch.no_grad():"
        },
        {
            "sha": "6001f2058d99ec0c3ce21023b4a06f7ab5f4dabb",
            "filename": "tests/models/moonshine/test_modeling_moonshine.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fmoonshine%2Ftest_modeling_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fmoonshine%2Ftest_modeling_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmoonshine%2Ftest_modeling_moonshine.py?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -167,7 +167,8 @@ def test_attention_outputs(self):\n             inputs_dict[\"output_attentions\"] = True\n             inputs_dict[\"output_hidden_states\"] = False\n             config.return_dict = True\n-            model = model_class(config)\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            config = model.config\n             model.to(torch_device)\n             model.eval()\n "
        },
        {
            "sha": "291e2709bbb48bc6985262c3f506990c0b2a4f0b",
            "filename": "tests/models/omdet_turbo/test_modeling_omdet_turbo.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fomdet_turbo%2Ftest_modeling_omdet_turbo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fomdet_turbo%2Ftest_modeling_omdet_turbo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fomdet_turbo%2Ftest_modeling_omdet_turbo.py?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -462,7 +462,8 @@ def test_attention_outputs(self):\n             inputs_dict[\"output_attentions\"] = True\n             inputs_dict[\"output_hidden_states\"] = False\n             config.return_dict = True\n-            model = model_class(config)\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            config = model.config\n             model.to(torch_device)\n             model.eval()\n             with torch.no_grad():"
        },
        {
            "sha": "388661ac1fd346d54fe37ad0e3b4cae1e97fa1c7",
            "filename": "tests/models/paligemma/test_modeling_paligemma.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -355,7 +355,8 @@ def test_attention_mask_with_token_types(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n \n         for model_class in self.all_model_classes:\n-            model = model_class(config)\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            config = model.config\n             model.to(torch_device)\n             model.eval()\n "
        },
        {
            "sha": "20cdac98fbeddfcf34dda9df3c2ef57459673f94",
            "filename": "tests/models/pegasus_x/test_modeling_pegasus_x.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fpegasus_x%2Ftest_modeling_pegasus_x.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fpegasus_x%2Ftest_modeling_pegasus_x.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpegasus_x%2Ftest_modeling_pegasus_x.py?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -304,7 +304,8 @@ def test_attention_outputs(self):\n             inputs_dict[\"output_attentions\"] = True\n             inputs_dict[\"output_hidden_states\"] = False\n             config.return_dict = True\n-            model = model_class(config)\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            config = model.config\n             model.to(torch_device)\n             model.eval()\n             with torch.no_grad():"
        },
        {
            "sha": "fddf1db71a304f9a51a67f3a5f8ce42ab7e69ef5",
            "filename": "tests/models/perceiver/test_modeling_perceiver.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fperceiver%2Ftest_modeling_perceiver.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fperceiver%2Ftest_modeling_perceiver.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fperceiver%2Ftest_modeling_perceiver.py?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -456,7 +456,8 @@ def test_attention_outputs(self):\n             inputs_dict[\"output_attentions\"] = True\n             inputs_dict[\"output_hidden_states\"] = False\n             config.return_dict = True\n-            model = model_class(config)\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            config = model.config\n             model.to(torch_device)\n             model.eval()\n             with torch.no_grad():"
        },
        {
            "sha": "fa2938160d73406b171928d7ac8d2f5c064b6992",
            "filename": "tests/models/rt_detr/test_modeling_rt_detr.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Frt_detr%2Ftest_modeling_rt_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Frt_detr%2Ftest_modeling_rt_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frt_detr%2Ftest_modeling_rt_detr.py?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -335,7 +335,8 @@ def test_attention_outputs(self):\n             inputs_dict[\"output_attentions\"] = True\n             inputs_dict[\"output_hidden_states\"] = False\n             config.return_dict = True\n-            model = model_class(config)\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            config = model.config\n             model.to(torch_device)\n             model.eval()\n             with torch.no_grad():"
        },
        {
            "sha": "a78f11ea46c85041acee1c0381a8e246927e03a9",
            "filename": "tests/models/rt_detr_v2/test_modeling_rt_detr_v2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Frt_detr_v2%2Ftest_modeling_rt_detr_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Frt_detr_v2%2Ftest_modeling_rt_detr_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frt_detr_v2%2Ftest_modeling_rt_detr_v2.py?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -339,7 +339,8 @@ def test_attention_outputs(self):\n             inputs_dict[\"output_attentions\"] = True\n             inputs_dict[\"output_hidden_states\"] = False\n             config.return_dict = True\n-            model = model_class(config)\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            config = model.config\n             model.to(torch_device)\n             model.eval()\n             with torch.no_grad():"
        },
        {
            "sha": "4e92baf2d3d2eb04a3fe71ab26756e0dd9df5bd4",
            "filename": "tests/models/rwkv/test_modeling_rwkv.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Frwkv%2Ftest_modeling_rwkv.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Frwkv%2Ftest_modeling_rwkv.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frwkv%2Ftest_modeling_rwkv.py?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -312,7 +312,8 @@ def test_attention_outputs(self):\n             inputs_dict[\"output_attentions\"] = True\n             inputs_dict[\"output_hidden_states\"] = False\n             config.return_dict = True\n-            model = model_class(config)\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            config = model.config\n             model.to(torch_device)\n             model.eval()\n "
        },
        {
            "sha": "978323413ca5136e801d49bd1dc733cf98ffa043",
            "filename": "tests/models/sam/test_modeling_sam.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fsam%2Ftest_modeling_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fsam%2Ftest_modeling_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam%2Ftest_modeling_sam.py?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -202,7 +202,8 @@ def test_attention_outputs(self):\n             inputs_dict[\"output_attentions\"] = True\n             inputs_dict[\"output_hidden_states\"] = False\n             config.return_dict = True\n-            model = model_class(config)\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            config = model.config\n             model.to(torch_device)\n             model.eval()\n             with torch.no_grad():\n@@ -590,7 +591,8 @@ def test_attention_outputs(self):\n             inputs_dict[\"output_attentions\"] = True\n             inputs_dict[\"output_hidden_states\"] = False\n             config.return_dict = True\n-            model = model_class(config)\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            config = model.config\n             model.to(torch_device)\n             model.eval()\n             with torch.no_grad():"
        },
        {
            "sha": "6b4cd75467f83c5742795e02a6b08b608ea6214b",
            "filename": "tests/models/sam/test_modeling_tf_sam.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fsam%2Ftest_modeling_tf_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fsam%2Ftest_modeling_tf_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam%2Ftest_modeling_tf_sam.py?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -562,7 +562,8 @@ def test_attention_outputs(self):\n             inputs_dict[\"output_attentions\"] = True\n             inputs_dict[\"output_hidden_states\"] = False\n             config.return_dict = True\n-            model = model_class(config)\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            config = model.config\n             outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n \n             vision_attentions = outputs.vision_attentions"
        },
        {
            "sha": "915ce022fc04df0bbb01acd59034b807c1311c3c",
            "filename": "tests/models/sam_hq/test_modeling_sam_hq.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fsam_hq%2Ftest_modeling_sam_hq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fsam_hq%2Ftest_modeling_sam_hq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam_hq%2Ftest_modeling_sam_hq.py?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -210,7 +210,8 @@ def test_attention_outputs(self):\n             inputs_dict[\"output_attentions\"] = True\n             inputs_dict[\"output_hidden_states\"] = False\n             config.return_dict = True\n-            model = model_class(config)\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            config = model.config\n             model.to(torch_device)\n             model.eval()\n             with torch.no_grad():\n@@ -637,7 +638,8 @@ def test_attention_outputs(self):\n             inputs_dict[\"output_attentions\"] = True\n             inputs_dict[\"output_hidden_states\"] = False\n             config.return_dict = True\n-            model = model_class(config)\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            config = model.config\n             model.to(torch_device)\n             model.eval()\n             with torch.no_grad():"
        },
        {
            "sha": "e802e8cfb921f031fef72d4ba4057ea5e37e4141",
            "filename": "tests/models/seamless_m4t/test_modeling_seamless_m4t.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fseamless_m4t%2Ftest_modeling_seamless_m4t.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fseamless_m4t%2Ftest_modeling_seamless_m4t.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fseamless_m4t%2Ftest_modeling_seamless_m4t.py?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -475,7 +475,8 @@ def test_attention_outputs(self):\n             inputs_dict[\"output_attentions\"] = True\n             inputs_dict[\"output_hidden_states\"] = False\n             config.return_dict = True\n-            model = model_class(config)\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            config = model.config\n             model.to(torch_device)\n             model.eval()\n             with torch.no_grad():"
        },
        {
            "sha": "75ff7edccbdd879a53b55d527fe2c33699a5c6be",
            "filename": "tests/models/seamless_m4t_v2/test_modeling_seamless_m4t_v2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fseamless_m4t_v2%2Ftest_modeling_seamless_m4t_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fseamless_m4t_v2%2Ftest_modeling_seamless_m4t_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fseamless_m4t_v2%2Ftest_modeling_seamless_m4t_v2.py?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -491,7 +491,8 @@ def test_attention_outputs(self):\n             inputs_dict[\"output_attentions\"] = True\n             inputs_dict[\"output_hidden_states\"] = False\n             config.return_dict = True\n-            model = model_class(config)\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            config = model.config\n             model.to(torch_device)\n             model.eval()\n             with torch.no_grad():"
        },
        {
            "sha": "cd75545c62aa42a21cf6fc93e325c290983b9a80",
            "filename": "tests/models/segformer/test_modeling_segformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fsegformer%2Ftest_modeling_segformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fsegformer%2Ftest_modeling_segformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsegformer%2Ftest_modeling_segformer.py?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -216,7 +216,8 @@ def test_attention_outputs(self):\n             inputs_dict[\"output_attentions\"] = True\n             inputs_dict[\"output_hidden_states\"] = False\n             config.return_dict = True\n-            model = model_class(config)\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            config = model.config\n             model.to(torch_device)\n             model.eval()\n             with torch.no_grad():"
        },
        {
            "sha": "593aca140666b1e6bd094bf293ab39bf1df18d1d",
            "filename": "tests/models/speech_to_text/test_modeling_speech_to_text.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fspeech_to_text%2Ftest_modeling_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fspeech_to_text%2Ftest_modeling_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fspeech_to_text%2Ftest_modeling_speech_to_text.py?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -434,7 +434,8 @@ def test_attention_outputs(self):\n             inputs_dict[\"output_attentions\"] = True\n             inputs_dict[\"output_hidden_states\"] = False\n             config.return_dict = True\n-            model = model_class(config)\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            config = model.config\n             model.to(torch_device)\n             model.eval()\n "
        },
        {
            "sha": "613081a82e07502668d49d920ce6175b47e4df2a",
            "filename": "tests/models/speech_to_text/test_modeling_tf_speech_to_text.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fspeech_to_text%2Ftest_modeling_tf_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fspeech_to_text%2Ftest_modeling_tf_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fspeech_to_text%2Ftest_modeling_tf_speech_to_text.py?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -323,7 +323,8 @@ def test_attention_outputs(self):\n             inputs_dict[\"output_attentions\"] = True\n             inputs_dict[\"output_hidden_states\"] = False\n             config.return_dict = True\n-            model = model_class(config)\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            config = model.config\n \n             subsampled_encoder_seq_length = model._get_feat_extract_output_lengths(encoder_seq_length)\n             subsampled_encoder_key_length = model._get_feat_extract_output_lengths(encoder_key_length)"
        },
        {
            "sha": "5fc1d6706666f7b8b580b261abafe598489436fb",
            "filename": "tests/models/speecht5/test_modeling_speecht5.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fspeecht5%2Ftest_modeling_speecht5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fspeecht5%2Ftest_modeling_speecht5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fspeecht5%2Ftest_modeling_speecht5.py?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -415,7 +415,8 @@ def test_attention_outputs(self):\n             inputs_dict[\"output_attentions\"] = True\n             inputs_dict[\"output_hidden_states\"] = False\n             config.return_dict = True\n-            model = model_class(config)\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            config = model.config\n             model.to(torch_device)\n             model.eval()\n \n@@ -1524,7 +1525,8 @@ def test_attention_outputs(self):\n             inputs_dict[\"output_attentions\"] = True\n             inputs_dict[\"output_hidden_states\"] = False\n             config.return_dict = True\n-            model = model_class(config)\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            config = model.config\n             model.to(torch_device)\n             model.eval()\n "
        },
        {
            "sha": "a8e35d429fb635c07369780fc8985f92465a10c8",
            "filename": "tests/models/swin/test_modeling_swin.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fswin%2Ftest_modeling_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fswin%2Ftest_modeling_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fswin%2Ftest_modeling_swin.py?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -303,7 +303,8 @@ def test_attention_outputs(self):\n             inputs_dict[\"output_attentions\"] = True\n             inputs_dict[\"output_hidden_states\"] = False\n             config.return_dict = True\n-            model = model_class(config)\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            config = model.config\n             model.to(torch_device)\n             model.eval()\n             with torch.no_grad():"
        },
        {
            "sha": "9f8376984fd8282a829635634c4393447202daf1",
            "filename": "tests/models/swin2sr/test_modeling_swin2sr.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fswin2sr%2Ftest_modeling_swin2sr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fswin2sr%2Ftest_modeling_swin2sr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fswin2sr%2Ftest_modeling_swin2sr.py?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -263,7 +263,8 @@ def test_attention_outputs(self):\n             inputs_dict[\"output_attentions\"] = True\n             inputs_dict[\"output_hidden_states\"] = False\n             config.return_dict = True\n-            model = model_class(config)\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            config = model.config\n             model.to(torch_device)\n             model.eval()\n             with torch.no_grad():"
        },
        {
            "sha": "69f0a77f3a25e248bcd5c158f4722b02006c778c",
            "filename": "tests/models/swinv2/test_modeling_swinv2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fswinv2%2Ftest_modeling_swinv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fswinv2%2Ftest_modeling_swinv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fswinv2%2Ftest_modeling_swinv2.py?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -286,7 +286,8 @@ def test_attention_outputs(self):\n             inputs_dict[\"output_attentions\"] = True\n             inputs_dict[\"output_hidden_states\"] = False\n             config.return_dict = True\n-            model = model_class(config)\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            config = model.config\n             model.to(torch_device)\n             model.eval()\n             with torch.no_grad():"
        },
        {
            "sha": "bac298a12edffe9d7be51bd0c79235931855b446",
            "filename": "tests/models/table_transformer/test_modeling_table_transformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Ftable_transformer%2Ftest_modeling_table_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Ftable_transformer%2Ftest_modeling_table_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftable_transformer%2Ftest_modeling_table_transformer.py?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -293,7 +293,8 @@ def test_attention_outputs(self):\n             inputs_dict[\"output_attentions\"] = True\n             inputs_dict[\"output_hidden_states\"] = False\n             config.return_dict = True\n-            model = model_class(config)\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            config = model.config\n             model.to(torch_device)\n             model.eval()\n             with torch.no_grad():"
        },
        {
            "sha": "02c7a1111c08930795f95e5a92449a1d03707e11",
            "filename": "tests/models/time_series_transformer/test_modeling_time_series_transformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Ftime_series_transformer%2Ftest_modeling_time_series_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Ftime_series_transformer%2Ftest_modeling_time_series_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftime_series_transformer%2Ftest_modeling_time_series_transformer.py?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -286,7 +286,8 @@ def test_attention_outputs(self):\n             inputs_dict[\"output_attentions\"] = True\n             inputs_dict[\"output_hidden_states\"] = False\n             config.return_dict = True\n-            model = model_class(config)\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            config = model.config\n             model.to(torch_device)\n             model.eval()\n             with torch.no_grad():"
        },
        {
            "sha": "4537003b099b4b5aab06a6a874140c071e0ee693",
            "filename": "tests/models/vilt/test_modeling_vilt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fvilt%2Ftest_modeling_vilt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fvilt%2Ftest_modeling_vilt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvilt%2Ftest_modeling_vilt.py?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -373,7 +373,8 @@ def test_attention_outputs(self):\n             inputs_dict[\"output_attentions\"] = True\n             inputs_dict[\"output_hidden_states\"] = False\n             config.return_dict = True\n-            model = model_class(config)\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            config = model.config\n             model.to(torch_device)\n             model.eval()\n             with torch.no_grad():"
        },
        {
            "sha": "8b715137a37f23bb00ebd51055e62019cdd90fcd",
            "filename": "tests/models/visual_bert/test_modeling_visual_bert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fvisual_bert%2Ftest_modeling_visual_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fvisual_bert%2Ftest_modeling_visual_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvisual_bert%2Ftest_modeling_visual_bert.py?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -412,7 +412,8 @@ def test_attention_outputs(self):\n             inputs_dict[\"output_attentions\"] = True\n             inputs_dict[\"output_hidden_states\"] = False\n             config.return_dict = True\n-            model = model_class(config)\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            config = model.config\n             model.to(torch_device)\n             model.eval()\n             with torch.no_grad():"
        },
        {
            "sha": "d4d3efe374886ae9f7813c6d1090185298ab77a5",
            "filename": "tests/models/vivit/test_modeling_vivit.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fvivit%2Ftest_modeling_vivit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fvivit%2Ftest_modeling_vivit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvivit%2Ftest_modeling_vivit.py?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -243,7 +243,8 @@ def test_attention_outputs(self):\n             inputs_dict[\"output_attentions\"] = True\n             inputs_dict[\"output_hidden_states\"] = False\n             config.return_dict = True\n-            model = model_class(config)\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            config = model.config\n             model.to(torch_device)\n             model.eval()\n             with torch.no_grad():"
        },
        {
            "sha": "161bb33a801d98e75e3464676aab71f5b08d38e8",
            "filename": "tests/models/whisper/test_modeling_whisper.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -676,7 +676,8 @@ def test_attention_outputs(self):\n             inputs_dict[\"output_attentions\"] = True\n             inputs_dict[\"output_hidden_states\"] = False\n             config.return_dict = True\n-            model = model_class(config)\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            config = model.config\n             model.to(torch_device)\n             model.eval()\n "
        },
        {
            "sha": "1a0c7dda6e0974c06137238d41ae84ad2d4e157d",
            "filename": "tests/models/x_clip/test_modeling_x_clip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fx_clip%2Ftest_modeling_x_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fx_clip%2Ftest_modeling_x_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fx_clip%2Ftest_modeling_x_clip.py?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -236,7 +236,8 @@ def test_attention_outputs(self):\n             inputs_dict[\"output_attentions\"] = True\n             inputs_dict[\"output_hidden_states\"] = False\n             config.return_dict = True\n-            model = model_class(config)\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            config = model.config\n             model.to(torch_device)\n             model.eval()\n             with torch.no_grad():"
        },
        {
            "sha": "3c2a78a87200b50135c205e25e59e879c2d3cda9",
            "filename": "tests/models/yolos/test_modeling_yolos.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fyolos%2Ftest_modeling_yolos.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fyolos%2Ftest_modeling_yolos.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fyolos%2Ftest_modeling_yolos.py?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -233,7 +233,8 @@ def test_attention_outputs(self):\n             inputs_dict[\"output_attentions\"] = True\n             inputs_dict[\"output_hidden_states\"] = False\n             config.return_dict = True\n-            model = model_class(config)\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            config = model.config\n             model.to(torch_device)\n             model.eval()\n             with torch.no_grad():"
        },
        {
            "sha": "2a142bfc73ee097ead0cff750bcf5424fbddf33e",
            "filename": "tests/models/zamba/test_modeling_zamba.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fzamba%2Ftest_modeling_zamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fzamba%2Ftest_modeling_zamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fzamba%2Ftest_modeling_zamba.py?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -401,7 +401,8 @@ def test_attention_outputs(self):\n             inputs_dict[\"output_attentions\"] = True\n             inputs_dict[\"output_hidden_states\"] = False\n             config.return_dict = True\n-            model = model_class(config)\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            config = model.config\n             model.to(torch_device)\n             model.eval()\n "
        },
        {
            "sha": "894cde8be33fda1d6f2c70a2d6695e0835c34789",
            "filename": "tests/models/zamba2/test_modeling_zamba2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fzamba2%2Ftest_modeling_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fmodels%2Fzamba2%2Ftest_modeling_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fzamba2%2Ftest_modeling_zamba2.py?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -422,7 +422,8 @@ def test_attention_outputs(self):\n             inputs_dict[\"output_attentions\"] = True\n             inputs_dict[\"output_hidden_states\"] = False\n             config.return_dict = True\n-            model = model_class(config)\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            config = model.config\n             model.to(torch_device)\n             model.eval()\n "
        },
        {
            "sha": "8a358f45de123ef1316885a0301c16ab421a32af",
            "filename": "tests/pipelines/test_pipelines_text_to_audio.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fpipelines%2Ftest_pipelines_text_to_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Fpipelines%2Ftest_pipelines_text_to_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_text_to_audio.py?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -259,6 +259,7 @@ def get_test_pipeline(\n         model_test_kwargs = {}\n         if model.can_generate():  # not all models in this pipeline can generate and, therefore, take `generate` kwargs\n             model_test_kwargs[\"max_new_tokens\"] = 5\n+        model.config._attn_implementation = \"eager\"\n         speech_generator = TextToAudioPipeline(\n             model=model,\n             tokenizer=tokenizer,"
        },
        {
            "sha": "4ddbbcb47f27a8c1438e3eac0a6bf26f50acf3ba",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -974,7 +974,8 @@ def test_attention_outputs(self):\n             inputs_dict[\"output_attentions\"] = True\n             inputs_dict[\"output_hidden_states\"] = False\n             config.return_dict = True\n-            model = model_class(config)\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            config = model.config\n             model.to(torch_device)\n             model.eval()\n             with torch.no_grad():\n@@ -1720,7 +1721,7 @@ def test_retain_grad_hidden_states_attentions(self):\n \n         # no need to test all models as different heads yield the same functionality\n         model_class = self.all_model_classes[0]\n-        model = model_class(config)\n+        model = model_class._from_config(config, attn_implementation=\"eager\")\n         model.to(torch_device)\n \n         inputs = self._prepare_for_class(inputs_dict, model_class)"
        },
        {
            "sha": "a34e9a5ea9a790bc5e556bc64c1758fd7a4dd21f",
            "filename": "tests/utils/test_configuration_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Futils%2Ftest_configuration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d45d89c4f1d3282fc216a96dc6830b88d41249/tests%2Futils%2Ftest_configuration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_configuration_utils.py?ref=f5d45d89c4f1d3282fc216a96dc6830b88d41249",
            "patch": "@@ -189,6 +189,7 @@ def test_config_common_kwargs_is_complete(self):\n         self.assertListEqual(\n             missing_keys,\n             [\n+                \"_output_attentions\",\n                 \"is_encoder_decoder\",\n                 \"_name_or_path\",\n                 \"_commit_hash\","
        }
    ],
    "stats": {
        "total": 301,
        "additions": 157,
        "deletions": 144
    }
}