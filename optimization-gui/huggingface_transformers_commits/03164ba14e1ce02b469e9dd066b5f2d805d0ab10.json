{
    "author": "Muennighoff",
    "message": "Add paper link (#33305)",
    "sha": "03164ba14e1ce02b469e9dd066b5f2d805d0ab10",
    "files": [
        {
            "sha": "5ebcf3f943b30b623bf8c0fd82dc99454fceada7",
            "filename": "docs/source/en/model_doc/olmoe.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/03164ba14e1ce02b469e9dd066b5f2d805d0ab10/docs%2Fsource%2Fen%2Fmodel_doc%2Folmoe.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/03164ba14e1ce02b469e9dd066b5f2d805d0ab10/docs%2Fsource%2Fen%2Fmodel_doc%2Folmoe.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Folmoe.md?ref=03164ba14e1ce02b469e9dd066b5f2d805d0ab10",
            "patch": "@@ -18,7 +18,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The OLMoE model was proposed in [OLMoE: Open Mixture-of-Experts Language Models](TODO) by Niklas Muennighoff, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Jacob Morrison, Sewon Min, Weijia Shi, Pete Walsh, Oyvind Tafjord, Nathan Lambert, Yuling Gu, Shane Arora, Akshita Bhagia, Dustin Schwenk, David Wadden, Alexander Wettig, Binyuan Hui, Tim Dettmers, Douwe Kiela, Ali Farhadi, Noah A. Smith, Pang Wei Koh, Amanpreet Singh, Hannaneh Hajishirzi.\n+The OLMoE model was proposed in [OLMoE: Open Mixture-of-Experts Language Models](https://arxiv.org/abs/2409.02060) by Niklas Muennighoff, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Jacob Morrison, Sewon Min, Weijia Shi, Pete Walsh, Oyvind Tafjord, Nathan Lambert, Yuling Gu, Shane Arora, Akshita Bhagia, Dustin Schwenk, David Wadden, Alexander Wettig, Binyuan Hui, Tim Dettmers, Douwe Kiela, Ali Farhadi, Noah A. Smith, Pang Wei Koh, Amanpreet Singh, Hannaneh Hajishirzi.\n \n OLMoE is a series of **O**pen **L**anguage **Mo**dels using sparse **M**ixture-**o**f-**E**xperts designed to enable the science of language models. We release all code, checkpoints, logs, and details involved in training these models.\n "
        }
    ],
    "stats": {
        "total": 2,
        "additions": 1,
        "deletions": 1
    }
}