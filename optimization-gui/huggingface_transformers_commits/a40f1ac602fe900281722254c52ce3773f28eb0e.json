{
    "author": "SunMarc",
    "message": "Fix couples of issues from #36335 (#36453)\n\n* fix\n\n* style\n\n* better allocation\n\n* fix\n\n* fix\n\n* style\n\n* revert disk\n\n* exit\n\n* style\n\n* return if nothing to cache\n\n* dtensor guard\n\n* fix regressiion\n\n* fix regression\n\n* fix\n\n* fix",
    "sha": "a40f1ac602fe900281722254c52ce3773f28eb0e",
    "files": [
        {
            "sha": "73328e3af59eb12f8864235991b1616b6c3e0c9b",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 158,
            "deletions": 137,
            "changes": 295,
            "blob_url": "https://github.com/huggingface/transformers/blob/a40f1ac602fe900281722254c52ce3773f28eb0e/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a40f1ac602fe900281722254c52ce3773f28eb0e/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=a40f1ac602fe900281722254c52ce3773f28eb0e",
            "patch": "@@ -41,7 +41,6 @@\n from huggingface_hub import split_torch_state_dict_into_shards\n from packaging import version\n from torch import Tensor, nn\n-from torch.distributed.tensor import DTensor, Shard\n from torch.distributions import constraints\n from torch.nn import CrossEntropyLoss, Identity\n from torch.utils.checkpoint import checkpoint\n@@ -67,7 +66,6 @@\n     translate_to_torch_parallel_style,\n )\n from .quantizers import AutoHfQuantizer, HfQuantizer\n-from .quantizers.quantizers_utils import get_module_from_name\n from .safetensors_conversion import auto_conversion\n from .utils import (\n     ACCELERATE_MIN_VERSION,\n@@ -181,6 +179,9 @@ def is_local_dist_rank_0():\n if is_peft_available():\n     from .utils import find_adapter_config_file\n \n+if is_torch_greater_or_equal(\"2.5\"):\n+    from torch.distributed.tensor import DTensor, Shard\n+\n SpecificPreTrainedModelType = TypeVar(\"SpecificPreTrainedModelType\", bound=\"PreTrainedModel\")\n \n TORCH_INIT_FUNCTIONS = {\n@@ -702,7 +703,7 @@ def _find_identical(tensors: List[Set[str]], state_dict: Dict[str, torch.Tensor]\n     return shared_tensors, identical\n \n \n-def find_submodule_and_param_name(model, long_key, start_prefix):\n+def find_submodule_and_param_name(model, long_key, start_prefix=\"\"):\n     \"\"\"\n     A helper util to find the last sub-module and the param/buffer name. If `start_prefix` is supplied it'll be removed\n     from the start of the key\n@@ -767,7 +768,6 @@ def _load_state_dict_into_meta_model(\n     is_safetensors=False,\n     keep_in_fp32_modules=None,\n     unexpected_keys=None,  # passing `unexpected` for cleanup from quantization items\n-    pretrained_model_name_or_path=None,  # for flagging the user when the model contains renamed keys\n     device_mesh=None,\n     shard_file=None,\n ):\n@@ -786,145 +786,153 @@ def _load_state_dict_into_meta_model(\n     if device_map is not None and device_map.get(\"\", None) is not None:\n         tensor_device = device_map[\"\"].index if isinstance(device_map[\"\"], torch.device) else device_map[\"\"]\n \n-    with safe_open(shard_file, framework=\"pt\", device=tensor_device) as file_pointer:\n-        error_msgs = []\n+    device_map_regex = \"|\".join(sorted(device_map.keys(), reverse=True))\n \n-        is_quantized = hf_quantizer is not None\n+    # we need this later to initialize tensor parallelism\n+    if device_mesh is not None:\n+        full_tp_plan = model.config.base_model_tp_plan\n+        for submodule in model.modules():\n+            full_tp_plan.update(getattr(submodule, \"_tp_plan\", {}))\n \n-        is_torch_e4m3fn_available = hasattr(torch, \"float8_e4m3fn\")\n+    file_pointer = None\n+    bin_state_dict = None\n+    if shard_file.endswith(\".safetensors\"):\n+        file_pointer = safe_open(shard_file, framework=\"pt\", device=tensor_device)\n+    else:\n+        bin_state_dict = load_state_dict(shard_file, map_location=\"cpu\")\n \n-        # we need this later to initialize tensor parallelism\n-        if device_mesh is not None:\n-            full_tp_plan = model.config.base_model_tp_plan\n-            for submodule in model.modules():\n-                full_tp_plan.update(getattr(submodule, \"_tp_plan\", {}))\n-\n-        for serialized_param_name, empty_param in state_dict.items():\n-            # param_name is the raw, serialized name\n-            # new_param_name is the model's equivalent\n-            module_name, _ = model.rename_key(serialized_param_name)\n-            if module_name not in expected_keys:\n-                continue\n-            layer, param_type = module_name.rsplit(\".\", 1)\n-\n-            # param name needs to stay untouched as it's in the file\n-            param = file_pointer.get_slice(serialized_param_name)\n-            # We convert floating dtypes to the `dtype` passed except for float8_e4m3fn type. We also want to keep the buffers/params\n-            # in int/uint/bool and not cast them.\n-            param_casting_dtype = None\n-            is_param_float8_e4m3fn = is_torch_e4m3fn_available and empty_param.dtype == torch.float8_e4m3fn\n-            if dtype is not None and empty_param.dtype.is_floating_point and not is_param_float8_e4m3fn:\n-                if (\n-                    keep_in_fp32_modules is not None\n-                    and keep_in_fp32_modules.search(module_name)\n-                    and dtype == torch.float16\n-                ):\n-                    param_casting_dtype = torch.float32\n-                else:\n-                    param_casting_dtype = dtype\n+    error_msgs = []\n \n-            if device_mesh is not None:  # In this case, the param is already on the correct device!\n-                try:\n-                    module_to_tp: torch.nn.Module = model.get_submodule(layer)\n-                except Exception:\n-                    raise ValueError(\n-                        \"The config tp plan is wrong because the layer is not a liner layer, nor an embedding\"\n-                    )\n-                current_module_plan = None\n-                full_tp_plan_ = \"|\".join(full_tp_plan.keys()).replace(\"*\", \"[0-9]+\")\n-                if plan := re.search(full_tp_plan_, module_name):\n-                    match = re.sub(\"[0-9]+\", \"*\", plan[0])\n-                    current_module_plan = full_tp_plan[match]\n-\n-                if current_module_plan is not None:\n-                    tp_layer = translate_to_torch_parallel_style(current_module_plan)\n-                    rank = tensor_device\n-                    row, col = empty_param.shape\n-                    if \"rowwise\" == current_module_plan:\n-                        param = param[:, rank * (col // device_mesh.size()) : (rank + 1) * (col // device_mesh.size())]\n-                        shard = Shard(1)\n-                        tp_layer.desired_input_layouts = (Shard(-1),)\n-                    elif \"colwise\" == current_module_plan:\n-                        param = param[rank * (row // device_mesh.size()) : (rank + 1) * (row // device_mesh.size()), :]\n-                        shard = Shard(0)\n-                    else:\n-                        param = param[rank * (row // device_mesh.size()) : (rank + 1) * (row // device_mesh.size()), :]\n-                        shard = Shard(0)\n-                    if param_casting_dtype is not None and param_casting_dtype != empty_param.dtype:\n-                        param = param.to(param_casting_dtype)\n-                    local_parameter = DTensor.from_local(\n-                        param,\n-                        device_mesh=device_mesh,\n-                        placements=[shard] * device_mesh.ndim,\n-                    )\n-                    if isinstance(module_to_tp.weight, nn.Parameter):\n-                        local_parameter = torch.nn.Parameter(local_parameter)\n-                    module_to_tp.weight = local_parameter\n-                    input_fn = partial(\n-                        tp_layer._prepare_input_fn, tp_layer.input_layouts, tp_layer.desired_input_layouts\n-                    )\n-                    output_fn = partial(\n-                        tp_layer._prepare_output_fn, tp_layer.output_layouts, tp_layer.use_local_output\n-                    )\n-                    distribute_module(module_to_tp, device_mesh, None, input_fn, output_fn)\n-                else:\n-                    module_to_tp.load_state_dict({param_type: param[:]}, False, True)\n+    is_quantized = hf_quantizer is not None\n+\n+    is_torch_e4m3fn_available = hasattr(torch, \"float8_e4m3fn\")\n \n+    for serialized_param_name, empty_param in state_dict.items():\n+        # serialized_param_name is the raw, serialized name\n+        # fixed_param_name is the model's equivalent\n+        fixed_param_name, _ = model.rename_key(serialized_param_name)\n+\n+        if fixed_param_name not in expected_keys:\n+            continue\n+\n+        # we need to use serialized_param_name as file pointer is untouched\n+        param = (\n+            file_pointer.get_slice(serialized_param_name)\n+            if shard_file.endswith(\".safetensors\")\n+            else bin_state_dict[serialized_param_name]\n+        )\n+        # We convert floating dtypes to the `dtype` passed except for float8_e4m3fn type. We also want to keep the buffers/params\n+        # in int/uint/bool and not cast them.\n+        param_casting_dtype = None\n+        is_param_float8_e4m3fn = is_torch_e4m3fn_available and empty_param.dtype == torch.float8_e4m3fn\n+\n+        if dtype is not None and empty_param.dtype.is_floating_point and not is_param_float8_e4m3fn:\n+            if (\n+                keep_in_fp32_modules is not None\n+                and keep_in_fp32_modules.search(fixed_param_name)\n+                and dtype == torch.float16\n+            ):\n+                param_casting_dtype = torch.float32\n             else:\n-                if device_map is None:\n-                    param_device = \"cpu\"\n+                param_casting_dtype = dtype\n+\n+        if device_mesh is not None:  # In this case, the param is already on the correct device!\n+            module_to_tp, param_type = find_submodule_and_param_name(model, fixed_param_name)\n+            current_module_plan = None\n+            full_tp_plan_ = \"|\".join(full_tp_plan.keys()).replace(\"*\", \"[0-9]+\")\n+            if plan := re.search(full_tp_plan_, fixed_param_name):\n+                match = re.sub(\"[0-9]+\", \"*\", plan[0])\n+                current_module_plan = full_tp_plan[match]\n+\n+            if current_module_plan is not None:\n+                tp_layer = translate_to_torch_parallel_style(current_module_plan)\n+                rank = tensor_device\n+                row, col = empty_param.shape\n+                if \"rowwise\" == current_module_plan:\n+                    param = param[:, rank * (col // device_mesh.size()) : (rank + 1) * (col // device_mesh.size())]\n+                    shard = Shard(1)\n+                    tp_layer.desired_input_layouts = (Shard(-1),)\n+                elif \"colwise\" == current_module_plan:\n+                    param = param[rank * (row // device_mesh.size()) : (rank + 1) * (row // device_mesh.size()), :]\n+                    shard = Shard(0)\n                 else:\n-                    module_name = module_name.rsplit(\".\", 1)[0]\n-                    device_map_regex = \"|\".join(device_map.keys())\n-                    module_layer = re.search(device_map_regex, module_name)\n-                    if module_name == \"\" or device_map_regex is None:\n-                        raise ValueError(\n-                            f\"`device_map` is used, but {module_name} doesn't have any device set. {device_map}\"\n-                        )\n-                    else:\n-                        param_device = device_map[module_layer.group()]\n-\n-                if param_device == \"disk\" and not is_safetensors:\n-                    offload_index = offload_weight(param[:], module_name, offload_folder, offload_index)\n-                elif param_device == \"cpu\" and state_dict_index is not None:\n-                    state_dict_index = offload_weight(param[:], module_name, state_dict_folder, state_dict_index)\n-                elif (\n-                    not is_quantized\n-                    or (not hf_quantizer.requires_parameters_quantization)\n-                    or (\n-                        not hf_quantizer.check_quantized_param(\n-                            model, param, module_name, state_dict, param_device=param_device, device_map=device_map\n-                        )\n-                    )\n-                ):\n-                    if is_fsdp_enabled():\n-                        param_device = \"cpu\" if is_local_dist_rank_0() else \"meta\"\n-                    module = model.get_submodule(layer)\n-                    if param_casting_dtype is not None and param_casting_dtype != empty_param.dtype:\n-                        param = param[:].to(param_casting_dtype)\n-                    module.load_state_dict(\n-                        {param_type: param[:].to(param_device)},\n-                        False,\n-                        True,\n-                    )\n+                    param = param[rank * (row // device_mesh.size()) : (rank + 1) * (row // device_mesh.size()), :]\n+                    shard = Shard(0)\n+                if param_casting_dtype is not None and param_casting_dtype != empty_param.dtype:\n+                    param = param.to(param_casting_dtype)\n+                local_parameter = DTensor.from_local(\n+                    param,\n+                    device_mesh=device_mesh,\n+                    placements=[shard] * device_mesh.ndim,\n+                )\n+                if isinstance(module_to_tp.weight, nn.Parameter):\n+                    local_parameter = torch.nn.Parameter(local_parameter)\n+                module_to_tp.weight = local_parameter\n+                input_fn = partial(tp_layer._prepare_input_fn, tp_layer.input_layouts, tp_layer.desired_input_layouts)\n+                output_fn = partial(tp_layer._prepare_output_fn, tp_layer.output_layouts, tp_layer.use_local_output)\n+                distribute_module(module_to_tp, device_mesh, None, input_fn, output_fn)\n+            else:\n+                module_to_tp.load_state_dict({param_type: param[:]}, strict=False, assign=True)\n+\n+        else:\n+            if device_map is None:\n+                param_device = \"cpu\"\n+            else:\n+                module_layer = re.search(device_map_regex, fixed_param_name)\n+                if not module_layer:\n+                    raise ValueError(f\"{fixed_param_name} doesn't have any device set.\")\n                 else:\n-                    hf_quantizer.create_quantized_param(\n-                        model, param[:], module_name, param_device, state_dict, unexpected_keys\n+                    param_device = device_map[module_layer.group()]\n+\n+            if param_device == \"disk\":\n+                if not is_safetensors:\n+                    offload_index = offload_weight(param[:], fixed_param_name, offload_folder, offload_index)\n+            elif param_device == \"cpu\" and state_dict_index is not None:\n+                state_dict_index = offload_weight(param[:], fixed_param_name, state_dict_folder, state_dict_index)\n+            elif (\n+                not is_quantized\n+                or (not hf_quantizer.requires_parameters_quantization)\n+                or (\n+                    not hf_quantizer.check_quantized_param(\n+                        model,\n+                        param,\n+                        fixed_param_name,\n+                        state_dict,\n+                        param_device=param_device,\n+                        device_map=device_map,\n                     )\n-                    # For quantized modules with FSDP/DeepSpeed Stage 3, we need to quantize the parameter on the GPU\n-                    # and then cast it to CPU to avoid excessive memory usage on each GPU\n-                    # in comparison to the sharded model across GPUs.\n-                    if is_fsdp_enabled() or is_deepspeed_zero3_enabled():\n-                        module, tensor_name = get_module_from_name(model, module_name)\n-                        value = getattr(module, tensor_name)\n-                        param_to = \"cpu\"\n-                        if is_fsdp_enabled() and not is_local_dist_rank_0():\n-                            param_to = \"meta\"\n-                        val_kwargs = {}\n-                        if hasattr(module, \"weight\") and module.weight.__class__.__name__ == \"Int8Params\":\n-                            val_kwargs[\"requires_grad\"] = False\n-                        value = type(value)(value.data.to(param_to), **val_kwargs, **value.__dict__)\n-                        setattr(module, tensor_name, value)\n+                )\n+            ):\n+                if is_fsdp_enabled():\n+                    param_device = \"cpu\" if is_local_dist_rank_0() else \"meta\"\n+                module, param_type = find_submodule_and_param_name(model, fixed_param_name)\n+                if param_casting_dtype is not None and param_casting_dtype != empty_param.dtype:\n+                    param = param[:].to(param_casting_dtype)\n+                module.load_state_dict(\n+                    {param_type: param[:].to(param_device)},\n+                    strict=False,\n+                    assign=True,\n+                )\n+            else:\n+                hf_quantizer.create_quantized_param(\n+                    model, param[:], fixed_param_name, param_device, state_dict, unexpected_keys\n+                )\n+                # For quantized modules with FSDP/DeepSpeed Stage 3, we need to quantize the parameter on the GPU\n+                # and then cast it to CPU to avoid excessive memory usage on each GPU\n+                # in comparison to the sharded model across GPUs.\n+                if is_fsdp_enabled() or is_deepspeed_zero3_enabled():\n+                    module, param_type = find_submodule_and_param_name(model, fixed_param_name)\n+                    value = getattr(module, param_type)\n+                    param_to = \"cpu\"\n+                    if is_fsdp_enabled() and not is_local_dist_rank_0():\n+                        param_to = \"meta\"\n+                    val_kwargs = {}\n+                    if hasattr(module, \"weight\") and module.weight.__class__.__name__ == \"Int8Params\":\n+                        val_kwargs[\"requires_grad\"] = False\n+                    value = type(value)(value.data.to(param_to), **val_kwargs, **value.__dict__)\n+                    setattr(module, param_type, value)\n+    if file_pointer is not None:\n+        file_pointer.__exit__(None, None, None)\n \n     return error_msgs, offload_index, state_dict_index\n \n@@ -4966,7 +4974,7 @@ def _load_pretrained_model(\n                     ignore_mismatched_sizes,\n                     prefix,\n                 )\n-                if low_cpu_mem_usage and shard_file.endswith(\".safetensors\"):\n+                if low_cpu_mem_usage:\n                     if is_fsdp_enabled() and not is_local_dist_rank_0() and not is_quantized:\n                         for key, param in model_to_load.state_dict().items():\n                             if param.device == torch.device(\"meta\"):\n@@ -5840,18 +5848,31 @@ def caching_allocator_warmup(model: PreTrainedModel, expanded_device_map: Dict,\n     accelerator_device_map = {\n         param: torch.device(device) for param, device in expanded_device_map.items() if device not in [\"cpu\", \"disk\"]\n     }\n+    if not len(accelerator_device_map):\n+        return\n+\n     parameter_count = defaultdict(lambda: 0)\n+    allocation_factor = 1\n+    if torch.distributed.is_initialized() or len(set(accelerator_device_map.values())) >= 2:\n+        allocation_factor = 2\n+\n     for param_name, device in accelerator_device_map.items():\n         try:\n             param = model.get_parameter(param_name)\n         except AttributeError:\n             param = model.get_buffer(param_name)\n-        parameter_count[device] += int(math.prod(param.shape) * 2)\n+        parameter_count[device] += int(math.prod(param.shape) * allocation_factor)\n \n     dtype = dtype if dtype is not None else torch.float32\n+\n     # This will kick off the caching allocator to avoid having to Malloc afterwards\n     for device, param_count in parameter_count.items():\n-        _ = torch.empty(int(param_count), dtype=dtype, device=device, requires_grad=False)\n+        max_memory_device = None\n+        if device.type == \"cuda\":\n+            max_memory_device = torch.cuda.mem_get_info(device.index)[0]\n+        # allocate only if we have enough memory\n+        if max_memory_device is None or max_memory_device > param_count * dtype_byte_size(dtype):\n+            _ = torch.empty(param_count, dtype=dtype, device=device, requires_grad=False)\n \n \n def get_disk_only_shard_files(device_map, sharded_metadata, start_prefix):"
        }
    ],
    "stats": {
        "total": 295,
        "additions": 158,
        "deletions": 137
    }
}