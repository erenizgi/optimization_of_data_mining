{
    "author": "saswatmeher",
    "message": "chore: update model card for SigLIP (#37585)\n\n* edit siglip model card\n\n* fix syntax\n\n* Update docs/source/en/model_doc/siglip.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/siglip.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/siglip.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/siglip.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/siglip.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/siglip.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* address comments\n\n---------\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "27a25bee4fcb865e8799ba026f1ea4455f2cca98",
    "files": [
        {
            "sha": "e443a6f0cbd616860f8b67a3cb7decb83168309e",
            "filename": "docs/source/en/model_doc/siglip.md",
            "status": "modified",
            "additions": 81,
            "deletions": 149,
            "changes": 230,
            "blob_url": "https://github.com/huggingface/transformers/blob/27a25bee4fcb865e8799ba026f1ea4455f2cca98/docs%2Fsource%2Fen%2Fmodel_doc%2Fsiglip.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/27a25bee4fcb865e8799ba026f1ea4455f2cca98/docs%2Fsource%2Fen%2Fmodel_doc%2Fsiglip.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsiglip.md?ref=27a25bee4fcb865e8799ba026f1ea4455f2cca98",
            "patch": "@@ -14,184 +14,116 @@ rendered properly in your Markdown viewer.\n \n -->\n \n-# SigLIP\n-\n-<div class=\"flex flex-wrap space-x-1\">\n-<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n-<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+            <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+            <img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+            <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+    </div>\n </div>\n \n-## Overview\n-\n-The SigLIP model was proposed in [Sigmoid Loss for Language Image Pre-Training](https://arxiv.org/abs/2303.15343) by Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, Lucas Beyer. SigLIP proposes to replace the loss function used in [CLIP](clip) by a simple pairwise sigmoid loss. This results in better performance in terms of zero-shot classification accuracy on ImageNet.\n-\n-The abstract from the paper is the following:\n-\n-*We propose a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP). Unlike standard contrastive learning with softmax normalization, the sigmoid loss operates solely on image-text pairs and does not require a global view of the pairwise similarities for normalization. The sigmoid loss simultaneously allows further scaling up the batch size, while also performing better at smaller batch sizes. Combined with Locked-image Tuning, with only four TPUv4 chips, we train a SigLiT model that achieves 84.5% ImageNet zero-shot accuracy in two days. The disentanglement of the batch size from the loss further allows us to study the impact of examples vs pairs and negative to positive ratio. Finally, we push the batch size to the extreme, up to one million, and find that the benefits of growing batch size quickly diminish, with a more reasonable batch size of 32k being sufficient.*\n-\n-## Usage tips\n-\n-- Usage of SigLIP is similar to [CLIP](clip). The main difference is the training loss, which does not require a global view of all the pairwise similarities of images and texts within a batch. One needs to apply the sigmoid activation function to the logits, rather than the softmax.\n-- Training is supported but does not use `torch.distributed` utilities which may limit the scalability of batch size. However, DDP and FDSP works on single-node multi-gpu setup.\n-- When using the standalone [`SiglipTokenizer`] or [`SiglipProcessor`], make sure to pass `padding=\"max_length\"` as that's how the model was trained.\n-- To get the same results as the pipeline, a prompt template of \"This is a photo of {label}.\" should be used.\n-\n-<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/siglip_table.jpeg\"\n-alt=\"drawing\" width=\"600\"/>\n+# SigLIP\n \n-<small> SigLIP evaluation results compared to CLIP. Taken from the <a href=\"https://arxiv.org/abs/2303.15343\">original paper</a>.</small>\n+[SigLIP](https://huggingface.co/papers/2303.15343) is a multimodal image-text model similar to [CLIP](clip). It uses separate image and text encoders to generate representations for both modalities.\n \n-This model was contributed by [nielsr](https://huggingface.co/nielsr).\n-The original code can be found [here](https://github.com/google-research/big_vision/tree/main).\n+Unlike CLIP, SigLIP employs a pairwise sigmoid loss on image-text pairs during training. This training loss eliminates the need for a global view of all pairwise similarities between images and texts within a batch. Consequently, it enables more efficient scaling to larger batch sizes while also delivering superior performance with smaller batch sizes.\n \n-## Usage example\n+You can find all the original SigLIP checkpoints under the [SigLIP](https://huggingface.co/collections/google/siglip-659d5e62f0ae1a57ae0e83ba) collection.\n \n-There are 2 main ways to use SigLIP: either using the pipeline API, which abstracts away all the complexity for you, or by using the `SiglipModel` class yourself.\n \n-### Pipeline API\n+> [!TIP]\n+> Click on the SigLIP models in the right sidebar for more examples of how to apply SigLIP to different image and text tasks.\n \n-The pipeline allows to use the model in a few lines of code:\n+The example below demonstrates how to generate similarity scores between texts and image(s) with [`Pipeline`] or the [`AutoModel`] class.\n \n-```python\n->>> from transformers import pipeline\n->>> from PIL import Image\n->>> import requests\n+<hfoptions id=\"usage\">\n+<hfoption id=\"Pipeline\">\n \n->>> # load pipe\n->>> image_classifier = pipeline(task=\"zero-shot-image-classification\", model=\"google/siglip-base-patch16-224\")\n+```py\n+import torch\n+from transformers import pipeline\n \n->>> # load image\n->>> url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n->>> image = Image.open(requests.get(url, stream=True).raw)\n+image = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n+candidate_labels = [\"a Pallas cat\", \"a lion\", \"a Siberian tiger\"]\n \n->>> # inference\n->>> candidate_labels = [\"2 cats\", \"a plane\", \"a remote\"]\n->>> outputs = image_classifier(image, candidate_labels=candidate_labels)\n->>> outputs = [{\"score\": round(output[\"score\"], 4), \"label\": output[\"label\"] } for output in outputs]\n->>> print(outputs)\n-[{'score': 0.1979, 'label': '2 cats'}, {'score': 0.0, 'label': 'a remote'}, {'score': 0.0, 'label': 'a plane'}]\n+pipeline = pipeline(task=\"zero-shot-image-classification\", model=\"google/siglip-base-patch16-224\", device=0, torch_dtype=torch.bfloat16)\n+pipeline(image, candidate_labels=candidate_labels)\n ```\n \n-### Using the model yourself\n-\n-If you want to do the pre- and postprocessing yourself, here's how to do that:\n+</hfoption>\n+<hfoption id=\"AutoModel\">\n \n-```python\n->>> from PIL import Image\n->>> import requests\n->>> from transformers import AutoProcessor, AutoModel\n->>> import torch\n+```py\n+import torch\n+import requests\n+from PIL import Image\n+from transformers import AutoProcessor, AutoModel\n \n->>> model = AutoModel.from_pretrained(\"google/siglip-base-patch16-224\")\n->>> processor = AutoProcessor.from_pretrained(\"google/siglip-base-patch16-224\")\n+model = AutoModel.from_pretrained(\"google/siglip-base-patch16-224\", torch_dtype=torch.float16, device_map=\"auto\", attn_implementation=\"sdpa\")\n+processor = AutoProcessor.from_pretrained(\"google/siglip-base-patch16-224\")\n \n->>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n->>> image = Image.open(requests.get(url, stream=True).raw)\n+url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n+image = Image.open(requests.get(url, stream=True).raw)\n+candidate_labels = [\"a Pallas cat\", \"a lion\", \"a Siberian tiger\"]\n+texts = [f'This is a photo of {label}.' for label in candidate_labels]\n+inputs = processor(text=texts, images=image, padding=\"max_length\", return_tensors=\"pt\").to(\"cuda\")\n \n->>> candidate_labels = [\"2 cats\", \"2 dogs\"]\n-# follows the pipeline prompt template to get same results\n->>> texts = [f'This is a photo of {label}.' for label in candidate_labels]\n-# important: we pass `padding=max_length` since the model was trained with this\n->>> inputs = processor(text=texts, images=image, padding=\"max_length\", return_tensors=\"pt\")\n+with torch.no_grad():\n+    outputs = model(**inputs)\n \n->>> with torch.no_grad():\n-...     outputs = model(**inputs)\n-\n->>> logits_per_image = outputs.logits_per_image\n->>> probs = torch.sigmoid(logits_per_image) # these are the probabilities\n->>> print(f\"{probs[0][0]:.1%} that image 0 is '{candidate_labels[0]}'\")\n-19.8% that image 0 is '2 cats'\n+logits_per_image = outputs.logits_per_image\n+probs = torch.sigmoid(logits_per_image)\n+print(f\"{probs[0][0]:.1%} that image 0 is '{candidate_labels[0]}'\")\n ```\n \n-## Resources\n-\n-A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you get started with SigLIP.\n-\n-- [Zero-shot image classification task guide](../tasks/zero_shot_image_classification)\n-- Demo notebooks for SigLIP can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/SigLIP). ðŸŒŽ\n+</hfoption>\n+</hfoptions>\n \n-If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n+Quantization reduces the memory burden of large models by representing the weights in a lower precision. Refer to the [Quantization](../quantization/overview) overview for more available quantization backends.\n \n+The example below uses [bitsandbytes](../quantization/bitsandbytes) to only quantize the weights to int4.\n \n-## Combining SigLIP and Flash Attention 2\n+```py\n+import torch\n+import requests\n+from PIL import Image\n+from transformers import AutoProcessor, AutoModel, BitsAndBytesConfig\n \n-First, make sure to install the latest version of Flash Attention 2.\n+bnb_config = BitsAndBytesConfig(load_in_4bit=True)\n+model = AutoModel.from_pretrained(\"google/siglip-base-patch16-224\", quantization_config=bnb_config, device_map=\"auto\", attn_implementation=\"sdpa\")\n+processor = AutoProcessor.from_pretrained(\"google/siglip-base-patch16-224\")\n \n-```bash\n-pip install -U flash-attn --no-build-isolation\n-```\n-\n-Make also sure that you have a hardware that is compatible with Flash-Attention 2. Read more about it in the official documentation of flash-attn repository. Make also sure to load your model in half-precision (e.g. `torch.float16``)\n-\n-To load and run a model using Flash Attention 2, refer to the snippet below:\n-\n-```python\n->>> import torch\n->>> import requests\n->>> from PIL import Image\n->>> from transformers import SiglipProcessor, SiglipModel\n->>> device = \"cuda\" # the device to load the model onto\n-\n->>> model = SiglipModel.from_pretrained(\n-...     \"google/siglip-so400m-patch14-384\",\n-...     attn_implementation=\"flash_attention_2\",\n-...     torch_dtype=torch.float16,\n-...     device_map=device,\n-... )\n->>> processor = SiglipProcessor.from_pretrained(\"google/siglip-so400m-patch14-384\")\n-\n->>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n->>> image = Image.open(requests.get(url, stream=True).raw)\n-\n->>> candidate_labels = [\"2 cats\", \"2 dogs\"]\n-# follows the pipeline prompt template to get same results\n->>> texts = [f'This is a photo of {label}.' for label in candidate_labels]\n-# important: we pass `padding=max_length` since the model was trained with this\n->>> inputs = processor(text=texts, images=image, padding=\"max_length\", return_tensors=\"pt\").to(device)\n-\n->>> with torch.no_grad():\n-...     with torch.autocast(device):\n-...         outputs = model(**inputs)\n-\n->>> logits_per_image = outputs.logits_per_image\n->>> probs = torch.sigmoid(logits_per_image) # these are the probabilities\n->>> print(f\"{probs[0][0]:.1%} that image 0 is '{candidate_labels[0]}'\")\n-19.8% that image 0 is '2 cats'\n-```\n-\n-\n-## Using Scaled Dot Product Attention (SDPA)\n-\n-PyTorch includes a native scaled dot-product attention (SDPA) operator as part of `torch.nn.functional`. This function \n-encompasses several implementations that can be applied depending on the inputs and the hardware in use. See the \n-[official documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html) \n-or the [GPU Inference](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#pytorch-scaled-dot-product-attention)\n-page for more information.\n-\n-You may set `attn_implementation=\"sdpa\"` in `from_pretrained()` to explicitly request SDPA to be used. Make sure you have `torch>=2.1.1`.\n+url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n+image = Image.open(requests.get(url, stream=True).raw)\n+candidate_labels = [\"a Pallas cat\", \"a lion\", \"a Siberian tiger\"]\n+texts = [f'This is a photo of {label}.' for label in candidate_labels]\n+inputs = processor(text=texts, images=image, padding=\"max_length\", return_tensors=\"pt\").to(\"cuda\")\n \n-```python\n->>> from transformers import SiglipModel\n+with torch.no_grad():\n+    outputs = model(**inputs)\n \n->>> model = SiglipModel.from_pretrained(\n-...     \"google/siglip-so400m-patch14-384\",\n-...     attn_implementation=\"sdpa\",\n-...     torch_dtype=torch.float16,\n-...     device_map=device,\n-... )\n+logits_per_image = outputs.logits_per_image\n+probs = torch.sigmoid(logits_per_image)\n+print(f\"{probs[0][0]:.1%} that image 0 is '{candidate_labels[0]}'\")\n ```\n-\n-For the best speedups, we recommend loading the model in half-precision (e.g. `torch.float16` or `torch.bfloat16`).\n-\n-\n-## Expected speedups\n-\n-Below is an expected speedup diagram that compares inference time between the native implementation in transformers using `google/siglip-so400m-patch14-384` checkpoint in `float16` precision and the Flash Attention 2 / SDPA version of the model using different batch sizes.\n-\n-<div style=\"text-align: center\">\n-<img src=\"https://i.imgur.com/cWm4rsn.png\">\n-</div>\n+## Notes\n+\n+- Training is supported for DDP and FSDP on single-node multi-GPU setups. However, it does not use [torch.distributed](https://pytorch.org/tutorials/beginner/dist_overview.html) utilities which may limit the scalability of batch size.\n+- When using the standalone [`SiglipTokenizer`] or [`SiglipProcessor`], make sure to pass `padding=\"max_length\"` because that is how the model was trained.\n+- To get the same results as the [`Pipeline`], a prompt template of `\"This is a photo of {label}.\"` should be passed to the processor.\n+- Toggle the `attn_implementation` parameter to either `\"sdpa\"` or `\"flash_attention_2\"` to use a more memory-efficient attention.\n+    ```py\n+    # pip install -U flash-attn --no-build-isolation\n+\n+    from transformers import SiglipModel\n+\n+    model = SiglipModel.from_pretrained(\n+        \"google/siglip-so400m-patch14-384\",\n+        attn_implementation=\"flash_attention_2\",\n+        torch_dtype=torch.float16,\n+        device_map=device,\n+    )\n+    ```\n \n \n ## SiglipConfig"
        }
    ],
    "stats": {
        "total": 230,
        "additions": 81,
        "deletions": 149
    }
}