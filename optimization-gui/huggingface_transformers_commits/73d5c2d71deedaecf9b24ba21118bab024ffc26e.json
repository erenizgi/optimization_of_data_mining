{
    "author": "Cyrilvallez",
    "message": "[loading] Use fewer threads by default for much better performances (#42324)\n\nmuch better default",
    "sha": "73d5c2d71deedaecf9b24ba21118bab024ffc26e",
    "files": [
        {
            "sha": "80cf941c37edcdd34de644f5bfb89f569be326e4",
            "filename": "src/transformers/core_model_loading.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/73d5c2d71deedaecf9b24ba21118bab024ffc26e/src%2Ftransformers%2Fcore_model_loading.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73d5c2d71deedaecf9b24ba21118bab024ffc26e/src%2Ftransformers%2Fcore_model_loading.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcore_model_loading.py?ref=73d5c2d71deedaecf9b24ba21118bab024ffc26e",
            "patch": "@@ -359,7 +359,10 @@ def convert(self, layer_name: str, config=None, quantizer=None, missing_keys: Op\n         return collected_tensors, misc\n \n \n-GLOBAL_WORKERS = min(16, (os.cpu_count() or 8) * 2)  # NVMe: 8-16; HDD/NFS: 2-4\n+# For I/O bound operations (i.e. here reading files), it is better to have fewer threads, e.g. 4 is a good default.\n+# Having too many is actually harming performances quite a lot, i.e. using 16 can sometimes lead to taking TWICE\n+# as much time to load the same model\n+GLOBAL_WORKERS = min(4, os.cpu_count() or 4)\n \n \n def _materialize_copy(tensor, device=None, dtype=None):"
        }
    ],
    "stats": {
        "total": 5,
        "additions": 4,
        "deletions": 1
    }
}