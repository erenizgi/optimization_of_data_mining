{
    "author": "ydshieh",
    "message": "avoid calling `gc.collect` and `cuda.empty_cache` (#34514)\n\n* update\r\n\r\n* update\r\n\r\n* update\r\n\r\n* update\r\n\r\n* update\r\n\r\n---------\r\n\r\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "ab98f0b0a1cd90b1c72948daf83c098037212fc4",
    "files": [
        {
            "sha": "8d6c1b19377eca115c9b7cc63035ecd2fbf4c5a9",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/ab98f0b0a1cd90b1c72948daf83c098037212fc4/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ab98f0b0a1cd90b1c72948daf83c098037212fc4/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=ab98f0b0a1cd90b1c72948daf83c098037212fc4",
            "patch": "@@ -16,6 +16,7 @@\n import contextlib\n import doctest\n import functools\n+import gc\n import importlib\n import inspect\n import logging\n@@ -2679,3 +2680,10 @@ def compare_pipeline_output_to_hub_spec(output, hub_spec):\n         if unexpected_keys:\n             error.append(f\"Keys in pipeline output that are not in Hub spec: {unexpected_keys}\")\n         raise KeyError(\"\\n\".join(error))\n+\n+\n+@require_torch\n+def cleanup(device: str, gc_collect=False):\n+    if gc_collect:\n+        gc.collect()\n+    backend_empty_cache(device)"
        },
        {
            "sha": "1f059ca46944e1798c1cbe0fe6d645067a9bed58",
            "filename": "tests/models/clvp/test_feature_extraction_clvp.py",
            "status": "modified",
            "additions": 8,
            "deletions": 4,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/ab98f0b0a1cd90b1c72948daf83c098037212fc4/tests%2Fmodels%2Fclvp%2Ftest_feature_extraction_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ab98f0b0a1cd90b1c72948daf83c098037212fc4/tests%2Fmodels%2Fclvp%2Ftest_feature_extraction_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclvp%2Ftest_feature_extraction_clvp.py?ref=ab98f0b0a1cd90b1c72948daf83c098037212fc4",
            "patch": "@@ -13,7 +13,6 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-import gc\n import itertools\n import os\n import random\n@@ -24,7 +23,13 @@\n from datasets import Audio, load_dataset\n \n from transformers import ClvpFeatureExtractor\n-from transformers.testing_utils import check_json_file_has_correct_format, require_torch, slow\n+from transformers.testing_utils import (\n+    check_json_file_has_correct_format,\n+    cleanup,\n+    require_torch,\n+    slow,\n+    torch_device,\n+)\n from transformers.utils.import_utils import is_torch_available\n \n from ...test_sequence_feature_extraction_common import SequenceFeatureExtractionTestMixin\n@@ -116,8 +121,7 @@ def setUp(self):\n     def tearDown(self):\n         super().tearDown()\n         # clean-up as much as possible GPU memory occupied by PyTorch\n-        gc.collect()\n-        torch.cuda.empty_cache()\n+        cleanup(torch_device)\n \n     # Copied from transformers.tests.models.whisper.test_feature_extraction_whisper.WhisperFeatureExtractionTest.test_feat_extract_from_and_save_pretrained\n     def test_feat_extract_from_and_save_pretrained(self):"
        },
        {
            "sha": "12e58500063a17c27b742f1cce21e74e89b39e70",
            "filename": "tests/models/clvp/test_modeling_clvp.py",
            "status": "modified",
            "additions": 5,
            "deletions": 9,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/ab98f0b0a1cd90b1c72948daf83c098037212fc4/tests%2Fmodels%2Fclvp%2Ftest_modeling_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ab98f0b0a1cd90b1c72948daf83c098037212fc4/tests%2Fmodels%2Fclvp%2Ftest_modeling_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclvp%2Ftest_modeling_clvp.py?ref=ab98f0b0a1cd90b1c72948daf83c098037212fc4",
            "patch": "@@ -14,7 +14,6 @@\n # limitations under the License.\n \"\"\"Testing suite for the PyTorch Clvp model.\"\"\"\n \n-import gc\n import tempfile\n import unittest\n \n@@ -23,6 +22,7 @@\n \n from transformers import ClvpConfig, ClvpDecoderConfig, ClvpEncoderConfig\n from transformers.testing_utils import (\n+    cleanup,\n     require_torch,\n     slow,\n     torch_device,\n@@ -174,8 +174,7 @@ def setUp(self):\n     def tearDown(self):\n         super().tearDown()\n         # clean-up as much as possible GPU memory occupied by PyTorch\n-        gc.collect()\n-        torch.cuda.empty_cache()\n+        cleanup(torch_device)\n \n     def test_config(self):\n         self.encoder_config_tester.run_common_tests()\n@@ -294,8 +293,7 @@ def setUp(self):\n     def tearDown(self):\n         super().tearDown()\n         # clean-up as much as possible GPU memory occupied by PyTorch\n-        gc.collect()\n-        torch.cuda.empty_cache()\n+        cleanup(torch_device)\n \n     def test_model(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n@@ -421,8 +419,7 @@ def setUp(self):\n     def tearDown(self):\n         super().tearDown()\n         # clean-up as much as possible GPU memory occupied by PyTorch\n-        gc.collect()\n-        torch.cuda.empty_cache()\n+        cleanup(torch_device)\n \n     def test_model(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n@@ -571,8 +568,7 @@ def setUp(self):\n     def tearDown(self):\n         super().tearDown()\n         # clean-up as much as possible GPU memory occupied by PyTorch\n-        gc.collect()\n-        torch.cuda.empty_cache()\n+        cleanup(torch_device, gc_collect=True)\n \n     def test_conditional_encoder(self):\n         with torch.no_grad():"
        },
        {
            "sha": "88efa9bb1891610fd5176867e4861f3f9a7ff044",
            "filename": "tests/models/ctrl/test_modeling_ctrl.py",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/ab98f0b0a1cd90b1c72948daf83c098037212fc4/tests%2Fmodels%2Fctrl%2Ftest_modeling_ctrl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ab98f0b0a1cd90b1c72948daf83c098037212fc4/tests%2Fmodels%2Fctrl%2Ftest_modeling_ctrl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fctrl%2Ftest_modeling_ctrl.py?ref=ab98f0b0a1cd90b1c72948daf83c098037212fc4",
            "patch": "@@ -13,11 +13,10 @@\n # limitations under the License.\n \n \n-import gc\n import unittest\n \n from transformers import CTRLConfig, is_torch_available\n-from transformers.testing_utils import backend_empty_cache, require_torch, slow, torch_device\n+from transformers.testing_utils import cleanup, require_torch, slow, torch_device\n \n from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n@@ -235,8 +234,7 @@ def setUp(self):\n     def tearDown(self):\n         super().tearDown()\n         # clean-up as much as possible GPU memory occupied by PyTorch\n-        gc.collect()\n-        backend_empty_cache(torch_device)\n+        cleanup(torch_device)\n \n     def test_config(self):\n         self.config_tester.run_common_tests()\n@@ -261,8 +259,7 @@ class CTRLModelLanguageGenerationTest(unittest.TestCase):\n     def tearDown(self):\n         super().tearDown()\n         # clean-up as much as possible GPU memory occupied by PyTorch\n-        gc.collect()\n-        backend_empty_cache(torch_device)\n+        cleanup(torch_device, gc_collect=True)\n \n     @slow\n     def test_lm_generate_ctrl(self):"
        },
        {
            "sha": "012444b472c0fcdcff4fbd36df7e8d0caa23599c",
            "filename": "tests/models/gpt2/test_modeling_gpt2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/ab98f0b0a1cd90b1c72948daf83c098037212fc4/tests%2Fmodels%2Fgpt2%2Ftest_modeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ab98f0b0a1cd90b1c72948daf83c098037212fc4/tests%2Fmodels%2Fgpt2%2Ftest_modeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt2%2Ftest_modeling_gpt2.py?ref=ab98f0b0a1cd90b1c72948daf83c098037212fc4",
            "patch": "@@ -15,15 +15,14 @@\n \n \n import datetime\n-import gc\n import math\n import unittest\n \n import pytest\n \n from transformers import GPT2Config, is_torch_available\n from transformers.testing_utils import (\n-    backend_empty_cache,\n+    cleanup,\n     require_flash_attn,\n     require_torch,\n     require_torch_gpu,\n@@ -542,8 +541,7 @@ def setUp(self):\n     def tearDown(self):\n         super().tearDown()\n         # clean-up as much as possible GPU memory occupied by PyTorch\n-        gc.collect()\n-        backend_empty_cache(torch_device)\n+        cleanup(torch_device)\n \n     def test_config(self):\n         self.config_tester.run_common_tests()\n@@ -753,8 +751,7 @@ class GPT2ModelLanguageGenerationTest(unittest.TestCase):\n     def tearDown(self):\n         super().tearDown()\n         # clean-up as much as possible GPU memory occupied by PyTorch\n-        gc.collect()\n-        backend_empty_cache(torch_device)\n+        cleanup(torch_device, gc_collect=True)\n \n     def _test_lm_generate_gpt2_helper(\n         self,"
        },
        {
            "sha": "1db484c4062c35039bffcd443be98d81a0f11ac5",
            "filename": "tests/models/gpt_bigcode/test_modeling_gpt_bigcode.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/ab98f0b0a1cd90b1c72948daf83c098037212fc4/tests%2Fmodels%2Fgpt_bigcode%2Ftest_modeling_gpt_bigcode.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ab98f0b0a1cd90b1c72948daf83c098037212fc4/tests%2Fmodels%2Fgpt_bigcode%2Ftest_modeling_gpt_bigcode.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt_bigcode%2Ftest_modeling_gpt_bigcode.py?ref=ab98f0b0a1cd90b1c72948daf83c098037212fc4",
            "patch": "@@ -18,7 +18,7 @@\n from parameterized import parameterized\n \n from transformers import GPTBigCodeConfig, is_torch_available\n-from transformers.testing_utils import require_torch, slow, torch_device\n+from transformers.testing_utils import cleanup, require_torch, slow, torch_device\n \n from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n@@ -422,9 +422,9 @@ def setUp(self):\n         self.config_tester = ConfigTester(self, config_class=GPTBigCodeConfig, n_embd=37)\n \n     def tearDown(self):\n-        import gc\n-\n-        gc.collect()\n+        super().tearDown()\n+        # clean-up as much as possible GPU memory occupied by PyTorch\n+        cleanup(torch_device)\n \n     def test_config(self):\n         self.config_tester.run_common_tests()"
        },
        {
            "sha": "0b0f3c1f3d8483571b50b36c384693201088032a",
            "filename": "tests/models/idefics2/test_modeling_idefics2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/ab98f0b0a1cd90b1c72948daf83c098037212fc4/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ab98f0b0a1cd90b1c72948daf83c098037212fc4/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py?ref=ab98f0b0a1cd90b1c72948daf83c098037212fc4",
            "patch": "@@ -15,7 +15,6 @@\n \"\"\"Testing suite for the PyTorch Idefics2 model.\"\"\"\n \n import copy\n-import gc\n import tempfile\n import unittest\n from io import BytesIO\n@@ -31,6 +30,7 @@\n     is_vision_available,\n )\n from transformers.testing_utils import (\n+    cleanup,\n     require_bitsandbytes,\n     require_flash_attn,\n     require_torch,\n@@ -583,8 +583,7 @@ def setUp(self):\n         )\n \n     def tearDown(self):\n-        gc.collect()\n-        torch.cuda.empty_cache()\n+        cleanup(torch_device, gc_collect=True)\n \n     @slow\n     @require_torch_multi_gpu"
        },
        {
            "sha": "dc5aad2fd043955327c31824876b0fbb45765d52",
            "filename": "tests/models/idefics3/test_modeling_idefics3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ab98f0b0a1cd90b1c72948daf83c098037212fc4/tests%2Fmodels%2Fidefics3%2Ftest_modeling_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ab98f0b0a1cd90b1c72948daf83c098037212fc4/tests%2Fmodels%2Fidefics3%2Ftest_modeling_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics3%2Ftest_modeling_idefics3.py?ref=ab98f0b0a1cd90b1c72948daf83c098037212fc4",
            "patch": "@@ -15,7 +15,6 @@\n \"\"\"Testing suite for the PyTorch Idefics3 model.\"\"\"\n \n import copy\n-import gc\n import unittest\n from io import BytesIO\n \n@@ -26,7 +25,7 @@\n     is_torch_available,\n     is_vision_available,\n )\n-from transformers.testing_utils import require_bitsandbytes, require_torch, slow, torch_device\n+from transformers.testing_utils import cleanup, require_bitsandbytes, require_torch, slow, torch_device\n \n from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n@@ -497,8 +496,7 @@ def setUp(self):\n         )\n \n     def tearDown(self):\n-        gc.collect()\n-        torch.cuda.empty_cache()\n+        cleanup(torch_device, gc_collect=True)\n \n     @slow\n     @unittest.skip(\"multi-gpu tests are disabled for now\")"
        },
        {
            "sha": "9e67f4f7381e24b2b1dad0c488ac7bdfdf80070c",
            "filename": "tests/models/llama/test_modeling_llama.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ab98f0b0a1cd90b1c72948daf83c098037212fc4/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ab98f0b0a1cd90b1c72948daf83c098037212fc4/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py?ref=ab98f0b0a1cd90b1c72948daf83c098037212fc4",
            "patch": "@@ -14,7 +14,6 @@\n # limitations under the License.\n \"\"\"Testing suite for the PyTorch LLaMA model.\"\"\"\n \n-import gc\n import tempfile\n import unittest\n \n@@ -25,7 +24,7 @@\n from transformers import AutoTokenizer, LlamaConfig, StaticCache, is_torch_available, set_seed\n from transformers.generation.configuration_utils import GenerationConfig\n from transformers.testing_utils import (\n-    backend_empty_cache,\n+    cleanup,\n     require_flash_attn,\n     require_read_token,\n     require_torch,\n@@ -891,8 +890,7 @@ def test_export_static_cache(self):\n @require_torch_accelerator\n class Mask4DTestHard(unittest.TestCase):\n     def tearDown(self):\n-        gc.collect()\n-        backend_empty_cache(torch_device)\n+        cleanup(torch_device, gc_collect=True)\n \n     def setUp(self):\n         model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\""
        },
        {
            "sha": "af0eddcd35b8978d81cc0988813ee24305dd5a2b",
            "filename": "tests/models/llava/test_modeling_llava.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/ab98f0b0a1cd90b1c72948daf83c098037212fc4/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ab98f0b0a1cd90b1c72948daf83c098037212fc4/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py?ref=ab98f0b0a1cd90b1c72948daf83c098037212fc4",
            "patch": "@@ -14,7 +14,6 @@\n # limitations under the License.\n \"\"\"Testing suite for the PyTorch Llava model.\"\"\"\n \n-import gc\n import unittest\n \n import requests\n@@ -28,6 +27,7 @@\n     is_vision_available,\n )\n from transformers.testing_utils import (\n+    cleanup,\n     require_bitsandbytes,\n     require_torch,\n     require_torch_gpu,\n@@ -307,8 +307,7 @@ def setUp(self):\n         self.processor = AutoProcessor.from_pretrained(\"llava-hf/bakLlava-v1-hf\")\n \n     def tearDown(self):\n-        gc.collect()\n-        torch.cuda.empty_cache()\n+        cleanup(torch_device, gc_collect=True)\n \n     @slow\n     @require_bitsandbytes"
        },
        {
            "sha": "e960f9f6759981cd99324f20ae52ba1299affb43",
            "filename": "tests/models/llava_next/test_modeling_llava_next.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/ab98f0b0a1cd90b1c72948daf83c098037212fc4/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ab98f0b0a1cd90b1c72948daf83c098037212fc4/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py?ref=ab98f0b0a1cd90b1c72948daf83c098037212fc4",
            "patch": "@@ -14,7 +14,6 @@\n # limitations under the License.\n \"\"\"Testing suite for the PyTorch Llava-NeXT model.\"\"\"\n \n-import gc\n import unittest\n \n import requests\n@@ -28,6 +27,7 @@\n     is_vision_available,\n )\n from transformers.testing_utils import (\n+    cleanup,\n     require_bitsandbytes,\n     require_torch,\n     slow,\n@@ -370,8 +370,7 @@ def setUp(self):\n         self.prompt = \"[INST] <image>\\nWhat is shown in this image? [/INST]\"\n \n     def tearDown(self):\n-        gc.collect()\n-        torch.cuda.empty_cache()\n+        cleanup(torch_device, gc_collect=True)\n \n     @slow\n     @require_bitsandbytes"
        },
        {
            "sha": "89cdce65ece95de8f415d176d9e686764e1a444f",
            "filename": "tests/models/llava_next_video/test_modeling_llava_next_video.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/ab98f0b0a1cd90b1c72948daf83c098037212fc4/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ab98f0b0a1cd90b1c72948daf83c098037212fc4/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py?ref=ab98f0b0a1cd90b1c72948daf83c098037212fc4",
            "patch": "@@ -14,7 +14,6 @@\n # limitations under the License.\n \"\"\"Testing suite for the PyTorch Llava-NeXT-Video model.\"\"\"\n \n-import gc\n import unittest\n \n import numpy as np\n@@ -29,6 +28,7 @@\n     is_vision_available,\n )\n from transformers.testing_utils import (\n+    cleanup,\n     require_bitsandbytes,\n     require_torch,\n     slow,\n@@ -400,8 +400,7 @@ def setUp(self):\n         self.prompt_video = \"USER: <video>\\nWhy is this video funny? ASSISTANT:\"\n \n     def tearDown(self):\n-        gc.collect()\n-        torch.cuda.empty_cache()\n+        cleanup(torch_device, gc_collect=True)\n \n     @slow\n     @require_bitsandbytes"
        },
        {
            "sha": "107b6321b65cffb59e04152f0c6d9be689b85dce",
            "filename": "tests/models/llava_onevision/test_modeling_llava_onevision.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/ab98f0b0a1cd90b1c72948daf83c098037212fc4/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ab98f0b0a1cd90b1c72948daf83c098037212fc4/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py?ref=ab98f0b0a1cd90b1c72948daf83c098037212fc4",
            "patch": "@@ -14,7 +14,6 @@\n # limitations under the License.\n \"\"\"Testing suite for the PyTorch Llava-NeXT model.\"\"\"\n \n-import gc\n import unittest\n \n import numpy as np\n@@ -29,6 +28,7 @@\n     is_vision_available,\n )\n from transformers.testing_utils import (\n+    cleanup,\n     require_bitsandbytes,\n     require_torch,\n     slow,\n@@ -336,8 +336,7 @@ def setUp(self):\n         self.prompt_video = \"user\\n<video>\\nWhat do you see in this video?<|im_end|>\\n<|im_start|>assistant\\n\"\n \n     def tearDown(self):\n-        gc.collect()\n-        torch.cuda.empty_cache()\n+        cleanup(torch_device, gc_collect=True)\n \n     @slow\n     @require_bitsandbytes"
        },
        {
            "sha": "c5ea050edf92ef2bc2c4e5878d41df762c094539",
            "filename": "tests/models/mistral/test_modeling_mistral.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/ab98f0b0a1cd90b1c72948daf83c098037212fc4/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ab98f0b0a1cd90b1c72948daf83c098037212fc4/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py?ref=ab98f0b0a1cd90b1c72948daf83c098037212fc4",
            "patch": "@@ -23,6 +23,7 @@\n from transformers import AutoTokenizer, MistralConfig, is_torch_available, set_seed\n from transformers.testing_utils import (\n     backend_empty_cache,\n+    cleanup,\n     require_bitsandbytes,\n     require_flash_attn,\n     require_read_token,\n@@ -436,8 +437,7 @@ def setUpClass(cls):\n             cls.cuda_compute_capability_major_version = torch.cuda.get_device_capability()[0]\n \n     def tearDown(self):\n-        torch.cuda.empty_cache()\n-        gc.collect()\n+        cleanup(torch_device, gc_collect=True)\n \n     @slow\n     def test_model_7b_logits(self):\n@@ -656,8 +656,7 @@ class Mask4DTestHard(unittest.TestCase):\n     _model = None\n \n     def tearDown(self):\n-        gc.collect()\n-        backend_empty_cache(torch_device)\n+        cleanup(torch_device, gc_collect=True)\n \n     @property\n     def model(self):"
        },
        {
            "sha": "42bf6fd708161825b54aed65b77000b7df744d26",
            "filename": "tests/models/mllama/test_modeling_mllama.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/ab98f0b0a1cd90b1c72948daf83c098037212fc4/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ab98f0b0a1cd90b1c72948daf83c098037212fc4/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py?ref=ab98f0b0a1cd90b1c72948daf83c098037212fc4",
            "patch": "@@ -14,7 +14,6 @@\n # limitations under the License.\n \"\"\"Testing suite for the PyTorch Mllama model.\"\"\"\n \n-import gc\n import unittest\n \n import requests\n@@ -30,6 +29,7 @@\n )\n from transformers.models.mllama.configuration_mllama import MllamaTextConfig\n from transformers.testing_utils import (\n+    cleanup,\n     is_flaky,\n     require_bitsandbytes,\n     require_read_token,\n@@ -396,8 +396,7 @@ def setUp(self):\n         self.instruct_model_checkpoint = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n \n     def tearDown(self):\n-        gc.collect()\n-        torch.cuda.empty_cache()\n+        cleanup(torch_device, gc_collect=True)\n \n     @slow\n     @require_torch_gpu"
        },
        {
            "sha": "4c591805766f8616761a256c1c3b1920df739b73",
            "filename": "tests/models/paligemma/test_modeling_paligemma.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/ab98f0b0a1cd90b1c72948daf83c098037212fc4/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ab98f0b0a1cd90b1c72948daf83c098037212fc4/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py?ref=ab98f0b0a1cd90b1c72948daf83c098037212fc4",
            "patch": "@@ -14,7 +14,6 @@\n # limitations under the License.\n \"\"\"Testing suite for the PyTorch PaliGemma model.\"\"\"\n \n-import gc\n import unittest\n \n import requests\n@@ -28,6 +27,7 @@\n     is_vision_available,\n )\n from transformers.testing_utils import (\n+    cleanup,\n     require_read_token,\n     require_torch,\n     require_torch_sdpa,\n@@ -365,8 +365,7 @@ def setUp(self):\n         self.processor = PaliGemmaProcessor.from_pretrained(\"google/paligemma-3b-pt-224\")\n \n     def tearDown(self):\n-        gc.collect()\n-        torch.cuda.empty_cache()\n+        cleanup(torch_device, gc_collect=True)\n \n     def test_small_model_integration_test(self):\n         # Let' s make sure we test the preprocessing to replace what is used"
        },
        {
            "sha": "42b521e518e22e009a20ff1c42bf653a8f117d80",
            "filename": "tests/models/qwen2_audio/test_modeling_qwen2_audio.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/ab98f0b0a1cd90b1c72948daf83c098037212fc4/tests%2Fmodels%2Fqwen2_audio%2Ftest_modeling_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ab98f0b0a1cd90b1c72948daf83c098037212fc4/tests%2Fmodels%2Fqwen2_audio%2Ftest_modeling_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_audio%2Ftest_modeling_qwen2_audio.py?ref=ab98f0b0a1cd90b1c72948daf83c098037212fc4",
            "patch": "@@ -14,7 +14,6 @@\n # limitations under the License.\n \"\"\"Testing suite for the PyTorch Qwen2Audio model.\"\"\"\n \n-import gc\n import tempfile\n import unittest\n from io import BytesIO\n@@ -29,6 +28,7 @@\n     is_torch_available,\n )\n from transformers.testing_utils import (\n+    cleanup,\n     require_torch,\n     require_torch_sdpa,\n     slow,\n@@ -222,8 +222,7 @@ def setUp(self):\n         self.processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\")\n \n     def tearDown(self):\n-        gc.collect()\n-        torch.cuda.empty_cache()\n+        cleanup(torch_device, gc_collect=True)\n \n     @slow\n     def test_small_model_integration_test_single(self):"
        },
        {
            "sha": "3e3f7b9c457589621ede4527ab9451be436fd62e",
            "filename": "tests/models/rag/test_modeling_rag.py",
            "status": "modified",
            "additions": 4,
            "deletions": 7,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/ab98f0b0a1cd90b1c72948daf83c098037212fc4/tests%2Fmodels%2Frag%2Ftest_modeling_rag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ab98f0b0a1cd90b1c72948daf83c098037212fc4/tests%2Fmodels%2Frag%2Ftest_modeling_rag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frag%2Ftest_modeling_rag.py?ref=ab98f0b0a1cd90b1c72948daf83c098037212fc4",
            "patch": "@@ -14,7 +14,6 @@\n # limitations under the License.\n \n \n-import gc\n import json\n import os\n import shutil\n@@ -29,6 +28,7 @@\n from transformers.models.dpr.tokenization_dpr import DPRContextEncoderTokenizer, DPRQuestionEncoderTokenizer\n from transformers.models.roberta.tokenization_roberta import VOCAB_FILES_NAMES as BART_VOCAB_FILES_NAMES\n from transformers.testing_utils import (\n+    cleanup,\n     get_tests_dir,\n     require_sentencepiece,\n     require_tokenizers,\n@@ -196,8 +196,7 @@ def tearDown(self):\n         shutil.rmtree(self.tmpdirname)\n \n         # clean-up as much as possible GPU memory occupied by PyTorch\n-        gc.collect()\n-        torch.cuda.empty_cache()\n+        cleanup(torch_device)\n \n     def get_retriever(self, config):\n         dataset = Dataset.from_dict(\n@@ -684,8 +683,7 @@ class RagModelIntegrationTests(unittest.TestCase):\n     def tearDown(self):\n         super().tearDown()\n         # clean-up as much as possible GPU memory occupied by PyTorch\n-        gc.collect()\n-        torch.cuda.empty_cache()\n+        cleanup(torch_device, gc_collect=True)\n \n     @cached_property\n     def sequence_model(self):\n@@ -1043,8 +1041,7 @@ class RagModelSaveLoadTests(unittest.TestCase):\n     def tearDown(self):\n         super().tearDown()\n         # clean-up as much as possible GPU memory occupied by PyTorch\n-        gc.collect()\n-        torch.cuda.empty_cache()\n+        cleanup(torch_device, gc_collect=True)\n \n     def get_rag_config(self):\n         question_encoder_config = AutoConfig.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")"
        },
        {
            "sha": "7faace0096c8de2612e511cb1bd8ab08f0521ad8",
            "filename": "tests/models/sam/test_modeling_sam.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ab98f0b0a1cd90b1c72948daf83c098037212fc4/tests%2Fmodels%2Fsam%2Ftest_modeling_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ab98f0b0a1cd90b1c72948daf83c098037212fc4/tests%2Fmodels%2Fsam%2Ftest_modeling_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam%2Ftest_modeling_sam.py?ref=ab98f0b0a1cd90b1c72948daf83c098037212fc4",
            "patch": "@@ -14,13 +14,12 @@\n # limitations under the License.\n \"\"\"Testing suite for the PyTorch SAM model.\"\"\"\n \n-import gc\n import unittest\n \n import requests\n \n from transformers import SamConfig, SamMaskDecoderConfig, SamPromptEncoderConfig, SamVisionConfig, pipeline\n-from transformers.testing_utils import backend_empty_cache, require_torch, slow, torch_device\n+from transformers.testing_utils import cleanup, require_torch, slow, torch_device\n from transformers.utils import is_torch_available, is_vision_available\n \n from ...test_configuration_common import ConfigTester\n@@ -469,8 +468,7 @@ class SamModelIntegrationTest(unittest.TestCase):\n     def tearDown(self):\n         super().tearDown()\n         # clean-up as much as possible GPU memory occupied by PyTorch\n-        gc.collect()\n-        backend_empty_cache(torch_device)\n+        cleanup(torch_device, gc_collect=True)\n \n     def test_inference_mask_generation_no_point(self):\n         model = SamModel.from_pretrained(\"facebook/sam-vit-base\")"
        },
        {
            "sha": "6310224e07f2974c6f911a9aa595e7ba433e472e",
            "filename": "tests/models/univnet/test_modeling_univnet.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ab98f0b0a1cd90b1c72948daf83c098037212fc4/tests%2Fmodels%2Funivnet%2Ftest_modeling_univnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ab98f0b0a1cd90b1c72948daf83c098037212fc4/tests%2Fmodels%2Funivnet%2Ftest_modeling_univnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Funivnet%2Ftest_modeling_univnet.py?ref=ab98f0b0a1cd90b1c72948daf83c098037212fc4",
            "patch": "@@ -12,7 +12,6 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-import gc\n import inspect\n import random\n import unittest\n@@ -21,7 +20,7 @@\n \n from transformers import UnivNetConfig, UnivNetFeatureExtractor\n from transformers.testing_utils import (\n-    backend_empty_cache,\n+    cleanup,\n     is_torch_available,\n     require_torch,\n     require_torch_accelerator,\n@@ -211,8 +210,7 @@ def test_unbatched_inputs_outputs(self):\n class UnivNetModelIntegrationTests(unittest.TestCase):\n     def tearDown(self):\n         super().tearDown()\n-        gc.collect()\n-        backend_empty_cache(torch_device)\n+        cleanup(torch_device, gc_collect=True)\n \n     def _load_datasamples(self, num_samples, sampling_rate=24000):\n         ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")"
        },
        {
            "sha": "0044ef02720c2b9d7e41d47abdeeaf6afb4aa1ea",
            "filename": "tests/models/video_llava/test_modeling_video_llava.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/ab98f0b0a1cd90b1c72948daf83c098037212fc4/tests%2Fmodels%2Fvideo_llava%2Ftest_modeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ab98f0b0a1cd90b1c72948daf83c098037212fc4/tests%2Fmodels%2Fvideo_llava%2Ftest_modeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvideo_llava%2Ftest_modeling_video_llava.py?ref=ab98f0b0a1cd90b1c72948daf83c098037212fc4",
            "patch": "@@ -14,7 +14,6 @@\n # limitations under the License.\n \"\"\"Testing suite for the PyTorch VideoLlava model.\"\"\"\n \n-import gc\n import unittest\n \n import numpy as np\n@@ -29,6 +28,7 @@\n     is_vision_available,\n )\n from transformers.testing_utils import (\n+    cleanup,\n     require_bitsandbytes,\n     require_torch,\n     require_torch_gpu,\n@@ -437,8 +437,7 @@ def setUp(self):\n         self.processor = VideoLlavaProcessor.from_pretrained(\"LanguageBind/Video-LLaVA-7B-hf\")\n \n     def tearDown(self):\n-        gc.collect()\n-        torch.cuda.empty_cache()\n+        cleanup(torch_device, gc_collect=True)\n \n     @slow\n     @require_bitsandbytes"
        },
        {
            "sha": "b97c2516704e48a7c022867423ec978985f3987c",
            "filename": "tests/models/vipllava/test_modeling_vipllava.py",
            "status": "modified",
            "additions": 9,
            "deletions": 4,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/ab98f0b0a1cd90b1c72948daf83c098037212fc4/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ab98f0b0a1cd90b1c72948daf83c098037212fc4/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py?ref=ab98f0b0a1cd90b1c72948daf83c098037212fc4",
            "patch": "@@ -14,7 +14,6 @@\n # limitations under the License.\n \"\"\"Testing suite for the PyTorch VipLlava model.\"\"\"\n \n-import gc\n import unittest\n \n import requests\n@@ -26,7 +25,14 @@\n     is_torch_available,\n     is_vision_available,\n )\n-from transformers.testing_utils import require_bitsandbytes, require_torch, require_torch_gpu, slow, torch_device\n+from transformers.testing_utils import (\n+    cleanup,\n+    require_bitsandbytes,\n+    require_torch,\n+    require_torch_gpu,\n+    slow,\n+    torch_device,\n+)\n \n from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n@@ -290,8 +296,7 @@ def setUp(self):\n         self.processor = AutoProcessor.from_pretrained(\"llava-hf/vip-llava-7b-hf\")\n \n     def tearDown(self):\n-        gc.collect()\n-        torch.cuda.empty_cache()\n+        cleanup(torch_device, gc_collect=True)\n \n     @slow\n     @require_bitsandbytes"
        },
        {
            "sha": "b2d90adc79da96f224a9f4d8fb1dad1dc0e4d93e",
            "filename": "tests/models/wav2vec2/test_modeling_wav2vec2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ab98f0b0a1cd90b1c72948daf83c098037212fc4/tests%2Fmodels%2Fwav2vec2%2Ftest_modeling_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ab98f0b0a1cd90b1c72948daf83c098037212fc4/tests%2Fmodels%2Fwav2vec2%2Ftest_modeling_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwav2vec2%2Ftest_modeling_wav2vec2.py?ref=ab98f0b0a1cd90b1c72948daf83c098037212fc4",
            "patch": "@@ -14,7 +14,6 @@\n # limitations under the License.\n \"\"\"Testing suite for the PyTorch Wav2Vec2 model.\"\"\"\n \n-import gc\n import math\n import multiprocessing\n import os\n@@ -30,7 +29,7 @@\n from transformers import Wav2Vec2Config, is_torch_available\n from transformers.testing_utils import (\n     CaptureLogger,\n-    backend_empty_cache,\n+    cleanup,\n     is_pt_flax_cross_test,\n     is_pyctcdecode_available,\n     is_torchaudio_available,\n@@ -1460,8 +1459,7 @@ class Wav2Vec2ModelIntegrationTest(unittest.TestCase):\n     def tearDown(self):\n         super().tearDown()\n         # clean-up as much as possible GPU memory occupied by PyTorch\n-        gc.collect()\n-        backend_empty_cache(torch_device)\n+        cleanup(torch_device, gc_collect=True)\n \n     def _load_datasamples(self, num_samples):\n         ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")"
        },
        {
            "sha": "373bc9eea4a3889f8eb0d9f7e5288b537871feba",
            "filename": "tests/models/xglm/test_modeling_xglm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/ab98f0b0a1cd90b1c72948daf83c098037212fc4/tests%2Fmodels%2Fxglm%2Ftest_modeling_xglm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ab98f0b0a1cd90b1c72948daf83c098037212fc4/tests%2Fmodels%2Fxglm%2Ftest_modeling_xglm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxglm%2Ftest_modeling_xglm.py?ref=ab98f0b0a1cd90b1c72948daf83c098037212fc4",
            "patch": "@@ -14,12 +14,12 @@\n # limitations under the License.\n \n import datetime\n-import gc\n import math\n import unittest\n \n from transformers import XGLMConfig, is_torch_available\n from transformers.testing_utils import (\n+    cleanup,\n     require_torch,\n     require_torch_accelerator,\n     require_torch_fp16,\n@@ -343,8 +343,7 @@ class XGLMModelLanguageGenerationTest(unittest.TestCase):\n     def tearDown(self):\n         super().tearDown()\n         # clean-up as much as possible GPU memory occupied by PyTorch\n-        gc.collect()\n-        torch.cuda.empty_cache()\n+        cleanup(torch_device, gc_collect=True)\n \n     def _test_lm_generate_xglm_helper(\n         self,"
        }
    ],
    "stats": {
        "total": 171,
        "additions": 77,
        "deletions": 94
    }
}