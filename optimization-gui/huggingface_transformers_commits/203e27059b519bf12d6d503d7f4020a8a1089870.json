{
    "author": "yonigozlan",
    "message": "Add image text to text pipeline (#34170)\n\n* Standardize image-text-to-text-models-output\r\n\r\nadd post_process_image_text_to_text to chameleon and cleanup\r\n\r\nFix legacy kwarg behavior and deprecation warning\r\n\r\nadd post_process_image_text_to_text to qwen2_vl and llava_onevision\r\n\r\nAdd post_process_image_text_to_text to idefics3, mllama, pixtral processor\r\n\r\n* nit var name post_process_image_text_to_text udop\r\n\r\n* nit fix deprecation warnings\r\n\r\n* Add image-text-to-text pipeline\r\n\r\n* add support for image url in chat template for pipeline\r\n\r\n* Reformat to be fully compatible with chat templates\r\n\r\n* Add tests chat template\r\n\r\n* Fix imports and tests\r\n\r\n* Add pipeline tag\r\n\r\n* change logic handling of single prompt ans multiple images\r\n\r\n* add pipeline mapping to models\r\n\r\n* fix batched inference\r\n\r\n* fix tests\r\n\r\n* Add manual batching for preprocessing\r\n\r\n* Fix outputs with nested images\r\n\r\n* Add support for all common processing kwargs\r\n\r\n* Add default padding when multiple text inputs (batch size>1)\r\n\r\n* nit change version deprecation warning\r\n\r\n* Add support for text only inference\r\n\r\n* add chat_template warnings\r\n\r\n* Add pipeline tests and add copied from post process function\r\n\r\n* Fix batched pipeline tests\r\n\r\n* nit\r\n\r\n* Fix pipeline tests blip2\r\n\r\n* remove unnecessary max_new_tokens\r\n\r\n* revert processing kosmos2 and remove unnecessary max_new_tokens\r\n\r\n* fix pipeline tests idefics\r\n\r\n* Force try loading processor if pipeline supports it\r\n\r\n* revert load_processor change\r\n\r\n* hardcode loading only processor\r\n\r\n* remove unnecessary try except\r\n\r\n* skip imagetexttotext tests for kosmos2 as tiny model causes problems\r\n\r\n* Make code clearer\r\n\r\n* Address review comments\r\n\r\n* remove preprocessing logic from pipeline\r\n\r\n* fix fuyu\r\n\r\n* add BC resize fuyu\r\n\r\n* Move post_process_image_text_to_text to ProcessorMixin\r\n\r\n* add guard in post_process\r\n\r\n* fix zero shot object detection pipeline\r\n\r\n* add support for generator input in pipeline\r\n\r\n* nit\r\n\r\n* change default image-text-to-text model to llava onevision\r\n\r\n* fix owlv2 size dict\r\n\r\n* Change legacy deprecation warning to only show when True",
    "sha": "203e27059b519bf12d6d503d7f4020a8a1089870",
    "files": [
        {
            "sha": "59e474fcc49f7523f74a44cf761738e4aea9fcea",
            "filename": "docs/source/en/main_classes/pipelines.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/203e27059b519bf12d6d503d7f4020a8a1089870/docs%2Fsource%2Fen%2Fmain_classes%2Fpipelines.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/203e27059b519bf12d6d503d7f4020a8a1089870/docs%2Fsource%2Fen%2Fmain_classes%2Fpipelines.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmain_classes%2Fpipelines.md?ref=203e27059b519bf12d6d503d7f4020a8a1089870",
            "patch": "@@ -478,6 +478,12 @@ Pipelines available for multimodal tasks include the following.\n     - __call__\n     - all\n \n+### ImageTextToTextPipeline\n+\n+[[autodoc]] ImageTextToTextPipeline\n+    - __call__\n+    - all\n+\n ### MaskGenerationPipeline\n \n [[autodoc]] MaskGenerationPipeline"
        },
        {
            "sha": "3980becebbde36ed8cc224bcb721dd338ac079b0",
            "filename": "docs/source/ja/main_classes/pipelines.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/203e27059b519bf12d6d503d7f4020a8a1089870/docs%2Fsource%2Fja%2Fmain_classes%2Fpipelines.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/203e27059b519bf12d6d503d7f4020a8a1089870/docs%2Fsource%2Fja%2Fmain_classes%2Fpipelines.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmain_classes%2Fpipelines.md?ref=203e27059b519bf12d6d503d7f4020a8a1089870",
            "patch": "@@ -481,6 +481,12 @@ my_pipeline = pipeline(model=\"xxxx\", pipeline_class=MyPipeline)\n     - __call__\n     - all\n \n+### ImageTextToTextPipeline\n+\n+[[autodoc]] ImageTextToTextPipeline\n+    - __call__\n+    - all\n+\n ### VisualQuestionAnsweringPipeline\n \n [[autodoc]] VisualQuestionAnsweringPipeline"
        },
        {
            "sha": "bc16709d8b483271ba7df7b2e4afded833e58244",
            "filename": "docs/source/zh/main_classes/pipelines.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/203e27059b519bf12d6d503d7f4020a8a1089870/docs%2Fsource%2Fzh%2Fmain_classes%2Fpipelines.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/203e27059b519bf12d6d503d7f4020a8a1089870/docs%2Fsource%2Fzh%2Fmain_classes%2Fpipelines.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Fmain_classes%2Fpipelines.md?ref=203e27059b519bf12d6d503d7f4020a8a1089870",
            "patch": "@@ -455,6 +455,12 @@ See [`TokenClassificationPipeline`] for all details.\n     - __call__\n     - all\n \n+### ImageTextToTextPipeline\n+\n+[[autodoc]] ImageTextToTextPipeline\n+    - __call__\n+    - all\n+\n ### MaskGenerationPipeline\n \n [[autodoc]] MaskGenerationPipeline"
        },
        {
            "sha": "47b43e0b90896f87e2d301e233a1cfa48b7f3008",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/203e27059b519bf12d6d503d7f4020a8a1089870/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/203e27059b519bf12d6d503d7f4020a8a1089870/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=203e27059b519bf12d6d503d7f4020a8a1089870",
            "patch": "@@ -868,6 +868,7 @@\n         \"ImageClassificationPipeline\",\n         \"ImageFeatureExtractionPipeline\",\n         \"ImageSegmentationPipeline\",\n+        \"ImageTextToTextPipeline\",\n         \"ImageToImagePipeline\",\n         \"ImageToTextPipeline\",\n         \"JsonPipelineDataFormat\",\n@@ -5794,6 +5795,7 @@\n         ImageClassificationPipeline,\n         ImageFeatureExtractionPipeline,\n         ImageSegmentationPipeline,\n+        ImageTextToTextPipeline,\n         ImageToImagePipeline,\n         ImageToTextPipeline,\n         JsonPipelineDataFormat,"
        },
        {
            "sha": "f59b99b490d38d7abeae9f13d9382a9342218216",
            "filename": "src/transformers/image_utils.py",
            "status": "modified",
            "additions": 21,
            "deletions": 0,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/203e27059b519bf12d6d503d7f4020a8a1089870/src%2Ftransformers%2Fimage_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/203e27059b519bf12d6d503d7f4020a8a1089870/src%2Ftransformers%2Fimage_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_utils.py?ref=203e27059b519bf12d6d503d7f4020a8a1089870",
            "patch": "@@ -385,6 +385,27 @@ def load_image(image: Union[str, \"PIL.Image.Image\"], timeout: Optional[float] =\n     return image\n \n \n+def load_images(\n+    images: Union[List, Tuple, str, \"PIL.Image.Image\"], timeout: Optional[float] = None\n+) -> Union[\"PIL.Image.Image\", List[\"PIL.Image.Image\"], List[List[\"PIL.Image.Image\"]]]:\n+    \"\"\"Loads images, handling different levels of nesting.\n+\n+    Args:\n+      images: A single image, a list of images, or a list of lists of images to load.\n+      timeout: Timeout for loading images.\n+\n+    Returns:\n+      A single image, a list of images, a list of lists of images.\n+    \"\"\"\n+    if isinstance(images, (list, tuple)):\n+        if len(images) and isinstance(images[0], (list, tuple)):\n+            return [[load_image(image, timeout=timeout) for image in image_group] for image_group in images]\n+        else:\n+            return [load_image(image, timeout=timeout) for image in images]\n+    else:\n+        return load_image(images, timeout=timeout)\n+\n+\n def validate_preprocess_arguments(\n     do_rescale: Optional[bool] = None,\n     rescale_factor: Optional[float] = None,"
        },
        {
            "sha": "a8960d80acc838f1d3ac0a60826d6c505eadb5be",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/203e27059b519bf12d6d503d7f4020a8a1089870/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/203e27059b519bf12d6d503d7f4020a8a1089870/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=203e27059b519bf12d6d503d7f4020a8a1089870",
            "patch": "@@ -114,6 +114,7 @@\n             (\"oneformer\", (\"OneFormerImageProcessor\",)),\n             (\"owlv2\", (\"Owlv2ImageProcessor\",)),\n             (\"owlvit\", (\"OwlViTImageProcessor\",)),\n+            (\"paligemma\", (\"SiglipImageProcessor\",)),\n             (\"perceiver\", (\"PerceiverImageProcessor\",)),\n             (\"pix2struct\", (\"Pix2StructImageProcessor\",)),\n             (\"pixtral\", (\"PixtralImageProcessor\",)),"
        },
        {
            "sha": "b46ff4bcfab9022ddd9a4a91e990be5190806432",
            "filename": "src/transformers/models/donut/processing_donut.py",
            "status": "modified",
            "additions": 16,
            "deletions": 0,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/203e27059b519bf12d6d503d7f4020a8a1089870/src%2Ftransformers%2Fmodels%2Fdonut%2Fprocessing_donut.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/203e27059b519bf12d6d503d7f4020a8a1089870/src%2Ftransformers%2Fmodels%2Fdonut%2Fprocessing_donut.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdonut%2Fprocessing_donut.py?ref=203e27059b519bf12d6d503d7f4020a8a1089870",
            "patch": "@@ -24,12 +24,16 @@\n from ...image_utils import ImageInput\n from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n+from ...utils import logging\n \n \n class DonutProcessorKwargs(ProcessingKwargs, total=False):\n     _defaults = {}\n \n \n+logger = logging.get_logger(__name__)\n+\n+\n class DonutProcessor(ProcessorMixin):\n     r\"\"\"\n     Constructs a Donut processor which wraps a Donut image processor and an XLMRoBERTa tokenizer into a single\n@@ -85,6 +89,16 @@ def __call__(\n         [`~DonutTokenizer.__call__`]. Please refer to the doctsring of the above two methods for more information.\n         \"\"\"\n         # For backward compatibility\n+        legacy = kwargs.pop(\"legacy\", True)\n+        if legacy:\n+            # With `add_special_tokens=True`, the performance of donut are degraded when working with both images and text.\n+            logger.warning_once(\n+                \"Legacy behavior is being used. The current behavior will be deprecated in version 5.0.0. \"\n+                \"In the new behavior, if both images and text are provided, the default value of `add_special_tokens` \"\n+                \"will be changed to `False` when calling the tokenizer if `add_special_tokens` is unset. \"\n+                \"To test the new behavior, set `legacy=False`as a processor call argument.\"\n+            )\n+\n         if self._in_target_context_manager:\n             return self.current_processor(images, text, **kwargs)\n \n@@ -100,6 +114,8 @@ def __call__(\n         if images is not None:\n             inputs = self.image_processor(images, **output_kwargs[\"images_kwargs\"])\n         if text is not None:\n+            if not legacy and images is not None:\n+                output_kwargs[\"text_kwargs\"].setdefault(\"add_special_tokens\", False)\n             encodings = self.tokenizer(text, **output_kwargs[\"text_kwargs\"])\n \n         if text is None:"
        },
        {
            "sha": "4bb9ea7964d4161222818146495e08f2b3b7e9d9",
            "filename": "src/transformers/models/fuyu/image_processing_fuyu.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/203e27059b519bf12d6d503d7f4020a8a1089870/src%2Ftransformers%2Fmodels%2Ffuyu%2Fimage_processing_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/203e27059b519bf12d6d503d7f4020a8a1089870/src%2Ftransformers%2Fmodels%2Ffuyu%2Fimage_processing_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fimage_processing_fuyu.py?ref=203e27059b519bf12d6d503d7f4020a8a1089870",
            "patch": "@@ -19,7 +19,7 @@\n \n import numpy as np\n \n-from ...image_processing_utils import BaseImageProcessor, BatchFeature\n+from ...image_processing_utils import BaseImageProcessor, BatchFeature, get_size_dict\n from ...image_transforms import (\n     pad,\n     resize,\n@@ -475,6 +475,7 @@ def preprocess(\n             input_data_format = infer_channel_dimension_format(batch_images[0][0])\n \n         original_image_sizes = [get_image_size(images[0], channel_dim=input_data_format) for images in batch_images]\n+        size = get_size_dict(size)  # for BC\n \n         if do_resize:\n             batch_images = ["
        },
        {
            "sha": "e24f2fd4d1abd03367f54e05476ee199cdeb4b27",
            "filename": "src/transformers/models/fuyu/processing_fuyu.py",
            "status": "modified",
            "additions": 28,
            "deletions": 2,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/203e27059b519bf12d6d503d7f4020a8a1089870/src%2Ftransformers%2Fmodels%2Ffuyu%2Fprocessing_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/203e27059b519bf12d6d503d7f4020a8a1089870/src%2Ftransformers%2Fmodels%2Ffuyu%2Fprocessing_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fprocessing_fuyu.py?ref=203e27059b519bf12d6d503d7f4020a8a1089870",
            "patch": "@@ -264,10 +264,10 @@ def _tokenize_prompts_with_image_and_batch(\n         bos_token = tokenizer.vocab[\"|ENDOFTEXT|\"]\n     prompts_tokens = [[[bos_token] + x for x in prompt_seq] for prompt_seq in prompts_tokens]\n     if add_beginning_of_answer_token:\n-        boa = tokenizer.vocab[BEGINNING_OF_ANSWER_STRING]\n+        beginning_of_answer = tokenizer.vocab[BEGINNING_OF_ANSWER_STRING]\n         # Only add bbox open token to the last subsequence since that is what will be completed\n         for token_seq in prompts_tokens:\n-            token_seq[-1].append(boa)\n+            token_seq[-1].append(beginning_of_answer)\n \n     # Now we have a list of list of tokens which each list has a different\n     # size. We want to extend this list to:\n@@ -682,6 +682,32 @@ def tokens_to_points(tokens, original_size):\n \n         return results\n \n+    def post_process_image_text_to_text(self, generated_outputs):\n+        \"\"\"\n+        Post-processes the output of `FuyuForConditionalGeneration` to only return the text output.\n+\n+        Args:\n+            generated_outputs (`torch.Tensor` or `np.ndarray`):\n+                The output of the model. The output is expected to be a tensor of shape `(batch_size, sequence_length)`\n+                containing the token ids of the generated sequences.\n+\n+        Returns:\n+            `List[str]`: The decoded text output.\n+        \"\"\"\n+        beginning_of_answer = self.tokenizer.convert_tokens_to_ids(BEGINNING_OF_ANSWER_STRING)\n+        # get boa index for each outputted sequence tensor\n+        # start all generated sequences from the beginning of the answer token, pad to have consistent length\n+        unpadded_output_sequences = [\n+            seq[(seq == beginning_of_answer).nonzero(as_tuple=True)[0] + 1 :] for seq in generated_outputs\n+        ]\n+        max_len = max(len(seq) for seq in unpadded_output_sequences)\n+        # convert to torch and pad sequences\n+        padded_output_sequences = torch.full((len(unpadded_output_sequences), max_len), self.pad_token_id)\n+        for i, seq in enumerate(unpadded_output_sequences):\n+            padded_output_sequences[i, : len(seq)] = torch.tensor(seq)\n+\n+        return self.batch_decode(padded_output_sequences, skip_special_tokens=True)\n+\n     def batch_decode(self, *args, **kwargs):\n         \"\"\"\n         This method forwards all its arguments to LlamaTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please"
        },
        {
            "sha": "e9e96fa765d841337e32b80252dc5d11a8ebf4d9",
            "filename": "src/transformers/models/git/processing_git.py",
            "status": "modified",
            "additions": 17,
            "deletions": 0,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/203e27059b519bf12d6d503d7f4020a8a1089870/src%2Ftransformers%2Fmodels%2Fgit%2Fprocessing_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/203e27059b519bf12d6d503d7f4020a8a1089870/src%2Ftransformers%2Fmodels%2Fgit%2Fprocessing_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgit%2Fprocessing_git.py?ref=203e27059b519bf12d6d503d7f4020a8a1089870",
            "patch": "@@ -22,12 +22,16 @@\n from ...image_utils import ImageInput\n from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack, _validate_images_text_input_order\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n+from ...utils import logging\n \n \n class GitProcessorKwargs(ProcessingKwargs, total=False):\n     _defaults = {}\n \n \n+logger = logging.get_logger(__name__)\n+\n+\n class GitProcessor(ProcessorMixin):\n     r\"\"\"\n     Constructs a GIT processor which wraps a CLIP image processor and a BERT tokenizer into a single processor.\n@@ -91,6 +95,15 @@ def __call__(\n               `None`).\n             - **pixel_values** -- Pixel values to be fed to a model. Returned when `images` is not `None`.\n         \"\"\"\n+        legacy = kwargs.pop(\"legacy\", True)\n+        if legacy:\n+            logger.warning_once(\n+                \"Legacy behavior is being used. The current behavior will be deprecated in version 5.0.0. \"\n+                \"In the new behavior, if both images and text are provided, the last token (EOS token) \"\n+                \"of the input_ids and attention_mask tensors will be removed. \"\n+                \"To test the new behavior, set `legacy=False`as a processor call argument.\"\n+            )\n+\n         if text is None and images is None:\n             raise ValueError(\"You have to specify either text or images. Both cannot be none.\")\n \n@@ -110,6 +123,10 @@ def __call__(\n         if images is not None:\n             image_features = self.image_processor(images, **output_kwargs[\"images_kwargs\"])\n             data.update(image_features)\n+            if not legacy:\n+                data[\"input_ids\"] = data[\"input_ids\"][:, :-1]\n+                data[\"attention_mask\"] = data[\"attention_mask\"][:, :-1]\n+\n         return BatchFeature(data=data, tensor_type=output_kwargs[\"common_kwargs\"].get(\"return_tensors\"))\n \n     def batch_decode(self, *args, **kwargs):"
        },
        {
            "sha": "d7befd899f3ad37eb6dafdc71675e93c5f4d8bb8",
            "filename": "src/transformers/models/kosmos2/processing_kosmos2.py",
            "status": "modified",
            "additions": 15,
            "deletions": 0,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/203e27059b519bf12d6d503d7f4020a8a1089870/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fprocessing_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/203e27059b519bf12d6d503d7f4020a8a1089870/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fprocessing_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fprocessing_kosmos2.py?ref=203e27059b519bf12d6d503d7f4020a8a1089870",
            "patch": "@@ -428,6 +428,21 @@ def post_process_generation(self, text, cleanup_and_extract=True):\n             return clean_text_and_extract_entities_with_bboxes(caption)\n         return caption\n \n+    def post_process_image_text_to_text(self, generated_outputs):\n+        \"\"\"\n+        Post-process the output of the model to decode the text.\n+\n+        Args:\n+            generated_outputs (`torch.Tensor` or `np.ndarray`):\n+                The output of the model `generate` function. The output is expected to be a tensor of shape `(batch_size, sequence_length)`\n+                or `(sequence_length,)`.\n+\n+        Returns:\n+            `List[str]`: The decoded text.\n+        \"\"\"\n+        generated_texts = self.batch_decode(generated_outputs, skip_special_tokens=True)\n+        return [self.post_process_generation(text, cleanup_and_extract=False) for text in generated_texts]\n+\n     @property\n     # Copied from transformers.models.blip.processing_blip.BlipProcessor.model_input_names\n     def model_input_names(self):"
        },
        {
            "sha": "f183c3c1b62b52b788b8147f1e8d152e9a9d4089",
            "filename": "src/transformers/models/mllama/processing_mllama.py",
            "status": "modified",
            "additions": 16,
            "deletions": 0,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/203e27059b519bf12d6d503d7f4020a8a1089870/src%2Ftransformers%2Fmodels%2Fmllama%2Fprocessing_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/203e27059b519bf12d6d503d7f4020a8a1089870/src%2Ftransformers%2Fmodels%2Fmllama%2Fprocessing_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fprocessing_mllama.py?ref=203e27059b519bf12d6d503d7f4020a8a1089870",
            "patch": "@@ -342,6 +342,22 @@ def decode(self, *args, **kwargs):\n         \"\"\"\n         return self.tokenizer.decode(*args, **kwargs)\n \n+    def post_process_image_text_to_text(self, generated_outputs):\n+        \"\"\"\n+        Post-process the output of the model to decode the text.\n+\n+        Args:\n+            generated_outputs (`torch.Tensor` or `np.ndarray`):\n+                The output of the model `generate` function. The output is expected to be a tensor of shape `(batch_size, sequence_length)`\n+                or `(sequence_length,)`.\n+\n+        Returns:\n+            `List[str]`: The decoded text.\n+        \"\"\"\n+        return self.tokenizer.batch_decode(\n+            generated_outputs, skip_special_tokens=True, clean_up_tokenization_spaces=False\n+        )\n+\n     @property\n     def model_input_names(self):\n         tokenizer_input_names = self.tokenizer.model_input_names"
        },
        {
            "sha": "3dcf145ea41ffcaf0fa68899da8731aa20138326",
            "filename": "src/transformers/models/owlv2/image_processing_owlv2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/203e27059b519bf12d6d503d7f4020a8a1089870/src%2Ftransformers%2Fmodels%2Fowlv2%2Fimage_processing_owlv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/203e27059b519bf12d6d503d7f4020a8a1089870/src%2Ftransformers%2Fmodels%2Fowlv2%2Fimage_processing_owlv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlv2%2Fimage_processing_owlv2.py?ref=203e27059b519bf12d6d503d7f4020a8a1089870",
            "patch": "@@ -19,7 +19,7 @@\n \n import numpy as np\n \n-from ...image_processing_utils import BaseImageProcessor, BatchFeature\n+from ...image_processing_utils import BaseImageProcessor, BatchFeature, get_size_dict\n from ...image_transforms import (\n     center_to_corners_format,\n     pad,\n@@ -399,6 +399,7 @@ def preprocess(\n         image_std = image_std if image_std is not None else self.image_std\n \n         size = size if size is not None else self.size\n+        size = get_size_dict(size)  # for BC\n \n         images = make_list_of_images(images)\n "
        },
        {
            "sha": "bf02531ffb864fbb129397cb36a111918c9346e9",
            "filename": "src/transformers/models/pix2struct/processing_pix2struct.py",
            "status": "modified",
            "additions": 20,
            "deletions": 0,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/203e27059b519bf12d6d503d7f4020a8a1089870/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fprocessing_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/203e27059b519bf12d6d503d7f4020a8a1089870/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fprocessing_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fprocessing_pix2struct.py?ref=203e27059b519bf12d6d503d7f4020a8a1089870",
            "patch": "@@ -21,6 +21,7 @@\n from ...feature_extraction_utils import BatchFeature\n from ...processing_utils import ImagesKwargs, ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import BatchEncoding, PreTokenizedInput, TextInput\n+from ...utils import logging\n \n \n class Pix2StructImagesKwargs(ImagesKwargs, total=False):\n@@ -48,6 +49,9 @@ class Pix2StructProcessorKwargs(ProcessingKwargs, total=False):\n     }\n \n \n+logger = logging.get_logger(__name__)\n+\n+\n class Pix2StructProcessor(ProcessorMixin):\n     r\"\"\"\n     Constructs a PIX2STRUCT processor which wraps a BERT tokenizer and PIX2STRUCT image processor into a single\n@@ -85,6 +89,15 @@ def __call__(\n \n         Please refer to the docstring of the above two methods for more information.\n         \"\"\"\n+        legacy = kwargs.pop(\"legacy\", True)\n+        if legacy:\n+            logger.warning_once(\n+                \"Legacy behavior is being used. The current behavior will be deprecated in version 5.0.0. \"\n+                \"In the new behavior, If both images and text are provided, image_processor is not a VQA processor, and `add_special_tokens` is unset, \"\n+                \"the default value of `add_special_tokens` will be changed to `False` when calling the tokenizer. \"\n+                \"To test the new behavior, set `legacy=False`as a processor call argument.\"\n+            )\n+\n         if images is None and text is None:\n             raise ValueError(\"You have to specify either images or text.\")\n \n@@ -93,8 +106,12 @@ def __call__(\n             tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n             **kwargs,\n         )\n+        add_special_tokens = output_kwargs[\"text_kwargs\"].pop(\"add_special_tokens\", None)\n         # Get only text\n         if images is None and not self.image_processor.is_vqa:\n+            output_kwargs[\"text_kwargs\"][\"add_special_tokens\"] = (\n+                add_special_tokens if add_special_tokens is not None else True\n+            )\n             self.current_processor = self.tokenizer\n             text_encoding = self.tokenizer(text=text, **output_kwargs[\"text_kwargs\"])\n             return text_encoding\n@@ -108,6 +125,9 @@ def __call__(\n             encoding_image_processor = self.image_processor(images, **output_kwargs[\"images_kwargs\"])\n \n         if text is not None and not self.image_processor.is_vqa:\n+            output_kwargs[\"text_kwargs\"][\"add_special_tokens\"] = (\n+                add_special_tokens if add_special_tokens is not None else legacy\n+            )\n             text_encoding = self.tokenizer(text=text, **output_kwargs[\"text_kwargs\"])\n \n             if \"attention_mask\" in text_encoding:"
        },
        {
            "sha": "b453b4078c7e810217c4da52ded40cba4df703b9",
            "filename": "src/transformers/models/qwen2_vl/processing_qwen2_vl.py",
            "status": "modified",
            "additions": 16,
            "deletions": 0,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/203e27059b519bf12d6d503d7f4020a8a1089870/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fprocessing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/203e27059b519bf12d6d503d7f4020a8a1089870/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fprocessing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fprocessing_qwen2_vl.py?ref=203e27059b519bf12d6d503d7f4020a8a1089870",
            "patch": "@@ -168,6 +168,22 @@ def decode(self, *args, **kwargs):\n         \"\"\"\n         return self.tokenizer.decode(*args, **kwargs)\n \n+    def post_process_image_text_to_text(self, generated_outputs):\n+        \"\"\"\n+        Post-process the output of the model to decode the text.\n+\n+        Args:\n+            generated_outputs (`torch.Tensor` or `np.ndarray`):\n+                The output of the model `generate` function. The output is expected to be a tensor of shape `(batch_size, sequence_length)`\n+                or `(sequence_length,)`.\n+\n+        Returns:\n+            `List[str]`: The decoded text.\n+        \"\"\"\n+        return self.tokenizer.batch_decode(\n+            generated_outputs, skip_special_tokens=True, clean_up_tokenization_spaces=False\n+        )\n+\n     @property\n     def model_input_names(self):\n         tokenizer_input_names = self.tokenizer.model_input_names"
        },
        {
            "sha": "33349af0366d77e68190472624f6637eeae9d1e3",
            "filename": "src/transformers/models/udop/processing_udop.py",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/203e27059b519bf12d6d503d7f4020a8a1089870/src%2Ftransformers%2Fmodels%2Fudop%2Fprocessing_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/203e27059b519bf12d6d503d7f4020a8a1089870/src%2Ftransformers%2Fmodels%2Fudop%2Fprocessing_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Fprocessing_udop.py?ref=203e27059b519bf12d6d503d7f4020a8a1089870",
            "patch": "@@ -208,20 +208,6 @@ def decode(self, *args, **kwargs):\n         \"\"\"\n         return self.tokenizer.decode(*args, **kwargs)\n \n-    def post_process_image_text_to_text(self, generated_outputs):\n-        \"\"\"\n-        Post-process the output of the model to decode the text.\n-\n-        Args:\n-            generated_outputs (`torch.Tensor` or `np.ndarray`):\n-                The output of the model `generate` function. The output is expected to be a tensor of shape `(batch_size, sequence_length)`\n-                or `(sequence_length,)`.\n-\n-        Returns:\n-            `List[str]`: The decoded text.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(generated_outputs, skip_special_tokens=True)\n-\n     @property\n     def model_input_names(self):\n         return [\"pixel_values\", \"input_ids\", \"bbox\", \"attention_mask\"]"
        },
        {
            "sha": "07156b3cf1dbe23f947fc546f0e54afda408cb58",
            "filename": "src/transformers/pipelines/__init__.py",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/203e27059b519bf12d6d503d7f4020a8a1089870/src%2Ftransformers%2Fpipelines%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/203e27059b519bf12d6d503d7f4020a8a1089870/src%2Ftransformers%2Fpipelines%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2F__init__.py?ref=203e27059b519bf12d6d503d7f4020a8a1089870",
            "patch": "@@ -67,6 +67,7 @@\n from .image_classification import ImageClassificationPipeline\n from .image_feature_extraction import ImageFeatureExtractionPipeline\n from .image_segmentation import ImageSegmentationPipeline\n+from .image_text_to_text import ImageTextToTextPipeline\n from .image_to_image import ImageToImagePipeline\n from .image_to_text import ImageToTextPipeline\n from .mask_generation import MaskGenerationPipeline\n@@ -119,6 +120,7 @@\n         AutoModelForDocumentQuestionAnswering,\n         AutoModelForImageClassification,\n         AutoModelForImageSegmentation,\n+        AutoModelForImageTextToText,\n         AutoModelForMaskedLM,\n         AutoModelForMaskGeneration,\n         AutoModelForObjectDetection,\n@@ -384,6 +386,17 @@\n         },\n         \"type\": \"multimodal\",\n     },\n+    \"image-text-to-text\": {\n+        \"impl\": ImageTextToTextPipeline,\n+        \"tf\": (),\n+        \"pt\": (AutoModelForImageTextToText,) if is_torch_available() else (),\n+        \"default\": {\n+            \"model\": {\n+                \"pt\": (\"llava-hf/llava-onevision-qwen2-0.5b-ov-hf\", \"2c9ba3b\"),\n+            }\n+        },\n+        \"type\": \"multimodal\",\n+    },\n     \"object-detection\": {\n         \"impl\": ObjectDetectionPipeline,\n         \"tf\": (),\n@@ -601,6 +614,7 @@ def pipeline(\n             - `\"image-classification\"`: will return a [`ImageClassificationPipeline`].\n             - `\"image-feature-extraction\"`: will return an [`ImageFeatureExtractionPipeline`].\n             - `\"image-segmentation\"`: will return a [`ImageSegmentationPipeline`].\n+            - `\"image-text-to-text\"`: will return a [`ImageTextToTextPipeline`].\n             - `\"image-to-image\"`: will return a [`ImageToImagePipeline`].\n             - `\"image-to-text\"`: will return a [`ImageToTextPipeline`].\n             - `\"mask-generation\"`: will return a [`MaskGenerationPipeline`]."
        },
        {
            "sha": "d2d4f198d418476dcc4a7974b35a11957086c498",
            "filename": "src/transformers/pipelines/base.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/203e27059b519bf12d6d503d7f4020a8a1089870/src%2Ftransformers%2Fpipelines%2Fbase.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/203e27059b519bf12d6d503d7f4020a8a1089870/src%2Ftransformers%2Fpipelines%2Fbase.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fbase.py?ref=203e27059b519bf12d6d503d7f4020a8a1089870",
            "patch": "@@ -951,6 +951,14 @@ def __init__(\n         self._num_workers = kwargs.pop(\"num_workers\", None)\n         self._preprocess_params, self._forward_params, self._postprocess_params = self._sanitize_parameters(**kwargs)\n \n+        # In processor only mode, we can get the modality processors from the processor\n+        if self.processor is not None and all(\n+            [self.tokenizer is None, self.feature_extractor is None, self.image_processor is None]\n+        ):\n+            self.tokenizer = getattr(self.processor, \"tokenizer\", None)\n+            self.feature_extractor = getattr(self.processor, \"feature_extractor\", None)\n+            self.image_processor = getattr(self.processor, \"image_processor\", None)\n+\n         if self.image_processor is None and self.feature_extractor is not None:\n             if isinstance(self.feature_extractor, BaseImageProcessor):\n                 # Backward compatible change, if users called"
        },
        {
            "sha": "39738ffc385dbede063f94d3b3872447b2f262cf",
            "filename": "src/transformers/pipelines/image_text_to_text.py",
            "status": "added",
            "additions": 416,
            "deletions": 0,
            "changes": 416,
            "blob_url": "https://github.com/huggingface/transformers/blob/203e27059b519bf12d6d503d7f4020a8a1089870/src%2Ftransformers%2Fpipelines%2Fimage_text_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/203e27059b519bf12d6d503d7f4020a8a1089870/src%2Ftransformers%2Fpipelines%2Fimage_text_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fimage_text_to_text.py?ref=203e27059b519bf12d6d503d7f4020a8a1089870",
            "patch": "@@ -0,0 +1,416 @@\n+# coding=utf-8\n+# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import enum\n+from typing import Dict, List, Optional, Union\n+\n+from ..processing_utils import ProcessingKwargs, Unpack\n+from ..utils import (\n+    add_end_docstrings,\n+    is_torch_available,\n+    is_vision_available,\n+    logging,\n+    requires_backends,\n+)\n+from .base import Pipeline, build_pipeline_init_args\n+\n+\n+if is_vision_available():\n+    from PIL import Image\n+\n+    from ..image_utils import load_images, valid_images\n+\n+\n+if is_torch_available():\n+    from ..models.auto.modeling_auto import MODEL_FOR_IMAGE_TEXT_TO_TEXT_MAPPING_NAMES\n+    from .pt_utils import KeyDataset\n+\n+logger = logging.get_logger(__name__)\n+\n+IMAGE_TOKEN = \"<image>\"\n+\n+\n+class ReturnType(enum.Enum):\n+    TENSORS = 0\n+    NEW_TEXT = 1\n+    FULL_TEXT = 2\n+\n+\n+class Chat:\n+    \"\"\"This class is intended to just be used internally in this pipeline and not exposed to users. We convert chats\n+    to this format because the rest of the pipeline code tends to assume that lists of messages are\n+    actually a batch of samples rather than messages in the same conversation.\"\"\"\n+\n+    def __init__(self, messages: Dict, images: Union[str, List[str], \"Image.Image\", List[\"Image.Image\"]]):\n+        for message in messages:\n+            if not (\"role\" in message and \"content\" in message):\n+                raise ValueError(\"When passing chat dicts as input, each dict must have a 'role' and 'content' key.\")\n+        images = retrieve_images_in_messages(messages, images)\n+\n+        self.messages = messages\n+        self.images = images\n+\n+\n+def retrieve_images_in_messages(\n+    messages: dict, images: Optional[Union[str, List[str], \"Image.Image\", List[\"Image.Image\"]]]\n+):\n+    \"\"\"\n+    Retrieve and combine images from the chat and the images passed as input.\n+    \"\"\"\n+    if images is None:\n+        images = []\n+    idx_images = 0\n+    retrieved_images = []\n+    for message in messages:\n+        for content in message[\"content\"]:\n+            if isinstance(content, dict) and content.get(\"type\") == \"image\":\n+                if \"image\" in content:\n+                    retrieved_images.append(content[\"image\"])\n+                elif idx_images < len(images):\n+                    retrieved_images.append(images[idx_images])\n+                    idx_images += 1\n+                else:\n+                    raise ValueError(\n+                        \"The number of images in the chat messages should be the same as the number of images passed to the pipeline.\"\n+                    )\n+\n+    # The number of images passed should be consistent with the number of images in the chat without an image key\n+    if idx_images != len(images):\n+        raise ValueError(\n+            \"The number of images in the chat messages should be the same as the number of images passed to the pipeline.\"\n+        )\n+\n+    return retrieved_images\n+\n+\n+@add_end_docstrings(build_pipeline_init_args(has_processor=True))\n+class ImageTextToTextPipeline(Pipeline):\n+    \"\"\"\n+    Image-text-to-text pipeline using an `AutoModelForImageTextToText`. This pipeline generates text given an image and text.\n+    When the underlying model is a conversational model, it can also accept one or more chats,\n+    in which case the pipeline will operate in chat mode and will continue the chat(s) by adding its response(s).\n+    Each chat takes the form of a list of dicts, where each dict contains \"role\" and \"content\" keys.\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import pipeline\n+\n+    >>> pipe = pipeline(task=\"image-text-to-text\", model=\"Salesforce/blip-image-captioning-base\")\n+    >>> pipe(\"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\", text=\"A photo of\")\n+    [{'generated_text': 'a photo of two birds'}]\n+    ```\n+\n+    ```python\n+    >>> from transformers import pipeline\n+\n+    >>> pipe = pipeline(\"image-text-to-text\", model=\"llava-hf/llava-interleave-qwen-0.5b-hf\")\n+    >>> messages = [\n+    >>>     {\n+    >>>         \"role\": \"user\",\n+    >>>         \"content\": [\n+    >>>             {\n+    >>>                 \"type\": \"image\",\n+    >>>                 \"url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n+    >>>             },\n+    >>>             {\"type\": \"text\", \"text\": \"Describe this image.\"},\n+    >>>         ],\n+    >>>     },\n+    >>>     {\n+    >>>         \"role\": \"assistant\",\n+    >>>         \"content\": [\n+    >>>             {\"type\": \"text\", \"text\": \"There is a dog and\"},\n+    >>>         ],\n+    >>>     },\n+    >>> ]\n+    >>> pipe(text=messages, max_new_tokens=20, return_full_text=False)\n+    [{'input_text': [{'role': 'user',\n+        'content': [{'type': 'image',\n+        'url': 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg'},\n+        {'type': 'text', 'text': 'Describe this image.'}]},\n+    {'role': 'assistant',\n+        'content': [{'type': 'text', 'text': 'There is a dog and'}]}],\n+    'generated_text': ' a person in the image. The dog is sitting on the sand, and the person is sitting on'}]\n+    ```\n+\n+    Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)\n+\n+    This image-text to text pipeline can currently be loaded from pipeline() using the following task identifier:\n+    \"image-text-to-text\".\n+\n+    See the list of available models on\n+    [huggingface.co/models](https://huggingface.co/models?pipeline_tag=image-text-to-text).\n+    \"\"\"\n+\n+    _load_processor = True\n+    _load_image_processor = False\n+    _load_feature_extractor = False\n+    _load_tokenizer = False\n+\n+    def __init__(self, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        requires_backends(self, \"vision\")\n+        self.check_model_type(MODEL_FOR_IMAGE_TEXT_TO_TEXT_MAPPING_NAMES)\n+\n+    def _sanitize_parameters(\n+        self,\n+        max_new_tokens=None,\n+        generate_kwargs=None,\n+        timeout=None,\n+        return_full_text=None,\n+        return_tensors=None,\n+        return_type=None,\n+        continue_final_message=None,\n+        **kwargs: Unpack[ProcessingKwargs],\n+    ):\n+        forward_kwargs = {}\n+        preprocess_params = {}\n+        postprocess_params = {}\n+\n+        preprocess_params[\"processing_kwargs\"] = kwargs\n+\n+        if timeout is not None:\n+            preprocess_params[\"timeout\"] = timeout\n+\n+        if continue_final_message is not None:\n+            preprocess_params[\"continue_final_message\"] = continue_final_message\n+\n+        if generate_kwargs is not None:\n+            forward_kwargs[\"generate_kwargs\"] = generate_kwargs\n+\n+        if max_new_tokens is not None:\n+            if \"generate_kwargs\" not in forward_kwargs:\n+                forward_kwargs[\"generate_kwargs\"] = {}\n+            if \"max_new_tokens\" in forward_kwargs[\"generate_kwargs\"]:\n+                raise ValueError(\n+                    \"'max_new_tokens' is defined twice, once in 'generate_kwargs' and once as a direct parameter,\"\n+                    \" please use only one\"\n+                )\n+            forward_kwargs[\"generate_kwargs\"][\"max_new_tokens\"] = max_new_tokens\n+\n+        if return_full_text is not None and return_type is None:\n+            if return_tensors is not None:\n+                raise ValueError(\"`return_full_text` is mutually exclusive with `return_tensors`\")\n+            return_type = ReturnType.FULL_TEXT if return_full_text else ReturnType.NEW_TEXT\n+        if return_tensors is not None and return_type is None:\n+            return_type = ReturnType.TENSORS\n+        if return_type is not None:\n+            postprocess_params[\"return_type\"] = return_type\n+        if continue_final_message is not None:\n+            postprocess_params[\"continue_final_message\"] = continue_final_message\n+\n+        return preprocess_params, forward_kwargs, postprocess_params\n+\n+    def __call__(\n+        self,\n+        images: Optional[\n+            Union[str, List[str], List[List[str]], \"Image.Image\", List[\"Image.Image\"], List[List[\"Image.Image\"]]]\n+        ] = None,\n+        text: Optional[Union[str, List[str], List[dict]]] = None,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Generate a text given text and the image(s) passed as inputs.\n+\n+        Args:\n+            images (`str`, `List[str]`, `PIL.Image or `List[PIL.Image]`):\n+                The pipeline handles three types of images:\n+\n+                - A string containing a HTTP(s) link pointing to an image\n+                - A string containing a local path to an image\n+                - An image loaded in PIL directly\n+\n+                The pipeline accepts either a single image or a batch of images.\n+            text (str, List[str], `List[Dict[str, Union[str, PIL.Image]]]`):\n+                The text to be used for generation. If a list of strings is passed, the length of the list should be the\n+                same as the number of images. Text can also follow the chat format: a list of dictionaries where each\n+                dictionary represents a message in a conversation. Each dictionary should have two keys: 'role' and\n+                'content'. 'role' should be one of 'user', 'system' or 'assistant'. 'content' should be a list of dictionary\n+                containing the text of the message and the type of the message. The type of the message can be either\n+                'text' or 'image'. If the type is 'image', no text is needed.\n+            return_tensors (`bool`, *optional*, defaults to `False`):\n+                Returns the tensors of predictions (as token indices) in the outputs. If set to\n+                `True`, the decoded text is not returned.\n+            return_text (`bool`, *optional*):\n+                Returns the decoded texts in the outputs.\n+            return_full_text (`bool`, *optional*, defaults to `True`):\n+                If set to `False` only added text is returned, otherwise the full text is returned. Cannot be\n+                specified at the same time as `return_text`.\n+            continue_final_message( `bool`, *optional*): This indicates that you want the model to continue the\n+                last message in the input chat rather than starting a new one, allowing you to \"prefill\" its response.\n+                By default this is `True` when the final message in the input chat has the `assistant` role and\n+                `False` otherwise, but you can manually override that behaviour by setting this flag.\n+\n+        Return:\n+            A list or a list of list of `dict`: Each result comes as a dictionary with the following key (cannot return a combination\n+            of both `generated_text` and `generated_token_ids`):\n+\n+            - **generated_text** (`str`, present when `return_text=True`) -- The generated text.\n+            - **generated_token_ids** (`torch.Tensor`, present when `return_tensors=True`) -- The token\n+                ids of the generated text.\n+            - **input_text** (`str`) -- The input text.\n+        \"\"\"\n+        if images is None and text is None:\n+            raise ValueError(\"You must at least provide either text or images.\")\n+        if images is not None and text is None and not valid_images(images):\n+            \"\"\"\n+            Supports the following format\n+            - {\"image\": image, \"text\": text}\n+            - [{\"image\": image, \"text\": text}]\n+            - Generator and datasets\n+            This is a common pattern in other multimodal pipelines, so we support it here as well.\n+            \"\"\"\n+            return super().__call__(images, **kwargs)\n+\n+        if isinstance(text, (list, tuple, KeyDataset)) and isinstance(text[0], (list, tuple, dict)):\n+            # We have one or more prompts in list-of-dicts format, so this is chat mode\n+            if isinstance(text[0], dict):\n+                return super().__call__(Chat(text, images), **kwargs)\n+            else:\n+                if images is None:\n+                    images = [None] * len(text)\n+                chats = [Chat(chat, image) for chat, image in zip(text, images)]  #   \n+                return super().__call__(chats, **kwargs)\n+\n+        # encourage the user to use the chat format if supported\n+        if getattr(self.processor, \"chat_template\", None) is not None:\n+            logger.warning_once(\n+                \"The input data was not formatted as a chat with dicts containing 'role' and 'content' keys, even though this model supports chat. \"\n+                \"Consider using the chat format for better results. For more information, see https://huggingface.co/docs/transformers/en/chat_templating\"\n+            )\n+\n+        # support text only generation\n+        if images is None:\n+            return super().__call__(text, **kwargs)\n+        if text is None:\n+            raise ValueError(\"You must provide text for this pipeline.\")\n+\n+        return super().__call__({\"images\": images, \"text\": text}, **kwargs)\n+\n+    def preprocess(self, inputs=None, timeout=None, continue_final_message=None, processing_kwargs=None):\n+        # In case we only have text inputs\n+        if isinstance(inputs, (list, tuple, str)):\n+            images = None\n+            text = inputs\n+            inputs_text = inputs\n+        else:\n+            if isinstance(inputs, Chat):\n+                # If the user passes a chat that ends in an assistant message, we treat it as a prefill by default\n+                # because very few models support multiple separate, consecutive assistant messages\n+                if continue_final_message is None:\n+                    continue_final_message = inputs.messages[-1][\"role\"] == \"assistant\"\n+                text = self.processor.apply_chat_template(\n+                    inputs.messages,\n+                    add_generation_prompt=not continue_final_message,\n+                    continue_final_message=continue_final_message,\n+                    return_tensors=self.framework,\n+                )\n+                inputs_text = inputs\n+                images = inputs.images\n+            else:\n+                text = inputs[\"text\"]\n+                inputs_text = inputs[\"text\"]\n+                images = inputs[\"images\"]\n+\n+            images = load_images(images)\n+\n+        # if batched text inputs, we set padding to True unless specified otherwise\n+        if isinstance(text, (list, tuple)) and len(text) > 1:\n+            processing_kwargs.setdefault(\"padding\", True)\n+        model_inputs = self.processor(\n+            images=images, text=text, return_tensors=self.framework, legacy=False, **processing_kwargs\n+        ).to(dtype=self.torch_dtype)\n+\n+        model_inputs[\"text\"] = inputs_text\n+\n+        return model_inputs\n+\n+    def _forward(self, model_inputs, generate_kwargs=None):\n+        generate_kwargs = {} if generate_kwargs is None else generate_kwargs\n+        prompt_text = model_inputs.pop(\"text\")\n+        input_ids = (\n+            model_inputs[\"input_ids\"] if \"input_ids\" in model_inputs else model_inputs[\"decoder_input_ids\"]\n+        )  # for decoder-only models\n+        generated_sequence = self.model.generate(**model_inputs, **generate_kwargs)\n+\n+        return {\"generated_sequence\": generated_sequence, \"prompt_text\": prompt_text, \"input_ids\": input_ids}\n+\n+    def postprocess(self, model_outputs, return_type=ReturnType.FULL_TEXT, continue_final_message=None):\n+        input_texts = model_outputs[\"prompt_text\"]\n+        input_texts = [input_texts] if isinstance(input_texts, (str, Chat)) else input_texts\n+        generated_sequence = model_outputs[\"generated_sequence\"]\n+        input_ids = model_outputs[\"input_ids\"]\n+        if return_type == ReturnType.TENSORS:\n+            return [\n+                {\"input_text\": input_texts[i], \"generated_token_ids\": generated_sequence[i]}\n+                for i in range(len(input_texts))\n+            ]\n+\n+        # Decode inputs and outputs the same way to remove input text from generated text if present\n+        generated_texts = self.processor.post_process_image_text_to_text(generated_sequence)\n+        decoded_inputs = self.processor.post_process_image_text_to_text(input_ids)\n+\n+        # Force consistent behavior for including the input text in the output\n+        if return_type in {ReturnType.NEW_TEXT, ReturnType.FULL_TEXT}:\n+            # Remove the input text from the generated text if the generated text starts with the input text\n+            # (accounting for the possibility of a space between the input and generated text)\n+            new_generated_texts = []\n+            for text_generated, decoded_input in zip(generated_texts, decoded_inputs):\n+                # There can be added characters before the input text, so we need to find the beginning of the input text in the generated text\n+                index_input_text = text_generated.find(decoded_input)\n+                # Limit the search to 2 residual characters, like spaces or new lines, to avoid removing a large part of the answer\n+                if 0 <= index_input_text <= 2:\n+                    # If the input text is found, we remove it\n+                    new_generated_texts.append(text_generated[index_input_text + len(decoded_input) :])\n+                else:\n+                    new_generated_texts.append(text_generated)\n+            generated_texts = new_generated_texts\n+        if return_type == ReturnType.FULL_TEXT:\n+            full_texts = []\n+            for prompt_text, generated_text in zip(input_texts, generated_texts):\n+                if isinstance(prompt_text, str):\n+                    generated_text = prompt_text + generated_text\n+                elif isinstance(prompt_text, Chat):\n+                    if continue_final_message is None:\n+                        # If the user passes a chat ending in an assistant message, we treat it as a prefill by\n+                        # default because very few models support multiple separate, consecutive assistant messages\n+                        continue_final_message = prompt_text.messages[-1][\"role\"] == \"assistant\"\n+                    if continue_final_message:\n+                        # With assistant prefill, concat onto the end of the last message\n+                        new_text = dict(prompt_text.messages[-1][\"content\"][-1].items())\n+                        new_text[\"text\"] += generated_text\n+                        generated_text = list(prompt_text.messages)[:-1] + [\n+                            {\n+                                \"role\": prompt_text.messages[-1][\"role\"],\n+                                \"content\": prompt_text.messages[-1][\"content\"][:-1] + [new_text],\n+                            }\n+                        ]\n+                    else:\n+                        # When we're not starting from a prefill, the output is a new assistant message\n+                        generated_text = list(prompt_text.messages) + [\n+                            {\"role\": \"assistant\", \"content\": generated_text}\n+                        ]\n+                full_texts.append(generated_text)\n+            generated_texts = full_texts\n+\n+        records = [\n+            {\n+                \"input_text\": input_text.messages if isinstance(input_text, Chat) else input_text,\n+                \"generated_text\": generated_text,\n+            }\n+            for input_text, generated_text in zip(input_texts, generated_texts)\n+        ]\n+\n+        return records"
        },
        {
            "sha": "afd67b6ac9edee9c751ab612b873d37534ea1706",
            "filename": "src/transformers/pipelines/image_to_text.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/203e27059b519bf12d6d503d7f4020a8a1089870/src%2Ftransformers%2Fpipelines%2Fimage_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/203e27059b519bf12d6d503d7f4020a8a1089870/src%2Ftransformers%2Fpipelines%2Fimage_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fimage_to_text.py?ref=203e27059b519bf12d6d503d7f4020a8a1089870",
            "patch": "@@ -134,6 +134,10 @@ def preprocess(self, image, prompt=None, timeout=None):\n         image = load_image(image, timeout=timeout)\n \n         if prompt is not None:\n+            logger.warning_once(\n+                \"Passing `prompt` to the `image-to-text` pipeline is deprecated and will be removed in version 4.48\"\n+                \" of  Transformers. Use the `image-text-to-text` pipeline instead\",\n+            )\n             if not isinstance(prompt, str):\n                 raise ValueError(\n                     f\"Received an invalid text input, got - {type(prompt)} - but expected a single string. \""
        },
        {
            "sha": "ce8da7340bcce527f6ef8c013f1f609c341f9857",
            "filename": "src/transformers/pipelines/zero_shot_object_detection.py",
            "status": "modified",
            "additions": 16,
            "deletions": 1,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/203e27059b519bf12d6d503d7f4020a8a1089870/src%2Ftransformers%2Fpipelines%2Fzero_shot_object_detection.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/203e27059b519bf12d6d503d7f4020a8a1089870/src%2Ftransformers%2Fpipelines%2Fzero_shot_object_detection.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fzero_shot_object_detection.py?ref=203e27059b519bf12d6d503d7f4020a8a1089870",
            "patch": "@@ -7,7 +7,7 @@\n if is_vision_available():\n     from PIL import Image\n \n-    from ..image_utils import load_image\n+    from ..image_utils import load_image, valid_images\n \n if is_torch_available():\n     import torch\n@@ -130,8 +130,23 @@ def __call__(\n \n         if isinstance(image, (str, Image.Image)):\n             inputs = {\"image\": image, \"candidate_labels\": candidate_labels}\n+        elif isinstance(image, (list, tuple)) and valid_images(image):\n+            return list(\n+                super().__call__(\n+                    ({\"image\": img, \"candidate_labels\": labels} for img, labels in zip(image, candidate_labels)),\n+                    **kwargs,\n+                )\n+            )\n         else:\n+            \"\"\"\n+            Supports the following format\n+            - {\"image\": image, \"candidate_labels\": candidate_labels}\n+            - [{\"image\": image, \"candidate_labels\": candidate_labels}]\n+            - Generator and datasets\n+            This is a common pattern in other multimodal pipelines, so we support it here as well.\n+            \"\"\"\n             inputs = image\n+\n         results = super().__call__(inputs, **kwargs)\n         return results\n "
        },
        {
            "sha": "b5b02f6a00aa09184abdff3ae2b5c585ab5a4b3d",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/203e27059b519bf12d6d503d7f4020a8a1089870/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/203e27059b519bf12d6d503d7f4020a8a1089870/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=203e27059b519bf12d6d503d7f4020a8a1089870",
            "patch": "@@ -1107,6 +1107,20 @@ def apply_chat_template(\n             conversation, chat_template=chat_template, tokenize=tokenize, **kwargs\n         )\n \n+    def post_process_image_text_to_text(self, generated_outputs):\n+        \"\"\"\n+        Post-process the output of a vlm to decode the text.\n+\n+        Args:\n+            generated_outputs (`torch.Tensor` or `np.ndarray`):\n+                The output of the model `generate` function. The output is expected to be a tensor of shape `(batch_size, sequence_length)`\n+                or `(sequence_length,)`.\n+\n+        Returns:\n+            `List[str]`: The decoded text.\n+        \"\"\"\n+        return self.tokenizer.batch_decode(generated_outputs, skip_special_tokens=True)\n+\n \n def _validate_images_text_input_order(images, text):\n     \"\"\""
        },
        {
            "sha": "d60c76393f02bbc92ed7b5cff003d952beeb8af0",
            "filename": "tests/models/blip/test_modeling_blip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/203e27059b519bf12d6d503d7f4020a8a1089870/tests%2Fmodels%2Fblip%2Ftest_modeling_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/203e27059b519bf12d6d503d7f4020a8a1089870/tests%2Fmodels%2Fblip%2Ftest_modeling_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip%2Ftest_modeling_blip.py?ref=203e27059b519bf12d6d503d7f4020a8a1089870",
            "patch": "@@ -436,6 +436,7 @@ class BlipModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n             \"feature-extraction\": BlipModel,\n             \"image-to-text\": BlipForConditionalGeneration,\n             \"visual-question-answering\": BlipForQuestionAnswering,\n+            \"image-text-to-text\": BlipForConditionalGeneration,\n         }\n         if is_torch_available()\n         else {}"
        },
        {
            "sha": "f2b945ef4451e46eb5ac6f247cf8864c4107fb27",
            "filename": "tests/models/blip_2/test_modeling_blip_2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/203e27059b519bf12d6d503d7f4020a8a1089870/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/203e27059b519bf12d6d503d7f4020a8a1089870/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py?ref=203e27059b519bf12d6d503d7f4020a8a1089870",
            "patch": "@@ -767,6 +767,7 @@ class Blip2ModelTest(ModelTesterMixin, PipelineTesterMixin, GenerationTesterMixi\n             \"feature-extraction\": Blip2Model,\n             \"image-to-text\": Blip2ForConditionalGeneration,\n             \"visual-question-answering\": Blip2ForConditionalGeneration,\n+            \"image-text-to-text\": Blip2ForConditionalGeneration,\n         }\n         if is_torch_available()\n         else {}"
        },
        {
            "sha": "bb2ba8b342817458b729be10f4f9d2e5af323bce",
            "filename": "tests/models/chameleon/test_modeling_chameleon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/203e27059b519bf12d6d503d7f4020a8a1089870/tests%2Fmodels%2Fchameleon%2Ftest_modeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/203e27059b519bf12d6d503d7f4020a8a1089870/tests%2Fmodels%2Fchameleon%2Ftest_modeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fchameleon%2Ftest_modeling_chameleon.py?ref=203e27059b519bf12d6d503d7f4020a8a1089870",
            "patch": "@@ -276,6 +276,7 @@ class ChameleonModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTester\n         {\n             \"feature-extraction\": ChameleonModel,\n             \"text-generation\": ChameleonForConditionalGeneration,\n+            \"image-text-to-text\": ChameleonForConditionalGeneration,\n         }\n         if is_torch_available()\n         else {}"
        },
        {
            "sha": "4bd66ab945f441585dd0130c8a038d0af1039b31",
            "filename": "tests/models/fuyu/test_modeling_fuyu.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/203e27059b519bf12d6d503d7f4020a8a1089870/tests%2Fmodels%2Ffuyu%2Ftest_modeling_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/203e27059b519bf12d6d503d7f4020a8a1089870/tests%2Fmodels%2Ffuyu%2Ftest_modeling_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffuyu%2Ftest_modeling_fuyu.py?ref=203e27059b519bf12d6d503d7f4020a8a1089870",
            "patch": "@@ -265,7 +265,9 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class FuyuModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (FuyuForCausalLM,) if is_torch_available() else ()\n-    pipeline_model_mapping = {\"text-generation\": FuyuForCausalLM} if is_torch_available() else {}\n+    pipeline_model_mapping = (\n+        {\"text-generation\": FuyuForCausalLM, \"image-text-to-text\": FuyuForCausalLM} if is_torch_available() else {}\n+    )\n \n     test_head_masking = False\n     test_pruning = False"
        },
        {
            "sha": "ccfb41459caf73ac684bafa4e1e420324939b3d0",
            "filename": "tests/models/git/test_modeling_git.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/203e27059b519bf12d6d503d7f4020a8a1089870/tests%2Fmodels%2Fgit%2Ftest_modeling_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/203e27059b519bf12d6d503d7f4020a8a1089870/tests%2Fmodels%2Fgit%2Ftest_modeling_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgit%2Ftest_modeling_git.py?ref=203e27059b519bf12d6d503d7f4020a8a1089870",
            "patch": "@@ -401,7 +401,12 @@ class GitModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin,\n     all_model_classes = (GitModel, GitForCausalLM) if is_torch_available() else ()\n     all_generative_model_classes = (GitForCausalLM,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n-        {\"feature-extraction\": GitModel, \"image-to-text\": GitForCausalLM, \"text-generation\": GitForCausalLM}\n+        {\n+            \"feature-extraction\": GitModel,\n+            \"image-to-text\": GitForCausalLM,\n+            \"text-generation\": GitForCausalLM,\n+            \"image-text-to-text\": GitForCausalLM,\n+        }\n         if is_torch_available()\n         else {}\n     )"
        },
        {
            "sha": "7be87fd78390aba190765df8bf6d344aebd2dcb4",
            "filename": "tests/models/idefics/test_modeling_idefics.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/203e27059b519bf12d6d503d7f4020a8a1089870/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/203e27059b519bf12d6d503d7f4020a8a1089870/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py?ref=203e27059b519bf12d6d503d7f4020a8a1089870",
            "patch": "@@ -332,7 +332,11 @@ def test_eager_matches_sdpa_generate(self):\n @require_torch\n class IdeficsModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (IdeficsModel, IdeficsForVisionText2Text) if is_torch_available() else ()\n-    pipeline_model_mapping = {\"feature-extraction\": IdeficsModel} if is_torch_available() else {}\n+    pipeline_model_mapping = (\n+        {\"feature-extraction\": IdeficsModel, \"image-text-to-text\": IdeficsForVisionText2Text}\n+        if is_torch_available()\n+        else {}\n+    )\n     test_pruning = False\n     test_headmasking = False\n     test_torchscript = False"
        },
        {
            "sha": "3dcd0bf5fbcdeb2dd1e45cfc9ea613d9aa42dd05",
            "filename": "tests/models/idefics2/test_modeling_idefics2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/203e27059b519bf12d6d503d7f4020a8a1089870/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/203e27059b519bf12d6d503d7f4020a8a1089870/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py?ref=203e27059b519bf12d6d503d7f4020a8a1089870",
            "patch": "@@ -375,6 +375,7 @@ class Idefics2ForConditionalGenerationModelTest(GenerationTesterMixin, ModelTest\n \n     all_model_classes = (Idefics2ForConditionalGeneration,) if is_torch_available() else ()\n     all_generative_model_classes = (Idefics2ForConditionalGeneration,) if is_torch_available() else ()\n+    pipeline_model_mapping = {\"image-text-to-text\": Idefics2ForConditionalGeneration} if is_torch_available() else ()\n     fx_compatible = False\n     test_pruning = False\n     test_resize_embeddings = True"
        },
        {
            "sha": "598f5882470e99e4b9c149e443b87d08963482bc",
            "filename": "tests/models/idefics3/test_modeling_idefics3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/203e27059b519bf12d6d503d7f4020a8a1089870/tests%2Fmodels%2Fidefics3%2Ftest_modeling_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/203e27059b519bf12d6d503d7f4020a8a1089870/tests%2Fmodels%2Fidefics3%2Ftest_modeling_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics3%2Ftest_modeling_idefics3.py?ref=203e27059b519bf12d6d503d7f4020a8a1089870",
            "patch": "@@ -317,6 +317,7 @@ class Idefics3ForConditionalGenerationModelTest(GenerationTesterMixin, ModelTest\n \n     all_model_classes = (Idefics3ForConditionalGeneration,) if is_torch_available() else ()\n     all_generative_model_classes = (Idefics3ForConditionalGeneration,) if is_torch_available() else ()\n+    pipeline_model_mapping = {\"image-text-to-text\": Idefics3ForConditionalGeneration} if is_torch_available() else ()\n     fx_compatible = False\n     test_pruning = False\n     test_resize_embeddings = True"
        },
        {
            "sha": "2771dac1e3767e27a9e06725ed3e7b8bbc7e398a",
            "filename": "tests/models/instructblip/test_modeling_instructblip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/203e27059b519bf12d6d503d7f4020a8a1089870/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/203e27059b519bf12d6d503d7f4020a8a1089870/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py?ref=203e27059b519bf12d6d503d7f4020a8a1089870",
            "patch": "@@ -455,6 +455,7 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class InstructBlipForConditionalGenerationDecoderOnlyTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n     all_model_classes = (InstructBlipForConditionalGeneration,) if is_torch_available() else ()\n+    pipeline_model_mapping = {\"image-text-to-text\": InstructBlipForConditionalGeneration}\n     fx_compatible = False\n     test_head_masking = False\n     test_pruning = False"
        },
        {
            "sha": "43266a750b8d6c4a189407018c4a379fa2c93c73",
            "filename": "tests/models/kosmos2/test_modeling_kosmos2.py",
            "status": "modified",
            "additions": 10,
            "deletions": 2,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/203e27059b519bf12d6d503d7f4020a8a1089870/tests%2Fmodels%2Fkosmos2%2Ftest_modeling_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/203e27059b519bf12d6d503d7f4020a8a1089870/tests%2Fmodels%2Fkosmos2%2Ftest_modeling_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fkosmos2%2Ftest_modeling_kosmos2.py?ref=203e27059b519bf12d6d503d7f4020a8a1089870",
            "patch": "@@ -257,7 +257,11 @@ class Kosmos2ModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase)\n     all_model_classes = (Kosmos2Model, Kosmos2ForConditionalGeneration) if is_torch_available() else ()\n     all_generative_model_classes = (Kosmos2ForConditionalGeneration,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n-        {\"feature-extraction\": Kosmos2Model, \"image-to-text\": Kosmos2ForConditionalGeneration}\n+        {\n+            \"feature-extraction\": Kosmos2Model,\n+            \"image-to-text\": Kosmos2ForConditionalGeneration,\n+            \"image-text-to-text\": Kosmos2ForConditionalGeneration,\n+        }\n         if is_torch_available()\n         else {}\n     )\n@@ -269,6 +273,7 @@ class Kosmos2ModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase)\n     _is_composite = True\n \n     # TODO: `image-to-text` pipeline for this model needs Processor.\n+    # TODO: Tiny model needs fixing for `image-text-to-text` (latent_query_num=3 not compatible with num_image_tokens=64).\n     def is_pipeline_test_to_skip(\n         self,\n         pipeline_test_case_name,\n@@ -279,7 +284,10 @@ def is_pipeline_test_to_skip(\n         feature_extractor_name,\n         processor_name,\n     ):\n-        return pipeline_test_case_name == \"ImageToTextPipelineTests\"\n+        return (\n+            pipeline_test_case_name == \"ImageToTextPipelineTests\"\n+            or pipeline_test_case_name == \"ImageTextToTextPipelineTests\"\n+        )\n \n     def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):\n         inputs_dict = copy.deepcopy(inputs_dict)"
        },
        {
            "sha": "9810ff7c2a56d4a8bce67a1e824fbd2ad759b32a",
            "filename": "tests/models/llava/test_modeling_llava.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/203e27059b519bf12d6d503d7f4020a8a1089870/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/203e27059b519bf12d6d503d7f4020a8a1089870/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py?ref=203e27059b519bf12d6d503d7f4020a8a1089870",
            "patch": "@@ -183,7 +183,11 @@ class LlavaForConditionalGenerationModelTest(ModelTesterMixin, GenerationTesterM\n \n     all_model_classes = (LlavaForConditionalGeneration,) if is_torch_available() else ()\n     all_generative_model_classes = (LlavaForConditionalGeneration,) if is_torch_available() else ()\n-    pipeline_model_mapping = {\"image-to-text\": LlavaForConditionalGeneration} if is_torch_available() else {}\n+    pipeline_model_mapping = (\n+        {\"image-to-text\": LlavaForConditionalGeneration, \"image-text-to-text\": LlavaForConditionalGeneration}\n+        if is_torch_available()\n+        else {}\n+    )\n     test_pruning = False\n     test_head_masking = False\n     _is_composite = True"
        },
        {
            "sha": "2146c94c18a4b402ad87d5968258eb38dfe39129",
            "filename": "tests/models/llava_next/test_modeling_llava_next.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/203e27059b519bf12d6d503d7f4020a8a1089870/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/203e27059b519bf12d6d503d7f4020a8a1089870/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py?ref=203e27059b519bf12d6d503d7f4020a8a1089870",
            "patch": "@@ -216,6 +216,7 @@ class LlavaNextForConditionalGenerationModelTest(ModelTesterMixin, GenerationTes\n \n     all_model_classes = (LlavaNextForConditionalGeneration,) if is_torch_available() else ()\n     all_generative_model_classes = (LlavaNextForConditionalGeneration,) if is_torch_available() else ()\n+    pipeline_model_mapping = {\"image-text-to-text\": LlavaNextForConditionalGeneration} if is_torch_available() else {}\n     test_pruning = False\n     test_head_masking = False\n     _is_composite = True"
        },
        {
            "sha": "7a5781fa039b5b22eb33f132cdc4b0df875cadae",
            "filename": "tests/models/llava_onevision/test_modeling_llava_onevision.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/203e27059b519bf12d6d503d7f4020a8a1089870/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/203e27059b519bf12d6d503d7f4020a8a1089870/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py?ref=203e27059b519bf12d6d503d7f4020a8a1089870",
            "patch": "@@ -217,6 +217,9 @@ class LlavaOnevisionForConditionalGenerationModelTest(ModelTesterMixin, Generati\n \n     all_model_classes = (LlavaOnevisionForConditionalGeneration,) if is_torch_available() else ()\n     all_generative_model_classes = (LlavaOnevisionForConditionalGeneration,) if is_torch_available() else ()\n+    pipeline_model_mapping = (\n+        {\"image-text-to-text\": LlavaOnevisionForConditionalGeneration} if is_torch_available() else {}\n+    )\n     test_pruning = False\n     test_head_masking = False\n     _is_composite = True"
        },
        {
            "sha": "91f2169a02f42df2dcf783fbb6745b587ee299b8",
            "filename": "tests/models/mllama/test_modeling_mllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/203e27059b519bf12d6d503d7f4020a8a1089870/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/203e27059b519bf12d6d503d7f4020a8a1089870/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py?ref=203e27059b519bf12d6d503d7f4020a8a1089870",
            "patch": "@@ -264,6 +264,7 @@ class MllamaForConditionalGenerationModelTest(ModelTesterMixin, GenerationTester\n \n     all_model_classes = (MllamaForConditionalGeneration,) if is_torch_available() else ()\n     all_generative_model_classes = (MllamaForConditionalGeneration,) if is_torch_available() else ()\n+    pipeline_model_mapping = {\"image-text-to-text\": MllamaForConditionalGeneration} if is_torch_available() else ()\n     test_pruning = False\n     test_head_masking = False\n     test_torchscript = False"
        },
        {
            "sha": "074e0083fd02022383ba886a3d44910c996f3d51",
            "filename": "tests/models/paligemma/test_modeling_paligemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/203e27059b519bf12d6d503d7f4020a8a1089870/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/203e27059b519bf12d6d503d7f4020a8a1089870/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py?ref=203e27059b519bf12d6d503d7f4020a8a1089870",
            "patch": "@@ -183,6 +183,7 @@ class PaliGemmaForConditionalGenerationModelTest(ModelTesterMixin, GenerationTes\n \n     all_model_classes = (PaliGemmaForConditionalGeneration,) if is_torch_available() else ()\n     all_generative_model_classes = (PaliGemmaForConditionalGeneration,) if is_torch_available() else ()\n+    pipeline_model_mapping = {\"image-text-to-text\": PaliGemmaForConditionalGeneration}\n     fx_compatible = False\n     test_pruning = False\n     test_torchscript = False"
        },
        {
            "sha": "7438dc6d66617924b300f911bd9747f968a92c17",
            "filename": "tests/models/pix2struct/test_modeling_pix2struct.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/203e27059b519bf12d6d503d7f4020a8a1089870/tests%2Fmodels%2Fpix2struct%2Ftest_modeling_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/203e27059b519bf12d6d503d7f4020a8a1089870/tests%2Fmodels%2Fpix2struct%2Ftest_modeling_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpix2struct%2Ftest_modeling_pix2struct.py?ref=203e27059b519bf12d6d503d7f4020a8a1089870",
            "patch": "@@ -420,7 +420,11 @@ def prepare_config_and_inputs_for_common(self):\n class Pix2StructModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (Pix2StructForConditionalGeneration,) if is_torch_available() else ()\n     all_generative_model_classes = (Pix2StructForConditionalGeneration,) if is_torch_available() else {}\n-    pipeline_model_mapping = {\"image-to-text\": Pix2StructForConditionalGeneration} if is_torch_available() else {}\n+    pipeline_model_mapping = (\n+        {\"image-to-text\": Pix2StructForConditionalGeneration, \"image-text-to-text\": Pix2StructForConditionalGeneration}\n+        if is_torch_available()\n+        else {}\n+    )\n     fx_compatible = False\n     test_head_masking = False\n     test_pruning = False"
        },
        {
            "sha": "6c04ba40df19d6fccb4681ef11a6fdc923226d6b",
            "filename": "tests/models/qwen2_vl/test_modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/203e27059b519bf12d6d503d7f4020a8a1089870/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/203e27059b519bf12d6d503d7f4020a8a1089870/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py?ref=203e27059b519bf12d6d503d7f4020a8a1089870",
            "patch": "@@ -224,6 +224,7 @@ class Qwen2VLModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCas\n \n     all_model_classes = (Qwen2VLForConditionalGeneration,) if is_torch_available() else ()\n     all_generative_model_classes = (Qwen2VLForConditionalGeneration,) if is_torch_available() else ()\n+    pipeline_model_mapping = {\"image-text-to-text\": Qwen2VLForConditionalGeneration}\n     test_pruning = False\n     test_head_masking = False\n "
        },
        {
            "sha": "d55400799dbd30258b44f671cb29536cfa31a7c6",
            "filename": "tests/models/udop/test_modeling_udop.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/203e27059b519bf12d6d503d7f4020a8a1089870/tests%2Fmodels%2Fudop%2Ftest_modeling_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/203e27059b519bf12d6d503d7f4020a8a1089870/tests%2Fmodels%2Fudop%2Ftest_modeling_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fudop%2Ftest_modeling_udop.py?ref=203e27059b519bf12d6d503d7f4020a8a1089870",
            "patch": "@@ -275,7 +275,11 @@ class UdopModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n         else ()\n     )\n     all_generative_model_classes = (UdopForConditionalGeneration,) if is_torch_available() else ()\n-    pipeline_model_mapping = {\"feature-extraction\": UdopModel} if is_torch_available() else {}\n+    pipeline_model_mapping = (\n+        {\"feature-extraction\": UdopModel, \"image-text-to-text\": UdopForConditionalGeneration}\n+        if is_torch_available()\n+        else {}\n+    )\n     fx_compatible = False\n     test_pruning = False\n     test_torchscript = False"
        },
        {
            "sha": "e2f9ae1ccfdea74d337fa9df489d1a65b5536309",
            "filename": "tests/models/vipllava/test_modeling_vipllava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/203e27059b519bf12d6d503d7f4020a8a1089870/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/203e27059b519bf12d6d503d7f4020a8a1089870/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py?ref=203e27059b519bf12d6d503d7f4020a8a1089870",
            "patch": "@@ -170,6 +170,7 @@ class VipLlavaForConditionalGenerationModelTest(ModelTesterMixin, GenerationTest\n \n     all_model_classes = (VipLlavaForConditionalGeneration,) if is_torch_available() else ()\n     all_generative_model_classes = (VipLlavaForConditionalGeneration,) if is_torch_available() else ()\n+    pipeline_model_mapping = {\"image-text-to-text\": VipLlavaForConditionalGeneration} if is_torch_available() else {}\n     fx_compatible = False\n     test_pruning = False\n     test_resize_embeddings = True"
        },
        {
            "sha": "b44b9decf98bbdaca76ad6809fcb26130110c0e1",
            "filename": "tests/pipelines/test_pipelines_image_text_to_text.py",
            "status": "added",
            "additions": 260,
            "deletions": 0,
            "changes": 260,
            "blob_url": "https://github.com/huggingface/transformers/blob/203e27059b519bf12d6d503d7f4020a8a1089870/tests%2Fpipelines%2Ftest_pipelines_image_text_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/203e27059b519bf12d6d503d7f4020a8a1089870/tests%2Fpipelines%2Ftest_pipelines_image_text_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_image_text_to_text.py?ref=203e27059b519bf12d6d503d7f4020a8a1089870",
            "patch": "@@ -0,0 +1,260 @@\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import unittest\n+\n+from transformers import MODEL_FOR_IMAGE_TEXT_TO_TEXT_MAPPING, is_vision_available\n+from transformers.pipelines import ImageTextToTextPipeline, pipeline\n+from transformers.testing_utils import (\n+    is_pipeline_test,\n+    require_torch,\n+    require_vision,\n+    slow,\n+)\n+\n+from .test_pipelines_common import ANY\n+\n+\n+if is_vision_available():\n+    from PIL import Image\n+else:\n+\n+    class Image:\n+        @staticmethod\n+        def open(*args, **kwargs):\n+            pass\n+\n+\n+@is_pipeline_test\n+@require_vision\n+class ImageTextToTextPipelineTests(unittest.TestCase):\n+    model_mapping = MODEL_FOR_IMAGE_TEXT_TO_TEXT_MAPPING\n+\n+    def get_test_pipeline(self, model, tokenizer, processor, image_processor, torch_dtype=\"float32\"):\n+        pipe = ImageTextToTextPipeline(model=model, processor=processor, torch_dtype=torch_dtype)\n+        image_token = getattr(processor.tokenizer, \"image_token\", \"\")\n+        examples = [\n+            {\n+                \"images\": Image.open(\"./tests/fixtures/tests_samples/COCO/000000039769.png\"),\n+                \"text\": f\"{image_token}This is a \",\n+            },\n+            {\n+                \"images\": \"./tests/fixtures/tests_samples/COCO/000000039769.png\",\n+                \"text\": f\"{image_token}Here I see a \",\n+            },\n+        ]\n+        return pipe, examples\n+\n+    def run_pipeline_test(self, pipe, examples):\n+        outputs = pipe(examples[0].get(\"images\"), text=examples[0].get(\"text\"))\n+        self.assertEqual(\n+            outputs,\n+            [\n+                {\"input_text\": ANY(str), \"generated_text\": ANY(str)},\n+            ],\n+        )\n+\n+    @require_torch\n+    def test_small_model_pt_token(self):\n+        pipe = pipeline(\"image-text-to-text\", model=\"llava-hf/llava-interleave-qwen-0.5b-hf\")\n+        image = \"./tests/fixtures/tests_samples/COCO/000000039769.png\"\n+        text = \"<image> What this is? Assistant: This is\"\n+\n+        outputs = pipe(image, text=text)\n+        self.assertEqual(\n+            outputs,\n+            [\n+                {\n+                    \"input_text\": \"<image> What this is? Assistant: This is\",\n+                    \"generated_text\": \"<image> What this is? Assistant: This is a photo of two cats lying on a pink blanket. The cats are sleeping and appear to be comfortable\",\n+                }\n+            ],\n+        )\n+\n+        outputs = pipe([image, image], text=[text, text])\n+        self.assertEqual(\n+            outputs,\n+            [\n+                {\n+                    \"input_text\": \"<image> What this is? Assistant: This is\",\n+                    \"generated_text\": \"<image> What this is? Assistant: This is a photo of two cats lying on a pink blanket. The cats are sleeping and appear to be comfortable\",\n+                },\n+                {\n+                    \"input_text\": \"<image> What this is? Assistant: This is\",\n+                    \"generated_text\": \"<image> What this is? Assistant: This is a photo of two cats lying on a pink blanket. The cats are sleeping and appear to be comfortable\",\n+                },\n+            ],\n+        )\n+\n+    @require_torch\n+    def test_consistent_batching_behaviour(self):\n+        pipe = pipeline(\"image-text-to-text\", model=\"microsoft/kosmos-2-patch14-224\")\n+        image = \"./tests/fixtures/tests_samples/COCO/000000039769.png\"\n+        prompt = \"a photo of\"\n+\n+        outputs = pipe([image, image], text=[prompt, prompt])\n+        outputs_batched = pipe([image, image], text=[prompt, prompt], batch_size=2)\n+        self.assertEqual(outputs, outputs_batched)\n+\n+    @slow\n+    @require_torch\n+    def test_model_pt_chat_template(self):\n+        pipe = pipeline(\"image-text-to-text\", model=\"llava-hf/llava-interleave-qwen-0.5b-hf\")\n+        image_ny = \"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\"\n+        image_chicago = \"https://cdn.britannica.com/59/94459-050-DBA42467/Skyline-Chicago.jpg\"\n+        messages = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\"type\": \"text\", \"text\": \"Whats the difference between these two images?\"},\n+                    {\"type\": \"image\"},\n+                    {\"type\": \"image\"},\n+                ],\n+            }\n+        ]\n+        outputs = pipe([image_ny, image_chicago], text=messages)\n+        self.assertEqual(\n+            outputs,\n+            [\n+                {\n+                    \"input_text\": [\n+                        {\n+                            \"role\": \"user\",\n+                            \"content\": [\n+                                {\"type\": \"text\", \"text\": \"Whats the difference between these two images?\"},\n+                                {\"type\": \"image\"},\n+                                {\"type\": \"image\"},\n+                            ],\n+                        }\n+                    ],\n+                    \"generated_text\": [\n+                        {\n+                            \"role\": \"user\",\n+                            \"content\": [\n+                                {\"type\": \"text\", \"text\": \"Whats the difference between these two images?\"},\n+                                {\"type\": \"image\"},\n+                                {\"type\": \"image\"},\n+                            ],\n+                        },\n+                        {\n+                            \"role\": \"assistant\",\n+                            \"content\": \"The first image shows a statue of the Statue of Liberty in the foreground, while the second image shows\",\n+                        },\n+                    ],\n+                }\n+            ],\n+        )\n+\n+    @slow\n+    @require_torch\n+    def test_model_pt_chat_template_continue_final_message(self):\n+        pipe = pipeline(\"image-text-to-text\", model=\"llava-hf/llava-interleave-qwen-0.5b-hf\")\n+        messages = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\n+                        \"type\": \"image\",\n+                        \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n+                    },\n+                    {\"type\": \"text\", \"text\": \"Describe this image.\"},\n+                ],\n+            },\n+            {\n+                \"role\": \"assistant\",\n+                \"content\": [\n+                    {\"type\": \"text\", \"text\": \"There is a dog and\"},\n+                ],\n+            },\n+        ]\n+        outputs = pipe(text=messages)\n+        self.assertEqual(\n+            outputs,\n+            [\n+                {\n+                    \"input_text\": [\n+                        {\n+                            \"role\": \"user\",\n+                            \"content\": [\n+                                {\n+                                    \"type\": \"image\",\n+                                    \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n+                                },\n+                                {\"type\": \"text\", \"text\": \"Describe this image.\"},\n+                            ],\n+                        },\n+                        {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": \"There is a dog and\"}]},\n+                    ],\n+                    \"generated_text\": [\n+                        {\n+                            \"role\": \"user\",\n+                            \"content\": [\n+                                {\n+                                    \"type\": \"image\",\n+                                    \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n+                                },\n+                                {\"type\": \"text\", \"text\": \"Describe this image.\"},\n+                            ],\n+                        },\n+                        {\n+                            \"role\": \"assistant\",\n+                            \"content\": [\n+                                {\n+                                    \"type\": \"text\",\n+                                    \"text\": \"There is a dog and a person in the image. The dog is sitting on the sand, and the person is sitting on\",\n+                                }\n+                            ],\n+                        },\n+                    ],\n+                }\n+            ],\n+        )\n+\n+    @slow\n+    @require_torch\n+    def test_model_pt_chat_template_new_text(self):\n+        pipe = pipeline(\"image-text-to-text\", model=\"llava-hf/llava-interleave-qwen-0.5b-hf\")\n+        messages = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\n+                        \"type\": \"image\",\n+                        \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n+                    },\n+                    {\"type\": \"text\", \"text\": \"Describe this image.\"},\n+                ],\n+            }\n+        ]\n+        outputs = pipe(text=messages, return_full_text=False)\n+        self.assertEqual(\n+            outputs,\n+            [\n+                {\n+                    \"input_text\": [\n+                        {\n+                            \"role\": \"user\",\n+                            \"content\": [\n+                                {\n+                                    \"type\": \"image\",\n+                                    \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n+                                },\n+                                {\"type\": \"text\", \"text\": \"Describe this image.\"},\n+                            ],\n+                        }\n+                    ],\n+                    \"generated_text\": \"In the image, a woman is sitting on the sandy beach, her legs crossed in a relaxed manner\",\n+                }\n+            ],\n+        )"
        },
        {
            "sha": "5ed48de3610eb93eb7dfe10802669477a24e5622",
            "filename": "tests/pipelines/test_pipelines_zero_shot_object_detection.py",
            "status": "modified",
            "additions": 12,
            "deletions": 5,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/203e27059b519bf12d6d503d7f4020a8a1089870/tests%2Fpipelines%2Ftest_pipelines_zero_shot_object_detection.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/203e27059b519bf12d6d503d7f4020a8a1089870/tests%2Fpipelines%2Ftest_pipelines_zero_shot_object_detection.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_zero_shot_object_detection.py?ref=203e27059b519bf12d6d503d7f4020a8a1089870",
            "patch": "@@ -14,7 +14,12 @@\n \n import unittest\n \n-from transformers import MODEL_FOR_ZERO_SHOT_OBJECT_DETECTION_MAPPING, is_vision_available, pipeline\n+from transformers import (\n+    MODEL_FOR_ZERO_SHOT_OBJECT_DETECTION_MAPPING,\n+    ZeroShotObjectDetectionPipeline,\n+    is_vision_available,\n+    pipeline,\n+)\n from transformers.testing_utils import (\n     is_pipeline_test,\n     nested_simplify,\n@@ -52,9 +57,11 @@ def get_test_pipeline(\n         processor=None,\n         torch_dtype=\"float32\",\n     ):\n-        object_detector = pipeline(\n-            \"zero-shot-object-detection\",\n-            model=\"hf-internal-testing/tiny-random-owlvit-object-detection\",\n+        object_detector = ZeroShotObjectDetectionPipeline(\n+            model=model,\n+            processor=processor,\n+            tokenizer=tokenizer,\n+            image_processor=image_processor,\n             torch_dtype=torch_dtype,\n         )\n \n@@ -67,7 +74,7 @@ def get_test_pipeline(\n         return object_detector, examples\n \n     def run_pipeline_test(self, object_detector, examples):\n-        outputs = object_detector(examples[0], threshold=0.0)\n+        outputs = object_detector(examples[0].get(\"image\"), examples[0].get(\"candidate_labels\"), threshold=0.0)\n \n         n = len(outputs)\n         self.assertGreater(n, 0)"
        },
        {
            "sha": "94bc3d5fae1ad2e13a292beed6e4930b0a6c9d05",
            "filename": "tests/test_pipeline_mixin.py",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/203e27059b519bf12d6d503d7f4020a8a1089870/tests%2Ftest_pipeline_mixin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/203e27059b519bf12d6d503d7f4020a8a1089870/tests%2Ftest_pipeline_mixin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_pipeline_mixin.py?ref=203e27059b519bf12d6d503d7f4020a8a1089870",
            "patch": "@@ -71,6 +71,7 @@\n from .pipelines.test_pipelines_image_classification import ImageClassificationPipelineTests\n from .pipelines.test_pipelines_image_feature_extraction import ImageFeatureExtractionPipelineTests\n from .pipelines.test_pipelines_image_segmentation import ImageSegmentationPipelineTests\n+from .pipelines.test_pipelines_image_text_to_text import ImageTextToTextPipelineTests\n from .pipelines.test_pipelines_image_to_image import ImageToImagePipelineTests\n from .pipelines.test_pipelines_image_to_text import ImageToTextPipelineTests\n from .pipelines.test_pipelines_mask_generation import MaskGenerationPipelineTests\n@@ -102,6 +103,7 @@\n     \"image-classification\": {\"test\": ImageClassificationPipelineTests},\n     \"image-feature-extraction\": {\"test\": ImageFeatureExtractionPipelineTests},\n     \"image-segmentation\": {\"test\": ImageSegmentationPipelineTests},\n+    \"image-text-to-text\": {\"test\": ImageTextToTextPipelineTests},\n     \"image-to-image\": {\"test\": ImageToImagePipelineTests},\n     \"image-to-text\": {\"test\": ImageToTextPipelineTests},\n     \"mask-generation\": {\"test\": MaskGenerationPipelineTests},\n@@ -586,6 +588,18 @@ def test_pipeline_image_segmentation(self):\n     def test_pipeline_image_segmentation_fp16(self):\n         self.run_task_tests(task=\"image-segmentation\", torch_dtype=\"float16\")\n \n+    @is_pipeline_test\n+    @require_vision\n+    @require_torch\n+    def test_pipeline_image_text_to_text(self):\n+        self.run_task_tests(task=\"image-text-to-text\")\n+\n+    @is_pipeline_test\n+    @require_vision\n+    @require_torch\n+    def test_pipeline_image_text_to_text_fp16(self):\n+        self.run_task_tests(task=\"image-text-to-text\", torch_dtype=\"float16\")\n+\n     @is_pipeline_test\n     @require_vision\n     def test_pipeline_image_to_text(self):"
        },
        {
            "sha": "f27f720ec3d593c61211b704ca0ac2cf89721ba5",
            "filename": "tests/utils/tiny_model_summary.json",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/203e27059b519bf12d6d503d7f4020a8a1089870/tests%2Futils%2Ftiny_model_summary.json",
            "raw_url": "https://github.com/huggingface/transformers/raw/203e27059b519bf12d6d503d7f4020a8a1089870/tests%2Futils%2Ftiny_model_summary.json",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftiny_model_summary.json?ref=203e27059b519bf12d6d503d7f4020a8a1089870",
            "patch": "@@ -2896,7 +2896,7 @@\n         \"model_classes\": [\n             \"IdeficsForVisionText2Text\"\n         ],\n-        \"sha\": \"2c2f2e2cd6b02a77d0cdd8c3767ba9a6267dbd20\"\n+        \"sha\": \"a6be81294ff7a3d44f3aef0ed18e42b97c426831\"\n     },\n     \"IdeficsModel\": {\n         \"tokenizer_classes\": ["
        },
        {
            "sha": "0be960f4a33e6dcaa198694850f6b33f82756c85",
            "filename": "utils/check_docstrings.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/203e27059b519bf12d6d503d7f4020a8a1089870/utils%2Fcheck_docstrings.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/203e27059b519bf12d6d503d7f4020a8a1089870/utils%2Fcheck_docstrings.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_docstrings.py?ref=203e27059b519bf12d6d503d7f4020a8a1089870",
            "patch": "@@ -335,6 +335,7 @@\n     \"ImageFeatureExtractionPipeline\",\n     \"ImageGPTConfig\",\n     \"ImageSegmentationPipeline\",\n+    \"ImageTextToTextPipeline\",\n     \"ImageToImagePipeline\",\n     \"ImageToTextPipeline\",\n     \"InformerConfig\","
        },
        {
            "sha": "b6ee1e7c8c13c230df2fd968d2da6158c8e8d722",
            "filename": "utils/update_metadata.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/203e27059b519bf12d6d503d7f4020a8a1089870/utils%2Fupdate_metadata.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/203e27059b519bf12d6d503d7f4020a8a1089870/utils%2Fupdate_metadata.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fupdate_metadata.py?ref=203e27059b519bf12d6d503d7f4020a8a1089870",
            "patch": "@@ -69,6 +69,7 @@\n     (\"automatic-speech-recognition\", \"MODEL_FOR_CTC_MAPPING_NAMES\", \"AutoModelForCTC\"),\n     (\"image-classification\", \"MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING_NAMES\", \"AutoModelForImageClassification\"),\n     (\"image-segmentation\", \"MODEL_FOR_IMAGE_SEGMENTATION_MAPPING_NAMES\", \"AutoModelForImageSegmentation\"),\n+    (\"image-text-to-text\", \"MODEL_FOR_IMAGE_TEXT_TO_TEXT_MAPPING_NAMES\", \"AutoModelForImageTextToText\"),\n     (\"image-to-image\", \"MODEL_FOR_IMAGE_TO_IMAGE_MAPPING_NAMES\", \"AutoModelForImageToImage\"),\n     (\"fill-mask\", \"MODEL_FOR_MASKED_LM_MAPPING_NAMES\", \"AutoModelForMaskedLM\"),\n     (\"object-detection\", \"MODEL_FOR_OBJECT_DETECTION_MAPPING_NAMES\", \"AutoModelForObjectDetection\"),"
        }
    ],
    "stats": {
        "total": 1021,
        "additions": 988,
        "deletions": 33
    }
}