{
    "author": "bozheng-hit",
    "message": "Fix the misalignment between the l2norm in GDN of Qwen3-Next and the implementation in the FLA library. (#40842)\n\n* align torch implementation of gdn with fla.\n\n* fix fla import.\n\n* fix\n\n* remove unused attr\n\n* fixes\n\n* strictly align l2norm in Qwen3-Next with FLA implementation.\n\n---------\n\nCo-authored-by: bozheng-hit <dsoul0621@gmail.com>\nCo-authored-by: Cyril Vallez <cyril.vallez@gmail.com>",
    "sha": "98a8078127b4a41d1c2d78487abf3bf3831e914b",
    "files": [
        {
            "sha": "ae2d3664e10ec9611e6e4681277c71da56d8c7b9",
            "filename": "src/transformers/models/qwen3_next/modeling_qwen3_next.py",
            "status": "modified",
            "additions": 10,
            "deletions": 8,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/98a8078127b4a41d1c2d78487abf3bf3831e914b/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodeling_qwen3_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/98a8078127b4a41d1c2d78487abf3bf3831e914b/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodeling_qwen3_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodeling_qwen3_next.py?ref=98a8078127b4a41d1c2d78487abf3bf3831e914b",
            "patch": "@@ -433,6 +433,12 @@ def torch_causal_conv1d_update(\n     return out\n \n \n+def l2norm(x: torch.FloatTensor, dim: int = -1, eps: float = 1e-6):\n+    \"\"\"This function is intended to align with the l2norm implementation in the FLA library.\"\"\"\n+    inv_norm = 1 / torch.sqrt((x * x).sum(dim=dim, keepdim=True) + eps)\n+    return x * inv_norm\n+\n+\n def torch_chunk_gated_delta_rule(\n     query,\n     key,\n@@ -446,10 +452,8 @@ def torch_chunk_gated_delta_rule(\n ):\n     initial_dtype = query.dtype\n     if use_qk_l2norm_in_kernel:\n-        head_dim = query.size(-1)\n-        inv_scale = head_dim**-0.5\n-        query = F.rms_norm(query, (head_dim,), eps=1e-6) * inv_scale\n-        key = F.rms_norm(key, (head_dim,), eps=1e-6) * inv_scale\n+        query = l2norm(query, dim=-1, eps=1e-6)\n+        key = l2norm(key, dim=-1, eps=1e-6)\n     query, key, value, beta, g = [\n         x.transpose(1, 2).contiguous().to(torch.float32) for x in (query, key, value, beta, g)\n     ]\n@@ -520,10 +524,8 @@ def torch_recurrent_gated_delta_rule(\n ):\n     initial_dtype = query.dtype\n     if use_qk_l2norm_in_kernel:\n-        head_dim = query.size(-1)\n-        inv_scale = head_dim**-0.5\n-        query = F.rms_norm(query, (head_dim,), eps=1e-6) * inv_scale\n-        key = F.rms_norm(key, (head_dim,), eps=1e-6) * inv_scale\n+        query = l2norm(query, dim=-1, eps=1e-6)\n+        key = l2norm(key, dim=-1, eps=1e-6)\n     query, key, value, beta, g = [\n         x.transpose(1, 2).contiguous().to(torch.float32) for x in (query, key, value, beta, g)\n     ]"
        },
        {
            "sha": "1854a391cfb649ade9b02bf2661571f7f59ff2b1",
            "filename": "src/transformers/models/qwen3_next/modular_qwen3_next.py",
            "status": "modified",
            "additions": 10,
            "deletions": 8,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/98a8078127b4a41d1c2d78487abf3bf3831e914b/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodular_qwen3_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/98a8078127b4a41d1c2d78487abf3bf3831e914b/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodular_qwen3_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodular_qwen3_next.py?ref=98a8078127b4a41d1c2d78487abf3bf3831e914b",
            "patch": "@@ -269,6 +269,12 @@ def torch_causal_conv1d_update(\n     return out\n \n \n+def l2norm(x: torch.FloatTensor, dim: int = -1, eps: float = 1e-6):\n+    \"\"\"This function is intended to align with the l2norm implementation in the FLA library.\"\"\"\n+    inv_norm = 1 / torch.sqrt((x * x).sum(dim=dim, keepdim=True) + eps)\n+    return x * inv_norm\n+\n+\n def torch_chunk_gated_delta_rule(\n     query,\n     key,\n@@ -282,10 +288,8 @@ def torch_chunk_gated_delta_rule(\n ):\n     initial_dtype = query.dtype\n     if use_qk_l2norm_in_kernel:\n-        head_dim = query.size(-1)\n-        inv_scale = head_dim**-0.5\n-        query = F.rms_norm(query, (head_dim,), eps=1e-6) * inv_scale\n-        key = F.rms_norm(key, (head_dim,), eps=1e-6) * inv_scale\n+        query = l2norm(query, dim=-1, eps=1e-6)\n+        key = l2norm(key, dim=-1, eps=1e-6)\n     query, key, value, beta, g = [\n         x.transpose(1, 2).contiguous().to(torch.float32) for x in (query, key, value, beta, g)\n     ]\n@@ -356,10 +360,8 @@ def torch_recurrent_gated_delta_rule(\n ):\n     initial_dtype = query.dtype\n     if use_qk_l2norm_in_kernel:\n-        head_dim = query.size(-1)\n-        inv_scale = head_dim**-0.5\n-        query = F.rms_norm(query, (head_dim,), eps=1e-6) * inv_scale\n-        key = F.rms_norm(key, (head_dim,), eps=1e-6) * inv_scale\n+        query = l2norm(query, dim=-1, eps=1e-6)\n+        key = l2norm(key, dim=-1, eps=1e-6)\n     query, key, value, beta, g = [\n         x.transpose(1, 2).contiguous().to(torch.float32) for x in (query, key, value, beta, g)\n     ]"
        }
    ],
    "stats": {
        "total": 36,
        "additions": 20,
        "deletions": 16
    }
}