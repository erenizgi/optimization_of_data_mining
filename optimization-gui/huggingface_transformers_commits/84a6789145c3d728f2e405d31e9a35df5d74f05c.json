{
    "author": "zucchini-nlp",
    "message": "Enable different torch dtype in sub models (#34873)\n\n* fix\r\n\r\n* fix test\r\n\r\n* add tests\r\n\r\n* add more tests\r\n\r\n* fix tests\r\n\r\n* supposed to be a torch.dtype test\r\n\r\n* handle BC and make fp32 default",
    "sha": "84a6789145c3d728f2e405d31e9a35df5d74f05c",
    "files": [
        {
            "sha": "dfb64fcd08698c8e55e1fefdc3e413538d10ec0d",
            "filename": "src/transformers/configuration_utils.py",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/84a6789145c3d728f2e405d31e9a35df5d74f05c/src%2Ftransformers%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84a6789145c3d728f2e405d31e9a35df5d74f05c/src%2Ftransformers%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconfiguration_utils.py?ref=84a6789145c3d728f2e405d31e9a35df5d74f05c",
            "patch": "@@ -994,8 +994,11 @@ def dict_torch_dtype_to_str(self, d: Dict[str, Any]) -> None:\n         converts torch.dtype to a string of just the type. For example, `torch.float32` get converted into *\"float32\"*\n         string, which can then be stored in the json format.\n         \"\"\"\n-        if d.get(\"torch_dtype\", None) is not None and not isinstance(d[\"torch_dtype\"], str):\n-            d[\"torch_dtype\"] = str(d[\"torch_dtype\"]).split(\".\")[1]\n+        if d.get(\"torch_dtype\", None) is not None:\n+            if isinstance(d[\"torch_dtype\"], dict):\n+                d[\"torch_dtype\"] = {k: str(v).split(\".\")[-1] for k, v in d[\"torch_dtype\"].items()}\n+            elif not isinstance(d[\"torch_dtype\"], str):\n+                d[\"torch_dtype\"] = str(d[\"torch_dtype\"]).split(\".\")[1]\n         for value in d.values():\n             if isinstance(value, dict):\n                 self.dict_torch_dtype_to_str(value)"
        },
        {
            "sha": "c09c11050041d8964bd3ea1b30601482889302c9",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 37,
            "deletions": 9,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/84a6789145c3d728f2e405d31e9a35df5d74f05c/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84a6789145c3d728f2e405d31e9a35df5d74f05c/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=84a6789145c3d728f2e405d31e9a35df5d74f05c",
            "patch": "@@ -1312,11 +1312,10 @@ def __init__(self, config: PretrainedConfig, *inputs, **kwargs):\n                 \"`PretrainedConfig`. To create a model from a pretrained model use \"\n                 f\"`model = {self.__class__.__name__}.from_pretrained(PRETRAINED_MODEL_NAME)`\"\n             )\n-        # Save config and origin of the pretrained weights if given in model\n         if not getattr(config, \"_attn_implementation_autoset\", False):\n-            config = self._autoset_attn_implementation(\n-                config, torch_dtype=torch.get_default_dtype(), check_device_map=False\n-            )\n+            # config usually has a `torch_dtype` but we need the next line for the `no_super_init` tests\n+            dtype = config.torch_dtype if hasattr(config, \"torch_dtype\") else torch.get_default_dtype()\n+            config = self._autoset_attn_implementation(config, torch_dtype=dtype, check_device_map=False)\n         self.config = config\n \n         # for initialization of the loss\n@@ -1411,7 +1410,10 @@ def _from_config(cls, config, **kwargs):\n         # when we init a model from within another model (e.g. VLMs) and dispatch on FA2\n         # a warning is raised that dtype should be fp16. Since we never pass dtype from within\n         # modeling code, we can try to infer it here same way as done in `from_pretrained`\n-        torch_dtype = kwargs.pop(\"torch_dtype\", torch.get_default_dtype())\n+        torch_dtype = kwargs.pop(\"torch_dtype\", config.torch_dtype)\n+        if isinstance(torch_dtype, str):\n+            torch_dtype = getattr(torch, torch_dtype)\n+\n         use_flash_attention_2 = kwargs.pop(\"use_flash_attention_2\", False)\n \n         # override default dtype if needed\n@@ -4020,11 +4022,37 @@ def from_pretrained(\n                             )\n                     elif hasattr(torch, torch_dtype):\n                         torch_dtype = getattr(torch, torch_dtype)\n-                    else:\n-                        raise ValueError(\n-                            f'`torch_dtype` can be one of: `torch.dtype`, `\"auto\"` or a string of a valid `torch.dtype`, but received {torch_dtype}'\n-                        )\n+                        for sub_config_key in config.sub_configs.keys():\n+                            sub_config = getattr(config, sub_config_key)\n+                            sub_config.torch_dtype = torch_dtype\n+                elif isinstance(torch_dtype, torch.dtype):\n+                    pass\n+                elif isinstance(torch_dtype, dict):\n+                    for key, curr_dtype in torch_dtype.items():\n+                        if hasattr(config, key):\n+                            value = getattr(config, key)\n+                            value.torch_dtype = curr_dtype\n+                    # main torch dtype for modules that aren't part of any sub-config\n+                    torch_dtype = torch_dtype.get(\"\")\n+                    config.torch_dtype = torch_dtype\n+                    if isinstance(torch_dtype, str) and hasattr(torch, torch_dtype):\n+                        torch_dtype = getattr(torch, torch_dtype)\n+                    elif torch_dtype is None:\n+                        torch_dtype = torch.float32\n+                else:\n+                    raise ValueError(\n+                        f\"`torch_dtype` can be one of: `torch.dtype`, `'auto'`, a string of a valid `torch.dtype` or a `dict` with valid `torch_dtype` \"\n+                        f\"for each sub-config in composite configs, but received {torch_dtype}\"\n+                    )\n+\n                 dtype_orig = cls._set_default_torch_dtype(torch_dtype)\n+            else:\n+                # set fp32 as the default dtype for BC\n+                default_dtype = str(torch.get_default_dtype()).split(\".\")[-1]\n+                config.torch_dtype = default_dtype\n+                for key in config.sub_configs.keys():\n+                    value = getattr(config, key)\n+                    value.torch_dtype = default_dtype\n \n             # Check if `_keep_in_fp32_modules` is not None\n             use_keep_in_fp32_modules = (cls._keep_in_fp32_modules is not None) and ("
        },
        {
            "sha": "edbac91bb0609eba98c4a338a014a7573c88a1c3",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 57,
            "deletions": 57,
            "changes": 114,
            "blob_url": "https://github.com/huggingface/transformers/blob/84a6789145c3d728f2e405d31e9a35df5d74f05c/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84a6789145c3d728f2e405d31e9a35df5d74f05c/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=84a6789145c3d728f2e405d31e9a35df5d74f05c",
            "patch": "@@ -967,62 +967,6 @@ def forward(self, pixel_values: torch.LongTensor):\n         return last_hidden_state\n \n \n-CHAMELEON_VQ_START_DOCSTRING = r\"\"\"\n-    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n-    etc.)\n-\n-    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n-    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n-    and behavior.\n-\n-    Parameters:\n-        config ([`ChameleonVQVAEConfig`]):\n-            Model configuration class with all the parameters of the model. Initializing with a config file does not\n-            load the weights associated with the model, only the configuration. Check out the\n-            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"\"\"The VQ-VAE model used in Chameleon for encoding/decoding images into discrete tokens.\n-    This model follows the \"Make-a-scene: Scene-based text-to-image generation with human priors\" paper from\n-    [ Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman](https://arxiv.org/abs/2203.13131).\n-    \"\"\",\n-    CHAMELEON_VQ_START_DOCSTRING,\n-)\n-class ChameleonVQVAE(PreTrainedModel):\n-    config_class = ChameleonVQVAEConfig\n-    _no_split_modules = [\"ChameleonVQVAEVectorQuantizer\"]\n-\n-    def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-        elif isinstance(module, nn.GroupNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n-        elif isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-\n-    def __init__(self, config: ChameleonVQVAEConfig):\n-        super().__init__(config)\n-\n-        self.encoder = ChameleonVQVAEEncoder(config)\n-        self.quantize = ChameleonVQVAEVectorQuantizer(config)\n-        self.quant_conv = torch.nn.Conv2d(config.latent_channels, config.embed_dim, 1)\n-        self.post_quant_conv = torch.nn.Conv2d(config.embed_dim, config.latent_channels, 1)\n-        self.eval()  # Chameleon's VQ model is frozen\n-\n-    def encode(self, pixel_values: torch.LongTensor):\n-        hidden_states = self.encoder(pixel_values)\n-        hidden_states = self.quant_conv(hidden_states)\n-        quant, emb_loss, indices = self.quantize(hidden_states)\n-        return quant, emb_loss, indices\n-\n-\n class ChameleonImageVocabularyMapping:\n     \"\"\"\n     A class for mapping discrete image tokens from VQGAN to BPE tokens.\n@@ -1118,6 +1062,62 @@ def _init_weights(self, module):\n                 module.weight.data[module.padding_idx].zero_()\n \n \n+CHAMELEON_VQ_START_DOCSTRING = r\"\"\"\n+    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n+    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n+    etc.)\n+\n+    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n+    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n+    and behavior.\n+\n+    Parameters:\n+        config ([`ChameleonVQVAEConfig`]):\n+            Model configuration class with all the parameters of the model. Initializing with a config file does not\n+            load the weights associated with the model, only the configuration. Check out the\n+            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"\"\"The VQ-VAE model used in Chameleon for encoding/decoding images into discrete tokens.\n+    This model follows the \"Make-a-scene: Scene-based text-to-image generation with human priors\" paper from\n+    [ Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman](https://arxiv.org/abs/2203.13131).\n+    \"\"\",\n+    CHAMELEON_VQ_START_DOCSTRING,\n+)\n+class ChameleonVQVAE(ChameleonPreTrainedModel):\n+    config_class = ChameleonVQVAEConfig\n+    _no_split_modules = [\"ChameleonVQVAEVectorQuantizer\"]\n+\n+    def _init_weights(self, module):\n+        std = self.config.initializer_range\n+        if isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+        elif isinstance(module, nn.GroupNorm):\n+            module.bias.data.zero_()\n+            module.weight.data.fill_(1.0)\n+        elif isinstance(module, (nn.Linear, nn.Conv2d)):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+\n+    def __init__(self, config: ChameleonVQVAEConfig):\n+        super().__init__(config)\n+\n+        self.encoder = ChameleonVQVAEEncoder(config)\n+        self.quantize = ChameleonVQVAEVectorQuantizer(config)\n+        self.quant_conv = torch.nn.Conv2d(config.latent_channels, config.embed_dim, 1)\n+        self.post_quant_conv = torch.nn.Conv2d(config.embed_dim, config.latent_channels, 1)\n+        self.eval()  # Chameleon's VQ model is frozen\n+\n+    def encode(self, pixel_values: torch.LongTensor):\n+        hidden_states = self.encoder(pixel_values)\n+        hidden_states = self.quant_conv(hidden_states)\n+        quant, emb_loss, indices = self.quantize(hidden_states)\n+        return quant, emb_loss, indices\n+\n+\n CHAMELEON_INPUTS_DOCSTRING = r\"\"\"\n     Args:\n         input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n@@ -1211,7 +1211,7 @@ def __init__(self, config: ChameleonConfig):\n             [decoder_layer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n         )\n         self.norm = ChameleonRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n-        self.vqmodel = ChameleonVQVAE(config.vq_config)\n+        self.vqmodel = ChameleonVQVAE._from_config(config.vq_config)\n         self.gradient_checkpointing = False\n \n         # Initialize weights and apply final processing"
        },
        {
            "sha": "1b2891fe6dac9cf9f91990f44b847b53a6667514",
            "filename": "tests/models/qwen2_vl/test_modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/84a6789145c3d728f2e405d31e9a35df5d74f05c/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84a6789145c3d728f2e405d31e9a35df5d74f05c/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py?ref=84a6789145c3d728f2e405d31e9a35df5d74f05c",
            "patch": "@@ -227,6 +227,7 @@ class Qwen2VLModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCas\n     pipeline_model_mapping = {\"image-text-to-text\": Qwen2VLForConditionalGeneration}\n     test_pruning = False\n     test_head_masking = False\n+    _is_composite = True\n \n     def setUp(self):\n         self.model_tester = Qwen2VLVisionText2TextModelTester(self)"
        },
        {
            "sha": "b8e10ff8ad4d426c30a119e322427156e901449b",
            "filename": "tests/utils/test_modeling_utils.py",
            "status": "modified",
            "additions": 55,
            "deletions": 0,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/84a6789145c3d728f2e405d31e9a35df5d74f05c/tests%2Futils%2Ftest_modeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84a6789145c3d728f2e405d31e9a35df5d74f05c/tests%2Futils%2Ftest_modeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_utils.py?ref=84a6789145c3d728f2e405d31e9a35df5d74f05c",
            "patch": "@@ -37,6 +37,7 @@\n     AutoModel,\n     AutoModelForImageClassification,\n     AutoModelForSequenceClassification,\n+    LlavaForConditionalGeneration,\n     OwlViTForObjectDetection,\n     PretrainedConfig,\n     is_torch_available,\n@@ -300,6 +301,7 @@ def test_local_files_only(self):\n TINY_BERT_FOR_TOKEN_CLASSIFICATION = \"hf-internal-testing/tiny-bert-for-token-classification\"\n TINY_MISTRAL = \"hf-internal-testing/tiny-random-MistralForCausalLM\"\n TINY_IMAGE_CLASSIF = \"hf-internal-testing/tiny-random-SiglipForImageClassification\"\n+TINY_LLAVA = \"hf-internal-testing/tiny-random-LlavaForConditionalGeneration\"\n \n LOG = logging.get_logger(__name__)\n \n@@ -460,6 +462,59 @@ def test_model_from_config_torch_dtype_str(self):\n         with self.assertRaises(ValueError):\n             model = AutoModel.from_pretrained(TINY_T5, torch_dtype=\"int64\")\n \n+    def test_model_from_config_torch_dtype_composite(self):\n+        \"\"\"\n+        Test that from_pretrained works with torch_dtype being as a dict per each sub-config in composite config\n+        \"\"\"\n+        # should be able to set torch_dtype as a simple string and the model loads it correctly\n+        model = LlavaForConditionalGeneration.from_pretrained(TINY_LLAVA, torch_dtype=\"float32\")\n+        self.assertEqual(model.language_model.dtype, torch.float32)\n+        self.assertEqual(model.vision_tower.dtype, torch.float32)\n+\n+        model = LlavaForConditionalGeneration.from_pretrained(TINY_LLAVA, torch_dtype=\"float16\")\n+        self.assertEqual(model.language_model.dtype, torch.float16)\n+        self.assertEqual(model.vision_tower.dtype, torch.float16)\n+\n+        # should be able to set torch_dtype as a dict for each sub-config\n+        model = LlavaForConditionalGeneration.from_pretrained(\n+            TINY_LLAVA, torch_dtype={\"text_config\": \"float32\", \"vision_config\": \"float16\", \"\": \"bfloat16\"}\n+        )\n+        self.assertEqual(model.language_model.dtype, torch.float32)\n+        self.assertEqual(model.vision_tower.dtype, torch.float16)\n+        self.assertEqual(model.multi_modal_projector.linear_1.weight.dtype, torch.bfloat16)\n+\n+        # should be able to set the values as torch.dtype (not str)\n+        model = LlavaForConditionalGeneration.from_pretrained(\n+            TINY_LLAVA, torch_dtype={\"text_config\": torch.float32, \"vision_config\": torch.float16, \"\": torch.bfloat16}\n+        )\n+        self.assertEqual(model.language_model.dtype, torch.float32)\n+        self.assertEqual(model.vision_tower.dtype, torch.float16)\n+        self.assertEqual(model.multi_modal_projector.linear_1.weight.dtype, torch.bfloat16)\n+\n+        # should be able to set the values in configs directly and pass it to `from_pretrained`\n+        config = copy.deepcopy(model.config)\n+        config.text_config.torch_dtype = torch.float32\n+        config.vision_config.torch_dtype = torch.bfloat16\n+        config.torch_dtype = torch.float16\n+        model = LlavaForConditionalGeneration.from_pretrained(TINY_LLAVA, config=config, torch_dtype=\"auto\")\n+        self.assertEqual(model.language_model.dtype, torch.float32)\n+        self.assertEqual(model.vision_tower.dtype, torch.bfloat16)\n+        self.assertEqual(model.multi_modal_projector.linear_1.weight.dtype, torch.float16)\n+\n+        # but if the model has `_keep_in_fp32_modules` then those modules should be in fp32 no matter what\n+        LlavaForConditionalGeneration._keep_in_fp32_modules = [\"multi_modal_projector\"]\n+        model = LlavaForConditionalGeneration.from_pretrained(TINY_LLAVA, config=config, torch_dtype=\"auto\")\n+        self.assertEqual(model.language_model.dtype, torch.float32)\n+        self.assertEqual(model.vision_tower.dtype, torch.bfloat16)\n+        self.assertEqual(model.multi_modal_projector.linear_1.weight.dtype, torch.float32)\n+\n+        # torch.set_default_dtype() supports only float dtypes, so will fail with non-float type\n+        with self.assertRaises(ValueError):\n+            model = LlavaForConditionalGeneration.from_pretrained(TINY_LLAVA, torch_dtype=\"int64\")\n+            model = LlavaForConditionalGeneration.from_pretrained(\n+                TINY_LLAVA, torch_dtype={\"text_config\": \"float32\", \"vision_config\": \"int64\", \"\": \"float16\"}\n+            )\n+\n     @require_torch\n     def test_model_from_pretrained_meta_device(self):\n         def is_on_meta(model_id, dtype):"
        }
    ],
    "stats": {
        "total": 223,
        "additions": 155,
        "deletions": 68
    }
}