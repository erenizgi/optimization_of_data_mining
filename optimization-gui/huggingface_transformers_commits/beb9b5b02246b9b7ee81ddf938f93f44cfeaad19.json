{
    "author": "yonigozlan",
    "message": "Fix Pan and Scan on batched images Gemma3 (#36864)\n\n* process flattened images in fast image proc\n\n* process flattened images in low proc and add tests\n\n* remove print\n\n* add unbalanced batch test pas image proc\n\n* fix integration tests",
    "sha": "beb9b5b02246b9b7ee81ddf938f93f44cfeaad19",
    "files": [
        {
            "sha": "ddf29a3331bed4197a3c825411cd75f536ca9aa7",
            "filename": "src/transformers/models/gemma3/image_processing_gemma3.py",
            "status": "modified",
            "additions": 35,
            "deletions": 40,
            "changes": 75,
            "blob_url": "https://github.com/huggingface/transformers/blob/beb9b5b02246b9b7ee81ddf938f93f44cfeaad19/src%2Ftransformers%2Fmodels%2Fgemma3%2Fimage_processing_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/beb9b5b02246b9b7ee81ddf938f93f44cfeaad19/src%2Ftransformers%2Fmodels%2Fgemma3%2Fimage_processing_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fimage_processing_gemma3.py?ref=beb9b5b02246b9b7ee81ddf938f93f44cfeaad19",
            "patch": "@@ -35,7 +35,7 @@\n     get_image_size,\n     infer_channel_dimension_format,\n     is_scaled_image,\n-    make_nested_list_of_images,\n+    make_flat_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_preprocess_arguments,\n@@ -334,9 +334,9 @@ def preprocess(\n             else self.pan_and_scan_min_ratio_to_activate\n         )\n \n-        images_list = make_nested_list_of_images(images)\n+        images = make_flat_list_of_images(images)\n \n-        if not valid_images(images_list[0]):\n+        if not valid_images(images):\n             raise ValueError(\n                 \"Invalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \"\n                 \"torch.Tensor, tf.Tensor or jax.ndarray.\"\n@@ -353,58 +353,53 @@ def preprocess(\n             resample=resample,\n         )\n         if do_convert_rgb:\n-            images_list = [[convert_to_rgb(image) for image in images] for images in images_list]\n+            images = [convert_to_rgb(image) for image in images]\n \n         # All transformations expect numpy arrays.\n-        images_list = [[to_numpy_array(image) for image in images] for images in images_list]\n+        images = [to_numpy_array(image) for image in images]\n \n-        if do_rescale and is_scaled_image(images_list[0][0]):\n+        if do_rescale and is_scaled_image(images[0]):\n             logger.warning_once(\n                 \"It looks like you are trying to rescale already rescaled images. If the input\"\n                 \" images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\"\n             )\n \n         if input_data_format is None:\n             # We assume that all images have the same channel dimension format.\n-            input_data_format = infer_channel_dimension_format(images_list[0][0])\n+            input_data_format = infer_channel_dimension_format(images[0])\n \n         if do_pan_and_scan:\n-            images_list_and_num_crops = [\n-                self._process_images_for_pan_and_scan(\n-                    images=images,\n-                    do_pan_and_scan=do_pan_and_scan,\n-                    pan_and_scan_min_crop_size=pan_and_scan_min_crop_size,\n-                    pan_and_scan_max_num_crops=pan_and_scan_max_num_crops,\n-                    pan_and_scan_min_ratio_to_activate=pan_and_scan_min_ratio_to_activate,\n-                    data_format=data_format,\n-                    input_data_format=input_data_format,\n-                )\n-                for images in images_list\n-            ]\n-            images_list = [images for images, _ in images_list_and_num_crops]\n-            num_crops = [num_crops for _, num_crops in images_list_and_num_crops]\n+            images, num_crops = self._process_images_for_pan_and_scan(\n+                images=images,\n+                do_pan_and_scan=do_pan_and_scan,\n+                pan_and_scan_min_crop_size=pan_and_scan_min_crop_size,\n+                pan_and_scan_max_num_crops=pan_and_scan_max_num_crops,\n+                pan_and_scan_min_ratio_to_activate=pan_and_scan_min_ratio_to_activate,\n+                data_format=data_format,\n+                input_data_format=input_data_format,\n+            )\n+\n         else:\n-            num_crops = [[0] for _ in images_list]\n+            num_crops = [0 for _ in images]\n \n         processed_images = []\n-        for images in images_list:\n-            for image in images:\n-                if do_resize:\n-                    height, width = size[\"height\"], size[\"width\"]\n-                    image = resize(\n-                        image=image, size=(height, width), resample=resample, input_data_format=input_data_format\n-                    )\n-\n-                if do_rescale:\n-                    image = self.rescale(image=image, scale=rescale_factor, input_data_format=input_data_format)\n-\n-                if do_normalize:\n-                    image = self.normalize(\n-                        image=image, mean=image_mean, std=image_std, input_data_format=input_data_format\n-                    )\n-\n-                image = to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format)\n-                processed_images.append(image)\n+        for image in images:\n+            if do_resize:\n+                height, width = size[\"height\"], size[\"width\"]\n+                image = resize(\n+                    image=image, size=(height, width), resample=resample, input_data_format=input_data_format\n+                )\n+\n+            if do_rescale:\n+                image = self.rescale(image=image, scale=rescale_factor, input_data_format=input_data_format)\n+\n+            if do_normalize:\n+                image = self.normalize(\n+                    image=image, mean=image_mean, std=image_std, input_data_format=input_data_format\n+                )\n+\n+            image = to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format)\n+            processed_images.append(image)\n \n         data = {\"pixel_values\": processed_images, \"num_crops\": num_crops}\n         return BatchFeature(data=data, tensor_type=return_tensors)"
        },
        {
            "sha": "f86dbd3c756a25cb2534f32261c19488cafc1008",
            "filename": "src/transformers/models/gemma3/image_processing_gemma3_fast.py",
            "status": "modified",
            "additions": 56,
            "deletions": 87,
            "changes": 143,
            "blob_url": "https://github.com/huggingface/transformers/blob/beb9b5b02246b9b7ee81ddf938f93f44cfeaad19/src%2Ftransformers%2Fmodels%2Fgemma3%2Fimage_processing_gemma3_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/beb9b5b02246b9b7ee81ddf938f93f44cfeaad19/src%2Ftransformers%2Fmodels%2Fgemma3%2Fimage_processing_gemma3_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fimage_processing_gemma3_fast.py?ref=beb9b5b02246b9b7ee81ddf938f93f44cfeaad19",
            "patch": "@@ -16,7 +16,6 @@\n \n import itertools\n import math\n-from functools import partial\n from typing import List, Optional, Union\n \n from ...image_processing_utils_fast import (\n@@ -31,11 +30,8 @@\n from ...image_utils import (\n     IMAGENET_STANDARD_MEAN,\n     IMAGENET_STANDARD_STD,\n-    ChannelDimension,\n     ImageInput,\n     SizeDict,\n-    get_image_size,\n-    make_nested_list_of_images,\n )\n from ...processing_utils import Unpack\n from ...utils import (\n@@ -103,52 +99,9 @@ class Gemma3ImageProcessorFast(BaseImageProcessorFast):\n     def __init__(self, **kwargs: Unpack[Gemma3FastImageProcessorKwargs]):\n         super().__init__(**kwargs)\n \n-    def _prepare_images_structure(\n+    def pan_and_scan_batched(\n         self,\n-        images: ImageInput,\n-    ) -> ImageInput:\n-        \"\"\"\n-        Prepare the images structure for processing.\n-\n-        Args:\n-            images (`ImageInput`):\n-                The input images to process.\n-\n-        Returns:\n-            `ImageInput`: The images with a valid nesting.\n-        \"\"\"\n-        return make_nested_list_of_images(images)\n-\n-    def _prepare_input_images(\n-        self,\n-        images: ImageInput,\n-        do_convert_rgb: bool = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-        device: Optional[\"torch.device\"] = None,\n-    ) -> List[\"torch.Tensor\"]:\n-        \"\"\"\n-        Prepare the input images for processing.\n-        \"\"\"\n-        batch_images = self._prepare_images_structure(images)\n-        process_image_fn = partial(\n-            self._process_image,\n-            do_convert_rgb=do_convert_rgb,\n-            input_data_format=input_data_format,\n-            device=device,\n-        )\n-        # todo: yoni - check if we can parallelize this efficiently\n-        batch_processed_images = []\n-        for image_list in batch_images:\n-            processed_images = []\n-            for image in image_list:\n-                processed_images.append(process_image_fn(image))\n-            batch_processed_images.append(processed_images)\n-\n-        return batch_processed_images\n-\n-    def pan_and_scan(\n-        self,\n-        image: \"torch.Tensor\",\n+        images: \"torch.Tensor\",\n         pan_and_scan_min_crop_size: int,\n         pan_and_scan_max_num_crops: int,\n         pan_and_scan_min_ratio_to_activate: float,\n@@ -167,7 +120,7 @@ def pan_and_scan(\n             pan_and_scan_min_ratio_to_activate (`float`, *optional*):\n                 Minimum aspect ratio to activate pan and scan.\n         \"\"\"\n-        height, width = get_image_size(image, channel_dim=ChannelDimension.FIRST)\n+        height, width = images.shape[-2:]\n \n         # Square or landscape image.\n         if width >= height:\n@@ -210,7 +163,7 @@ def pan_and_scan(\n         crop_positions_h = [crop_size_h * i for i in range(num_crops_h)]\n \n         return [\n-            image[:, pos_h : pos_h + crop_size_h, pos_w : pos_w + crop_size_w]\n+            images[..., pos_h : pos_h + crop_size_h, pos_w : pos_w + crop_size_w]\n             for pos_h, pos_w in itertools.product(crop_positions_h, crop_positions_w)\n         ]\n \n@@ -222,18 +175,14 @@ def _process_images_for_pan_and_scan(\n         pan_and_scan_max_num_crops: int,\n         pan_and_scan_min_ratio_to_activate: float,\n     ):\n-        pas_images_list = []\n-        num_crops = []\n-        for image in images:\n-            pas_images = self.pan_and_scan(\n-                image=image,\n-                pan_and_scan_min_crop_size=pan_and_scan_min_crop_size,\n-                pan_and_scan_max_num_crops=pan_and_scan_max_num_crops,\n-                pan_and_scan_min_ratio_to_activate=pan_and_scan_min_ratio_to_activate,\n-            )\n-            pas_images_list.extend([image] + pas_images)\n-            num_crops.append(len(pas_images))\n-        return pas_images_list, num_crops\n+        pas_images = self.pan_and_scan_batched(\n+            images=images,\n+            pan_and_scan_min_crop_size=pan_and_scan_min_crop_size,\n+            pan_and_scan_max_num_crops=pan_and_scan_max_num_crops,\n+            pan_and_scan_min_ratio_to_activate=pan_and_scan_min_ratio_to_activate,\n+        )\n+        num_crops = [len(pas_images) for _ in images]\n+        return pas_images, num_crops\n \n     @add_start_docstrings(\n         BASE_IMAGE_PROCESSOR_FAST_DOCSTRING_PREPROCESS,\n@@ -274,46 +223,66 @@ def _preprocess(\n         image_std: Optional[Union[float, List[float]]],\n         return_tensors: Optional[Union[str, TensorType]],\n     ) -> BatchFeature:\n-        processed_images = []\n-        batch_num_crops = []\n-\n-        for images_list in images:\n+        # Group images by size for batched processing\n+        processed_images_grouped = {}\n+        num_crops_grouped = {}\n+        grouped_images, grouped_images_index = group_images_by_shape(images)\n+        for shape_images, stacked_images in grouped_images.items():\n             if do_pan_and_scan:\n-                images_list, num_crops = self._process_images_for_pan_and_scan(\n-                    images=images_list,\n+                pas_images, num_crops = self._process_images_for_pan_and_scan(\n+                    images=stacked_images,\n                     do_pan_and_scan=do_pan_and_scan,\n                     pan_and_scan_min_crop_size=pan_and_scan_min_crop_size,\n                     pan_and_scan_max_num_crops=pan_and_scan_max_num_crops,\n                     pan_and_scan_min_ratio_to_activate=pan_and_scan_min_ratio_to_activate,\n                 )\n+                # Add the thumbnails to the image patches\n+                stacked_images = [stacked_images] + pas_images\n+                # Group images by size for batched resizing (this will typically group thumbnails together and cropped patches together)\n+                processed_image_patches_grouped = {}\n+                grouped_image_patches, grouped_image_patches_index = group_images_by_shape(stacked_images)\n+                for shape, stacked_image_patches in grouped_image_patches.items():\n+                    stacked_image_patches = self.resize(\n+                        image=stacked_image_patches,\n+                        size=size,\n+                        interpolation=interpolation,\n+                    )\n+                    processed_image_patches_grouped[shape] = stacked_image_patches\n+                processed_image_patches = reorder_images(processed_image_patches_grouped, grouped_image_patches_index)\n+                # Transpose to have the thumbnails with their corresponding patches\n+                stacked_images = torch.stack(processed_image_patches, dim=0).transpose(0, 1).contiguous()\n             else:\n-                num_crops = [[0] for _ in images_list]\n+                num_crops = [0 for _ in stacked_images]\n \n-            # Group images by size for batched processing\n-            processed_image_patches_grouped = {}\n-            grouped_image_patches, grouped_image_patches_index = group_images_by_shape(images_list)\n-            for shape, stacked_image_patches in grouped_image_patches.items():\n                 if do_resize:\n-                    stacked_image_patches = self.resize(\n-                        image=stacked_image_patches,\n+                    stacked_images = self.resize(\n+                        image=stacked_images,\n                         size=size,\n                         interpolation=interpolation,\n                     )\n-                # Fused rescale and normalize\n-                stacked_image_patches = self.rescale_and_normalize(\n-                    stacked_image_patches, do_rescale, rescale_factor, do_normalize, image_mean, image_std\n-                )\n-                processed_image_patches_grouped[shape] = stacked_image_patches\n-            processed_image_patches = reorder_images(processed_image_patches_grouped, grouped_image_patches_index)\n-            processed_image_patches = (\n-                torch.stack(processed_image_patches, dim=0) if return_tensors else processed_image_patches\n+            num_crops_grouped[shape_images] = num_crops\n+            processed_images_grouped[shape_images] = stacked_images\n+        resized_images = reorder_images(processed_images_grouped, grouped_images_index)\n+        # If pan and scan is enabled, we need to flatten the list of images\n+        if do_pan_and_scan:\n+            resized_images = [image for images_list in resized_images for image in images_list]\n+        num_crops = reorder_images(num_crops_grouped, grouped_images_index)\n+\n+        # Group images by size for further processing\n+        # Needed in case do_resize is False, or resize returns images with different sizes\n+        grouped_images, grouped_images_index = group_images_by_shape(resized_images)\n+        processed_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            # Fused rescale and normalize\n+            stacked_images = self.rescale_and_normalize(\n+                stacked_images, do_rescale, rescale_factor, do_normalize, image_mean, image_std\n             )\n-            processed_images.extend(processed_image_patches)\n-            batch_num_crops.extend(num_crops)\n+            processed_images_grouped[shape] = stacked_images\n \n+        processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n         processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n         return BatchFeature(\n-            data={\"pixel_values\": processed_images, \"num_crops\": batch_num_crops}, tensor_type=return_tensors\n+            data={\"pixel_values\": processed_images, \"num_crops\": num_crops}, tensor_type=return_tensors\n         )\n \n "
        },
        {
            "sha": "bfdf400212c8991cc91f62f232c2bf126ae63578",
            "filename": "src/transformers/models/gemma3/processing_gemma3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/beb9b5b02246b9b7ee81ddf938f93f44cfeaad19/src%2Ftransformers%2Fmodels%2Fgemma3%2Fprocessing_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/beb9b5b02246b9b7ee81ddf938f93f44cfeaad19/src%2Ftransformers%2Fmodels%2Fgemma3%2Fprocessing_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fprocessing_gemma3.py?ref=beb9b5b02246b9b7ee81ddf938f93f44cfeaad19",
            "patch": "@@ -113,7 +113,8 @@ def __call__(\n                 )\n \n             # Replace image tokens by the full expanded sequence\n-            batch_num_crops = to_py_obj(image_inputs.pop(\"num_crops\"))\n+            num_crops = to_py_obj(image_inputs.pop(\"num_crops\"))\n+            batch_num_crops = [[num_crops.pop(0) for _ in range(len(images))] for images in batched_images]\n             for batch_idx, (prompt, images, num_crops) in enumerate(zip(text, batched_images, batch_num_crops)):\n                 image_indexes = [m.start() for m in re.finditer(self.boi_token, prompt)]\n \n@@ -139,7 +140,7 @@ def __call__(\n         text_inputs = self.tokenizer(text=text, **output_kwargs[\"text_kwargs\"], return_tensors=\"np\")\n \n         # Add token type ids manually, as tokenizer can't do arbitrary position token types\n-        array_ids = np.array(text_inputs[\"input_ids\"])\n+        array_ids = text_inputs[\"input_ids\"]\n         mm_token_type_ids = np.zeros_like(text_inputs[\"input_ids\"])\n         mm_token_type_ids[array_ids == self.image_token_id] = 1\n         text_inputs = {k: v.tolist() for k, v in text_inputs.items()}  # in case user requested list inputs"
        },
        {
            "sha": "28410f2d51e99bf11c6e09455194812cfd4d363e",
            "filename": "tests/models/gemma3/test_image_processing_gemma3.py",
            "status": "modified",
            "additions": 41,
            "deletions": 0,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/beb9b5b02246b9b7ee81ddf938f93f44cfeaad19/tests%2Fmodels%2Fgemma3%2Ftest_image_processing_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/beb9b5b02246b9b7ee81ddf938f93f44cfeaad19/tests%2Fmodels%2Fgemma3%2Ftest_image_processing_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3%2Ftest_image_processing_gemma3.py?ref=beb9b5b02246b9b7ee81ddf938f93f44cfeaad19",
            "patch": "@@ -189,6 +189,13 @@ def test_pan_and_scan(self):\n             expected_output_image_shape = (9, 3, 18, 18)\n             self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n \n+            # Test batched unbalanced, 9 images because we have base image + 2 crops per each item\n+            encoded_images = image_processing(\n+                [[image_inputs[0], image_inputs[1]], [image_inputs[2]]], return_tensors=\"pt\"\n+            ).pixel_values\n+            expected_output_image_shape = (9, 3, 18, 18)\n+            self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n+\n     def test_call_pil(self):\n         for image_processing_class in self.image_processor_list:\n             # Initialize image_processing\n@@ -250,3 +257,37 @@ def test_call_pytorch(self):\n     @unittest.skip(\"Gemma3 doesn't work with 4 channels due to pan and scan method\")\n     def test_call_numpy_4_channels(self):\n         pass\n+\n+    @require_vision\n+    @require_torch\n+    def test_slow_fast_equivalence_batched_pas(self):\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n+\n+        if hasattr(self.image_processor_tester, \"do_center_crop\") and self.image_processor_tester.do_center_crop:\n+            self.skipTest(\n+                reason=\"Skipping as do_center_crop is True and center_crop functions are not equivalent for fast and slow processors\"\n+            )\n+        crop_config = {\n+            \"do_pan_and_scan\": True,\n+            \"pan_and_scan_max_num_crops\": 448,\n+            \"pan_and_scan_min_crop_size\": 32,\n+            \"pan_and_scan_min_ratio_to_activate\": 0.3,\n+        }\n+        image_processor_dict = self.image_processor_dict\n+        image_processor_dict.update(crop_config)\n+        dummy_images = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n+        image_processor_slow = self.image_processing_class(**image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**image_processor_dict)\n+\n+        encoding_slow = image_processor_slow(dummy_images, return_tensors=\"pt\")\n+        encoding_fast = image_processor_fast(dummy_images, return_tensors=\"pt\")\n+\n+        torch.testing.assert_close(encoding_slow.num_crops, encoding_fast.num_crops)\n+        self.assertTrue(torch.allclose(encoding_slow.pixel_values, encoding_fast.pixel_values, atol=1e-1))\n+        self.assertLessEqual(\n+            torch.mean(torch.abs(encoding_slow.pixel_values - encoding_fast.pixel_values)).item(), 1e-3\n+        )"
        },
        {
            "sha": "e5e3fde291003fdaac86158af31712fce0e7d4c3",
            "filename": "tests/models/gemma3/test_modeling_gemma3.py",
            "status": "modified",
            "additions": 51,
            "deletions": 2,
            "changes": 53,
            "blob_url": "https://github.com/huggingface/transformers/blob/beb9b5b02246b9b7ee81ddf938f93f44cfeaad19/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/beb9b5b02246b9b7ee81ddf938f93f44cfeaad19/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py?ref=beb9b5b02246b9b7ee81ddf938f93f44cfeaad19",
            "patch": "@@ -395,7 +395,7 @@ def test_model_4b_bf16(self):\n         output = model.generate(**inputs, max_new_tokens=30, do_sample=False)\n         output_text = self.processor.batch_decode(output, skip_special_tokens=True)\n \n-        EXPECTED_TEXTS = ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nCertainly! \\n\\nThe image shows a brown cow standing on a sandy beach with clear blue water and a blue sky in the background. It looks like']  # fmt: skip\n+        EXPECTED_TEXTS = ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nCertainly! \\n\\nThe image shows a brown cow standing on a sandy beach with clear turquoise water and a blue sky in the background. It looks like']  # fmt: skip\n         self.assertEqual(output_text, EXPECTED_TEXTS)\n \n     def test_model_4b_batch(self):\n@@ -467,7 +467,56 @@ def test_model_4b_crops(self):\n         output_text = self.processor.batch_decode(output, skip_special_tokens=True)\n \n         EXPECTED_NUM_IMAGES = 3  # one for the origin image and two crops of images\n-        EXPECTED_TEXTS = ['user\\nYou are a helpful assistant.\\n\\nHere is the original image \\n\\n\\n\\n and here are some crops to help you see better \\n\\n\\n\\n \\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nThe image shows a brown cow standing on a beach with a turquoise ocean and blue sky in the background.']  # fmt: skip\n+        EXPECTED_TEXTS = ['user\\nYou are a helpful assistant.\\n\\nHere is the original image \\n\\n\\n\\n and here are some crops to help you see better \\n\\n\\n\\n \\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nThe image shows a brown cow standing on a beach with a turquoise ocean and blue sky in the background. It looks like the cow is enjoying the beach']  # fmt: skip\n+        self.assertEqual(len(inputs[\"pixel_values\"]), EXPECTED_NUM_IMAGES)\n+        self.assertEqual(output_text, EXPECTED_TEXTS)\n+\n+    def test_model_4b_batch_crops(self):\n+        model_id = \"google/gemma-3-4b-it\"\n+\n+        model = Gemma3ForConditionalGeneration.from_pretrained(\n+            model_id, low_cpu_mem_usage=True, torch_dtype=torch.bfloat16\n+        ).to(torch_device)\n+        crop_config = {\n+            \"images_kwargs\": {\n+                \"do_pan_and_scan\": True,\n+                \"pan_and_scan_max_num_crops\": 448,\n+                \"pan_and_scan_min_crop_size\": 32,\n+                \"pan_and_scan_min_ratio_to_activate\": 0.3,\n+            }\n+        }\n+        messages_2 = [\n+            {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]},\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\n+                        \"type\": \"image\",\n+                        \"url\": \"https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/cow_beach_1.png\",\n+                    },\n+                    {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"},\n+                    {\"type\": \"text\", \"text\": \"Are these images identical?\"},\n+                ],\n+            },\n+        ]\n+\n+        inputs = self.processor.apply_chat_template(\n+            [self.messages, messages_2],\n+            tokenize=True,\n+            return_dict=True,\n+            return_tensors=\"pt\",\n+            padding=True,\n+            add_generation_prompt=True,\n+            **crop_config,\n+        ).to(torch_device)\n+\n+        output = model.generate(**inputs, max_new_tokens=30, do_sample=False)\n+        output_text = self.processor.batch_decode(output, skip_special_tokens=True)\n+        EXPECTED_NUM_IMAGES = 9  # 3 * (one for the origin image and two crops of images) = 9\n+        EXPECTED_TEXTS = [\n+            \"user\\nYou are a helpful assistant.\\n\\nHere is the original image \\n\\n\\n\\n and here are some crops to help you see better \\n\\n\\n\\n \\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nThe image shows a brown cow standing on a beach with a turquoise ocean and blue sky in the background. It looks like the cow is enjoying the beach\",\n+            \"user\\nYou are a helpful assistant.\\n\\nHere is the original image \\n\\n\\n\\n and here are some crops to help you see better \\n\\n\\n\\n \\n\\n\\n\\nHere is the original image \\n\\n\\n\\n and here are some crops to help you see better \\n\\n\\n\\n \\n\\n\\n\\nAre these images identical?\\nmodel\\nNo, the images are not identical. \\n\\nWhile they all feature a brown cow in the foreground and a similar background (including the stop signs and\",\n+        ]  # fmt: skip\n         self.assertEqual(len(inputs[\"pixel_values\"]), EXPECTED_NUM_IMAGES)\n         self.assertEqual(output_text, EXPECTED_TEXTS)\n "
        }
    ],
    "stats": {
        "total": 317,
        "additions": 186,
        "deletions": 131
    }
}