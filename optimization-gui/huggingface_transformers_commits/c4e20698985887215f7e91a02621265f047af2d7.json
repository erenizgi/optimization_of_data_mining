{
    "author": "manueldeprada",
    "message": "Fix Cache.max_cache_len max value for Hybrid models (#39737)\n\n* fix gemma\n\n* fix min\n\n* fix quant init issue\n\n* fix gemma 3n\n\n* skip quant cache test\n\n* fix modular\n\n* new test for Gemma\n\n* include cyril change\n\n---------\n\nCo-authored-by: Cyril Vallez <cyril.vallez@gmail.com>",
    "sha": "c4e20698985887215f7e91a02621265f047af2d7",
    "files": [
        {
            "sha": "93c4af7cdc188d38f7c2804f4a56aeaf0da3d30f",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 7,
            "deletions": 12,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/c4e20698985887215f7e91a02621265f047af2d7/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c4e20698985887215f7e91a02621265f047af2d7/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=c4e20698985887215f7e91a02621265f047af2d7",
            "patch": "@@ -325,8 +325,9 @@ def __init__(self, sliding_window, *args, **kwargs):\n             sliding_window (`int`):\n                 Effective window size: number of tokens that are kept on each update call.\n         \"\"\"\n-        kwargs.pop(\"max_cache_len\", None)\n-        super().__init__(*args, max_cache_len=sliding_window, *args, **kwargs)\n+        max_cache_len = kwargs.pop(\"max_cache_len\", None)\n+        max_cache_len = min(sliding_window, max_cache_len) if max_cache_len is not None else sliding_window\n+        super().__init__(*args, max_cache_len=max_cache_len, *args, **kwargs)\n \n     def update(\n         self,\n@@ -1277,9 +1278,7 @@ def max_batch_size(self) -> int:\n     def max_cache_len(self) -> int:\n         \"\"\"Return the maximum cache length of the cache\"\"\"\n         values = [layer.max_cache_len for layer in self.layers]\n-        if len(set(values)) > 1:\n-            raise ValueError(f\"Max cache length is not consistent across layers: {values}\")\n-        return values[0]\n+        return max(values)\n \n     @property\n     def is_compileable(self) -> bool:\n@@ -1655,7 +1654,7 @@ class QuantoQuantizedCache(QuantizedCache):\n     \"\"\"\n \n     def __init__(self, **kwargs) -> None:\n-        Cache.__init__(self, cache_processor=QuantoQuantizedCacheProcessor, **kwargs)\n+        DynamicCache.__init__(self, cache_processor=QuantoQuantizedCacheProcessor, **kwargs)\n \n \n class HQQQuantizedCache(QuantizedCache):\n@@ -1697,7 +1696,7 @@ class HQQQuantizedCache(QuantizedCache):\n \n     def __init__(self, backend=\"HQQ\", **kwargs) -> None:\n         assert backend == \"HQQ\"\n-        Cache.__init__(self, cache_processor=HQQQuantizedCacheProcessor, **kwargs)\n+        DynamicCache.__init__(self, cache_processor=HQQQuantizedCacheProcessor, **kwargs)\n \n \n class EncoderDecoderCache(Cache):\n@@ -1951,10 +1950,6 @@ def parse_layer_args_from_model_config(\n                 )\n         # Adjust max_cache_len for sliding window layers (they can't be larger than sliding window)\n         max_cache_len = max_cache_len or config.max_position_embeddings\n-        if getattr(config, \"sliding_window\", None) is not None:\n-            sliding_window_len = min(config.sliding_window, max_cache_len)\n-        else:\n-            sliding_window_len = None\n         # Some model define a custom `head_dim` != config.hidden_size // config.num_attention_heads:\n         head_dim = (\n             config.head_dim\n@@ -1981,7 +1976,7 @@ def parse_layer_args_from_model_config(\n             \"layer_device_map\": layer_device_map,\n             \"head_dim\": head_dim,\n             \"num_heads\": num_heads,\n-            \"sliding_window\": sliding_window_len,\n+            \"sliding_window\": getattr(config, \"sliding_window\", None),\n         }\n         return {k: v for k, v in layer_args.items() if v is not None}\n "
        },
        {
            "sha": "3cf07819be52097ed794f6ed921937c663adb981",
            "filename": "src/transformers/models/gemma3n/modeling_gemma3n.py",
            "status": "modified",
            "additions": 12,
            "deletions": 14,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/c4e20698985887215f7e91a02621265f047af2d7/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c4e20698985887215f7e91a02621265f047af2d7/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py?ref=c4e20698985887215f7e91a02621265f047af2d7",
            "patch": "@@ -30,7 +30,7 @@\n import torch.nn.functional as F\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, HybridCache\n+from ...cache_utils import Cache, DynamicCache, SlidingWindowLayer\n from ...generation import GenerationMixin\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n@@ -1327,22 +1327,20 @@ def forward(\n         query_states = query_states.transpose(1, 2)\n \n         if self.is_kv_shared_layer and self.kv_shared_layer_index is not None and past_key_value is not None:\n-            # Device of past layer may be different from current one\n-            indices = cache_position.to(past_key_value.layers[self.kv_shared_layer_index].keys.device)\n             # In this case we need special handling of the slice as the layer is of fixed small size (for full layers, we never go beyond)\n-            if isinstance(past_key_value, HybridCache) and self.is_sliding:\n-                max_length = past_key_value.sliding_window\n-                indices = (\n-                    slice(0, max_length)\n-                    if cache_position.shape[0] > max_length\n-                    else cache_position.clamp(min=0, max=max_length - 1)\n-                )\n+            layer = past_key_value.layers[self.kv_shared_layer_index]\n+            # Device of past layer may be different from current one\n+            indices = cache_position.to(layer.keys.device)\n+            # Sliding window cache layers might have smaller size (for full layers, we never go beyond)\n+            if isinstance(layer, SlidingWindowLayer):\n+                if cache_position.shape[0] > layer.get_max_cache_shape():\n+                    indices = slice(0, layer.get_max_cache_shape())\n+                else:\n+                    indices = indices.clamp(min=0, max=layer.get_max_cache_shape() - 1)\n \n             # Device of past layer may be different from current one\n-            key_states = past_key_value.layers[self.kv_shared_layer_index].keys[:, :, indices].to(query_states.device)\n-            value_states = (\n-                past_key_value.layers[self.kv_shared_layer_index].values[:, :, indices].to(query_states.device)\n-            )\n+            key_states = layer.keys[:, :, indices].to(query_states.device)\n+            value_states = layer.values[:, :, indices].to(query_states.device)\n         else:\n             key_states = self.k_proj(hidden_states).view(hidden_shape)\n             key_states = self.k_norm(key_states)"
        },
        {
            "sha": "2716e9bdfe799dccedebcf0a3ccbc2d469a4c7f5",
            "filename": "src/transformers/models/gemma3n/modular_gemma3n.py",
            "status": "modified",
            "additions": 12,
            "deletions": 14,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/c4e20698985887215f7e91a02621265f047af2d7/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c4e20698985887215f7e91a02621265f047af2d7/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py?ref=c4e20698985887215f7e91a02621265f047af2d7",
            "patch": "@@ -23,7 +23,7 @@\n import torch.nn.functional as F\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, HybridCache\n+from ...cache_utils import Cache, DynamicCache, SlidingWindowLayer\n from ...configuration_utils import PretrainedConfig, layer_type_validation\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n@@ -1769,22 +1769,20 @@ def forward(\n         query_states = query_states.transpose(1, 2)\n \n         if self.is_kv_shared_layer and self.kv_shared_layer_index is not None and past_key_value is not None:\n-            # Device of past layer may be different from current one\n-            indices = cache_position.to(past_key_value.layers[self.kv_shared_layer_index].keys.device)\n             # In this case we need special handling of the slice as the layer is of fixed small size (for full layers, we never go beyond)\n-            if isinstance(past_key_value, HybridCache) and self.is_sliding:\n-                max_length = past_key_value.sliding_window\n-                indices = (\n-                    slice(0, max_length)\n-                    if cache_position.shape[0] > max_length\n-                    else cache_position.clamp(min=0, max=max_length - 1)\n-                )\n+            layer = past_key_value.layers[self.kv_shared_layer_index]\n+            # Device of past layer may be different from current one\n+            indices = cache_position.to(layer.keys.device)\n+            # Sliding window cache layers might have smaller size (for full layers, we never go beyond)\n+            if isinstance(layer, SlidingWindowLayer):\n+                if cache_position.shape[0] > layer.get_max_cache_shape():\n+                    indices = slice(0, layer.get_max_cache_shape())\n+                else:\n+                    indices = indices.clamp(min=0, max=layer.get_max_cache_shape() - 1)\n \n             # Device of past layer may be different from current one\n-            key_states = past_key_value.layers[self.kv_shared_layer_index].keys[:, :, indices].to(query_states.device)\n-            value_states = (\n-                past_key_value.layers[self.kv_shared_layer_index].values[:, :, indices].to(query_states.device)\n-            )\n+            key_states = layer.keys[:, :, indices].to(query_states.device)\n+            value_states = layer.values[:, :, indices].to(query_states.device)\n         else:\n             key_states = self.k_proj(hidden_states).view(hidden_shape)\n             key_states = self.k_norm(key_states)"
        },
        {
            "sha": "43ac57dbb56670ca04e8ebdfc6a8d66b4ceebc8c",
            "filename": "tests/models/gemma3/test_modeling_gemma3.py",
            "status": "modified",
            "additions": 46,
            "deletions": 0,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/c4e20698985887215f7e91a02621265f047af2d7/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c4e20698985887215f7e91a02621265f047af2d7/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py?ref=c4e20698985887215f7e91a02621265f047af2d7",
            "patch": "@@ -151,6 +151,52 @@ def test_eager_padding_matches_padding_free_with_position_ids(self):\n     def test_sdpa_padding_matches_padding_free_with_position_ids(self):\n         pass\n \n+    def test_generation_beyond_sliding_window_tiny_model(self):\n+        \"\"\"Test generation with a tiny randomly initialised model whose input length is larger than the `sliding_window`.\n+        The model is configured with both `full_attention` and `sliding_attention` layers to make sure the hybrid cache\n+        and mask slicing logic is covered.\n+        \"\"\"\n+        config = Gemma3TextConfig.from_pretrained(\"hf-internal-testing/tiny-random-Gemma3ForCausalLM\")\n+        config.attn_implementation = \"eager\"\n+        config.layer_types = [\"full_attention\", \"sliding_attention\"]\n+        config.sliding_window = 8\n+        config.max_position_embeddings = 128\n+        model = AutoModelForCausalLM.from_pretrained(\n+            \"hf-internal-testing/tiny-random-Gemma3ForCausalLM\", config=config\n+        ).to(torch_device)\n+\n+        input_len = 10\n+        input_ids = torch.tensor(\n+            [\n+                [42300, 241087, 255445, 81315, 193760, 184471, 67719, 98191, 210651, 124725],\n+                [102294, 205314, 226646, 62020, 60245, 68025, 251839, 114053, 4695, 175511],\n+            ],\n+            device=torch_device,\n+        )\n+        attention_mask = torch.ones_like(input_ids).to(torch_device)\n+        with torch.no_grad():\n+            _ = model.generate(\n+                input_ids,\n+                attention_mask=attention_mask,\n+                max_new_tokens=1,\n+                do_sample=False,\n+                use_cache=True,\n+                cache_implementation=\"hybrid\",\n+            )\n+            # 2 generations are needed to trigger https://github.com/huggingface/transformers/issues/39711\n+            # Since it requires model._cache to have been previously initialized\n+            output = model.generate(\n+                input_ids,\n+                attention_mask=attention_mask,\n+                max_new_tokens=5,\n+                do_sample=False,\n+                use_cache=True,\n+                cache_implementation=\"hybrid\",\n+            )\n+        generated_sequences = output[:, input_len:].cpu()\n+        EXPECTED_OUTPUT = torch.tensor([[90109, 90109, 90109, 83191, 83191], [246901, 69832, 69832, 69832, 62288]])\n+        torch.testing.assert_close(generated_sequences, EXPECTED_OUTPUT)\n+\n \n class Gemma3Vision2TextModelTester:\n     def __init__("
        },
        {
            "sha": "34e474129d72a239733330bf9548837777926e29",
            "filename": "tests/models/gemma3n/test_modeling_gemma3n.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/c4e20698985887215f7e91a02621265f047af2d7/tests%2Fmodels%2Fgemma3n%2Ftest_modeling_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c4e20698985887215f7e91a02621265f047af2d7/tests%2Fmodels%2Fgemma3n%2Ftest_modeling_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3n%2Ftest_modeling_gemma3n.py?ref=c4e20698985887215f7e91a02621265f047af2d7",
            "patch": "@@ -431,6 +431,11 @@ def test_contrastive_generate_low_memory(self):\n     def test_dola_decoding_sample(self):\n         pass\n \n+    @pytest.mark.generate\n+    @unittest.skip(\"Gemma3n does not support QuantizedCache as it performs cache manipulation in the forward pass\")\n+    def test_generate_with_quant_cache(self):\n+        pass\n+\n \n class Gemma3nVision2TextModelTester:\n     text_config = {\"activation_sparsity_pattern\": None}"
        }
    ],
    "stats": {
        "total": 122,
        "additions": 82,
        "deletions": 40
    }
}