{
    "author": "yonigozlan",
    "message": "Fix MaskFormer/Mask2Former fast image processors (#41393)\n\n* Merge conflict\n\n* add fast processor\n\n* add fast processor\n\n* make style\n\n* add new convert rgb\n\n* use nested group by shape in mllama fast, add support for multiple inputs in group by shape\n\n* fix maskformer mask2 former fast im proc and add tests\n\n* refactor after review\n\n* add _iterate_items utility\n\n* Fix failing tests\n\n* fix copies and improve docs\n\n---------\n\nCo-authored-by: Vincent <phamvinh257@gmail.com>",
    "sha": "21913b2e100959467e9d97052575d33b45e06e0a",
    "files": [
        {
            "sha": "30b066b1a3c0d59bd86b57b5d500a3e80d15f098",
            "filename": "src/transformers/image_transforms.py",
            "status": "modified",
            "additions": 41,
            "deletions": 26,
            "changes": 67,
            "blob_url": "https://github.com/huggingface/transformers/blob/21913b2e100959467e9d97052575d33b45e06e0a/src%2Ftransformers%2Fimage_transforms.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21913b2e100959467e9d97052575d33b45e06e0a/src%2Ftransformers%2Fimage_transforms.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_transforms.py?ref=21913b2e100959467e9d97052575d33b45e06e0a",
            "patch": "@@ -828,7 +828,25 @@ def _cast_tensor_to_float(x):\n \n \n def _group_images_by_shape(nested_images, *paired_inputs, is_nested: bool = False):\n-    \"\"\"Helper function to flatten a single level of nested image and batch structures and group by shape.\"\"\"\n+    \"\"\"\n+    Helper function to flatten a single level of nested image and batch structures and group by shape.\n+    Args:\n+        nested_images (list):\n+            A list of images or a single tensor\n+        paired_inputs (Any, *optional*):\n+            Zero or more lists that mirror the structure of `nested_images` (flat list, or list of lists when\n+            `is_nested=True`). Each element is paired 1:1 with the corresponding image so it can be grouped by the\n+            same shape key. These paired values are grouped alongside `nested_images` but are not stacked in the output, so\n+            they do not need to be tensors.\n+        is_nested (bool, *optional*, defaults to False):\n+            Whether the images are nested.\n+    Returns:\n+        tuple[dict, ...]:\n+            - A dictionary with shape as key and list of images with that shape as value\n+            - A dictionary with shape as key and list of paired values with that shape as value\n+            - A dictionary mapping original indices to (shape, index) tuples\n+            - A dictionary mapping original indices to (shape, index) tuples for each paired input\n+    \"\"\"\n     grouped_images = defaultdict(list)\n     grouped_images_index = {}\n     paired_grouped_values = [defaultdict(list) for _ in paired_inputs]\n@@ -880,27 +898,20 @@ def _reconstruct_nested_structure(indices, processed_images):\n     return result\n \n \n-def _disable_grouping_output_nested(images, *paired_inputs):\n-    \"\"\"Build the disable_grouping output tuple for a single-level nested structure.\"\"\"\n-    outer_range = range(len(images))\n-    inner_ranges = [range(len(images[i])) for i in outer_range]\n-\n-    # Precompute all (i, j) pairs\n-    ij_pairs = [(i, j) for i in outer_range for j in inner_ranges[i]]\n-\n-    images_dict = {(i, j): images[i][j].unsqueeze(0) for (i, j) in ij_pairs}\n-    paired_dicts = [{(i, j): paired_list[i][j].unsqueeze(0) for (i, j) in ij_pairs} for paired_list in paired_inputs]\n-    index_map = {(i, j): ((i, j), 0) for (i, j) in ij_pairs}\n-    return images_dict, *paired_dicts, index_map\n-\n+def _iterate_items(items, is_nested: bool):\n+    \"\"\"\n+    Helper function to iterate over items yielding (key, item) pairs.\n \n-def _disable_grouping_output_flat(images, *paired_inputs):\n-    \"\"\"Build the disable_grouping output tuple for a flat list structure.\"\"\"\n-    idx_range = range(len(images))\n-    images_dict = {i: images[i].unsqueeze(0) for i in idx_range}\n-    paired_dicts = [{i: paired_list[i].unsqueeze(0) for i in idx_range} for paired_list in paired_inputs]\n-    index_map = {i: (i, 0) for i in idx_range}\n-    return images_dict, *paired_dicts, index_map\n+    For nested structures, yields ((row_index, col_index), item).\n+    For flat structures, yields (index, item).\n+    \"\"\"\n+    if is_nested:\n+        for i, row in enumerate(items):\n+            for j, item in enumerate(row):\n+                yield (i, j), item\n+    else:\n+        for i, item in enumerate(items):\n+            yield i, item\n \n \n def group_images_by_shape(\n@@ -920,7 +931,7 @@ def group_images_by_shape(\n     Args:\n         images (Union[list[\"torch.Tensor\"], \"torch.Tensor\"]):\n             A list of images or a single tensor\n-        *paired_inputs (Any):\n+        paired_inputs (Any, *optional*):\n             Zero or more lists that mirror the structure of `images` (flat list, or list of lists when\n             `is_nested=True`). Each element is paired 1:1 with the corresponding image so it can be grouped by the\n             same shape key. These paired values are grouped alongside `images` but are not stacked in the output, so\n@@ -944,10 +955,14 @@ def group_images_by_shape(\n         disable_grouping = device == \"cpu\"\n \n     if disable_grouping:\n-        if is_nested:\n-            return _disable_grouping_output_nested(images, *paired_inputs)\n-        else:\n-            return _disable_grouping_output_flat(images, *paired_inputs)\n+        return (\n+            {key: img.unsqueeze(0) for key, img in _iterate_items(images, is_nested)},\n+            *[\n+                {key: item.unsqueeze(0) for key, item in _iterate_items(paired_list, is_nested)}\n+                for paired_list in paired_inputs\n+            ],\n+            {key: (key, 0) for key, _ in _iterate_items(images, is_nested)},\n+        )\n \n     # Handle single level nested structure\n     grouped_images, *paired_grouped_values, grouped_images_index = _group_images_by_shape("
        },
        {
            "sha": "f8d176dcf042e2d13d9998986dbb512a6bce27d2",
            "filename": "src/transformers/models/mask2former/image_processing_mask2former_fast.py",
            "status": "modified",
            "additions": 17,
            "deletions": 13,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/21913b2e100959467e9d97052575d33b45e06e0a/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21913b2e100959467e9d97052575d33b45e06e0a/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former_fast.py?ref=21913b2e100959467e9d97052575d33b45e06e0a",
            "patch": "@@ -109,9 +109,6 @@ class Mask2FormerImageProcessorFast(BaseImageProcessorFast):\n     valid_kwargs = Mask2FormerImageProcessorKwargs\n \n     def __init__(self, **kwargs: Unpack[Mask2FormerImageProcessorKwargs]) -> None:\n-        if \"pad_and_return_pixel_mask\" in kwargs:\n-            kwargs[\"do_pad\"] = kwargs.pop(\"pad_and_return_pixel_mask\")\n-\n         size = kwargs.pop(\"size\", None)\n         max_size = kwargs.pop(\"max_size\", None)\n \n@@ -224,7 +221,7 @@ def pad(\n             padding = [0, 0, padding_right, padding_bottom]\n             images = F.pad(images, padding, fill=fill)\n             if segmentation_maps is not None:\n-                segmentation_maps = F.pad(segmentation_maps, padding, fill=ignore_index)\n+                segmentation_maps = [F.pad(mask, padding, fill=ignore_index) for mask in segmentation_maps]\n \n         # Make a pixel mask for the image, where 1 indicates a valid pixel and 0 indicates padding.\n         pixel_mask = torch.zeros((images.shape[0], *padded_size), dtype=torch.int64, device=images.device)\n@@ -318,9 +315,11 @@ def _preprocess(\n                 stacked_images = self.resize(\n                     image=stacked_images, size=size, size_divisor=size_divisor, interpolation=interpolation\n                 )\n-                if segmentation_maps is not None:\n+            if segmentation_maps is not None:\n+                stacked_segmentation_maps = grouped_segmentation_maps[shape]\n+                if do_resize:\n                     stacked_segmentation_maps = self.resize(\n-                        image=grouped_segmentation_maps[shape],\n+                        image=stacked_segmentation_maps,\n                         size=size,\n                         size_divisor=size_divisor,\n                         interpolation=F.InterpolationMode.NEAREST_EXACT,\n@@ -357,14 +356,18 @@ def _preprocess(\n                 mask_labels.append(masks)\n                 class_labels.append(classes)\n \n-        grouped_images, grouped_images_index = group_images_by_shape(resized_images, disable_grouping=disable_grouping)\n-        processed_images_grouped = {}\n-        processed_pixel_masks_grouped = {}\n         if segmentation_maps is not None:\n-            grouped_segmentation_maps, grouped_segmentation_maps_index = group_images_by_shape(\n-                mask_labels, disable_grouping=disable_grouping\n+            # group mask_labels as paired inputs and not images so as not to stack them\n+            grouped_images, grouped_segmentation_maps, grouped_images_index = group_images_by_shape(\n+                resized_images, mask_labels, disable_grouping=disable_grouping\n             )\n             processed_segmentation_maps_grouped = {}\n+        else:\n+            grouped_images, grouped_images_index = group_images_by_shape(\n+                resized_images, disable_grouping=disable_grouping\n+            )\n+        processed_images_grouped = {}\n+        processed_pixel_masks_grouped = {}\n         for shape, stacked_images in grouped_images.items():\n             # Fused rescale and normalize\n             stacked_images = self.rescale_and_normalize(\n@@ -379,7 +382,8 @@ def _preprocess(\n             processed_images_grouped[shape] = padded_images\n             processed_pixel_masks_grouped[shape] = pixel_masks\n             if segmentation_maps is not None:\n-                processed_segmentation_maps_grouped[shape] = padded_segmentation_maps.squeeze(1)\n+                processed_segmentation_maps_grouped[shape] = padded_segmentation_maps\n+\n         processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n         processed_pixel_masks = reorder_images(processed_pixel_masks_grouped, grouped_images_index)\n         encoded_inputs = BatchFeature(\n@@ -390,7 +394,7 @@ def _preprocess(\n             tensor_type=return_tensors,\n         )\n         if segmentation_maps is not None:\n-            mask_labels = reorder_images(processed_segmentation_maps_grouped, grouped_segmentation_maps_index)\n+            mask_labels = reorder_images(processed_segmentation_maps_grouped, grouped_images_index)\n             # we cannot batch them since they don't share a common class size\n             encoded_inputs[\"mask_labels\"] = mask_labels\n             encoded_inputs[\"class_labels\"] = class_labels"
        },
        {
            "sha": "59570545e5bd6c156c9e51c0f564aeabb3760517",
            "filename": "src/transformers/models/maskformer/image_processing_maskformer_fast.py",
            "status": "modified",
            "additions": 17,
            "deletions": 13,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/21913b2e100959467e9d97052575d33b45e06e0a/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21913b2e100959467e9d97052575d33b45e06e0a/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer_fast.py?ref=21913b2e100959467e9d97052575d33b45e06e0a",
            "patch": "@@ -114,9 +114,6 @@ class MaskFormerImageProcessorFast(BaseImageProcessorFast):\n     valid_kwargs = MaskFormerImageProcessorKwargs\n \n     def __init__(self, **kwargs: Unpack[MaskFormerImageProcessorKwargs]) -> None:\n-        if \"pad_and_return_pixel_mask\" in kwargs:\n-            kwargs[\"do_pad\"] = kwargs.pop(\"pad_and_return_pixel_mask\")\n-\n         size = kwargs.pop(\"size\", None)\n         max_size = kwargs.pop(\"max_size\", None)\n \n@@ -229,7 +226,7 @@ def pad(\n             padding = [0, 0, padding_right, padding_bottom]\n             images = F.pad(images, padding, fill=fill)\n             if segmentation_maps is not None:\n-                segmentation_maps = F.pad(segmentation_maps, padding, fill=ignore_index)\n+                segmentation_maps = [F.pad(mask, padding, fill=ignore_index) for mask in segmentation_maps]\n \n         # Make a pixel mask for the image, where 1 indicates a valid pixel and 0 indicates padding.\n         pixel_mask = torch.zeros((images.shape[0], *padded_size), dtype=torch.int64, device=images.device)\n@@ -323,9 +320,11 @@ def _preprocess(\n                 stacked_images = self.resize(\n                     image=stacked_images, size=size, size_divisor=size_divisor, interpolation=interpolation\n                 )\n-                if segmentation_maps is not None:\n+            if segmentation_maps is not None:\n+                stacked_segmentation_maps = grouped_segmentation_maps[shape]\n+                if do_resize:\n                     stacked_segmentation_maps = self.resize(\n-                        image=grouped_segmentation_maps[shape],\n+                        image=stacked_segmentation_maps,\n                         size=size,\n                         size_divisor=size_divisor,\n                         interpolation=F.InterpolationMode.NEAREST_EXACT,\n@@ -362,14 +361,18 @@ def _preprocess(\n                 mask_labels.append(masks)\n                 class_labels.append(classes)\n \n-        grouped_images, grouped_images_index = group_images_by_shape(resized_images, disable_grouping=disable_grouping)\n-        processed_images_grouped = {}\n-        processed_pixel_masks_grouped = {}\n         if segmentation_maps is not None:\n-            grouped_segmentation_maps, grouped_segmentation_maps_index = group_images_by_shape(\n-                mask_labels, disable_grouping=disable_grouping\n+            # group mask_labels as paired inputs and not images so as not to stack them\n+            grouped_images, grouped_segmentation_maps, grouped_images_index = group_images_by_shape(\n+                resized_images, mask_labels, disable_grouping=disable_grouping\n             )\n             processed_segmentation_maps_grouped = {}\n+        else:\n+            grouped_images, grouped_images_index = group_images_by_shape(\n+                resized_images, disable_grouping=disable_grouping\n+            )\n+        processed_images_grouped = {}\n+        processed_pixel_masks_grouped = {}\n         for shape, stacked_images in grouped_images.items():\n             # Fused rescale and normalize\n             stacked_images = self.rescale_and_normalize(\n@@ -384,7 +387,8 @@ def _preprocess(\n             processed_images_grouped[shape] = padded_images\n             processed_pixel_masks_grouped[shape] = pixel_masks\n             if segmentation_maps is not None:\n-                processed_segmentation_maps_grouped[shape] = padded_segmentation_maps.squeeze(1)\n+                processed_segmentation_maps_grouped[shape] = padded_segmentation_maps\n+\n         processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n         processed_pixel_masks = reorder_images(processed_pixel_masks_grouped, grouped_images_index)\n         encoded_inputs = BatchFeature(\n@@ -395,7 +399,7 @@ def _preprocess(\n             tensor_type=return_tensors,\n         )\n         if segmentation_maps is not None:\n-            mask_labels = reorder_images(processed_segmentation_maps_grouped, grouped_segmentation_maps_index)\n+            mask_labels = reorder_images(processed_segmentation_maps_grouped, grouped_images_index)\n             # we cannot batch them since they don't share a common class size\n             encoded_inputs[\"mask_labels\"] = mask_labels\n             encoded_inputs[\"class_labels\"] = class_labels"
        },
        {
            "sha": "6ac66de63125030a50be16af6899d4f095b88261",
            "filename": "tests/models/mask2former/test_image_processing_mask2former.py",
            "status": "modified",
            "additions": 92,
            "deletions": 88,
            "changes": 180,
            "blob_url": "https://github.com/huggingface/transformers/blob/21913b2e100959467e9d97052575d33b45e06e0a/tests%2Fmodels%2Fmask2former%2Ftest_image_processing_mask2former.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21913b2e100959467e9d97052575d33b45e06e0a/tests%2Fmodels%2Fmask2former%2Ftest_image_processing_mask2former.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmask2former%2Ftest_image_processing_mask2former.py?ref=21913b2e100959467e9d97052575d33b45e06e0a",
            "patch": "@@ -194,13 +194,14 @@ def test_image_processor_properties(self):\n     def comm_get_image_processing_inputs(\n         self,\n         image_processor_tester,\n+        image_processing_class,\n         with_segmentation_maps=False,\n         is_instance_map=False,\n         segmentation_type=\"np\",\n         numpify=False,\n         input_data_format=None,\n     ):\n-        image_processing = self.image_processing_class(**image_processor_tester.prepare_image_processor_dict())\n+        image_processing = image_processing_class(**image_processor_tester.prepare_image_processor_dict())\n         # prepare image and target\n         num_labels = image_processor_tester.num_labels\n         annotations = None\n@@ -228,7 +229,6 @@ def comm_get_image_processing_inputs(\n             annotations,\n             return_tensors=\"pt\",\n             instance_id_to_semantic_id=instance_id_to_semantic_id,\n-            pad_and_return_pixel_mask=True,\n             input_data_format=input_data_format,\n         )\n \n@@ -264,25 +264,26 @@ def common(\n                 image_mean=[0.5] * num_channels,\n                 image_std=[0.5] * num_channels,\n             )\n+            for image_processing_class in self.image_processor_list:\n+                inputs = self.comm_get_image_processing_inputs(\n+                    image_processor_tester=image_processor_tester,\n+                    image_processing_class=image_processing_class,\n+                    with_segmentation_maps=True,\n+                    is_instance_map=is_instance_map,\n+                    segmentation_type=segmentation_type,\n+                    numpify=numpify,\n+                    input_data_format=input_data_format,\n+                )\n \n-            inputs = self.comm_get_image_processing_inputs(\n-                image_processor_tester=image_processor_tester,\n-                with_segmentation_maps=True,\n-                is_instance_map=is_instance_map,\n-                segmentation_type=segmentation_type,\n-                numpify=numpify,\n-                input_data_format=input_data_format,\n-            )\n-\n-            mask_labels = inputs[\"mask_labels\"]\n-            class_labels = inputs[\"class_labels\"]\n-            pixel_values = inputs[\"pixel_values\"]\n+                mask_labels = inputs[\"mask_labels\"]\n+                class_labels = inputs[\"class_labels\"]\n+                pixel_values = inputs[\"pixel_values\"]\n \n-            # check the batch_size\n-            for mask_label, class_label in zip(mask_labels, class_labels):\n-                self.assertEqual(mask_label.shape[0], class_label.shape[0])\n-                # this ensure padding has happened\n-                self.assertEqual(mask_label.shape[1:], pixel_values.shape[2:])\n+                # check the batch_size\n+                for mask_label, class_label in zip(mask_labels, class_labels):\n+                    self.assertEqual(mask_label.shape[0], class_label.shape[0])\n+                    # this ensure padding has happened\n+                    self.assertEqual(mask_label.shape[1:], pixel_values.shape[2:])\n \n         common()\n         common(is_instance_map=True)\n@@ -335,31 +336,32 @@ def get_instance_segmentation_and_mapping(annotation):\n         instance_seg2, inst2class2 = get_instance_segmentation_and_mapping(annotation2)\n \n         # create a image processor\n-        image_processing = Mask2FormerImageProcessor(do_reduce_labels=True, ignore_index=255, size=(512, 512))\n-\n-        # prepare the images and annotations\n-        inputs = image_processing(\n-            [image1, image2],\n-            [instance_seg1, instance_seg2],\n-            instance_id_to_semantic_id=[inst2class1, inst2class2],\n-            return_tensors=\"pt\",\n-        )\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(do_reduce_labels=True, ignore_index=255, size=(512, 512))\n+\n+            # prepare the images and annotations\n+            inputs = image_processing(\n+                [image1, image2],\n+                [instance_seg1, instance_seg2],\n+                instance_id_to_semantic_id=[inst2class1, inst2class2],\n+                return_tensors=\"pt\",\n+            )\n \n-        # verify the pixel values and pixel mask\n-        self.assertEqual(inputs[\"pixel_values\"].shape, (2, 3, 512, 512))\n-        self.assertEqual(inputs[\"pixel_mask\"].shape, (2, 512, 512))\n+            # verify the pixel values and pixel mask\n+            self.assertEqual(inputs[\"pixel_values\"].shape, (2, 3, 512, 512))\n+            self.assertEqual(inputs[\"pixel_mask\"].shape, (2, 512, 512))\n \n-        # verify the class labels\n-        self.assertEqual(len(inputs[\"class_labels\"]), 2)\n-        torch.testing.assert_close(inputs[\"class_labels\"][0], torch.tensor([30, 55]))\n-        torch.testing.assert_close(inputs[\"class_labels\"][1], torch.tensor([4, 4, 23, 55]))\n+            # verify the class labels\n+            self.assertEqual(len(inputs[\"class_labels\"]), 2)\n+            torch.testing.assert_close(inputs[\"class_labels\"][0], torch.tensor([30, 55]))\n+            torch.testing.assert_close(inputs[\"class_labels\"][1], torch.tensor([4, 4, 23, 55]))\n \n-        # verify the mask labels\n-        self.assertEqual(len(inputs[\"mask_labels\"]), 2)\n-        self.assertEqual(inputs[\"mask_labels\"][0].shape, (2, 512, 512))\n-        self.assertEqual(inputs[\"mask_labels\"][1].shape, (4, 512, 512))\n-        self.assertEqual(inputs[\"mask_labels\"][0].sum().item(), 41527.0)\n-        self.assertEqual(inputs[\"mask_labels\"][1].sum().item(), 26259.0)\n+            # verify the mask labels\n+            self.assertEqual(len(inputs[\"mask_labels\"]), 2)\n+            self.assertEqual(inputs[\"mask_labels\"][0].shape, (2, 512, 512))\n+            self.assertEqual(inputs[\"mask_labels\"][1].shape, (4, 512, 512))\n+            self.assertEqual(inputs[\"mask_labels\"][0].sum().item(), 41527.0)\n+            self.assertEqual(inputs[\"mask_labels\"][1].sum().item(), 26259.0)\n \n     def test_integration_semantic_segmentation(self):\n         # load 2 images and corresponding semantic annotations from the hub\n@@ -378,30 +380,31 @@ def test_integration_semantic_segmentation(self):\n         )\n \n         # create a image processor\n-        image_processing = Mask2FormerImageProcessor(do_reduce_labels=True, ignore_index=255, size=(512, 512))\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(do_reduce_labels=True, ignore_index=255, size=(512, 512))\n \n-        # prepare the images and annotations\n-        inputs = image_processing(\n-            [image1, image2],\n-            [annotation1, annotation2],\n-            return_tensors=\"pt\",\n-        )\n+            # prepare the images and annotations\n+            inputs = image_processing(\n+                [image1, image2],\n+                [annotation1, annotation2],\n+                return_tensors=\"pt\",\n+            )\n \n-        # verify the pixel values and pixel mask\n-        self.assertEqual(inputs[\"pixel_values\"].shape, (2, 3, 512, 512))\n-        self.assertEqual(inputs[\"pixel_mask\"].shape, (2, 512, 512))\n+            # verify the pixel values and pixel mask\n+            self.assertEqual(inputs[\"pixel_values\"].shape, (2, 3, 512, 512))\n+            self.assertEqual(inputs[\"pixel_mask\"].shape, (2, 512, 512))\n \n-        # verify the class labels\n-        self.assertEqual(len(inputs[\"class_labels\"]), 2)\n-        torch.testing.assert_close(inputs[\"class_labels\"][0], torch.tensor([2, 4, 60]))\n-        torch.testing.assert_close(inputs[\"class_labels\"][1], torch.tensor([0, 3, 7, 8, 15, 28, 30, 143]))\n+            # verify the class labels\n+            self.assertEqual(len(inputs[\"class_labels\"]), 2)\n+            torch.testing.assert_close(inputs[\"class_labels\"][0], torch.tensor([2, 4, 60]))\n+            torch.testing.assert_close(inputs[\"class_labels\"][1], torch.tensor([0, 3, 7, 8, 15, 28, 30, 143]))\n \n-        # verify the mask labels\n-        self.assertEqual(len(inputs[\"mask_labels\"]), 2)\n-        self.assertEqual(inputs[\"mask_labels\"][0].shape, (3, 512, 512))\n-        self.assertEqual(inputs[\"mask_labels\"][1].shape, (8, 512, 512))\n-        self.assertEqual(inputs[\"mask_labels\"][0].sum().item(), 170200.0)\n-        self.assertEqual(inputs[\"mask_labels\"][1].sum().item(), 257036.0)\n+            # verify the mask labels\n+            self.assertEqual(len(inputs[\"mask_labels\"]), 2)\n+            self.assertEqual(inputs[\"mask_labels\"][0].shape, (3, 512, 512))\n+            self.assertEqual(inputs[\"mask_labels\"][1].shape, (8, 512, 512))\n+            self.assertEqual(inputs[\"mask_labels\"][0].sum().item(), 170200.0)\n+            self.assertEqual(inputs[\"mask_labels\"][1].sum().item(), 257036.0)\n \n     def test_integration_panoptic_segmentation(self):\n         # load 2 images and corresponding panoptic annotations from the hub\n@@ -435,34 +438,35 @@ def create_panoptic_map(annotation, segments_info):\n         panoptic_map2, inst2class2 = create_panoptic_map(annotation2, segments_info2)\n \n         # create a image processor\n-        image_processing = Mask2FormerImageProcessor(ignore_index=0, do_resize=False)\n-\n-        # prepare the images and annotations\n-        pixel_values_list = [np.moveaxis(np.array(image1), -1, 0), np.moveaxis(np.array(image2), -1, 0)]\n-        inputs = image_processing.encode_inputs(\n-            pixel_values_list,\n-            [panoptic_map1, panoptic_map2],\n-            instance_id_to_semantic_id=[inst2class1, inst2class2],\n-            return_tensors=\"pt\",\n-        )\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(ignore_index=0, do_resize=False)\n+\n+            # prepare the images and annotations\n+            pixel_values_list = [np.moveaxis(np.array(image1), -1, 0), np.moveaxis(np.array(image2), -1, 0)]\n+            inputs = image_processing(\n+                pixel_values_list,\n+                [panoptic_map1, panoptic_map2],\n+                instance_id_to_semantic_id=[inst2class1, inst2class2],\n+                return_tensors=\"pt\",\n+            )\n \n-        # verify the pixel values and pixel mask\n-        self.assertEqual(inputs[\"pixel_values\"].shape, (2, 3, 512, 711))\n-        self.assertEqual(inputs[\"pixel_mask\"].shape, (2, 512, 711))\n-\n-        # verify the class labels\n-        self.assertEqual(len(inputs[\"class_labels\"]), 2)\n-        expected_class_labels = torch.tensor([4, 17, 32, 42, 42, 42, 42, 42, 42, 42, 32, 12, 12, 12, 12, 12, 42, 42, 12, 12, 12, 42, 12, 12, 12, 12, 12, 3, 12, 12, 12, 12, 42, 42, 42, 12, 42, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 5, 12, 12, 12, 12, 12, 12, 12, 0, 43, 43, 43, 96, 43, 104, 43, 31, 125, 31, 125, 138, 87, 125, 149, 138, 125, 87, 87])  # fmt: skip\n-        torch.testing.assert_close(inputs[\"class_labels\"][0], torch.tensor(expected_class_labels))\n-        expected_class_labels = torch.tensor([19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 67, 82, 19, 19, 17, 19, 19, 19, 19, 19, 19, 19, 19, 19, 12, 12, 42, 12, 12, 12, 12, 3, 14, 12, 12, 12, 12, 12, 12, 12, 12, 14, 5, 12, 12, 0, 115, 43, 43, 115, 43, 43, 43, 8, 8, 8, 138, 138, 125, 143])  # fmt: skip\n-        torch.testing.assert_close(inputs[\"class_labels\"][1], expected_class_labels)\n-\n-        # verify the mask labels\n-        self.assertEqual(len(inputs[\"mask_labels\"]), 2)\n-        self.assertEqual(inputs[\"mask_labels\"][0].shape, (79, 512, 711))\n-        self.assertEqual(inputs[\"mask_labels\"][1].shape, (61, 512, 711))\n-        self.assertEqual(inputs[\"mask_labels\"][0].sum().item(), 315193.0)\n-        self.assertEqual(inputs[\"mask_labels\"][1].sum().item(), 350747.0)\n+            # verify the pixel values and pixel mask\n+            self.assertEqual(inputs[\"pixel_values\"].shape, (2, 3, 512, 711))\n+            self.assertEqual(inputs[\"pixel_mask\"].shape, (2, 512, 711))\n+\n+            # verify the class labels\n+            self.assertEqual(len(inputs[\"class_labels\"]), 2)\n+            expected_class_labels = torch.tensor([4, 17, 32, 42, 42, 42, 42, 42, 42, 42, 32, 12, 12, 12, 12, 12, 42, 42, 12, 12, 12, 42, 12, 12, 12, 12, 12, 3, 12, 12, 12, 12, 42, 42, 42, 12, 42, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 5, 12, 12, 12, 12, 12, 12, 12, 0, 43, 43, 43, 96, 43, 104, 43, 31, 125, 31, 125, 138, 87, 125, 149, 138, 125, 87, 87])  # fmt: skip\n+            torch.testing.assert_close(inputs[\"class_labels\"][0], torch.tensor(expected_class_labels))\n+            expected_class_labels = torch.tensor([19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 67, 82, 19, 19, 17, 19, 19, 19, 19, 19, 19, 19, 19, 19, 12, 12, 42, 12, 12, 12, 12, 3, 14, 12, 12, 12, 12, 12, 12, 12, 12, 14, 5, 12, 12, 0, 115, 43, 43, 115, 43, 43, 43, 8, 8, 8, 138, 138, 125, 143])  # fmt: skip\n+            torch.testing.assert_close(inputs[\"class_labels\"][1], expected_class_labels)\n+\n+            # verify the mask labels\n+            self.assertEqual(len(inputs[\"mask_labels\"]), 2)\n+            self.assertEqual(inputs[\"mask_labels\"][0].shape, (79, 512, 711))\n+            self.assertEqual(inputs[\"mask_labels\"][1].shape, (61, 512, 711))\n+            self.assertEqual(inputs[\"mask_labels\"][0].sum().item(), 315193.0)\n+            self.assertEqual(inputs[\"mask_labels\"][1].sum().item(), 350747.0)\n \n     def test_binary_mask_to_rle(self):\n         fake_binary_mask = np.zeros((20, 50))"
        },
        {
            "sha": "bd5c8abec5bcb26a100bbdf8c906f9b70412d568",
            "filename": "tests/models/maskformer/test_image_processing_maskformer.py",
            "status": "modified",
            "additions": 89,
            "deletions": 83,
            "changes": 172,
            "blob_url": "https://github.com/huggingface/transformers/blob/21913b2e100959467e9d97052575d33b45e06e0a/tests%2Fmodels%2Fmaskformer%2Ftest_image_processing_maskformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21913b2e100959467e9d97052575d33b45e06e0a/tests%2Fmodels%2Fmaskformer%2Ftest_image_processing_maskformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmaskformer%2Ftest_image_processing_maskformer.py?ref=21913b2e100959467e9d97052575d33b45e06e0a",
            "patch": "@@ -188,9 +188,9 @@ def test_image_processor_properties(self):\n             self.assertTrue(hasattr(image_processing, \"num_labels\"))\n \n     def comm_get_image_processing_inputs(\n-        self, with_segmentation_maps=False, is_instance_map=False, segmentation_type=\"np\"\n+        self, image_processing_class, with_segmentation_maps=False, is_instance_map=False, segmentation_type=\"np\"\n     ):\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n+        image_processing = image_processing_class(**self.image_processor_dict)\n         # prepare image and target\n         num_labels = self.image_processor_tester.num_labels\n         annotations = None\n@@ -212,7 +212,6 @@ def comm_get_image_processing_inputs(\n             annotations,\n             return_tensors=\"pt\",\n             instance_id_to_semantic_id=instance_id_to_semantic_id,\n-            pad_and_return_pixel_mask=True,\n         )\n \n         return inputs\n@@ -233,19 +232,23 @@ def test_with_size_divisor(self):\n \n     def test_call_with_segmentation_maps(self):\n         def common(is_instance_map=False, segmentation_type=None):\n-            inputs = self.comm_get_image_processing_inputs(\n-                with_segmentation_maps=True, is_instance_map=is_instance_map, segmentation_type=segmentation_type\n-            )\n+            for image_processing_class in self.image_processor_list:\n+                inputs = self.comm_get_image_processing_inputs(\n+                    image_processing_class=image_processing_class,\n+                    with_segmentation_maps=True,\n+                    is_instance_map=is_instance_map,\n+                    segmentation_type=segmentation_type,\n+                )\n \n-            mask_labels = inputs[\"mask_labels\"]\n-            class_labels = inputs[\"class_labels\"]\n-            pixel_values = inputs[\"pixel_values\"]\n+                mask_labels = inputs[\"mask_labels\"]\n+                class_labels = inputs[\"class_labels\"]\n+                pixel_values = inputs[\"pixel_values\"]\n \n-            # check the batch_size\n-            for mask_label, class_label in zip(mask_labels, class_labels):\n-                self.assertEqual(mask_label.shape[0], class_label.shape[0])\n-                # this ensure padding has happened\n-                self.assertEqual(mask_label.shape[1:], pixel_values.shape[2:])\n+                # check the batch_size\n+                for mask_label, class_label in zip(mask_labels, class_labels):\n+                    self.assertEqual(mask_label.shape[0], class_label.shape[0])\n+                    # this ensure padding has happened\n+                    self.assertEqual(mask_label.shape[1:], pixel_values.shape[2:])\n \n         common()\n         common(is_instance_map=True)\n@@ -286,31 +289,32 @@ def get_instance_segmentation_and_mapping(annotation):\n         instance_seg2, inst2class2 = get_instance_segmentation_and_mapping(annotation2)\n \n         # create a image processor\n-        image_processing = MaskFormerImageProcessor(do_reduce_labels=True, ignore_index=255, size=(512, 512))\n-\n-        # prepare the images and annotations\n-        inputs = image_processing(\n-            [image1, image2],\n-            [instance_seg1, instance_seg2],\n-            instance_id_to_semantic_id=[inst2class1, inst2class2],\n-            return_tensors=\"pt\",\n-        )\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(do_reduce_labels=True, ignore_index=255, size=(512, 512))\n+\n+            # prepare the images and annotations\n+            inputs = image_processing(\n+                [image1, image2],\n+                [instance_seg1, instance_seg2],\n+                instance_id_to_semantic_id=[inst2class1, inst2class2],\n+                return_tensors=\"pt\",\n+            )\n \n-        # verify the pixel values and pixel mask\n-        self.assertEqual(inputs[\"pixel_values\"].shape, (2, 3, 512, 512))\n-        self.assertEqual(inputs[\"pixel_mask\"].shape, (2, 512, 512))\n+            # verify the pixel values and pixel mask\n+            self.assertEqual(inputs[\"pixel_values\"].shape, (2, 3, 512, 512))\n+            self.assertEqual(inputs[\"pixel_mask\"].shape, (2, 512, 512))\n \n-        # verify the class labels\n-        self.assertEqual(len(inputs[\"class_labels\"]), 2)\n-        torch.testing.assert_close(inputs[\"class_labels\"][0], torch.tensor([30, 55]))\n-        torch.testing.assert_close(inputs[\"class_labels\"][1], torch.tensor([4, 4, 23, 55]))\n+            # verify the class labels\n+            self.assertEqual(len(inputs[\"class_labels\"]), 2)\n+            torch.testing.assert_close(inputs[\"class_labels\"][0], torch.tensor([30, 55]))\n+            torch.testing.assert_close(inputs[\"class_labels\"][1], torch.tensor([4, 4, 23, 55]))\n \n-        # verify the mask labels\n-        self.assertEqual(len(inputs[\"mask_labels\"]), 2)\n-        self.assertEqual(inputs[\"mask_labels\"][0].shape, (2, 512, 512))\n-        self.assertEqual(inputs[\"mask_labels\"][1].shape, (4, 512, 512))\n-        self.assertEqual(inputs[\"mask_labels\"][0].sum().item(), 41527.0)\n-        self.assertEqual(inputs[\"mask_labels\"][1].sum().item(), 26259.0)\n+            # verify the mask labels\n+            self.assertEqual(len(inputs[\"mask_labels\"]), 2)\n+            self.assertEqual(inputs[\"mask_labels\"][0].shape, (2, 512, 512))\n+            self.assertEqual(inputs[\"mask_labels\"][1].shape, (4, 512, 512))\n+            self.assertEqual(inputs[\"mask_labels\"][0].sum().item(), 41527.0)\n+            self.assertEqual(inputs[\"mask_labels\"][1].sum().item(), 26259.0)\n \n     def test_integration_semantic_segmentation(self):\n         # load 2 images and corresponding semantic annotations from the hub\n@@ -329,30 +333,31 @@ def test_integration_semantic_segmentation(self):\n         )\n \n         # create a image processor\n-        image_processing = MaskFormerImageProcessor(do_reduce_labels=True, ignore_index=255, size=(512, 512))\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(do_reduce_labels=True, ignore_index=255, size=(512, 512))\n \n-        # prepare the images and annotations\n-        inputs = image_processing(\n-            [image1, image2],\n-            [annotation1, annotation2],\n-            return_tensors=\"pt\",\n-        )\n+            # prepare the images and annotations\n+            inputs = image_processing(\n+                [image1, image2],\n+                [annotation1, annotation2],\n+                return_tensors=\"pt\",\n+            )\n \n-        # verify the pixel values and pixel mask\n-        self.assertEqual(inputs[\"pixel_values\"].shape, (2, 3, 512, 512))\n-        self.assertEqual(inputs[\"pixel_mask\"].shape, (2, 512, 512))\n+            # verify the pixel values and pixel mask\n+            self.assertEqual(inputs[\"pixel_values\"].shape, (2, 3, 512, 512))\n+            self.assertEqual(inputs[\"pixel_mask\"].shape, (2, 512, 512))\n \n-        # verify the class labels\n-        self.assertEqual(len(inputs[\"class_labels\"]), 2)\n-        torch.testing.assert_close(inputs[\"class_labels\"][0], torch.tensor([2, 4, 60]))\n-        torch.testing.assert_close(inputs[\"class_labels\"][1], torch.tensor([0, 3, 7, 8, 15, 28, 30, 143]))\n+            # verify the class labels\n+            self.assertEqual(len(inputs[\"class_labels\"]), 2)\n+            torch.testing.assert_close(inputs[\"class_labels\"][0], torch.tensor([2, 4, 60]))\n+            torch.testing.assert_close(inputs[\"class_labels\"][1], torch.tensor([0, 3, 7, 8, 15, 28, 30, 143]))\n \n-        # verify the mask labels\n-        self.assertEqual(len(inputs[\"mask_labels\"]), 2)\n-        self.assertEqual(inputs[\"mask_labels\"][0].shape, (3, 512, 512))\n-        self.assertEqual(inputs[\"mask_labels\"][1].shape, (8, 512, 512))\n-        self.assertEqual(inputs[\"mask_labels\"][0].sum().item(), 170200.0)\n-        self.assertEqual(inputs[\"mask_labels\"][1].sum().item(), 257036.0)\n+            # verify the mask labels\n+            self.assertEqual(len(inputs[\"mask_labels\"]), 2)\n+            self.assertEqual(inputs[\"mask_labels\"][0].shape, (3, 512, 512))\n+            self.assertEqual(inputs[\"mask_labels\"][1].shape, (8, 512, 512))\n+            self.assertEqual(inputs[\"mask_labels\"][0].sum().item(), 170200.0)\n+            self.assertEqual(inputs[\"mask_labels\"][1].sum().item(), 257036.0)\n \n     def test_integration_panoptic_segmentation(self):\n         # load 2 images and corresponding panoptic annotations from the hub\n@@ -386,34 +391,35 @@ def create_panoptic_map(annotation, segments_info):\n         panoptic_map2, inst2class2 = create_panoptic_map(annotation2, segments_info2)\n \n         # create a image processor\n-        image_processing = MaskFormerImageProcessor(ignore_index=0, do_resize=False)\n-\n-        # prepare the images and annotations\n-        pixel_values_list = [np.moveaxis(np.array(image1), -1, 0), np.moveaxis(np.array(image2), -1, 0)]\n-        inputs = image_processing.encode_inputs(\n-            pixel_values_list,\n-            [panoptic_map1, panoptic_map2],\n-            instance_id_to_semantic_id=[inst2class1, inst2class2],\n-            return_tensors=\"pt\",\n-        )\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(ignore_index=0, do_resize=False)\n+\n+            # prepare the images and annotations\n+            pixel_values_list = [np.moveaxis(np.array(image1), -1, 0), np.moveaxis(np.array(image2), -1, 0)]\n+            inputs = image_processing(\n+                pixel_values_list,\n+                [panoptic_map1, panoptic_map2],\n+                instance_id_to_semantic_id=[inst2class1, inst2class2],\n+                return_tensors=\"pt\",\n+            )\n \n-        # verify the pixel values and pixel mask\n-        self.assertEqual(inputs[\"pixel_values\"].shape, (2, 3, 512, 711))\n-        self.assertEqual(inputs[\"pixel_mask\"].shape, (2, 512, 711))\n-\n-        # verify the class labels\n-        self.assertEqual(len(inputs[\"class_labels\"]), 2)\n-        expected_class_labels = torch.tensor([4, 17, 32, 42, 42, 42, 42, 42, 42, 42, 32, 12, 12, 12, 12, 12, 42, 42, 12, 12, 12, 42, 12, 12, 12, 12, 12, 3, 12, 12, 12, 12, 42, 42, 42, 12, 42, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 5, 12, 12, 12, 12, 12, 12, 12, 0, 43, 43, 43, 96, 43, 104, 43, 31, 125, 31, 125, 138, 87, 125, 149, 138, 125, 87, 87])  # fmt: skip\n-        torch.testing.assert_close(inputs[\"class_labels\"][0], torch.tensor(expected_class_labels))\n-        expected_class_labels = torch.tensor([19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 67, 82, 19, 19, 17, 19, 19, 19, 19, 19, 19, 19, 19, 19, 12, 12, 42, 12, 12, 12, 12, 3, 14, 12, 12, 12, 12, 12, 12, 12, 12, 14, 5, 12, 12, 0, 115, 43, 43, 115, 43, 43, 43, 8, 8, 8, 138, 138, 125, 143])  # fmt: skip\n-        torch.testing.assert_close(inputs[\"class_labels\"][1], expected_class_labels)\n-\n-        # verify the mask labels\n-        self.assertEqual(len(inputs[\"mask_labels\"]), 2)\n-        self.assertEqual(inputs[\"mask_labels\"][0].shape, (79, 512, 711))\n-        self.assertEqual(inputs[\"mask_labels\"][1].shape, (61, 512, 711))\n-        self.assertEqual(inputs[\"mask_labels\"][0].sum().item(), 315193.0)\n-        self.assertEqual(inputs[\"mask_labels\"][1].sum().item(), 350747.0)\n+            # verify the pixel values and pixel mask\n+            self.assertEqual(inputs[\"pixel_values\"].shape, (2, 3, 512, 711))\n+            self.assertEqual(inputs[\"pixel_mask\"].shape, (2, 512, 711))\n+\n+            # verify the class labels\n+            self.assertEqual(len(inputs[\"class_labels\"]), 2)\n+            expected_class_labels = torch.tensor([4, 17, 32, 42, 42, 42, 42, 42, 42, 42, 32, 12, 12, 12, 12, 12, 42, 42, 12, 12, 12, 42, 12, 12, 12, 12, 12, 3, 12, 12, 12, 12, 42, 42, 42, 12, 42, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 5, 12, 12, 12, 12, 12, 12, 12, 0, 43, 43, 43, 96, 43, 104, 43, 31, 125, 31, 125, 138, 87, 125, 149, 138, 125, 87, 87])  # fmt: skip\n+            torch.testing.assert_close(inputs[\"class_labels\"][0], torch.tensor(expected_class_labels))\n+            expected_class_labels = torch.tensor([19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 67, 82, 19, 19, 17, 19, 19, 19, 19, 19, 19, 19, 19, 19, 12, 12, 42, 12, 12, 12, 12, 3, 14, 12, 12, 12, 12, 12, 12, 12, 12, 14, 5, 12, 12, 0, 115, 43, 43, 115, 43, 43, 43, 8, 8, 8, 138, 138, 125, 143])  # fmt: skip\n+            torch.testing.assert_close(inputs[\"class_labels\"][1], expected_class_labels)\n+\n+            # verify the mask labels\n+            self.assertEqual(len(inputs[\"mask_labels\"]), 2)\n+            self.assertEqual(inputs[\"mask_labels\"][0].shape, (79, 512, 711))\n+            self.assertEqual(inputs[\"mask_labels\"][1].shape, (61, 512, 711))\n+            self.assertEqual(inputs[\"mask_labels\"][0].sum().item(), 315193.0)\n+            self.assertEqual(inputs[\"mask_labels\"][1].sum().item(), 350747.0)\n \n     def test_binary_mask_to_rle(self):\n         fake_binary_mask = np.zeros((20, 50))"
        }
    ],
    "stats": {
        "total": 479,
        "additions": 256,
        "deletions": 223
    }
}