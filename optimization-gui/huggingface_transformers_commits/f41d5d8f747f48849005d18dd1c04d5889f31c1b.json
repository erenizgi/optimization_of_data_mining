{
    "author": "jla524",
    "message": "Add type hints for forward functions in Gemma2 (#35034)\n\n* feat: add gemma2 type hints\r\n\r\n* fix: mask is optional",
    "sha": "f41d5d8f747f48849005d18dd1c04d5889f31c1b",
    "files": [
        {
            "sha": "5dd4ffe0c8ac75d305b06c11da426ca0d81c2acc",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 34,
            "deletions": 4,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/f41d5d8f747f48849005d18dd1c04d5889f31c1b/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f41d5d8f747f48849005d18dd1c04d5889f31c1b/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=f41d5d8f747f48849005d18dd1c04d5889f31c1b",
            "patch": "@@ -170,7 +170,14 @@ def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n \n \n-def eager_attention_forward(config, query, key, value, mask, **_kwargs):\n+def eager_attention_forward(\n+    config: Gemma2Config,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    mask: Optional[torch.Tensor],\n+    **_kwargs,\n+) -> Tuple[torch.Tensor, torch.Tensor]:\n     key_states = repeat_kv(key, config.num_key_value_groups)\n     value_states = repeat_kv(value, config.num_key_value_groups)\n \n@@ -192,7 +199,15 @@ def eager_attention_forward(config, query, key, value, mask, **_kwargs):\n     return attn_output, attn_weights\n \n \n-def flash_attention_forward(config, query, key, value, mask, target_dtype=torch.float16, **_kwargs):\n+def flash_attention_forward(\n+    config: Gemma2Config,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    mask: Optional[torch.Tensor],\n+    target_dtype: torch.dtype = torch.float16,\n+    **_kwargs,\n+) -> Tuple[torch.Tensor, None]:\n     if mask is not None:\n         seq_len = mask.shape[1]\n         query = query[:, :, :seq_len]\n@@ -229,7 +244,15 @@ def flash_attention_forward(config, query, key, value, mask, target_dtype=torch.\n     return attn_output, None\n \n \n-def flex_attention_forward(config, query, key, value, mask, output_attentions=False, **_kwargs):\n+def flex_attention_forward(\n+    config: Gemma2Config,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    mask: Optional[torch.Tensor],\n+    output_attentions: bool = False,\n+    **_kwargs,\n+) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n     def tanh_softcap(score, b, h, q_idx, kv_idx):\n         soft_cap = config.attn_logit_softcapping\n         score = soft_cap * torch.tanh(score / soft_cap)\n@@ -255,7 +278,14 @@ def tanh_softcap(score, b, h, q_idx, kv_idx):\n     return attn_output, attn_weights\n \n \n-def sdpa_attention_forward(config, query, key, value, mask, **_kwargs):\n+def sdpa_attention_forward(\n+    config: Gemma2Config,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    mask: Optional[torch.Tensor],\n+    **_kwargs,\n+) -> Tuple[torch.Tensor, None]:\n     key = repeat_kv(key, config.num_key_value_groups)\n     value = repeat_kv(value, config.num_key_value_groups)\n "
        },
        {
            "sha": "7236ae2f5c9f87212725f7fae47a398c12cb7906",
            "filename": "src/transformers/models/gemma2/modular_gemma2.py",
            "status": "modified",
            "additions": 34,
            "deletions": 4,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/f41d5d8f747f48849005d18dd1c04d5889f31c1b/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f41d5d8f747f48849005d18dd1c04d5889f31c1b/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py?ref=f41d5d8f747f48849005d18dd1c04d5889f31c1b",
            "patch": "@@ -213,7 +213,14 @@ class Gemma2RotaryEmbedding(GemmaRotaryEmbedding):\n     pass\n \n \n-def eager_attention_forward(config, query, key, value, mask, **_kwargs):\n+def eager_attention_forward(\n+    config: Gemma2Config,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    mask: Optional[torch.Tensor],\n+    **_kwargs,\n+) -> Tuple[torch.Tensor, torch.Tensor]:\n     key_states = repeat_kv(key, config.num_key_value_groups)\n     value_states = repeat_kv(value, config.num_key_value_groups)\n \n@@ -235,7 +242,15 @@ def eager_attention_forward(config, query, key, value, mask, **_kwargs):\n     return attn_output, attn_weights\n \n \n-def flash_attention_forward(config, query, key, value, mask, target_dtype=torch.float16, **_kwargs):\n+def flash_attention_forward(\n+    config: Gemma2Config,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    mask: Optional[torch.Tensor],\n+    target_dtype: torch.dtype = torch.float16,\n+    **_kwargs,\n+) -> Tuple[torch.Tensor, None]:\n     if mask is not None:\n         seq_len = mask.shape[1]\n         query = query[:, :, :seq_len]\n@@ -272,7 +287,15 @@ def flash_attention_forward(config, query, key, value, mask, target_dtype=torch.\n     return attn_output, None\n \n \n-def flex_attention_forward(config, query, key, value, mask, output_attentions=False, **_kwargs):\n+def flex_attention_forward(\n+    config: Gemma2Config,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    mask: Optional[torch.Tensor],\n+    output_attentions: bool = False,\n+    **_kwargs,\n+) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n     def tanh_softcap(score, b, h, q_idx, kv_idx):\n         soft_cap = config.attn_logit_softcapping\n         score = soft_cap * torch.tanh(score / soft_cap)\n@@ -298,7 +321,14 @@ def tanh_softcap(score, b, h, q_idx, kv_idx):\n     return attn_output, attn_weights\n \n \n-def sdpa_attention_forward(config, query, key, value, mask, **_kwargs):\n+def sdpa_attention_forward(\n+    config: Gemma2Config,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    mask: Optional[torch.Tensor],\n+    **_kwargs,\n+) -> Tuple[torch.Tensor, None]:\n     key = repeat_kv(key, config.num_key_value_groups)\n     value = repeat_kv(value, config.num_key_value_groups)\n "
        }
    ],
    "stats": {
        "total": 76,
        "additions": 68,
        "deletions": 8
    }
}