{
    "author": "nvpohanh",
    "message": "Fix broken Llama4 accuracy in MoE part (#40609)\n\n* Fix broken Llama4 accuracy in MoE part\n\nLlama4 accuracy is broken by a bug in\nhttps://github.com/huggingface/transformers/pull/39501 . It forgot to\ntranspose the router_scores before applying it to routed_in, causing\nLlama4 to generate garbage output.\n\nThis PR fixes that issue by adding back the transpose() and adding some\ncomments explaining why the transpose() is needed.\n\nSigned-off-by: Po-Han Huang <pohanh@nvidia.com>\n\n* remove comment\n\n---------\n\nSigned-off-by: Po-Han Huang <pohanh@nvidia.com>\nCo-authored-by: Cyril Vallez <cyril.vallez@gmail.com>",
    "sha": "519c2524af3de7c2147a04aded83a19eba286894",
    "files": [
        {
            "sha": "059011629586a60d6b3aa0a701a6f0ea2a9c2a7b",
            "filename": "src/transformers/models/llama4/modeling_llama4.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/519c2524af3de7c2147a04aded83a19eba286894/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/519c2524af3de7c2147a04aded83a19eba286894/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py?ref=519c2524af3de7c2147a04aded83a19eba286894",
            "patch": "@@ -158,7 +158,7 @@ def forward(self, hidden_states):\n         hidden_states = hidden_states.reshape(-1, self.hidden_dim)\n         router_scores, router_logits = self.router(hidden_states)\n         routed_in = hidden_states.repeat(router_scores.shape[1], 1)\n-        routed_in = routed_in * router_scores.reshape(-1, 1)\n+        routed_in = routed_in * router_scores.transpose(0, 1).reshape(-1, 1)\n         routed_out = self.experts(routed_in)\n         out = self.shared_expert(hidden_states)\n         out.add_(routed_out.reshape(router_scores.shape[1], -1, routed_out.shape[-1]).sum(dim=0))"
        }
    ],
    "stats": {
        "total": 2,
        "additions": 1,
        "deletions": 1
    }
}