{
    "author": "SunMarc",
    "message": "Fix cuda index issue in cache allocator (#36937)\n\nfix",
    "sha": "80b4c5dcc9ec3dae38a80ff426dd60f5658ab4bc",
    "files": [
        {
            "sha": "8438865a0b0a91348c9ac1968758c1d35189a8f4",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/80b4c5dcc9ec3dae38a80ff426dd60f5658ab4bc/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/80b4c5dcc9ec3dae38a80ff426dd60f5658ab4bc/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=80b4c5dcc9ec3dae38a80ff426dd60f5658ab4bc",
            "patch": "@@ -5870,7 +5870,8 @@ def caching_allocator_warmup(model: PreTrainedModel, expanded_device_map: Dict):\n     # This will kick off the caching allocator to avoid having to Malloc afterwards\n     for device, byte_count in total_byte_count.items():\n         if device.type == \"cuda\":\n-            device_memory = torch.cuda.mem_get_info(device)[0]\n+            index = device.index if device.index is not None else torch.cuda.current_device()\n+            device_memory = torch.cuda.mem_get_info(index)[0]\n             # Allow up to 95% of max device memory\n             byte_count = min(byte_count, int(0.95 * device_memory))\n         # Allocate memory"
        }
    ],
    "stats": {
        "total": 3,
        "additions": 2,
        "deletions": 1
    }
}