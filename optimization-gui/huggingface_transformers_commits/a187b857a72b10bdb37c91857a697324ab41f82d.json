{
    "author": "Cyrilvallez",
    "message": "Remove tied weights from internal attribute if they are not tied (#42871)\n\nfix",
    "sha": "a187b857a72b10bdb37c91857a697324ab41f82d",
    "files": [
        {
            "sha": "949a68eff5641297f473da6ce0493d0d4ac2c499",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a187b857a72b10bdb37c91857a697324ab41f82d/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a187b857a72b10bdb37c91857a697324ab41f82d/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=a187b857a72b10bdb37c91857a697324ab41f82d",
            "patch": "@@ -2398,13 +2398,15 @@ def tie_weights(self, missing_keys: Optional[set[str]] = None, recompute_mapping\n                 source_is_there = source_param_name not in missing_keys\n                 target_is_there = target_param_name not in missing_keys\n                 # Both are already present -> it means the config is wrong and do not reflect the actual\n-                # checkpoint -> let's raise a warning and do nothing\n+                # checkpoint -> let's raise a warning and NOT tie them\n                 if source_is_there and target_is_there:\n                     logger.warning(\n                         f\"The tied weights mapping and config for this model specifies to tie {source_param_name} to \"\n                         f\"{target_param_name}, but both are present in the checkpoints, so we will NOT tie them. \"\n                         \"You should update the config with `tie_word_embeddings=False` to silence this warning\"\n                     )\n+                    # Remove from internal attribute to correctly reflect actual tied weights\n+                    self.all_tied_weights_keys.pop(target_param_name)\n                     # Skip to next iteration\n                     continue\n                 # We're missing the source but we have the target -> we swap them, tying the parameter that exists"
        },
        {
            "sha": "73d24ed9ff70c48b2ecaa08c3b2c4058c3090ac3",
            "filename": "src/transformers/models/sam3_video/modeling_sam3_video.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a187b857a72b10bdb37c91857a697324ab41f82d/src%2Ftransformers%2Fmodels%2Fsam3_video%2Fmodeling_sam3_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a187b857a72b10bdb37c91857a697324ab41f82d/src%2Ftransformers%2Fmodels%2Fsam3_video%2Fmodeling_sam3_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam3_video%2Fmodeling_sam3_video.py?ref=a187b857a72b10bdb37c91857a697324ab41f82d",
            "patch": "@@ -505,8 +505,6 @@ class Sam3VideoPreTrainedModel(PreTrainedModel):\n \n @auto_docstring\n class Sam3VideoModel(Sam3VideoPreTrainedModel):\n-    all_tied_weights_keys = {}\n-\n     def __init__(self, config: Sam3VideoConfig):\n         super().__init__(config)\n         self.config = config"
        }
    ],
    "stats": {
        "total": 6,
        "additions": 3,
        "deletions": 3
    }
}