{
    "author": "cyyever",
    "message": "Remove deprecated training arguments (#36946)\n\n* Remove deprecated training arguments\n\n* More fixes\n\n* More fixes\n\n* More fixes",
    "sha": "2b550c47b208fc1a57feca46307349ab575cba2c",
    "files": [
        {
            "sha": "565e2ec71dd900ded276fe9ca3b56a89bb9c21bb",
            "filename": "examples/pytorch/instance-segmentation/README.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b550c47b208fc1a57feca46307349ab575cba2c/examples%2Fpytorch%2Finstance-segmentation%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b550c47b208fc1a57feca46307349ab575cba2c/examples%2Fpytorch%2Finstance-segmentation%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Finstance-segmentation%2FREADME.md?ref=2b550c47b208fc1a57feca46307349ab575cba2c",
            "patch": "@@ -66,7 +66,7 @@ python run_instance_segmentation.py \\\n     --dataloader_persistent_workers \\\n     --dataloader_prefetch_factor 4 \\\n     --do_eval \\\n-    --evaluation_strategy epoch \\\n+    --eval_strategy epoch \\\n     --logging_strategy epoch \\\n     --save_strategy epoch \\\n     --save_total_limit 2 \\"
        },
        {
            "sha": "3c0ce460f0d5e462fed6762eb4697a7415250f57",
            "filename": "examples/pytorch/object-detection/README.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b550c47b208fc1a57feca46307349ab575cba2c/examples%2Fpytorch%2Fobject-detection%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b550c47b208fc1a57feca46307349ab575cba2c/examples%2Fpytorch%2Fobject-detection%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fobject-detection%2FREADME.md?ref=2b550c47b208fc1a57feca46307349ab575cba2c",
            "patch": "@@ -56,7 +56,7 @@ python run_object_detection.py \\\n     --greater_is_better true \\\n     --load_best_model_at_end true \\\n     --logging_strategy epoch \\\n-    --evaluation_strategy epoch \\\n+    --eval_strategy epoch \\\n     --save_strategy epoch \\\n     --save_total_limit 2 \\\n     --push_to_hub true \\"
        },
        {
            "sha": "0df63ee946d9d6c1a1f4340ec89478f94a633155",
            "filename": "examples/pytorch/test_pytorch_examples.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b550c47b208fc1a57feca46307349ab575cba2c/examples%2Fpytorch%2Ftest_pytorch_examples.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b550c47b208fc1a57feca46307349ab575cba2c/examples%2Fpytorch%2Ftest_pytorch_examples.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Ftest_pytorch_examples.py?ref=2b550c47b208fc1a57feca46307349ab575cba2c",
            "patch": "@@ -667,7 +667,7 @@ def test_run_instance_segmentation(self):\n             --per_device_train_batch_size 2\n             --per_device_eval_batch_size 1\n             --do_eval\n-            --evaluation_strategy epoch\n+            --eval_strategy epoch\n             --seed 32\n         \"\"\".split()\n "
        },
        {
            "sha": "fa22fb3ce36cf813e248f1c2e83004a5d5f0bc99",
            "filename": "src/transformers/trainer_pt_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b550c47b208fc1a57feca46307349ab575cba2c/src%2Ftransformers%2Ftrainer_pt_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b550c47b208fc1a57feca46307349ab575cba2c/src%2Ftransformers%2Ftrainer_pt_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer_pt_utils.py?ref=2b550c47b208fc1a57feca46307349ab575cba2c",
            "patch": "@@ -1263,7 +1263,7 @@ class AcceleratorConfig:\n             \" in your script multiplied by the number of processes.\"\n         },\n     )\n-    dispatch_batches: bool = field(\n+    dispatch_batches: Optional[bool] = field(\n         default=None,\n         metadata={\n             \"help\": \"If set to `True`, the dataloader prepared by the Accelerator is only iterated through on the main process\""
        },
        {
            "sha": "75ebb357356d7105addb53c5eb9d663d69cc6bea",
            "filename": "src/transformers/training_args.py",
            "status": "modified",
            "additions": 1,
            "deletions": 46,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b550c47b208fc1a57feca46307349ab575cba2c/src%2Ftransformers%2Ftraining_args.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b550c47b208fc1a57feca46307349ab575cba2c/src%2Ftransformers%2Ftraining_args.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftraining_args.py?ref=2b550c47b208fc1a57feca46307349ab575cba2c",
            "patch": "@@ -768,14 +768,6 @@ class TrainingArguments:\n             Refer to the PyTorch doc for possible values and note that they may change across PyTorch versions.\n \n             This flag is experimental and subject to change in future releases.\n-        split_batches (`bool`, *optional*):\n-            Whether or not the accelerator should split the batches yielded by the dataloaders across the devices\n-            during distributed training. If\n-\n-            set to `True`, the actual batch size used will be the same on any kind of distributed processes, but it\n-            must be a\n-\n-            round multiple of the number of processes you are using (such as GPUs).\n         include_tokens_per_second (`bool`, *optional*):\n             Whether or not to compute the number of tokens per second per device for training speed metrics.\n \n@@ -1426,10 +1418,6 @@ class TrainingArguments:\n             \"choices\": [\"auto\", \"apex\", \"cpu_amp\"],\n         },\n     )\n-    evaluation_strategy: Union[IntervalStrategy, str] = field(\n-        default=None,\n-        metadata={\"help\": \"Deprecated. Use `eval_strategy` instead\"},\n-    )\n     push_to_hub_model_id: Optional[str] = field(\n         default=None, metadata={\"help\": \"The name of the repository to which push the `Trainer`.\"}\n     )\n@@ -1504,16 +1492,6 @@ class TrainingArguments:\n         },\n     )\n \n-    dispatch_batches: Optional[bool] = field(\n-        default=None,\n-        metadata={\"help\": \"Deprecated. Pass {'dispatch_batches':VALUE} to `accelerator_config`.\"},\n-    )\n-\n-    split_batches: Optional[bool] = field(\n-        default=None,\n-        metadata={\"help\": \"Deprecated. Pass {'split_batches':True} to `accelerator_config`.\"},\n-    )\n-\n     include_tokens_per_second: Optional[bool] = field(\n         default=False,\n         metadata={\"help\": \"If set to `True`, the speed metrics will include `tgs` (tokens per second per device).\"},\n@@ -1606,13 +1584,6 @@ def __post_init__(self):\n         if self.disable_tqdm is None:\n             self.disable_tqdm = logger.getEffectiveLevel() > logging.WARN\n \n-        if self.evaluation_strategy is not None:\n-            warnings.warn(\n-                \"`evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\",\n-                FutureWarning,\n-            )\n-            self.eval_strategy = self.evaluation_strategy\n-\n         if isinstance(self.eval_strategy, EvaluationStrategy):\n             warnings.warn(\n                 \"using `EvaluationStrategy` for `eval_strategy` is deprecated and will be removed in version 5\"\n@@ -1771,7 +1742,7 @@ def __post_init__(self):\n \n         # We need to setup the accelerator config here *before* the first call to `self.device`\n         if is_accelerate_available():\n-            if not isinstance(self.accelerator_config, (AcceleratorConfig)):\n+            if not isinstance(self.accelerator_config, AcceleratorConfig):\n                 if self.accelerator_config is None:\n                     self.accelerator_config = AcceleratorConfig()\n                 elif isinstance(self.accelerator_config, dict):\n@@ -1786,22 +1757,6 @@ def __post_init__(self):\n                 else:\n                     self.accelerator_config = AcceleratorConfig.from_json_file(self.accelerator_config)\n \n-            if self.dispatch_batches is not None:\n-                warnings.warn(\n-                    \"Using `--dispatch_batches` is deprecated and will be removed in version 4.41 of ðŸ¤— Transformers. Use\"\n-                    \" `--accelerator_config {'dispatch_batches':VALUE} instead\",\n-                    FutureWarning,\n-                )\n-                self.accelerator_config.dispatch_batches = self.dispatch_batches\n-\n-            if self.split_batches is not None:\n-                warnings.warn(\n-                    \"Using `--split_batches` is deprecated and will be removed in version 4.41 of ðŸ¤— Transformers. Use\"\n-                    \" `--accelerator_config {'split_batches':VALUE} instead\",\n-                    FutureWarning,\n-                )\n-                self.accelerator_config.split_batches = self.split_batches\n-\n         # Initialize device before we proceed\n         if self.framework == \"pt\" and is_torch_available():\n             self.device"
        },
        {
            "sha": "59854401ed3f5f9ef9a0a220f70631c1aabc72c2",
            "filename": "src/transformers/utils/quantization_config.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b550c47b208fc1a57feca46307349ab575cba2c/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b550c47b208fc1a57feca46307349ab575cba2c/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fquantization_config.py?ref=2b550c47b208fc1a57feca46307349ab575cba2c",
            "patch": "@@ -646,7 +646,7 @@ def __init__(\n         sym: bool = True,\n         true_sequential: bool = True,\n         checkpoint_format: str = \"gptq\",\n-        meta: Optional[Dict[str, any]] = None,\n+        meta: Optional[Dict[str, Any]] = None,\n         backend: Optional[str] = None,\n         use_cuda_fp16: bool = False,\n         model_seqlen: Optional[int] = None,"
        },
        {
            "sha": "7e157550589e3ab0491321fc8b0ba460c6ac8b60",
            "filename": "tests/trainer/test_trainer.py",
            "status": "modified",
            "additions": 6,
            "deletions": 56,
            "changes": 62,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b550c47b208fc1a57feca46307349ab575cba2c/tests%2Ftrainer%2Ftest_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b550c47b208fc1a57feca46307349ab575cba2c/tests%2Ftrainer%2Ftest_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer.py?ref=2b550c47b208fc1a57feca46307349ab575cba2c",
            "patch": "@@ -28,7 +28,7 @@\n from functools import partial\n from itertools import product\n from pathlib import Path\n-from typing import Any, Dict, List\n+from typing import Any\n from unittest.mock import Mock, patch\n \n import numpy as np\n@@ -2982,7 +2982,7 @@ def __init__(self):\n                 self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n                 self.tokenizer.add_tokens([\"<NEW_TOKEN1>\", \"<NEW_TOKEN2>\"])\n \n-            def __call__(self, features: List[Any], return_tensors=\"pt\") -> Dict[str, Any]:\n+            def __call__(self, features: list[Any], return_tensors=\"pt\") -> dict[str, Any]:\n                 return default_data_collator(features, return_tensors)\n \n         data_collator = FakeCollator()\n@@ -2999,7 +2999,7 @@ def test_load_best_model_with_save(self):\n         trainer = get_regression_trainer(\n             output_dir=tmp_dir,\n             save_steps=5,\n-            evaluation_strategy=\"steps\",\n+            eval_strategy=\"steps\",\n             eval_steps=5,\n             max_steps=9,\n         )\n@@ -3020,7 +3020,7 @@ def test_load_best_model_with_save(self):\n         trainer = get_regression_trainer(\n             output_dir=tmp_dir,\n             save_steps=5,\n-            evaluation_strategy=\"steps\",\n+            eval_strategy=\"steps\",\n             eval_steps=5,\n             load_best_model_at_end=True,\n             save_total_limit=2,\n@@ -4260,7 +4260,7 @@ def test_accelerator_config_from_dict(self):\n             model = RegressionPreTrainedModel(config)\n             eval_dataset = SampleIterableDataset()\n \n-            accelerator_config = {\n+            accelerator_config: dict[str, Any] = {\n                 \"split_batches\": True,\n                 \"dispatch_batches\": True,\n                 \"even_batches\": False,\n@@ -4370,56 +4370,6 @@ def test_accelerator_config_from_partial(self):\n             self.assertEqual(trainer.accelerator.even_batches, True)\n             self.assertEqual(trainer.accelerator.use_seedable_sampler, True)\n \n-    def test_accelerator_config_from_dict_with_deprecated_args(self):\n-        # Checks that accelerator kwargs can be passed through\n-        # and the accelerator is initialized respectively\n-        # and maintains the deprecated args if passed in\n-        with tempfile.TemporaryDirectory() as tmp_dir:\n-            config = RegressionModelConfig(a=1.5, b=2.5)\n-            model = RegressionPreTrainedModel(config)\n-            eval_dataset = SampleIterableDataset()\n-\n-            # Leaves all options as something *not* basic\n-            with self.assertWarns(FutureWarning) as cm:\n-                args = RegressionTrainingArguments(\n-                    output_dir=tmp_dir,\n-                    accelerator_config={\n-                        \"split_batches\": True,\n-                    },\n-                    dispatch_batches=False,\n-                )\n-                self.assertIn(\"dispatch_batches\", str(cm.warnings[0].message))\n-            trainer = Trainer(model=model, args=args, eval_dataset=eval_dataset)\n-            self.assertEqual(trainer.accelerator.dispatch_batches, False)\n-            self.assertEqual(trainer.accelerator.split_batches, True)\n-            with self.assertWarns(FutureWarning) as cm:\n-                args = RegressionTrainingArguments(\n-                    output_dir=tmp_dir,\n-                    accelerator_config={\n-                        \"even_batches\": False,\n-                    },\n-                    split_batches=True,\n-                )\n-                self.assertIn(\"split_batches\", str(cm.warnings[0].message))\n-            trainer = Trainer(model=model, args=args, eval_dataset=eval_dataset)\n-            self.assertEqual(trainer.accelerator.split_batches, True)\n-            self.assertEqual(trainer.accelerator.even_batches, False)\n-            self.assertEqual(trainer.accelerator.dispatch_batches, None)\n-\n-    def test_accelerator_config_only_deprecated_args(self):\n-        with tempfile.TemporaryDirectory() as tmp_dir:\n-            with self.assertWarns(FutureWarning) as cm:\n-                args = RegressionTrainingArguments(\n-                    output_dir=tmp_dir,\n-                    split_batches=True,\n-                )\n-                self.assertIn(\"split_batches\", str(cm.warnings[0].message))\n-                config = RegressionModelConfig(a=1.5, b=2.5)\n-                model = RegressionPreTrainedModel(config)\n-                eval_dataset = SampleIterableDataset()\n-                trainer = Trainer(model=model, args=args, eval_dataset=eval_dataset)\n-                self.assertEqual(trainer.accelerator.split_batches, True)\n-\n     def test_accelerator_custom_state(self):\n         AcceleratorState._reset_state(reset_partial_state=True)\n         with tempfile.TemporaryDirectory() as tmp_dir:\n@@ -5191,7 +5141,7 @@ def model_init(trial):\n         def hp_name(trial):\n             return MyTrialShortNamer.shortname(trial.params)\n \n-        def compute_objective(metrics: Dict[str, float]) -> List[float]:\n+        def compute_objective(metrics: dict[str, float]) -> list[float]:\n             return metrics[\"eval_loss\"], metrics[\"eval_accuracy\"]\n \n         with tempfile.TemporaryDirectory() as tmp_dir:"
        },
        {
            "sha": "86c34c4efd01e22a9c0a4133f7de96a5193b6440",
            "filename": "tests/trainer/test_trainer_distributed.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b550c47b208fc1a57feca46307349ab575cba2c/tests%2Ftrainer%2Ftest_trainer_distributed.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b550c47b208fc1a57feca46307349ab575cba2c/tests%2Ftrainer%2Ftest_trainer_distributed.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer_distributed.py?ref=2b550c47b208fc1a57feca46307349ab575cba2c",
            "patch": "@@ -200,6 +200,8 @@ def compute_metrics(p: EvalPrediction) -> Dict:\n     model = RegressionModel()\n     training_args.per_device_train_batch_size = 1\n     training_args.max_steps = 1\n-    training_args.dispatch_batches = False\n+    training_args.accelerator_config = {\n+        \"dispatch_batches\": False,\n+    }\n     trainer = Trainer(model, training_args, train_dataset=train_dataset)\n     trainer.train()"
        }
    ],
    "stats": {
        "total": 123,
        "additions": 15,
        "deletions": 108
    }
}