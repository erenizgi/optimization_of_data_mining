{
    "author": "notkisk",
    "message": "[detection] fix correct `k_proj` weight and bias slicing in D-FINE (#40257)\n\nFix: correct k_proj weight and bias conversion in D-FINE",
    "sha": "c93594e286a15d2b3d69433451e4a3bf4e025f8a",
    "files": [
        {
            "sha": "a2d23b3165bfcbf99686e1855bcf1e12808469d3",
            "filename": "src/transformers/models/d_fine/convert_d_fine_original_pytorch_checkpoint_to_hf.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c93594e286a15d2b3d69433451e4a3bf4e025f8a/src%2Ftransformers%2Fmodels%2Fd_fine%2Fconvert_d_fine_original_pytorch_checkpoint_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c93594e286a15d2b3d69433451e4a3bf4e025f8a/src%2Ftransformers%2Fmodels%2Fd_fine%2Fconvert_d_fine_original_pytorch_checkpoint_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fd_fine%2Fconvert_d_fine_original_pytorch_checkpoint_to_hf.py?ref=c93594e286a15d2b3d69433451e4a3bf4e025f8a",
            "patch": "@@ -364,8 +364,8 @@ def read_in_q_k_v(state_dict, config, model_name):\n         if model_name in [\"dfine_n_coco\", \"dfine_n_obj2coco_e25\", \"dfine_n_obj365\"]:\n             state_dict[f\"model.decoder.layers.{i}.self_attn.q_proj.weight\"] = in_proj_weight[:128, :]\n             state_dict[f\"model.decoder.layers.{i}.self_attn.q_proj.bias\"] = in_proj_bias[:128]\n-            state_dict[f\"model.decoder.layers.{i}.self_attn.k_proj.weight\"] = in_proj_weight[256:384, :]\n-            state_dict[f\"model.decoder.layers.{i}.self_attn.k_proj.bias\"] = in_proj_bias[256:384]\n+            state_dict[f\"model.decoder.layers.{i}.self_attn.k_proj.weight\"] = in_proj_weight[128:256, :]\n+            state_dict[f\"model.decoder.layers.{i}.self_attn.k_proj.bias\"] = in_proj_bias[128:256]\n             state_dict[f\"model.decoder.layers.{i}.self_attn.v_proj.weight\"] = in_proj_weight[-128:, :]\n             state_dict[f\"model.decoder.layers.{i}.self_attn.v_proj.bias\"] = in_proj_bias[-128:]\n         else:"
        }
    ],
    "stats": {
        "total": 4,
        "additions": 2,
        "deletions": 2
    }
}