{
    "author": "PrathmeshAdsod",
    "message": "feature: Add robust token counting with padding exclusion  (#40416)\n\n* created robust token counting by using existing include_num_input_tokens_seen variable and kept bool for backward compatibility and added string also to ensure everything goes well and kept default as is. also robust test cases are created\n\n* some codebase mismatched in my local and remote, commiting to solve it and also solved code quality issue\n\n* ci: retrigger tests\n\n* another attemp to trigger CI for checks",
    "sha": "ec532f20fba67d08f29a6ecdb1404f42a351221f",
    "files": [
        {
            "sha": "d98fdef1867e7d38e053a399db8c1b14d47aaff1",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 20,
            "deletions": 2,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/ec532f20fba67d08f29a6ecdb1404f42a351221f/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ec532f20fba67d08f29a6ecdb1404f42a351221f/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=ec532f20fba67d08f29a6ecdb1404f42a351221f",
            "patch": "@@ -2627,7 +2627,7 @@ def _inner_training_loop(\n                     # Since we perform prefetching, we need to manually set sync_gradients\n                     self.accelerator.gradient_state._set_sync_gradients(do_sync_step)\n \n-                    if self.args.include_num_input_tokens_seen:\n+                    if self.args.include_num_input_tokens_seen not in [\"no\", False]:\n                         main_input_name = getattr(self.model, \"main_input_name\", \"input_ids\")\n                         if main_input_name not in inputs:\n                             logger.warning(\n@@ -2636,7 +2636,25 @@ def _inner_training_loop(\n                                 \"a `main_input_name` attribute to the model class you are using.\"\n                             )\n                         else:\n-                            input_tokens = inputs[main_input_name].numel()\n+                            if self.args.include_num_input_tokens_seen == \"non_padding\":\n+                                if \"attention_mask\" in inputs:\n+                                    input_tokens = inputs[\"attention_mask\"].sum()\n+                                elif (\n+                                    self.processing_class is not None\n+                                    and hasattr(self.processing_class, \"pad_token_id\")\n+                                    and self.processing_class.pad_token_id is not None\n+                                ):\n+                                    input_tokens = (\n+                                        inputs[main_input_name] != self.processing_class.pad_token_id\n+                                    ).sum()\n+                                else:\n+                                    logger.warning(\n+                                        \"Could not determine method to count non-padding tokens, falling back to counting all tokens.\"\n+                                    )\n+                                    input_tokens = inputs[main_input_name].numel()\n+                            else:\n+                                input_tokens = inputs[main_input_name].numel()\n+\n                             input_tokens = torch.tensor(input_tokens, device=self.args.device, dtype=torch.int64)\n                             self.state.num_input_tokens_seen += self.accelerator.gather(input_tokens).sum().item()\n                     if rng_to_sync:"
        },
        {
            "sha": "19cfadb992f1aef447b6e993abe42429e9e21e18",
            "filename": "src/transformers/training_args.py",
            "status": "modified",
            "additions": 11,
            "deletions": 2,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/ec532f20fba67d08f29a6ecdb1404f42a351221f/src%2Ftransformers%2Ftraining_args.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ec532f20fba67d08f29a6ecdb1404f42a351221f/src%2Ftransformers%2Ftraining_args.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftraining_args.py?ref=ec532f20fba67d08f29a6ecdb1404f42a351221f",
            "patch": "@@ -1495,10 +1495,14 @@ class TrainingArguments:\n         metadata={\"help\": \"If set to `True`, the speed metrics will include `tgs` (tokens per second per device).\"},\n     )\n \n-    include_num_input_tokens_seen: Optional[bool] = field(\n+    include_num_input_tokens_seen: Optional[Union[str, bool]] = field(\n         default=False,\n         metadata={\n-            \"help\": \"If set to `True`, will track the number of input tokens seen throughout training. (May be slower in distributed training)\"\n+            \"help\": (\n+                \"Whether to track the number of input tokens seen. \"\n+                \"Can be `'all'` to count all tokens, `'non_padding'` to count only non-padding tokens, \"\n+                \"or a boolean (`True` maps to `'all'`, `False` to `'no'`).\"\n+            )\n         },\n     )\n \n@@ -2139,6 +2143,11 @@ def __post_init__(self):\n             )\n             self.include_for_metrics.append(\"inputs\")\n \n+        if self.include_num_input_tokens_seen is True:\n+            self.include_num_input_tokens_seen = \"all\"\n+        elif self.include_num_input_tokens_seen is False:\n+            self.include_num_input_tokens_seen = \"no\"\n+\n     def __str__(self):\n         self_as_dict = asdict(self)\n "
        },
        {
            "sha": "8fc1628c7f6d32750c017045f2541ef31fa0074c",
            "filename": "tests/trainer/test_trainer.py",
            "status": "modified",
            "additions": 98,
            "deletions": 0,
            "changes": 98,
            "blob_url": "https://github.com/huggingface/transformers/blob/ec532f20fba67d08f29a6ecdb1404f42a351221f/tests%2Ftrainer%2Ftest_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ec532f20fba67d08f29a6ecdb1404f42a351221f/tests%2Ftrainer%2Ftest_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer.py?ref=ec532f20fba67d08f29a6ecdb1404f42a351221f",
            "patch": "@@ -1297,6 +1297,104 @@ def test_tf32(self):\n             trainer.train()\n             self.check_trained_model(trainer.model)\n \n+    def test_include_num_input_tokens_seen(self):\n+        model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)\n+        tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n+        tokenizer.pad_token = \"[PAD]\"\n+        model.config.pad_token_id = tokenizer.pad_token_id\n+\n+        sentences = [\"This is a short sentence.\", \"This is a much longer sentence that will require padding.\"]\n+        labels = torch.tensor([0, 1])\n+\n+        # 1. Test with attention_mask\n+        tokenized_dataset_with_mask = tokenizer(sentences, truncation=True, padding=\"longest\", return_tensors=\"pt\")\n+        tokenized_dataset_with_mask[\"labels\"] = labels\n+        dataset_with_mask = datasets.Dataset.from_dict(tokenized_dataset_with_mask)\n+\n+        # 2. Test without attention_mask\n+        tokenized_dataset_no_mask = {k: v for k, v in tokenized_dataset_with_mask.items() if k != \"attention_mask\"}\n+        dataset_no_mask = datasets.Dataset.from_dict(tokenized_dataset_no_mask)\n+\n+        # 3. Test with no padding information\n+        tokenizer_no_pad = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n+        tokenizer_no_pad.pad_token = None\n+\n+        data_collator = default_data_collator\n+\n+        with tempfile.TemporaryDirectory() as tmp_dir:\n+            # Test case 1: \"non_padding\" with attention_mask\n+            args = TrainingArguments(\n+                output_dir=tmp_dir,\n+                include_num_input_tokens_seen=\"non_padding\",\n+                per_device_train_batch_size=2,\n+                max_steps=1,\n+                report_to=\"none\",\n+            )\n+            trainer = Trainer(\n+                model=model,\n+                args=args,\n+                train_dataset=dataset_with_mask,\n+                data_collator=data_collator,\n+                processing_class=tokenizer,\n+            )\n+            trainer.train()\n+            attention_mask = tokenized_dataset_with_mask[\"attention_mask\"]\n+            non_padded_tokens_with_mask = attention_mask.sum().item()\n+            self.assertEqual(trainer.state.num_input_tokens_seen, non_padded_tokens_with_mask)\n+\n+            # Test case 2: \"non_padding\" without attention_mask (fallback to pad_token_id)\n+            trainer = Trainer(\n+                model=model,\n+                args=args,\n+                train_dataset=dataset_no_mask,\n+                data_collator=data_collator,\n+                processing_class=tokenizer,\n+            )\n+            trainer.train()\n+            input_ids = tokenized_dataset_with_mask[\"input_ids\"]  # use original to compute expected\n+            non_padded_tokens_no_mask = (input_ids != tokenizer.pad_token_id).sum().item()\n+            self.assertEqual(trainer.state.num_input_tokens_seen, non_padded_tokens_no_mask)\n+\n+            # Test case 3: \"non_padding\" with no padding info (fallback to numel)\n+            with self.assertLogs(\"transformers.trainer\", level=\"WARNING\") as cm:\n+                trainer = Trainer(\n+                    model=model,\n+                    args=args,\n+                    train_dataset=dataset_no_mask,  # still has input_ids\n+                    data_collator=data_collator,\n+                    processing_class=tokenizer_no_pad,  # tokenizer without pad token\n+                )\n+                trainer.train()\n+                self.assertTrue(\n+                    any(\"Could not determine method to count non-padding tokens\" in log for log in cm.output)\n+                )\n+            total_tokens = input_ids.numel()\n+            self.assertEqual(trainer.state.num_input_tokens_seen, total_tokens)\n+\n+            # Test case 4: \"all\"\n+            args.include_num_input_tokens_seen = \"all\"\n+            trainer = Trainer(\n+                model=model,\n+                args=args,\n+                train_dataset=dataset_with_mask,\n+                data_collator=data_collator,\n+                processing_class=tokenizer,\n+            )\n+            trainer.train()\n+            self.assertEqual(trainer.state.num_input_tokens_seen, total_tokens)\n+\n+            # Test case 5: True (backward compatibility)\n+            args.include_num_input_tokens_seen = True\n+            trainer = Trainer(\n+                model=model,\n+                args=args,\n+                train_dataset=dataset_with_mask,\n+                data_collator=data_collator,\n+                processing_class=tokenizer,\n+            )\n+            trainer.train()\n+            self.assertEqual(trainer.state.num_input_tokens_seen, total_tokens)\n+\n \n @require_torch\n @require_sentencepiece"
        }
    ],
    "stats": {
        "total": 133,
        "additions": 129,
        "deletions": 4
    }
}