{
    "author": "amd-xiaoyu12",
    "message": "Expand quantized data type support for tensor parallelism  (#37719)\n\nUpdate tensor_parallel.py\n\nCo-authored-by: Xiao YU <Xiao.YU@xilinx.com>",
    "sha": "4fee3209264a3b99a66c953ff0883b289b54d34e",
    "files": [
        {
            "sha": "d1fef6d492ca89371140c6f880703cd02c2bea79",
            "filename": "src/transformers/integrations/tensor_parallel.py",
            "status": "modified",
            "additions": 11,
            "deletions": 6,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/4fee3209264a3b99a66c953ff0883b289b54d34e/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4fee3209264a3b99a66c953ff0883b289b54d34e/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py?ref=4fee3209264a3b99a66c953ff0883b289b54d34e",
            "patch": "@@ -307,7 +307,8 @@ def partition_tensor(self, param, empty_param, param_type, param_casting_dtype,\n             parameter = parameter.contiguous()\n         if self.use_dtensor:\n             parameter = DTensor.from_local(parameter, device_mesh, shard, run_check=False)\n-        return nn.Parameter(parameter)\n+        requires_grad = True if parameter.is_floating_point() else False\n+        return nn.Parameter(parameter, requires_grad=requires_grad)\n \n     @staticmethod\n     def _prepare_output_fn(output_layouts, use_local_output, mod, outputs, device_mesh):\n@@ -329,7 +330,8 @@ def partition_tensor(self, param, empty_param, param_type, param_casting_dtype,\n             parameter = parameter.contiguous()\n         if self.use_dtensor:\n             parameter = DTensor.from_local(parameter, device_mesh, [Shard(-2)], run_check=False)\n-        return nn.Parameter(parameter)\n+        requires_grad = True if parameter.is_floating_point() else False\n+        return nn.Parameter(parameter, requires_grad=requires_grad)\n \n \n class RowwiseParallel(TensorParallelLayer):\n@@ -381,7 +383,8 @@ def partition_tensor(self, param, empty_param, param_type, param_casting_dtype,\n             parameter = parameter.contiguous()\n         if self.use_dtensor:\n             parameter = DTensor.from_local(parameter, device_mesh, shard, run_check=False)\n-        return nn.Parameter(parameter)\n+        requires_grad = True if parameter.is_floating_point() else False\n+        return nn.Parameter(parameter, requires_grad=requires_grad)\n \n     @staticmethod\n     def _prepare_input_fn(input_layouts, desired_input_layouts, mod, inputs, device_mesh):\n@@ -443,7 +446,8 @@ def partition_tensor(self, param, empty_param, param_type, param_casting_dtype,\n             parameter = parameter.contiguous()\n         if self.use_dtensor:\n             parameter = DTensor.from_local(parameter, device_mesh, [Shard(-1)], run_check=False)\n-        return nn.Parameter(parameter)\n+        requires_grad = True if parameter.is_floating_point() else False\n+        return nn.Parameter(parameter, requires_grad=requires_grad)\n \n \n class SequenceParallel(TensorParallelLayer):\n@@ -521,13 +525,14 @@ def partition_tensor(self, param, empty_param, param_type, param_casting_dtype,\n         # colwise shard weight/bias to Shard(0), weight be Shard(-2) (0 if you have 1 dim only)\n         # means Colwise as Linear is input * weight^T + bias, where\n         # weight would become Shard(1)\n-        parameter = param[:]\n+        parameter = param[...]\n         parameter = parameter.to(param_casting_dtype)\n         if to_contiguous:\n             parameter = parameter.contiguous()\n         if self.use_dtensor:\n             parameter = DTensor.from_local(parameter, device_mesh, [Replicate()], run_check=False)\n-        return nn.Parameter(parameter)\n+        requires_grad = True if parameter.is_floating_point() else False\n+        return nn.Parameter(parameter, requires_grad=requires_grad)\n \n \n SUPPORTED_TP_STYLES = {"
        }
    ],
    "stats": {
        "total": 17,
        "additions": 11,
        "deletions": 6
    }
}