{
    "author": "alexrs-cohere",
    "message": "Add Cohere2 model (#35224)",
    "sha": "64478c76313d20cac925aab5ad762a110e704774",
    "files": [
        {
            "sha": "c4707d5f20a0277340f649613531f5e61c86a1db",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/64478c76313d20cac925aab5ad762a110e704774/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/64478c76313d20cac925aab5ad762a110e704774/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=64478c76313d20cac925aab5ad762a110e704774",
            "patch": "@@ -362,6 +362,8 @@\n         title: CodeLlama\n       - local: model_doc/cohere\n         title: Cohere\n+      - local: model_doc/cohere2\n+        title: Cohere2\n       - local: model_doc/convbert\n         title: ConvBERT\n       - local: model_doc/cpm"
        },
        {
            "sha": "49c44874e320ef3ae65cba111408c948503232c1",
            "filename": "docs/source/en/index.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/64478c76313d20cac925aab5ad762a110e704774/docs%2Fsource%2Fen%2Findex.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/64478c76313d20cac925aab5ad762a110e704774/docs%2Fsource%2Fen%2Findex.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Findex.md?ref=64478c76313d20cac925aab5ad762a110e704774",
            "patch": "@@ -99,6 +99,7 @@ Flax), PyTorch, and/or TensorFlow.\n |                       [CodeGen](model_doc/codegen)                       |       ‚úÖ        |         ‚ùå         |      ‚ùå      |\n |                    [CodeLlama](model_doc/code_llama)                     |       ‚úÖ        |         ‚ùå         |      ‚úÖ      |\n |                        [Cohere](model_doc/cohere)                        |       ‚úÖ        |         ‚ùå         |      ‚ùå      |\n+|                       [Cohere2](model_doc/cohere2)                       |       ‚úÖ        |         ‚ùå         |      ‚ùå      |\n |              [Conditional DETR](model_doc/conditional_detr)              |       ‚úÖ        |         ‚ùå         |      ‚ùå      |\n |                      [ConvBERT](model_doc/convbert)                      |       ‚úÖ        |         ‚úÖ         |      ‚ùå      |\n |                      [ConvNeXT](model_doc/convnext)                      |       ‚úÖ        |         ‚úÖ         |      ‚ùå      |"
        },
        {
            "sha": "4d3a1f0cb0929fc8d4bd653edf0172006f7d9fe6",
            "filename": "docs/source/en/model_doc/cohere2.md",
            "status": "added",
            "additions": 44,
            "deletions": 0,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/64478c76313d20cac925aab5ad762a110e704774/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/64478c76313d20cac925aab5ad762a110e704774/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere2.md?ref=64478c76313d20cac925aab5ad762a110e704774",
            "patch": "@@ -0,0 +1,44 @@\n+# Cohere\n+\n+## Usage tips\n+The model and tokenizer can be loaded via:\n+\n+```python\n+# pip install transformers\n+from transformers import AutoTokenizer, AutoModelForCausalLM\n+\n+model_id = \"CohereForAI/c4ai-command-r7b-12-2024\"\n+tokenizer = AutoTokenizer.from_pretrained(model_id)\n+model = AutoModelForCausalLM.from_pretrained(model_id)\n+\n+# Format message with the command-r chat template\n+messages = [{\"role\": \"user\", \"content\": \"Hello, how are you?\"}]\n+input_ids = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n+\n+gen_tokens = model.generate(\n+    input_ids,\n+    max_new_tokens=100,\n+    do_sample=True,\n+    temperature=0.3,\n+    )\n+\n+gen_text = tokenizer.decode(gen_tokens[0])\n+print(gen_text)\n+```\n+\n+## Cohere2Config\n+\n+[[autodoc]] Cohere2Config\n+\n+## Cohere2Model\n+\n+[[autodoc]] Cohere2Model\n+    - forward\n+\n+\n+## Cohere2ForCausalLM\n+\n+[[autodoc]] Cohere2ForCausalLM\n+    - forward\n+\n+"
        },
        {
            "sha": "4d7852a66307e23831fb0399c4629eff382525f4",
            "filename": "docs/source/en/perf_infer_gpu_one.md",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/64478c76313d20cac925aab5ad762a110e704774/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/64478c76313d20cac925aab5ad762a110e704774/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md?ref=64478c76313d20cac925aab5ad762a110e704774",
            "patch": "@@ -43,6 +43,7 @@ FlashAttention-2 is currently supported for the following architectures:\n * [Chameleon](https://huggingface.co/docs/transformers/model_doc/chameleon#transformers.Chameleon)\n * [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPModel)\n * [Cohere](https://huggingface.co/docs/transformers/model_doc/cohere#transformers.CohereModel)\n+* [Cohere2](https://huggingface.co/docs/transformers/model_doc/cohere2#transformers.Cohere2Model)\n * [GLM](https://huggingface.co/docs/transformers/model_doc/glm#transformers.GLMModel)\n * [Dbrx](https://huggingface.co/docs/transformers/model_doc/dbrx#transformers.DbrxModel)\n * [DistilBert](https://huggingface.co/docs/transformers/model_doc/distilbert#transformers.DistilBertModel)\n@@ -227,6 +228,7 @@ For now, Transformers supports SDPA inference and training for the following arc\n * [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPModel)\n * [GLM](https://huggingface.co/docs/transformers/model_doc/glm#transformers.GLMModel)\n * [Cohere](https://huggingface.co/docs/transformers/model_doc/cohere#transformers.CohereModel)\n+* [Cohere2](https://huggingface.co/docs/transformers/model_doc/cohere2#transformers.Cohere2Model)\n * [data2vec_audio](https://huggingface.co/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecAudioModel)\n * [Dbrx](https://huggingface.co/docs/transformers/model_doc/dbrx#transformers.DbrxModel)\n * [DeiT](https://huggingface.co/docs/transformers/model_doc/deit#transformers.DeiTModel)"
        },
        {
            "sha": "1eb34b48fda856efda1fa975ac725b6a89216255",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/64478c76313d20cac925aab5ad762a110e704774/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/64478c76313d20cac925aab5ad762a110e704774/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=64478c76313d20cac925aab5ad762a110e704774",
            "patch": "@@ -305,6 +305,7 @@\n         \"CodeGenTokenizer\",\n     ],\n     \"models.cohere\": [\"CohereConfig\"],\n+    \"models.cohere2\": [\"Cohere2Config\"],\n     \"models.conditional_detr\": [\"ConditionalDetrConfig\"],\n     \"models.convbert\": [\n         \"ConvBertConfig\",\n@@ -1787,6 +1788,7 @@\n         ]\n     )\n     _import_structure[\"models.cohere\"].extend([\"CohereForCausalLM\", \"CohereModel\", \"CoherePreTrainedModel\"])\n+    _import_structure[\"models.cohere2\"].extend([\"Cohere2ForCausalLM\", \"Cohere2Model\", \"Cohere2PreTrainedModel\"])\n     _import_structure[\"models.conditional_detr\"].extend(\n         [\n             \"ConditionalDetrForObjectDetection\",\n@@ -5204,6 +5206,7 @@\n         CodeGenTokenizer,\n     )\n     from .models.cohere import CohereConfig\n+    from .models.cohere2 import Cohere2Config\n     from .models.conditional_detr import (\n         ConditionalDetrConfig,\n     )\n@@ -6681,6 +6684,11 @@\n             CohereModel,\n             CoherePreTrainedModel,\n         )\n+        from .models.cohere2 import (\n+            Cohere2ForCausalLM,\n+            Cohere2Model,\n+            Cohere2PreTrainedModel,\n+        )\n         from .models.conditional_detr import (\n             ConditionalDetrForObjectDetection,\n             ConditionalDetrForSegmentation,"
        },
        {
            "sha": "f38fc8f9824d3b88eb00296451994d16347eab2a",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/64478c76313d20cac925aab5ad762a110e704774/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/64478c76313d20cac925aab5ad762a110e704774/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=64478c76313d20cac925aab5ad762a110e704774",
            "patch": "@@ -1634,8 +1634,9 @@ def __init__(\n         self.num_key_value_heads = (\n             config.num_attention_heads if config.num_key_value_heads is None else config.num_key_value_heads\n         )\n+        layer_switch = config.sliding_window_pattern if hasattr(config, \"sliding_window_pattern\") else 2  # 2 is for BC\n         self.is_sliding = torch.tensor(\n-            [not bool(i % 2) for i in range(config.num_hidden_layers)], dtype=torch.bool, device=device\n+            [bool((i + 1) % layer_switch) for i in range(config.num_hidden_layers)], dtype=torch.bool, device=device\n         )\n         self.key_cache: List[torch.Tensor] = []\n         self.value_cache: List[torch.Tensor] = []"
        },
        {
            "sha": "2e3b48da96e9668e22cd7aebc0bd7b201ca7a25f",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/64478c76313d20cac925aab5ad762a110e704774/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/64478c76313d20cac925aab5ad762a110e704774/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=64478c76313d20cac925aab5ad762a110e704774",
            "patch": "@@ -52,6 +52,7 @@\n     code_llama,\n     codegen,\n     cohere,\n+    cohere2,\n     conditional_detr,\n     convbert,\n     convnext,"
        },
        {
            "sha": "1d9db837e8d27cfd903d6ced58f58127cfa2d522",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/64478c76313d20cac925aab5ad762a110e704774/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/64478c76313d20cac925aab5ad762a110e704774/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=64478c76313d20cac925aab5ad762a110e704774",
            "patch": "@@ -69,6 +69,7 @@\n         (\"code_llama\", \"LlamaConfig\"),\n         (\"codegen\", \"CodeGenConfig\"),\n         (\"cohere\", \"CohereConfig\"),\n+        (\"cohere2\", \"Cohere2Config\"),\n         (\"conditional_detr\", \"ConditionalDetrConfig\"),\n         (\"convbert\", \"ConvBertConfig\"),\n         (\"convnext\", \"ConvNextConfig\"),\n@@ -371,6 +372,7 @@\n         (\"code_llama\", \"CodeLlama\"),\n         (\"codegen\", \"CodeGen\"),\n         (\"cohere\", \"Cohere\"),\n+        (\"cohere2\", \"Cohere2\"),\n         (\"conditional_detr\", \"Conditional DETR\"),\n         (\"convbert\", \"ConvBERT\"),\n         (\"convnext\", \"ConvNeXT\"),"
        },
        {
            "sha": "bec72a4e7b84ec976208464c7ff270a2e09ce212",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/64478c76313d20cac925aab5ad762a110e704774/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/64478c76313d20cac925aab5ad762a110e704774/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=64478c76313d20cac925aab5ad762a110e704774",
            "patch": "@@ -69,6 +69,7 @@\n         (\"code_llama\", \"LlamaModel\"),\n         (\"codegen\", \"CodeGenModel\"),\n         (\"cohere\", \"CohereModel\"),\n+        (\"cohere2\", \"Cohere2Model\"),\n         (\"conditional_detr\", \"ConditionalDetrModel\"),\n         (\"convbert\", \"ConvBertModel\"),\n         (\"convnext\", \"ConvNextModel\"),\n@@ -482,6 +483,7 @@\n         (\"code_llama\", \"LlamaForCausalLM\"),\n         (\"codegen\", \"CodeGenForCausalLM\"),\n         (\"cohere\", \"CohereForCausalLM\"),\n+        (\"cohere2\", \"Cohere2ForCausalLM\"),\n         (\"cpmant\", \"CpmAntForCausalLM\"),\n         (\"ctrl\", \"CTRLLMHeadModel\"),\n         (\"data2vec-text\", \"Data2VecTextForCausalLM\"),"
        },
        {
            "sha": "386ca11abedcf4733445d12f452bd79bc6d8050b",
            "filename": "src/transformers/models/auto/tokenization_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/64478c76313d20cac925aab5ad762a110e704774/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/64478c76313d20cac925aab5ad762a110e704774/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py?ref=64478c76313d20cac925aab5ad762a110e704774",
            "patch": "@@ -147,6 +147,7 @@\n             ),\n             (\"codegen\", (\"CodeGenTokenizer\", \"CodeGenTokenizerFast\" if is_tokenizers_available() else None)),\n             (\"cohere\", (None, \"CohereTokenizerFast\" if is_tokenizers_available() else None)),\n+            (\"cohere2\", (None, \"CohereTokenizerFast\" if is_tokenizers_available() else None)),\n             (\"convbert\", (\"ConvBertTokenizer\", \"ConvBertTokenizerFast\" if is_tokenizers_available() else None)),\n             (\n                 \"cpm\","
        },
        {
            "sha": "1447f65935601f0fffd8a88dac25bc5916b35f83",
            "filename": "src/transformers/models/cohere2/__init__.py",
            "status": "added",
            "additions": 27,
            "deletions": 0,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/64478c76313d20cac925aab5ad762a110e704774/src%2Ftransformers%2Fmodels%2Fcohere2%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/64478c76313d20cac925aab5ad762a110e704774/src%2Ftransformers%2Fmodels%2Fcohere2%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2F__init__.py?ref=64478c76313d20cac925aab5ad762a110e704774",
            "patch": "@@ -0,0 +1,27 @@\n+# Copyright 2024 Cohere and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_cohere2 import *\n+    from .modeling_cohere2 import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "aa22ec8eabef710ebee180dd44787b60d690483b",
            "filename": "src/transformers/models/cohere2/configuration_cohere2.py",
            "status": "added",
            "additions": 209,
            "deletions": 0,
            "changes": 209,
            "blob_url": "https://github.com/huggingface/transformers/blob/64478c76313d20cac925aab5ad762a110e704774/src%2Ftransformers%2Fmodels%2Fcohere2%2Fconfiguration_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/64478c76313d20cac925aab5ad762a110e704774/src%2Ftransformers%2Fmodels%2Fcohere2%2Fconfiguration_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fconfiguration_cohere2.py?ref=64478c76313d20cac925aab5ad762a110e704774",
            "patch": "@@ -0,0 +1,209 @@\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+#           This file was automatically generated from src/transformers/models/cohere2/modular_cohere2.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_cohere2.py file directly. One of our CI enforces this.\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+# coding=utf-8\n+# Copyright 2024 Cohere Inc. HuggingFace Inc. team. All rights reserved.\n+#\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from ...configuration_utils import PretrainedConfig\n+from ...modeling_rope_utils import rope_config_validation\n+\n+\n+class Cohere2Config(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`CohereModel`]. It is used to instantiate an Cohere\n+    model according to the specified arguments, defining the model architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of the [CohereForAI/c4ai-command-r-v01](https://huggingface.co/CohereForAI/c4ai-command-r-v01) model.\n+\n+\n+    Args:\n+        vocab_size (`int`, *optional*, defaults to 256000):\n+            Vocabulary size of the Cohere model. Defines the number of different tokens that can be represented by the\n+            `inputs_ids` passed when calling [`CohereModel`]\n+        hidden_size (`int`, *optional*, defaults to 8192):\n+            Dimension of the hidden representations.\n+        intermediate_size (`int`, *optional*, defaults to 22528):\n+            Dimension of the MLP representations.\n+        logit_scale (`float`, *optional*, defaults to 0.0625):\n+            The scaling factor for the output logits.\n+        num_hidden_layers (`int`, *optional*, defaults to 40):\n+            Number of hidden layers in the Transformer decoder.\n+        num_attention_heads (`int`, *optional*, defaults to 64):\n+            Number of attention heads for each attention layer in the Transformer decoder.\n+        num_key_value_heads (`int`, *optional*):\n+            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n+            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n+            `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n+            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n+            by meanpooling all the original heads within that group. For more details checkout [this\n+            paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n+            `num_attention_heads`.\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n+            The non-linear activation function (function or string) in the decoder.\n+        max_position_embeddings (`int`, *optional*, defaults to 8192):\n+            The maximum sequence length that this model might ever be used with.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        layer_norm_eps (`float`, *optional*, defaults to 1e-05):\n+            The epsilon used by the layer normalization.\n+        use_cache (`bool`, *optional*, defaults to `True`):\n+            Whether or not the model should return the last key/values attentions (not used by all models). Only\n+            relevant if `config.is_decoder=True`.\n+        pad_token_id (`int`, *optional*, defaults to 0):\n+            Padding token id.\n+        bos_token_id (`int`, *optional*, defaults to 5):\n+            Beginning of stream token id.\n+        eos_token_id (`int`, *optional*, defaults to 255001):\n+            End of stream token id.\n+        tie_word_embeddings (`bool`, *optional*, defaults to `True`):\n+            Whether to tie weight embeddings\n+        rope_theta (`float`, *optional*, defaults to 10000.0):\n+            The base period of the RoPE embeddings.\n+        rope_scaling (`Dict`, *optional*):\n+            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n+            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n+            accordingly.\n+            Expected contents:\n+                `rope_type` (`str`):\n+                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n+                    'llama3'], with 'default' being the original RoPE implementation.\n+                `factor` (`float`, *optional*):\n+                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n+                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n+                    original maximum pre-trained length.\n+                `original_max_position_embeddings` (`int`, *optional*):\n+                    Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n+                    pretraining.\n+                `attention_factor` (`float`, *optional*):\n+                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n+                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n+                    `factor` field to infer the suggested value.\n+                `beta_fast` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 32.\n+                `beta_slow` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 1.\n+                `short_factor` (`List[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `long_factor` (`List[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `low_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n+                `high_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n+        attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):\n+            Whether to use a bias in the query, key, value and output projection layers during self-attention.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        sliding_window (`int`, *optional*, defaults to 4096):\n+            Size of the sliding window attention context.\n+        sliding_window_pattern (`int`, *optional*, defaults to 4):\n+            Pattern for the sliding window attention.\n+        cache_implementation (`str`, *optional*, defaults to `\"hybrid\"`): the cache type to be used with `generate`.\n+\n+    ```python\n+    >>> from transformers import Cohere2Model, Cohere2Config\n+\n+    >>> # Initializing a Cohere Nextmodel configuration\n+    >>> configuration = Cohere2Config()\n+\n+    >>> # Initializing a model from the Cohere2 configuration\n+    >>> model = Cohere2Model(configuration) # doctest: +SKIP\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config # doctest: +SKIP\n+    ```\n+    \"\"\"\n+\n+    model_type = \"cohere2\"\n+    keys_to_ignore_at_inference = [\"past_key_values\"]\n+\n+    def __init__(\n+        self,\n+        vocab_size=256000,\n+        hidden_size=8192,\n+        intermediate_size=22528,\n+        logit_scale=0.0625,\n+        num_hidden_layers=40,\n+        num_attention_heads=64,\n+        num_key_value_heads=None,\n+        hidden_act=\"silu\",\n+        max_position_embeddings=8192,\n+        initializer_range=0.02,\n+        layer_norm_eps=1e-5,\n+        use_cache=True,\n+        pad_token_id=0,\n+        bos_token_id=5,\n+        eos_token_id=255001,\n+        tie_word_embeddings=True,\n+        rope_theta=10000.0,\n+        rope_scaling=None,\n+        attention_bias=False,\n+        attention_dropout=0.0,\n+        sliding_window=4096,\n+        sliding_window_pattern=4,\n+        cache_implementation=\"hybrid\",\n+        **kwargs,\n+    ):\n+        self.vocab_size = vocab_size\n+        self.max_position_embeddings = max_position_embeddings\n+        self.hidden_size = hidden_size\n+        self.logit_scale = logit_scale\n+        self.intermediate_size = intermediate_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+\n+        # for backward compatibility\n+        if num_key_value_heads is None:\n+            num_key_value_heads = num_attention_heads\n+\n+        self.num_key_value_heads = num_key_value_heads\n+        self.hidden_act = hidden_act\n+        self.initializer_range = initializer_range\n+        self.layer_norm_eps = layer_norm_eps\n+        self.use_cache = use_cache\n+        self.rope_theta = rope_theta\n+        self.rope_scaling = rope_scaling\n+        self.attention_bias = attention_bias\n+        self.attention_dropout = attention_dropout\n+        self.sliding_window = sliding_window\n+        self.sliding_window_pattern = sliding_window_pattern\n+        # Need to specify head_dim in the config so it can be used in the attention forward functions\n+        self.head_dim = hidden_size // num_attention_heads\n+        self.cache_implementation = cache_implementation\n+\n+        # Validate the correctness of rotary position embeddings parameters\n+        rope_config_validation(self)\n+\n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n+\n+\n+__all__ = [\"Cohere2Config\"]"
        },
        {
            "sha": "6b19d178341fbbd23521477972c196dea7c0e7f9",
            "filename": "src/transformers/models/cohere2/modeling_cohere2.py",
            "status": "added",
            "additions": 1082,
            "deletions": 0,
            "changes": 1082,
            "blob_url": "https://github.com/huggingface/transformers/blob/64478c76313d20cac925aab5ad762a110e704774/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/64478c76313d20cac925aab5ad762a110e704774/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py?ref=64478c76313d20cac925aab5ad762a110e704774",
            "patch": "@@ -0,0 +1,1082 @@\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+#           This file was automatically generated from src/transformers/models/cohere2/modular_cohere2.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_cohere2.py file directly. One of our CI enforces this.\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+# coding=utf-8\n+# Copyright 2024 Cohere Inc. HuggingFace Inc. team. All rights reserved.\n+#\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import math\n+from typing import List, Optional, Tuple, Union\n+\n+import torch\n+import torch.nn as nn\n+\n+from ...activations import ACT2FN\n+from ...cache_utils import Cache, HybridCache\n+from ...generation import GenerationMixin\n+from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n+from ...modeling_utils import PreTrainedModel\n+from ...utils import (\n+    add_start_docstrings,\n+    add_start_docstrings_to_model_forward,\n+    is_flash_attn_2_available,\n+    logging,\n+    replace_return_docstrings,\n+)\n+from .configuration_cohere2 import Cohere2Config\n+\n+\n+if is_flash_attn_2_available():\n+    from ...modeling_flash_attention_utils import _flash_attention_forward\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+_CONFIG_FOR_DOC = \"Cohere2Config\"\n+\n+\n+class Cohere2RotaryEmbedding(nn.Module):\n+    # Note: the forward pass of this RoPE is slightly different from Llama's, resulting in different `sin`/`cos` for\n+    # the same parameterization. The differences are highlighted with a comment.\n+\n+    def __init__(\n+        self,\n+        dim=None,\n+        max_position_embeddings=2048,\n+        base=10000,\n+        device=None,\n+        scaling_factor=1.0,\n+        rope_type=\"default\",\n+        config: Optional[Cohere2Config] = None,\n+    ):\n+        super().__init__()\n+        # TODO (joao): remove the `if` below, only used for BC\n+        self.rope_kwargs = {}\n+        if config is None:\n+            logger.warning_once(\n+                \"`Cohere2RotaryEmbedding` can now be fully parameterized by passing the model config through the \"\n+                \"`config` argument. All other arguments will be removed in v4.46\"\n+            )\n+            self.rope_kwargs = {\n+                \"rope_type\": rope_type,\n+                \"factor\": scaling_factor,\n+                \"dim\": dim,\n+                \"base\": base,\n+                \"max_position_embeddings\": max_position_embeddings,\n+            }\n+            self.rope_type = rope_type\n+            self.max_seq_len_cached = max_position_embeddings\n+            self.original_max_seq_len = max_position_embeddings\n+        else:\n+            # BC: \"rope_type\" was originally \"type\"\n+            if config.rope_scaling is not None:\n+                self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+            else:\n+                self.rope_type = \"default\"\n+            self.max_seq_len_cached = config.max_position_embeddings\n+            self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+    def _dynamic_frequency_update(self, position_ids, device):\n+        \"\"\"\n+        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n+        1 - growing beyond the cached sequence length (allow scaling)\n+        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n+        \"\"\"\n+        seq_len = torch.max(position_ids) + 1\n+        if seq_len > self.max_seq_len_cached:  # growth\n+            inv_freq, self.attention_scaling = self.rope_init_fn(\n+                self.config, device, seq_len=seq_len, **self.rope_kwargs\n+            )\n+            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n+            self.max_seq_len_cached = seq_len\n+\n+        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n+            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n+            self.max_seq_len_cached = self.original_max_seq_len\n+\n+    @torch.no_grad()\n+    def forward(self, x, position_ids):\n+        if \"dynamic\" in self.rope_type:\n+            self._dynamic_frequency_update(position_ids, device=x.device)\n+\n+        # Core RoPE block\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n+        device_type = x.device.type\n+        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.repeat_interleave(freqs, 2, dim=-1)  # This line differs from Llama's implementation\n+            cos = emb.cos()\n+            sin = emb.sin()\n+\n+        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n+        cos = cos * self.attention_scaling\n+        sin = sin * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n+class Cohere2LayerNorm(nn.Module):\n+    def __init__(self, hidden_size=None, eps=1e-5, bias=False):\n+        \"\"\"The hidden size can be a tuple or an int. The tuple is used for QKNorm to normalize across head_dim\"\"\"\n+        super().__init__()\n+        self.weight = nn.Parameter(torch.ones(hidden_size))\n+        self.variance_epsilon = eps\n+\n+    def forward(self, hidden_states):\n+        input_dtype = hidden_states.dtype\n+        hidden_states = hidden_states.to(torch.float32)\n+        mean = hidden_states.mean(-1, keepdim=True)\n+        variance = (hidden_states - mean).pow(2).mean(-1, keepdim=True)\n+        hidden_states = (hidden_states - mean) * torch.rsqrt(variance + self.variance_epsilon)\n+        hidden_states = self.weight.to(torch.float32) * hidden_states\n+        return hidden_states.to(input_dtype)\n+\n+\n+def rotate_half(x):\n+    # Split and rotate. Note that this function is different from e.g. Llama.\n+    x1 = x[..., ::2]\n+    x2 = x[..., 1::2]\n+    rot_x = torch.stack([-x2, x1], dim=-1).flatten(-2)\n+    return rot_x\n+\n+\n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n+    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n+\n+    Args:\n+        q (`torch.Tensor`): The query tensor.\n+        k (`torch.Tensor`): The key tensor.\n+        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n+        sin (`torch.Tensor`): The sine part of the rotary embedding.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n+        unsqueeze_dim (`int`, *optional*, defaults to 1):\n+            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n+            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n+            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n+            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n+            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n+            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n+    Returns:\n+        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n+    \"\"\"\n+    dtype = q.dtype\n+    q = q.float()\n+    k = k.float()\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n+    q_embed = (q * cos) + (rotate_half(q) * sin)\n+    k_embed = (k * cos) + (rotate_half(k) * sin)\n+    return q_embed.to(dtype=dtype), k_embed.to(dtype=dtype)\n+\n+\n+def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n+    \"\"\"\n+    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n+    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n+    \"\"\"\n+    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n+    if n_rep == 1:\n+        return hidden_states\n+    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n+    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n+\n+\n+def eager_attention_forward(\n+    config: Cohere2Config,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    mask: Optional[torch.Tensor],\n+    **_kwargs,\n+) -> Tuple[torch.Tensor, torch.Tensor]:\n+    key_states = repeat_kv(key, config.num_key_value_groups)\n+    value_states = repeat_kv(value, config.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) / math.sqrt(config.head_dim)\n+\n+    if mask is not None:  # no matter the length, we just slice it\n+        causal_mask = mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    # upcast attention to fp32\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=config.attention_dropout, training=config.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+    return attn_output, attn_weights\n+\n+\n+def flash_attention_forward(\n+    config: Cohere2Config,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    mask: Optional[torch.Tensor],\n+    target_dtype: torch.dtype = torch.float16,\n+    **_kwargs,\n+) -> Tuple[torch.Tensor, None]:\n+    if mask is not None:\n+        seq_len = mask.shape[1]\n+        query = query[:, :, :seq_len]\n+        value = value[:, :, :seq_len]\n+\n+    # TODO: These transpose are quite inefficient but Flash Attention requires the layout\n+    # [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor rotary embedding\n+    query_states = query.transpose(1, 2)\n+    key_states = key.transpose(1, 2)\n+    value_states = value.transpose(1, 2)\n+\n+    dropout_rate = config.attention_dropout if config.training else 0.0\n+\n+    input_dtype = query_states.dtype\n+    if input_dtype == torch.float32:\n+        query_states = query_states.to(target_dtype)\n+        key_states = key_states.to(target_dtype)\n+        value_states = value_states.to(target_dtype)\n+\n+    attn_output = _flash_attention_forward(\n+        query_states,\n+        key_states,\n+        value_states,\n+        mask,\n+        seq_len,\n+        dropout=dropout_rate,\n+        is_causal=config.is_causal,\n+        sliding_window=config.sliding_window,\n+        use_top_left_mask=config._flash_attn_uses_top_left_mask,\n+    )\n+\n+    return attn_output, None\n+\n+\n+def sdpa_attention_forward(\n+    config: Cohere2Config,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    mask: Optional[torch.Tensor],\n+    **_kwargs,\n+) -> Tuple[torch.Tensor, None]:\n+    key = repeat_kv(key, config.num_key_value_groups)\n+    value = repeat_kv(value, config.num_key_value_groups)\n+\n+    causal_mask = mask\n+    if mask is not None:\n+        causal_mask = causal_mask[:, :, :, : key.shape[-2]]\n+\n+    # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n+    # Reference: https://github.com/pytorch/pytorch/issues/112577.\n+    if query.device.type == \"cuda\" and causal_mask is not None:\n+        query = query.contiguous()\n+        key = key.contiguous()\n+        value = value.contiguous()\n+\n+    # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n+    # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n+    is_causal = True if causal_mask is None and query.shape[1] > 1 else False\n+\n+    attn_output = torch.nn.functional.scaled_dot_product_attention(\n+        query,\n+        key,\n+        value,\n+        attn_mask=causal_mask,\n+        dropout_p=config.attention_dropout if config.training else 0.0,\n+        is_causal=is_causal,\n+    )\n+\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+    return attn_output, None\n+\n+\n+COHERE2_ATTENTION_FUNCTION = {\n+    \"flash_attention_2\": flash_attention_forward,\n+    \"eager\": eager_attention_forward,\n+    \"sdpa\": sdpa_attention_forward,\n+}\n+\n+\n+class Cohere2Attention(nn.Module):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+\n+    def __init__(self, config: Cohere2Config, layer_idx: Optional[int] = None):\n+        super().__init__()\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        if layer_idx is None:\n+            logger.warning_once(\n+                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n+                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n+                \"when creating this class.\"\n+            )\n+\n+        self.attention_dropout = config.attention_dropout\n+        self.hidden_size = config.hidden_size\n+        self.num_heads = config.num_attention_heads\n+        self.head_dim = config.head_dim\n+        self.num_key_value_heads = config.num_key_value_heads\n+        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n+        self.max_position_embeddings = config.max_position_embeddings\n+        self.rope_theta = config.rope_theta\n+        self.is_causal = True\n+\n+        if (self.head_dim * self.num_heads) != self.hidden_size:\n+            raise ValueError(\n+                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n+                f\" and `num_heads`: {self.num_heads}).\"\n+            )\n+\n+        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n+        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n+        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n+        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=config.attention_bias)\n+\n+        self.sliding_window = (\n+            config.sliding_window if (self.layer_idx + 1) % self.config.sliding_window_pattern != 0 else None\n+        )\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor] = None,\n+        past_key_value: Optional[Cache] = None,\n+        output_attentions: bool = False,\n+        use_cache: bool = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+        bsz, q_len, _ = hidden_states.size()\n+\n+        query_states = self.q_proj(hidden_states)\n+        key_states = self.k_proj(hidden_states)\n+        value_states = self.v_proj(hidden_states)\n+\n+        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n+        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+\n+        cos, sin = position_embeddings\n+\n+        if self.sliding_window is not None:\n+            query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        if past_key_value is not None:\n+            cache_kwargs = {\n+                \"sin\": sin,\n+                \"cos\": cos,\n+                \"sliding_window\": self.sliding_window,\n+                \"cache_position\": cache_position,\n+            }\n+            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        if output_attentions and self.config._attn_implementation in [\"sdpa\", \"flash_attention_2\"]:\n+            logger.warning_once(\"Setting `attention_type` to `eager` because `output_attentions=True`\")\n+            attention_type = \"eager\"\n+        else:\n+            attention_type = self.config._attn_implementation\n+\n+        attn_output, attn_weights = COHERE2_ATTENTION_FUNCTION[attention_type](\n+            self, query_states, key_states, value_states, attention_mask, output_attentions=output_attentions\n+        )\n+\n+        attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+\n+        if not output_attentions:\n+            attn_weights = None\n+\n+        return attn_output, attn_weights, past_key_value\n+\n+\n+class Cohere2MLP(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.hidden_size = config.hidden_size\n+        self.intermediate_size = config.intermediate_size\n+        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n+        self.act_fn = ACT2FN[config.hidden_act]\n+\n+    # Ignore copy\n+    def forward(self, x):\n+        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n+        return down_proj\n+\n+\n+class Cohere2DecoderLayer(nn.Module):\n+    def __init__(self, config: Cohere2Config, layer_idx: int):\n+        super().__init__()\n+        self.hidden_size = config.hidden_size\n+        self.self_attn = Cohere2Attention(config, layer_idx)\n+\n+        self.mlp = Cohere2MLP(config)\n+        self.input_layernorm = Cohere2LayerNorm(hidden_size=(config.hidden_size), eps=config.layer_norm_eps)\n+        self.config = config\n+        self.is_sliding = (layer_idx + 1) % self.config.sliding_window_pattern != 0\n+        self.sliding_window = config.sliding_window\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor] = None,\n+        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n+        output_attentions: Optional[bool] = False,\n+        use_cache: Optional[bool] = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+        \"\"\"\n+        Args:\n+            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n+            position_embeddings (`Tuple[torch.FloatTensor, torch.FloatTensor]`):\n+                Tuple containing the cosine and sine positional embeddings of shape `(batch_size, seq_len, head_dim)`,\n+                with `head_dim` being the embedding dimension of each attention head.\n+            attention_mask (`torch.FloatTensor`, *optional*):\n+                attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,\n+                query_sequence_length, key_sequence_length)` if default attention is used.\n+            output_attentions (`bool`, *optional*):\n+                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n+                returned tensors for more detail.\n+            use_cache (`bool`, *optional*):\n+                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n+                (see `past_key_values`).\n+            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+                Indices depicting the position of the input sequence tokens in the sequence\n+        \"\"\"\n+\n+        if self.is_sliding and attention_mask is not None:  # efficient SDPA and no padding\n+            # Flash-attn is a 2D tensor\n+            if self.config._attn_implementation == \"flash_attention_2\":\n+                if past_key_value is not None:  # when decoding\n+                    attention_mask = attention_mask[:, -self.sliding_window :]\n+            else:\n+                min_dtype = torch.finfo(hidden_states.dtype).min\n+                sliding_window_mask = torch.tril(\n+                    torch.ones_like(attention_mask, dtype=torch.bool), diagonal=-self.sliding_window\n+                )\n+                attention_mask = torch.where(sliding_window_mask, min_dtype, attention_mask)\n+                if attention_mask.shape[-1] <= 1:  # when decoding\n+                    attention_mask = attention_mask[:, :, :, -self.sliding_window :]\n+\n+        residual = hidden_states\n+\n+        hidden_states = self.input_layernorm(hidden_states)\n+\n+        # Self Attention\n+        hidden_states_attention, self_attn_weights, present_key_value = self.self_attn(\n+            hidden_states=hidden_states,\n+            position_embeddings=position_embeddings,\n+            attention_mask=attention_mask,\n+            past_key_value=past_key_value,\n+            output_attentions=output_attentions,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+        )\n+\n+        # Fully Connected\n+        hidden_states_mlp = self.mlp(hidden_states)\n+\n+        # Add everything together\n+        hidden_states = residual + hidden_states_attention + hidden_states_mlp\n+\n+        outputs = (hidden_states,)\n+\n+        if output_attentions:\n+            outputs += (self_attn_weights,)\n+\n+        if use_cache:\n+            outputs += (present_key_value,)\n+\n+        return outputs\n+\n+\n+COHERE2_START_DOCSTRING = r\"\"\"\n+    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n+    library implements for all its model (such as downloading or saving, resizing the input embeddings etc.).\n+\n+    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n+    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n+    and behavior.\n+\n+    Parameters:\n+        config ([`Cohere2Config`]):\n+            Model configuration class with all the parameters of the model. Initializing with a config file does not\n+            load the weights associated with the model, only the configuration. Check out the\n+            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"The bare Cohere2 Model outputting raw hidden-states without any specific head on top.\",\n+    COHERE2_START_DOCSTRING,\n+)\n+class Cohere2PreTrainedModel(PreTrainedModel):\n+    config_class = Cohere2Config\n+    base_model_prefix = \"model\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"Cohere2DecoderLayer\"]\n+    _skip_keys_device_placement = [\"past_key_values\"]\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n+    _supports_cache_class = True\n+    _supports_quantized_cache = True\n+    _supports_static_cache = True\n+\n+    def _init_weights(self, module):\n+        std = self.config.initializer_range\n+        if isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+\n+\n+COHERE2_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n+            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n+            it.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n+\n+            - 1 for tokens that are **not masked**,\n+            - 0 for tokens that are **masked**.\n+\n+            [What are attention masks?](../glossary#attention-mask)\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see\n+            `past_key_values`).\n+\n+            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n+            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n+            information on the default strategy.\n+\n+            - 1 indicates the head is **not masked**,\n+            - 0 indicates the head is **masked**.\n+        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n+            config.n_positions - 1]`.\n+\n+            [What are position IDs?](../glossary#position-ids)\n+        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):\n+            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n+            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n+            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n+\n+            Two formats are allowed:\n+            - a [`~cache_utils.Cache`] instance, see our\n+            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n+            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n+            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n+            cache format.\n+\n+            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the\n+            legacy cache format will be returned.\n+\n+            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n+            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n+            of shape `(batch_size, sequence_length)`.\n+        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n+            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n+            model's internal embedding lookup matrix.\n+        use_cache (`bool`, *optional*):\n+            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n+            `past_key_values`).\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n+            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n+            the complete sequence length.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"The bare Cohere2 Model outputting raw hidden-states without any specific head on top.\",\n+    COHERE2_START_DOCSTRING,\n+)\n+class Cohere2Model(Cohere2PreTrainedModel):\n+    \"\"\"\n+    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`Cohere2DecoderLayer`]\n+    Args:\n+        config: Cohere2Config\n+    \"\"\"\n+\n+    def __init__(self, config: Cohere2Config):\n+        super().__init__(config)\n+        self.padding_idx = config.pad_token_id\n+        self.vocab_size = config.vocab_size\n+\n+        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n+        self.layers = nn.ModuleList(\n+            [Cohere2DecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+        self.norm = Cohere2LayerNorm(hidden_size=(config.hidden_size), eps=config.layer_norm_eps)\n+\n+        self.gradient_checkpointing = False\n+        if getattr(config, \"pretraining_tp\", 1) != 1:\n+            logger.warn(\"`pretraining_tp` is deprecated, please use `model.tensor_parallel` instead.\")\n+        self.rotary_emb = Cohere2RotaryEmbedding(config=config)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.embed_tokens\n+\n+    def set_input_embeddings(self, value):\n+        self.embed_tokens = value\n+\n+    @add_start_docstrings_to_model_forward(COHERE2_INPUTS_DOCSTRING)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[HybridCache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+    ) -> Union[Tuple, BaseModelOutputWithPast]:\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        use_cache = use_cache if use_cache is not None else self.config.use_cache\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if self.gradient_checkpointing and self.training and use_cache:\n+            logger.warning_once(\n+                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n+            )\n+            use_cache = False\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.embed_tokens(input_ids)\n+\n+        if use_cache and past_key_values is None and not self.training:\n+            batch_size, seq_len, _ = inputs_embeds.shape\n+            past_key_values = HybridCache(\n+                self.config,\n+                batch_size=batch_size,\n+                max_cache_len=seq_len,\n+                device=self.device,\n+                dtype=inputs_embeds.dtype,\n+            )\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n+        causal_mask = self._update_causal_mask(\n+            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+        )\n+        hidden_states = inputs_embeds\n+\n+        # create position embeddings to be shared across the decoder layers\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n+        # decoder layers\n+        all_hidden_states = () if output_hidden_states else None\n+        all_self_attns = () if output_attentions else None\n+\n+        for decoder_layer in self.layers:\n+            if output_hidden_states:\n+                all_hidden_states += (hidden_states,)\n+\n+            if self.gradient_checkpointing and self.training:\n+                layer_outputs = self._gradient_checkpointing_func(\n+                    decoder_layer.__call__,\n+                    hidden_states,\n+                    position_embeddings,\n+                    causal_mask,\n+                    past_key_values,\n+                    output_attentions,\n+                    use_cache,\n+                    cache_position,\n+                )\n+            else:\n+                layer_outputs = decoder_layer(\n+                    hidden_states,\n+                    position_embeddings=position_embeddings,\n+                    attention_mask=causal_mask,\n+                    past_key_value=past_key_values,\n+                    output_attentions=output_attentions,\n+                    use_cache=use_cache,\n+                    cache_position=cache_position,\n+                )\n+\n+            hidden_states = layer_outputs[0]\n+\n+            if output_attentions:\n+                all_self_attns += (layer_outputs[1],)\n+\n+        hidden_states = self.norm(hidden_states)\n+\n+        # add hidden states from the last decoder layer\n+        if output_hidden_states:\n+            all_hidden_states += (hidden_states,)\n+\n+        next_cache = past_key_values if use_cache else None\n+\n+        if not return_dict:\n+            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n+        return BaseModelOutputWithPast(\n+            last_hidden_state=hidden_states,\n+            past_key_values=next_cache,\n+            hidden_states=all_hidden_states,\n+            attentions=all_self_attns,\n+        )\n+\n+    @torch.no_grad()\n+    def _update_causal_mask(\n+        self,\n+        attention_mask: torch.Tensor,\n+        input_tensor: torch.Tensor,\n+        cache_position: torch.Tensor,\n+        past_key_values: HybridCache,\n+        output_attentions: bool,\n+    ):\n+        # Flash Attention currently doesn't support static cache but Cohere2 work only with static cache.\n+        # So we will pass in attention mask as is in any case, not only when ther's padding. Then we'll use its shape\n+        # to cut out keys/values trailing 0 used in static cache. This workaround should be compile compatible\n+        # as it doesn't cause dynamic control issues.\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            return attention_mask\n+\n+        dtype, device = input_tensor.dtype, input_tensor.device\n+        sequence_length = input_tensor.shape[1]\n+        if isinstance(past_key_values, HybridCache):\n+            target_length = past_key_values.get_max_cache_shape()\n+        else:\n+            target_length = attention_mask.shape[-1] if attention_mask is not None else input_tensor.shape[1]\n+\n+        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n+        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n+            attention_mask,\n+            sequence_length=sequence_length,\n+            target_length=target_length,\n+            dtype=dtype,\n+            device=device,\n+            cache_position=cache_position,\n+            batch_size=input_tensor.shape[0],\n+        )\n+        return causal_mask\n+\n+    @staticmethod\n+    def _prepare_4d_causal_attention_mask_with_cache_position(\n+        attention_mask: torch.Tensor,\n+        sequence_length: int,\n+        target_length: int,\n+        dtype: torch.dtype,\n+        device: torch.device,\n+        cache_position: torch.Tensor,\n+        batch_size: int,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+\n+        Args:\n+            attention_mask (`torch.Tensor`):\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n+                `(batch_size, 1, query_length, key_value_length)`.\n+            sequence_length (`int`):\n+                The sequence length being processed.\n+            target_length (`int`):\n+                The target length: when generating with static cache, the mask should be as long as the static cache,\n+                to account for the 0 padding, the part of the cache that is not filled yet.\n+            dtype (`torch.dtype`):\n+                The dtype to use for the 4D attention mask.\n+            device (`torch.device`):\n+                The device to plcae the 4D attention mask on.\n+            cache_position (`torch.Tensor`):\n+                Indices depicting the position of the input sequence tokens in the sequence.\n+            batch_size (`torch.Tensor`):\n+                Batch size.\n+        \"\"\"\n+        if attention_mask is not None and attention_mask.dim() == 4:\n+            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n+            causal_mask = attention_mask\n+        else:\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = torch.full(\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+            )\n+            if sequence_length != 1:\n+                causal_mask = torch.triu(causal_mask, diagonal=1)\n+            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n+            if attention_mask is not None:\n+                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+                mask_length = attention_mask.shape[-1]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = padding_mask == 0\n+                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                    padding_mask, min_dtype\n+                )\n+\n+        return causal_mask\n+\n+\n+# TODO: re-enable check: Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM with Llama->Cohere2\n+class Cohere2ForCausalLM(Cohere2PreTrainedModel, GenerationMixin):\n+    _tied_weights_keys = [\"lm_head.weight\"]\n+\n+    # Ignore copy\n+    def __init__(self, config: Cohere2Config):\n+        super().__init__(config)\n+        self.model = Cohere2Model(config)\n+        self.vocab_size = config.vocab_size\n+        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n+        self.logit_scale = config.logit_scale\n+        self.tie_word_embeddings = config.tie_word_embeddings\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.model.embed_tokens\n+\n+    def set_input_embeddings(self, value):\n+        self.model.embed_tokens = value\n+\n+    def get_output_embeddings(self):\n+        return self.lm_head\n+\n+    def set_output_embeddings(self, new_embeddings):\n+        self.lm_head = new_embeddings\n+\n+    def set_decoder(self, decoder):\n+        self.model = decoder\n+\n+    def get_decoder(self):\n+        return self.model\n+\n+    # Ignore copy\n+    @add_start_docstrings_to_model_forward(COHERE2_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        num_logits_to_keep: int = 0,\n+        **loss_kwargs,\n+    ) -> Union[Tuple, CausalLMOutputWithPast]:\n+        r\"\"\"\n+        Args:\n+            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+\n+            num_logits_to_keep (`int`, *optional*):\n+                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n+                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+\n+        Returns:\n+\n+        Example:\n+\n+        ```python\n+        >> from transformers import AutoTokenizer, Cohere2ForCausalLM\n+\n+        >> model = Cohere2ForCausalLM.from_pretrained(\"Cohere2ForAI/c4ai-command-r-v01\")\n+        >> tokenizer = AutoTokenizer.from_pretrained(\"Cohere2ForAI/c4ai-command-r-v01\")\n+\n+        >> prompt = \"Hey, are you conscious? Can you talk to me?\"\n+        >> inputs = tokenizer(prompt, return_tensors=\"pt\")\n+\n+        >> # Generate\n+        >> generate_ids = model.generate(inputs.input_ids, max_length=30)\n+        >> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n+        ```\"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n+        outputs = self.model(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+            cache_position=cache_position,\n+        )\n+\n+        hidden_states = outputs[0]\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n+        logits = logits * self.logit_scale\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(logits, labels, self.vocab_size, **loss_kwargs)\n+\n+        if not return_dict:\n+            output = (logits,) + outputs[1:]\n+            return (loss,) + output if loss is not None else output\n+\n+        return CausalLMOutputWithPast(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+    def prepare_inputs_for_generation(\n+        self,\n+        input_ids,\n+        past_key_values=None,\n+        attention_mask=None,\n+        inputs_embeds=None,\n+        cache_position=None,\n+        position_ids=None,\n+        use_cache=True,\n+        num_logits_to_keep=None,\n+        **kwargs,\n+    ):\n+        # Overwritten: has a special cache type, `HybridCache`\n+\n+        # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n+        # Exception 1: when passing input_embeds, input_ids may be missing entries\n+        # Exception 2: some generation methods do special slicing of input_ids, so we don't need to do it here\n+        if past_key_values is not None:\n+            if inputs_embeds is not None:  # Exception 1\n+                input_ids = input_ids[:, -cache_position.shape[0] :]\n+            elif input_ids.shape[1] != cache_position.shape[0]:  # Default case (the \"else\", a no op, is Exception 2)\n+                input_ids = input_ids[:, cache_position]\n+        if attention_mask is not None and position_ids is None:\n+            # create position_ids on the fly for batch generation\n+            position_ids = attention_mask.long().cumsum(-1) - 1\n+            position_ids.masked_fill_(attention_mask == 0, 1)\n+            if past_key_values:\n+                position_ids = position_ids[:, -input_ids.shape[1] :]\n+                # This `clone` call is needed to avoid recapturing cuda graphs with `torch.compile`'s\n+                # `mode=\"reduce-overhead`, as otherwise the input `position_ids` would have various stride\n+                # during the decoding. Here, simply using `.contiguous()` is not sufficient as in the\n+                # batch size = 1 case, `position_ids` is already contiguous but with varying stride\n+                # which retriggers a capture.\n+                position_ids = position_ids.clone(memory_format=torch.contiguous_format)\n+\n+        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n+        if inputs_embeds is not None and cache_position[0] == 0:\n+            model_inputs = {\"inputs_embeds\": inputs_embeds, \"input_ids\": None}\n+        else:\n+            # The clone here is for the same reason as for `position_ids`.\n+            model_inputs = {\"input_ids\": input_ids.clone(memory_format=torch.contiguous_format), \"inputs_embeds\": None}\n+\n+        if (\n+            isinstance(past_key_values, HybridCache)\n+            and attention_mask.ndim == 2\n+            and not self.config._attn_implementation == \"flash_attention_2\"\n+        ):\n+            if model_inputs[\"inputs_embeds\"] is not None:\n+                batch_size, sequence_length, _ = model_inputs[\"inputs_embeds\"].shape\n+                device = model_inputs[\"inputs_embeds\"].device\n+            else:\n+                batch_size, sequence_length = model_inputs[\"input_ids\"].shape\n+                device = model_inputs[\"input_ids\"].device\n+\n+            attention_mask = self.model._prepare_4d_causal_attention_mask_with_cache_position(\n+                attention_mask,\n+                sequence_length=sequence_length,\n+                target_length=past_key_values.get_max_cache_shape(),\n+                dtype=self.lm_head.weight.dtype,\n+                device=device,\n+                cache_position=cache_position,\n+                batch_size=batch_size,\n+            )\n+\n+        if num_logits_to_keep is not None:\n+            model_inputs[\"num_logits_to_keep\"] = num_logits_to_keep\n+\n+        model_inputs.update(\n+            {\n+                \"position_ids\": position_ids,\n+                \"cache_position\": cache_position,\n+                \"past_key_values\": past_key_values,\n+                \"use_cache\": use_cache,\n+                \"attention_mask\": attention_mask,\n+            }\n+        )\n+        return model_inputs\n+\n+\n+__all__ = [\"Cohere2ForCausalLM\", \"Cohere2Model\", \"Cohere2PreTrainedModel\"]"
        },
        {
            "sha": "3e6999b29bbfa119d69d883a9ce65d15533efec5",
            "filename": "src/transformers/models/cohere2/modular_cohere2.py",
            "status": "added",
            "additions": 744,
            "deletions": 0,
            "changes": 744,
            "blob_url": "https://github.com/huggingface/transformers/blob/64478c76313d20cac925aab5ad762a110e704774/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/64478c76313d20cac925aab5ad762a110e704774/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py?ref=64478c76313d20cac925aab5ad762a110e704774",
            "patch": "@@ -0,0 +1,744 @@\n+# coding=utf-8\n+# Copyright 2024 Cohere Inc. HuggingFace Inc. team. All rights reserved.\n+#\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import math\n+from typing import Optional, Tuple, Union\n+\n+import torch\n+import torch.nn as nn\n+import torch.utils.checkpoint\n+\n+from ...cache_utils import Cache, HybridCache\n+from ...configuration_utils import PretrainedConfig\n+from ...modeling_outputs import (\n+    BaseModelOutputWithPast,\n+)\n+from ...modeling_rope_utils import rope_config_validation\n+from ...utils import (\n+    is_flash_attn_2_available,\n+    logging,\n+)\n+from ..cohere.modeling_cohere import (\n+    CohereDecoderLayer,\n+    CohereForCausalLM,\n+    CohereLayerNorm,\n+    CoherePreTrainedModel,\n+    CohereRotaryEmbedding,\n+    apply_rotary_pos_emb,\n+    repeat_kv,\n+)\n+from ..gemma2.modeling_gemma2 import Gemma2Model\n+\n+\n+if is_flash_attn_2_available():\n+    from ...modeling_flash_attention_utils import _flash_attention_forward\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class Cohere2Config(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`CohereModel`]. It is used to instantiate an Cohere\n+    model according to the specified arguments, defining the model architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of the [CohereForAI/c4ai-command-r-v01](https://huggingface.co/CohereForAI/c4ai-command-r-v01) model.\n+\n+\n+    Args:\n+        vocab_size (`int`, *optional*, defaults to 256000):\n+            Vocabulary size of the Cohere model. Defines the number of different tokens that can be represented by the\n+            `inputs_ids` passed when calling [`CohereModel`]\n+        hidden_size (`int`, *optional*, defaults to 8192):\n+            Dimension of the hidden representations.\n+        intermediate_size (`int`, *optional*, defaults to 22528):\n+            Dimension of the MLP representations.\n+        logit_scale (`float`, *optional*, defaults to 0.0625):\n+            The scaling factor for the output logits.\n+        num_hidden_layers (`int`, *optional*, defaults to 40):\n+            Number of hidden layers in the Transformer decoder.\n+        num_attention_heads (`int`, *optional*, defaults to 64):\n+            Number of attention heads for each attention layer in the Transformer decoder.\n+        num_key_value_heads (`int`, *optional*):\n+            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n+            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n+            `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n+            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n+            by meanpooling all the original heads within that group. For more details checkout [this\n+            paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n+            `num_attention_heads`.\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n+            The non-linear activation function (function or string) in the decoder.\n+        max_position_embeddings (`int`, *optional*, defaults to 8192):\n+            The maximum sequence length that this model might ever be used with.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        layer_norm_eps (`float`, *optional*, defaults to 1e-05):\n+            The epsilon used by the layer normalization.\n+        use_cache (`bool`, *optional*, defaults to `True`):\n+            Whether or not the model should return the last key/values attentions (not used by all models). Only\n+            relevant if `config.is_decoder=True`.\n+        pad_token_id (`int`, *optional*, defaults to 0):\n+            Padding token id.\n+        bos_token_id (`int`, *optional*, defaults to 5):\n+            Beginning of stream token id.\n+        eos_token_id (`int`, *optional*, defaults to 255001):\n+            End of stream token id.\n+        tie_word_embeddings (`bool`, *optional*, defaults to `True`):\n+            Whether to tie weight embeddings\n+        rope_theta (`float`, *optional*, defaults to 10000.0):\n+            The base period of the RoPE embeddings.\n+        rope_scaling (`Dict`, *optional*):\n+            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n+            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n+            accordingly.\n+            Expected contents:\n+                `rope_type` (`str`):\n+                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n+                    'llama3'], with 'default' being the original RoPE implementation.\n+                `factor` (`float`, *optional*):\n+                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n+                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n+                    original maximum pre-trained length.\n+                `original_max_position_embeddings` (`int`, *optional*):\n+                    Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n+                    pretraining.\n+                `attention_factor` (`float`, *optional*):\n+                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n+                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n+                    `factor` field to infer the suggested value.\n+                `beta_fast` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 32.\n+                `beta_slow` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 1.\n+                `short_factor` (`List[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `long_factor` (`List[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `low_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n+                `high_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n+        attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):\n+            Whether to use a bias in the query, key, value and output projection layers during self-attention.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        sliding_window (`int`, *optional*, defaults to 4096):\n+            Size of the sliding window attention context.\n+        sliding_window_pattern (`int`, *optional*, defaults to 4):\n+            Pattern for the sliding window attention.\n+        cache_implementation (`str`, *optional*, defaults to `\"hybrid\"`): the cache type to be used with `generate`.\n+\n+    ```python\n+    >>> from transformers import Cohere2Model, Cohere2Config\n+\n+    >>> # Initializing a Cohere Nextmodel configuration\n+    >>> configuration = Cohere2Config()\n+\n+    >>> # Initializing a model from the Cohere2 configuration\n+    >>> model = Cohere2Model(configuration) # doctest: +SKIP\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config # doctest: +SKIP\n+    ```\n+    \"\"\"\n+\n+    model_type = \"cohere2\"\n+    keys_to_ignore_at_inference = [\"past_key_values\"]\n+\n+    def __init__(\n+        self,\n+        vocab_size=256000,\n+        hidden_size=8192,\n+        intermediate_size=22528,\n+        logit_scale=0.0625,\n+        num_hidden_layers=40,\n+        num_attention_heads=64,\n+        num_key_value_heads=None,\n+        hidden_act=\"silu\",\n+        max_position_embeddings=8192,\n+        initializer_range=0.02,\n+        layer_norm_eps=1e-5,\n+        use_cache=True,\n+        pad_token_id=0,\n+        bos_token_id=5,\n+        eos_token_id=255001,\n+        tie_word_embeddings=True,\n+        rope_theta=10000.0,\n+        rope_scaling=None,\n+        attention_bias=False,\n+        attention_dropout=0.0,\n+        sliding_window=4096,\n+        sliding_window_pattern=4,\n+        cache_implementation=\"hybrid\",\n+        **kwargs,\n+    ):\n+        self.vocab_size = vocab_size\n+        self.max_position_embeddings = max_position_embeddings\n+        self.hidden_size = hidden_size\n+        self.logit_scale = logit_scale\n+        self.intermediate_size = intermediate_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+\n+        # for backward compatibility\n+        if num_key_value_heads is None:\n+            num_key_value_heads = num_attention_heads\n+\n+        self.num_key_value_heads = num_key_value_heads\n+        self.hidden_act = hidden_act\n+        self.initializer_range = initializer_range\n+        self.layer_norm_eps = layer_norm_eps\n+        self.use_cache = use_cache\n+        self.rope_theta = rope_theta\n+        self.rope_scaling = rope_scaling\n+        self.attention_bias = attention_bias\n+        self.attention_dropout = attention_dropout\n+        self.sliding_window = sliding_window\n+        self.sliding_window_pattern = sliding_window_pattern\n+        # Need to specify head_dim in the config so it can be used in the attention forward functions\n+        self.head_dim = hidden_size // num_attention_heads\n+        self.cache_implementation = cache_implementation\n+\n+        # Validate the correctness of rotary position embeddings parameters\n+        rope_config_validation(self)\n+\n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n+\n+\n+class Cohere2RotaryEmbedding(CohereRotaryEmbedding):\n+    pass\n+\n+\n+class Cohere2LayerNorm(CohereLayerNorm):\n+    pass\n+\n+\n+def eager_attention_forward(\n+    config: Cohere2Config,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    mask: Optional[torch.Tensor],\n+    **_kwargs,\n+) -> Tuple[torch.Tensor, torch.Tensor]:\n+    key_states = repeat_kv(key, config.num_key_value_groups)\n+    value_states = repeat_kv(value, config.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) / math.sqrt(config.head_dim)\n+\n+    if mask is not None:  # no matter the length, we just slice it\n+        causal_mask = mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    # upcast attention to fp32\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=config.attention_dropout, training=config.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+    return attn_output, attn_weights\n+\n+\n+def flash_attention_forward(\n+    config: Cohere2Config,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    mask: Optional[torch.Tensor],\n+    target_dtype: torch.dtype = torch.float16,\n+    **_kwargs,\n+) -> Tuple[torch.Tensor, None]:\n+    if mask is not None:\n+        seq_len = mask.shape[1]\n+        query = query[:, :, :seq_len]\n+        value = value[:, :, :seq_len]\n+\n+    # TODO: These transpose are quite inefficient but Flash Attention requires the layout\n+    # [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor rotary embedding\n+    query_states = query.transpose(1, 2)\n+    key_states = key.transpose(1, 2)\n+    value_states = value.transpose(1, 2)\n+\n+    dropout_rate = config.attention_dropout if config.training else 0.0\n+\n+    input_dtype = query_states.dtype\n+    if input_dtype == torch.float32:\n+        query_states = query_states.to(target_dtype)\n+        key_states = key_states.to(target_dtype)\n+        value_states = value_states.to(target_dtype)\n+\n+    attn_output = _flash_attention_forward(\n+        query_states,\n+        key_states,\n+        value_states,\n+        mask,\n+        seq_len,\n+        dropout=dropout_rate,\n+        is_causal=config.is_causal,\n+        sliding_window=config.sliding_window,\n+        use_top_left_mask=config._flash_attn_uses_top_left_mask,\n+    )\n+\n+    return attn_output, None\n+\n+\n+def sdpa_attention_forward(\n+    config: Cohere2Config,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    mask: Optional[torch.Tensor],\n+    **_kwargs,\n+) -> Tuple[torch.Tensor, None]:\n+    key = repeat_kv(key, config.num_key_value_groups)\n+    value = repeat_kv(value, config.num_key_value_groups)\n+\n+    causal_mask = mask\n+    if mask is not None:\n+        causal_mask = causal_mask[:, :, :, : key.shape[-2]]\n+\n+    # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n+    # Reference: https://github.com/pytorch/pytorch/issues/112577.\n+    if query.device.type == \"cuda\" and causal_mask is not None:\n+        query = query.contiguous()\n+        key = key.contiguous()\n+        value = value.contiguous()\n+\n+    # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n+    # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n+    is_causal = True if causal_mask is None and query.shape[1] > 1 else False\n+\n+    attn_output = torch.nn.functional.scaled_dot_product_attention(\n+        query,\n+        key,\n+        value,\n+        attn_mask=causal_mask,\n+        dropout_p=config.attention_dropout if config.training else 0.0,\n+        is_causal=is_causal,\n+    )\n+\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+    return attn_output, None\n+\n+\n+COHERE2_ATTENTION_FUNCTION = {\n+    \"flash_attention_2\": flash_attention_forward,\n+    \"eager\": eager_attention_forward,\n+    \"sdpa\": sdpa_attention_forward,\n+}\n+\n+\n+class Cohere2Attention(nn.Module):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+\n+    def __init__(self, config: Cohere2Config, layer_idx: Optional[int] = None):\n+        super().__init__()\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        if layer_idx is None:\n+            logger.warning_once(\n+                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n+                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n+                \"when creating this class.\"\n+            )\n+\n+        self.attention_dropout = config.attention_dropout\n+        self.hidden_size = config.hidden_size\n+        self.num_heads = config.num_attention_heads\n+        self.head_dim = config.head_dim\n+        self.num_key_value_heads = config.num_key_value_heads\n+        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n+        self.max_position_embeddings = config.max_position_embeddings\n+        self.rope_theta = config.rope_theta\n+        self.is_causal = True\n+\n+        if (self.head_dim * self.num_heads) != self.hidden_size:\n+            raise ValueError(\n+                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n+                f\" and `num_heads`: {self.num_heads}).\"\n+            )\n+\n+        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n+        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n+        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n+        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=config.attention_bias)\n+\n+        self.sliding_window = (\n+            config.sliding_window if (self.layer_idx + 1) % self.config.sliding_window_pattern != 0 else None\n+        )\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor] = None,\n+        past_key_value: Optional[Cache] = None,\n+        output_attentions: bool = False,\n+        use_cache: bool = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+        bsz, q_len, _ = hidden_states.size()\n+\n+        query_states = self.q_proj(hidden_states)\n+        key_states = self.k_proj(hidden_states)\n+        value_states = self.v_proj(hidden_states)\n+\n+        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n+        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+\n+        cos, sin = position_embeddings\n+\n+        if self.sliding_window is not None:\n+            query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        if past_key_value is not None:\n+            cache_kwargs = {\n+                \"sin\": sin,\n+                \"cos\": cos,\n+                \"sliding_window\": self.sliding_window,\n+                \"cache_position\": cache_position,\n+            }\n+            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        if output_attentions and self.config._attn_implementation in [\"sdpa\", \"flash_attention_2\"]:\n+            logger.warning_once(\"Setting `attention_type` to `eager` because `output_attentions=True`\")\n+            attention_type = \"eager\"\n+        else:\n+            attention_type = self.config._attn_implementation\n+\n+        attn_output, attn_weights = COHERE2_ATTENTION_FUNCTION[attention_type](\n+            self, query_states, key_states, value_states, attention_mask, output_attentions=output_attentions\n+        )\n+\n+        attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+\n+        if not output_attentions:\n+            attn_weights = None\n+\n+        return attn_output, attn_weights, past_key_value\n+\n+\n+class Cohere2DecoderLayer(CohereDecoderLayer):\n+    def __init__(self, config: Cohere2Config, layer_idx: int):\n+        super().__init__(config, layer_idx)\n+        self.self_attn = Cohere2Attention(config, layer_idx)\n+        self.config = config\n+        self.is_sliding = (layer_idx + 1) % self.config.sliding_window_pattern != 0\n+        self.sliding_window = config.sliding_window\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor] = None,\n+        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n+        output_attentions: Optional[bool] = False,\n+        use_cache: Optional[bool] = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+        \"\"\"\n+        Args:\n+            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n+            position_embeddings (`Tuple[torch.FloatTensor, torch.FloatTensor]`):\n+                Tuple containing the cosine and sine positional embeddings of shape `(batch_size, seq_len, head_dim)`,\n+                with `head_dim` being the embedding dimension of each attention head.\n+            attention_mask (`torch.FloatTensor`, *optional*):\n+                attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,\n+                query_sequence_length, key_sequence_length)` if default attention is used.\n+            output_attentions (`bool`, *optional*):\n+                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n+                returned tensors for more detail.\n+            use_cache (`bool`, *optional*):\n+                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n+                (see `past_key_values`).\n+            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+                Indices depicting the position of the input sequence tokens in the sequence\n+        \"\"\"\n+\n+        if self.is_sliding and attention_mask is not None:  # efficient SDPA and no padding\n+            # Flash-attn is a 2D tensor\n+            if self.config._attn_implementation == \"flash_attention_2\":\n+                if past_key_value is not None:  # when decoding\n+                    attention_mask = attention_mask[:, -self.sliding_window :]\n+            else:\n+                min_dtype = torch.finfo(hidden_states.dtype).min\n+                sliding_window_mask = torch.tril(\n+                    torch.ones_like(attention_mask, dtype=torch.bool), diagonal=-self.sliding_window\n+                )\n+                attention_mask = torch.where(sliding_window_mask, min_dtype, attention_mask)\n+                if attention_mask.shape[-1] <= 1:  # when decoding\n+                    attention_mask = attention_mask[:, :, :, -self.sliding_window :]\n+\n+        residual = hidden_states\n+\n+        hidden_states = self.input_layernorm(hidden_states)\n+\n+        # Self Attention\n+        hidden_states_attention, self_attn_weights, present_key_value = self.self_attn(\n+            hidden_states=hidden_states,\n+            position_embeddings=position_embeddings,\n+            attention_mask=attention_mask,\n+            past_key_value=past_key_value,\n+            output_attentions=output_attentions,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+        )\n+\n+        # Fully Connected\n+        hidden_states_mlp = self.mlp(hidden_states)\n+\n+        # Add everything together\n+        hidden_states = residual + hidden_states_attention + hidden_states_mlp\n+\n+        outputs = (hidden_states,)\n+\n+        if output_attentions:\n+            outputs += (self_attn_weights,)\n+\n+        if use_cache:\n+            outputs += (present_key_value,)\n+\n+        return outputs\n+\n+\n+class Cohere2PreTrainedModel(CoherePreTrainedModel):\n+    config_class = Cohere2Config\n+\n+\n+class Cohere2Model(Gemma2Model):\n+    \"\"\"\n+    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`Cohere2DecoderLayer`]\n+    Args:\n+        config: Cohere2Config\n+    \"\"\"\n+\n+    def __init__(self, config: Cohere2Config):\n+        super().__init__(config)\n+        self.norm = Cohere2LayerNorm(hidden_size=(config.hidden_size), eps=config.layer_norm_eps)\n+        self.rotary_emb = Cohere2RotaryEmbedding(config=config)\n+\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[HybridCache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+    ) -> Union[Tuple, BaseModelOutputWithPast]:\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        use_cache = use_cache if use_cache is not None else self.config.use_cache\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if self.gradient_checkpointing and self.training and use_cache:\n+            logger.warning_once(\n+                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n+            )\n+            use_cache = False\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.embed_tokens(input_ids)\n+\n+        if use_cache and past_key_values is None and not self.training:\n+            batch_size, seq_len, _ = inputs_embeds.shape\n+            past_key_values = HybridCache(\n+                self.config,\n+                batch_size=batch_size,\n+                max_cache_len=seq_len,\n+                device=self.device,\n+                dtype=inputs_embeds.dtype,\n+            )\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n+        causal_mask = self._update_causal_mask(\n+            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+        )\n+        hidden_states = inputs_embeds\n+\n+        # create position embeddings to be shared across the decoder layers\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n+        # decoder layers\n+        all_hidden_states = () if output_hidden_states else None\n+        all_self_attns = () if output_attentions else None\n+\n+        for decoder_layer in self.layers:\n+            if output_hidden_states:\n+                all_hidden_states += (hidden_states,)\n+\n+            if self.gradient_checkpointing and self.training:\n+                layer_outputs = self._gradient_checkpointing_func(\n+                    decoder_layer.__call__,\n+                    hidden_states,\n+                    position_embeddings,\n+                    causal_mask,\n+                    past_key_values,\n+                    output_attentions,\n+                    use_cache,\n+                    cache_position,\n+                )\n+            else:\n+                layer_outputs = decoder_layer(\n+                    hidden_states,\n+                    position_embeddings=position_embeddings,\n+                    attention_mask=causal_mask,\n+                    past_key_value=past_key_values,\n+                    output_attentions=output_attentions,\n+                    use_cache=use_cache,\n+                    cache_position=cache_position,\n+                )\n+\n+            hidden_states = layer_outputs[0]\n+\n+            if output_attentions:\n+                all_self_attns += (layer_outputs[1],)\n+\n+        hidden_states = self.norm(hidden_states)\n+\n+        # add hidden states from the last decoder layer\n+        if output_hidden_states:\n+            all_hidden_states += (hidden_states,)\n+\n+        next_cache = past_key_values if use_cache else None\n+\n+        if not return_dict:\n+            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n+        return BaseModelOutputWithPast(\n+            last_hidden_state=hidden_states,\n+            past_key_values=next_cache,\n+            hidden_states=all_hidden_states,\n+            attentions=all_self_attns,\n+        )\n+\n+\n+class Cohere2ForCausalLM(CohereForCausalLM):\n+    def __init__(self, config: Cohere2Config):\n+        super().__init__(config)\n+\n+    def prepare_inputs_for_generation(\n+        self,\n+        input_ids,\n+        past_key_values=None,\n+        attention_mask=None,\n+        inputs_embeds=None,\n+        cache_position=None,\n+        position_ids=None,\n+        use_cache=True,\n+        num_logits_to_keep=None,\n+        **kwargs,\n+    ):\n+        # Overwritten: has a special cache type, `HybridCache`\n+\n+        # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n+        # Exception 1: when passing input_embeds, input_ids may be missing entries\n+        # Exception 2: some generation methods do special slicing of input_ids, so we don't need to do it here\n+        if past_key_values is not None:\n+            if inputs_embeds is not None:  # Exception 1\n+                input_ids = input_ids[:, -cache_position.shape[0] :]\n+            elif input_ids.shape[1] != cache_position.shape[0]:  # Default case (the \"else\", a no op, is Exception 2)\n+                input_ids = input_ids[:, cache_position]\n+        if attention_mask is not None and position_ids is None:\n+            # create position_ids on the fly for batch generation\n+            position_ids = attention_mask.long().cumsum(-1) - 1\n+            position_ids.masked_fill_(attention_mask == 0, 1)\n+            if past_key_values:\n+                position_ids = position_ids[:, -input_ids.shape[1] :]\n+                # This `clone` call is needed to avoid recapturing cuda graphs with `torch.compile`'s\n+                # `mode=\"reduce-overhead`, as otherwise the input `position_ids` would have various stride\n+                # during the decoding. Here, simply using `.contiguous()` is not sufficient as in the\n+                # batch size = 1 case, `position_ids` is already contiguous but with varying stride\n+                # which retriggers a capture.\n+                position_ids = position_ids.clone(memory_format=torch.contiguous_format)\n+\n+        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n+        if inputs_embeds is not None and cache_position[0] == 0:\n+            model_inputs = {\"inputs_embeds\": inputs_embeds, \"input_ids\": None}\n+        else:\n+            # The clone here is for the same reason as for `position_ids`.\n+            model_inputs = {\"input_ids\": input_ids.clone(memory_format=torch.contiguous_format), \"inputs_embeds\": None}\n+\n+        if (\n+            isinstance(past_key_values, HybridCache)\n+            and attention_mask.ndim == 2\n+            and not self.config._attn_implementation == \"flash_attention_2\"\n+        ):\n+            if model_inputs[\"inputs_embeds\"] is not None:\n+                batch_size, sequence_length, _ = model_inputs[\"inputs_embeds\"].shape\n+                device = model_inputs[\"inputs_embeds\"].device\n+            else:\n+                batch_size, sequence_length = model_inputs[\"input_ids\"].shape\n+                device = model_inputs[\"input_ids\"].device\n+\n+            attention_mask = self.model._prepare_4d_causal_attention_mask_with_cache_position(\n+                attention_mask,\n+                sequence_length=sequence_length,\n+                target_length=past_key_values.get_max_cache_shape(),\n+                dtype=self.lm_head.weight.dtype,\n+                device=device,\n+                cache_position=cache_position,\n+                batch_size=batch_size,\n+            )\n+\n+        if num_logits_to_keep is not None:\n+            model_inputs[\"num_logits_to_keep\"] = num_logits_to_keep\n+\n+        model_inputs.update(\n+            {\n+                \"position_ids\": position_ids,\n+                \"cache_position\": cache_position,\n+                \"past_key_values\": past_key_values,\n+                \"use_cache\": use_cache,\n+                \"attention_mask\": attention_mask,\n+            }\n+        )\n+        return model_inputs\n+\n+\n+__all__ = [\"Cohere2Config\", \"Cohere2ForCausalLM\", \"Cohere2Model\", \"Cohere2PreTrainedModel\"]"
        },
        {
            "sha": "c6057088b7d50684d3efa51f3311a67b58d744b3",
            "filename": "src/transformers/utils/dummy_pt_objects.py",
            "status": "modified",
            "additions": 21,
            "deletions": 0,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/64478c76313d20cac925aab5ad762a110e704774/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/64478c76313d20cac925aab5ad762a110e704774/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py?ref=64478c76313d20cac925aab5ad762a110e704774",
            "patch": "@@ -2237,6 +2237,27 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n+class Cohere2ForCausalLM(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n+class Cohere2Model(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n+class Cohere2PreTrainedModel(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n class ConditionalDetrForObjectDetection(metaclass=DummyObject):\n     _backends = [\"torch\"]\n "
        },
        {
            "sha": "d02dee553b4668f08fd7f576d3352506ee8de1e4",
            "filename": "tests/models/cohere/test_modeling_cohere.py",
            "status": "modified",
            "additions": 12,
            "deletions": 8,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/64478c76313d20cac925aab5ad762a110e704774/tests%2Fmodels%2Fcohere%2Ftest_modeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/64478c76313d20cac925aab5ad762a110e704774/tests%2Fmodels%2Fcohere%2Ftest_modeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcohere%2Ftest_modeling_cohere.py?ref=64478c76313d20cac925aab5ad762a110e704774",
            "patch": "@@ -40,6 +40,11 @@\n \n # Copied from transformers.tests.models.llama.LlamaModelTester with Llama->Cohere\n class CohereModelTester:\n+    config_class = CohereConfig\n+    if is_torch_available():\n+        model_class = CohereModel\n+        for_causal_lm_class = CohereForCausalLM\n+\n     def __init__(\n         self,\n         parent,\n@@ -51,7 +56,7 @@ def __init__(\n         use_labels=True,\n         vocab_size=99,\n         hidden_size=32,\n-        num_hidden_layers=2,\n+        num_hidden_layers=4,\n         num_attention_heads=4,\n         intermediate_size=37,\n         hidden_act=\"gelu\",\n@@ -115,7 +120,7 @@ def prepare_config_and_inputs(self):\n \n     # Ignore copy\n     def get_config(self):\n-        return CohereConfig(\n+        return self.config_class(\n             vocab_size=self.vocab_size,\n             hidden_size=self.hidden_size,\n             num_hidden_layers=self.num_hidden_layers,\n@@ -129,13 +134,12 @@ def get_config(self):\n             is_decoder=False,\n             initializer_range=self.initializer_range,\n             pad_token_id=self.pad_token_id,\n-            eos_token_id=self.pad_token_id,\n         )\n \n     def create_and_check_model(\n         self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n     ):\n-        model = CohereModel(config=config)\n+        model = self.model_class(config=config)\n         model.to(torch_device)\n         model.eval()\n         result = model(input_ids, attention_mask=input_mask)\n@@ -155,7 +159,7 @@ def create_and_check_model_as_decoder(\n         encoder_attention_mask,\n     ):\n         config.add_cross_attention = True\n-        model = CohereModel(config)\n+        model = self.model_class(config)\n         model.to(torch_device)\n         model.eval()\n         result = model(\n@@ -184,7 +188,7 @@ def create_and_check_for_causal_lm(\n         encoder_hidden_states,\n         encoder_attention_mask,\n     ):\n-        model = CohereForCausalLM(config=config)\n+        model = self.for_causal_lm_class(config=config)\n         model.to(torch_device)\n         model.eval()\n         result = model(input_ids, attention_mask=input_mask, labels=token_labels)\n@@ -204,7 +208,7 @@ def create_and_check_decoder_model_past_large_inputs(\n     ):\n         config.is_decoder = True\n         config.add_cross_attention = True\n-        model = CohereForCausalLM(config=config)\n+        model = self.for_causal_lm_class(config=config)\n         model.to(torch_device)\n         model.eval()\n \n@@ -281,7 +285,7 @@ class CohereModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMix\n     )\n     test_headmasking = False\n     test_pruning = False\n-    fx_compatible = True\n+    fx_compatible = False\n \n     # Need to use `0.8` instead of `0.9` for `test_cpu_offload`\n     # This is because we are hitting edge cases with the causal_mask buffer"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/cohere2/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/64478c76313d20cac925aab5ad762a110e704774/tests%2Fmodels%2Fcohere2%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/64478c76313d20cac925aab5ad762a110e704774/tests%2Fmodels%2Fcohere2%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcohere2%2F__init__.py?ref=64478c76313d20cac925aab5ad762a110e704774"
        },
        {
            "sha": "8e1a4834d1ed41b6fc28190d4cb81884df9aab86",
            "filename": "tests/models/cohere2/test_modeling_cohere2.py",
            "status": "added",
            "additions": 347,
            "deletions": 0,
            "changes": 347,
            "blob_url": "https://github.com/huggingface/transformers/blob/64478c76313d20cac925aab5ad762a110e704774/tests%2Fmodels%2Fcohere2%2Ftest_modeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/64478c76313d20cac925aab5ad762a110e704774/tests%2Fmodels%2Fcohere2%2Ftest_modeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcohere2%2Ftest_modeling_cohere2.py?ref=64478c76313d20cac925aab5ad762a110e704774",
            "patch": "@@ -0,0 +1,347 @@\n+# coding=utf-8\n+# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch Cohere2 model.\"\"\"\n+\n+import unittest\n+\n+from packaging import version\n+from parameterized import parameterized\n+from pytest import mark\n+\n+from transformers import AutoModelForCausalLM, AutoTokenizer, Cohere2Config, HybridCache, is_torch_available, pipeline\n+from transformers.generation.configuration_utils import GenerationConfig\n+from transformers.testing_utils import (\n+    require_flash_attn,\n+    require_read_token,\n+    require_torch,\n+    require_torch_gpu,\n+    slow,\n+    torch_device,\n+)\n+\n+from ...models.cohere.test_modeling_cohere import CohereModelTest, CohereModelTester\n+from ...test_configuration_common import ConfigTester\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+    from transformers import (\n+        Cohere2ForCausalLM,\n+        Cohere2Model,\n+    )\n+\n+\n+class Cohere2ModelTester(CohereModelTester):\n+    config_class = Cohere2Config\n+    if is_torch_available():\n+        model_class = Cohere2Model\n+        for_causal_lm_class = Cohere2ForCausalLM\n+\n+\n+@require_torch\n+class Cohere2ModelTest(CohereModelTest, unittest.TestCase):\n+    all_model_classes = (Cohere2Model, Cohere2ForCausalLM) if is_torch_available() else ()\n+    all_generative_model_classes = (Cohere2ForCausalLM,) if is_torch_available() else ()\n+    pipeline_model_mapping = (\n+        {\n+            \"feature-extraction\": Cohere2Model,\n+            \"text-generation\": Cohere2ForCausalLM,\n+        }\n+        if is_torch_available()\n+        else {}\n+    )\n+    _is_stateful = True\n+\n+    def setUp(self):\n+        self.model_tester = Cohere2ModelTester(self)\n+        self.config_tester = ConfigTester(self, config_class=Cohere2Config, hidden_size=37)\n+\n+    @unittest.skip(\"Failing because of unique cache (HybridCache)\")\n+    def test_model_outputs_equivalence(self, **kwargs):\n+        pass\n+\n+    @unittest.skip(\"Cohere2's forcefully disables sdpa due to softcapping\")\n+    def test_sdpa_can_dispatch_non_composite_models(self):\n+        pass\n+\n+    @parameterized.expand([(\"float16\",), (\"bfloat16\",), (\"float32\",)])\n+    @unittest.skip(\"Cohere2's eager attn/sdpa attn outputs are expected to be different\")\n+    def test_eager_matches_sdpa_inference(self):\n+        pass\n+\n+    @unittest.skip(\"Cohere2's eager attn/sdpa attn outputs are expected to be different\")\n+    def test_eager_matches_sdpa_generate(self):\n+        pass\n+\n+    @parameterized.expand([(\"random\",), (\"same\",)])\n+    @unittest.skip(\"Cohere2 has HybridCache which is not compatible with assisted decoding\")\n+    def test_assisted_decoding_matches_greedy_search(self, assistant_type):\n+        pass\n+\n+    @unittest.skip(\"Cohere2 has HybridCache which is not compatible with assisted decoding\")\n+    def test_prompt_lookup_decoding_matches_greedy_search(self, assistant_type):\n+        pass\n+\n+    @unittest.skip(\"Cohere2 has HybridCache which is not compatible with assisted decoding\")\n+    def test_assisted_decoding_sample(self):\n+        pass\n+\n+    @unittest.skip(\"Cohere2 has HybridCache which is not compatible with dola decoding\")\n+    def test_dola_decoding_sample(self):\n+        pass\n+\n+    @parameterized.expand([(1, False), (1, True), (4, False)])\n+    @unittest.skip(\"Cohere2 has HybridCache and doesn't support old tuple format at all\")\n+    def test_new_cache_format(self, num_beams, do_sample):\n+        pass\n+\n+    @unittest.skip(\"Cohere2 has HybridCache and doesn't support continue from past kv\")\n+    def test_generate_continue_from_past_key_values(self):\n+        pass\n+\n+    @unittest.skip(\"Cohere2 has HybridCache and doesn't support low_memory generation\")\n+    def test_beam_search_low_memory(self):\n+        pass\n+\n+    @unittest.skip(\"Cohere2 has HybridCache and doesn't support contrastive generation\")\n+    def test_contrastive_generate(self):\n+        pass\n+\n+    @unittest.skip(\"Cohere2 has HybridCache and doesn't support contrastive generation\")\n+    def test_contrastive_generate_dict_outputs_use_cache(self):\n+        pass\n+\n+    @unittest.skip(\"Cohere2 has HybridCache and doesn't support contrastive generation\")\n+    def test_contrastive_generate_low_memory(self):\n+        pass\n+\n+    @unittest.skip(\"Cohere2 has HybridCache and doesn't support StaticCache. Though it could, it shouldn't support.\")\n+    def test_generate_with_static_cache(self):\n+        pass\n+\n+    @unittest.skip(\"Cohere2 has HybridCache and doesn't support StaticCache. Though it could, it shouldn't support.\")\n+    def test_generate_from_inputs_embeds_with_static_cache(self):\n+        pass\n+\n+    # overwrite because HybridCache has fixed length for key/values\n+    def _check_attentions_for_generate(\n+        self, batch_size, attentions, min_length, max_length, config, use_cache=False, num_beam_groups=1\n+    ):\n+        self.assertIsInstance(attentions, tuple)\n+        self.assertListEqual(\n+            [isinstance(iter_attentions, tuple) for iter_attentions in attentions], [True] * len(attentions)\n+        )\n+        self.assertEqual(len(attentions), (max_length - min_length) * num_beam_groups)\n+\n+        for idx, iter_attentions in enumerate(attentions):\n+            tgt_len = min_length + idx if not use_cache else 1\n+            src_len = min_length + idx if not use_cache else max_length\n+\n+            expected_shape = (\n+                batch_size * num_beam_groups,\n+                config.num_attention_heads,\n+                tgt_len,\n+                src_len,\n+            )\n+            # check attn size\n+            self.assertListEqual(\n+                [layer_attention.shape for layer_attention in iter_attentions], [expected_shape] * len(iter_attentions)\n+            )\n+\n+    # overwrite because HybridCache has fixed length for key/values\n+    def _check_past_key_values_for_generate(self, batch_size, past_key_values, seq_length, config, num_beam_groups=1):\n+        self.assertIsInstance(past_key_values, HybridCache)\n+\n+        # check shape key, value (batch, head, max_seq_length, head_features)\n+        head_dim = config.head_dim if hasattr(config, \"head_dim\") else config.hidden_size // config.num_attention_heads\n+        num_key_value_heads = (\n+            config.num_attention_heads\n+            if getattr(config, \"num_key_value_heads\", None) is None\n+            else config.num_key_value_heads\n+        )\n+        num_hidden_layers = config.num_hidden_layers\n+\n+        # we should get `max_length` in shape, not `max_length - embeds_length`\n+        # `+1` because the test in Mixin subtracts 1 which is needed for tuple cache\n+        static_cache_shape = (batch_size, num_key_value_heads, seq_length + 1, head_dim)\n+        static_layers = [layer_idx for layer_idx, boolean in enumerate(past_key_values.is_sliding) if not boolean]\n+        self.assertTrue(len(past_key_values.key_cache) == num_hidden_layers)\n+        self.assertTrue(past_key_values.key_cache[static_layers[0]].shape == static_cache_shape)\n+\n+    @unittest.skip(\"Cohere2's eager attn/sdpa attn outputs are expected to be different\")\n+    def test_sdpa_equivalence(self):\n+        pass\n+\n+\n+@slow\n+@require_torch_gpu\n+class Cohere2IntegrationTest(unittest.TestCase):\n+    input_text = [\"Hello I am doing\", \"Hi today\"]\n+    # This variable is used to determine which CUDA device are we using for our runners (A10 or T4)\n+    # Depending on the hardware we get different logits / generations\n+    cuda_compute_capability_major_version = None\n+\n+    @classmethod\n+    def setUpClass(cls):\n+        if is_torch_available() and torch.cuda.is_available():\n+            # 8 is for A100 / A10 and 7 for T4\n+            cls.cuda_compute_capability_major_version = torch.cuda.get_device_capability()[0]\n+\n+    @require_read_token\n+    @unittest.skip(\"Cohere2 has not been released yet\")\n+    def test_model_bf16(self):\n+        model_id = \"CohereForAI/command-r7b-12-2024\"\n+        EXPECTED_TEXTS = [\n+            \"<BOS_TOKEN>Hello I am doing a project on the 1918 flu pandemic and I am trying to find out how many\",\n+            \"<PAD><PAD><BOS_TOKEN>Hi today I'm going to be talking about the history of the United States. The United States of America\",\n+        ]\n+\n+        model = AutoModelForCausalLM.from_pretrained(\n+            model_id, low_cpu_mem_usage=True, torch_dtype=torch.bfloat16, attn_implementation=\"eager\"\n+        ).to(torch_device)\n+\n+        tokenizer = AutoTokenizer.from_pretrained(model_id)\n+        inputs = tokenizer(self.input_text, return_tensors=\"pt\", padding=True).to(torch_device)\n+\n+        output = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n+        output_text = tokenizer.batch_decode(output, skip_special_tokens=False)\n+\n+        self.assertEqual(output_text, EXPECTED_TEXTS)\n+\n+    @require_read_token\n+    @unittest.skip(\"Cohere2 has not been released yet\")\n+    def test_model_fp16(self):\n+        model_id = \"CohereForAI/command-r7b-12-2024\"\n+        EXPECTED_TEXTS = [\n+            \"<BOS_TOKEN>Hello I am doing a project on the 1918 flu pandemic and I am trying to find out how many\",\n+            \"<PAD><PAD><BOS_TOKEN>Hi today I'm going to be talking about the history of the United States. The United States of America\",\n+        ]\n+\n+        model = AutoModelForCausalLM.from_pretrained(\n+            model_id, low_cpu_mem_usage=True, torch_dtype=torch.float16, attn_implementation=\"eager\"\n+        ).to(torch_device)\n+\n+        tokenizer = AutoTokenizer.from_pretrained(model_id)\n+        inputs = tokenizer(self.input_text, return_tensors=\"pt\", padding=True).to(torch_device)\n+\n+        output = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n+        output_text = tokenizer.batch_decode(output, skip_special_tokens=False)\n+\n+        self.assertEqual(output_text, EXPECTED_TEXTS)\n+\n+    @require_read_token\n+    @unittest.skip(\"Cohere2 has not been released yet\")\n+    def test_model_pipeline_bf16(self):\n+        # See https://github.com/huggingface/transformers/pull/31747 -- pipeline was broken for Cohere2 before this PR\n+        model_id = \"CohereForAI/command-r7b-12-2024\"\n+        # EXPECTED_TEXTS should match the same non-pipeline test, minus the special tokens\n+        EXPECTED_TEXTS = [\n+            \"Hello I am doing a project on the 1918 flu pandemic and I am trying to find out how many\",\n+            \"Hi today I'm going to be talking about the history of the United States. The United States of America\",\n+        ]\n+\n+        model = AutoModelForCausalLM.from_pretrained(\n+            model_id, low_cpu_mem_usage=True, torch_dtype=torch.bfloat16, attn_implementation=\"flex_attention\"\n+        ).to(torch_device)\n+        tokenizer = AutoTokenizer.from_pretrained(model_id)\n+        pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n+\n+        output = pipe(self.input_text, max_new_tokens=20, do_sample=False, padding=True)\n+\n+        self.assertEqual(output[0][0][\"generated_text\"], EXPECTED_TEXTS[0])\n+        self.assertEqual(output[1][0][\"generated_text\"], EXPECTED_TEXTS[1])\n+\n+    @require_read_token\n+    @require_flash_attn\n+    @require_torch_gpu\n+    @mark.flash_attn_test\n+    @slow\n+    @unittest.skip(\"Cohere2 has not been released yet\")\n+    def test_model_flash_attn(self):\n+        # See https://github.com/huggingface/transformers/issues/31953 --- flash attn was generating garbage for Gemma2, especially in long context\n+        model_id = \"CohereForAI/command-r7b-12-2024\"\n+        EXPECTED_TEXTS = [\n+            '<BOS_TOKEN>Hello I am doing a project on the 1918 flu pandemic and I am trying to find out how many people died in the United States. I have found a few sites that say 500,000 but I am not sure if that is correct. I have also found a site that says 675,000 but I am not sure if that is correct either. I am trying to find out how many people died in the United States. I have found a few',\n+            \"<PAD><PAD><BOS_TOKEN>Hi today I'm going to be talking about the history of the United States. The United States of America is a country in North America. It is the third largest country in the world by total area and the third most populous country with over 320 million people. The United States is a federal republic consisting of 50 states and a federal district. The 48 contiguous states and the district of Columbia are in central North America between Canada and Mexico. The state of Alaska is in the\"\n+        ]  # fmt: skip\n+\n+        model = AutoModelForCausalLM.from_pretrained(\n+            model_id, attn_implementation=\"flash_attention_2\", torch_dtype=\"float16\"\n+        ).to(torch_device)\n+        tokenizer = AutoTokenizer.from_pretrained(model_id)\n+        inputs = tokenizer(self.input_text, return_tensors=\"pt\", padding=True).to(torch_device)\n+\n+        output = model.generate(**inputs, max_new_tokens=100, do_sample=False)\n+        output_text = tokenizer.batch_decode(output, skip_special_tokens=False)\n+\n+        self.assertEqual(output_text, EXPECTED_TEXTS)\n+\n+    @slow\n+    @require_read_token\n+    @unittest.skip(\"Cohere2 has not been released yet\")\n+    def test_export_static_cache(self):\n+        if version.parse(torch.__version__) < version.parse(\"2.5.0\"):\n+            self.skipTest(reason=\"This test requires torch >= 2.5 to run.\")\n+\n+        from transformers.integrations.executorch import (\n+            TorchExportableModuleWithStaticCache,\n+            convert_and_export_with_cache,\n+        )\n+\n+        tokenizer = AutoTokenizer.from_pretrained(\n+            \"CohereForAI/command-r7b-12-2024\", pad_token=\"<PAD>\", padding_side=\"right\"\n+        )\n+        EXPECTED_TEXT_COMPLETION = [\n+            \"Hello I am doing a project for my school and I need to know how to make a program that will take a number\",\n+        ]\n+        max_generation_length = tokenizer(EXPECTED_TEXT_COMPLETION, return_tensors=\"pt\", padding=True)[\n+            \"input_ids\"\n+        ].shape[-1]\n+\n+        # Load model\n+        device = \"cpu\"\n+        dtype = torch.bfloat16\n+        cache_implementation = \"static\"\n+        attn_implementation = \"sdpa\"\n+        batch_size = 1\n+        model = AutoModelForCausalLM.from_pretrained(\n+            \"CohereForAI/command-r7b-12-2024\",\n+            device_map=device,\n+            torch_dtype=dtype,\n+            attn_implementation=attn_implementation,\n+            generation_config=GenerationConfig(\n+                use_cache=True,\n+                cache_implementation=cache_implementation,\n+                max_length=max_generation_length,\n+                cache_config={\n+                    \"batch_size\": batch_size,\n+                    \"max_cache_len\": max_generation_length,\n+                },\n+            ),\n+        )\n+\n+        prompts = [\"Hello I am doing\"]\n+        prompt_tokens = tokenizer(prompts, return_tensors=\"pt\", padding=True).to(model.device)\n+        prompt_token_ids = prompt_tokens[\"input_ids\"]\n+        max_new_tokens = max_generation_length - prompt_token_ids.shape[-1]\n+\n+        # Static Cache + export\n+        exported_program = convert_and_export_with_cache(model)\n+        ep_generated_ids = TorchExportableModuleWithStaticCache.generate(\n+            exported_program=exported_program, prompt_token_ids=prompt_token_ids, max_new_tokens=max_new_tokens\n+        )\n+        ep_generated_text = tokenizer.batch_decode(ep_generated_ids, skip_special_tokens=True)\n+        self.assertEqual(EXPECTED_TEXT_COMPLETION, ep_generated_text)"
        },
        {
            "sha": "a125387ff29268b5689460d822572ee0f6ac2f93",
            "filename": "utils/check_config_attributes.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/64478c76313d20cac925aab5ad762a110e704774/utils%2Fcheck_config_attributes.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/64478c76313d20cac925aab5ad762a110e704774/utils%2Fcheck_config_attributes.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_config_attributes.py?ref=64478c76313d20cac925aab5ad762a110e704774",
            "patch": "@@ -47,6 +47,7 @@\n     # `cache_implementation` should be in the default generation config, but we don't yet support per-model\n     # generation configs (TODO joao)\n     \"Gemma2Config\": [\"tie_word_embeddings\", \"cache_implementation\"],\n+    \"Cohere2Config\": [\"cache_implementation\"],\n     # used to compute the property `self.chunk_length`\n     \"EncodecConfig\": [\"overlap\"],\n     # used to compute the property `self.layers_block_type`"
        }
    ],
    "stats": {
        "total": 2517,
        "additions": 2508,
        "deletions": 9
    }
}