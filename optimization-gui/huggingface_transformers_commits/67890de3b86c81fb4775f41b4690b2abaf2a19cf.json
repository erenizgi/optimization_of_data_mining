{
    "author": "SunMarc",
    "message": "Torchao weights only + prequantized compability (#34355)\n\n* weights only compability\r\n\r\n* better tests from code review\r\n\r\n* ping torch version\r\n\r\n* add weights_only check",
    "sha": "67890de3b86c81fb4775f41b4690b2abaf2a19cf",
    "files": [
        {
            "sha": "f679f7a190f0a0b3b4b7fd8e68de36f6ea41883c",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/67890de3b86c81fb4775f41b4690b2abaf2a19cf/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/67890de3b86c81fb4775f41b4690b2abaf2a19cf/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=67890de3b86c81fb4775f41b4690b2abaf2a19cf",
            "patch": "@@ -3602,7 +3602,11 @@ def from_pretrained(\n \n         if hf_quantizer is not None:\n             hf_quantizer.validate_environment(\n-                torch_dtype=torch_dtype, from_tf=from_tf, from_flax=from_flax, device_map=device_map\n+                torch_dtype=torch_dtype,\n+                from_tf=from_tf,\n+                from_flax=from_flax,\n+                device_map=device_map,\n+                weights_only=weights_only,\n             )\n             torch_dtype = hf_quantizer.update_torch_dtype(torch_dtype)\n             device_map = hf_quantizer.update_device_map(device_map)"
        },
        {
            "sha": "e6c2dc1ce36b3f7c357a3282e3bcd07d59575ece",
            "filename": "src/transformers/quantizers/quantizer_torchao.py",
            "status": "modified",
            "additions": 19,
            "deletions": 0,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/67890de3b86c81fb4775f41b4690b2abaf2a19cf/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/67890de3b86c81fb4775f41b4690b2abaf2a19cf/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py?ref=67890de3b86c81fb4775f41b4690b2abaf2a19cf",
            "patch": "@@ -91,6 +91,15 @@ def validate_environment(self, *args, **kwargs):\n                     )\n                 else:\n                     self.offload = True\n+        if self.pre_quantized:\n+            weights_only = kwargs.get(\"weights_only\", None)\n+            if weights_only:\n+                torch_version = version.parse(importlib.metadata.version(\"torch\"))\n+                if torch_version < version.parse(\"2.5.0\"):\n+                    raise RuntimeError(\n+                        f\"In order to use torchao pre-quantized model, you need to have torch>=2.5.0. However, the current version is {torch_version}.\"\n+                        f\" You can also set with `weights_only=False` in `from_pretrained` if you don't want to update torch\"\n+                    )\n \n     def update_torch_dtype(self, torch_dtype):\n         if self.quantization_config.quant_type == \"int4_weight_only\":\n@@ -103,6 +112,10 @@ def update_torch_dtype(self, torch_dtype):\n                     \"Setting torch_dtype to torch.bfloat16 for int4_weight_only quantization since only bfloat16 is supported right now. Please set torch_dtype=torch.bfloat16 to remove this warning.\"\n                 )\n                 torch_dtype = torch.bfloat16\n+        if self.quantization_config.quant_type == \"int8_dynamic_activation_int8_weight\":\n+            if torch_dtype is None:\n+                # we need to set the torch_dtype, otherwise we have dtype mismatch when performing the quantized linear op\n+                torch_dtype = torch.float32\n         return torch_dtype\n \n     def adjust_target_dtype(self, target_dtype: \"torch.dtype\") -> \"torch.dtype\":\n@@ -198,6 +211,12 @@ def is_serializable(self, safe_serialization=None):\n         )\n         if not _is_torchao_serializable:\n             logger.warning(\"torchao quantized model is only serializable after huggingface_hub >= 0.25.0 \")\n+        if self.offload and self.quantization_config.modules_to_not_convert is None:\n+            logger.warning(\n+                \"The model contains offloaded modules and these modules are not quantized. We don't recommend saving the model as we won't be able to reload them.\"\n+                \"If you want to specify modules to not quantize, please specify modules_to_not_convert in the quantization_config.\"\n+            )\n+            return False\n         return _is_torchao_serializable\n \n     @property"
        },
        {
            "sha": "3733d6dcf42156d70679e0c811f5dacf6646db0f",
            "filename": "tests/quantization/torchao_integration/test_torchao.py",
            "status": "modified",
            "additions": 95,
            "deletions": 0,
            "changes": 95,
            "blob_url": "https://github.com/huggingface/transformers/blob/67890de3b86c81fb4775f41b4690b2abaf2a19cf/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/67890de3b86c81fb4775f41b4690b2abaf2a19cf/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py?ref=67890de3b86c81fb4775f41b4690b2abaf2a19cf",
            "patch": "@@ -14,6 +14,7 @@\n # limitations under the License.\n \n import gc\n+import tempfile\n import unittest\n \n from transformers import AutoModelForCausalLM, AutoTokenizer, TorchAoConfig\n@@ -236,5 +237,99 @@ def test_int8_dynamic_activation_int8_weight_quant(self):\n         self.assertEqual(tokenizer.decode(output[0], skip_special_tokens=True), EXPECTED_OUTPUT)\n \n \n+@require_torch_gpu\n+@require_torchao\n+class TorchAoSerializationTest(unittest.TestCase):\n+    input_text = \"What are we having for dinner?\"\n+    max_new_tokens = 10\n+    ORIGINAL_EXPECTED_OUTPUT = \"What are we having for dinner?\\n- 1. What is the temperature outside\"\n+    # TODO: investigate why we don't have the same output as the original model for this test\n+    SERIALIZED_EXPECTED_OUTPUT = \"What are we having for dinner?\\n\\nJessica: (smiling)\"\n+    model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n+    quant_config = TorchAoConfig(\"int4_weight_only\", group_size=32)\n+    device = \"cuda:0\"\n+\n+    # called only once for all test in this class\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.quantized_model = AutoModelForCausalLM.from_pretrained(\n+            cls.model_name,\n+            torch_dtype=torch.bfloat16,\n+            device_map=cls.device,\n+            quantization_config=cls.quant_config,\n+        )\n+        cls.tokenizer = AutoTokenizer.from_pretrained(cls.model_name)\n+\n+    def tearDown(self):\n+        gc.collect()\n+        torch.cuda.empty_cache()\n+        gc.collect()\n+\n+    def test_original_model_expected_output(self):\n+        input_ids = self.tokenizer(self.input_text, return_tensors=\"pt\").to(self.device)\n+        output = self.quantized_model.generate(**input_ids, max_new_tokens=self.max_new_tokens)\n+\n+        self.assertEqual(self.tokenizer.decode(output[0], skip_special_tokens=True), self.ORIGINAL_EXPECTED_OUTPUT)\n+\n+    def check_serialization_expected_output(self, device, expected_output):\n+        \"\"\"\n+        Test if we can serialize and load/infer the model again on the same device\n+        \"\"\"\n+        with tempfile.TemporaryDirectory() as tmpdirname:\n+            self.quantized_model.save_pretrained(tmpdirname, safe_serialization=False)\n+            loaded_quantized_model = AutoModelForCausalLM.from_pretrained(\n+                self.model_name, torch_dtype=torch.bfloat16, device_map=self.device\n+            )\n+            input_ids = self.tokenizer(self.input_text, return_tensors=\"pt\").to(self.device)\n+\n+            output = loaded_quantized_model.generate(**input_ids, max_new_tokens=self.max_new_tokens)\n+            self.assertEqual(self.tokenizer.decode(output[0], skip_special_tokens=True), expected_output)\n+\n+    def test_serialization_expected_output(self):\n+        self.check_serialization_expected_output(self.device, self.SERIALIZED_EXPECTED_OUTPUT)\n+\n+\n+class TorchAoSerializationW8A8Test(TorchAoSerializationTest):\n+    quant_config = TorchAoConfig(\"int8_dynamic_activation_int8_weight\")\n+    ORIGINAL_EXPECTED_OUTPUT = \"What are we having for dinner?\\n\\nJessica: (smiling)\"\n+    SERIALIZED_EXPECTED_OUTPUT = ORIGINAL_EXPECTED_OUTPUT\n+    device = \"cuda:0\"\n+\n+\n+class TorchAoSerializationW8Test(TorchAoSerializationTest):\n+    quant_config = TorchAoConfig(\"int8_weight_only\")\n+    ORIGINAL_EXPECTED_OUTPUT = \"What are we having for dinner?\\n\\nJessica: (smiling)\"\n+    SERIALIZED_EXPECTED_OUTPUT = ORIGINAL_EXPECTED_OUTPUT\n+    device = \"cuda:0\"\n+\n+\n+class TorchAoSerializationW8A8CPUTest(TorchAoSerializationTest):\n+    quant_config = TorchAoConfig(\"int8_dynamic_activation_int8_weight\")\n+    ORIGINAL_EXPECTED_OUTPUT = \"What are we having for dinner?\\n\\nJessica: (smiling)\"\n+    SERIALIZED_EXPECTED_OUTPUT = ORIGINAL_EXPECTED_OUTPUT\n+    device = \"cpu\"\n+\n+    def test_serialization_expected_output_cuda(self):\n+        \"\"\"\n+        Test if we can serialize on device (cpu) and load/infer the model on cuda\n+        \"\"\"\n+        new_device = \"cuda:0\"\n+        self.check_serialization_expected_output(new_device, self.SERIALIZED_EXPECTED_OUTPUT)\n+\n+\n+class TorchAoSerializationW8CPUTest(TorchAoSerializationTest):\n+    quant_config = TorchAoConfig(\"int8_weight_only\")\n+    ORIGINAL_EXPECTED_OUTPUT = \"What are we having for dinner?\\n\\nJessica: (smiling)\"\n+    SERIALIZED_EXPECTED_OUTPUT = ORIGINAL_EXPECTED_OUTPUT\n+    device = \"cpu\"\n+\n+    def test_serialization_expected_output_cuda(self):\n+        \"\"\"\n+        Test if we can serialize on device (cpu) and load/infer the model on cuda\n+        \"\"\"\n+        new_device = \"cuda:0\"\n+        self.check_serialization_expected_output(new_device, self.SERIALIZED_EXPECTED_OUTPUT)\n+\n+\n if __name__ == \"__main__\":\n     unittest.main()"
        }
    ],
    "stats": {
        "total": 120,
        "additions": 119,
        "deletions": 1
    }
}