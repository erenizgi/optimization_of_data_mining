{
    "author": "Cyrilvallez",
    "message": "Clean-up kernel loading and dispatch (#40542)\n\n* clean\n\n* clean imporrts\n\n* fix imports\n\n* oups\n\n* more imports\n\n* more imports\n\n* more\n\n* move it to integrations\n\n* fix\n\n* style\n\n* fix doc",
    "sha": "f0e778112fe6438f25142960fc4a3781c5e32566",
    "files": [
        {
            "sha": "78432215d9e1ab067851949e975b203b79384ebc",
            "filename": "src/transformers/integrations/hub_kernels.py",
            "status": "modified",
            "additions": 64,
            "deletions": 7,
            "changes": 71,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0e778112fe6438f25142960fc4a3781c5e32566/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0e778112fe6438f25142960fc4a3781c5e32566/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py?ref=f0e778112fe6438f25142960fc4a3781c5e32566",
            "patch": "@@ -11,20 +11,26 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from typing import Union\n+import re\n+from functools import partial\n+from typing import Optional, Union\n+\n+from ..modeling_flash_attention_utils import lazy_import_flash_attention\n+from .flash_attention import flash_attention_forward\n \n \n try:\n     from kernels import (\n         Device,\n         LayerRepository,\n         Mode,\n+        get_kernel,\n         register_kernel_mapping,\n         replace_kernel_forward_from_hub,\n         use_kernel_forward_from_hub,\n     )\n \n-    _hub_kernels_available = True\n+    _kernels_available = True\n \n     _KERNEL_MAPPING: dict[str, dict[Union[Device, str], LayerRepository]] = {\n         \"MultiScaleDeformableAttention\": {\n@@ -82,8 +88,9 @@\n \n     register_kernel_mapping(_KERNEL_MAPPING)\n \n-\n except ImportError:\n+    _kernels_available = False\n+\n     # Stub to make decorators int transformers work when `kernels`\n     # is not installed.\n     def use_kernel_forward_from_hub(*args, **kwargs):\n@@ -104,16 +111,66 @@ def replace_kernel_forward_from_hub(*args, **kwargs):\n     def register_kernel_mapping(*args, **kwargs):\n         raise RuntimeError(\"register_kernel_mapping requires `kernels` to be installed. Run `pip install kernels`.\")\n \n-    _hub_kernels_available = False\n+\n+def is_kernel(attn_implementation: Optional[str]) -> bool:\n+    \"\"\"Check whether `attn_implementation` matches a kernel pattern from the hub.\"\"\"\n+    return (\n+        attn_implementation is not None\n+        and re.search(r\"^[^/:]+/[^/:]+(?:@[^/:]+)?(?::[^/:]+)?$\", attn_implementation) is not None\n+    )\n \n \n-def is_hub_kernels_available():\n-    return _hub_kernels_available\n+def load_and_register_kernel(attn_implementation: str) -> None:\n+    \"\"\"Load and register the kernel associated to `attn_implementation`.\"\"\"\n+    if not is_kernel(attn_implementation):\n+        return\n+    if not _kernels_available:\n+        raise ImportError(\"`kernels` is not installed. Please install it with `pip install kernels`.\")\n+\n+    # Need to be imported here as otherwise we have a circular import in `modeling_utils`\n+    from ..masking_utils import ALL_MASK_ATTENTION_FUNCTIONS\n+    from ..modeling_utils import ALL_ATTENTION_FUNCTIONS\n+\n+    attention_wrapper = None\n+    # FIXME: @ArthurZucker this is dirty, did not want to do a lof of extra work\n+    actual_attn_name = attn_implementation\n+    if \"|\" in attn_implementation:\n+        attention_wrapper, actual_attn_name = attn_implementation.split(\"|\")\n+        # `transformers` has wrapper for sdpa, paged, flash, flex etc.\n+        attention_wrapper = ALL_ATTENTION_FUNCTIONS.get(attention_wrapper)\n+    # Extract repo_id and kernel_name from the string\n+    if \":\" in actual_attn_name:\n+        repo_id, kernel_name = actual_attn_name.split(\":\")\n+        kernel_name = kernel_name.strip()\n+    else:\n+        repo_id = actual_attn_name\n+        kernel_name = None\n+    repo_id = repo_id.strip()\n+    # extract the rev after the @ if it exists\n+    repo_id, _, rev = repo_id.partition(\"@\")\n+    repo_id = repo_id.strip()\n+    rev = rev.strip() if rev else None\n+\n+    # Load the kernel from hub\n+    try:\n+        kernel = get_kernel(repo_id, revision=rev)\n+    except Exception as e:\n+        raise ValueError(f\"An error occured while trying to load from '{repo_id}': {e}.\")\n+    # correctly wrap the kernel\n+    if hasattr(kernel, \"flash_attn_varlen_func\"):\n+        if attention_wrapper is None:\n+            attention_wrapper = flash_attention_forward\n+        kernel_function = partial(attention_wrapper, implementation=kernel)\n+        lazy_import_flash_attention(kernel)\n+    elif kernel_name is not None:\n+        kernel_function = getattr(kernel, kernel_name)\n+    # Register the kernel as a valid attention\n+    ALL_ATTENTION_FUNCTIONS.register(attn_implementation, kernel_function)\n+    ALL_MASK_ATTENTION_FUNCTIONS.register(attn_implementation, ALL_MASK_ATTENTION_FUNCTIONS[\"flash_attention_2\"])\n \n \n __all__ = [\n     \"LayerRepository\",\n-    \"is_hub_kernels_available\",\n     \"use_kernel_forward_from_hub\",\n     \"register_kernel_mapping\",\n     \"replace_kernel_forward_from_hub\","
        },
        {
            "sha": "37554773a85fe7b85ccee107f58cdd48bca3ff3f",
            "filename": "src/transformers/modeling_flash_attention_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0e778112fe6438f25142960fc4a3781c5e32566/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0e778112fe6438f25142960fc4a3781c5e32566/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_flash_attention_utils.py?ref=f0e778112fe6438f25142960fc4a3781c5e32566",
            "patch": "@@ -126,10 +126,10 @@ def _lazy_define_process_function(flash_function):\n \n def lazy_import_flash_attention(implementation: Optional[str]):\n     \"\"\"\n-    Lazy loading flash attention and returning the respective functions + flags back\n+    Lazily import flash attention and return the respective functions + flags.\n \n-    NOTE: For fullgraph, this needs to be called before compile while no fullgraph can\n-          can work without preloading. See `_check_and_adjust_attn_implementation` in `modeling_utils`.\n+    NOTE: For fullgraph, this needs to be called before compile, while no fullgraph can\n+    work without preloading. See `load_and_register_kernel` in `integrations.hub_kernels`.\n     \"\"\"\n     global _flash_fn, _flash_varlen_fn, _pad_fn, _unpad_fn\n     if any(k is None for k in [_flash_fn, _flash_varlen_fn, _pad_fn, _unpad_fn]):"
        },
        {
            "sha": "973ee405cb3a0acb37558385da7fbdb158065556",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 27,
            "deletions": 80,
            "changes": 107,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0e778112fe6438f25142960fc4a3781c5e32566/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0e778112fe6438f25142960fc4a3781c5e32566/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=f0e778112fe6438f25142960fc4a3781c5e32566",
            "patch": "@@ -44,12 +44,6 @@\n from torch.distributions import constraints\n from torch.utils.checkpoint import checkpoint\n \n-from transformers.utils import is_torchao_available\n-\n-\n-if is_torchao_available():\n-    from torchao.quantization import Int4WeightOnlyConfig\n-\n from .configuration_utils import PretrainedConfig\n from .distributed import DistributedConfig\n from .dynamic_module_utils import custom_object_save\n@@ -61,6 +55,7 @@\n from .integrations.flash_attention import flash_attention_forward\n from .integrations.flash_paged import paged_attention_forward\n from .integrations.flex_attention import flex_attention_forward\n+from .integrations.hub_kernels import is_kernel, load_and_register_kernel\n from .integrations.sdpa_attention import sdpa_attention_forward\n from .integrations.sdpa_paged import sdpa_attention_paged_forward\n from .integrations.tensor_parallel import (\n@@ -73,17 +68,8 @@\n     verify_tp_plan,\n )\n from .loss.loss_utils import LOSS_MAPPING\n-from .masking_utils import ALL_MASK_ATTENTION_FUNCTIONS\n from .modeling_flash_attention_utils import lazy_import_flash_attention\n-from .pytorch_utils import (  # noqa: F401\n-    Conv1D,\n-    apply_chunking_to_forward,\n-    find_pruneable_heads_and_indices,\n-    id_tensor_storage,\n-    prune_conv1d_layer,\n-    prune_layer,\n-    prune_linear_layer,\n-)\n+from .pytorch_utils import id_tensor_storage\n from .quantizers import HfQuantizer\n from .quantizers.auto import get_hf_quantizer\n from .quantizers.quantizers_utils import get_module_from_name\n@@ -124,6 +110,7 @@\n     is_torch_npu_available,\n     is_torch_xla_available,\n     is_torch_xpu_available,\n+    is_torchao_available,\n     logging,\n )\n from .utils.generic import _CAN_RECORD_REGISTRY, GeneralInterface, OutputRecorder\n@@ -138,9 +125,8 @@\n from .utils.quantization_config import BitsAndBytesConfig, QuantizationMethod\n \n \n-XLA_USE_BF16 = os.environ.get(\"XLA_USE_BF16\", \"0\").upper()\n-XLA_DOWNCAST_BF16 = os.environ.get(\"XLA_DOWNCAST_BF16\", \"0\").upper()\n-\n+if is_torchao_available():\n+    from torchao.quantization import Int4WeightOnlyConfig\n \n if is_accelerate_available():\n     from accelerate import dispatch_model, infer_auto_device_map\n@@ -164,32 +150,14 @@\n     from safetensors.torch import load_file as safe_load_file\n     from safetensors.torch import save_file as safe_save_file\n \n+if is_peft_available():\n+    from .utils import find_adapter_config_file\n \n-if is_kernels_available():\n-    from kernels import get_kernel\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-\n-_init_weights = True\n-_is_quantized = False\n-_is_ds_init_called = False\n _torch_distributed_available = torch.distributed.is_available()\n-\n _is_dtensor_available = _torch_distributed_available and is_torch_greater_or_equal(\"2.5\")\n if _is_dtensor_available:\n     from torch.distributed.tensor import DTensor\n \n-\n-def is_local_dist_rank_0():\n-    return (\n-        torch.distributed.is_available()\n-        and torch.distributed.is_initialized()\n-        and int(os.environ.get(\"LOCAL_RANK\", \"-1\")) == 0\n-    )\n-\n-\n if is_sagemaker_mp_enabled():\n     import smdistributed.modelparallel.torch as smp\n     from smdistributed.modelparallel import __version__ as SMP_VERSION\n@@ -198,11 +166,24 @@ def is_local_dist_rank_0():\n else:\n     IS_SAGEMAKER_MP_POST_1_10 = False\n \n-if is_peft_available():\n-    from .utils import find_adapter_config_file\n \n+logger = logging.get_logger(__name__)\n \n+XLA_USE_BF16 = os.environ.get(\"XLA_USE_BF16\", \"0\").upper()\n+XLA_DOWNCAST_BF16 = os.environ.get(\"XLA_DOWNCAST_BF16\", \"0\").upper()\n SpecificPreTrainedModelType = TypeVar(\"SpecificPreTrainedModelType\", bound=\"PreTrainedModel\")\n+_init_weights = True\n+_is_quantized = False\n+_is_ds_init_called = False\n+\n+\n+def is_local_dist_rank_0():\n+    return (\n+        torch.distributed.is_available()\n+        and torch.distributed.is_initialized()\n+        and int(os.environ.get(\"LOCAL_RANK\", \"-1\")) == 0\n+    )\n+\n \n TORCH_INIT_FUNCTIONS = {\n     \"uniform_\": nn.init.uniform_,\n@@ -2801,44 +2782,10 @@ def _check_and_adjust_attn_implementation(\n             and is_kernels_available()\n         ):\n             applicable_attn_implementation = \"kernels-community/flash-attn\"\n-        if applicable_attn_implementation is not None and re.match(\n-            r\"^[^/:]+/[^/:]+(?:@[^/:]+)?(?::[^/:]+)?$\", applicable_attn_implementation\n-        ):\n-            if not is_kernels_available():\n-                raise ValueError(\"kernels is not installed. Please install it with `pip install kernels`.\")\n-            attention_wrapper = None\n-            # FIXME: @ArthurZucker this is dirty, did not want to do a lof of extra work\n-            actual_attn_name = applicable_attn_implementation\n-            if \"|\" in applicable_attn_implementation:\n-                attention_wrapper, actual_attn_name = applicable_attn_implementation.split(\"|\")\n-                # `transformers` has wrapper for sdpa, paged, flash, flex etc.\n-                attention_wrapper = ALL_ATTENTION_FUNCTIONS.get(attention_wrapper)\n-            # Extract repo_id and kernel_name from the string\n-            if \":\" in actual_attn_name:\n-                repo_id, kernel_name = actual_attn_name.split(\":\")\n-                kernel_name = kernel_name.strip()\n-            else:\n-                repo_id = actual_attn_name\n-                kernel_name = None\n-            repo_id = repo_id.strip()\n-            # extract the rev after the @ if it exists\n-            repo_id, _, rev = repo_id.partition(\"@\")\n-            repo_id = repo_id.strip()\n-            rev = rev.strip() if rev else None\n+        if is_kernel(applicable_attn_implementation):\n             try:\n-                kernel = get_kernel(repo_id, revision=rev)\n-                if hasattr(kernel, \"flash_attn_varlen_func\"):\n-                    if attention_wrapper is None:\n-                        attention_wrapper = flash_attention_forward\n-                    kernel_function = partial(attention_wrapper, implementation=kernel)\n-                    lazy_import_flash_attention(kernel)\n-                elif kernel_name is not None:\n-                    kernel_function = getattr(kernel, kernel_name)\n-                ALL_ATTENTION_FUNCTIONS.register(applicable_attn_implementation, kernel_function)\n-                ALL_MASK_ATTENTION_FUNCTIONS.register(\n-                    applicable_attn_implementation, ALL_MASK_ATTENTION_FUNCTIONS[\"flash_attention_2\"]\n-                )\n-                # log that we used kernel fallback\n+                load_and_register_kernel(applicable_attn_implementation)\n+                # log that we used kernel fallback if successful\n                 if attn_implementation == \"flash_attention_2\":\n                     logger.warning_once(\n                         \"You do not have `flash_attn` installed, using `kernels-community/flash-attn` from the `kernels` \"\n@@ -2848,8 +2795,8 @@ def _check_and_adjust_attn_implementation(\n                 if attn_implementation == \"flash_attention_2\":\n                     self._flash_attn_2_can_dispatch()  # will fail as fa2 is not available but raise the proper exception\n                 logger.warning_once(\n-                    f\"Could not find a kernel repository '{repo_id}' compatible with your device in the hub: {e}. Using \"\n-                    \"default attention implementation instead (sdpa if available, eager otherwise).\"\n+                    f\"Could not find a kernel matching `{applicable_attn_implementation}` compatible with your device in the \"\n+                    f\"hub:\\n{e}.\\nUsing default attention implementation instead (sdpa if available, eager otherwise).\"\n                 )\n                 try:\n                     self._sdpa_can_dispatch(is_init_check)"
        },
        {
            "sha": "2eaf655d01be5bdc999a23994561745fe4a2811f",
            "filename": "src/transformers/models/blip/modeling_blip_text.py",
            "status": "modified",
            "additions": 2,
            "deletions": 6,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0e778112fe6438f25142960fc4a3781c5e32566/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0e778112fe6438f25142960fc4a3781c5e32566/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py?ref=f0e778112fe6438f25142960fc4a3781c5e32566",
            "patch": "@@ -31,12 +31,8 @@\n     BaseModelOutputWithPoolingAndCrossAttentions,\n     CausalLMOutputWithCrossAttentions,\n )\n-from ...modeling_utils import (\n-    PreTrainedModel,\n-    apply_chunking_to_forward,\n-    find_pruneable_heads_and_indices,\n-    prune_linear_layer,\n-)\n+from ...modeling_utils import PreTrainedModel\n+from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import logging\n from ...utils.deprecation import deprecate_kwarg\n from .configuration_blip import BlipTextConfig"
        },
        {
            "sha": "ba29c6fe324d443124ef307cb55e5a0b06ff26d8",
            "filename": "src/transformers/models/bridgetower/modeling_bridgetower.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0e778112fe6438f25142960fc4a3781c5e32566/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0e778112fe6438f25142960fc4a3781c5e32566/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py?ref=f0e778112fe6438f25142960fc4a3781c5e32566",
            "patch": "@@ -34,8 +34,8 @@\n     ModelOutput,\n     SequenceClassifierOutput,\n )\n-from ...modeling_utils import PreTrainedModel, apply_chunking_to_forward\n-from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n+from ...modeling_utils import PreTrainedModel\n+from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import auto_docstring, logging, torch_int\n from ...utils.deprecation import deprecate_kwarg\n from .configuration_bridgetower import BridgeTowerConfig, BridgeTowerTextConfig, BridgeTowerVisionConfig"
        },
        {
            "sha": "85e2bde325e2291f6e80e81b27918f50f695afdb",
            "filename": "src/transformers/models/cvt/modeling_cvt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0e778112fe6438f25142960fc4a3781c5e32566/src%2Ftransformers%2Fmodels%2Fcvt%2Fmodeling_cvt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0e778112fe6438f25142960fc4a3781c5e32566/src%2Ftransformers%2Fmodels%2Fcvt%2Fmodeling_cvt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcvt%2Fmodeling_cvt.py?ref=f0e778112fe6438f25142960fc4a3781c5e32566",
            "patch": "@@ -24,7 +24,8 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...modeling_outputs import ImageClassifierOutputWithNoAttention, ModelOutput\n-from ...modeling_utils import PreTrainedModel, find_pruneable_heads_and_indices, prune_linear_layer\n+from ...modeling_utils import PreTrainedModel\n+from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import auto_docstring, logging\n from .configuration_cvt import CvtConfig\n "
        },
        {
            "sha": "3c9d259e82158d1095f406d1a65368a464c95fd3",
            "filename": "src/transformers/models/deprecated/mctct/modeling_mctct.py",
            "status": "modified",
            "additions": 2,
            "deletions": 6,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0e778112fe6438f25142960fc4a3781c5e32566/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Fmodeling_mctct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0e778112fe6438f25142960fc4a3781c5e32566/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Fmodeling_mctct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Fmodeling_mctct.py?ref=f0e778112fe6438f25142960fc4a3781c5e32566",
            "patch": "@@ -28,12 +28,8 @@\n from ....modeling_attn_mask_utils import _prepare_4d_attention_mask\n from ....modeling_layers import GradientCheckpointingLayer\n from ....modeling_outputs import BaseModelOutput, CausalLMOutput\n-from ....modeling_utils import (\n-    PreTrainedModel,\n-    apply_chunking_to_forward,\n-    find_pruneable_heads_and_indices,\n-    prune_linear_layer,\n-)\n+from ....modeling_utils import PreTrainedModel\n+from ....pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ....utils import logging\n from .configuration_mctct import MCTCTConfig\n "
        },
        {
            "sha": "5db366aa61974d93ced97c6103821bea59d97c3c",
            "filename": "src/transformers/models/esm/modeling_esm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 6,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0e778112fe6438f25142960fc4a3781c5e32566/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0e778112fe6438f25142960fc4a3781c5e32566/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py?ref=f0e778112fe6438f25142960fc4a3781c5e32566",
            "patch": "@@ -31,13 +31,9 @@\n     SequenceClassifierOutput,\n     TokenClassifierOutput,\n )\n-from ...modeling_utils import (\n-    ALL_ATTENTION_FUNCTIONS,\n-    PreTrainedModel,\n-    find_pruneable_heads_and_indices,\n-    prune_linear_layer,\n-)\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n+from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n from ...utils.generic import OutputRecorder, check_model_inputs\n from .configuration_esm import EsmConfig"
        },
        {
            "sha": "f0677d5f600d3c78cf12cb39409b1e5084216c12",
            "filename": "src/transformers/models/evolla/modeling_evolla.py",
            "status": "modified",
            "additions": 2,
            "deletions": 8,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0e778112fe6438f25142960fc4a3781c5e32566/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0e778112fe6438f25142960fc4a3781c5e32566/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py?ref=f0e778112fe6438f25142960fc4a3781c5e32566",
            "patch": "@@ -41,15 +41,9 @@\n     ModelOutput,\n )\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n-from ...modeling_utils import (\n-    ALL_ATTENTION_FUNCTIONS,\n-    ModuleUtilsMixin,\n-    PreTrainedModel,\n-    find_pruneable_heads_and_indices,\n-    get_parameter_dtype,\n-    prune_linear_layer,\n-)\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, ModuleUtilsMixin, PreTrainedModel, get_parameter_dtype\n from ...processing_utils import Unpack\n+from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import OutputRecorder, check_model_inputs"
        },
        {
            "sha": "cafd6e589adf885539c6c7016548ec325e799c99",
            "filename": "src/transformers/models/flava/modeling_flava.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0e778112fe6438f25142960fc4a3781c5e32566/src%2Ftransformers%2Fmodels%2Fflava%2Fmodeling_flava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0e778112fe6438f25142960fc4a3781c5e32566/src%2Ftransformers%2Fmodels%2Fflava%2Fmodeling_flava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflava%2Fmodeling_flava.py?ref=f0e778112fe6438f25142960fc4a3781c5e32566",
            "patch": "@@ -27,7 +27,8 @@\n from ...activations import ACT2FN\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n-from ...modeling_utils import PreTrainedModel, find_pruneable_heads_and_indices, prune_linear_layer\n+from ...modeling_utils import PreTrainedModel\n+from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import ModelOutput, auto_docstring, logging, torch_int\n from .configuration_flava import (\n     FlavaConfig,"
        },
        {
            "sha": "aeb817be706028a1bf1f95ad63e2b1e371889f54",
            "filename": "src/transformers/models/markuplm/modeling_markuplm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 7,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0e778112fe6438f25142960fc4a3781c5e32566/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Fmodeling_markuplm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0e778112fe6438f25142960fc4a3781c5e32566/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Fmodeling_markuplm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Fmodeling_markuplm.py?ref=f0e778112fe6438f25142960fc4a3781c5e32566",
            "patch": "@@ -32,13 +32,8 @@\n     SequenceClassifierOutput,\n     TokenClassifierOutput,\n )\n-from ...modeling_utils import (\n-    ALL_ATTENTION_FUNCTIONS,\n-    PreTrainedModel,\n-    apply_chunking_to_forward,\n-    find_pruneable_heads_and_indices,\n-    prune_linear_layer,\n-)\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import auto_docstring, can_return_tuple, logging\n from .configuration_markuplm import MarkupLMConfig\n "
        }
    ],
    "stats": {
        "total": 237,
        "additions": 110,
        "deletions": 127
    }
}