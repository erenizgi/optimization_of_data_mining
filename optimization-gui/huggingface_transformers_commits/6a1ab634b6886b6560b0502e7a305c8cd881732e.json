{
    "author": "gewenbin0992",
    "message": "qwen2.5vl: fix bugs when using flash2+bf16 or num_return_sequences>1 (#36083)\n\n* qwen2.5vl: fix bugs when using flash2+bf16 or num_return_sequences>1\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* add tests\n\n* fix test bugs\n\n* fix\n\n* fix failed tests\n\n* fix",
    "sha": "6a1ab634b6886b6560b0502e7a305c8cd881732e",
    "files": [
        {
            "sha": "4436b066107fe2b9710ef5d41fd13f9427d2c442",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/6a1ab634b6886b6560b0502e7a305c8cd881732e/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6a1ab634b6886b6560b0502e7a305c8cd881732e/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=6a1ab634b6886b6560b0502e7a305c8cd881732e",
            "patch": "@@ -2824,8 +2824,12 @@ def _contrastive_search(\n \n                 if not sequential:\n                     # Expands model inputs top_k times, for batched forward passes (akin to beam search).\n+                    # input_ids is required for expanding visual inputs in qwen2vl\n                     _, model_kwargs = self._expand_inputs_for_generation(\n-                        expand_size=top_k, is_encoder_decoder=self.config.is_encoder_decoder, **model_kwargs\n+                        input_ids=input_ids,\n+                        expand_size=top_k,\n+                        is_encoder_decoder=self.config.is_encoder_decoder,\n+                        **model_kwargs,\n                     )\n \n                 past_key_values = model_kwargs.get(\"past_key_values\")"
        },
        {
            "sha": "5fd887a3a53b61067c3f9d98312fe09fcfdea8a9",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 125,
            "deletions": 3,
            "changes": 128,
            "blob_url": "https://github.com/huggingface/transformers/blob/6a1ab634b6886b6560b0502e7a305c8cd881732e/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6a1ab634b6886b6560b0502e7a305c8cd881732e/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=6a1ab634b6886b6560b0502e7a305c8cd881732e",
            "patch": "@@ -26,7 +26,7 @@\n \n import math\n from dataclasses import dataclass\n-from typing import List, Optional, Tuple, Union\n+from typing import Any, Dict, List, Optional, Tuple, Union\n \n import torch\n import torch.nn as nn\n@@ -452,7 +452,7 @@ def get_window_index(self, grid_thw):\n     def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor) -> torch.Tensor:\n         \"\"\"\n         Args:\n-            hidden_states (`torch.Tensor` of shape `(batch_size, seq_len, hidden_size)`):\n+            hidden_states (`torch.Tensor` of shape `(seq_len, hidden_size)`):\n                 The final hidden states of the model.\n             grid_thw (`torch.Tensor` of shape `(num_images_or_videos, 3)`):\n                 The temporal, height and width of feature shape of each image in LLM.\n@@ -1459,7 +1459,7 @@ class Qwen2_5_VLCausalLMOutputWithPast(ModelOutput):\n class Qwen2_5_VLForConditionalGeneration(Qwen2_5_VLPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n     config_class = Qwen2_5_VLConfig\n-    _no_split_modules = [\"Qwen2VLDecoderLayer\", \"Qwen2_5_VLVisionBlock\"]\n+    _no_split_modules = [\"Qwen2_5_VLDecoderLayer\", \"Qwen2_5_VLVisionBlock\"]\n \n     def __init__(self, config):\n         super().__init__(config)\n@@ -1933,5 +1933,127 @@ def prepare_inputs_for_generation(\n         )\n         return model_inputs\n \n+    def _get_image_nums_and_video_nums(\n+        self,\n+        input_ids: Optional[torch.LongTensor],\n+    ) -> Tuple[torch.Tensor, torch.Tensor]:\n+        \"\"\"\n+        Get the number of images and videos for each sample to calculate the separation length of the sample tensor.\n+        These parameters are not passed through the processor to avoid unpredictable impacts from interface modifications.\n+\n+        Args:\n+            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n+                Indices of input sequence tokens in the vocabulary.\n+\n+        Returns:\n+            image_nums (`torch.LongTensor` of shape `(batch_size, num_images_sample)`)\n+            video_nums (`torch.LongTensor` of shape `(batch_size, num_videos_sample)`)\n+        \"\"\"\n+        image_token_id = self.config.image_token_id\n+        video_token_id = self.config.video_token_id\n+        vision_start_token_id = self.config.vision_start_token_id\n+\n+        vision_start_mask = input_ids == vision_start_token_id\n+        vision_first_mask = torch.roll(vision_start_mask, shifts=1, dims=1)\n+        image_mask = input_ids == image_token_id\n+        video_mask = input_ids == video_token_id\n+        image_nums = torch.sum(vision_first_mask & image_mask, dim=1)\n+        video_nums = torch.sum(vision_first_mask & video_mask, dim=1)\n+\n+        return image_nums, video_nums\n+\n+    def _expand_inputs_for_generation(\n+        self,\n+        expand_size: int = 1,\n+        is_encoder_decoder: bool = False,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        **model_kwargs,\n+    ) -> Tuple[torch.LongTensor, Dict[str, Any]]:\n+        # Overwritten -- Support for expanding tensors without a batch size dimension\n+        # e.g., pixel_values, image_grid_thw, pixel_values_videos, video_grid_thw, second_per_grid_t\n+        # pixel_values.shape[0] is sum(seqlen_images for samples)\n+        # image_grid_thw.shape[0] is sum(num_images for samples)\n+\n+        if expand_size == 1:\n+            return input_ids, model_kwargs\n+\n+        visual_keys = [\"pixel_values\", \"image_grid_thw\", \"pixel_values_videos\", \"video_grid_thw\", \"second_per_grid_ts\"]\n+\n+        def _expand_dict_for_generation_visual(dict_to_expand):\n+            image_grid_thw = model_kwargs.get(\"image_grid_thw\", None)\n+            video_grid_thw = model_kwargs.get(\"video_grid_thw\", None)\n+            image_nums, video_nums = self._get_image_nums_and_video_nums(input_ids)\n+\n+            def _repeat_interleave_samples(x, lengths, repeat_times):\n+                samples = torch.split(x, lengths)\n+                repeat_args = [repeat_times] + [1] * (x.dim() - 1)\n+                result = torch.cat([sample.repeat(*repeat_args) for sample in samples], dim=0)\n+                return result\n+\n+            for key in dict_to_expand:\n+                if key == \"pixel_values\":\n+                    # split images into samples\n+                    samples = torch.split(image_grid_thw, list(image_nums))\n+                    # compute the sequence length of images for each sample\n+                    lengths = [torch.prod(sample, dim=1).sum() for sample in samples]\n+                    dict_to_expand[key] = _repeat_interleave_samples(\n+                        dict_to_expand[key], lengths=lengths, repeat_times=expand_size\n+                    )\n+                elif key == \"image_grid_thw\":\n+                    # get the num of images for each sample\n+                    lengths = list(image_nums)\n+                    dict_to_expand[key] = _repeat_interleave_samples(\n+                        dict_to_expand[key], lengths=lengths, repeat_times=expand_size\n+                    )\n+                elif key == \"pixel_values_videos\":\n+                    samples = torch.split(video_grid_thw, list(video_nums))\n+                    lengths = [torch.prod(sample, dim=1).sum() for sample in samples]\n+                    dict_to_expand[key] = _repeat_interleave_samples(\n+                        dict_to_expand[key], lengths=lengths, repeat_times=expand_size\n+                    )\n+                elif key == \"video_grid_thw\":\n+                    lengths = list(video_nums)\n+                    dict_to_expand[key] = _repeat_interleave_samples(\n+                        dict_to_expand[key], lengths=lengths, repeat_times=expand_size\n+                    )\n+                elif key == \"second_per_grid_ts\":\n+                    if not isinstance(dict_to_expand[key], list):\n+                        raise TypeError(\n+                            f\"Expected value for key '{key}' to be a list, but got {type(dict_to_expand[key])} instead.\"\n+                        )\n+                    tensor = torch.tensor(dict_to_expand[key])\n+                    lengths = list(video_nums)\n+                    tensor = _repeat_interleave_samples(tensor, lengths=lengths, repeat_times=expand_size)\n+                    dict_to_expand[key] = tensor.tolist()\n+            return dict_to_expand\n+\n+        def _expand_dict_for_generation(dict_to_expand):\n+            for key in dict_to_expand:\n+                if (\n+                    key != \"cache_position\"\n+                    and dict_to_expand[key] is not None\n+                    and isinstance(dict_to_expand[key], torch.Tensor)\n+                    and key not in visual_keys\n+                ):\n+                    dict_to_expand[key] = dict_to_expand[key].repeat_interleave(expand_size, dim=0)\n+            return dict_to_expand\n+\n+        # input_ids is required for expanding visual inputs\n+        # If input_ids is unavailable, visual inputs will not be used; therefore, there is no need to expand visual inputs.\n+        if input_ids is not None and input_ids.numel() != 0:\n+            model_kwargs = _expand_dict_for_generation_visual(model_kwargs)\n+\n+        if input_ids is not None:\n+            input_ids = input_ids.repeat_interleave(expand_size, dim=0)\n+\n+        model_kwargs = _expand_dict_for_generation(model_kwargs)\n+\n+        if is_encoder_decoder:\n+            if model_kwargs.get(\"encoder_outputs\") is None:\n+                raise ValueError(\"If `is_encoder_decoder` is True, make sure that `encoder_outputs` is defined.\")\n+            model_kwargs[\"encoder_outputs\"] = _expand_dict_for_generation(model_kwargs[\"encoder_outputs\"])\n+\n+        return input_ids, model_kwargs\n+\n \n __all__ = [\"Qwen2_5_VLForConditionalGeneration\", \"Qwen2_5_VLModel\", \"Qwen2_5_VLPreTrainedModel\"]"
        },
        {
            "sha": "662b2555827fbaa39436677feda6974b995f290e",
            "filename": "src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6a1ab634b6886b6560b0502e7a305c8cd881732e/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6a1ab634b6886b6560b0502e7a305c8cd881732e/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py?ref=6a1ab634b6886b6560b0502e7a305c8cd881732e",
            "patch": "@@ -312,7 +312,7 @@ def get_window_index(self, grid_thw):\n     def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor) -> torch.Tensor:\n         \"\"\"\n         Args:\n-            hidden_states (`torch.Tensor` of shape `(batch_size, seq_len, hidden_size)`):\n+            hidden_states (`torch.Tensor` of shape `(seq_len, hidden_size)`):\n                 The final hidden states of the model.\n             grid_thw (`torch.Tensor` of shape `(num_images_or_videos, 3)`):\n                 The temporal, height and width of feature shape of each image in LLM.\n@@ -382,7 +382,7 @@ class Qwen2_5_VLCausalLMOutputWithPast(Qwen2VLCausalLMOutputWithPast):\n \n class Qwen2_5_VLForConditionalGeneration(Qwen2VLForConditionalGeneration):\n     config_class = Qwen2_5_VLConfig\n-    _no_split_modules = [\"Qwen2VLDecoderLayer\", \"Qwen2_5_VLVisionBlock\"]\n+    _no_split_modules = [\"Qwen2_5_VLDecoderLayer\", \"Qwen2_5_VLVisionBlock\"]\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "55a595f2d9644b83ae745f4a2e23f5acde277546",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 123,
            "deletions": 1,
            "changes": 124,
            "blob_url": "https://github.com/huggingface/transformers/blob/6a1ab634b6886b6560b0502e7a305c8cd881732e/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6a1ab634b6886b6560b0502e7a305c8cd881732e/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=6a1ab634b6886b6560b0502e7a305c8cd881732e",
            "patch": "@@ -21,7 +21,7 @@\n \n import math\n from dataclasses import dataclass\n-from typing import List, Optional, Tuple, Union\n+from typing import Any, Dict, List, Optional, Tuple, Union\n \n import torch\n import torch.nn as nn\n@@ -1796,5 +1796,127 @@ def prepare_inputs_for_generation(\n         )\n         return model_inputs\n \n+    def _get_image_nums_and_video_nums(\n+        self,\n+        input_ids: Optional[torch.LongTensor],\n+    ) -> Tuple[torch.Tensor, torch.Tensor]:\n+        \"\"\"\n+        Get the number of images and videos for each sample to calculate the separation length of the sample tensor.\n+        These parameters are not passed through the processor to avoid unpredictable impacts from interface modifications.\n+\n+        Args:\n+            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n+                Indices of input sequence tokens in the vocabulary.\n+\n+        Returns:\n+            image_nums (`torch.LongTensor` of shape `(batch_size, num_images_sample)`)\n+            video_nums (`torch.LongTensor` of shape `(batch_size, num_videos_sample)`)\n+        \"\"\"\n+        image_token_id = self.config.image_token_id\n+        video_token_id = self.config.video_token_id\n+        vision_start_token_id = self.config.vision_start_token_id\n+\n+        vision_start_mask = input_ids == vision_start_token_id\n+        vision_first_mask = torch.roll(vision_start_mask, shifts=1, dims=1)\n+        image_mask = input_ids == image_token_id\n+        video_mask = input_ids == video_token_id\n+        image_nums = torch.sum(vision_first_mask & image_mask, dim=1)\n+        video_nums = torch.sum(vision_first_mask & video_mask, dim=1)\n+\n+        return image_nums, video_nums\n+\n+    def _expand_inputs_for_generation(\n+        self,\n+        expand_size: int = 1,\n+        is_encoder_decoder: bool = False,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        **model_kwargs,\n+    ) -> Tuple[torch.LongTensor, Dict[str, Any]]:\n+        # Overwritten -- Support for expanding tensors without a batch size dimension\n+        # e.g., pixel_values, image_grid_thw, pixel_values_videos, video_grid_thw, second_per_grid_t\n+        # pixel_values.shape[0] is sum(seqlen_images for samples)\n+        # image_grid_thw.shape[0] is sum(num_images for samples)\n+\n+        if expand_size == 1:\n+            return input_ids, model_kwargs\n+\n+        visual_keys = [\"pixel_values\", \"image_grid_thw\", \"pixel_values_videos\", \"video_grid_thw\", \"second_per_grid_ts\"]\n+\n+        def _expand_dict_for_generation_visual(dict_to_expand):\n+            image_grid_thw = model_kwargs.get(\"image_grid_thw\", None)\n+            video_grid_thw = model_kwargs.get(\"video_grid_thw\", None)\n+            image_nums, video_nums = self._get_image_nums_and_video_nums(input_ids)\n+\n+            def _repeat_interleave_samples(x, lengths, repeat_times):\n+                samples = torch.split(x, lengths)\n+                repeat_args = [repeat_times] + [1] * (x.dim() - 1)\n+                result = torch.cat([sample.repeat(*repeat_args) for sample in samples], dim=0)\n+                return result\n+\n+            for key in dict_to_expand:\n+                if key == \"pixel_values\":\n+                    # split images into samples\n+                    samples = torch.split(image_grid_thw, list(image_nums))\n+                    # compute the sequence length of images for each sample\n+                    lengths = [torch.prod(sample, dim=1).sum() for sample in samples]\n+                    dict_to_expand[key] = _repeat_interleave_samples(\n+                        dict_to_expand[key], lengths=lengths, repeat_times=expand_size\n+                    )\n+                elif key == \"image_grid_thw\":\n+                    # get the num of images for each sample\n+                    lengths = list(image_nums)\n+                    dict_to_expand[key] = _repeat_interleave_samples(\n+                        dict_to_expand[key], lengths=lengths, repeat_times=expand_size\n+                    )\n+                elif key == \"pixel_values_videos\":\n+                    samples = torch.split(video_grid_thw, list(video_nums))\n+                    lengths = [torch.prod(sample, dim=1).sum() for sample in samples]\n+                    dict_to_expand[key] = _repeat_interleave_samples(\n+                        dict_to_expand[key], lengths=lengths, repeat_times=expand_size\n+                    )\n+                elif key == \"video_grid_thw\":\n+                    lengths = list(video_nums)\n+                    dict_to_expand[key] = _repeat_interleave_samples(\n+                        dict_to_expand[key], lengths=lengths, repeat_times=expand_size\n+                    )\n+                elif key == \"second_per_grid_ts\":\n+                    if not isinstance(dict_to_expand[key], list):\n+                        raise TypeError(\n+                            f\"Expected value for key '{key}' to be a list, but got {type(dict_to_expand[key])} instead.\"\n+                        )\n+                    tensor = torch.tensor(dict_to_expand[key])\n+                    lengths = list(video_nums)\n+                    tensor = _repeat_interleave_samples(tensor, lengths=lengths, repeat_times=expand_size)\n+                    dict_to_expand[key] = tensor.tolist()\n+            return dict_to_expand\n+\n+        def _expand_dict_for_generation(dict_to_expand):\n+            for key in dict_to_expand:\n+                if (\n+                    key != \"cache_position\"\n+                    and dict_to_expand[key] is not None\n+                    and isinstance(dict_to_expand[key], torch.Tensor)\n+                    and key not in visual_keys\n+                ):\n+                    dict_to_expand[key] = dict_to_expand[key].repeat_interleave(expand_size, dim=0)\n+            return dict_to_expand\n+\n+        # input_ids is required for expanding visual inputs\n+        # If input_ids is unavailable, visual inputs will not be used; therefore, there is no need to expand visual inputs.\n+        if input_ids is not None and input_ids.numel() != 0:\n+            model_kwargs = _expand_dict_for_generation_visual(model_kwargs)\n+\n+        if input_ids is not None:\n+            input_ids = input_ids.repeat_interleave(expand_size, dim=0)\n+\n+        model_kwargs = _expand_dict_for_generation(model_kwargs)\n+\n+        if is_encoder_decoder:\n+            if model_kwargs.get(\"encoder_outputs\") is None:\n+                raise ValueError(\"If `is_encoder_decoder` is True, make sure that `encoder_outputs` is defined.\")\n+            model_kwargs[\"encoder_outputs\"] = _expand_dict_for_generation(model_kwargs[\"encoder_outputs\"])\n+\n+        return input_ids, model_kwargs\n+\n \n __all__ = [\"Qwen2VLForConditionalGeneration\", \"Qwen2VLModel\", \"Qwen2VLPreTrainedModel\"]"
        },
        {
            "sha": "a0628e15b179a866dc6b2eff8e0e2a055a0ba788",
            "filename": "tests/models/qwen2_5_vl/test_modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 22,
            "deletions": 0,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/6a1ab634b6886b6560b0502e7a305c8cd881732e/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_modeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6a1ab634b6886b6560b0502e7a305c8cd881732e/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_modeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_modeling_qwen2_5_vl.py?ref=6a1ab634b6886b6560b0502e7a305c8cd881732e",
            "patch": "@@ -171,7 +171,9 @@ def prepare_config_and_inputs_for_common(self):\n         input_ids[:, -1] = self.pad_token_id\n         input_ids[input_ids == self.video_token_id] = self.pad_token_id\n         input_ids[input_ids == self.image_token_id] = self.pad_token_id\n+        input_ids[input_ids == self.vision_start_token_id] = self.pad_token_id\n         input_ids[:, self.num_image_tokens] = self.image_token_id\n+        input_ids[:, self.num_image_tokens - 1] = self.vision_start_token_id\n         labels = torch.zeros(\n             (self.batch_size, self.seq_length),\n             dtype=torch.long,\n@@ -426,6 +428,26 @@ def test_small_model_integration_test_batch(self):\n             EXPECTED_DECODED_TEXT,\n         )\n \n+    @slow\n+    def test_small_model_integration_test_expand(self):\n+        model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n+            \"Qwen/Qwen2.5-VL-7B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\n+        )\n+        text = self.processor.apply_chat_template(self.messages, tokenize=False, add_generation_prompt=True)\n+        inputs = self.processor(text=[text], images=[self.image], return_tensors=\"pt\").to(torch_device)\n+\n+        output = model.generate(**inputs, max_new_tokens=30, num_return_sequences=3)\n+\n+        EXPECTED_DECODED_TEXT = [\n+            'system\\nYou are a helpful assistant.\\nuser\\nWhat kind of dog is this?\\nassistant\\nThe dog in the picture appears to be a Labrador Retriever. Labradors are known for their friendly and intelligent nature, making them popular choices',\n+            'system\\nYou are a helpful assistant.\\nuser\\nWhat kind of dog is this?\\nassistant\\nThe dog in the picture appears to be a Labrador Retriever. Labradors are known for their friendly and intelligent nature, making them popular choices',\n+            'system\\nYou are a helpful assistant.\\nuser\\nWhat kind of dog is this?\\nassistant\\nThe dog in the picture appears to be a Labrador Retriever. Labradors are known for their friendly and intelligent nature, making them popular choices',\n+        ]  # fmt: skip\n+        self.assertEqual(\n+            self.processor.batch_decode(output, skip_special_tokens=True),\n+            EXPECTED_DECODED_TEXT,\n+        )\n+\n     @slow\n     def test_small_model_integration_test_batch_wo_image(self):\n         model = Qwen2_5_VLForConditionalGeneration.from_pretrained("
        },
        {
            "sha": "68f94bc8f8a0296cf3cec74738e02babc6c958ce",
            "filename": "tests/models/qwen2_vl/test_modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 22,
            "deletions": 0,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/6a1ab634b6886b6560b0502e7a305c8cd881732e/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6a1ab634b6886b6560b0502e7a305c8cd881732e/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py?ref=6a1ab634b6886b6560b0502e7a305c8cd881732e",
            "patch": "@@ -167,7 +167,9 @@ def prepare_config_and_inputs_for_common(self):\n         input_ids[:, -1] = self.pad_token_id\n         input_ids[input_ids == self.video_token_id] = self.pad_token_id\n         input_ids[input_ids == self.image_token_id] = self.pad_token_id\n+        input_ids[input_ids == self.vision_start_token_id] = self.pad_token_id\n         input_ids[:, self.num_image_tokens] = self.image_token_id\n+        input_ids[:, self.num_image_tokens - 1] = self.vision_start_token_id\n         labels = torch.zeros(\n             (self.batch_size, self.seq_length),\n             dtype=torch.long,\n@@ -435,6 +437,26 @@ def test_small_model_integration_test_batch(self):\n             EXPECTED_DECODED_TEXT,\n         )\n \n+    @slow\n+    def test_small_model_integration_test_expand(self):\n+        model = Qwen2VLForConditionalGeneration.from_pretrained(\n+            \"Qwen/Qwen2-VL-7B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\n+        )\n+        text = self.processor.apply_chat_template(self.messages, tokenize=False, add_generation_prompt=True)\n+        inputs = self.processor(text=[text], images=[self.image], return_tensors=\"pt\").to(torch_device)\n+\n+        output = model.generate(**inputs, max_new_tokens=30, num_return_sequences=3)\n+\n+        EXPECTED_DECODED_TEXT = [\n+            'system\\nYou are a helpful assistant.\\nuser\\nWhat kind of dog is this?\\nassistant\\nThe dog in the picture appears to be a Labrador Retriever. Labradors are known for their friendly and intelligent nature, making them popular choices',\n+            'system\\nYou are a helpful assistant.\\nuser\\nWhat kind of dog is this?\\nassistant\\nThe dog in the picture appears to be a Labrador Retriever. Labradors are known for their friendly and intelligent nature, making them popular choices',\n+            'system\\nYou are a helpful assistant.\\nuser\\nWhat kind of dog is this?\\nassistant\\nThe dog in the picture appears to be a Labrador Retriever. Labradors are known for their friendly and intelligent nature, making them popular choices',\n+        ]  # fmt: skip\n+        self.assertEqual(\n+            self.processor.batch_decode(output, skip_special_tokens=True),\n+            EXPECTED_DECODED_TEXT,\n+        )\n+\n     @slow\n     def test_small_model_integration_test_batch_wo_image(self):\n         model = Qwen2VLForConditionalGeneration.from_pretrained("
        }
    ],
    "stats": {
        "total": 306,
        "additions": 299,
        "deletions": 7
    }
}