{
    "author": "yao-matrix",
    "message": "extend fp_quant cases to xpu (#41833)\n\n* extend fp_quant UTs to xpu\n\nSigned-off-by: Yao, Matrix <matrix.yao@intel.com>\n\n* fix style\n\nSigned-off-by: Yao, Matrix <matrix.yao@intel.com>\n\n* Update tests/quantization/fp_quant_integration/test_fp_quant.py\n\nCo-authored-by: Yih-Dar <2521628+ydshieh@users.noreply.github.com>\n\n---------\n\nSigned-off-by: Yao, Matrix <matrix.yao@intel.com>\nCo-authored-by: Yih-Dar <2521628+ydshieh@users.noreply.github.com>\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>",
    "sha": "36b640562b11f54044841ae3665930023d4ff27d",
    "files": [
        {
            "sha": "b5c9f2c8179ff6be8620c8515ab7f0acc512c396",
            "filename": "src/transformers/quantizers/quantizer_fp_quant.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/36b640562b11f54044841ae3665930023d4ff27d/src%2Ftransformers%2Fquantizers%2Fquantizer_fp_quant.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/36b640562b11f54044841ae3665930023d4ff27d/src%2Ftransformers%2Fquantizers%2Fquantizer_fp_quant.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_fp_quant.py?ref=36b640562b11f54044841ae3665930023d4ff27d",
            "patch": "@@ -20,7 +20,7 @@\n if TYPE_CHECKING:\n     from ..modeling_utils import PreTrainedModel\n \n-from ..utils import is_fp_quant_available, is_qutlass_available, is_torch_available, logging\n+from ..utils import is_fp_quant_available, is_qutlass_available, is_torch_available, is_torch_xpu_available, logging\n from ..utils.quantization_config import QuantizationConfigMixin\n \n \n@@ -45,9 +45,9 @@ def __init__(self, quantization_config: QuantizationConfigMixin, **kwargs):\n         self.quantization_config = quantization_config\n \n     def validate_environment(self, device_map, **kwargs):\n-        if not torch.cuda.is_available():\n+        if not torch.cuda.is_available() and not is_torch_xpu_available():\n             raise NotImplementedError(\n-                \"FPQuant quantization is only supported on GPU. Please use a different quantizer.\"\n+                \"FPQuant quantization is only supported on GPU or Intel XPU. Please use a different quantizer.\"\n             )\n \n         if not is_qutlass_available() and not self.quantization_config.pseudoquantization:"
        },
        {
            "sha": "71291460934f7b04dd6d309bdc2d6956207e7be3",
            "filename": "tests/quantization/fp_quant_integration/test_fp_quant.py",
            "status": "modified",
            "additions": 16,
            "deletions": 15,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/36b640562b11f54044841ae3665930023d4ff27d/tests%2Fquantization%2Ffp_quant_integration%2Ftest_fp_quant.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/36b640562b11f54044841ae3665930023d4ff27d/tests%2Fquantization%2Ffp_quant_integration%2Ftest_fp_quant.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Ffp_quant_integration%2Ftest_fp_quant.py?ref=36b640562b11f54044841ae3665930023d4ff27d",
            "patch": "@@ -22,14 +22,14 @@\n     require_accelerate,\n     require_fp_quant,\n     require_qutlass,\n-    require_torch_gpu,\n-    require_torch_multi_gpu,\n+    require_torch_accelerator,\n+    require_torch_multi_accelerator,\n     slow,\n     torch_device,\n )\n \n \n-@require_torch_gpu\n+@require_torch_accelerator\n class FPQuantConfigTest(unittest.TestCase):\n     def test_to_dict(self):\n         \"\"\"\n@@ -53,7 +53,7 @@ def test_from_dict(self):\n \n \n @slow\n-@require_torch_gpu\n+@require_torch_accelerator\n @require_fp_quant\n @require_accelerate\n class FPQuantBaseTest(unittest.TestCase):\n@@ -64,7 +64,7 @@ class FPQuantBaseTest(unittest.TestCase):\n \n     EXPECTED_OUTPUT = \"1 2 3 4 5 6\"\n \n-    device_map = \"cuda\"\n+    device_map = torch_device\n \n     @classmethod\n     def getQuantizationConfig(cls):\n@@ -77,10 +77,10 @@ def setUpClass(cls):\n         Setup quantized model\n         \"\"\"\n \n-        quantization_config = cls.getQuantizationConfig()\n+        cls.quantization_config = cls.getQuantizationConfig()\n         cls.tokenizer = AutoTokenizer.from_pretrained(cls.model_name)\n         cls.quantized_model = AutoModelForCausalLM.from_pretrained(\n-            cls.model_name, device_map=cls.device_map, quantization_config=quantization_config\n+            cls.model_name, device_map=cls.device_map, quantization_config=cls.quantization_config\n         )\n \n     def tearDown(self):\n@@ -111,24 +111,25 @@ def test_save_pretrained(self):\n             output = model.generate(**input_ids, max_new_tokens=self.max_new_tokens)\n             self.assertEqual(self.tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUT)\n \n-    @require_torch_multi_gpu\n-    def test_quantized_model_multi_gpu(self):\n+    @require_torch_multi_accelerator\n+    def test_quantized_model_multi_accelerator(self):\n         \"\"\"\n-        Simple test that checks if the quantized model is working properly with multiple GPUs\n-        set CUDA_VISIBLE_DEVICES=0,1 if you have more than 2 GPUs\n+        Simple test that checks if the quantized model is working properly with multiple accelerators.\n+        Set CUDA_VISIBLE_DEVICES=0,1 if you have more than 2 CUDA GPUs. Or set ZE_AFFINITY_MASK=0,1\n+        if you have more than 2 Intel XPUs.\n         \"\"\"\n         input_ids = self.tokenizer(self.input_text, return_tensors=\"pt\").to(torch_device)\n-        quantization_config = FPQuantConfig()\n+\n         quantized_model = AutoModelForCausalLM.from_pretrained(\n-            self.model_name, device_map=\"auto\", quantization_config=quantization_config\n+            self.model_name, device_map=\"auto\", quantization_config=self.quantization_config\n         )\n         self.assertTrue(set(quantized_model.hf_device_map.values()) == {0, 1})\n \n         output = quantized_model.generate(**input_ids, max_new_tokens=self.max_new_tokens)\n         self.assertEqual(self.tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUT)\n \n-    @require_torch_multi_gpu\n-    def test_save_pretrained_multi_gpu(self):\n+    @require_torch_multi_accelerator\n+    def test_save_pretrained_multi_accelerator(self):\n         \"\"\"\n         Simple test that checks if the quantized model is working properly after being saved and loaded\n         \"\"\""
        }
    ],
    "stats": {
        "total": 37,
        "additions": 19,
        "deletions": 18
    }
}