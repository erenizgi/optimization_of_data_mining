{
    "author": "Cyrilvallez",
    "message": "ðŸš¨ðŸš¨ Remove all traces of legacy cache format (#41378)\n\n* remove\n\n* more\n\n* add back\n\n* tests\n\n* revert classes\n\n* tests\n\n* add exceptions\n\n* reapply modular\n\n* rename\n\n* oupsi\n\n* start with whisper\n\n* fix tests\n\n* fix\n\n* fix\n\n* fix\n\n* typing",
    "sha": "46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
    "files": [
        {
            "sha": "26355a539493efd0e604ab12e5b385eb4037e712",
            "filename": "docs/source/en/cache_explanation.md",
            "status": "modified",
            "additions": 0,
            "deletions": 28,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/docs%2Fsource%2Fen%2Fcache_explanation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/docs%2Fsource%2Fen%2Fcache_explanation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fcache_explanation.md?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -156,31 +156,3 @@ inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, ret\n generated_ids = model.generate(**inputs, use_cache=True, max_new_tokens=10)\n \n ```\n-\n-## Legacy cache format\n-\n-Before the [`Cache`] class, the cache used to be stored as a tuple of tuples of tensors. This format is dynamic because it grows as text is generated, similar to [`DynamicCache`].\n-\n-The legacy format is essentially the same data structure but organized differently.\n-\n-- It's a tuple of tuples, where each inner tuple contains the key and value tensors for a layer.\n-- The tensors have the same shape `[batch_size, num_heads, seq_len, head_dim]`.\n-- The format is less flexible and doesn't support features like quantization or offloading.\n-\n-If your project depends on this legacy format, we recommend to convert to [`DynamicCache`] with [`~DynamicCache.from_legacy_cache`]. Note that legacy cache format is deprecated and not used anymore in `Transformers`. You can convert back to tuple format with [`DynamicCache.to_legacy_cache`] functions, which is helpful if you have custom logic for manipulating a cache in a specific format.\n-\n-```py\n-import torch\n-from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache\n-\n-tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n-model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", dtype=torch.float16, device_map=\"auto\")\n-inputs = tokenizer(\"Hello, my name is\", return_tensors=\"pt\").to(model.device)\n-\n-# `return_dict_in_generate=True` is required to return the cache and `return_legacy_cache` forces the returned cache\n-# in the legacy format\n-generation_outputs = model.generate(**inputs, return_dict_in_generate=True, return_legacy_cache=True, max_new_tokens=5)\n-\n-cache = DynamicCache.from_legacy_cache(generation_outputs.past_key_values)\n-legacy_format_cache = cache.to_legacy_cache()\n-```"
        },
        {
            "sha": "239d3285ccd7decfe6b5d33a0a78fd63a0313609",
            "filename": "docs/source/en/internal/generation_utils.md",
            "status": "modified",
            "additions": 4,
            "deletions": 8,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/docs%2Fsource%2Fen%2Finternal%2Fgeneration_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/docs%2Fsource%2Fen%2Finternal%2Fgeneration_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Finternal%2Fgeneration_utils.md?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -248,19 +248,19 @@ A [`StoppingCriteria`] can be used to change when to stop generation (other than\n     - batch_select_indices\n \n [[autodoc]] DynamicCache\n-    - to_legacy_cache\n-    - from_legacy_cache\n+\n+[[autodoc]] StaticCache\n \n [[autodoc]] QuantizedCache\n \n+[[autodoc]] EncoderDecoderCache\n+\n [[autodoc]] QuantoQuantizedCache\n \n [[autodoc]] HQQQuantizedCache\n \n [[autodoc]] OffloadedCache\n \n-[[autodoc]] StaticCache\n-\n [[autodoc]] OffloadedStaticCache\n \n [[autodoc]] HybridCache\n@@ -269,10 +269,6 @@ A [`StoppingCriteria`] can be used to change when to stop generation (other than\n \n [[autodoc]] SlidingWindowCache\n \n-[[autodoc]] EncoderDecoderCache\n-    - to_legacy_cache\n-    - from_legacy_cache\n-\n ## Watermark Utils\n \n [[autodoc]] WatermarkingConfig"
        },
        {
            "sha": "a341f1b5d79957b29dfced5afa44ef5317d6f40e",
            "filename": "docs/source/en/modular_transformers.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/docs%2Fsource%2Fen%2Fmodular_transformers.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/docs%2Fsource%2Fen%2Fmodular_transformers.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodular_transformers.md?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -488,7 +488,7 @@ class LlamaForCausalLM(nn.Module):\n       input_ids: torch.LongTensor = None,\n       attention_mask: Optional[torch.Tensor] = None,\n       position_ids: Optional[torch.LongTensor] = None,\n-      past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n+      past_key_values: Optional[Cache] = None,\n       inputs_embeds: Optional[torch.FloatTensor] = None,\n       labels: Optional[torch.LongTensor] = None,\n       use_cache: Optional[bool] = None,\n@@ -514,7 +514,7 @@ class NewModelForCausalLM(LlamaForCausalLM):    |    class LlamaForCausalLM(nn.M\n                                                 |         input_ids: torch.LongTensor = None,\n                                                 |         attention_mask: Optional[torch.Tensor] = None,\n                                                 |         position_ids: Optional[torch.LongTensor] = None,\n-                                                |         past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = |None,\n+                                                |         past_key_values: Optional[Cache] = |None,\n                                                 |         inputs_embeds: Optional[torch.FloatTensor] = None,\n                                                 |         labels: Optional[torch.LongTensor] = None,\n                                                 |         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "821d9e2dd39e59e87de4e0eaa105213558f96db4",
            "filename": "docs/source/ko/cache_explanation.md",
            "status": "modified",
            "additions": 0,
            "deletions": 28,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/docs%2Fsource%2Fko%2Fcache_explanation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/docs%2Fsource%2Fko%2Fcache_explanation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fcache_explanation.md?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -158,31 +158,3 @@ inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, ret\n generated_ids = model.generate(**inputs, use_cache=True, max_new_tokens=10)\n \n ```\n-\n-\n-## ë ˆê±°ì‹œ ìºì‹œ í˜•ì‹[[legacy-cache-format]]\n-\n-[`Cache`] í´ëž˜ìŠ¤ ì´ì „ì—ëŠ” ìºì‹œê°€ í…ì„œì˜ íŠœí”Œì˜ íŠœí”Œë¡œ ì €ìž¥ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ í˜•ì‹ì€ í…ìŠ¤íŠ¸ê°€ ìƒì„±ë¨ì— ë”°ë¼ ì¦ê°€í•˜ê¸° ë•Œë¬¸ì— ë™ì ì´ë©°, [`DynamicCache`]ì™€ ìœ ì‚¬í•©ë‹ˆë‹¤.\n-\n-ë ˆê±°ì‹œ í˜•ì‹ì€ ë³¸ì§ˆì ìœ¼ë¡œ ë™ì¼í•œ ë°ì´í„° êµ¬ì¡°ì´ì§€ë§Œ ë‹¤ë¥´ê²Œ ì¡°ì§í™”ë˜ì—ˆìŠµë‹ˆë‹¤.\n-- ê° ë‚´ë¶€ íŠœí”Œì€ ë ˆì´ì–´ì˜ í‚¤ì™€ ê°’ í…ì„œë¥¼ í¬í•¨í•˜ëŠ” íŠœí”Œì˜ íŠœí”Œìž…ë‹ˆë‹¤.\n-- í…ì„œëŠ” ë™ì¼í•œ í˜•íƒœ `[batch_size, num_heads, seq_len, head_dim]`ë¥¼ ê°–ìŠµë‹ˆë‹¤.\n-- ì´ í˜•ì‹ì€ ëœ ìœ ì—°í•˜ë©° ì–‘ìží™”ë‚˜ ì˜¤í”„ë¡œë”©ê³¼ ê°™ì€ ê¸°ëŠ¥ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n-\n-í”„ë¡œì íŠ¸ê°€ ì´ ë ˆê±°ì‹œ í˜•ì‹ì— ì˜ì¡´í•œë‹¤ë©´, [`~DynamicCache.from_legacy_cache`]ë¥¼ ì‚¬ìš©í•˜ì—¬ [`DynamicCache`]ë¡œ ë³€í™˜í•˜ëŠ” ê²ƒì„ ê¶Œìž¥í•©ë‹ˆë‹¤. ë ˆê±°ì‹œ ìºì‹œ í˜•ì‹ì€ ì‚¬ìš©ì´ ì¤‘ë‹¨ë˜ì—ˆìœ¼ë©° `Transformers`ì—ì„œ ë” ì´ìƒ ì‚¬ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. íŠ¹ì • í˜•ì‹ì—ì„œ ìºì‹œë¥¼ ì¡°ìž‘í•˜ëŠ” ì»¤ìŠ¤í…€ ë¡œì§ì´ ìžˆëŠ” ê²½ìš° ë„ì›€ì´ ë˜ëŠ” [`DynamicCache.to_legacy_cache`] í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ íŠœí”Œ í˜•ì‹ìœ¼ë¡œ ë‹¤ì‹œ ë³€í™˜í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.\n-\n-```py\n-import torch\n-from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache\n-\n-tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n-model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", dtype=torch.float16, device_map=\"auto\")\n-inputs = tokenizer(\"Hello, my name is\", return_tensors=\"pt\").to(model.device)\n-\n-# ìºì‹œë¥¼ ë°˜í™˜í•˜ë ¤ë©´ `return_dict_in_generate=True`ê°€ í•„ìš”í•˜ê³  `return_legacy_cache`ëŠ” ë°˜í™˜ëœ ìºì‹œë¥¼\n-# ë ˆê±°ì‹œ í˜•ì‹ìœ¼ë¡œ ê°•ì œí•©ë‹ˆë‹¤\n-generation_outputs = model.generate(**inputs, return_dict_in_generate=True, return_legacy_cache=True, max_new_tokens=5)\n-\n-cache = DynamicCache.from_legacy_cache(generation_outputs.past_key_values)\n-legacy_format_cache = cache.to_legacy_cache()\n-```"
        },
        {
            "sha": "67c1374e50cd4e71d8568267fd6af46c3d4884ec",
            "filename": "docs/source/ko/internal/generation_utils.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/docs%2Fsource%2Fko%2Finternal%2Fgeneration_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/docs%2Fsource%2Fko%2Finternal%2Fgeneration_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Finternal%2Fgeneration_utils.md?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -232,8 +232,6 @@ generation_output[:2]\n     - batch_select_indices\n \n [[autodoc]] DynamicCache\n-    - to_legacy_cache\n-    - from_legacy_cache\n \n [[autodoc]] QuantizedCache\n "
        },
        {
            "sha": "57030093abe42710959aa0a138854d6929595d23",
            "filename": "examples/modular-transformers/configuration_new_model.py",
            "status": "modified",
            "additions": 15,
            "deletions": 5,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/examples%2Fmodular-transformers%2Fconfiguration_new_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/examples%2Fmodular-transformers%2Fconfiguration_new_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fconfiguration_new_model.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -6,7 +6,7 @@\n #                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n # Example where we only want to overwrite the defaults of an init\n \n-from ...configuration_utils import PreTrainedConfig\n+from ...configuration_utils import PreTrainedConfig, layer_type_validation\n \n \n class NewModelConfig(PreTrainedConfig):\n@@ -17,6 +17,7 @@ class NewModelConfig(PreTrainedConfig):\n     e.g. [google/new_model-7b](https://huggingface.co/google/new_model-7b)\n     Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n     documentation from [`PreTrainedConfig`] for more information.\n+\n     Args:\n         vocab_size (`int`, *optional*, defaults to 256000):\n             Vocabulary size of the NewModel model. Defines the number of different tokens that can be represented by the\n@@ -41,9 +42,6 @@ class NewModelConfig(PreTrainedConfig):\n             The attention head dimension.\n         hidden_act (`str` or `function`, *optional*, defaults to `\"gelu_pytorch_tanh\"`):\n             The legacy activation function. It is overwritten by the `hidden_activation`.\n-        hidden_activation (`str` or `function`, *optional*):\n-            The non-linear activation function (function or string) in the decoder. Will default to `\"gelu_pytorch_tanh\"`\n-            if not specified. `\"gelu_pytorch_tanh\"` uses an approximation of the `\"gelu\"` activation function.\n         max_position_embeddings (`int`, *optional*, defaults to 8192):\n             The maximum sequence length that this model might ever be used with.\n         initializer_range (`float`, *optional*, defaults to 0.02):\n@@ -67,6 +65,11 @@ class NewModelConfig(PreTrainedConfig):\n             Whether to use a bias in the query, key, value and output projection layers during self-attention.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n             The dropout ratio for the attention probabilities.\n+        layer_types (`list`, *optional*):\n+            Attention pattern for each layer.\n+        use_bidirectional_attention (`bool`, *optional*):\n+            If True, the model will attend to all text tokens instead of using a causal mask.\n+\n     ```python\n     >>> from transformers import NewModelModel, NewModelConfig\n     >>> # Initializing a NewModel new_model-7b style configuration\n@@ -116,6 +119,8 @@ def __init__(\n         rope_theta=10000.0,\n         attention_bias=False,\n         attention_dropout=0.0,\n+        use_bidirectional_attention=False,\n+        layer_types=None,\n         **kwargs,\n     ):\n         super().__init__(\n@@ -134,13 +139,18 @@ def __init__(\n         self.head_dim = head_dim\n         self.num_key_value_heads = num_key_value_heads\n         self.hidden_act = hidden_act\n-        self.hidden_activation = hidden_activation\n         self.initializer_range = initializer_range\n         self.rms_norm_eps = rms_norm_eps\n         self.use_cache = use_cache\n         self.rope_theta = rope_theta\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n+        self.use_bidirectional_attention = use_bidirectional_attention\n+\n+        self.layer_types = layer_types\n+        if self.layer_types is None:\n+            self.layer_types = [\"full_attention\" for _ in range(self.num_hidden_layers)]\n+        layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n     @property\n     def num_heads(self):"
        },
        {
            "sha": "a1a683bdc737d75f63748df06567b17489e242a9",
            "filename": "examples/modular-transformers/modeling_dummy_bert.py",
            "status": "modified",
            "additions": 4,
            "deletions": 17,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/examples%2Fmodular-transformers%2Fmodeling_dummy_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/examples%2Fmodular-transformers%2Fmodeling_dummy_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_dummy_bert.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -10,15 +10,15 @@\n from torch import nn\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, EncoderDecoderCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...masking_utils import create_causal_mask\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPastAndCrossAttentions, BaseModelOutputWithPoolingAndCrossAttentions\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n-from ...utils import TransformersKwargs, auto_docstring, is_torch_flex_attn_available, logging\n+from ...utils import TransformersKwargs, auto_docstring, is_torch_flex_attn_available\n from ...utils.generic import check_model_inputs\n from .configuration_dummy_bert import DummyBertConfig\n \n@@ -27,9 +27,6 @@\n     from ...integrations.flex_attention import make_flex_block_causal_mask\n \n \n-logger = logging.get_logger(__name__)\n-\n-\n class DummyBertEmbeddings(nn.Module):\n     \"\"\"Construct the embeddings from word, position and token_type embeddings.\"\"\"\n \n@@ -622,15 +619,8 @@ def forward(\n         else:\n             use_cache = False\n \n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n-            logger.warning_once(\n-                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n-            )\n-            return_legacy_cache = True\n-            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n+        if use_cache and past_key_values is None:\n+            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n@@ -679,9 +669,6 @@ def forward(\n         sequence_output = encoder_outputs.last_hidden_state\n         pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n \n-        if return_legacy_cache:\n-            encoder_outputs.past_key_values = encoder_outputs.past_key_values.to_legacy_cache()\n-\n         return BaseModelOutputWithPoolingAndCrossAttentions(\n             last_hidden_state=sequence_output,\n             pooler_output=pooled_output,"
        },
        {
            "sha": "2c9dbdf97f6dda1060241e97bc0b7feca14284be",
            "filename": "examples/modular-transformers/modeling_my_new_model2.py",
            "status": "modified",
            "additions": 9,
            "deletions": 1,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -137,7 +137,7 @@ def __init__(self, config: MyNewModel2Config, layer_idx: int):\n         self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n         self.scaling = self.head_dim**-0.5\n         self.attention_dropout = config.attention_dropout\n-        self.is_causal = True\n+        self.is_causal = not getattr(config, \"use_bidirectional_attention\", False)\n \n         self.q_proj = nn.Linear(\n             config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n@@ -206,6 +206,7 @@ def __init__(self, config: MyNewModel2Config, layer_idx: int):\n         self.mlp = MyNewModel2MLP(config)\n         self.input_layernorm = MyNewModel2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = MyNewModel2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.attention_type = config.layer_types[layer_idx]\n \n     def forward(\n         self,\n@@ -259,6 +260,13 @@ class MyNewModel2PreTrainedModel(PreTrainedModel):\n         \"attentions\": MyNewModel2Attention,\n     }\n \n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+\n+        # We initialize with 0s to be 1 centered as the RMSNorm here does (1 + weight)\n+        if \"RMSNorm\" in module.__class__.__name__:\n+            module.weight.data.zero_()\n+\n \n class MyNewModel2ForSequenceClassification(GenericForSequenceClassification, MyNewModel2PreTrainedModel):\n     pass"
        },
        {
            "sha": "36615e25c9977b00ae2e141a4d5054d453aaa3ac",
            "filename": "examples/modular-transformers/modeling_new_task_model.py",
            "status": "modified",
            "additions": 152,
            "deletions": 145,
            "changes": 297,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -5,22 +5,27 @@\n #                          modular_new_task_model.py file directly. One of our CI enforces this.\n #                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n from dataclasses import dataclass\n-from typing import ClassVar, Optional, Union\n+from typing import Callable, ClassVar, Optional, Union\n \n import torch\n from torch import nn\n \n-from ...cache_utils import Cache, StaticCache\n+from ...cache_utils import Cache\n+from ...configuration_utils import PreTrainedConfig\n from ...generation import GenerationMixin\n+from ...masking_utils import create_masks_for_generate\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutputWithPast\n from ...modeling_utils import PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import ModelOutput, auto_docstring, can_return_tuple\n+from ...utils import ModelOutput, auto_docstring, can_return_tuple, logging\n from ..auto import AutoModel\n from .configuration_new_task_model import NewTaskModelConfig\n \n \n+logger = logging.get_logger(__name__)\n+\n+\n @dataclass\n @auto_docstring(\n     custom_intro=\"\"\"\n@@ -29,12 +34,6 @@\n )\n class NewTaskModelModelOutputWithPast(BaseModelOutputWithPast):\n     r\"\"\"\n-    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n-\n-        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n-        `past_key_values` input) to speed up sequential decoding.\n     image_hidden_states (`torch.FloatTensor`, *optional*):\n         A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n         image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n@@ -56,8 +55,7 @@ class NewTaskModelCausalLMOutputWithPast(ModelOutput):\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.text_config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n     past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n         `past_key_values` input) to speed up sequential decoding.\n@@ -68,7 +66,7 @@ class NewTaskModelCausalLMOutputWithPast(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[Union[list[torch.FloatTensor], Cache]] = None\n+    past_key_values: Optional[Cache] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n     image_hidden_states: Optional[torch.FloatTensor] = None\n@@ -110,6 +108,109 @@ def _init_weights(self, module):\n                 module.bias.data.zero_()\n \n \n+def token_type_ids_mask_function(\n+    token_type_ids: Optional[torch.Tensor],\n+    image_group_ids: Optional[torch.Tensor],\n+) -> Optional[Callable]:\n+    \"\"\"\n+    This function adds the correct offsets to the `q_idx` and `kv_idx` as the torch API can only accept lengths,\n+    not start and end indices.\n+    \"\"\"\n+    # Do not return an additional mask in this case\n+    if token_type_ids is None:\n+        return None\n+\n+    def inner_mask(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool:\n+        # If it's 1 for both query and key/value, we are in an image block\n+        # NOTE: static cache shape goes beyond input seq length, while token_type_ids.shape[1] == input seq length\n+        # Since vmap doesn't support `if statement` we workaround it with `torch.where`\n+        safe_idx = torch.where(kv_idx < token_type_ids.shape[1], kv_idx, 0)\n+        token_type_ids_at_kv_idx = token_type_ids[batch_idx, safe_idx]\n+        token_type_ids_at_kv_idx = torch.where(kv_idx < token_type_ids.shape[1], token_type_ids_at_kv_idx, 0)\n+\n+        image_group_ids_at_kv_idx = image_group_ids[batch_idx, safe_idx]\n+        image_group_ids_at_kv_idx = torch.where(kv_idx < image_group_ids.shape[1], image_group_ids_at_kv_idx, -1)\n+\n+        is_image_block = (token_type_ids[batch_idx, q_idx] == 1) & (token_type_ids_at_kv_idx == 1)\n+        same_image_block = image_group_ids[batch_idx, q_idx] == image_group_ids_at_kv_idx\n+\n+        # This is bidirectional attention whenever we are dealing with image tokens\n+        return is_image_block & same_image_block\n+\n+    return inner_mask\n+\n+\n+def create_causal_mask_mapping(\n+    config: PreTrainedConfig,\n+    input_embeds: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    cache_position: torch.Tensor,\n+    past_key_values: Optional[Cache],\n+    position_ids: Optional[torch.Tensor],\n+    token_type_ids: Optional[torch.Tensor] = None,\n+    pixel_values: Optional[torch.FloatTensor] = None,\n+    is_training: bool = False,\n+    **kwargs,\n+) -> dict:\n+    \"\"\"\n+    Overwrites the base `create_masks_for_generate` with `token_type_ids` masking to create the causal mask mapping\n+    for all kinds of forward passes. NewTaskModel uses a bidirectional mask on the prompt tokens.\n+\n+    Uses `pixel_values` as an optional input to disambiguate edge cases.\n+    \"\"\"\n+    if is_training and token_type_ids is None:\n+        raise ValueError(\"`token_type_ids` is required as a model input when training\")\n+\n+    mask_kwargs = {\n+        \"config\": config.get_text_config(),\n+        \"input_embeds\": input_embeds,\n+        \"attention_mask\": attention_mask,\n+        \"cache_position\": cache_position,\n+        \"past_key_values\": past_key_values,\n+        \"position_ids\": position_ids,\n+    }\n+    # NOTE: this `is_prompt` logic is not flawless, it fails when we're using a cache eagerly initialized\n+    # (e.g. compiled prefill) AND `pixel_values` are not provided (i.e. the image data is provided through other\n+    # means). Determining prefill in that case requires checking data values, which is not compile-compatible.\n+    maybe_is_prompt = past_key_values is None or not past_key_values.is_initialized or pixel_values is not None\n+\n+    if maybe_is_prompt:\n+        if token_type_ids is not None:\n+            # The logic bellow was originally written for Gemma3, where `token_type_ids` is reversed. Let's reverse\n+            # it to then use exactly the same logic.\n+            token_type_ids = 1 - token_type_ids\n+        else:\n+            logger.warning_once(\n+                \"The input may be the prompt, but `token_type_ids` is not provided. We recommend \"\n+                \"passing `token_type_ids` to the model to prevent bad attention masking.\"\n+            )\n+            # BC: when NOT training, use bidirectional mask if sequence length > 1. Otherwise, use the default causal\n+            # mask. This is incorrect in some advanced use cases, hence the warning above.\n+            # NOTE: this branch can't be reached when training because `token_type_ids` is required as a model input.\n+            if input_embeds.shape[1] > 1:\n+                token_type_ids = torch.ones_like(input_embeds)[:, :, 0]\n+\n+    # Logic originally copied from Gemma3. It holds up for NewTaskModel as well because NewTaskModel assumes up to one image\n+    # per prompt AND we reverse `token_type_ids` above. Gemma3 uses a bidirectional mask for images, tagged through\n+    # `token_type_ids` 1s.\n+    if token_type_ids is not None and maybe_is_prompt:\n+        # We need to pass an additional mask function to account for token type ids, and it needs to be an `or` (to\n+        # undo the causal masking)\n+\n+        # First find where a new image block starts: 1 if image and previous not image\n+        # The images cannot attend to future images, but can attend to all prev images and to itself bidirectionally\n+        is_image = (token_type_ids == 1).to(cache_position.device)\n+        is_previous_image = nn.functional.pad(is_image, (1, 0), value=0)[:, :-1]\n+        new_image_start = is_image & ~is_previous_image\n+        image_group_ids = torch.cumsum(new_image_start.int(), dim=1) - 1\n+        image_group_ids = torch.where(is_image, image_group_ids, torch.full_like(token_type_ids, -1))\n+        mask_kwargs[\"or_mask_function\"] = token_type_ids_mask_function(\n+            token_type_ids.to(cache_position.device), image_group_ids\n+        )\n+\n+    return create_masks_for_generate(**mask_kwargs)\n+\n+\n @auto_docstring(\n     custom_intro=\"\"\"\n     The Base NewTaskModel model which consists of a vision backbone and a language model without language modeling head.,\n@@ -130,6 +231,7 @@ def __init__(self, config: NewTaskModelConfig):\n         self.language_model = language_model\n \n         self.pad_token_id = self.config.pad_token_id if self.config.pad_token_id is not None else -1\n+        self.text_config_dtype = self.config.get_text_config().dtype or self.dtype\n         self.post_init()\n \n     def get_input_embeddings(self):\n@@ -144,72 +246,6 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.language_model\n \n-    def _update_causal_mask(\n-        self,\n-        attention_mask,\n-        token_type_ids=None,\n-        past_key_values=None,\n-        cache_position=None,\n-        input_tensor=None,\n-        is_training: Optional[bool] = None,\n-    ):\n-        if self.config.text_config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and 0.0 in attention_mask:\n-                return attention_mask\n-            return None\n-        is_training = is_training if is_training is not None else self.training\n-        using_static_cache = isinstance(past_key_values, StaticCache)\n-        min_dtype = torch.finfo(self.dtype).min\n-        if input_tensor is None:\n-            input_tensor = attention_mask\n-\n-        inputs_lead_dim, sequence_length = input_tensor.shape[:2]\n-        if using_static_cache:\n-            target_length = past_key_values.get_max_cache_shape()\n-        else:\n-            target_length = (\n-                attention_mask.shape[-1]\n-                if isinstance(attention_mask, torch.Tensor)\n-                else cache_position[0] + sequence_length + 1\n-            )\n-\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            return attention_mask\n-\n-        causal_mask = torch.full(\n-            (sequence_length, target_length), fill_value=min_dtype, dtype=self.dtype, device=cache_position.device\n-        )\n-        # Causal diagonal mask only if training, otherwise attend to the whole prefix. Training-specific attn for prefix is handled below\n-        if sequence_length != 1:\n-            if is_training:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            else:\n-                causal_mask[:, :sequence_length] = 0.0\n-\n-        causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-        causal_mask = causal_mask[None, None, :, :].expand(inputs_lead_dim, 1, -1, -1)\n-        if attention_mask is not None:\n-            causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-            mask_length = attention_mask.shape[-1]\n-\n-            # First unmask prefix tokens during training\n-            if is_training:\n-                if token_type_ids is None:\n-                    raise ValueError(\"Token type ids must be provided during training\")\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    token_type_ids[:, None, None, :].to(causal_mask.device) == 0, 0\n-                )\n-\n-            # Then apply padding mask (will mask pad tokens)\n-            padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(causal_mask.device)\n-            padding_mask = padding_mask == 0\n-            causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                padding_mask, min_dtype\n-            )\n-\n-        return causal_mask\n-\n     def get_image_features(self, pixel_values: torch.FloatTensor):\n         \"\"\"\n         Obtains image last hidden states from the vision tower and apply multimodal projection.\n@@ -258,7 +294,7 @@ def forward(\n         pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[list[torch.FloatTensor], Cache]] = None,\n+        past_key_values: Optional[Cache] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n@@ -306,8 +342,6 @@ def forward(\n         )\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        is_training = token_type_ids is not None and labels is not None\n-\n         # Replace image id with PAD if the image token if OOV, to avoid index-errors\n         if input_ids is not None and self.config.image_token_id >= self.vocab_size:\n             special_image_mask = input_ids == self.config.image_token_id\n@@ -337,11 +371,22 @@ def forward(\n             )\n             inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask, token_type_ids, past_key_values, cache_position, inputs_embeds, is_training\n-        )\n+        # It may already have been prepared by e.g. `generate`\n+        if not isinstance(causal_mask_mapping := attention_mask, dict):\n+            causal_mask_mapping = create_causal_mask_mapping(\n+                self.config,\n+                inputs_embeds,\n+                attention_mask,\n+                cache_position,\n+                past_key_values,\n+                position_ids,\n+                token_type_ids,\n+                pixel_values,\n+                is_training=self.training,\n+            )\n+\n         outputs = self.language_model(\n-            attention_mask=causal_mask,\n+            attention_mask=causal_mask_mapping,\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n@@ -425,7 +470,7 @@ def forward(\n         pixel_values: torch.FloatTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[list[torch.FloatTensor], Cache]] = None,\n+        past_key_values: Optional[Cache] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n@@ -498,75 +543,37 @@ def prepare_inputs_for_generation(\n         # position_ids in NewTaskModel are 1-indexed\n         if model_inputs.get(\"position_ids\") is not None:\n             model_inputs[\"position_ids\"] += 1\n+\n         # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n         # Otherwise we need pixel values to be passed to model. NOTE: use_cache=False needs pixel_values always\n         if cache_position[0] == 0:\n             model_inputs[\"pixel_values\"] = pixel_values\n-        is_training = token_type_ids is not None and labels is not None\n-        is_static_hybrid_cache = isinstance(past_key_values, StaticCache) and any(past_key_values.is_sliding)\n-        if cache_position[0] == 0 and is_static_hybrid_cache:\n-            input_tensor = inputs_embeds if inputs_embeds is not None else input_ids\n-            causal_mask = self.model._update_causal_mask(\n-                attention_mask, token_type_ids, past_key_values, cache_position, input_tensor, is_training\n-            )\n-            model_inputs[\"attention_mask\"] = causal_mask\n \n         return model_inputs\n \n     @staticmethod\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n+    def create_masks_for_generate(\n+        config: PreTrainedConfig,\n+        input_embeds: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor],\n         cache_position: torch.Tensor,\n-        batch_size: int,\n+        past_key_values: Optional[Cache],\n+        position_ids: Optional[torch.Tensor],\n+        token_type_ids: Optional[torch.Tensor] = None,\n         **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask\n+    ) -> dict:\n+        # Uses the overwritten `create_masks_for_generate` with `token_type_ids` masking\n+        return create_causal_mask_mapping(\n+            config,\n+            input_embeds,\n+            attention_mask,\n+            cache_position,\n+            past_key_values,\n+            position_ids,\n+            token_type_ids,\n+            pixel_values=kwargs.get(\"pixel_values\"),\n+            **{k: v for k, v in kwargs.items() if k != \"pixel_values\"},\n+        )\n \n     def resize_token_embeddings(\n         self, new_num_tokens: Optional[int] = None, pad_to_multiple_of=None, mean_resizing=True"
        },
        {
            "sha": "8c58e6d6c32e081dbaf662dc2f19350753eb2391",
            "filename": "examples/modular-transformers/modeling_roberta.py",
            "status": "modified",
            "additions": 5,
            "deletions": 18,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/examples%2Fmodular-transformers%2Fmodeling_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/examples%2Fmodular-transformers%2Fmodeling_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_roberta.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -10,15 +10,15 @@\n import torch.nn as nn\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, EncoderDecoderCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...masking_utils import create_causal_mask\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPastAndCrossAttentions, BaseModelOutputWithPoolingAndCrossAttentions\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n-from ...utils import TransformersKwargs, auto_docstring, is_torch_flex_attn_available, logging\n+from ...utils import TransformersKwargs, auto_docstring, is_torch_flex_attn_available\n from ...utils.generic import check_model_inputs\n from .configuration_roberta import RobertaConfig\n \n@@ -27,9 +27,6 @@\n     from ...integrations.flex_attention import make_flex_block_causal_mask\n \n \n-logger = logging.get_logger(__name__)\n-\n-\n class RobertaEmbeddings(nn.Module):\n     \"\"\"Construct the embeddings from word, position and token_type embeddings.\"\"\"\n \n@@ -612,7 +609,7 @@ def forward(\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Union[list[torch.FloatTensor], Cache]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -622,15 +619,8 @@ def forward(\n         else:\n             use_cache = False\n \n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n-            logger.warning_once(\n-                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n-            )\n-            return_legacy_cache = True\n-            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n+        if use_cache and past_key_values is None:\n+            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n@@ -679,9 +669,6 @@ def forward(\n         sequence_output = encoder_outputs.last_hidden_state\n         pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n \n-        if return_legacy_cache:\n-            encoder_outputs.past_key_values = encoder_outputs.past_key_values.to_legacy_cache()\n-\n         return BaseModelOutputWithPoolingAndCrossAttentions(\n             last_hidden_state=sequence_output,\n             pooler_output=pooled_output,"
        },
        {
            "sha": "76fcc4f8fd4fe3af19cb5167de267fa22a32817b",
            "filename": "examples/modular-transformers/modeling_super.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/examples%2Fmodular-transformers%2Fmodeling_super.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/examples%2Fmodular-transformers%2Fmodeling_super.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_super.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -327,7 +327,7 @@ def forward(\n         input_ids: torch.LongTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,"
        },
        {
            "sha": "8f5c7438a74d9b0f76c07c9f556a405f9e1e785d",
            "filename": "examples/modular-transformers/modular_new_model.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/examples%2Fmodular-transformers%2Fmodular_new_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/examples%2Fmodular-transformers%2Fmodular_new_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodular_new_model.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -26,6 +26,8 @@ def __init__(\n         rope_theta=10000.0,\n         attention_bias=False,\n         attention_dropout=0.0,\n+        use_bidirectional_attention=False,\n+        layer_types=None,\n         **kwargs,\n     ):\n         super().__init__(self, **kwargs)"
        },
        {
            "sha": "2a6dc470d74befe3557d97bc3083f17c2bb47303",
            "filename": "examples/modular-transformers/modular_new_task_model.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/examples%2Fmodular-transformers%2Fmodular_new_task_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/examples%2Fmodular-transformers%2Fmodular_new_task_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodular_new_task_model.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -1,4 +1,4 @@\n-from typing import ClassVar, Optional, Union\n+from typing import ClassVar, Optional\n \n import torch\n import torch.utils.checkpoint\n@@ -29,7 +29,7 @@ def forward(\n         pixel_values: torch.FloatTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[list[torch.FloatTensor], Cache]] = None,\n+        past_key_values: Optional[Cache] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,"
        },
        {
            "sha": "9a186d9c35175b9a52b5a4c01d253748c87b78de",
            "filename": "examples/modular-transformers/modular_super.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/examples%2Fmodular-transformers%2Fmodular_super.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/examples%2Fmodular-transformers%2Fmodular_super.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodular_super.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -15,7 +15,7 @@ def forward(\n         input_ids: torch.LongTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,"
        },
        {
            "sha": "c8ee70f5cd291e516b4bc9c04b16c1c319b48b89",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -356,21 +356,21 @@\n         \"DynamicLayer\",\n         \"StaticLayer\",\n         \"StaticSlidingWindowLayer\",\n-        \"SlidingWindowLayer\",\n-        \"ChunkedSlidingLayer\",\n         \"QuantoQuantizedLayer\",\n         \"HQQQuantizedLayer\",\n-        \"Cache\",\n-        \"DynamicCache\",\n-        \"EncoderDecoderCache\",\n+        \"SlidingWindowLayer\",\n+        \"ChunkedSlidingLayer\",\n         \"HQQQuantizedCache\",\n         \"HybridCache\",\n         \"HybridChunkedCache\",\n         \"OffloadedCache\",\n         \"OffloadedStaticCache\",\n-        \"QuantizedCache\",\n         \"QuantoQuantizedCache\",\n         \"SlidingWindowCache\",\n+        \"Cache\",\n+        \"DynamicCache\",\n+        \"EncoderDecoderCache\",\n+        \"QuantizedCache\",\n         \"StaticCache\",\n     ]\n     _import_structure[\"data.datasets\"] = ["
        },
        {
            "sha": "7fb80647851aa2c909274d26131d0211211a562f",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 103,
            "changes": 103,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -878,26 +878,6 @@ def is_sliding(self) -> list[bool]:\n         \"\"\"Return whether the layers of the cache are sliding window\"\"\"\n         return [getattr(layer, \"is_sliding\", False) for layer in self.layers]\n \n-    def __getitem__(self, layer_idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n-        \"\"\"\n-        Support for backwards-compatible `past_key_values` indexing, e.g. `past_key_values[0][0].shape[2]` to get the\n-        sequence length.\n-        \"\"\"\n-        if layer_idx < len(self.layers):\n-            return self.layers[layer_idx].keys, self.layers[layer_idx].values\n-        else:\n-            raise KeyError(\n-                f\"Cache only has {len(self.layers)} layers, attempted to access layer with index {layer_idx}\"\n-            )\n-\n-    def __iter__(self):\n-        \"\"\"\n-        Support for backwards-compatible `past_key_values` iteration, e.g. `for x in past_key_values:` to iterate over\n-        keys and values\n-        \"\"\"\n-        for layer_idx in range(len(self)):\n-            yield (self.layers[layer_idx].keys, self.layers[layer_idx].values)\n-\n     def __len__(self):\n         \"\"\"\n         This value corresponds to the number of layers in the model.\n@@ -1002,31 +982,6 @@ def __init__(\n         else:\n             super().__init__(layers=layers, offloading=offloading, offload_only_non_sliding=offload_only_non_sliding)\n \n-    def to_legacy_cache(self) -> tuple[tuple[torch.Tensor, torch.Tensor]]:\n-        \"\"\"\n-        Converts the `Cache` instance into the its equivalent in the legacy cache format. Used for\n-        backward compatibility.\n-        \"\"\"\n-        legacy_cache = ()\n-        for layer in self.layers:\n-            legacy_cache += ((layer.keys, layer.values),)\n-        return legacy_cache\n-\n-    @classmethod\n-    def from_legacy_cache(cls, past_key_values: tuple[tuple[torch.Tensor, torch.Tensor]]) -> \"DynamicCache\":\n-        \"\"\"\n-        Converts a cache in the legacy cache format into an equivalent `Cache`. Used for\n-        backward compatibility.\n-        \"\"\"\n-        cache = cls()\n-        if past_key_values is None:\n-            logger.warning_once(\"past_key_values should not be None in from_legacy_cache()\")\n-        if past_key_values is not None:\n-            for layer_idx in range(len(past_key_values)):\n-                key_states, value_states = past_key_values[layer_idx]\n-                cache.update(key_states, value_states, layer_idx)\n-        return cache\n-\n \n class StaticCache(Cache):\n     \"\"\"\n@@ -1228,71 +1183,13 @@ def __repr__(self) -> str:\n             f\"{self.cross_attention_cache})\"\n         )\n \n-    def __iter__(self):\n-        \"\"\"\n-        Support for backwards-compatible `past_key_values` iteration, e.g. `for x in past_key_values:` to iterate over\n-        keys and values\n-        \"\"\"\n-        for layer_idx in range(len(self)):\n-            yield (\n-                self.self_attention_cache.layers[layer_idx].keys,\n-                self.self_attention_cache.layers[layer_idx].values,\n-                self.cross_attention_cache.layers[layer_idx].keys,\n-                self.cross_attention_cache.layers[layer_idx].values,\n-            )\n-\n-    def __getitem__(self, layer_idx: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n-        \"\"\"\n-        Support for backwards-compatible `past_key_values` indexing, e.g. `past_key_values[0][0].shape[2]` to get the\n-        sequence length.\n-        \"\"\"\n-        if layer_idx < len(self):\n-            return (\n-                self.self_attention_cache.layers[layer_idx].keys,\n-                self.self_attention_cache.layers[layer_idx].values,\n-                self.cross_attention_cache.layers[layer_idx].keys,\n-                self.cross_attention_cache.layers[layer_idx].values,\n-            )\n-        else:\n-            raise KeyError(f\"Cache only has {len(self)} layers, attempted to access layer with index {layer_idx}\")\n-\n     def __len__(self):\n         \"\"\"\n         Support for backwards-compatible `past_key_values` length, e.g. `len(past_key_values)`. This value corresponds\n         to the number of layers in the model.\n         \"\"\"\n         return len(self.self_attention_cache)\n \n-    def to_legacy_cache(self) -> tuple[tuple[torch.Tensor]]:\n-        \"\"\"Converts the `EncoderDecoderCache` instance into its equivalent in the legacy cache format.\"\"\"\n-        legacy_cache = ()\n-        if len(self.cross_attention_cache) > 0:\n-            for self_attn, cross_attn in zip(\n-                self.self_attention_cache.to_legacy_cache(), self.cross_attention_cache.to_legacy_cache()\n-            ):\n-                legacy_cache += (self_attn + cross_attn,)\n-        else:\n-            legacy_cache = self.self_attention_cache.to_legacy_cache()\n-        return legacy_cache\n-\n-    @classmethod\n-    def from_legacy_cache(\n-        cls, past_key_values: Optional[Iterable[tuple[torch.FloatTensor, ...]]]\n-    ) -> \"EncoderDecoderCache\":\n-        \"\"\"Converts a cache in the legacy cache format into an equivalent `EncoderDecoderCache`.\"\"\"\n-        cache = cls(DynamicCache(), DynamicCache())\n-        if past_key_values is None:\n-            logger.warning_once(\"past_key_values should not be None in from_legacy_cache()\")\n-        else:\n-            for layer_idx, key_value_states in enumerate(past_key_values):\n-                key_states, value_states = key_value_states[:2]\n-                cache.self_attention_cache.update(key_states, value_states, layer_idx)\n-                if len(key_value_states) > 2:\n-                    key_states, value_states = key_value_states[2:]\n-                    cache.cross_attention_cache.update(key_states, value_states, layer_idx)\n-                    cache.is_updated[layer_idx] = True\n-        return cache\n-\n     def get_seq_length(self, layer_idx: int = 0) -> int:\n         \"\"\"Returns the sequence length of the cached states. A layer index can be optionally passed.\"\"\"\n         return self.self_attention_cache.get_seq_length(layer_idx)"
        },
        {
            "sha": "6d445529c866a17a82421fda59eb2225ee514b79",
            "filename": "src/transformers/generation/configuration_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -151,8 +151,6 @@ class GenerationConfig(PushToHubMixin):\n             our [cache documentation](https://huggingface.co/docs/transformers/en/kv_cache) for further information.\n         cache_config (`dict`, *optional*, default to `None`):\n             Arguments used in the key-value cache class can be passed in `cache_config`.\n-        return_legacy_cache (`bool`, *optional*, default to `True`):\n-            Whether to return the legacy or new format of the cache when `DynamicCache` is used by default.\n \n         > Parameters for manipulation of the model output logits\n \n@@ -349,7 +347,6 @@ def __init__(self, **kwargs):\n         self.cache_implementation = kwargs.pop(\"cache_implementation\", None)\n         self.cache_config = kwargs.pop(\"cache_config\", None)\n \n-        self.return_legacy_cache = kwargs.pop(\"return_legacy_cache\", None)\n         self.prefill_chunk_size = kwargs.pop(\"prefill_chunk_size\", None)\n \n         # Parameters for manipulation of the model output logits\n@@ -634,7 +631,7 @@ def validate(self, strict=False):\n                 \"You have set `use_cache` to `False`, but {cache_arg} is set to {cache_arg_value}. {cache_arg} will \"\n                 \"have no effect.\"\n             )\n-            for arg_name in (\"cache_implementation\", \"cache_config\", \"return_legacy_cache\"):\n+            for arg_name in (\"cache_implementation\", \"cache_config\"):\n                 if getattr(self, arg_name) is not None:\n                     minor_issues[arg_name] = no_cache_warning.format(\n                         cache_arg=arg_name, cache_arg_value=getattr(self, arg_name)"
        },
        {
            "sha": "304b34bf4a5c3b65a2e9bbc8ca52a5f898fbcfd8",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 6,
            "deletions": 26,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -1865,9 +1865,7 @@ def _get_cache(self, cache_implementation: str, batch_size: int, max_cache_len:\n     def _supports_default_dynamic_cache(cls) -> bool:\n         \"\"\"\n         Return `True` if current model can use a `DynamicCache` instance when initializing the `past_key_values`.\n-        This adds exception for some models like `Mamba` models which use their own caches\n-        and do not need to initialize the Cache in advance in order to save memory (because no back and forth\n-        `to_legacy_cache` and `from_legacy_cache` will be performed for mamba-based models).\n+        This adds exception for some models like `Mamba` models which use their own caches.\n         \"\"\"\n         # NOTE: remove xlnet/reformer when the models are deprecated, non-standard model architecture/cache name\n         return not cls._is_stateful and all(\n@@ -1901,25 +1899,17 @@ def _prepare_cache_for_generation(\n             self.config.is_encoder_decoder or model_kwargs.get(\"encoder_outputs\") is not None\n         )\n \n-        # Quick escape route 1: if the user specifies a cache, we only need to:\n-        # a) check for conflicting `generate` arguments\n-        # b) convert to the new cache format (if the user passes a legacy cache and model supports it)\n+        # Quick escape route 1: if the user specifies a cache, we only need to check for conflicting `generate` arguments\n         user_defined_cache = model_kwargs.get(cache_name)\n         if user_defined_cache is not None:\n             if generation_config.cache_implementation is not None:\n                 raise ValueError(\n                     f\"Passing both `cache_implementation` (used to initialize certain caches) and `{cache_name}` (a \"\n                     \"Cache object) is unsupported. Please use only one of the two.\"\n                 )\n-            if isinstance(user_defined_cache, tuple) and self._supports_default_dynamic_cache():\n-                logger.warning_once(\n-                    \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                    \"You should pass an instance of `Cache` instead.\"\n-                )\n-                model_kwargs[cache_name] = (\n-                    DynamicCache.from_legacy_cache(user_defined_cache)\n-                    if not requires_cross_attention_cache\n-                    else EncoderDecoderCache.from_legacy_cache(user_defined_cache)\n+            if isinstance(user_defined_cache, tuple):\n+                raise ValueError(\n+                    \"Passing a tuple of `past_key_values` is not supported anymore. Please use a `Cache` instance.\"\n                 )\n             return\n \n@@ -1928,8 +1918,7 @@ def _prepare_cache_for_generation(\n         if generation_config.use_cache is False:\n             return\n \n-        # Quick escape route 3: model that only supports legacy caches or models that supply it in\n-        # `prepare_inputs_for_generation` (mamba, zamba, ...)\n+        # Quick escape route 3: model that supply it in `prepare_inputs_for_generation` (mamba, zamba, ...)\n         if not self._supports_default_dynamic_cache():\n             if generation_config.cache_implementation is not None:\n                 logger.warning_once(\n@@ -2006,8 +1995,6 @@ def _prepare_cache_for_generation(\n             elif \"dynamic\" in generation_config.cache_implementation:\n                 model_kwargs[cache_name] = DynamicCache(**dynamic_cache_kwargs)\n \n-        # Use DynamicCache instance by default. This will avoid back and forth from legacy format that\n-        # keeps copying the cache thus using much more memory\n         # TODO (joao): remove this `else` when we remove the last traces of the legacy cache format (v4.58.0, search\n         # for `instance(past_key_values, Cache)` as well). In general, if `cache_implementation` is unset, cache\n         # initialization should happen inside the model at prefill time.\n@@ -2549,13 +2536,6 @@ def generate(\n             **model_kwargs,\n         )\n \n-        # Convert to legacy cache format if requested\n-        if (\n-            generation_config.return_legacy_cache is True\n-            and hasattr(result, \"past_key_values\")\n-            and getattr(result.past_key_values, \"to_legacy_cache\") is not None\n-        ):\n-            result.past_key_values = result.past_key_values.to_legacy_cache()\n         return result\n \n     def _has_unfinished_sequences(self, this_peer_finished: bool, synced_gpus: bool, device: torch.device) -> bool:"
        },
        {
            "sha": "57f546f3da440ada0f4d840abfbd03ed155ac79e",
            "filename": "src/transformers/models/autoformer/modeling_autoformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -1091,13 +1091,6 @@ def forward(\n \n         if use_cache and past_key_values is None:\n             past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n-        if use_cache and isinstance(past_key_values, tuple):\n-            logger.warning_once(\n-                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n-            )\n-            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n         # expand encoder attention mask\n         if encoder_hidden_states is not None and encoder_attention_mask is not None:"
        },
        {
            "sha": "e5e47be919e53ae604fe52f5dacaa97f6cb79264",
            "filename": "src/transformers/models/bark/modeling_bark.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -488,13 +488,6 @@ def forward(\n \n         if use_cache and past_key_values is None:\n             past_key_values = DynamicCache(config=self.config)\n-        if use_cache and isinstance(past_key_values, tuple):\n-            logger.warning_once(\n-                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `DynamicCache` instead, e.g. \"\n-                \"`past_key_values=DynamicCache.from_legacy_cache(past_key_values)`.\"\n-            )\n-            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n \n         past_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n "
        },
        {
            "sha": "3d9d82f6e5c608eb690843944494ab0806ccd5e0",
            "filename": "src/transformers/models/bart/modeling_bart.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -997,13 +997,6 @@ def forward(\n                 if encoder_hidden_states is not None\n                 else DynamicCache(config=self.config)\n             )\n-        if use_cache and isinstance(past_key_values, tuple):\n-            logger.warning_once(\n-                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n-            )\n-            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n         batch_size, seq_length = inputs_embeds.size()[:-1]\n         past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "49724dbab2561140f54f17b3ab13f6af5a10c345",
            "filename": "src/transformers/models/bert/modeling_bert.py",
            "status": "modified",
            "additions": 5,
            "deletions": 15,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -24,7 +24,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, EncoderDecoderCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...masking_utils import create_causal_mask\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n@@ -691,7 +691,7 @@ def forward(\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Union[list[torch.FloatTensor], Cache]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -701,15 +701,8 @@ def forward(\n         else:\n             use_cache = False\n \n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n-            logger.warning_once(\n-                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n-            )\n-            return_legacy_cache = True\n-            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n+        if use_cache and past_key_values is None:\n+            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n@@ -758,9 +751,6 @@ def forward(\n         sequence_output = encoder_outputs.last_hidden_state\n         pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n \n-        if return_legacy_cache:\n-            encoder_outputs.past_key_values = encoder_outputs.past_key_values.to_legacy_cache()\n-\n         return BaseModelOutputWithPoolingAndCrossAttentions(\n             last_hidden_state=sequence_output,\n             pooler_output=pooled_output,\n@@ -1007,7 +997,7 @@ def forward(\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Union[list[torch.FloatTensor], Cache]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],"
        },
        {
            "sha": "b0cf4bfb6dd8d4a548f9300cfcf545645c0695f1",
            "filename": "src/transformers/models/bert_generation/modeling_bert_generation.py",
            "status": "modified",
            "additions": 4,
            "deletions": 14,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -20,7 +20,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, EncoderDecoderCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...masking_utils import create_causal_mask\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n@@ -554,7 +554,7 @@ def forward(\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Union[list[torch.FloatTensor], Cache]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -564,15 +564,8 @@ def forward(\n         else:\n             use_cache = False\n \n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n-            logger.warning_once(\n-                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n-            )\n-            return_legacy_cache = True\n-            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n+        if use_cache and past_key_values is None:\n+            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n@@ -619,9 +612,6 @@ def forward(\n         )\n         sequence_output = encoder_outputs[0]\n \n-        if return_legacy_cache:\n-            encoder_outputs.past_key_values = encoder_outputs.past_key_values.to_legacy_cache()\n-\n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=sequence_output,\n             past_key_values=encoder_outputs.past_key_values,"
        },
        {
            "sha": "7f5f19363a3b8bfdd01d50495b0b7e5838cb2c68",
            "filename": "src/transformers/models/big_bird/modeling_big_bird.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -1389,13 +1389,6 @@ def forward(\n \n         if use_cache and past_key_values is None:\n             past_key_values = DynamicCache(config=self.config)\n-        if use_cache and isinstance(past_key_values, tuple):\n-            logger.warning_once(\n-                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `DynamicCache` instead, e.g. \"\n-                \"`past_key_values=DynamicCache.from_legacy_cache(past_key_values)`.\"\n-            )\n-            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n \n         for i, layer_module in enumerate(self.layer):\n             if output_hidden_states:"
        },
        {
            "sha": "f95ed89742c92aada4aa75c802ebab477d5f4739",
            "filename": "src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -2158,13 +2158,6 @@ def forward(\n                 if encoder_hidden_states is not None\n                 else DynamicCache(config=self.config)\n             )\n-        if use_cache and isinstance(past_key_values, tuple):\n-            logger.warning_once(\n-                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n-            )\n-            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n         batch_size, seq_length = inputs_embeds.size()[:-1]\n         past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "df2d39ed16443ea3019a3ff34dad13f21af4b747",
            "filename": "src/transformers/models/biogpt/modeling_biogpt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -548,13 +548,6 @@ def forward(\n         # initialize past_key_values\n         if use_cache and past_key_values is None:\n             past_key_values = DynamicCache(config=self.config)\n-        if use_cache and isinstance(past_key_values, tuple):\n-            logger.warning_once(\n-                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `DynamicCache` instead, e.g. \"\n-                \"`past_key_values=DynamicCache.from_legacy_cache(past_key_values)`.\"\n-            )\n-            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n \n         batch_size, seq_length = inputs_embeds.size()[:-1]\n         past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "5c335515753c64bcd01b9373a5848c706d5b0fd5",
            "filename": "src/transformers/models/biogpt/modular_biogpt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -374,13 +374,6 @@ def forward(\n         # initialize past_key_values\n         if use_cache and past_key_values is None:\n             past_key_values = DynamicCache(config=self.config)\n-        if use_cache and isinstance(past_key_values, tuple):\n-            logger.warning_once(\n-                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `DynamicCache` instead, e.g. \"\n-                \"`past_key_values=DynamicCache.from_legacy_cache(past_key_values)`.\"\n-            )\n-            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n \n         batch_size, seq_length = inputs_embeds.size()[:-1]\n         past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "20191b93c28a51ff775954527e02156a73238647",
            "filename": "src/transformers/models/blenderbot/modeling_blenderbot.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -950,13 +950,6 @@ def forward(\n                 if encoder_hidden_states is not None\n                 else DynamicCache(config=self.config)\n             )\n-        if use_cache and isinstance(past_key_values, tuple):\n-            logger.warning_once(\n-                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n-            )\n-            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n         batch_size, seq_length = inputs_embeds.size()[:-1]\n         past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "df1118e7d8ab812e708f2c0810a02859a1e8d435",
            "filename": "src/transformers/models/blenderbot_small/modeling_blenderbot_small.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -937,13 +937,6 @@ def forward(\n                 if encoder_hidden_states is not None\n                 else DynamicCache(config=self.config)\n             )\n-        if use_cache and isinstance(past_key_values, tuple):\n-            logger.warning_once(\n-                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n-            )\n-            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n         batch_size, seq_length = inputs_embeds.size()[:-1]\n         past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "d776ffa2f4708b5279566daade3b988fb633a0ca",
            "filename": "src/transformers/models/blip/modeling_blip_text.py",
            "status": "modified",
            "additions": 1,
            "deletions": 8,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -392,16 +392,9 @@ def forward(\n                 use_cache = False\n \n         if use_cache:\n-            if isinstance(past_key_values, tuple):\n-                logger.warning_once(\n-                    \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                    \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                    \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n-                )\n-                past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n             # The model acts as encoder decoder but is not an encoder decoder. So we cast all cache objects to\n             # `EncoderDecoderCache` type assuming that the incoming cache is from `self_attention`\n-            elif isinstance(past_key_values, DynamicCache):\n+            if isinstance(past_key_values, DynamicCache):\n                 past_key_values = EncoderDecoderCache(past_key_values, DynamicCache(config=self.config))\n             elif past_key_values is None:\n                 past_key_values = EncoderDecoderCache("
        },
        {
            "sha": "77ea42b1be47fd826290b1e4eeb0123827537167",
            "filename": "src/transformers/models/bloom/modeling_bloom.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -483,7 +483,7 @@ def set_input_embeddings(self, new_embeddings: torch.Tensor):\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, tuple[tuple[torch.Tensor, torch.Tensor], ...]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -814,7 +814,7 @@ def prepare_inputs_for_generation(\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, tuple[tuple[torch.Tensor, torch.Tensor], ...]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n@@ -924,7 +924,7 @@ def __init__(self, config: BloomConfig):\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, tuple[tuple[torch.Tensor, torch.Tensor], ...]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n@@ -1058,7 +1058,7 @@ def __init__(self, config: BloomConfig):\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, tuple[tuple[torch.Tensor, torch.Tensor], ...]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,"
        },
        {
            "sha": "fa590914e4f6c29304503aef3c6e8f6d81ab45df",
            "filename": "src/transformers/models/blt/modeling_blt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fblt%2Fmodeling_blt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fblt%2Fmodeling_blt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblt%2Fmodeling_blt.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -1221,7 +1221,7 @@ def forward(\n         cross_attention_states: Optional[torch.LongTensor] = None,  # Keep for compatibility\n         cross_attention_mask: Optional[torch.LongTensor] = None,\n         full_text_row_masked_out_mask: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n-        past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "2eb3c493b1bbb670adfda86b9096acc659fbeff2",
            "filename": "src/transformers/models/blt/modular_blt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fblt%2Fmodular_blt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fblt%2Fmodular_blt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblt%2Fmodular_blt.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -968,7 +968,7 @@ def forward(\n         cross_attention_states: Optional[torch.LongTensor] = None,  # Keep for compatibility\n         cross_attention_mask: Optional[torch.LongTensor] = None,\n         full_text_row_masked_out_mask: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n-        past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "dd7621d12cce5be80741519ea892a23b31024835",
            "filename": "src/transformers/models/bridgetower/modeling_bridgetower.py",
            "status": "modified",
            "additions": 4,
            "deletions": 14,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -23,7 +23,7 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN, QuickGELUActivation\n-from ...cache_utils import Cache, EncoderDecoderCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...masking_utils import create_causal_mask\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -1045,7 +1045,7 @@ def forward(\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Union[list[torch.FloatTensor], Cache]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -1069,15 +1069,8 @@ def forward(\n                 )\n                 use_cache = False\n \n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n-            logger.warning_once(\n-                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n-            )\n-            return_legacy_cache = True\n-            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n+        if use_cache and past_key_values is None:\n+            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n@@ -1128,9 +1121,6 @@ def forward(\n         sequence_output = encoder_outputs[0]\n         pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n \n-        if return_legacy_cache:\n-            encoder_outputs.past_key_values = encoder_outputs.past_key_values.to_legacy_cache()\n-\n         return BaseModelOutputWithPoolingAndCrossAttentions(\n             last_hidden_state=sequence_output,\n             pooler_output=pooled_output,"
        },
        {
            "sha": "1fb2c6a240e189d85bf5e80093edee59b56bfbb9",
            "filename": "src/transformers/models/camembert/modeling_camembert.py",
            "status": "modified",
            "additions": 4,
            "deletions": 14,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -27,7 +27,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN, gelu\n-from ...cache_utils import Cache, EncoderDecoderCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...masking_utils import create_causal_mask\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n@@ -671,7 +671,7 @@ def forward(\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Union[list[torch.FloatTensor], Cache]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -681,15 +681,8 @@ def forward(\n         else:\n             use_cache = False\n \n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n-            logger.warning_once(\n-                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n-            )\n-            return_legacy_cache = True\n-            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n+        if use_cache and past_key_values is None:\n+            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n@@ -738,9 +731,6 @@ def forward(\n         sequence_output = encoder_outputs.last_hidden_state\n         pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n \n-        if return_legacy_cache:\n-            encoder_outputs.past_key_values = encoder_outputs.past_key_values.to_legacy_cache()\n-\n         return BaseModelOutputWithPoolingAndCrossAttentions(\n             last_hidden_state=sequence_output,\n             pooler_output=pooled_output,"
        },
        {
            "sha": "d4d770505f306b5403ffc2804097d3303b2781b5",
            "filename": "src/transformers/models/clvp/modeling_clvp.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -1059,13 +1059,6 @@ def forward(\n \n         if use_cache and past_key_values is None:\n             past_key_values = DynamicCache(config=self.config)\n-        if use_cache and isinstance(past_key_values, tuple):\n-            logger.warning_once(\n-                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `DynamicCache` instead, e.g. \"\n-                \"`past_key_values=DynamicCache.from_legacy_cache(past_key_values)`.\"\n-            )\n-            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n \n         past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n         if position_ids is None:"
        },
        {
            "sha": "53d47720c39224c1e5dba5f2b515d7c5b7acfc8c",
            "filename": "src/transformers/models/codegen/modeling_codegen.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -326,7 +326,7 @@ def set_input_embeddings(self, new_embeddings):\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, tuple[tuple[torch.Tensor]]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n@@ -574,7 +574,7 @@ def __init__(self, config):\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, tuple[tuple[torch.Tensor]]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "920e3c41b857e2fecf7787772097c8e18d524070",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -458,7 +458,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "6d033a866d31146556e3c7e3592037556e64bf7f",
            "filename": "src/transformers/models/cohere/modular_cohere.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodular_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodular_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodular_cohere.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -275,7 +275,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "bb8b0e56c011a04f61b6888f8dd9ace797c2e56a",
            "filename": "src/transformers/models/cohere2/modeling_cohere2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -437,7 +437,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "fb70f61ecdf279d8b781da9bf90e5db877c47965",
            "filename": "src/transformers/models/cpmant/modeling_cpmant.py",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fcpmant%2Fmodeling_cpmant.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fcpmant%2Fmodeling_cpmant.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcpmant%2Fmodeling_cpmant.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -637,13 +637,6 @@ def forward(\n \n         if use_cache and past_key_values is None:\n             past_key_values = DynamicCache(config=self.config)\n-        if use_cache and isinstance(past_key_values, tuple):\n-            logger.warning_once(\n-                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `DynamicCache` instead, e.g. \"\n-                \"`past_key_values=DynamicCache.from_legacy_cache(past_key_values)`.\"\n-            )\n-            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n \n         past_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n         input_ids = input_ids.contiguous()\n@@ -796,12 +789,5 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, embeddings):\n         self.cpmant.input_embedding = embeddings\n \n-    def _reorder_cache(self, past_key_values, beam_idx):\n-        past_key_values = [list(each) if each is not None else each for each in past_key_values]\n-        for key_value_layer in past_key_values:\n-            key_value_layer[0] = key_value_layer[0][beam_idx]\n-            key_value_layer[1] = key_value_layer[1][beam_idx]\n-        return past_key_values\n-\n \n __all__ = [\"CpmAntForCausalLM\", \"CpmAntModel\", \"CpmAntPreTrainedModel\"]"
        },
        {
            "sha": "830b7e32b100bda005668d33839282dc3a639525",
            "filename": "src/transformers/models/csm/modeling_csm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -545,7 +545,7 @@ def forward(\n         backbone_last_hidden_state: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -925,7 +925,7 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         input_values_cutoffs: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "9967a1d972876e3ba9c8604f69ad492dbd13ce33",
            "filename": "src/transformers/models/csm/modular_csm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -314,7 +314,7 @@ def forward(\n         backbone_last_hidden_state: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -606,7 +606,7 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         input_values_cutoffs: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "e3f222488a177e724c45a277964e1d766f6558f4",
            "filename": "src/transformers/models/ctrl/modeling_ctrl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -330,13 +330,6 @@ def forward(\n \n         if use_cache and past_key_values is None:\n             past_key_values = DynamicCache(config=self.config)\n-        if use_cache and isinstance(past_key_values, tuple):\n-            logger.warning_once(\n-                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `DynamicCache` instead, e.g. \"\n-                \"`past_key_values=DynamicCache.from_legacy_cache(past_key_values)`.\"\n-            )\n-            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n \n         past_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n         if position_ids is None:"
        },
        {
            "sha": "1637ba6f016e7afeac119398830dbad5ef12f4d3",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_text.py",
            "status": "modified",
            "additions": 4,
            "deletions": 14,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -26,7 +26,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN, gelu\n-from ...cache_utils import Cache, EncoderDecoderCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...masking_utils import create_causal_mask\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n@@ -631,7 +631,7 @@ def forward(\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Union[list[torch.FloatTensor], Cache]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -641,15 +641,8 @@ def forward(\n         else:\n             use_cache = False\n \n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n-            logger.warning_once(\n-                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n-            )\n-            return_legacy_cache = True\n-            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n+        if use_cache and past_key_values is None:\n+            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n@@ -698,9 +691,6 @@ def forward(\n         sequence_output = encoder_outputs.last_hidden_state\n         pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n \n-        if return_legacy_cache:\n-            encoder_outputs.past_key_values = encoder_outputs.past_key_values.to_legacy_cache()\n-\n         return BaseModelOutputWithPoolingAndCrossAttentions(\n             last_hidden_state=sequence_output,\n             pooler_output=pooled_output,"
        },
        {
            "sha": "9fdad3b42b9d64e877b0c084f9bb9302ac776a40",
            "filename": "src/transformers/models/decision_transformer/modeling_decision_transformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -580,7 +580,6 @@ def forward(\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n         past_key_values = past_key_values if use_cache else None\n-        # no return to legacy cache\n         if not return_dict:\n             return tuple(\n                 v"
        },
        {
            "sha": "7342cba3d60837488095fcac577145abd8818591",
            "filename": "src/transformers/models/deprecated/mega/modeling_mega.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmega%2Fmodeling_mega.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmega%2Fmodeling_mega.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmega%2Fmodeling_mega.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -1782,14 +1782,6 @@ def prepare_inputs_for_generation(self, input_ids, past_key_values=None, attenti\n \n         return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"past_key_values\": past_key_values}\n \n-    def _reorder_cache(self, past_key_values, beam_idx):\n-        reordered_past = ()\n-        for layer_past in past_key_values:\n-            reordered_past += (\n-                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n-            )\n-        return reordered_past\n-\n \n @add_start_docstrings(\"\"\"MEGA Model with a `language modeling` head on top.\"\"\", MEGA_START_DOCSTRING)\n class MegaForMaskedLM(MegaPreTrainedModel):"
        },
        {
            "sha": "2f5eaf532459002b2139e841619187fd0b9dff31",
            "filename": "src/transformers/models/deprecated/open_llama/modeling_open_llama.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fmodeling_open_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fmodeling_open_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fmodeling_open_llama.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -806,15 +806,6 @@ def prepare_inputs_for_generation(\n         )\n         return model_inputs\n \n-    @staticmethod\n-    def _reorder_cache(past_key_values, beam_idx):\n-        reordered_past = ()\n-        for layer_past in past_key_values:\n-            reordered_past += (\n-                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n-            )\n-        return reordered_past\n-\n \n @add_start_docstrings(\n     \"\"\""
        },
        {
            "sha": "a8c1bf70d0bd92109d9bf969043f6dc5a877301c",
            "filename": "src/transformers/models/deprecated/qdqbert/modeling_qdqbert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fqdqbert%2Fmodeling_qdqbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fqdqbert%2Fmodeling_qdqbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fqdqbert%2Fmodeling_qdqbert.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -1024,14 +1024,6 @@ def prepare_inputs_for_generation(\n \n         return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"past_key_values\": past_key_values}\n \n-    def _reorder_cache(self, past_key_values, beam_idx):\n-        reordered_past = ()\n-        for layer_past in past_key_values:\n-            reordered_past += (\n-                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n-            )\n-        return reordered_past\n-\n \n @add_start_docstrings(\"\"\"QDQBERT Model with a `language modeling` head on top.\"\"\", QDQBERT_START_DOCSTRING)\n class QDQBertForMaskedLM(QDQBertPreTrainedModel):"
        },
        {
            "sha": "7716ef7b1e631698253baae591ae0256aaa0f7d3",
            "filename": "src/transformers/models/deprecated/speech_to_text_2/modeling_speech_to_text_2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Fmodeling_speech_to_text_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Fmodeling_speech_to_text_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Fmodeling_speech_to_text_2.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -830,14 +830,5 @@ def prepare_inputs_for_generation(\n             \"use_cache\": use_cache,\n         }\n \n-    @staticmethod\n-    def _reorder_cache(past_key_values, beam_idx):\n-        reordered_past = ()\n-        for layer_past in past_key_values:\n-            reordered_past += (\n-                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n-            )\n-        return reordered_past\n-\n \n __all__ = [\"Speech2Text2ForCausalLM\", \"Speech2Text2PreTrainedModel\"]"
        },
        {
            "sha": "f863be207c4ea95efe1f853d2a51ae3e1ea81a8c",
            "filename": "src/transformers/models/electra/modeling_electra.py",
            "status": "modified",
            "additions": 3,
            "deletions": 13,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -22,7 +22,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN, get_activation\n-from ...cache_utils import Cache, EncoderDecoderCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...masking_utils import create_causal_mask\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n@@ -641,15 +641,8 @@ def forward(\n         else:\n             use_cache = False\n \n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n-            logger.warning_once(\n-                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n-            )\n-            return_legacy_cache = True\n-            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n+        if use_cache and past_key_values is None:\n+            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n@@ -697,9 +690,6 @@ def forward(\n             **kwargs,\n         )\n \n-        if return_legacy_cache:\n-            encoder_outputs.past_key_values = encoder_outputs.past_key_values.to_legacy_cache()\n-\n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=encoder_outputs.last_hidden_state,\n             past_key_values=encoder_outputs.past_key_values,"
        },
        {
            "sha": "ecf39b25f149dcf7e0ddd51fc2d3fd79dce8ef39",
            "filename": "src/transformers/models/ernie/modeling_ernie.py",
            "status": "modified",
            "additions": 4,
            "deletions": 14,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -28,7 +28,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, EncoderDecoderCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...masking_utils import create_causal_mask\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n@@ -653,7 +653,7 @@ def forward(\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Union[list[torch.FloatTensor], Cache]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -677,15 +677,8 @@ def forward(\n                 )\n                 use_cache = False\n \n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n-            logger.warning_once(\n-                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n-            )\n-            return_legacy_cache = True\n-            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n+        if use_cache and past_key_values is None:\n+            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n \n         if input_ids is not None and inputs_embeds is not None:\n             raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n@@ -766,9 +759,6 @@ def forward(\n         sequence_output = encoder_outputs[0]\n         pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n \n-        if return_legacy_cache:\n-            encoder_outputs.past_key_values = encoder_outputs.past_key_values.to_legacy_cache()\n-\n         return BaseModelOutputWithPoolingAndCrossAttentions(\n             last_hidden_state=sequence_output,\n             pooler_output=pooled_output,"
        },
        {
            "sha": "aed239e7c4df0de913d649fb73fd041c050eff8f",
            "filename": "src/transformers/models/ernie/modular_ernie.py",
            "status": "modified",
            "additions": 4,
            "deletions": 14,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fernie%2Fmodular_ernie.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fernie%2Fmodular_ernie.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie%2Fmodular_ernie.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -21,7 +21,7 @@\n import torch.nn as nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n-from ...cache_utils import Cache, EncoderDecoderCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...masking_utils import create_causal_mask\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n from ...modeling_outputs import (\n@@ -214,7 +214,7 @@ def forward(\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Union[list[torch.FloatTensor], Cache]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -238,15 +238,8 @@ def forward(\n                 )\n                 use_cache = False\n \n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n-            logger.warning_once(\n-                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n-            )\n-            return_legacy_cache = True\n-            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n+        if use_cache and past_key_values is None:\n+            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n \n         if input_ids is not None and inputs_embeds is not None:\n             raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n@@ -327,9 +320,6 @@ def forward(\n         sequence_output = encoder_outputs[0]\n         pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n \n-        if return_legacy_cache:\n-            encoder_outputs.past_key_values = encoder_outputs.past_key_values.to_legacy_cache()\n-\n         return BaseModelOutputWithPoolingAndCrossAttentions(\n             last_hidden_state=sequence_output,\n             pooler_output=pooled_output,"
        },
        {
            "sha": "26d4fc1b621ffce40a7de6cb1c2ecd05a314deb6",
            "filename": "src/transformers/models/falcon/modeling_falcon.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -706,7 +706,7 @@ def set_input_embeddings(self, new_embeddings: torch.Tensor):\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, tuple[tuple[torch.Tensor, torch.Tensor], ...]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         inputs_embeds: Optional[torch.LongTensor] = None,\n@@ -989,7 +989,7 @@ def set_output_embeddings(self, new_embeddings: torch.Tensor):\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, tuple[tuple[torch.Tensor, torch.Tensor], ...]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,"
        },
        {
            "sha": "825828fcd9b2bed33fcc421a8d1c59f4e4443349",
            "filename": "src/transformers/models/flaubert/modeling_flaubert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fflaubert%2Fmodeling_flaubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fflaubert%2Fmodeling_flaubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflaubert%2Fmodeling_flaubert.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -851,9 +851,6 @@ def forward(\n         if cache is None:\n             cache = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n \n-        if isinstance(cache, tuple):\n-            cache = EncoderDecoderCache.from_legacy_cache(cache)\n-\n         if lengths is None:\n             if input_ids is not None:\n                 lengths = (input_ids != self.pad_index).sum(dim=1).long()"
        },
        {
            "sha": "422cc2b56accd258869a3492f545081ab66becba",
            "filename": "src/transformers/models/fsmt/modeling_fsmt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Ffsmt%2Fmodeling_fsmt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Ffsmt%2Fmodeling_fsmt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffsmt%2Fmodeling_fsmt.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -619,13 +619,6 @@ def forward(\n         # initialize `past_key_values`\n         if use_cache and past_key_values is None:\n             past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n-        if use_cache and isinstance(past_key_values, tuple):\n-            logger.warning_once(\n-                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n-            )\n-            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n         x += positions\n         x = nn.functional.dropout(x, p=self.dropout, training=self.training)"
        },
        {
            "sha": "447b004bd1c5ffeb0dc1afb5e75bcbd77d1ec995",
            "filename": "src/transformers/models/git/modeling_git.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -343,7 +343,7 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Union[Cache, tuple[tuple[torch.FloatTensor]]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,\n@@ -988,7 +988,7 @@ def forward(\n         position_ids: Optional[torch.Tensor] = None,\n         pixel_values: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -1170,7 +1170,7 @@ def forward(\n         pixel_values: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Union[Cache, list[torch.Tensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,"
        },
        {
            "sha": "3b9e3dcf81216d041162521d191defe24f5aac25",
            "filename": "src/transformers/models/gpt2/modeling_gpt2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -660,13 +660,6 @@ def forward(\n         if use_cache:\n             if past_key_values is None:\n                 past_key_values = DynamicCache(config=self.config)\n-            elif isinstance(past_key_values, tuple):\n-                logger.warning_once(\n-                    \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.53.0. \"\n-                    \"You should pass an instance of `Cache` instead, e.g. \"\n-                    \"`past_key_values=DynamicCache.from_legacy_cache(past_key_values)`.\"\n-                )\n-                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n \n             if self.config.add_cross_attention and not isinstance(past_key_values, EncoderDecoderCache):\n                 past_key_values = EncoderDecoderCache(past_key_values, DynamicCache(config=self.config))"
        },
        {
            "sha": "d6d33c624d1bbd3b6229976d74d875a48d75bb67",
            "filename": "src/transformers/models/gpt_neo/modeling_gpt_neo.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -430,7 +430,7 @@ def set_input_embeddings(self, new_embeddings):\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Union[Cache, tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n@@ -686,7 +686,7 @@ def __init__(self, config):\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Union[Cache, tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n@@ -794,7 +794,7 @@ def __init__(self, config):\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Union[Cache, tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         token_type_ids: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n@@ -915,7 +915,7 @@ def __init__(self, config):\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, tuple[tuple[torch.Tensor]]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "574514ba38f3fd252b1bb1fbd59a52067d75c92c",
            "filename": "src/transformers/models/gpt_neox/modeling_gpt_neox.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -516,7 +516,7 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Union[Cache, tuple[tuple[torch.FloatTensor]]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -611,7 +611,7 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Union[Cache, tuple[tuple[torch.FloatTensor]]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -686,7 +686,7 @@ def __init__(self, config):\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, tuple[tuple[torch.Tensor]]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "1be535ffd583243e9b1dde2058028d74793cb295",
            "filename": "src/transformers/models/gpt_neox/modular_gpt_neox.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodular_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodular_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodular_gpt_neox.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -389,7 +389,7 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Union[Cache, tuple[tuple[torch.FloatTensor]]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -484,7 +484,7 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Union[Cache, tuple[tuple[torch.FloatTensor]]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -559,7 +559,7 @@ def __init__(self, config):\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, tuple[tuple[torch.Tensor]]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "ebb92c19d943ec711dc3df5154048617184579b2",
            "filename": "src/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -405,7 +405,7 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Union[Cache, tuple[tuple[torch.FloatTensor]]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -655,7 +655,7 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Union[Cache, tuple[tuple[torch.FloatTensor]]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,"
        },
        {
            "sha": "d4eb17ab4441e31b4c9bd3c36c89519cf592dc3a",
            "filename": "src/transformers/models/gptj/modeling_gptj.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -506,7 +506,7 @@ def set_input_embeddings(self, new_embeddings):\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, tuple[tuple[torch.Tensor]]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n@@ -753,7 +753,7 @@ def __init__(self, config):\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, tuple[tuple[torch.Tensor]]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "45d82bb84b129367305a4f0328246e889981b251",
            "filename": "src/transformers/models/granite/modeling_granite.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -494,7 +494,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "9b2203cbff0eb078d9c57bcab4a9f8185f3a664f",
            "filename": "src/transformers/models/granite/modular_granite.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodular_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodular_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodular_granite.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -232,7 +232,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "427a92dfbef83770a3f873faa2bf9743922e95b0",
            "filename": "src/transformers/models/granitemoe/modeling_granitemoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -629,7 +629,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         output_router_logits: Optional[bool] = None,"
        },
        {
            "sha": "94a88380590b9e4c98c5803744b0f0b04f048e8f",
            "filename": "src/transformers/models/granitemoe/modular_granitemoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodular_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodular_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodular_granitemoe.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -240,7 +240,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         output_router_logits: Optional[bool] = None,"
        },
        {
            "sha": "5bc12365e3cc5e23498eecd71d742aa3ac20dc05",
            "filename": "src/transformers/models/granitemoehybrid/modeling_granitemoehybrid.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -1206,7 +1206,7 @@ def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -1379,7 +1379,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         output_router_logits: Optional[bool] = None,"
        },
        {
            "sha": "50291f41d3b00a998ad679e51570098d659cc1d5",
            "filename": "src/transformers/models/granitemoehybrid/modular_granitemoehybrid.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodular_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodular_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodular_granitemoehybrid.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -192,7 +192,7 @@ def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "81ae22877ecb7d540208599fc22e0a7c2b54b02d",
            "filename": "src/transformers/models/granitemoeshared/modeling_granitemoeshared.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -700,7 +700,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         output_router_logits: Optional[bool] = None,"
        },
        {
            "sha": "0d6fe3be0c87e1d8f5189b6b2ff4f502ae07eb7e",
            "filename": "src/transformers/models/informer/modeling_informer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -1151,13 +1151,6 @@ def forward(\n         # initialize `past_key_values`\n         if use_cache and past_key_values is None:\n             past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n-        if use_cache and isinstance(past_key_values, tuple):\n-            logger.warning_once(\n-                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n-            )\n-            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n         past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n         if cache_position is None:"
        },
        {
            "sha": "e8f11bcfe132a5a35a179dea879d5d53e50a347b",
            "filename": "src/transformers/models/kosmos2/modeling_kosmos2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -1031,13 +1031,6 @@ def forward(\n                 if encoder_hidden_states is not None\n                 else DynamicCache(config=self.config)\n             )\n-        if use_cache and isinstance(past_key_values, tuple):\n-            logger.warning_once(\n-                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n-            )\n-            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n         past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n "
        },
        {
            "sha": "e09289f465c741fd19131b0fa1bacbaa13e1fad1",
            "filename": "src/transformers/models/kosmos2_5/modeling_kosmos2_5.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fmodeling_kosmos2_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fmodeling_kosmos2_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fmodeling_kosmos2_5.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -321,7 +321,7 @@ class Kosmos2_5ForConditionalGenerationModelOutput(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None\n+    past_key_values: Optional[Cache] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n     width: Optional[torch.FloatTensor] = None\n@@ -1343,7 +1343,7 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         image_embeds: Optional[torch.Tensor] = None,\n         image_embeds_position_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -1408,7 +1408,7 @@ def forward(\n         height: Optional[torch.Tensor] = None,\n         image_embeds_position_mask: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         image_embeds: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n@@ -1539,7 +1539,7 @@ def forward(\n         image_embeds: Optional[torch.Tensor] = None,\n         image_embeds_position_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -1702,7 +1702,7 @@ def forward(\n         height: Optional[torch.Tensor] = None,\n         image_embeds_position_mask: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         image_embeds: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,"
        },
        {
            "sha": "c68d6835e95897bd7ad0c14e919cc656b50887cd",
            "filename": "src/transformers/models/kyutai_speech_to_text/modeling_kyutai_speech_to_text.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -821,7 +821,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,"
        },
        {
            "sha": "b0b4aecc1c920d06c5b6ff3ebc037bca657cf210",
            "filename": "src/transformers/models/led/modeling_led.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -1681,13 +1681,6 @@ def forward(\n \n         if use_cache and past_key_values is None:\n             past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n-        if use_cache and isinstance(past_key_values, tuple):\n-            logger.warning_once(\n-                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n-            )\n-            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n         past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n         # create causal mask"
        },
        {
            "sha": "341195466cf00f783315d20dcaf1a7bb0918ac3e",
            "filename": "src/transformers/models/lfm2/modeling_lfm2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -249,9 +249,6 @@ def crop(self, max_length: int):\n     def __len__(self) -> int:\n         return len(self.key_cache)\n \n-    def __getitem__(self, layer_idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n-        return self.key_cache[layer_idx], self.value_cache[layer_idx]\n-\n     def reset(self):\n         for layer_idx in range(len(self.conv_cache)):\n             # In-place ops prevent breaking the static address"
        },
        {
            "sha": "42ed1e0acfb139a4db4703951f4344a5ec0aa381",
            "filename": "src/transformers/models/lfm2/modular_lfm2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodular_lfm2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodular_lfm2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodular_lfm2.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -207,9 +207,6 @@ def crop(self, max_length: int):\n     def __len__(self) -> int:\n         return len(self.key_cache)\n \n-    def __getitem__(self, layer_idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n-        return self.key_cache[layer_idx], self.value_cache[layer_idx]\n-\n     def reset(self):\n         for layer_idx in range(len(self.conv_cache)):\n             # In-place ops prevent breaking the static address"
        },
        {
            "sha": "e019795766ad5df54217ed0fd67e3283bdde72f7",
            "filename": "src/transformers/models/lfm2_moe/modeling_lfm2_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fmodeling_lfm2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fmodeling_lfm2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fmodeling_lfm2_moe.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -312,9 +312,6 @@ def crop(self, max_length: int):\n     def __len__(self) -> int:\n         return len(self.key_cache)\n \n-    def __getitem__(self, layer_idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n-        return self.key_cache[layer_idx], self.value_cache[layer_idx]\n-\n     def reset(self):\n         for layer_idx in range(len(self.conv_cache)):\n             # In-place ops prevent breaking the static address"
        },
        {
            "sha": "63cf0a4469411676b7d02f745f1335b4049ca331",
            "filename": "src/transformers/models/llama4/modeling_llama4.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -584,7 +584,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "5f04f4017db5a4a00259c6a486ee3abf4484b828",
            "filename": "src/transformers/models/m2m_100/modeling_m2m_100.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -1019,13 +1019,6 @@ def forward(\n         # initialize `past_key_values`\n         if use_cache and past_key_values is None:\n             past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n-        if use_cache and isinstance(past_key_values, tuple):\n-            logger.warning_once(\n-                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n-            )\n-            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n         batch_size, seq_length = inputs_embeds.size()[:-1]\n         past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "d5e2690a2cc371c4a3fb67b4b6b9b30ada6bbb4d",
            "filename": "src/transformers/models/marian/modeling_marian.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -950,13 +950,6 @@ def forward(\n                 if encoder_hidden_states is not None\n                 else DynamicCache(config=self.config)\n             )\n-        if use_cache and isinstance(past_key_values, tuple):\n-            logger.warning_once(\n-                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n-            )\n-            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n         batch_size, seq_length = inputs_embeds.size()[:-1]\n         past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "15d6de54a89b08febc3d27a816ea252299232e52",
            "filename": "src/transformers/models/mbart/modeling_mbart.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -993,13 +993,6 @@ def forward(\n                 if encoder_hidden_states is not None\n                 else DynamicCache(config=self.config)\n             )\n-        if use_cache and isinstance(past_key_values, tuple):\n-            logger.warning_once(\n-                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n-            )\n-            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n         batch_size, seq_length = inputs_embeds.size()[:-1]\n         past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "3612beb3eb54a8c6af091858f3c5f48eda1f2ae4",
            "filename": "src/transformers/models/megatron_bert/modeling_megatron_bert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -393,13 +393,6 @@ def forward(\n                 use_cache = False\n         if use_cache and past_key_values is None:\n             past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n-        if use_cache and isinstance(past_key_values, tuple):\n-            logger.warning_once(\n-                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n-            )\n-            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n         all_hidden_states = () if output_hidden_states else None\n         all_self_attentions = () if output_attentions else None"
        },
        {
            "sha": "686569d42a8d0964ba12c145c7120db580648f82",
            "filename": "src/transformers/models/mimi/modeling_mimi.py",
            "status": "modified",
            "additions": 11,
            "deletions": 11,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -68,8 +68,8 @@ class MimiOutput(ModelOutput):\n \n     audio_codes: Optional[torch.LongTensor] = None\n     audio_values: Optional[torch.FloatTensor] = None\n-    encoder_past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None\n-    decoder_past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None\n+    encoder_past_key_values: Optional[Cache] = None\n+    decoder_past_key_values: Optional[Cache] = None\n \n \n class MimiConv1dPaddingCache:\n@@ -176,7 +176,7 @@ class MimiEncoderOutput(ModelOutput):\n     \"\"\"\n \n     audio_codes: Optional[torch.LongTensor] = None\n-    encoder_past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None\n+    encoder_past_key_values: Optional[Cache] = None\n     padding_cache: Optional[MimiConv1dPaddingCache] = None\n \n \n@@ -197,7 +197,7 @@ class MimiDecoderOutput(ModelOutput):\n     \"\"\"\n \n     audio_values: Optional[torch.FloatTensor] = None\n-    decoder_past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None\n+    decoder_past_key_values: Optional[Cache] = None\n \n \n class MimiConv1d(nn.Module):\n@@ -1013,7 +1013,7 @@ def forward(\n         hidden_states: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -1439,7 +1439,7 @@ def _encode_frame(\n         input_values: torch.Tensor,\n         num_quantizers: int,\n         padding_mask: int,\n-        past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         padding_cache: Optional[MimiConv1dPaddingCache] = None,\n         return_dict: Optional[bool] = None,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n@@ -1502,7 +1502,7 @@ def encode(\n         input_values: torch.Tensor,\n         padding_mask: Optional[torch.Tensor] = None,\n         num_quantizers: Optional[float] = None,\n-        encoder_past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n+        encoder_past_key_values: Optional[Cache] = None,\n         padding_cache: Optional[MimiConv1dPaddingCache] = None,\n         use_streaming: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n@@ -1590,7 +1590,7 @@ def encode(\n     def _decode_frame(\n         self,\n         codes: torch.Tensor,\n-        past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         return_dict: Optional[bool] = None,\n     ) -> torch.Tensor:\n         embeddings = self.quantizer.decode(codes)\n@@ -1611,7 +1611,7 @@ def decode(\n         self,\n         audio_codes: torch.Tensor,\n         padding_mask: Optional[torch.Tensor] = None,\n-        decoder_past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n+        decoder_past_key_values: Optional[Cache] = None,\n         return_dict: Optional[bool] = None,\n     ) -> Union[tuple[torch.Tensor, torch.Tensor], MimiDecoderOutput]:\n         \"\"\"\n@@ -1662,8 +1662,8 @@ def forward(\n         padding_mask: Optional[torch.Tensor] = None,\n         num_quantizers: Optional[int] = None,\n         audio_codes: Optional[torch.Tensor] = None,\n-        encoder_past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n-        decoder_past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n+        encoder_past_key_values: Optional[Cache] = None,\n+        decoder_past_key_values: Optional[Cache] = None,\n         return_dict: Optional[bool] = None,\n     ) -> Union[tuple[torch.Tensor, torch.Tensor], MimiOutput]:\n         r\"\"\""
        },
        {
            "sha": "b12325a17ef3098c637da5f0d2c6ea46efdf7102",
            "filename": "src/transformers/models/minimax/modeling_minimax.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -87,15 +87,6 @@ def get_linear_cache(self, layer_idx: int):\n     def __len__(self):\n         return max(super().__len__(), len(self.linear_cache))\n \n-    def __getitem__(self, layer_idx: int):\n-        if layer_idx < len(self.linear_cache) and self.linear_cache[layer_idx] != []:\n-            return (self.linear_cache[layer_idx],)\n-        return super().__getitem__(layer_idx)\n-\n-    def __iter__(self):\n-        for layer_idx in range(len(self)):\n-            yield self[layer_idx]\n-\n     def batch_repeat_interleave(self, repeats: int):\n         for layer_idx in range(len(self)):\n             if self.linear_cache[layer_idx] != []:"
        },
        {
            "sha": "2504b004878909fc362cf9e02a9b87017c41bfc1",
            "filename": "src/transformers/models/minimax/modular_minimax.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodular_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodular_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodular_minimax.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -203,15 +203,6 @@ def get_linear_cache(self, layer_idx: int):\n     def __len__(self):\n         return max(super().__len__(), len(self.linear_cache))\n \n-    def __getitem__(self, layer_idx: int):\n-        if layer_idx < len(self.linear_cache) and self.linear_cache[layer_idx] != []:\n-            return (self.linear_cache[layer_idx],)\n-        return super().__getitem__(layer_idx)\n-\n-    def __iter__(self):\n-        for layer_idx in range(len(self)):\n-            yield self[layer_idx]\n-\n     def batch_repeat_interleave(self, repeats: int):\n         for layer_idx in range(len(self)):\n             if self.linear_cache[layer_idx] != []:"
        },
        {
            "sha": "e622225f270e4b5d379f1c8d3f82b6797b535fe4",
            "filename": "src/transformers/models/mllama/modeling_mllama.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -1167,7 +1167,7 @@ def forward(\n         cross_attention_states: Optional[torch.FloatTensor] = None,\n         cross_attention_mask: Optional[torch.Tensor] = None,\n         full_text_row_masked_out_mask: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n-        past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -1303,7 +1303,7 @@ def forward(\n         cross_attention_states: Optional[torch.LongTensor] = None,\n         cross_attention_mask: Optional[torch.LongTensor] = None,\n         full_text_row_masked_out_mask: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n-        past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -1580,7 +1580,7 @@ def forward(\n         cross_attention_mask: Optional[torch.Tensor] = None,\n         cross_attention_states: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "ea9b5f7255a9ce487b14db8da59b77075fc4ca76",
            "filename": "src/transformers/models/moonshine/modeling_moonshine.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -889,7 +889,7 @@ def forward(\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n         encoder_outputs: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n-        past_key_values: Optional[Union[EncoderDecoderCache, tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[EncoderDecoderCache] = None,\n         decoder_inputs_embeds: Optional[tuple[torch.FloatTensor]] = None,\n         decoder_position_ids: Optional[tuple[torch.LongTensor]] = None,\n         use_cache: Optional[bool] = None,\n@@ -1010,7 +1010,7 @@ def forward(\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n         encoder_outputs: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n-        past_key_values: Optional[Union[EncoderDecoderCache, tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[EncoderDecoderCache] = None,\n         decoder_inputs_embeds: Optional[tuple[torch.FloatTensor]] = None,\n         decoder_position_ids: Optional[tuple[torch.LongTensor]] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "e3776b349ac5be95d7dadbd2e8350704ca93929b",
            "filename": "src/transformers/models/moonshine/modular_moonshine.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -725,7 +725,7 @@ def forward(\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n         encoder_outputs: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n-        past_key_values: Optional[Union[EncoderDecoderCache, tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[EncoderDecoderCache] = None,\n         decoder_inputs_embeds: Optional[tuple[torch.FloatTensor]] = None,\n         decoder_position_ids: Optional[tuple[torch.LongTensor]] = None,\n         use_cache: Optional[bool] = None,\n@@ -830,7 +830,7 @@ def forward(\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n         encoder_outputs: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n-        past_key_values: Optional[Union[EncoderDecoderCache, tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[EncoderDecoderCache] = None,\n         decoder_inputs_embeds: Optional[tuple[torch.FloatTensor]] = None,\n         decoder_position_ids: Optional[tuple[torch.LongTensor]] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "3d496c21ae8fb65841bf5da661d9c4869a3b38e5",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -948,7 +948,7 @@ def forward(\n             use_cache = False\n \n         if use_cache and past_key_values is None and not self.training:\n-            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n+            past_key_values = DynamicCache(config=self.config)\n \n         past_seen_tokens = 0 if past_key_values is None else past_key_values.get_seq_length()\n         if cache_position is None:\n@@ -1211,7 +1211,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -1472,7 +1472,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,"
        },
        {
            "sha": "c01d3e58bef3d129b1226c95729d8f4b0e27670d",
            "filename": "src/transformers/models/mpt/modeling_mpt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 26,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fmpt%2Fmodeling_mpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fmpt%2Fmodeling_mpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmpt%2Fmodeling_mpt.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -242,25 +242,6 @@ def _init_weights(self, module: nn.Module):\n                 module.bias.data.zero_()\n             module.weight.data.fill_(1.0)\n \n-    @staticmethod\n-    def _convert_to_mpt_cache(\n-        past_key_values: tuple[tuple[torch.Tensor, torch.Tensor]],\n-    ) -> tuple[tuple[torch.Tensor, torch.Tensor]]:\n-        \"\"\"\n-        Converts the cache to the format expected by Mpt, i.e. to tuple(tuple([batch_size * num_heads, ...]))\n-        \"\"\"\n-        batch_size, num_heads, head_dim, seq_length = past_key_values[0][0].shape\n-        batch_size_times_num_heads = batch_size * num_heads\n-        # key:  [batch_size, num_heads, head_dim, seq_length] -> [batch_size * num_heads, head_dim, seq_length]\n-        # value: [batch_size, num_heads, seq_length, head_dim] -> [batch_size * num_heads, seq_length, head_dim]\n-        return tuple(\n-            (\n-                layer_past[0].reshape(batch_size_times_num_heads, head_dim, seq_length),\n-                layer_past[1].reshape(batch_size_times_num_heads, seq_length, head_dim),\n-            )\n-            for layer_past in past_key_values\n-        )\n-\n \n @auto_docstring\n class MptModel(MptPreTrainedModel):\n@@ -350,13 +331,6 @@ def forward(\n \n         if use_cache and past_key_values is None:\n             past_key_values = DynamicCache(config=self.config)\n-        if use_cache and isinstance(past_key_values, tuple):\n-            logger.warning_once(\n-                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `DynamicCache` instead, e.g. \"\n-                \"`past_key_values=DynamicCache.from_legacy_cache(past_key_values)`.\"\n-            )\n-            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n \n         hidden_states = inputs_embeds\n "
        },
        {
            "sha": "beae7750c8d2dec41a80ba905956cb0b259c37cd",
            "filename": "src/transformers/models/musicgen/modeling_musicgen.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -540,13 +540,6 @@ def forward(\n \n         if use_cache and past_key_values is None:\n             past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n-        if use_cache and isinstance(past_key_values, tuple):\n-            logger.warning_once(\n-                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n-            )\n-            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n         past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n "
        },
        {
            "sha": "abab804807b44a44be505d4412612ddcd53aaf1f",
            "filename": "src/transformers/models/musicgen_melody/modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -513,13 +513,6 @@ def forward(\n \n         if use_cache and past_key_values is None:\n             past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n-        if use_cache and isinstance(past_key_values, tuple):\n-            logger.warning_once(\n-                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n-            )\n-            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n         past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n         if inputs_embeds is None:"
        },
        {
            "sha": "2a0abcd5bb0b573ff8e9a28abc92b6425963afa2",
            "filename": "src/transformers/models/mvp/modeling_mvp.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -805,13 +805,6 @@ def forward(\n                 if encoder_hidden_states is not None\n                 else DynamicCache(config=self.config)\n             )\n-        if use_cache and isinstance(past_key_values, tuple):\n-            logger.warning_once(\n-                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n-            )\n-            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n         past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n         attention_mask = _prepare_4d_causal_attention_mask("
        },
        {
            "sha": "8a4aac07c47014b7e5e8e67c189688e417308672",
            "filename": "src/transformers/models/nemotron/modeling_nemotron.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -637,7 +637,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -869,7 +869,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "159653721b4a9f02557c512edfb68fa3a4af32c0",
            "filename": "src/transformers/models/nllb_moe/modeling_nllb_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -857,14 +857,6 @@ def forward(\n         if use_cache and past_key_values is None:\n             past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n \n-        if use_cache and isinstance(past_key_values, tuple):\n-            logger.warning_once(\n-                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n-            )\n-            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n-\n         past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n         attention_mask = self._update_causal_mask(\n             attention_mask,"
        },
        {
            "sha": "60726336095f2715663194e6373e0cb18681c17e",
            "filename": "src/transformers/models/pegasus/modeling_pegasus.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -1000,13 +1000,6 @@ def forward(\n                 if encoder_hidden_states is not None\n                 else DynamicCache(config=self.config)\n             )\n-        if use_cache and isinstance(past_key_values, tuple):\n-            logger.warning_once(\n-                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n-            )\n-            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n         batch_size, seq_length = inputs_embeds.size()[:-1]\n         past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "da2ef82f3c10da0d8221084c01975c161d7cf974",
            "filename": "src/transformers/models/pegasus_x/modeling_pegasus_x.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -1298,13 +1298,6 @@ def forward(\n         # initialize `past_key_values`\n         if use_cache and past_key_values is None:\n             past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n-        if use_cache and isinstance(past_key_values, tuple):\n-            logger.warning_once(\n-                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n-            )\n-            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n         batch_size, seq_length = inputs_embeds.size()[:-1]\n         past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "7d6ef114c1951aabe8785f080b469c6d45de111e",
            "filename": "src/transformers/models/plbart/modeling_plbart.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -918,13 +918,6 @@ def forward(\n                 if encoder_hidden_states is not None\n                 else DynamicCache(config=self.config)\n             )\n-        if use_cache and isinstance(past_key_values, tuple):\n-            logger.warning_once(\n-                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n-            )\n-            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n         batch_size, seq_length = inputs_embeds.size()[:-1]\n         past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "bc5bf129e238b9337bd42c9fb0d612aa5cc0f6cb",
            "filename": "src/transformers/models/prophetnet/modeling_prophetnet.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -1185,13 +1185,6 @@ def forward(\n                 if encoder_hidden_states is not None\n                 else DynamicCache(config=self.config)\n             )\n-        if use_cache and isinstance(past_key_values, tuple):\n-            logger.warning_once(\n-                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n-            )\n-            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n         past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n "
        },
        {
            "sha": "95801fe257c715feb11c69a28346ddcbb4de0525",
            "filename": "src/transformers/models/qwen3_next/modeling_qwen3_next.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodeling_qwen3_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodeling_qwen3_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodeling_qwen3_next.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -115,9 +115,6 @@ def __init__(self, config: Qwen3NextConfig):\n     def __len__(self):\n         return len(self.layer_types)\n \n-    def __getitem__(self, layer_idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n-        return self.key_cache[layer_idx], self.value_cache[layer_idx]\n-\n     def update(\n         self,\n         key_states: torch.Tensor,"
        },
        {
            "sha": "59879cd50aac2f560c7fbfe40a154b6ce9a92d2f",
            "filename": "src/transformers/models/qwen3_next/modular_qwen3_next.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodular_qwen3_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodular_qwen3_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodular_qwen3_next.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -123,9 +123,6 @@ def __init__(self, config: Qwen3NextConfig):\n     def __len__(self):\n         return len(self.layer_types)\n \n-    def __getitem__(self, layer_idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n-        return self.key_cache[layer_idx], self.value_cache[layer_idx]\n-\n     def update(\n         self,\n         key_states: torch.Tensor,"
        },
        {
            "sha": "ea19441ddf4a81977d988f3b98a6764b10e4a1ae",
            "filename": "src/transformers/models/rag/modeling_rag.py",
            "status": "modified",
            "additions": 13,
            "deletions": 4,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -1184,15 +1184,24 @@ def _reorder_stacked(hidden_states, new_order):\n             return result\n \n         reordered_past = ()\n-        for layer_past in past_key_values:\n+        for idx in range(len(past_key_values)):\n+            if isinstance(past_key_values, EncoderDecoderCache):\n+                layer_past = (\n+                    past_key_values.self_attention_cache.layers[idx].keys,\n+                    past_key_values.self_attention_cache.layers[idx].values,\n+                    past_key_values.cross_attention_cache.layers[idx].keys,\n+                    past_key_values.cross_attention_cache.layers[idx].values,\n+                )\n+            else:\n+                layer_past = (past_key_values.layers[idx].keys, past_key_values.layers[idx].values)\n             # get the correct batch idx from decoder layer's batch dim for cross and self-attn\n             reordered_past += (\n                 tuple(_reorder_stacked(past_state, beam_idx.to(past_state.device)) for past_state in layer_past),\n             )\n-        if isinstance(past_key_values, EncoderDecoderCache):\n-            reordered_past = EncoderDecoderCache.from_legacy_cache(reordered_past)\n \n-        return reordered_past\n+        # Cast back to the correct cache class\n+        reordered_cache = type(past_key_values)(reordered_past)\n+        return reordered_cache\n \n     def marginalize(self, seq_logits, doc_scores, n_docs=None):\n         n_docs = n_docs if n_docs is not None else self.config.n_docs"
        },
        {
            "sha": "9cdc7253c312ec1ce00e216225a49dc7123598e0",
            "filename": "src/transformers/models/reformer/modeling_reformer.py",
            "status": "modified",
            "additions": 16,
            "deletions": 69,
            "changes": 85,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Freformer%2Fmodeling_reformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Freformer%2Fmodeling_reformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Freformer%2Fmodeling_reformer.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -76,24 +76,6 @@ def __init__(self, _distributed_cache_data: Optional[Iterable] = None) -> None:\n                 self.buckets_cache.append(buckets)\n                 self.states_cache.append(states)\n \n-    def __getitem__(self, layer_idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n-        \"\"\"\n-        Support for backwards-compatible `past_key_values` indexing, e.g. `past_key_values[0][0].shape[2]` to get the\n-        sequence length.\n-        \"\"\"\n-        if layer_idx < len(self):\n-            return (self.buckets_cache[layer_idx], self.states_cache[layer_idx])\n-        else:\n-            raise KeyError(f\"Cache only has {len(self)} layers, attempted to access layer with index {layer_idx}\")\n-\n-    def __iter__(self):\n-        \"\"\"\n-        Support for backwards-compatible `past_key_values` iteration, e.g. `for x in past_key_values:` to iterate over\n-        keys and values\n-        \"\"\"\n-        for layer_idx in range(len(self)):\n-            yield (self.buckets_cache[layer_idx], self.states_cache[layer_idx])\n-\n     def __len__(self):\n         \"\"\"\n         Support for backwards-compatible `past_key_values` length, e.g. `len(past_key_values)`. This value corresponds\n@@ -149,28 +131,20 @@ def update(\n     def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n         return None\n \n-    def to_legacy_cache(self) -> tuple[tuple[torch.Tensor, torch.Tensor]]:\n-        \"\"\"Converts the `ReformerDynamicCache` instance into the its equivalent in the legacy cache format. Used for\n-        backward compatibility.\"\"\"\n-        legacy_cache = ()\n+    def get_start_idx(self) -> int:\n+        if len(self) == 0:\n+            return 0\n+        return self.states_cache[0].shape[1]\n+\n+    def reorder_cache(self, beam_idx):\n         for layer_idx in range(len(self)):\n-            buckets, states = self.buckets_cache[layer_idx], self.states_cache[layer_idx]\n-            buckets = buckets if buckets.numel() != 0 else None\n-            legacy_cache += ((buckets, states),)\n-        return legacy_cache\n-\n-    @classmethod\n-    def from_legacy_cache(\n-        cls, past_buckets_states: Optional[tuple[tuple[torch.FloatTensor, torch.FloatTensor]]] = None\n-    ) -> \"ReformerDynamicCache\":\n-        \"\"\"Converts a cache in the legacy cache format into an equivalent `ReformerDynamicCache`. Used for\n-        backward compatibility.\"\"\"\n-        cache = cls()\n-        if past_buckets_states is not None:\n-            for layer_idx in range(len(past_buckets_states)):\n-                buckets, states = past_buckets_states[layer_idx]\n-                cache.update(buckets, states, layer_idx)\n-        return cache\n+            if self.buckets_cache[layer_idx] is not None and self.buckets_cache[layer_idx].numel() > 0:\n+                device = self.buckets_cache[layer_idx].device\n+                self.buckets_cache[layer_idx] = self.buckets_cache[layer_idx].index_select(0, beam_idx.to(device))\n+\n+            if self.states_cache[layer_idx] is not None and self.states_cache[layer_idx].numel() > 0:\n+                device = self.states_cache[layer_idx].device\n+                self.states_cache[layer_idx] = self.states_cache[layer_idx].index_select(0, beam_idx.to(device))\n \n \n def _stable_argsort(vector, dim):\n@@ -1803,13 +1777,6 @@ def forward(\n         # init cached hidden states if necessary\n         if use_cache and past_buckets_states is None:\n             past_buckets_states = ReformerDynamicCache()\n-        elif use_cache and isinstance(past_buckets_states, tuple):\n-            logger.warning_once(\n-                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `ReformerDynamicCache` instead, e.g. \"\n-                \"`past_key_values=ReformerDynamicCache.from_legacy_cache(past_key_values)`.\"\n-            )\n-            past_buckets_states = ReformerDynamicCache.from_legacy_cache(past_buckets_states)\n \n         # concat same tensor for reversible ResNet\n         hidden_states = torch.cat([hidden_states, hidden_states], dim=-1)\n@@ -1999,7 +1966,7 @@ def forward(\n         position_ids: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         num_hashes: Optional[int] = None,\n-        past_buckets_states: Optional[list[tuple[torch.Tensor]]] = None,\n+        past_buckets_states: Optional[ReformerDynamicCache] = None,\n         use_cache: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -2020,7 +1987,7 @@ def forward(\n             the default defined in `config.num_hashes`.\n \n             For more information, see `num_hashes` in [`ReformerConfig`].\n-        past_buckets_states (`list[tuple(torch.LongTensor, torch.FloatTensor)]`, *optional*):\n+        past_buckets_states (`ReformerDynamicCache`, *optional*):\n             List of `tuple(torch.LongTensor, torch.FloatTensor` of length `config.n_layers`, with the first element\n             being the previous *buckets* of shape `(batch_size, num_heads, num_hashes, sequence_length)`) and the\n             second being the previous *hidden_states* of shape `(batch_size, sequence_length, hidden_size)`).\n@@ -2090,10 +2057,7 @@ def forward(\n             )\n \n         # start index for position encoding depends on incremental decoding\n-        if past_buckets_states is not None:\n-            start_idx_pos_encodings = past_buckets_states[0][1].shape[1]\n-        else:\n-            start_idx_pos_encodings = 0\n+        start_idx_pos_encodings = past_buckets_states.get_start_idx() if past_buckets_states is not None else 0\n \n         embedding_output = self.embeddings(\n             input_ids=input_ids,\n@@ -2328,23 +2292,6 @@ def prepare_inputs_for_generation(\n \n         return model_inputs\n \n-    def _reorder_cache(self, past_key_values, beam_idx):\n-        reord_past_buckets_states = []\n-        for buckets, hidden_states in past_key_values:\n-            # buckets\n-            if buckets is not None and buckets.numel() > 0:\n-                reord_buckets = buckets.index_select(0, beam_idx.to(buckets.device))\n-            else:\n-                reord_buckets = None\n-\n-            # hidden states\n-            reord_hidden_states = hidden_states.index_select(0, beam_idx.to(hidden_states.device))\n-            reord_past_buckets_states.append((reord_buckets, reord_hidden_states))\n-\n-        if isinstance(past_key_values, ReformerDynamicCache):\n-            reord_past_buckets_states = ReformerDynamicCache.from_legacy_cache(reord_past_buckets_states)\n-        return reord_past_buckets_states\n-\n \n @auto_docstring\n class ReformerForMaskedLM(ReformerPreTrainedModel):"
        },
        {
            "sha": "92f601d662d071663deca70194918a4228a5d59d",
            "filename": "src/transformers/models/rembert/modeling_rembert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -408,13 +408,6 @@ def forward(\n \n         if use_cache and past_key_values is None:\n             past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n-        if use_cache and isinstance(past_key_values, tuple):\n-            logger.warning_once(\n-                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n-            )\n-            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n         hidden_states = self.embedding_hidden_mapping_in(hidden_states)\n         all_hidden_states = () if output_hidden_states else None"
        },
        {
            "sha": "7f7c970a72f6c45d7d42f4fb2b9d1712ddbe55fd",
            "filename": "src/transformers/models/roberta/modeling_roberta.py",
            "status": "modified",
            "additions": 4,
            "deletions": 14,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -27,7 +27,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN, gelu\n-from ...cache_utils import Cache, EncoderDecoderCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...masking_utils import create_causal_mask\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n@@ -641,7 +641,7 @@ def forward(\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Union[list[torch.FloatTensor], Cache]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -651,15 +651,8 @@ def forward(\n         else:\n             use_cache = False\n \n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n-            logger.warning_once(\n-                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n-            )\n-            return_legacy_cache = True\n-            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n+        if use_cache and past_key_values is None:\n+            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n@@ -708,9 +701,6 @@ def forward(\n         sequence_output = encoder_outputs.last_hidden_state\n         pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n \n-        if return_legacy_cache:\n-            encoder_outputs.past_key_values = encoder_outputs.past_key_values.to_legacy_cache()\n-\n         return BaseModelOutputWithPoolingAndCrossAttentions(\n             last_hidden_state=sequence_output,\n             pooler_output=pooled_output,"
        },
        {
            "sha": "9194fd67205e029845bb6ce12e9cc1450f0784b7",
            "filename": "src/transformers/models/roberta_prelayernorm/modeling_roberta_prelayernorm.py",
            "status": "modified",
            "additions": 3,
            "deletions": 13,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -22,7 +22,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN, gelu\n-from ...cache_utils import Cache, EncoderDecoderCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...masking_utils import create_causal_mask\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n@@ -676,15 +676,8 @@ def forward(\n         else:\n             use_cache = False\n \n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n-            logger.warning_once(\n-                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n-            )\n-            return_legacy_cache = True\n-            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n+        if use_cache and past_key_values is None:\n+            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n@@ -734,9 +727,6 @@ def forward(\n         sequence_output = self.LayerNorm(sequence_output)\n         pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n \n-        if return_legacy_cache:\n-            encoder_outputs.past_key_values = encoder_outputs.past_key_values.to_legacy_cache()\n-\n         return BaseModelOutputWithPoolingAndCrossAttentions(\n             last_hidden_state=sequence_output,\n             pooler_output=pooled_output,"
        },
        {
            "sha": "ecb514d6805daa62dc3f7c2fe8b56665f413f1f6",
            "filename": "src/transformers/models/roc_bert/modeling_roc_bert.py",
            "status": "modified",
            "additions": 4,
            "deletions": 14,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -21,7 +21,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, EncoderDecoderCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...masking_utils import create_causal_mask\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n@@ -734,7 +734,7 @@ def forward(\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Union[list[torch.FloatTensor], Cache]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -760,15 +760,8 @@ def forward(\n         else:\n             use_cache = False\n \n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n-            logger.warning_once(\n-                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n-            )\n-            return_legacy_cache = True\n-            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n+        if use_cache and past_key_values is None:\n+            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n@@ -819,9 +812,6 @@ def forward(\n         sequence_output = encoder_outputs[0]\n         pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n \n-        if return_legacy_cache:\n-            encoder_outputs.past_key_values = encoder_outputs.past_key_values.to_legacy_cache()\n-\n         return BaseModelOutputWithPoolingAndCrossAttentions(\n             last_hidden_state=sequence_output,\n             pooler_output=pooled_output,"
        },
        {
            "sha": "da8baec757b2299cffb5a1a72586ca01541bbdb9",
            "filename": "src/transformers/models/roformer/modeling_roformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_roformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_roformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_roformer.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -451,13 +451,6 @@ def forward(\n \n         if use_cache and past_key_values is None:\n             past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n-        if use_cache and isinstance(past_key_values, tuple):\n-            logger.warning_once(\n-                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n-            )\n-            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n         all_hidden_states = () if output_hidden_states else None\n         all_self_attentions = () if output_attentions else None"
        },
        {
            "sha": "4306059084603b941210c8f2c1f3435b0368b007",
            "filename": "src/transformers/models/seamless_m4t/modeling_seamless_m4t.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -1800,13 +1800,6 @@ def forward(\n         # initialize `past_key_values`\n         if use_cache and past_key_values is None:\n             past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n-        if use_cache and isinstance(past_key_values, tuple):\n-            logger.warning_once(\n-                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n-            )\n-            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n         past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n         attention_mask = _prepare_4d_causal_attention_mask("
        },
        {
            "sha": "00a069be389eb7d242a2c991a4aea602d65019c4",
            "filename": "src/transformers/models/seamless_m4t_v2/modeling_seamless_m4t_v2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -1842,13 +1842,6 @@ def forward(\n         # initialize `past_key_values`\n         if use_cache and past_key_values is None:\n             past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n-        if use_cache and isinstance(past_key_values, tuple):\n-            logger.warning_once(\n-                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n-            )\n-            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n         past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n         attention_mask = _prepare_4d_causal_attention_mask("
        },
        {
            "sha": "501d4715b8086ff5d542cbc3d98c35ad37cb3614",
            "filename": "src/transformers/models/speech_to_text/modeling_speech_to_text.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -826,13 +826,6 @@ def forward(\n \n         if use_cache and past_key_values is None:\n             past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n-        if use_cache and isinstance(past_key_values, tuple):\n-            logger.warning_once(\n-                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n-            )\n-            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n         past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n         attention_mask = self._update_causal_mask("
        },
        {
            "sha": "38acc81f068ce54954e9b5a24898d4cb0f7559c4",
            "filename": "src/transformers/models/speecht5/modeling_speecht5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -1520,13 +1520,6 @@ def forward(\n \n         if use_cache and past_key_values is None:\n             past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n-        if use_cache and isinstance(past_key_values, tuple):\n-            logger.warning_once(\n-                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n-            )\n-            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n         past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n "
        },
        {
            "sha": "04f5b5bc753ce670eec690e506bc0b01bdfd3bc3",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -328,7 +328,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "b1b80636224e8f0f4575126fcbd1986aa86ff0c9",
            "filename": "src/transformers/models/starcoder2/modular_starcoder2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodular_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodular_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodular_starcoder2.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -19,7 +19,7 @@\n # limitations under the License.\n \"\"\"PyTorch Starcoder2 model.\"\"\"\n \n-from typing import Callable, Optional, Union\n+from typing import Callable, Optional\n \n import torch\n from torch import nn\n@@ -154,7 +154,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "d08c15e4eea305305791749223f3e50a86dfd9ef",
            "filename": "src/transformers/models/tapas/modeling_tapas.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tapas.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tapas.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tapas.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -429,13 +429,6 @@ def forward(\n     ):\n         if use_cache and past_key_values is None:\n             past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n-        if use_cache and isinstance(past_key_values, tuple):\n-            logger.warning_once(\n-                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n-            )\n-            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n         all_hidden_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None"
        },
        {
            "sha": "9c693c095670c5c0b00b890abecdd64defd5b72b",
            "filename": "src/transformers/models/time_series_transformer/modeling_time_series_transformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -938,13 +938,6 @@ def forward(\n         # initialize `past_key_values`\n         if use_cache and past_key_values is None:\n             past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n-        if use_cache and isinstance(past_key_values, tuple):\n-            logger.warning_once(\n-                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n-            )\n-            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n         past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n         if cache_position is None:"
        },
        {
            "sha": "dc5d6782f5c1af1cd5c6d8300b92ac815a2b0b78",
            "filename": "src/transformers/models/trocr/modeling_trocr.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Ftrocr%2Fmodeling_trocr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Ftrocr%2Fmodeling_trocr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftrocr%2Fmodeling_trocr.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -553,13 +553,6 @@ def forward(\n                 if encoder_hidden_states is not None\n                 else DynamicCache(config=self.config)\n             )\n-        if use_cache and isinstance(past_key_values, tuple):\n-            logger.warning_once(\n-                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n-            )\n-            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n         past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n "
        },
        {
            "sha": "916aca89ace4a40413148fc28bc66345afb528e6",
            "filename": "src/transformers/models/whisper/generation_whisper.py",
            "status": "modified",
            "additions": 23,
            "deletions": 28,
            "changes": 51,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -1178,26 +1178,14 @@ def split_by_batch_index(values, key, batch_idx, is_shortform, beam_indices=None\n                 if not is_shortform:\n                     # we don't save `past_key_values` as this is too costly for longform\n                     return None\n-                elif isinstance(values, EncoderDecoderCache):\n-                    all_past_key_values = []\n-                    for layer_idx in range(self.config.decoder_layers):\n-                        layer_past_key_values = []\n-                        for cache_cls in [values.self_attention_cache, values.cross_attention_cache]:\n-                            for v in [cache_cls.layers[layer_idx].keys, cache_cls.layers[layer_idx].values]:\n-                                layer_past_key_values.append(v[batch_idx][None].cpu())\n-                        all_past_key_values.append(tuple(layer_past_key_values))\n-                    return EncoderDecoderCache.from_legacy_cache(tuple(all_past_key_values))\n-                else:\n-                    all_past_key_values = []\n-                    for v in range(len(values)):\n-                        layer_past_key_values = []\n-                        for w in values[v]:\n-                            if len(w) != 0:\n-                                layer_past_key_values.append(w[batch_idx][None].cpu())\n-                            else:\n-                                layer_past_key_values.append(w)\n-                        all_past_key_values.append(tuple(layer_past_key_values))\n-                    return tuple(all_past_key_values)\n+                all_past_key_values = []\n+                for layer_idx in range(self.config.decoder_layers):\n+                    layer_past_key_values = []\n+                    for cache_cls in [values.self_attention_cache, values.cross_attention_cache]:\n+                        for v in [cache_cls.layers[layer_idx].keys, cache_cls.layers[layer_idx].values]:\n+                            layer_past_key_values.append(v[batch_idx][None].cpu())\n+                    all_past_key_values.append(tuple(layer_past_key_values))\n+                return EncoderDecoderCache(tuple(all_past_key_values))\n \n             return values[batch_idx].cpu()\n \n@@ -1234,15 +1222,22 @@ def _stack_split_outputs(self, seek_outputs, model_output_type, device, kwargs):\n                 )\n             elif key == \"past_key_values\":\n                 if seek_outputs[0][key] is not None:\n-                    outputs[key] = tuple(\n-                        tuple(\n-                            torch.stack([v[key][i][j] for v in seek_outputs]).squeeze(1).to(device)\n-                            for j in range(len(seek_outputs[0][key][0]))\n+                    all_past_key_values = []\n+                    for layer_idx in range(len(seek_outputs[0][key])):\n+                        layer_past_key_values = tuple(\n+                            torch.stack(\n+                                [\n+                                    getattr(getattr(sub_output[key], sub_cache).layers[layer_idx], sub_key)\n+                                    for sub_output in seek_outputs\n+                                ]\n+                            )\n+                            .squeeze(1)\n+                            .to(device)\n+                            for sub_cache in [\"self_attention_cache\", \"cross_attention_cache\"]\n+                            for sub_key in [\"keys\", \"values\"]\n                         )\n-                        for i in range(len(seek_outputs[0][key]))\n-                    )\n-                    if isinstance(seek_outputs[0][key], EncoderDecoderCache):\n-                        outputs[key] = EncoderDecoderCache.from_legacy_cache(outputs[key])\n+                        all_past_key_values.append(layer_past_key_values)\n+                    outputs[key] = EncoderDecoderCache(tuple(all_past_key_values))\n                 else:\n                     outputs[key] = None\n "
        },
        {
            "sha": "9c572bfc336b50bb0e21c2b474ac88dc9fb44b3b",
            "filename": "src/transformers/models/xglm/modeling_xglm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -472,13 +472,6 @@ def forward(\n                 if encoder_hidden_states is not None\n                 else DynamicCache(config=self.config)\n             )\n-        if use_cache and isinstance(past_key_values, tuple):\n-            logger.warning_once(\n-                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n-            )\n-            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n         past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n         attention_mask = _prepare_4d_causal_attention_mask("
        },
        {
            "sha": "be38199f42d57e0214825730cc79edd7f75d1f19",
            "filename": "src/transformers/models/xlm/modeling_xlm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fxlm%2Fmodeling_xlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fxlm%2Fmodeling_xlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm%2Fmodeling_xlm.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -820,9 +820,6 @@ def forward(\n         if cache is None:\n             cache = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n \n-        if isinstance(cache, tuple):\n-            cache = EncoderDecoderCache.from_legacy_cache(cache)\n-\n         if lengths is None:\n             if input_ids is not None:\n                 lengths = (input_ids != self.pad_index).sum(dim=1).long()"
        },
        {
            "sha": "ec6cc860867496f908a27ba005d6484471ad3bef",
            "filename": "src/transformers/models/xlm_roberta/modeling_xlm_roberta.py",
            "status": "modified",
            "additions": 4,
            "deletions": 14,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -27,7 +27,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN, gelu\n-from ...cache_utils import Cache, EncoderDecoderCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...masking_utils import create_causal_mask\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n@@ -660,7 +660,7 @@ def forward(\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Union[list[torch.FloatTensor], Cache]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -670,15 +670,8 @@ def forward(\n         else:\n             use_cache = False\n \n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n-            logger.warning_once(\n-                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n-            )\n-            return_legacy_cache = True\n-            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n+        if use_cache and past_key_values is None:\n+            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n@@ -727,9 +720,6 @@ def forward(\n         sequence_output = encoder_outputs.last_hidden_state\n         pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n \n-        if return_legacy_cache:\n-            encoder_outputs.past_key_values = encoder_outputs.past_key_values.to_legacy_cache()\n-\n         return BaseModelOutputWithPoolingAndCrossAttentions(\n             last_hidden_state=sequence_output,\n             pooler_output=pooled_output,"
        },
        {
            "sha": "df81f943f85d8495065ed853ba8be1528b9b5d25",
            "filename": "src/transformers/models/xlm_roberta_xl/modeling_xlm_roberta_xl.py",
            "status": "modified",
            "additions": 4,
            "deletions": 14,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -32,7 +32,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN, gelu\n-from ...cache_utils import Cache, EncoderDecoderCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...masking_utils import create_causal_mask\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n@@ -647,7 +647,7 @@ def forward(\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Union[list[torch.FloatTensor], Cache]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -657,15 +657,8 @@ def forward(\n         else:\n             use_cache = False\n \n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n-            logger.warning_once(\n-                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n-            )\n-            return_legacy_cache = True\n-            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n+        if use_cache and past_key_values is None:\n+            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n@@ -714,9 +707,6 @@ def forward(\n         sequence_output = encoder_outputs.last_hidden_state\n         pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n \n-        if return_legacy_cache:\n-            encoder_outputs.past_key_values = encoder_outputs.past_key_values.to_legacy_cache()\n-\n         return BaseModelOutputWithPoolingAndCrossAttentions(\n             last_hidden_state=sequence_output,\n             pooler_output=pooled_output,"
        },
        {
            "sha": "3df4e1676b85ea22c7c3dad33d2de89c608a4bc6",
            "filename": "src/transformers/models/xmod/modeling_xmod.py",
            "status": "modified",
            "additions": 3,
            "deletions": 13,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fxmod%2Fmodeling_xmod.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fxmod%2Fmodeling_xmod.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxmod%2Fmodeling_xmod.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -21,7 +21,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN, gelu\n-from ...cache_utils import Cache, EncoderDecoderCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...masking_utils import create_causal_mask\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n@@ -773,15 +773,8 @@ def forward(\n         else:\n             use_cache = False\n \n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n-            logger.warning_once(\n-                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n-            )\n-            return_legacy_cache = True\n-            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n+        if use_cache and past_key_values is None:\n+            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n@@ -840,9 +833,6 @@ def forward(\n         sequence_output = encoder_outputs[0]\n         pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n \n-        if return_legacy_cache:\n-            encoder_outputs.past_key_values = encoder_outputs.past_key_values.to_legacy_cache()\n-\n         return BaseModelOutputWithPoolingAndCrossAttentions(\n             last_hidden_state=sequence_output,\n             pooler_output=pooled_output,"
        },
        {
            "sha": "35fe0cbb3c4d42a5265c8a9eee8aa3c3059fa0f4",
            "filename": "src/transformers/models/zamba/modeling_zamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -143,9 +143,6 @@ def __init__(self, config, batch_size, dtype=torch.float16, device=None):\n     def __len__(self):\n         return len(self.key_cache)\n \n-    def __getitem__(self, layer_idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n-        return self.key_cache[layer_idx], self.value_cache[layer_idx]\n-\n     # Copied from transformers.models.jamba.modeling_jamba.HybridMambaAttentionDynamicCache.update\n     def update(\n         self,\n@@ -1223,7 +1220,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "3f3648420c3a4969e1b0dec481c060d037314539",
            "filename": "src/transformers/models/zamba2/modeling_zamba2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -148,9 +148,6 @@ def __init__(\n     def __len__(self):\n         return len(self.key_cache)\n \n-    def __getitem__(self, layer_idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n-        return self.key_cache[layer_idx], self.value_cache[layer_idx]\n-\n     def update(\n         self,\n         key_states: torch.Tensor,\n@@ -1646,7 +1643,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "46f50175d1eff3659d75fbd60255def6a6f941b9",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 28,
            "deletions": 57,
            "changes": 85,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -1052,7 +1052,6 @@ def test_past_key_values_format(self, custom_all_cache_shapes=None):\n \n             # 2. retrieve the KV cache and compute its default expected shapes (if no custom shapes are provided)\n             past_kv = outputs[\"past_key_values\"]\n-            is_legacy_cache = not isinstance(past_kv, Cache)\n \n             num_decoder_layers = decoder_config.num_hidden_layers\n             if custom_all_cache_shapes is None:\n@@ -1090,30 +1089,19 @@ def test_past_key_values_format(self, custom_all_cache_shapes=None):\n             # 3. Check cache shapes\n             # 3.1. Encoder-Decoder checks\n             if config.is_encoder_decoder:\n-                num_cache_decoder_layers = len(past_kv) if is_legacy_cache else len(past_kv.self_attention_cache)\n+                num_cache_decoder_layers = len(past_kv)\n                 self.assertEqual(num_cache_decoder_layers, num_decoder_layers)\n \n                 for i in range(num_decoder_layers):\n-                    if is_legacy_cache:\n-                        self.assertEqual(len(past_kv[0]), 4)  # legacy check: confirm number of elements in tuple\n-\n                     # Self attention\n-                    self_attention_layer_keys = (\n-                        past_kv[i][0] if is_legacy_cache else past_kv.self_attention_cache.layers[i].keys\n-                    )\n-                    self_attention_layer_values = (\n-                        past_kv[i][1] if is_legacy_cache else past_kv.self_attention_cache.layers[i].values\n-                    )\n+                    self_attention_layer_keys = past_kv.self_attention_cache.layers[i].keys\n+                    self_attention_layer_values = past_kv.self_attention_cache.layers[i].values\n                     self.assertEqual(self_attention_layer_keys.shape, all_cache_shapes[i][0])\n                     self.assertEqual(self_attention_layer_values.shape, all_cache_shapes[i][1])\n \n                     # Cross attention (ignore 3rd dim, see default shape preparation)\n-                    cross_attention_layer_keys = (\n-                        past_kv[i][2] if is_legacy_cache else past_kv.cross_attention_cache.layers[i].keys\n-                    )\n-                    cross_attention_layer_values = (\n-                        past_kv[i][3] if is_legacy_cache else past_kv.cross_attention_cache.layers[i].values\n-                    )\n+                    cross_attention_layer_keys = past_kv.cross_attention_cache.layers[i].keys\n+                    cross_attention_layer_values = past_kv.cross_attention_cache.layers[i].values\n                     cross_attention_layer_keys = cross_attention_layer_keys[:, :, 0, :]\n                     cross_attention_layer_values = cross_attention_layer_values[:, :, 0, :]\n                     self.assertEqual(cross_attention_layer_keys.shape, all_cache_shapes[i][2])\n@@ -1129,15 +1117,8 @@ def test_past_key_values_format(self, custom_all_cache_shapes=None):\n                 )\n \n                 for i in range(num_cache_decoder_layers):\n-                    if is_legacy_cache:\n-                        self.assertEqual(len(past_kv[0]), 2)  # legacy check: confirm number of elements in tuple\n-\n-                    # Self attention\n-                    if is_legacy_cache:\n-                        self_attention_layer_keys = past_kv[i][0]\n-                        self_attention_layer_values = past_kv[i][1]\n-                    elif getattr(past_kv, \"layers\", None) is None:\n-                        # Cache is lot layered (i.e, Mamba derivatives)\n+                    # Cache is lot layered (i.e, Mamba derivatives)\n+                    if getattr(past_kv, \"layers\", None) is None:\n                         self_attention_layer_keys = past_kv.key_cache[i]\n                         self_attention_layer_values = past_kv.value_cache[i]\n                     else:\n@@ -1416,14 +1397,7 @@ def test_generate_continue_from_past_key_values(self):\n \n             # The two sets of generated text and past kv should be equal to each other\n             self.assertTrue(has_similar_generate_outputs(outputs, outputs_cached))\n-            for layer_idx in range(len(outputs_cached.past_key_values)):\n-                for kv_idx in range(len(outputs_cached.past_key_values[layer_idx])):\n-                    self.assertTrue(\n-                        torch.allclose(\n-                            outputs.past_key_values[layer_idx][kv_idx],\n-                            outputs_cached.past_key_values[layer_idx][kv_idx],\n-                        )\n-                    )\n+            self._check_caches_are_equal(outputs.past_key_values, outputs_cached.past_key_values)\n \n     @pytest.mark.generate\n     def test_generate_continue_from_inputs_embeds(self):\n@@ -1486,14 +1460,7 @@ def test_generate_continue_from_inputs_embeds(self):\n             combined_output_sequences = torch.concat([initial_output.sequences, cached_output.sequences], axis=1)\n             self.assertListEqual(outputs.sequences.tolist(), combined_output_sequences.tolist())\n             # The two sets of past kv should be equal to each other\n-            for layer_idx in range(len(cached_output.past_key_values)):\n-                for kv_idx in range(len(cached_output.past_key_values[layer_idx])):\n-                    self.assertTrue(\n-                        torch.allclose(\n-                            outputs.past_key_values[layer_idx][kv_idx],\n-                            cached_output.past_key_values[layer_idx][kv_idx],\n-                        )\n-                    )\n+            self._check_caches_are_equal(outputs.past_key_values, cached_output.past_key_values)\n \n     @pytest.mark.generate\n     def test_generate_with_static_cache(self):\n@@ -2536,22 +2503,8 @@ def _check_past_key_values_for_generate(self, batch_size, decoder_past_key_value\n                 [layer.values.shape for layer in decoder_past_key_values.layers],\n                 [expected_shape] * len(decoder_past_key_values.layers),\n             )\n-\n-        # Legacy cache format checks. This branch should be removed when all models use `Cache` by default\n         else:\n-            self.assertListEqual(\n-                [isinstance(iter_past_key_values, tuple) for iter_past_key_values in decoder_past_key_values],\n-                [True] * len(decoder_past_key_values),\n-            )\n-            # check shape key, value\n-            self.assertListEqual(\n-                [layer_past_key_values[0].shape for layer_past_key_values in decoder_past_key_values],\n-                [expected_shape] * len(decoder_past_key_values),\n-            )\n-            self.assertListEqual(\n-                [layer_past_key_values[1].shape for layer_past_key_values in decoder_past_key_values],\n-                [expected_shape] * len(decoder_past_key_values),\n-            )\n+            raise ValueError(\"The cache is not standard! Please overwrite `_check_past_key_values_for_generate`\")\n \n     def _check_sequence_inside_sequence(self, tensor_1, tensor_2):\n         # check if tensor_1 inside tensor_2 or tensor_2 inside tensor_1.\n@@ -2576,6 +2529,24 @@ def _check_sequence_inside_sequence(self, tensor_1, tensor_2):\n \n         self.assertTrue(flag)\n \n+    def _check_caches_are_equal(self, cache1: Cache, cache2: Cache):\n+        if not isinstance(cache1, Cache) or not isinstance(cache2, Cache):\n+            raise ValueError(\"The cache is not standard! Please overwrite `_check_caches_are_equal`\")\n+\n+        # In this case, we simply call recursively the function on both internal caches\n+        if isinstance(cache1, EncoderDecoderCache):\n+            self._check_caches_are_equal(cache1.self_attention_cache, cache2.self_attention_cache)\n+            self._check_caches_are_equal(cache1.cross_attention_cache, cache2.cross_attention_cache)\n+            return\n+\n+        if not len(cache1) == len(cache2):\n+            raise ValueError(\"Both caches do not have the same number of layers.\")\n+\n+        num_layers = len(cache1)\n+        for idx in range(num_layers):\n+            torch.testing.assert_close(cache1.layers[idx].keys, cache2.layers[idx].keys)\n+            torch.testing.assert_close(cache1.layers[idx].values, cache2.layers[idx].values)\n+\n \n @require_torch\n class UtilsFunctionsTest(unittest.TestCase):"
        },
        {
            "sha": "bb9b4cbc5982cd2a59671ed5d9d6bea114130549",
            "filename": "tests/models/bamba/test_modeling_bamba.py",
            "status": "modified",
            "additions": 18,
            "deletions": 0,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -316,6 +316,24 @@ def _check_past_key_values_for_generate(self, batch_size, decoder_past_key_value\n             [expected_shape] * len(decoder_past_key_values.value_cache),\n         )\n \n+    def _check_caches_are_equal(\n+        self, cache1: HybridMambaAttentionDynamicCache, cache2: HybridMambaAttentionDynamicCache\n+    ):\n+        if not isinstance(cache1, HybridMambaAttentionDynamicCache) or not isinstance(\n+            cache2, HybridMambaAttentionDynamicCache\n+        ):\n+            raise ValueError(\"The wrong cache is being used!\")\n+\n+        if not len(cache1) == len(cache2):\n+            raise ValueError(\"Both caches do not have the same number of layers.\")\n+\n+        num_layers = len(cache1)\n+        for idx in range(num_layers):\n+            torch.testing.assert_close(cache1.key_cache[idx], cache2.key_cache[idx])\n+            torch.testing.assert_close(cache1.value_cache[idx], cache2.value_cache[idx])\n+            torch.testing.assert_close(cache1.conv_states[idx], cache2.conv_states[idx])\n+            torch.testing.assert_close(cache1.ssm_states[idx], cache2.ssm_states[idx])\n+\n     def setUp(self):\n         self.model_tester = self.model_tester_class(self)\n         self.config_tester = ConfigTester(self, config_class=self.model_tester.config_class, hidden_size=64)"
        },
        {
            "sha": "b2915d3cb2e5fdf7cf2ea194ca580bd7d4c03069",
            "filename": "tests/models/dia/test_modeling_dia.py",
            "status": "modified",
            "additions": 1,
            "deletions": 8,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/tests%2Fmodels%2Fdia%2Ftest_modeling_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/tests%2Fmodels%2Fdia%2Ftest_modeling_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdia%2Ftest_modeling_dia.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -506,14 +506,7 @@ def test_generate_continue_from_past_key_values(self):\n \n             # The two sets of generated text and past kv should be equal to each other\n             self.assertTrue(has_similar_generate_outputs(outputs, outputs_cached))\n-            for layer_idx in range(len(outputs_cached.past_key_values)):\n-                for kv_idx in range(len(outputs_cached.past_key_values[layer_idx])):\n-                    self.assertTrue(\n-                        torch.allclose(\n-                            outputs.past_key_values[layer_idx][kv_idx],\n-                            outputs_cached.past_key_values[layer_idx][kv_idx],\n-                        )\n-                    )\n+            self._check_caches_are_equal(outputs.past_key_values, outputs_cached.past_key_values)\n \n     @pytest.mark.generate\n     def test_prepare_inputs_for_generation_kwargs_forwards(self):"
        },
        {
            "sha": "fbfab2031f77f323604b273b451e63656e13dcc3",
            "filename": "tests/models/falcon_h1/test_modeling_falcon_h1.py",
            "status": "modified",
            "additions": 16,
            "deletions": 0,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/tests%2Fmodels%2Ffalcon_h1%2Ftest_modeling_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/tests%2Fmodels%2Ffalcon_h1%2Ftest_modeling_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffalcon_h1%2Ftest_modeling_falcon_h1.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -290,6 +290,22 @@ def _check_past_key_values_for_generate(self, batch_size, decoder_past_key_value\n             [expected_shape] * len(decoder_past_key_values.value_cache),\n         )\n \n+    def _check_caches_are_equal(self, cache1, cache2):\n+        if not isinstance(cache1, FalconHybridMambaAttentionDynamicCache) or not isinstance(\n+            cache2, FalconHybridMambaAttentionDynamicCache\n+        ):\n+            raise ValueError(\"The wrong cache is being used!\")\n+\n+        if not len(cache1) == len(cache2):\n+            raise ValueError(\"Both caches do not have the same number of layers.\")\n+\n+        num_layers = len(cache1)\n+        for idx in range(num_layers):\n+            torch.testing.assert_close(cache1.key_cache[idx], cache2.key_cache[idx])\n+            torch.testing.assert_close(cache1.value_cache[idx], cache2.value_cache[idx])\n+            torch.testing.assert_close(cache1.conv_states[idx], cache2.conv_states[idx])\n+            torch.testing.assert_close(cache1.ssm_states[idx], cache2.ssm_states[idx])\n+\n     def setUp(self):\n         self.model_tester = FalconH1ModelTester(self)\n         self.config_tester = ConfigTester(self, config_class=FalconH1Config, hidden_size=64)"
        },
        {
            "sha": "c6a6ab2f5c1ca7fa22f47e2b0ab6d4165c182c5a",
            "filename": "tests/models/idefics/test_modeling_idefics.py",
            "status": "modified",
            "additions": 2,
            "deletions": 16,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -690,14 +690,7 @@ def test_generate_continue_from_past_key_values(self):\n \n             # The two sets of generated text and past kv should be equal to each other\n             self.assertListEqual(outputs.sequences.tolist(), outputs_cached.sequences.tolist())\n-            for layer_idx in range(len(outputs_cached.past_key_values)):\n-                for kv_idx in range(len(outputs_cached.past_key_values[layer_idx])):\n-                    self.assertTrue(\n-                        torch.allclose(\n-                            outputs.past_key_values[layer_idx][kv_idx],\n-                            outputs_cached.past_key_values[layer_idx][kv_idx],\n-                        )\n-                    )\n+            self._check_caches_are_equal(outputs.past_key_values, outputs_cached.past_key_values)\n \n     @pytest.mark.generate\n     def test_generate_without_input_ids(self):\n@@ -774,14 +767,7 @@ def test_generate_continue_from_inputs_embeds(self):\n             # Verify that the combined outputs match the full generation.\n             combined_output_sequences = torch.concat([initial_output.sequences, cached_output.sequences], axis=1)\n             self.assertListEqual(outputs.sequences.tolist(), combined_output_sequences.tolist())\n-            for layer_idx in range(len(cached_output.past_key_values)):\n-                for kv_idx in range(len(cached_output.past_key_values[layer_idx])):\n-                    self.assertTrue(\n-                        torch.allclose(\n-                            outputs.past_key_values[layer_idx][kv_idx],\n-                            cached_output.past_key_values[layer_idx][kv_idx],\n-                        )\n-                    )\n+            self._check_caches_are_equal(outputs.past_key_values, cached_output.past_key_values)\n \n     def _check_attentions_for_generate(\n         self, batch_size, attentions, prompt_length, output_length, config, decoder_past_key_values"
        },
        {
            "sha": "4bc3630248d3f33b5729d65e61a857357a836ddd",
            "filename": "tests/models/jamba/test_modeling_jamba.py",
            "status": "modified",
            "additions": 18,
            "deletions": 0,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/tests%2Fmodels%2Fjamba%2Ftest_modeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/tests%2Fmodels%2Fjamba%2Ftest_modeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fjamba%2Ftest_modeling_jamba.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -362,6 +362,24 @@ def _check_past_key_values_for_generate(self, batch_size, decoder_past_key_value\n             [expected_shape] * len(decoder_past_key_values.value_cache),\n         )\n \n+    def _check_caches_are_equal(\n+        self, cache1: HybridMambaAttentionDynamicCache, cache2: HybridMambaAttentionDynamicCache\n+    ):\n+        if not isinstance(cache1, HybridMambaAttentionDynamicCache) or not isinstance(\n+            cache2, HybridMambaAttentionDynamicCache\n+        ):\n+            raise ValueError(\"The wrong cache is being used!\")\n+\n+        if not len(cache1) == len(cache2):\n+            raise ValueError(\"Both caches do not have the same number of layers.\")\n+\n+        num_layers = len(cache1)\n+        for idx in range(num_layers):\n+            torch.testing.assert_close(cache1.key_cache[idx], cache2.key_cache[idx])\n+            torch.testing.assert_close(cache1.value_cache[idx], cache2.value_cache[idx])\n+            torch.testing.assert_close(cache1.conv_states[idx], cache2.conv_states[idx])\n+            torch.testing.assert_close(cache1.ssm_states[idx], cache2.ssm_states[idx])\n+\n     def setUp(self):\n         self.model_tester = JambaModelTester(self)\n         self.config_tester = JambaConfigTester(self, config_class=JambaConfig, hidden_size=37)"
        },
        {
            "sha": "39ec93aded61adf1ec645828821fba0f6fc19d61",
            "filename": "tests/models/kyutai_speech_to_text/test_modeling_kyutai_speech_to_text.py",
            "status": "modified",
            "additions": 1,
            "deletions": 8,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/tests%2Fmodels%2Fkyutai_speech_to_text%2Ftest_modeling_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/tests%2Fmodels%2Fkyutai_speech_to_text%2Ftest_modeling_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fkyutai_speech_to_text%2Ftest_modeling_kyutai_speech_to_text.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -429,14 +429,7 @@ def test_generate_continue_from_past_key_values(self):\n \n             # The two sets of generated text and past kv should be equal to each other\n             self.assertTrue(has_similar_generate_outputs(outputs, outputs_cached))\n-            for layer_idx in range(len(outputs_cached.past_key_values)):\n-                for kv_idx in range(len(outputs_cached.past_key_values[layer_idx])):\n-                    self.assertTrue(\n-                        torch.allclose(\n-                            outputs.past_key_values[layer_idx][kv_idx],\n-                            outputs_cached.past_key_values[layer_idx][kv_idx],\n-                        )\n-                    )\n+            self._check_caches_are_equal(outputs.past_key_values, outputs_cached.past_key_values)\n \n     # needs to be overridden to avoid to avoid casting of input_values to float16\n     # indeed, the codec model is kept in fp32, so we need to avoid casting input_values to float16"
        },
        {
            "sha": "3e34e0196f25189bae42ce677e831a66b5a31f94",
            "filename": "tests/models/lfm2/test_modeling_lfm2.py",
            "status": "modified",
            "additions": 33,
            "deletions": 0,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/tests%2Fmodels%2Flfm2%2Ftest_modeling_lfm2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/tests%2Fmodels%2Flfm2%2Ftest_modeling_lfm2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flfm2%2Ftest_modeling_lfm2.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -33,6 +33,7 @@\n     import torch\n \n     from transformers import Lfm2ForCausalLM, Lfm2Model\n+    from transformers.models.lfm2.modeling_lfm2 import Lfm2HybridConvCache\n \n \n class Lfm2ModelTester(CausalLMModelTester):\n@@ -63,6 +64,38 @@ class Lfm2ModelTest(CausalLMModelTest, unittest.TestCase):\n     # used in `test_torch_compile_for_training`\n     _torch_compile_train_cls = Lfm2ForCausalLM if is_torch_available() else None\n \n+    def _check_past_key_values_for_generate(self, batch_size, decoder_past_key_values, cache_length, config):\n+        self.assertIsInstance(decoder_past_key_values, Lfm2HybridConvCache)\n+\n+        # (batch, head, seq_length, head_features)\n+        attention_shape = (\n+            batch_size,\n+            config.num_key_value_heads if hasattr(config, \"num_key_value_heads\") else config.num_attention_heads,\n+            cache_length,\n+            config.hidden_size // config.num_attention_heads,\n+        )\n+        conv_shape = (batch_size, config.hidden_size, config.conv_L_cache)\n+\n+        for i in range(config.num_hidden_layers):\n+            if config.layer_types[i] == \"full_attention\":\n+                self.assertEqual(decoder_past_key_values.key_cache[i].shape, attention_shape)\n+                self.assertEqual(decoder_past_key_values.value_cache[i].shape, attention_shape)\n+            else:\n+                self.assertEqual(decoder_past_key_values.conv_cache[i], conv_shape)\n+\n+    def _check_caches_are_equal(self, cache1: Lfm2HybridConvCache, cache2: Lfm2HybridConvCache):\n+        if not isinstance(cache1, Lfm2HybridConvCache) or not isinstance(cache2, Lfm2HybridConvCache):\n+            raise ValueError(\"The wrong cache is being used!\")\n+\n+        if not len(cache1) == len(cache2):\n+            raise ValueError(\"Both caches do not have the same number of layers.\")\n+\n+        num_layers = len(cache1)\n+        for idx in range(num_layers):\n+            torch.testing.assert_close(cache1.key_cache[idx], cache2.key_cache[idx])\n+            torch.testing.assert_close(cache1.value_cache[idx], cache2.value_cache[idx])\n+            torch.testing.assert_close(cache1.conv_cache[idx], cache2.conv_cache[idx])\n+\n     def test_attention_outputs(self):\n         \"\"\"Lfm2Moe alternates between attention and short-conv layers.\"\"\"\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()"
        },
        {
            "sha": "f2300aada6aea98c8b5003949ad45732bf7fbc80",
            "filename": "tests/models/lfm2_moe/test_modeling_lfm2_moe.py",
            "status": "modified",
            "additions": 33,
            "deletions": 0,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/tests%2Fmodels%2Flfm2_moe%2Ftest_modeling_lfm2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/tests%2Fmodels%2Flfm2_moe%2Ftest_modeling_lfm2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flfm2_moe%2Ftest_modeling_lfm2_moe.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -35,6 +35,7 @@\n     import torch\n \n     from transformers import Lfm2MoeConfig, Lfm2MoeForCausalLM, Lfm2MoeModel\n+    from transformers.models.lfm2_moe.modeling_lfm2_moe import Lfm2MoeHybridConvCache\n \n \n class Lfm2MoeModelTester(CausalLMModelTester):\n@@ -70,6 +71,38 @@ class Lfm2MoeModelTest(CausalLMModelTest, unittest.TestCase):\n     # used in `test_torch_compile_for_training`\n     _torch_compile_train_cls = Lfm2MoeForCausalLM if is_torch_available() else None\n \n+    def _check_past_key_values_for_generate(self, batch_size, decoder_past_key_values, cache_length, config):\n+        self.assertIsInstance(decoder_past_key_values, Lfm2MoeHybridConvCache)\n+\n+        # (batch, head, seq_length, head_features)\n+        attention_shape = (\n+            batch_size,\n+            config.num_key_value_heads if hasattr(config, \"num_key_value_heads\") else config.num_attention_heads,\n+            cache_length,\n+            config.hidden_size // config.num_attention_heads,\n+        )\n+        conv_shape = (batch_size, config.hidden_size, config.conv_L_cache)\n+\n+        for i in range(config.num_hidden_layers):\n+            if config.layer_types[i] == \"full_attention\":\n+                self.assertEqual(decoder_past_key_values.key_cache[i].shape, attention_shape)\n+                self.assertEqual(decoder_past_key_values.value_cache[i].shape, attention_shape)\n+            else:\n+                self.assertEqual(decoder_past_key_values.conv_cache[i], conv_shape)\n+\n+    def _check_caches_are_equal(self, cache1: Lfm2MoeHybridConvCache, cache2: Lfm2MoeHybridConvCache):\n+        if not isinstance(cache1, Lfm2MoeHybridConvCache) or not isinstance(cache2, Lfm2MoeHybridConvCache):\n+            raise ValueError(\"The wrong cache is being used!\")\n+\n+        if not len(cache1) == len(cache2):\n+            raise ValueError(\"Both caches do not have the same number of layers.\")\n+\n+        num_layers = len(cache1)\n+        for idx in range(num_layers):\n+            torch.testing.assert_close(cache1.key_cache[idx], cache2.key_cache[idx])\n+            torch.testing.assert_close(cache1.value_cache[idx], cache2.value_cache[idx])\n+            torch.testing.assert_close(cache1.conv_cache[idx], cache2.conv_cache[idx])\n+\n     def test_attention_outputs(self):\n         \"\"\"Lfm2Moe alternates between attention and short-conv layers.\"\"\"\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()"
        },
        {
            "sha": "b53533f68e2c1117855be24ee7b43af52101b27a",
            "filename": "tests/models/lfm2_vl/test_modeling_lfm2_vl.py",
            "status": "modified",
            "additions": 15,
            "deletions": 0,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/tests%2Fmodels%2Flfm2_vl%2Ftest_modeling_lfm2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/tests%2Fmodels%2Flfm2_vl%2Ftest_modeling_lfm2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flfm2_vl%2Ftest_modeling_lfm2_vl.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -45,6 +45,7 @@\n     import torch\n \n     from transformers import Lfm2VlConfig, Lfm2VlForConditionalGeneration, Lfm2VlModel\n+    from transformers.models.lfm2.modeling_lfm2 import Lfm2HybridConvCache\n \n \n class Lfm2VlModelTester(CausalLMModelTester):\n@@ -172,6 +173,20 @@ def setUp(self):\n             self, config_class=Lfm2VlConfig, has_text_modality=False, common_properties=common_properties\n         )\n \n+    def _check_caches_are_equal(self, cache1: Lfm2HybridConvCache, cache2: Lfm2HybridConvCache):\n+        \"\"\"Text model uses lfm2, which has non-standard cache\"\"\"\n+        if not isinstance(cache1, Lfm2HybridConvCache) or not isinstance(cache2, Lfm2HybridConvCache):\n+            raise ValueError(\"The wrong cache is being used!\")\n+\n+        if not len(cache1) == len(cache2):\n+            raise ValueError(\"Both caches do not have the same number of layers.\")\n+\n+        num_layers = len(cache1)\n+        for idx in range(num_layers):\n+            torch.testing.assert_close(cache1.key_cache[idx], cache2.key_cache[idx])\n+            torch.testing.assert_close(cache1.value_cache[idx], cache2.value_cache[idx])\n+            torch.testing.assert_close(cache1.conv_cache[idx], cache2.conv_cache[idx])\n+\n     def test_config(self):\n         self.config_tester.run_common_tests()\n "
        },
        {
            "sha": "f9da5e17f89694d518bcdc7ed6a1810cc75a35ad",
            "filename": "tests/models/longt5/test_modeling_longt5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/tests%2Fmodels%2Flongt5%2Ftest_modeling_longt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/tests%2Fmodels%2Flongt5%2Ftest_modeling_longt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flongt5%2Ftest_modeling_longt5.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -234,8 +234,6 @@ def create_and_check_model(\n         self.parent.assertEqual(decoder_output.size(), (self.batch_size, self.decoder_seq_length, self.hidden_size))\n         # There should be `num_layers` key value embeddings stored in decoder_past\n         self.parent.assertEqual(len(decoder_past), config.num_layers)\n-        # There should be a self attn key, a self attn value, a cross attn key and a cross attn value stored in each decoder_past tuple\n-        self.parent.assertEqual(len(decoder_past[0]), 4)\n \n     def create_and_check_with_lm_head(\n         self,"
        },
        {
            "sha": "284b9d65c182f3a349b0e352ac417221b58fabf2",
            "filename": "tests/models/minimax/test_modeling_minimax.py",
            "status": "modified",
            "additions": 22,
            "deletions": 5,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/tests%2Fmodels%2Fminimax%2Ftest_modeling_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/tests%2Fmodels%2Fminimax%2Ftest_modeling_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fminimax%2Ftest_modeling_minimax.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -18,7 +18,6 @@\n import pytest\n \n from transformers import is_torch_available\n-from transformers.cache_utils import Cache\n from transformers.testing_utils import (\n     Expectations,\n     require_torch,\n@@ -38,6 +37,7 @@\n         MiniMaxForTokenClassification,\n         MiniMaxModel,\n     )\n+    from transformers.models.minimax.modeling_minimax import MiniMaxCache\n from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n \n \n@@ -142,7 +142,7 @@ def _check_attentions_for_generate(\n                     self.assertEqual(layer_attention.shape, expected_shape)\n \n     def _check_past_key_values_for_generate(self, batch_size, decoder_past_key_values, cache_length, config):\n-        self.assertIsInstance(decoder_past_key_values, (tuple, Cache))\n+        self.assertIsInstance(decoder_past_key_values, MiniMaxCache)\n \n         # (batch, head, seq_length, head_features)\n         key_value_cache_expected_shape = (\n@@ -161,10 +161,27 @@ def _check_past_key_values_for_generate(self, batch_size, decoder_past_key_value\n \n         for layer_idx in range(config.num_hidden_layers):\n             if config.layer_types[layer_idx] == \"full_attention\":\n-                self.assertEqual(decoder_past_key_values[layer_idx][0].shape, key_value_cache_expected_shape)\n-                self.assertEqual(decoder_past_key_values[layer_idx][1].shape, key_value_cache_expected_shape)\n+                self.assertEqual(decoder_past_key_values.layers[layer_idx].keys.shape, key_value_cache_expected_shape)\n+                self.assertEqual(\n+                    decoder_past_key_values.layers[layer_idx].values.shape, key_value_cache_expected_shape\n+                )\n             else:\n-                self.assertEqual(decoder_past_key_values[layer_idx][0].shape, linear_cache_expected_shape)\n+                self.assertEqual(decoder_past_key_values.linear_cache[layer_idx].shape, linear_cache_expected_shape)\n+\n+    def _check_caches_are_equal(self, cache1: MiniMaxCache, cache2: MiniMaxCache):\n+        if not isinstance(cache1, MiniMaxCache) or not isinstance(cache2, MiniMaxCache):\n+            raise ValueError(\"The wrong cache is being used!\")\n+\n+        if not len(cache1) == len(cache2):\n+            raise ValueError(\"Both caches do not have the same number of layers.\")\n+\n+        num_layers = len(cache1)\n+        for idx in range(num_layers):\n+            # We need this as MiniMaxCache uses the max between attention and linear caches for len...\n+            if idx < len(cache1.layers):\n+                torch.testing.assert_close(cache1.layers[idx].keys, cache1.layers[idx].keys)\n+                torch.testing.assert_close(cache1.layers[idx].values, cache1.layers[idx].values)\n+            torch.testing.assert_close(cache1.linear_cache[idx], cache2.linear_cache[idx])\n \n     @pytest.mark.generate\n     def test_past_key_values_format(self, custom_all_cache_shapes=None):"
        },
        {
            "sha": "b44deead03072218227feb905f585e773df2a1d2",
            "filename": "tests/models/mllama/test_modeling_mllama.py",
            "status": "modified",
            "additions": 8,
            "deletions": 12,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -433,33 +433,29 @@ def test_past_key_values_format(self):\n             self.assertEqual(len(past_kv), num_hidden_layers)\n             batch_size, seq_length = inputs[\"input_ids\"].shape\n             for i in range(num_hidden_layers):\n-                self.assertEqual(len(past_kv[0]), 2)  # K V for the decoder = 2\n                 if i in self.model_tester.text_config[\"cross_attention_layers\"]:\n                     self.assertEqual(\n-                        past_kv[i][0].shape,\n+                        past_kv.layers[i].keys.shape,\n                         (batch_size, num_attention_heads, self.model_tester.image_length, per_head_embed_dim),\n                     )\n                     self.assertEqual(\n-                        past_kv[i][1].shape,\n+                        past_kv.layers[i].values.shape,\n                         (batch_size, num_attention_heads, self.model_tester.image_length, per_head_embed_dim),\n                     )\n                 else:\n                     self.assertEqual(\n-                        past_kv[i][0].shape, (batch_size, num_attention_heads, seq_length, per_head_embed_dim)\n+                        past_kv.layers[i].keys.shape, (batch_size, num_attention_heads, seq_length, per_head_embed_dim)\n                     )\n                     self.assertEqual(\n-                        past_kv[i][1].shape, (batch_size, num_attention_heads, seq_length, per_head_embed_dim)\n+                        past_kv.layers[i].values.shape,\n+                        (batch_size, num_attention_heads, seq_length, per_head_embed_dim),\n                     )\n \n     # overridden because mllama has special cache for self and cross attentions\n     def _check_past_key_values_for_generate(self, batch_size, decoder_past_key_values, cache_length, config):\n         self.assertIsInstance(decoder_past_key_values, Cache)\n-        self.assertListEqual(\n-            [isinstance(iter_past_key_values, tuple) for iter_past_key_values in decoder_past_key_values],\n-            [True] * len(decoder_past_key_values),\n-        )\n \n-        for layer_idx, layer_past_key_values in enumerate(decoder_past_key_values):\n+        for layer_idx in range(len(decoder_past_key_values)):\n             if layer_idx in self.model_tester.text_config[\"cross_attention_layers\"]:\n                 expected_shape = (\n                     batch_size,\n@@ -480,8 +476,8 @@ def _check_past_key_values_for_generate(self, batch_size, decoder_past_key_value\n                     config.hidden_size // config.num_attention_heads,\n                 )\n             # check shape key, value\n-            self.assertListEqual([layer_past_key_values[0].shape], [expected_shape])\n-            self.assertListEqual([layer_past_key_values[1].shape], [expected_shape])\n+            self.assertListEqual([decoder_past_key_values.layers[layer_idx].keys.shape], [expected_shape])\n+            self.assertListEqual([decoder_past_key_values.layers[layer_idx].values.shape], [expected_shape])\n \n     def test_generate_text_only_with_cache(self):\n         \"\"\""
        },
        {
            "sha": "f0f5b1696a3f041fa1d60f7be86ee71706e6a7bd",
            "filename": "tests/models/mt5/test_modeling_mt5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/tests%2Fmodels%2Fmt5%2Ftest_modeling_mt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/tests%2Fmodels%2Fmt5%2Ftest_modeling_mt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmt5%2Ftest_modeling_mt5.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -231,8 +231,6 @@ def create_and_check_model(\n         self.parent.assertEqual(decoder_output.size(), (self.batch_size, self.decoder_seq_length, self.hidden_size))\n         # There should be `num_layers` key value embeddings stored in decoder_past\n         self.parent.assertEqual(len(decoder_past), config.num_layers)\n-        # There should be a self attn key, a self attn value, a cross attn key and a cross attn value stored in each decoder_past tuple\n-        self.parent.assertEqual(len(decoder_past[0]), 4)\n \n     def create_and_check_with_lm_head(\n         self,"
        },
        {
            "sha": "c9716be47fdbb8427c6b72e02d7b85086fa41b68",
            "filename": "tests/models/pop2piano/test_modeling_pop2piano.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/tests%2Fmodels%2Fpop2piano%2Ftest_modeling_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/tests%2Fmodels%2Fpop2piano%2Ftest_modeling_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpop2piano%2Ftest_modeling_pop2piano.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -208,8 +208,6 @@ def create_and_check_model(\n         self.parent.assertEqual(encoder_output.size(), (self.batch_size, self.encoder_seq_length, self.hidden_size))\n         # There should be `num_layers` key value embeddings stored in decoder_past\n         self.parent.assertEqual(len(decoder_past), config.num_layers)\n-        # There should be a self attn key, a self attn value, a cross attn key and a cross attn value stored in each decoder_past tuple\n-        self.parent.assertEqual(len(decoder_past[0]), 4)\n \n     def create_and_check_with_lm_head(\n         self,"
        },
        {
            "sha": "aaeed0cf81be674f95b81465c738e14411d9f6f2",
            "filename": "tests/models/prophetnet/test_modeling_prophetnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/tests%2Fmodels%2Fprophetnet%2Ftest_modeling_prophetnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/tests%2Fmodels%2Fprophetnet%2Ftest_modeling_prophetnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fprophetnet%2Ftest_modeling_prophetnet.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -242,8 +242,7 @@ def create_and_check_model(\n         self.parent.assertEqual(decoder_output.size(), (self.batch_size, self.decoder_seq_length, self.hidden_size))\n         # There should be `num_layers` key value embeddings stored in decoder_past\n         self.parent.assertEqual(len(decoder_past), config.num_decoder_layers)\n-        # There should be a self attn key, a self attn value, a cross attn key and a cross attn value stored in each decoder_past tuple\n-        self.parent.assertEqual(len(decoder_past[0]), 4)  # cross-attention + uni-directional self-attention\n+        # cross-attention + uni-directional self-attention\n \n     def create_and_check_with_lm_head(\n         self,"
        },
        {
            "sha": "7052470e6b174d07a1c3752790991253ebc97c27",
            "filename": "tests/models/qwen3_next/test_modeling_qwen3_next.py",
            "status": "modified",
            "additions": 12,
            "deletions": 103,
            "changes": 115,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/tests%2Fmodels%2Fqwen3_next%2Ftest_modeling_qwen3_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/tests%2Fmodels%2Fqwen3_next%2Ftest_modeling_qwen3_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen3_next%2Ftest_modeling_qwen3_next.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -27,6 +27,7 @@\n     import torch\n \n     from transformers import (\n+        Cache,\n         Qwen3NextForCausalLM,\n         Qwen3NextForQuestionAnswering,\n         Qwen3NextForSequenceClassification,\n@@ -36,7 +37,6 @@\n     from transformers.models.qwen3_next.modeling_qwen3_next import Qwen3NextDynamicCache\n \n from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n-from ...generation.test_utils import has_similar_generate_outputs\n from ...test_modeling_common import (\n     TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION,\n     _test_eager_matches_sdpa_inference,\n@@ -95,6 +95,17 @@ def _check_past_key_values_for_generate(self, batch_size, decoder_past_key_value\n             [expected_shape] * len(attention_layer_indices),\n         )\n \n+    def _check_caches_are_equal(self, cache1: Cache, cache2: Cache):\n+        \"Qwen3-Next has a special Cache as it alternates with gated deltanet layers\"\n+        if not len(cache1) == len(cache2):\n+            raise ValueError(\"Both caches do not have the same number of layers.\")\n+\n+        num_layers = len(cache1)\n+        for idx in range(num_layers):\n+            if cache1.key_cache[idx] is not None:\n+                torch.testing.assert_close(cache1.key_cache[idx], cache2.key_cache[idx])\n+                torch.testing.assert_close(cache1.value_cache[idx], cache2.value_cache[idx])\n+\n     @pytest.mark.generate\n     def test_past_key_values_format(self):\n         \"Needs to be overwritten as Qwen3-Next alternates between attention layers and gated deltanet layers.\"\n@@ -127,108 +138,6 @@ def test_past_key_values_format(self):\n                     self.assertEqual(self_attention_layer_keys.shape, default_self_attention_shape)\n                     self.assertEqual(self_attention_layer_values.shape, default_self_attention_shape)\n \n-    @pytest.mark.generate\n-    def test_generate_continue_from_past_key_values(self):\n-        \"Needs to be overwritten as Qwen3-Next has non-standard cache.\"\n-        # Tests that we can continue generating from past key values, returned from a previous `generate` call\n-        for model_class in self.all_generative_model_classes:\n-            config, inputs = self.model_tester.prepare_config_and_inputs_for_common()\n-            model = model_class(config).to(torch_device)\n-            model.eval()\n-\n-            generate_kwargs = {\n-                \"pad_token_id\": -1,\n-                \"eos_token_id\": -1,\n-                \"forced_eos_token_id\": None,\n-                \"encoder_no_repeat_ngram_size\": 0,\n-                \"use_cache\": True,\n-                \"do_sample\": False,\n-                \"return_dict_in_generate\": True,\n-                \"output_scores\": True,\n-            }\n-\n-            # Traditional way of generating text, with `return_dict_in_generate` to return the past key values\n-            outputs = model.generate(**inputs, **generate_kwargs, max_new_tokens=4)\n-            # Let's generate again, but passing the past key values in between (3 + 1 = 4 tokens). Note that the\n-            # inputs may need to be tweaked across `generate` calls (like the attention mask).\n-            outputs_cached = model.generate(**inputs, **generate_kwargs, max_new_tokens=3)\n-\n-            # Continue from the tokens generated above, preparing the inputs accordingly\n-            inputs[\"past_key_values\"] = outputs_cached.past_key_values\n-            new_attention_len = outputs_cached.sequences.shape[-1]\n-\n-            inputs[\"input_ids\"] = outputs_cached.sequences\n-            if \"attention_mask\" in inputs:\n-                inputs[\"attention_mask\"] = torch.nn.functional.pad(\n-                    inputs[\"attention_mask\"],\n-                    (0, new_attention_len - inputs[\"attention_mask\"].shape[1]),\n-                    mode=\"constant\",\n-                    value=1,\n-                )\n-            first_caches_scores = outputs_cached.scores\n-            outputs_cached = model.generate(**inputs, **generate_kwargs, max_new_tokens=1)\n-            full_cached_scores = first_caches_scores + outputs_cached.scores\n-            outputs_cached.scores = full_cached_scores\n-\n-            # The two sets of generated text and past kv should be equal to each other\n-            self.assertTrue(has_similar_generate_outputs(outputs, outputs_cached))\n-            for layer_idx in range(len(outputs_cached.past_key_values)):\n-                for kv_idx in range(len(outputs_cached.past_key_values[layer_idx])):\n-                    # Diff with the main test: we need to skip layers where it stays None\n-                    if outputs.past_key_values[layer_idx][kv_idx] is not None:\n-                        self.assertTrue(\n-                            torch.allclose(\n-                                outputs.past_key_values[layer_idx][kv_idx],\n-                                outputs_cached.past_key_values[layer_idx][kv_idx],\n-                            )\n-                        )\n-\n-    @pytest.mark.generate\n-    def test_generate_continue_from_inputs_embeds(self):\n-        \"Needs to be overwritten as Qwen3-Next has non-standard cache.\"\n-        for model_class in self.all_generative_model_classes:\n-            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n-            model = model_class(config).to(torch_device).eval()\n-            input_ids = inputs_dict.pop(\"input_ids\")\n-            model.generation_config.pad_token_id = model.generation_config.eos_token_id = -1\n-            model.generation_config.forced_eos_token_id = None\n-            model.config.is_decoder = True\n-            model.generation_config.use_cache = True\n-\n-            generation_kwargs = {\n-                \"return_dict_in_generate\": True,\n-                \"do_sample\": False,\n-            }\n-\n-            # Traditional way of generating text, with `return_dict_in_generate` to return the past key values.\n-            input_embeds = model.get_input_embeddings()(input_ids)\n-            outputs = model.generate(inputs_embeds=input_embeds, max_new_tokens=4, **generation_kwargs)\n-\n-            # Let's generate again, but passing the past key values in between (3 + 1 = 4 tokens)\n-            initial_output = model.generate(inputs_embeds=input_embeds, max_new_tokens=3, **generation_kwargs)\n-            continued_embeds = torch.cat([input_embeds, model.get_input_embeddings()(initial_output.sequences)], dim=1)\n-            cached_output = model.generate(\n-                inputs_embeds=continued_embeds,\n-                max_new_tokens=1,\n-                past_key_values=initial_output.past_key_values,\n-                **generation_kwargs,\n-            )\n-\n-            # Combine the (3 + 1) generated tokens and verify it matches with full generation.\n-            combined_output_sequences = torch.concat([initial_output.sequences, cached_output.sequences], axis=1)\n-            self.assertListEqual(outputs.sequences.tolist(), combined_output_sequences.tolist())\n-            # The two sets of past kv should be equal to each other\n-            for layer_idx in range(len(cached_output.past_key_values)):\n-                for kv_idx in range(len(cached_output.past_key_values[layer_idx])):\n-                    # Diff with the main test: we need to skip layers where it stays None\n-                    if outputs.past_key_values[layer_idx][kv_idx] is not None:\n-                        self.assertTrue(\n-                            torch.allclose(\n-                                outputs.past_key_values[layer_idx][kv_idx],\n-                                cached_output.past_key_values[layer_idx][kv_idx],\n-                            )\n-                        )\n-\n     def test_attention_outputs(self):\n         \"Needs to be overwritten as Qwen3-Next alternates between attention layers and gated deltanet layers.\"\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()"
        },
        {
            "sha": "991f74f3b50633e38e8a999af1cee2ec67a0de5b",
            "filename": "tests/models/seamless_m4t/test_modeling_seamless_m4t.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/tests%2Fmodels%2Fseamless_m4t%2Ftest_modeling_seamless_m4t.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/tests%2Fmodels%2Fseamless_m4t%2Ftest_modeling_seamless_m4t.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fseamless_m4t%2Ftest_modeling_seamless_m4t.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -254,8 +254,6 @@ def create_and_check_model(self, config, input_ids, decoder_input_ids, input_mas\n         self.parent.assertEqual(decoder_output.size(), (self.batch_size, decoder_input_ids.shape[1], self.vocab_size))\n         # There should be `num_layers` key value embeddings stored in decoder_past\n         self.parent.assertEqual(len(decoder_past), config.decoder_layers)\n-        # There should be a self attn key, a self attn value, a cross attn key and a cross attn value stored in each decoder_past tuple\n-        self.parent.assertEqual(len(decoder_past[0]), 4)\n \n     def create_and_check_decoder_model_past_large_inputs(\n         self,"
        },
        {
            "sha": "45b9821a5bb4c1acf69b9d88da134f532a56be6e",
            "filename": "tests/models/seamless_m4t_v2/test_modeling_seamless_m4t_v2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/tests%2Fmodels%2Fseamless_m4t_v2%2Ftest_modeling_seamless_m4t_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/tests%2Fmodels%2Fseamless_m4t_v2%2Ftest_modeling_seamless_m4t_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fseamless_m4t_v2%2Ftest_modeling_seamless_m4t_v2.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -280,8 +280,6 @@ def create_and_check_model(self, config, input_ids, decoder_input_ids, input_mas\n         self.parent.assertEqual(decoder_output.size(), (self.batch_size, decoder_input_ids.shape[1], self.vocab_size))\n         # There should be `num_layers` key value embeddings stored in decoder_past\n         self.parent.assertEqual(len(decoder_past), config.decoder_layers)\n-        # There should be a self attn key, a self attn value, a cross attn key and a cross attn value stored in each decoder_past tuple\n-        self.parent.assertEqual(len(decoder_past[0]), 4)\n \n     def create_and_check_decoder_model_past_large_inputs(\n         self,"
        },
        {
            "sha": "3c32b65a742289c987934e3d6559d824bc9b60a2",
            "filename": "tests/models/switch_transformers/test_modeling_switch_transformers.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/tests%2Fmodels%2Fswitch_transformers%2Ftest_modeling_switch_transformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/tests%2Fmodels%2Fswitch_transformers%2Ftest_modeling_switch_transformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fswitch_transformers%2Ftest_modeling_switch_transformers.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -244,8 +244,6 @@ def create_and_check_model(\n         self.parent.assertEqual(decoder_output.size(), (self.batch_size, self.decoder_seq_length, self.hidden_size))\n         # There should be `num_layers` key value embeddings stored in decoder_past\n         self.parent.assertEqual(len(decoder_past), config.num_layers)\n-        # There should be a self attn key, a self attn value, a cross attn key and a cross attn value stored in each decoder_past tuple\n-        self.parent.assertEqual(len(decoder_past[0]), 4)\n \n     def create_and_check_with_lm_head(\n         self,"
        },
        {
            "sha": "bbc165d12ca244bb234a554c9038407574fd05f9",
            "filename": "tests/models/t5/test_modeling_t5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -240,8 +240,6 @@ def create_and_check_model(\n         self.parent.assertEqual(decoder_output.size(), (self.batch_size, self.decoder_seq_length, self.hidden_size))\n         # There should be `num_layers` key value embeddings stored in decoder_past\n         self.parent.assertEqual(len(decoder_past), config.num_layers)\n-        # There should be a self attn key, a self attn value, a cross attn key and a cross attn value stored in each decoder_past tuple\n-        self.parent.assertEqual(len(decoder_past[0]), 4)\n \n     def create_and_check_with_lm_head(\n         self,"
        },
        {
            "sha": "d5c16ad61ce5d70d96bbcd340ba83d1400f228e8",
            "filename": "tests/models/t5gemma/test_modeling_t5gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 8,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/tests%2Fmodels%2Ft5gemma%2Ftest_modeling_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/tests%2Fmodels%2Ft5gemma%2Ftest_modeling_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ft5gemma%2Ftest_modeling_t5gemma.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -1084,14 +1084,7 @@ def test_generate_continue_from_past_key_values(self):\n \n             # The two sets of generated text and past kv should be equal to each other\n             self.assertTrue(has_similar_generate_outputs(outputs, outputs_cached))\n-            for layer_idx in range(len(outputs_cached.past_key_values)):\n-                for kv_idx in range(len(outputs_cached.past_key_values[layer_idx])):\n-                    self.assertTrue(\n-                        torch.allclose(\n-                            outputs.past_key_values[layer_idx][kv_idx],\n-                            outputs_cached.past_key_values[layer_idx][kv_idx],\n-                        )\n-                    )\n+            self._check_caches_are_equal(outputs.past_key_values, outputs_cached.past_key_values)\n \n     # Based on tests.test_modeling_common.ModelTesterMixin.test_inputs_embeds_matches_input_ids\n     # Update encoder and decoder embeddings"
        },
        {
            "sha": "eb0fa6f56d3a557eb14f4c9f02071c6808bf2c1d",
            "filename": "tests/models/udop/test_modeling_udop.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/tests%2Fmodels%2Fudop%2Ftest_modeling_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/tests%2Fmodels%2Fudop%2Ftest_modeling_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fudop%2Ftest_modeling_udop.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -177,8 +177,6 @@ def create_and_check_model(\n         self.parent.assertEqual(decoder_output.size(), (self.batch_size, self.decoder_seq_length, self.hidden_size))\n         # There should be `num_layers` key value embeddings stored in decoder_past\n         self.parent.assertEqual(len(decoder_past), config.num_layers)\n-        # There should be a self attn key, a self attn value, a cross attn key and a cross attn value stored in each decoder_past tuple\n-        self.parent.assertEqual(len(decoder_past[0]), 4)\n \n     def create_and_check_with_lm_head(\n         self,"
        },
        {
            "sha": "2680c2020f3f43b089fa3ca239c4049c439c3d5e",
            "filename": "tests/models/umt5/test_modeling_umt5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/tests%2Fmodels%2Fumt5%2Ftest_modeling_umt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/tests%2Fmodels%2Fumt5%2Ftest_modeling_umt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fumt5%2Ftest_modeling_umt5.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -208,8 +208,6 @@ def create_and_check_model(\n         self.parent.assertEqual(decoder_output.size(), (self.batch_size, self.decoder_seq_length, self.hidden_size))\n         # There should be `num_layers` key value embeddings stored in decoder_past\n         self.parent.assertEqual(len(decoder_past), config.num_layers)\n-        # There should be a self attn key, a self attn value, a cross attn key and a cross attn value stored in each decoder_past tuple\n-        self.parent.assertEqual(len(decoder_past[0]), 4)\n \n     def create_and_check_model_fp16_forward(\n         self,"
        },
        {
            "sha": "0d0e2cd20a3785512bf1036ee7baa254dbb2260e",
            "filename": "tests/models/zamba/test_modeling_zamba.py",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/tests%2Fmodels%2Fzamba%2Ftest_modeling_zamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/tests%2Fmodels%2Fzamba%2Ftest_modeling_zamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fzamba%2Ftest_modeling_zamba.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -322,6 +322,20 @@ def _check_past_key_values_for_generate(self, batch_size, decoder_past_key_value\n             [expected_shape] * len(decoder_past_key_values.value_cache),\n         )\n \n+    def _check_caches_are_equal(self, cache1: ZambaHybridDynamicCache, cache2: ZambaHybridDynamicCache):\n+        if not isinstance(cache1, ZambaHybridDynamicCache) or not isinstance(cache2, ZambaHybridDynamicCache):\n+            raise ValueError(\"The wrong cache is being used!\")\n+\n+        if not len(cache1) == len(cache2):\n+            raise ValueError(\"Both caches do not have the same number of layers.\")\n+\n+        num_layers = len(cache1)\n+        for idx in range(num_layers):\n+            torch.testing.assert_close(cache1.key_cache[idx], cache2.key_cache[idx])\n+            torch.testing.assert_close(cache1.value_cache[idx], cache2.value_cache[idx])\n+            torch.testing.assert_close(cache1.conv_states[idx], cache2.conv_states[idx])\n+            torch.testing.assert_close(cache1.ssm_states[idx], cache2.ssm_states[idx])\n+\n     def setUp(self):\n         self.model_tester = ZambaModelTester(self)\n         self.config_tester = ConfigTester(self, config_class=ZambaConfig, hidden_size=37)"
        },
        {
            "sha": "f8d3b51582bc351a95e712f9d2fc66ac8bdc0887",
            "filename": "tests/models/zamba2/test_modeling_zamba2.py",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/tests%2Fmodels%2Fzamba2%2Ftest_modeling_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/tests%2Fmodels%2Fzamba2%2Ftest_modeling_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fzamba2%2Ftest_modeling_zamba2.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -334,6 +334,20 @@ def _check_past_key_values_for_generate(self, batch_size, decoder_past_key_value\n             [expected_shape] * len(decoder_past_key_values.value_cache),\n         )\n \n+    def _check_caches_are_equal(self, cache1: Zamba2HybridDynamicCache, cache2: Zamba2HybridDynamicCache):\n+        if not isinstance(cache1, Zamba2HybridDynamicCache) or not isinstance(cache2, Zamba2HybridDynamicCache):\n+            raise ValueError(\"The wrong cache is being used!\")\n+\n+        if not len(cache1) == len(cache2):\n+            raise ValueError(\"Both caches do not have the same number of layers.\")\n+\n+        num_layers = len(cache1)\n+        for idx in range(num_layers):\n+            torch.testing.assert_close(cache1.key_cache[idx], cache2.key_cache[idx])\n+            torch.testing.assert_close(cache1.value_cache[idx], cache2.value_cache[idx])\n+            torch.testing.assert_close(cache1.conv_states[idx], cache2.conv_states[idx])\n+            torch.testing.assert_close(cache1.ssm_states[idx], cache2.ssm_states[idx])\n+\n     def setUp(self):\n         self.model_tester = Zamba2ModelTester(self)\n         self.config_tester = ConfigTester(self, config_class=Zamba2Config, hidden_size=37)"
        },
        {
            "sha": "510883b5b964b10e5f1288be3457d8546f2afbee",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -1606,7 +1606,7 @@ def _create_and_check_torch_fx_tracing(self, config, inputs_dict, output_loss=Fa\n                             )\n                             for i in range(model.config.num_hidden_layers)\n                         )\n-                        non_empty_pkv = DynamicCache.from_legacy_cache(non_empty_pkv)\n+                        non_empty_pkv = DynamicCache(non_empty_pkv)\n \n                         inps = copy.deepcopy(inputs_to_test[0])\n "
        },
        {
            "sha": "941e758569c671f067ae3ca769440947db13cbba",
            "filename": "tests/utils/test_cache_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 48,
            "changes": 48,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/tests%2Futils%2Ftest_cache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/tests%2Futils%2Ftest_cache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_cache_utils.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -71,54 +71,6 @@\n class CacheTest(unittest.TestCase):\n     \"\"\"Cache tests that don't require loading models\"\"\"\n \n-    def test_dynamic_cache_retrocompatibility(self):\n-        \"\"\"Tests that we can convert back and forth between the legacy cache format and DynamicCache\"\"\"\n-        legacy_cache = ()\n-        new_cache = DynamicCache()\n-\n-        # Creates a new cache with 10 layers in both formats\n-        for layer_idx in range(10):\n-            new_key = torch.rand((2, 4, 8, 16))\n-            new_value = torch.rand((2, 4, 8, 16))\n-            new_cache.update(new_key, new_value, layer_idx)\n-            legacy_cache += ((new_key, new_value),)\n-\n-        # Sanity check 1: they must have the same shapes\n-        self.assertTrue(len(legacy_cache), len(new_cache))\n-        for layer_idx in range(10):\n-            self.assertTrue(len(legacy_cache[layer_idx]), len(legacy_cache[layer_idx]))\n-            for key_value_idx in range(2):\n-                self.assertTrue(\n-                    legacy_cache[layer_idx][key_value_idx].shape == new_cache[layer_idx][key_value_idx].shape\n-                )\n-\n-        # Sanity check 2: we can get the sequence length in multiple ways with DynamicCache, and they return the\n-        # expected value\n-        self.assertTrue(legacy_cache[0][0].shape[-2] == new_cache[0][0].shape[-2] == new_cache.get_seq_length() == 8)\n-\n-        # Sanity check 3: they must be equal, and both support indexing\n-        for layer_idx in range(10):\n-            for key_value_idx in range(2):\n-                self.assertTrue(\n-                    torch.allclose(new_cache[layer_idx][key_value_idx], legacy_cache[layer_idx][key_value_idx])\n-                )\n-\n-        # Test 1: We can convert from legacy to new with no changes\n-        from_legacy = DynamicCache.from_legacy_cache(legacy_cache)\n-        for layer_idx in range(10):\n-            for key_value_idx in range(2):\n-                self.assertTrue(\n-                    torch.allclose(from_legacy[layer_idx][key_value_idx], legacy_cache[layer_idx][key_value_idx])\n-                )\n-\n-        # Test 2: We can convert from new to legacy with no changes\n-        to_legacy = new_cache.to_legacy_cache()\n-        for layer_idx in range(10):\n-            for key_value_idx in range(2):\n-                self.assertTrue(\n-                    torch.allclose(to_legacy[layer_idx][key_value_idx], new_cache[layer_idx][key_value_idx])\n-                )\n-\n     def test_static_cache_mha_mqa_gqa(self):\n         \"\"\"\n         Tests that static cache works with multi-head attention (MHA), grouped query attention (GQA), and multi-query"
        },
        {
            "sha": "9358166ed1eccc314ead2cb11c3849991573dcc3",
            "filename": "tests/utils/test_modeling_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/tests%2Futils%2Ftest_modeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46db0edf3b0a39b979e4a0e9b65e7f1adec6824a/tests%2Futils%2Ftest_modeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_utils.py?ref=46db0edf3b0a39b979e4a0e9b65e7f1adec6824a",
            "patch": "@@ -1807,7 +1807,7 @@ def test_cache_when_needed_at_train_time(self):\n         # simulate injecting virtual tokens like in prefix tuning\n         num_virtual_tokens = 3\n         past_key_values = [torch.randn(2, 1, 2, num_virtual_tokens, 8)] * 2\n-        past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n+        past_key_values = DynamicCache(past_key_values)\n         model_inputs[\"attention_mask\"] = torch.cat(\n             (\n                 model_inputs[\"attention_mask\"],"
        }
    ],
    "stats": {
        "total": 2084,
        "additions": 633,
        "deletions": 1451
    }
}