{
    "author": "cyyever",
    "message": "Fix typos (#40585)\n\nSigned-off-by: cyy <cyyever@outlook.com>",
    "sha": "a543095c9967ddd3dd0915ffa19df412a4788a44",
    "files": [
        {
            "sha": "83cc0fd854c9b4143bcbc6d9f5bca31979443e69",
            "filename": "examples/metrics-monitoring/README.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a543095c9967ddd3dd0915ffa19df412a4788a44/examples%2Fmetrics-monitoring%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a543095c9967ddd3dd0915ffa19df412a4788a44/examples%2Fmetrics-monitoring%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmetrics-monitoring%2FREADME.md?ref=a543095c9967ddd3dd0915ffa19df412a4788a44",
            "patch": "@@ -13,7 +13,7 @@ cd examples/metrics-monitoring\n docker compose up\n ```\n \n-Then, in your srcipt running CB, you will need to create a MeterProvider and TracerProvider as follows:\n+Then, in your script running CB, you will need to create a MeterProvider and TracerProvider as follows:\n \n ```py\n from opentelemetry import metrics, trace"
        },
        {
            "sha": "ffff54df93ba6aa642544a11bd1e2a11d7a53e01",
            "filename": "src/transformers/commands/add_new_model_like.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fcommands%2Fadd_new_model_like.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fcommands%2Fadd_new_model_like.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcommands%2Fadd_new_model_like.py?ref=a543095c9967ddd3dd0915ffa19df412a4788a44",
            "patch": "@@ -93,7 +93,7 @@ def visit_SimpleStatementLine(self, node: cst.SimpleStatementLine):\n \n class ModelInfos:\n     \"\"\"\n-    Retrieve the basic informations about an existing model classes.\n+    Retrieve the basic information about an existing model classes.\n     \"\"\"\n \n     def __init__(self, lowercase_name: str):\n@@ -158,7 +158,7 @@ def add_model_to_auto_mappings(\n \n     Args:\n         old_model_infos (`ModelInfos`):\n-            The structure containing the class informations of the old model.\n+            The structure containing the class information of the old model.\n         new_lowercase_name (`str`):\n             The new lowercase model name.\n         new_model_paper_name (`str`):\n@@ -357,7 +357,7 @@ def find_modular_structure(\n         module_name (`str`):\n             The full path to the python module to copy with modular.\n         old_model_infos (`ModelInfos`):\n-            The structure containing the class informations of the old model.\n+            The structure containing the class information of the old model.\n         new_cased_name (`str`):\n             The new cased model name.\n     \"\"\"\n@@ -383,7 +383,7 @@ def create_modular_file(\n \n     Args:\n         old_model_infos (`ModelInfos`):\n-            The structure containing the class informations of the old model.\n+            The structure containing the class information of the old model.\n         new_lowercase_name (`str`):\n             The new lowercase model name.\n         filenames_to_add (`list[tuple[str, bool]]`):\n@@ -430,7 +430,7 @@ def create_test_files(old_model_infos: ModelInfos, new_lowercase_name, filenames\n \n     Args:\n         old_model_infos (`ModelInfos`):\n-            The structure containing the class informations of the old model.\n+            The structure containing the class information of the old model.\n         new_lowercase_name (`str`):\n             The new lowercase model name.\n         filenames_to_add (`list[tuple[str, bool]]`):\n@@ -486,7 +486,7 @@ def create_new_model_like(\n \n     Args:\n         old_model_infos (`ModelInfos`):\n-            The structure containing the class informations of the old model.\n+            The structure containing the class information of the old model.\n         new_lowercase_name (`str`):\n             The new lowercase model name.\n         new_model_paper_name (`str`):"
        },
        {
            "sha": "10ee10e0195011e87e4353a98733161af54e7e8b",
            "filename": "src/transformers/data/data_collator.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fdata%2Fdata_collator.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fdata%2Fdata_collator.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdata%2Fdata_collator.py?ref=a543095c9967ddd3dd0915ffa19df412a4788a44",
            "patch": "@@ -2052,7 +2052,7 @@ def __call__(self, features, return_tensors=None, separator_id=None):\n             dtype_64 = np.int64\n             dtype_32 = np.int32\n         else:\n-            raise ValueError(f'return_tensors must be one of (\"pt\", \"np\"), {return_tensors=} not suported')\n+            raise ValueError(f'return_tensors must be one of (\"pt\", \"np\"), {return_tensors=} not supported')\n \n         for k, v in batch.items():\n             if k in self._batch_dim_keys:"
        },
        {
            "sha": "ba2820cb437a67ca814f813306a0d05c4674416b",
            "filename": "src/transformers/generation/beam_search.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fgeneration%2Fbeam_search.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fgeneration%2Fbeam_search.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fbeam_search.py?ref=a543095c9967ddd3dd0915ffa19df412a4788a44",
            "patch": "@@ -677,7 +677,7 @@ def step_sentence_constraint(\n         device = sent_beam_indices.device\n \n         # initialize states\n-        topk_contraint_states = self.make_constraint_states(orig_len)\n+        topk_constraint_states = self.make_constraint_states(orig_len)\n         advance_constraint_states = self.make_constraint_states(orig_len)\n \n         sidx, eidx = batch_idx * orig_len, (batch_idx + 1) * orig_len\n@@ -701,7 +701,7 @@ def step_sentence_constraint(\n             # either way, we need to sort them into \"banks\" later, so store a \"ConstraintListState\" for all types of\n             # hypotheses.\n \n-            topk_state = topk_contraint_states[seq_idx]\n+            topk_state = topk_constraint_states[seq_idx]\n             topk_state.reset(full_hypotheses[seq_idx].tolist())\n \n             advance_state = advance_constraint_states[seq_idx]\n@@ -763,7 +763,7 @@ def step_sentence_constraint(\n             new_tokens = torch.stack(track_new[\"new_tokens\"]).to(device)\n             new_scores = torch.stack(track_new[\"new_scores\"]).to(device)\n \n-            all_states = topk_contraint_states + track_new[\"new_states\"]\n+            all_states = topk_constraint_states + track_new[\"new_states\"]\n             all_tokens = torch.cat((sent_beam_tokens, new_tokens), -1)\n             all_scores = torch.cat((sent_beam_scores, new_scores), -1)\n             all_banks = torch.tensor([one.get_bank() for one in all_states], device=device)"
        },
        {
            "sha": "bd7f02e64cee19cb9e40e2e9d925bf230e9cb9ac",
            "filename": "src/transformers/generation/configuration_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py?ref=a543095c9967ddd3dd0915ffa19df412a4788a44",
            "patch": "@@ -220,10 +220,10 @@ class GenerationConfig(PushToHubMixin):\n             generated. The tuple shall consist of: `(start_index, decay_factor)` where `start_index` indicates where\n             penalty starts and `decay_factor` represents the factor of exponential decay\n         suppress_tokens (`list[int]`, *optional*):\n-            A list of tokens that will be suppressed at generation. The `SupressTokens` logit processor will set their\n+            A list of tokens that will be suppressed at generation. The `SuppressTokens` logit processor will set their\n             log probs to `-inf` so that they are not sampled.\n         begin_suppress_tokens  (`list[int]`, *optional*):\n-            A list of tokens that will be suppressed at the beginning of the generation. The `SupressBeginTokens` logit\n+            A list of tokens that will be suppressed at the beginning of the generation. The `SuppressBeginTokens` logit\n             processor will set their log probs to `-inf` so that they are not sampled.\n         sequence_bias (`dict[tuple[int], float]`, *optional*)):\n             Dictionary that maps a sequence of tokens to its bias term. Positive biases increase the odds of the"
        },
        {
            "sha": "7151c25ed21c78a7f2763271753523a0c511b055",
            "filename": "src/transformers/generation/continuous_batching/cache.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache.py?ref=a543095c9967ddd3dd0915ffa19df412a4788a44",
            "patch": "@@ -83,7 +83,7 @@ def __init__(\n             cache_dtype=self.dtype,\n         )\n \n-        # Add the infered attributes to the class\n+        # Add the inferred attributes to the class\n         self.num_blocks = num_blocks\n         self.max_batch_tokens = max_batch_tokens\n         logger.warning(f\"PagedAttentionCache initialized with {self.num_blocks = } and {self.max_batch_tokens = } \")"
        },
        {
            "sha": "248b6b1b0b9dd5f2eb89b3746e55161af65d906e",
            "filename": "src/transformers/integrations/hub_kernels.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py?ref=a543095c9967ddd3dd0915ffa19df412a4788a44",
            "patch": "@@ -155,7 +155,7 @@ def load_and_register_kernel(attn_implementation: str) -> None:\n     try:\n         kernel = get_kernel(repo_id, revision=rev)\n     except Exception as e:\n-        raise ValueError(f\"An error occured while trying to load from '{repo_id}': {e}.\")\n+        raise ValueError(f\"An error occurred while trying to load from '{repo_id}': {e}.\")\n     # correctly wrap the kernel\n     if hasattr(kernel, \"flash_attn_varlen_func\"):\n         if attention_wrapper is None:"
        },
        {
            "sha": "f74142ce23ad59d964d8564740e13b8c601e554c",
            "filename": "src/transformers/integrations/tensor_parallel.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py?ref=a543095c9967ddd3dd0915ffa19df412a4788a44",
            "patch": "@@ -40,7 +40,7 @@\n \n def initialize_tensor_parallelism(tp_plan, tp_size=None):\n     r\"\"\"\n-    Sets up the device mesh and initilized the backend for tensor parallelism.\n+    Sets up the device mesh and initialized the backend for tensor parallelism.\n     This function is called when the model is loaded and the TP plan is set to 'auto'.\n     \"\"\"\n     if tp_plan is None:\n@@ -128,7 +128,7 @@ def _get_parameter_tp_plan(parameter_name: str, tp_plan: dict[str, str], is_weig\n     The parameter name can be a generic name with wildcards (e.g. \"*.weight\") or a specific name (e.g. \"layer_1.weight\").\n \n     The `is_weight` is important because for weights, we want to support `.weights` and `.bias` cases seamlessly! but\n-    not parrent classes for `post_init` calls\n+    not parent classes for `post_init` calls\n     \"\"\"\n     generic_param_name = re.sub(r\"\\d+\", \"*\", parameter_name)\n     if generic_param_name in tp_plan:"
        },
        {
            "sha": "6b5162f86bc2d18ebc40c237245738f8431caafe",
            "filename": "src/transformers/models/clip/modeling_clip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py?ref=a543095c9967ddd3dd0915ffa19df412a4788a44",
            "patch": "@@ -646,7 +646,7 @@ def forward(\n                 input_ids.to(dtype=torch.int, device=last_hidden_state.device).argmax(dim=-1),\n             ]\n         else:\n-            # The config gets updated `eos_token_id` from PR #24773 (so the use of exta new tokens is possible)\n+            # The config gets updated `eos_token_id` from PR #24773 (so the use of extra new tokens is possible)\n             pooled_output = last_hidden_state[\n                 torch.arange(last_hidden_state.shape[0], device=last_hidden_state.device),\n                 # We need to get the first position of `eos_token_id` value (`pad_token_ids` might equal to `eos_token_id`)"
        },
        {
            "sha": "ab2e38827998ad4c67059b42387dbe9c192c6a18",
            "filename": "src/transformers/models/clip/modeling_tf_clip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_tf_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_tf_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_tf_clip.py?ref=a543095c9967ddd3dd0915ffa19df412a4788a44",
            "patch": "@@ -600,7 +600,7 @@ def call(\n                 ),\n             )\n         else:\n-            # The config gets updated `eos_token_id` from PR #24773 (so the use of exta new tokens is possible)\n+            # The config gets updated `eos_token_id` from PR #24773 (so the use of extra new tokens is possible)\n             pooled_output = tf.gather_nd(\n                 params=sequence_output,\n                 indices=tf.stack("
        },
        {
            "sha": "9c2f06e6562f8c8a741e3e4ab5ff5aeb4a6dfc65",
            "filename": "src/transformers/models/csm/generation_csm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fcsm%2Fgeneration_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fcsm%2Fgeneration_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fgeneration_csm.py?ref=a543095c9967ddd3dd0915ffa19df412a4788a44",
            "patch": "@@ -167,7 +167,7 @@ def _sample(\n         3. Use these generated codebook tokens as input_ids to sample the next first codebook token using the backbone model\n         4. Repeat until stopping criteria is met\n \n-        Csm supports two stopping criterias:\n+        Csm supports two stopping criteria:\n         - stop when the generated sequence is at max_length\n         - stop when all the generated codebook tokens are the codebook_eos_token_id\n         \"\"\""
        },
        {
            "sha": "b1a4b5942f65ca79290ac13b2bb27b6aa78b08af",
            "filename": "src/transformers/models/csm/modeling_csm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py?ref=a543095c9967ddd3dd0915ffa19df412a4788a44",
            "patch": "@@ -817,7 +817,7 @@ def _merge_input_ids_with_input_values(\n     ) -> Optional[torch.Tensor]:\n         \"\"\"\n         Merges the input_ids and input_values to produce a single inputs_embeds tensor:\n-        1 - Infers the codec model on the input_values to retreive codebook token.\n+        1 - Infers the codec model on the input_values to retrieve codebook token.\n         2 - Embeds codebook tokens and places them at the correct positions in the inputs_embeds tensor.\n         3 - If labels are provided, expands them to match codebook dimensions and position the target codebook tokens in the inputs_embeds tensor.\n "
        },
        {
            "sha": "94983a05b08ab840666b935a99c05e74dde94d37",
            "filename": "src/transformers/models/csm/modular_csm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py?ref=a543095c9967ddd3dd0915ffa19df412a4788a44",
            "patch": "@@ -96,7 +96,7 @@ class CsmOutputWithPast(ModelOutput):\n     backbone_loss: Optional[torch.FloatTensor] = None\n \n \n-# manually specify names for correct naming when converting from modualr\n+# manually specify names for correct naming when converting from modular\n class CsmRMSNorm(LlamaRMSNorm):\n     pass\n \n@@ -495,7 +495,7 @@ def _merge_input_ids_with_input_values(\n     ) -> Optional[torch.Tensor]:\n         \"\"\"\n         Merges the input_ids and input_values to produce a single inputs_embeds tensor:\n-        1 - Infers the codec model on the input_values to retreive codebook token.\n+        1 - Infers the codec model on the input_values to retrieve codebook token.\n         2 - Embeds codebook tokens and places them at the correct positions in the inputs_embeds tensor.\n         3 - If labels are provided, expands them to match codebook dimensions and position the target codebook tokens in the inputs_embeds tensor.\n "
        },
        {
            "sha": "97bfa836e9310c34efb1752ce47134fc6c0e9dbb",
            "filename": "src/transformers/models/dac/convert_dac_checkpoint.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fdac%2Fconvert_dac_checkpoint.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fdac%2Fconvert_dac_checkpoint.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdac%2Fconvert_dac_checkpoint.py?ref=a543095c9967ddd3dd0915ffa19df412a4788a44",
            "patch": "@@ -34,7 +34,7 @@\n # python3 -m dac download --model_type 44khz # downloads the 44kHz variant\n # python3 -m dac download --model_type 24khz # downloads the 24kHz variant\n # python3 -m dac download --model_type 16khz # downloads the 16kHz variant\n-# More informations: https://github.com/descriptinc/descript-audio-codec/tree/main\n+# More information: https://github.com/descriptinc/descript-audio-codec/tree/main\n \n logging.set_verbosity_info()\n logger = logging.get_logger(\"transformers.models.dac\")"
        },
        {
            "sha": "fa291e7689570d602b04e6efa3e9f7c921dcab10",
            "filename": "src/transformers/models/deepseek_v2/modeling_deepseek_v2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py?ref=a543095c9967ddd3dd0915ffa19df412a4788a44",
            "patch": "@@ -117,8 +117,8 @@ def moe(self, hidden_states: torch.Tensor, topk_ids: torch.Tensor, topk_weight:\n         cnts = topk_ids.new_zeros((topk_ids.shape[0], len(self.experts)))\n         cnts.scatter_(1, topk_ids, 1)\n         tokens_per_expert = cnts.sum(dim=0)\n-        indicies = topk_ids.view(-1).argsort()\n-        sorted_tokens = hidden_states[indicies // topk_ids.shape[1]]\n+        indices = topk_ids.view(-1).argsort()\n+        sorted_tokens = hidden_states[indices // topk_ids.shape[1]]\n \n         # Process experts\n         outputs = []\n@@ -137,7 +137,7 @@ def moe(self, hidden_states: torch.Tensor, topk_ids: torch.Tensor, topk_weight:\n \n         # Reorder and combine outputs\n         new_x = torch.empty_like(outs)\n-        new_x[indicies] = outs\n+        new_x[indices] = outs\n         hidden_states = (\n             new_x.view(*topk_ids.shape, -1)\n             .type(topk_weight.dtype)"
        },
        {
            "sha": "80fa2c8860a3442a2904ec1061514a020290132f",
            "filename": "src/transformers/models/deepseek_v2/modular_deepseek_v2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodular_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodular_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodular_deepseek_v2.py?ref=a543095c9967ddd3dd0915ffa19df412a4788a44",
            "patch": "@@ -311,8 +311,8 @@ def moe(self, hidden_states: torch.Tensor, topk_ids: torch.Tensor, topk_weight:\n         cnts = topk_ids.new_zeros((topk_ids.shape[0], len(self.experts)))\n         cnts.scatter_(1, topk_ids, 1)\n         tokens_per_expert = cnts.sum(dim=0)\n-        indicies = topk_ids.view(-1).argsort()\n-        sorted_tokens = hidden_states[indicies // topk_ids.shape[1]]\n+        indices = topk_ids.view(-1).argsort()\n+        sorted_tokens = hidden_states[indices // topk_ids.shape[1]]\n \n         # Process experts\n         outputs = []\n@@ -331,7 +331,7 @@ def moe(self, hidden_states: torch.Tensor, topk_ids: torch.Tensor, topk_weight:\n \n         # Reorder and combine outputs\n         new_x = torch.empty_like(outs)\n-        new_x[indicies] = outs\n+        new_x[indices] = outs\n         hidden_states = (\n             new_x.view(*topk_ids.shape, -1)\n             .type(topk_weight.dtype)"
        },
        {
            "sha": "a221bf6d64970e642522a1448b5259ee99112e31",
            "filename": "src/transformers/models/deprecated/van/modeling_van.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvan%2Fmodeling_van.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvan%2Fmodeling_van.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvan%2Fmodeling_van.py?ref=a543095c9967ddd3dd0915ffa19df412a4788a44",
            "patch": "@@ -315,7 +315,7 @@ def __init__(self, config: VanConfig):\n             x.item() for x in torch.linspace(0, config.drop_path_rate, sum(config.depths), device=\"cpu\")\n         ]\n \n-        for num_stage, (patch_size, stride, hidden_size, depth, mlp_expantion, drop_path_rate) in enumerate(\n+        for num_stage, (patch_size, stride, hidden_size, depth, mlp_expansion, drop_path_rate) in enumerate(\n             zip(patch_sizes, strides, hidden_sizes, depths, mlp_ratios, drop_path_rates)\n         ):\n             is_first_stage = num_stage == 0\n@@ -330,7 +330,7 @@ def __init__(self, config: VanConfig):\n                     patch_size=patch_size,\n                     stride=stride,\n                     depth=depth,\n-                    mlp_ratio=mlp_expantion,\n+                    mlp_ratio=mlp_expansion,\n                     drop_path_rate=drop_path_rate,\n                 )\n             )"
        },
        {
            "sha": "b4376b773b27365932774a35746b2928cf0af707",
            "filename": "src/transformers/models/dia/feature_extraction_dia.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fdia%2Ffeature_extraction_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fdia%2Ffeature_extraction_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Ffeature_extraction_dia.py?ref=a543095c9967ddd3dd0915ffa19df412a4788a44",
            "patch": "@@ -150,7 +150,7 @@ def __call__(\n         input_values = BatchFeature({\"input_values\": raw_audio})\n \n         # temporarily treat it as if we were mono as we also convert stereo to mono\n-        origingal_feature_size = self.feature_size\n+        original_feature_size = self.feature_size\n         self.feature_size = 1\n \n         # normal padding on batch\n@@ -175,7 +175,7 @@ def __call__(\n             padded_inputs = padded_inputs.convert_to_tensors(return_tensors)\n \n         # rewrite back to original feature size\n-        self.feature_size = origingal_feature_size\n+        self.feature_size = original_feature_size\n \n         return padded_inputs\n "
        },
        {
            "sha": "bfb9d1074f1424bfd8ae1fc143fe2267f9dc4017",
            "filename": "src/transformers/models/dinov3_vit/image_processing_dinov3_vit_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fimage_processing_dinov3_vit_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fimage_processing_dinov3_vit_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fimage_processing_dinov3_vit_fast.py?ref=a543095c9967ddd3dd0915ffa19df412a4788a44",
            "patch": "@@ -53,7 +53,7 @@ class DINOv3ViTImageProcessorFast(BaseImageProcessorFast):\n     do_rescale = True\n     do_normalize = True\n \n-    # Overriden for DINOv3 to preserve order of transforms\n+    # Overridden for DINOv3 to preserve order of transforms\n     # rescale -> resize -> normalize\n     def _preprocess(\n         self,"
        },
        {
            "sha": "e5ff63adfe9f657d8b07ff79317cd0deab6a34d7",
            "filename": "src/transformers/models/eomt/modeling_eomt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Feomt%2Fmodeling_eomt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Feomt%2Fmodeling_eomt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Feomt%2Fmodeling_eomt.py?ref=a543095c9967ddd3dd0915ffa19df412a4788a44",
            "patch": "@@ -77,7 +77,7 @@ class EomtForUniversalSegmentationOutput(ModelOutput):\n         Tuple of `tuple(torch.FloatTensor)` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n         sequence_length)`. Self and Cross Attentions weights from transformer decoder.\n     patch_offsets (`list[torch.Tensor]`, *optional*):\n-        list of tuples indicating the image index and start and end positions of patches for semantic segementation.\n+        list of tuples indicating the image index and start and end positions of patches for semantic segmentation.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n@@ -1105,7 +1105,7 @@ def forward(\n             list of target class labels of shape `(num_labels, height, width)` to be fed to a model. They identify the\n             labels of `mask_labels`, e.g. the label of `mask_labels[i][j]` if `class_labels[i][j]`.\n         patch_offsets (`list[torch.Tensor]`, *optional*):\n-            list of tuples indicating the image index and start and end positions of patches for semantic segementation.\n+            list of tuples indicating the image index and start and end positions of patches for semantic segmentation.\n         \"\"\"\n \n         masks_queries_logits_per_layer, class_queries_logits_per_layer = (), ()"
        },
        {
            "sha": "8ad0b76fe8ee29cce50501cb899ea15c522f682f",
            "filename": "src/transformers/models/eomt/modular_eomt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Feomt%2Fmodular_eomt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Feomt%2Fmodular_eomt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Feomt%2Fmodular_eomt.py?ref=a543095c9967ddd3dd0915ffa19df412a4788a44",
            "patch": "@@ -229,7 +229,7 @@ class EomtForUniversalSegmentationOutput(ModelOutput):\n         Tuple of `tuple(torch.FloatTensor)` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n         sequence_length)`. Self and Cross Attentions weights from transformer decoder.\n     patch_offsets (`list[torch.Tensor]`, *optional*):\n-        list of tuples indicating the image index and start and end positions of patches for semantic segementation.\n+        list of tuples indicating the image index and start and end positions of patches for semantic segmentation.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n@@ -510,7 +510,7 @@ def forward(\n             list of target class labels of shape `(num_labels, height, width)` to be fed to a model. They identify the\n             labels of `mask_labels`, e.g. the label of `mask_labels[i][j]` if `class_labels[i][j]`.\n         patch_offsets (`list[torch.Tensor]`, *optional*):\n-            list of tuples indicating the image index and start and end positions of patches for semantic segementation.\n+            list of tuples indicating the image index and start and end positions of patches for semantic segmentation.\n         \"\"\"\n \n         masks_queries_logits_per_layer, class_queries_logits_per_layer = (), ()"
        },
        {
            "sha": "7641c667dfe5a280324b8b699c5f51150e8affbb",
            "filename": "src/transformers/models/evolla/modeling_evolla.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py?ref=a543095c9967ddd3dd0915ffa19df412a4788a44",
            "patch": "@@ -975,7 +975,7 @@ def cross_attention(\n         attention_mask = query_attn_mask[:, None, :, None] * kv_attn_mask[:, None, None, :]\n         # Compute the scaled dot-product attention scores\n         attn_weights = torch.matmul(query_layer, key_layer.transpose(-1, -2))  # [bs, numheads, querylength, keylength]\n-        attn_weights = attn_weights - attn_weights.amax(dim=-1, keepdim=True).detach()  # To stablize score\n+        attn_weights = attn_weights - attn_weights.amax(dim=-1, keepdim=True).detach()  # To stabilize score\n         attention_scores = attn_weights.masked_fill(\n             (1 - attention_mask).bool(), torch.finfo(attn_weights.dtype).min\n         )  # [bs, numheads, querylength, keylength]"
        },
        {
            "sha": "b9cd717485e42a775e2cc838452c94d69e0a505e",
            "filename": "src/transformers/models/evolla/modular_evolla.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodular_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodular_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodular_evolla.py?ref=a543095c9967ddd3dd0915ffa19df412a4788a44",
            "patch": "@@ -607,7 +607,7 @@ def cross_attention(\n         attention_mask = query_attn_mask[:, None, :, None] * kv_attn_mask[:, None, None, :]\n         # Compute the scaled dot-product attention scores\n         attn_weights = torch.matmul(query_layer, key_layer.transpose(-1, -2))  # [bs, numheads, querylength, keylength]\n-        attn_weights = attn_weights - attn_weights.amax(dim=-1, keepdim=True).detach()  # To stablize score\n+        attn_weights = attn_weights - attn_weights.amax(dim=-1, keepdim=True).detach()  # To stabilize score\n         attention_scores = attn_weights.masked_fill(\n             (1 - attention_mask).bool(), torch.finfo(attn_weights.dtype).min\n         )  # [bs, numheads, querylength, keylength]"
        },
        {
            "sha": "93f608a5bdc6c92d5d1594faec37fe341da3ae58",
            "filename": "src/transformers/models/falcon_h1/modeling_falcon_h1.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py?ref=a543095c9967ddd3dd0915ffa19df412a4788a44",
            "patch": "@@ -1559,7 +1559,7 @@ def prepare_inputs_for_generation(\n         use_cache=True,\n         **kwargs,\n     ):\n-        # Overwitten -- has a unique cache type, `FalconHybridMambaAttentionDynamicCache`\n+        # Overwritten -- has a unique cache type, `FalconHybridMambaAttentionDynamicCache`\n \n         empty_past_kv = past_key_values is None\n "
        },
        {
            "sha": "8f62448517c90085a404b3a34977587600741b15",
            "filename": "src/transformers/models/falcon_h1/modular_falcon_h1.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodular_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodular_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodular_falcon_h1.py?ref=a543095c9967ddd3dd0915ffa19df412a4788a44",
            "patch": "@@ -1324,7 +1324,7 @@ def prepare_inputs_for_generation(\n         use_cache=True,\n         **kwargs,\n     ):\n-        # Overwitten -- has a unique cache type, `FalconHybridMambaAttentionDynamicCache`\n+        # Overwritten -- has a unique cache type, `FalconHybridMambaAttentionDynamicCache`\n \n         empty_past_kv = past_key_values is None\n "
        },
        {
            "sha": "71d0dc66ced520c9ef2da2facb9480d9cf3abd0a",
            "filename": "src/transformers/models/fsmt/modeling_fsmt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Ffsmt%2Fmodeling_fsmt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Ffsmt%2Fmodeling_fsmt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffsmt%2Fmodeling_fsmt.py?ref=a543095c9967ddd3dd0915ffa19df412a4788a44",
            "patch": "@@ -793,7 +793,7 @@ def forward(\n             else:\n                 curr_past_key_value = layer_state\n \n-        # NOTE: FSMT has format (seq_len, BS, model_dim) ofr inputs\n+        # NOTE: FSMT has format (seq_len, BS, model_dim) for inputs\n         current_states = key if self.encoder_decoder_attention else query\n         if self.encoder_decoder_attention and layer_state is not None and is_updated:\n             # reuse k,v, cross_attentions"
        },
        {
            "sha": "07b6d6388d3bee33388c36ebe6f6901603f99889",
            "filename": "src/transformers/models/fuyu/processing_fuyu.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Ffuyu%2Fprocessing_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Ffuyu%2Fprocessing_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fprocessing_fuyu.py?ref=a543095c9967ddd3dd0915ffa19df412a4788a44",
            "patch": "@@ -216,7 +216,7 @@ def _transform_within_tags(text: str, scale_factor: float, tokenizer) -> list[in\n \n     # Remove all spaces from num_ints\n     num_ints = [float(num.strip()) for num in num_int_strs]\n-    # scale to transformed image siz\n+    # scale to transformed image size\n     if len(num_ints) == 2:\n         num_ints_translated = scale_point_to_transformed_image(x=num_ints[0], y=num_ints[1], scale_factor=scale_factor)\n     elif len(num_ints) == 4:"
        },
        {
            "sha": "7a0ea8d38501a36446b3040d4ad5604950bc8cbb",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=a543095c9967ddd3dd0915ffa19df412a4788a44",
            "patch": "@@ -751,7 +751,7 @@ def inner_mask(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool:\n \n @auto_docstring(\n     custom_intro=\"\"\"\n-    The Base Gemma3 model which consists of a vision backbone and a language model withou language modeling head.,\n+    The Base Gemma3 model which consists of a vision backbone and a language model without language modeling head.,\n     \"\"\"\n )\n class Gemma3Model(Gemma3PreTrainedModel):"
        },
        {
            "sha": "efb9b2a648dd36c914092e9be35d1d5b50497e75",
            "filename": "src/transformers/models/gemma3n/configuration_gemma3n.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fconfiguration_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fconfiguration_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fconfiguration_gemma3n.py?ref=a543095c9967ddd3dd0915ffa19df412a4788a44",
            "patch": "@@ -56,7 +56,7 @@ class Gemma3nTextConfig(PretrainedConfig):\n             Dimension of the hidden representations for per-layer emebeddings.\n         intermediate_size (`int` or `Sequence[int]`, *optional*, defaults to 16384):\n             Dimension of the MLP representations. MatFormer configurations may wish to provide a sequence of integers\n-            to account for vairable intermediate_size values across layers. In such cases,\n+            to account for variable intermediate_size values across layers. In such cases,\n             `len(intermediate_size) == num_hidden_layers`.\n         num_hidden_layers (`int`, *optional*, defaults to 35):\n             Number of hidden layers in the Transformer decoder.\n@@ -93,7 +93,7 @@ class Gemma3nTextConfig(PretrainedConfig):\n         rope_theta (`float`, *optional*, defaults to 1000000.0):\n             The base period of the RoPE embeddings.\n         rope_scaling (`Dict`, *optional*):\n-            Dictionary containing the scaling configuration for the RoPE embeddings used in gloabl attention.\n+            Dictionary containing the scaling configuration for the RoPE embeddings used in global attention.\n             NOTE: if you apply new rope type and you expect the model to work on longer `max_position_embeddings`, we\n             recommend you to update this value accordingly.\n             Expected contents:\n@@ -147,7 +147,7 @@ class Gemma3nTextConfig(PretrainedConfig):\n         altup_active_idx (`int`, *optional*, defaults to 0):\n             The index of the prediction from which AltUp will compute additional predictions or correct\n         altup_coef_clip (`float`, *optional*, defaults to 120.0):\n-            The maximum amplitude of an AltUp prediction or correction coeficient weight.\n+            The maximum amplitude of an AltUp prediction or correction coefficient weight.\n         altup_correct_scale (`bool`, *optional*, defaults to `True`):\n             If True, apply the `AltUp.correct_output_scale` to the corrected prediction at `altup_active_idx`.\n         altup_num_inputs (`int`, *optional*, defaults to 4):\n@@ -328,7 +328,7 @@ class Gemma3nAudioConfig(PretrainedConfig):\n         rms_norm_eps (`float`, *optional*, defaults to 1e-06):\n             The epsilon used by the rms normalization layers.\n         gradient_clipping (`float`, *optional*, defaults to 10000000000.0):\n-            Clipping value used to stablize extremely large gradient values.\n+            Clipping value used to stabilize extremely large gradient values.\n         conf_attention_chunk_size (`int`, *optional*, defaults to 12):\n             The sub-sequence size for local attention processing inside the Conformer (\"conf\") section of the\n             Universal Speech Model."
        },
        {
            "sha": "62e3fb3878f73fc7c794f544ddd94232f9ae1b4d",
            "filename": "src/transformers/models/gemma3n/feature_extraction_gemma3n.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fgemma3n%2Ffeature_extraction_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fgemma3n%2Ffeature_extraction_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Ffeature_extraction_gemma3n.py?ref=a543095c9967ddd3dd0915ffa19df412a4788a44",
            "patch": "@@ -281,7 +281,7 @@ def __call__(\n \n         This implementation uses a different algorithm for windowing and preemphasis compared to the built-in\n         `transformers.audio_utils.spectrogram()` function that _will_ result in different outputs. Consider this\n-        carefully when selecting an audio feature extactor, especially with pre-trained models.\n+        carefully when selecting an audio feature extractor, especially with pre-trained models.\n \n         Args:\n             raw_speech:"
        },
        {
            "sha": "637af1181c7e25e5fe78361a05259a48e4944daa",
            "filename": "src/transformers/models/gemma3n/modular_gemma3n.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py?ref=a543095c9967ddd3dd0915ffa19df412a4788a44",
            "patch": "@@ -84,7 +84,7 @@ class Gemma3nTextConfig(Gemma2Config, PretrainedConfig):\n             Dimension of the hidden representations for per-layer emebeddings.\n         intermediate_size (`int` or `Sequence[int]`, *optional*, defaults to 16384):\n             Dimension of the MLP representations. MatFormer configurations may wish to provide a sequence of integers\n-            to account for vairable intermediate_size values across layers. In such cases,\n+            to account for variable intermediate_size values across layers. In such cases,\n             `len(intermediate_size) == num_hidden_layers`.\n         num_hidden_layers (`int`, *optional*, defaults to 35):\n             Number of hidden layers in the Transformer decoder.\n@@ -121,7 +121,7 @@ class Gemma3nTextConfig(Gemma2Config, PretrainedConfig):\n         rope_theta (`float`, *optional*, defaults to 1000000.0):\n             The base period of the RoPE embeddings.\n         rope_scaling (`Dict`, *optional*):\n-            Dictionary containing the scaling configuration for the RoPE embeddings used in gloabl attention.\n+            Dictionary containing the scaling configuration for the RoPE embeddings used in global attention.\n             NOTE: if you apply new rope type and you expect the model to work on longer `max_position_embeddings`, we\n             recommend you to update this value accordingly.\n             Expected contents:\n@@ -175,7 +175,7 @@ class Gemma3nTextConfig(Gemma2Config, PretrainedConfig):\n         altup_active_idx (`int`, *optional*, defaults to 0):\n             The index of the prediction from which AltUp will compute additional predictions or correct\n         altup_coef_clip (`float`, *optional*, defaults to 120.0):\n-            The maximum amplitude of an AltUp prediction or correction coeficient weight.\n+            The maximum amplitude of an AltUp prediction or correction coefficient weight.\n         altup_correct_scale (`bool`, *optional*, defaults to `True`):\n             If True, apply the `AltUp.correct_output_scale` to the corrected prediction at `altup_active_idx`.\n         altup_num_inputs (`int`, *optional*, defaults to 4):\n@@ -341,7 +341,7 @@ class Gemma3nAudioConfig(PretrainedConfig):\n         rms_norm_eps (`float`, *optional*, defaults to 1e-06):\n             The epsilon used by the rms normalization layers.\n         gradient_clipping (`float`, *optional*, defaults to 10000000000.0):\n-            Clipping value used to stablize extremely large gradient values.\n+            Clipping value used to stabilize extremely large gradient values.\n         conf_attention_chunk_size (`int`, *optional*, defaults to 12):\n             The sub-sequence size for local attention processing inside the Conformer (\"conf\") section of the\n             Universal Speech Model."
        },
        {
            "sha": "3f317a063b3acf701768b0dbe7b165b5f41e847e",
            "filename": "src/transformers/models/granitemoehybrid/configuration_granitemoehybrid.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fconfiguration_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fconfiguration_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fconfiguration_granitemoehybrid.py?ref=a543095c9967ddd3dd0915ffa19df412a4788a44",
            "patch": "@@ -94,7 +94,7 @@ class GraniteMoeHybridConfig(PretrainedConfig):\n         output_router_logits (`bool`, *optional*, defaults to `False`):\n             Whether or not the router logits should be returned by the model. Enabling this will also\n             allow the model to output the auxiliary loss.\n-        router_aux_loss_coef (`float`, *optional*, defaults to 0.001): router auxialiary loss coefficient\n+        router_aux_loss_coef (`float`, *optional*, defaults to 0.001): router auxiliary loss coefficient\n         shared_intermediate_size (`int`, *optional*, defaults to 1024): intermediate size for shared experts.\n         position_embedding_type (`str`, *optional*): Positional embedding\n             type to be used; defaults to None. Allowed options: `[None, \"rope\"]`"
        },
        {
            "sha": "1c999dca5f48faa2d49f0719d1c9ce1397efb72a",
            "filename": "src/transformers/models/groupvit/modeling_tf_groupvit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fmodeling_tf_groupvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fmodeling_tf_groupvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fmodeling_tf_groupvit.py?ref=a543095c9967ddd3dd0915ffa19df412a4788a44",
            "patch": "@@ -1220,7 +1220,7 @@ def call(\n                 ),\n             )\n         else:\n-            # The config gets updated `eos_token_id` from PR #24773 (so the use of exta new tokens is possible)\n+            # The config gets updated `eos_token_id` from PR #24773 (so the use of extra new tokens is possible)\n             pooled_output = tf.gather_nd(\n                 params=sequence_output,\n                 indices=tf.stack("
        },
        {
            "sha": "45c05ff3073762a73ba1b8084af9899616ebe446",
            "filename": "src/transformers/models/hubert/modeling_tf_hubert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_tf_hubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_tf_hubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_tf_hubert.py?ref=a543095c9967ddd3dd0915ffa19df412a4788a44",
            "patch": "@@ -63,7 +63,7 @@ def _sample_without_replacement(distribution, num_samples):\n # Copied from transformers.models.wav2vec2.modeling_tf_wav2vec2._scatter_values_on_batch_indices\n def _scatter_values_on_batch_indices(values, batch_indices, output_shape):\n     \"\"\"\n-    Scatter function as in PyTorch with indices in format (batch_dim, indixes)\n+    Scatter function as in PyTorch with indices in format (batch_dim, indices)\n     \"\"\"\n     indices_shape = shape_list(batch_indices)\n     # broadcast batch dim to indices_shape"
        },
        {
            "sha": "1d8cf94022189e8c59fc84812817641a3d255a62",
            "filename": "src/transformers/models/idefics/vision_tf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fidefics%2Fvision_tf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fidefics%2Fvision_tf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fvision_tf.py?ref=a543095c9967ddd3dd0915ffa19df412a4788a44",
            "patch": "@@ -249,7 +249,7 @@ def call(\n         attn_weights = tf.nn.softmax(attn_weights, axis=-1)\n \n         if output_attentions:\n-            # this operation is a bit akward, but it's required to\n+            # this operation is a bit awkward, but it's required to\n             # make sure that attn_weights keeps its gradient.\n             # In order to do so, attn_weights have to reshaped\n             # twice and have to be reused in the following"
        },
        {
            "sha": "0ec9ef5e43337a60b385593cf8f543521592552f",
            "filename": "src/transformers/models/imagegpt/image_processing_imagegpt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fimage_processing_imagegpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fimage_processing_imagegpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fimage_processing_imagegpt.py?ref=a543095c9967ddd3dd0915ffa19df412a4788a44",
            "patch": "@@ -247,7 +247,7 @@ def preprocess(\n             )\n \n         # Here, normalize() is using a constant factor to divide pixel values.\n-        # hence, the method does not need iamge_mean and image_std.\n+        # hence, the method does not need image_mean and image_std.\n         validate_preprocess_arguments(\n             do_resize=do_resize,\n             size=size,"
        },
        {
            "sha": "2a4d09f00e8d080dd3d8f28f26c568398b7c70f4",
            "filename": "src/transformers/models/kosmos2/processing_kosmos2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fprocessing_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fprocessing_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fprocessing_kosmos2.py?ref=a543095c9967ddd3dd0915ffa19df412a4788a44",
            "patch": "@@ -578,7 +578,7 @@ def patch_index_to_coordinate(ul_idx: int, lr_idx: int, num_patches_per_side: in\n def extract_entities_with_patch_indices(text):\n     \"\"\"Extract entities contained in `text`. The bounding bboxes is given in the form of patch indices.\n \n-    This functioin is only intended to be used within `clean_text_and_extract_entities_with_bboxes` where further\n+    This function is only intended to be used within `clean_text_and_extract_entities_with_bboxes` where further\n     processing happens, including converting to normalized coordinates and whitespace character cleaning up.\n \n     Examples:"
        },
        {
            "sha": "d603575ef32d46ab6d0dcef53fc16d79143e3f5b",
            "filename": "src/transformers/models/kosmos2_5/image_processing_kosmos2_5.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fimage_processing_kosmos2_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fimage_processing_kosmos2_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fimage_processing_kosmos2_5.py?ref=a543095c9967ddd3dd0915ffa19df412a4788a44",
            "patch": "@@ -48,7 +48,7 @@\n # Copied from transformers.models.pix2struct.image_processing_pix2struct.torch_extract_patches\n def torch_extract_patches(image_tensor, patch_height, patch_width):\n     \"\"\"\n-    Utiliy function to extract patches from a given image tensor. Returns a tensor of shape\n+    Utility function to extract patches from a given image tensor. Returns a tensor of shape\n     (1, `rows`, `columns`, `num_channels`x `patch_height` x `patch_width`).\n \n     Args:\n@@ -72,7 +72,7 @@ def torch_extract_patches(image_tensor, patch_height, patch_width):\n     return patches.unsqueeze(0)\n \n \n-# similar to transformers.models.pix2struct.image_processing_pix2struct.Pix2StructImageProcessor, but delete is_vqa and additionaly return width and height after resizing\n+# similar to transformers.models.pix2struct.image_processing_pix2struct.Pix2StructImageProcessor, but delete is_vqa and additionally return width and height after resizing\n class Kosmos2_5ImageProcessor(BaseImageProcessor):\n     r\"\"\"\n     Constructs a Kosmos2_5 image processor."
        },
        {
            "sha": "8a629d495b027066ccd7ffb0f69813466a72c8de",
            "filename": "src/transformers/models/levit/image_processing_levit.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Flevit%2Fimage_processing_levit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Flevit%2Fimage_processing_levit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flevit%2Fimage_processing_levit.py?ref=a543095c9967ddd3dd0915ffa19df412a4788a44",
            "patch": "@@ -58,8 +58,8 @@ class LevitImageProcessor(BaseImageProcessor):\n             Size of the output image after resizing. If size is a dict with keys \"width\" and \"height\", the image will\n             be resized to `(size[\"height\"], size[\"width\"])`. If size is a dict with key \"shortest_edge\", the shortest\n             edge value `c` is rescaled to `int(c * (256/224))`. The smaller edge of the image will be matched to this\n-            value i.e, if height > width, then image will be rescaled to `(size[\"shortest_egde\"] * height / width,\n-            size[\"shortest_egde\"])`. Can be overridden by the `size` parameter in the `preprocess` method.\n+            value i.e, if height > width, then image will be rescaled to `(size[\"shortest_edge\"] * height / width,\n+            size[\"shortest_edge\"])`. Can be overridden by the `size` parameter in the `preprocess` method.\n         resample (`PILImageResampling`, *optional*, defaults to `Resampling.BICUBIC`):\n             Resampling filter to use if resizing the image. Can be overridden by the `resample` parameter in the\n             `preprocess` method.\n@@ -136,7 +136,7 @@ def resize(\n \n         If size is a dict with key \"shortest_edge\", the shortest edge value `c` is rescaled to `int(c * (256/224))`.\n         The smaller edge of the image will be matched to this value i.e, if height > width, then image will be rescaled\n-        to `(size[\"shortest_egde\"] * height / width, size[\"shortest_egde\"])`.\n+        to `(size[\"shortest_edge\"] * height / width, size[\"shortest_edge\"])`.\n \n         Args:\n             image (`np.ndarray`):"
        },
        {
            "sha": "b9b6018a82fd278da73cff26e2666ce3aea7bcb6",
            "filename": "src/transformers/models/levit/image_processing_levit_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Flevit%2Fimage_processing_levit_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Flevit%2Fimage_processing_levit_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flevit%2Fimage_processing_levit_fast.py?ref=a543095c9967ddd3dd0915ffa19df412a4788a44",
            "patch": "@@ -62,7 +62,7 @@ def resize(\n \n         If size is a dict with key \"shortest_edge\", the shortest edge value `c` is rescaled to `int(c * (256/224))`.\n         The smaller edge of the image will be matched to this value i.e, if height > width, then image will be rescaled\n-        to `(size[\"shortest_egde\"] * height / width, size[\"shortest_egde\"])`.\n+        to `(size[\"shortest_edge\"] * height / width, size[\"shortest_edge\"])`.\n \n         Args:\n             image (`torch.Tensor`):"
        },
        {
            "sha": "fcb1555dd3169f5955bb2201b292a898edff1e9b",
            "filename": "src/transformers/models/llama4/image_processing_llama4_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fllama4%2Fimage_processing_llama4_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fllama4%2Fimage_processing_llama4_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fimage_processing_llama4_fast.py?ref=a543095c9967ddd3dd0915ffa19df412a4788a44",
            "patch": "@@ -198,7 +198,7 @@ def pad_to_best_fit(\n         background_color (`int` or `tuple[int, int, int]`, *optional*, defaults to 0):\n             The color to use for the padding. Can be an integer for single channel or a\n             tuple of integers representing for multi-channel images. If passed as integer\n-            in mutli-channel mode, it will default to `0` in subsequent channels.\n+            in multi-channel mode, it will default to `0` in subsequent channels.\n     Returns:\n         `torch.Tensor`: The padded images.\n     \"\"\""
        },
        {
            "sha": "d3aa81303bb846772c865298b6a220ea45d586b2",
            "filename": "src/transformers/models/llava/image_processing_llava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fllava%2Fimage_processing_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fllava%2Fimage_processing_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fimage_processing_llava.py?ref=a543095c9967ddd3dd0915ffa19df412a4788a44",
            "patch": "@@ -164,7 +164,7 @@ def pad_to_square(\n             background_color (`int` or `tuple[int, int, int]`, *optional*, defaults to 0):\n                 The color to use for the padding. Can be an integer for single channel or a\n                 tuple of integers representing for multi-channel images. If passed as integer\n-                in mutli-channel mode, it will default to `0` in subsequent channels.\n+                in multi-channel mode, it will default to `0` in subsequent channels.\n             data_format (`str` or `ChannelDimension`, *optional*):\n                 The channel dimension format for the output image. Can be one of:\n                     - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format."
        },
        {
            "sha": "02324f6393cd6c492635daf27cb4585b33171fb7",
            "filename": "src/transformers/models/llava/image_processing_llava_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fllava%2Fimage_processing_llava_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fllava%2Fimage_processing_llava_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fimage_processing_llava_fast.py?ref=a543095c9967ddd3dd0915ffa19df412a4788a44",
            "patch": "@@ -103,7 +103,7 @@ def pad_to_square(\n             background_color (`int` or `tuple[int, int, int]`, *optional*, defaults to 0):\n                 The color to use for the padding. Can be an integer for single channel or a\n                 tuple of integers representing for multi-channel images. If passed as integer\n-                in mutli-channel mode, it will default to `0` in subsequent channels.\n+                in multi-channel mode, it will default to `0` in subsequent channels.\n         Returns:\n             `torch.Tensor`: The padded images.\n         \"\"\""
        },
        {
            "sha": "eed243cf01160330cc1e01516b818a258eead694",
            "filename": "src/transformers/models/llava_next_video/modeling_llava_next_video.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py?ref=a543095c9967ddd3dd0915ffa19df412a4788a44",
            "patch": "@@ -627,7 +627,7 @@ def get_video_features(\n         Args:\n             pixel_values (`torch.FloatTensor]` of shape `(batch_size, num_frames, channels, height, width)`)\n                The tensors corresponding to the input video.\n-            vision_feature_layer (`Union[int, list[int]]`, *optiona;*):\n+            vision_feature_layer (`Union[int, list[int]]`, *optional;*):\n                 The index of the layer to select the vision feature. If multiple indices are provided,\n                 the vision feature of the corresponding indices will be concatenated to form the\n                 vision features."
        },
        {
            "sha": "d89168a7260db438244797362d7e7b4296102f0a",
            "filename": "src/transformers/models/llava_next_video/modular_llava_next_video.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py?ref=a543095c9967ddd3dd0915ffa19df412a4788a44",
            "patch": "@@ -354,7 +354,7 @@ def get_video_features(\n         Args:\n             pixel_values (`torch.FloatTensor]` of shape `(batch_size, num_frames, channels, height, width)`)\n                The tensors corresponding to the input video.\n-            vision_feature_layer (`Union[int, list[int]]`, *optiona;*):\n+            vision_feature_layer (`Union[int, list[int]]`, *optional;*):\n                 The index of the layer to select the vision feature. If multiple indices are provided,\n                 the vision feature of the corresponding indices will be concatenated to form the\n                 vision features."
        },
        {
            "sha": "d8ed03085f22fc6ac234df8f5d533c0338f2f8e4",
            "filename": "src/transformers/models/llava_onevision/image_processing_llava_onevision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision.py?ref=a543095c9967ddd3dd0915ffa19df412a4788a44",
            "patch": "@@ -460,7 +460,7 @@ def pad_to_square(\n             background_color (`int` or `tuple[int, int, int]`, *optional*, defaults to 0):\n                 The color to use for the padding. Can be an integer for single channel or a\n                 tuple of integers representing for multi-channel images. If passed as integer\n-                in mutli-channel mode, it will default to `0` in subsequent channels.\n+                in multi-channel mode, it will default to `0` in subsequent channels.\n             data_format (`str` or `ChannelDimension`, *optional*):\n                 The channel dimension format for the output image. Can be one of:\n                     - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format."
        },
        {
            "sha": "46ef482dad3614aed0d5d92725ba5355fb5803b4",
            "filename": "src/transformers/models/llava_onevision/image_processing_llava_onevision_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision_fast.py?ref=a543095c9967ddd3dd0915ffa19df412a4788a44",
            "patch": "@@ -323,7 +323,7 @@ def pad_to_square(\n             background_color (`int` or `tuple[int, int, int]`, *optional*, defaults to 0):\n                 The color to use for the padding. Can be an integer for single channel or a\n                 tuple of integers representing for multi-channel images. If passed as integer\n-                in mutli-channel mode, it will default to `0` in subsequent channels.\n+                in multi-channel mode, it will default to `0` in subsequent channels.\n         Returns:\n             `torch.Tensor`: The padded images.\n         \"\"\""
        },
        {
            "sha": "2f98f1f3d98730ccf0ea3ec877ee61dfc07eea12",
            "filename": "src/transformers/models/llava_onevision/modular_llava_onevision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodular_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodular_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodular_llava_onevision.py?ref=a543095c9967ddd3dd0915ffa19df412a4788a44",
            "patch": "@@ -112,7 +112,7 @@ def pad_to_square(\n             background_color (`int` or `tuple[int, int, int]`, *optional*, defaults to 0):\n                 The color to use for the padding. Can be an integer for single channel or a\n                 tuple of integers representing for multi-channel images. If passed as integer\n-                in mutli-channel mode, it will default to `0` in subsequent channels.\n+                in multi-channel mode, it will default to `0` in subsequent channels.\n         Returns:\n             `torch.Tensor`: The padded images.\n         \"\"\""
        },
        {
            "sha": "3a3e076a5a4c5e34da60cebae899ff77abb65bf6",
            "filename": "src/transformers/models/maskformer/modeling_maskformer_swin.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer_swin.py?ref=a543095c9967ddd3dd0915ffa19df412a4788a44",
            "patch": "@@ -537,9 +537,9 @@ def get_attn_mask(self, input_resolution):\n \n     def maybe_pad(self, hidden_states, height, width):\n         pad_left = pad_top = 0\n-        pad_rigth = (self.window_size - width % self.window_size) % self.window_size\n+        pad_right = (self.window_size - width % self.window_size) % self.window_size\n         pad_bottom = (self.window_size - height % self.window_size) % self.window_size\n-        pad_values = (0, 0, pad_left, pad_rigth, pad_top, pad_bottom)\n+        pad_values = (0, 0, pad_left, pad_right, pad_top, pad_bottom)\n         hidden_states = nn.functional.pad(hidden_states, pad_values)\n         return hidden_states, pad_values\n "
        },
        {
            "sha": "7fa62ff8d1805be8e5e9ec154e18262e941e53d1",
            "filename": "src/transformers/models/minimax/configuration_minimax.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fminimax%2Fconfiguration_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fminimax%2Fconfiguration_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fminimax%2Fconfiguration_minimax.py?ref=a543095c9967ddd3dd0915ffa19df412a4788a44",
            "patch": "@@ -87,7 +87,7 @@ class MiniMaxConfig(PretrainedConfig):\n         num_local_experts (`int`, *optional*, defaults to 8):\n             Number of experts per Sparse MLP layer.\n         output_router_logits (`bool`, *optional*, defaults to `False`):\n-            Whether or not the router logits should be returned by the model. Enabeling this will also\n+            Whether or not the router logits should be returned by the model. Enabling this will also\n             allow the model to output the auxiliary loss. See [here]() for more details\n         router_aux_loss_coef (`float`, *optional*, defaults to 0.001):\n             The aux loss factor for the total loss."
        },
        {
            "sha": "90338bde2f6e9f27ef8db7c52ebce0b9aca5ef52",
            "filename": "src/transformers/models/minimax/modeling_minimax.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py?ref=a543095c9967ddd3dd0915ffa19df412a4788a44",
            "patch": "@@ -228,7 +228,7 @@ def forward(\n                 current_attn_output = attn_output_inter + attn_output_intra\n                 attn_output.append(current_attn_output)\n \n-                # cacluate attn_weights_inter for next block or cache\n+                # calculate attn_weights_inter for next block or cache\n                 next_attn_weights_inter = torch.matmul(\n                     (current_key_states * current_key_decay).transpose(-1, -2), current_value_states\n                 )"
        },
        {
            "sha": "1090327af32cdca6dd895f4ebf44e87287ffcce2",
            "filename": "src/transformers/models/minimax/modular_minimax.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodular_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodular_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodular_minimax.py?ref=a543095c9967ddd3dd0915ffa19df412a4788a44",
            "patch": "@@ -115,7 +115,7 @@ class MiniMaxConfig(MixtralConfig):\n         num_local_experts (`int`, *optional*, defaults to 8):\n             Number of experts per Sparse MLP layer.\n         output_router_logits (`bool`, *optional*, defaults to `False`):\n-            Whether or not the router logits should be returned by the model. Enabeling this will also\n+            Whether or not the router logits should be returned by the model. Enabling this will also\n             allow the model to output the auxiliary loss. See [here]() for more details\n         router_aux_loss_coef (`float`, *optional*, defaults to 0.001):\n             The aux loss factor for the total loss.\n@@ -342,7 +342,7 @@ def forward(\n                 current_attn_output = attn_output_inter + attn_output_intra\n                 attn_output.append(current_attn_output)\n \n-                # cacluate attn_weights_inter for next block or cache\n+                # calculate attn_weights_inter for next block or cache\n                 next_attn_weights_inter = torch.matmul(\n                     (current_key_states * current_key_decay).transpose(-1, -2), current_value_states\n                 )"
        },
        {
            "sha": "9648fdac9c7b1427257c1a51b62c13fec1233753",
            "filename": "src/transformers/models/nllb_moe/modeling_nllb_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py?ref=a543095c9967ddd3dd0915ffa19df412a4788a44",
            "patch": "@@ -104,9 +104,9 @@ def load_balancing_loss_func(router_probs: torch.Tensor, expert_indices: torch.T\n \n     Args:\n         router_probs (`torch.Tensor`):\n-            Probability assigned to each expert per token. Shape: [batch_size, seqeunce_length, num_experts].\n+            Probability assigned to each expert per token. Shape: [batch_size, sequence_length, num_experts].\n         expert_indices (`torch.Tensor`):\n-            Indices tensor of shape [batch_size, seqeunce_length] identifying the selected expert for a given token.\n+            Indices tensor of shape [batch_size, sequence_length] identifying the selected expert for a given token.\n \n     Returns:\n         The auxiliary loss."
        },
        {
            "sha": "266198e865df1ee73f26834c3264beb2f408cc68",
            "filename": "src/transformers/models/nougat/tokenization_nougat_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fnougat%2Ftokenization_nougat_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fnougat%2Ftokenization_nougat_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnougat%2Ftokenization_nougat_fast.py?ref=a543095c9967ddd3dd0915ffa19df412a4788a44",
            "patch": "@@ -254,7 +254,7 @@ def get_slices(lines, clean_lines):\n     - The slice is less than 200 characters long.\n     - The slice is more than 3 characters long.\n     - The slice does not start with \"[MISSING_PAGE\".\n-    - The slice is either the same as the next slice or the ratio of the two in terms of Levensthein distance is\n+    - The slice is either the same as the next slice or the ratio of the two in terms of Levenshtein distance is\n       greater than 0.9.\n \n     Args:"
        },
        {
            "sha": "313a47b1dd597ffa4e686270a57fe9ad91d8d1b7",
            "filename": "src/transformers/models/paligemma/modeling_paligemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py?ref=a543095c9967ddd3dd0915ffa19df412a4788a44",
            "patch": "@@ -132,7 +132,7 @@ def _init_weights(self, module):\n \n @auto_docstring(\n     custom_intro=\"\"\"\n-    The Base Paligemma model which consists of a vision backbone and a language model withou language modeling head.,\n+    The Base Paligemma model which consists of a vision backbone and a language model without language modeling head.,\n     \"\"\"\n )\n class PaliGemmaModel(PaliGemmaPreTrainedModel):"
        },
        {
            "sha": "94ae657776926c653024911b0eee43d85c0f745a",
            "filename": "src/transformers/models/pix2struct/image_processing_pix2struct.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fimage_processing_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fimage_processing_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fimage_processing_pix2struct.py?ref=a543095c9967ddd3dd0915ffa19df412a4788a44",
            "patch": "@@ -51,7 +51,7 @@\n # adapted from: https://discuss.pytorch.org/t/tf-image-extract-patches-in-pytorch/171409/2\n def torch_extract_patches(image_tensor, patch_height, patch_width):\n     \"\"\"\n-    Utiliy function to extract patches from a given image tensor. Returns a tensor of shape\n+    Utility function to extract patches from a given image tensor. Returns a tensor of shape\n     (1, `rows`, `columns`, `num_channels`x `patch_height` x `patch_width`).\n \n     Args:"
        },
        {
            "sha": "fec76624196178d7793888132506803821673a5b",
            "filename": "src/transformers/models/pop2piano/modeling_pop2piano.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py?ref=a543095c9967ddd3dd0915ffa19df412a4788a44",
            "patch": "@@ -1264,7 +1264,7 @@ def generate(\n                 - 0 for tokens that are **padded**.\n             composer (`str`, *optional*, defaults to `\"composer1\"`):\n                 This value is passed to `Pop2PianoConcatEmbeddingToMel` to generate different embeddings for each\n-                `\"composer\"`. Please make sure that the composet value is present in `composer_to_feature_token` in\n+                `\"composer\"`. Please make sure that the composer value is present in `composer_to_feature_token` in\n                 `generation_config`. For an example please see\n                 https://huggingface.co/sweetcocoa/pop2piano/blob/main/generation_config.json .\n             generation_config (`~generation.GenerationConfig`, *optional*):"
        },
        {
            "sha": "f7aea3479f6ff3a97439154b4d4851bb723922b1",
            "filename": "src/transformers/models/pop2piano/tokenization_pop2piano.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fpop2piano%2Ftokenization_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fpop2piano%2Ftokenization_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpop2piano%2Ftokenization_pop2piano.py?ref=a543095c9967ddd3dd0915ffa19df412a4788a44",
            "patch": "@@ -555,7 +555,7 @@ def __call__(\n         # check if it is batched or not\n         # it is batched if its a list containing a list of `pretty_midi.Notes` where the outer list contains all the\n         # batches and the inner list contains all Notes for a single batch. Otherwise if np.ndarray is passed it will be\n-        # considered batched if it has shape of `[batch_size, seqence_length, 4]` or ndim=3.\n+        # considered batched if it has shape of `[batch_size, sequence_length, 4]` or ndim=3.\n         is_batched = notes.ndim == 3 if isinstance(notes, np.ndarray) else isinstance(notes[0], list)\n \n         # get the truncation and padding strategy"
        },
        {
            "sha": "dbdfd77b02f27d75f6a12b460cc4009d582e7ea2",
            "filename": "src/transformers/models/switch_transformers/modeling_switch_transformers.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py?ref=a543095c9967ddd3dd0915ffa19df412a4788a44",
            "patch": "@@ -94,9 +94,9 @@ def load_balancing_loss_func(router_probs: torch.Tensor, expert_indices: torch.T\n \n     Args:\n         router_probs (`torch.Tensor`):\n-            Probability assigned to each expert per token. Shape: [batch_size, seqeunce_length, num_experts].\n+            Probability assigned to each expert per token. Shape: [batch_size, sequence_length, num_experts].\n         expert_indices (`torch.Tensor`):\n-            Indices tensor of shape [batch_size, seqeunce_length] identifying the selected expert for a given token.\n+            Indices tensor of shape [batch_size, sequence_length] identifying the selected expert for a given token.\n \n     Returns:\n         The auxiliary loss."
        },
        {
            "sha": "790431caedee639156ddda1daef408d95a3594e0",
            "filename": "src/transformers/models/t5gemma/modeling_t5gemma.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py?ref=a543095c9967ddd3dd0915ffa19df412a4788a44",
            "patch": "@@ -217,7 +217,7 @@ def __init__(self, config: T5GemmaModuleConfig, layer_idx: int):\n         self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n         self.scaling = config.query_pre_attn_scalar**-0.5\n         self.attention_dropout = self.config.attention_dropout\n-        # Requied by flash attention: encoder selfattention is non-causal\n+        # Required by flash attention: encoder selfattention is non-causal\n         self.is_causal = config.is_decoder\n \n         self.q_proj = nn.Linear(\n@@ -599,7 +599,7 @@ class T5GemmaPreTrainedModel(PreTrainedModel):\n     }\n \n     def _init_weights(self, module):\n-        # TODO: support intialization for encoders and decoders separately(?)\n+        # TODO: support initialization for encoders and decoders separately(?)\n         super()._init_weights(module)\n         std = self.config.initializer_range\n         if isinstance(module, T5GemmaClassificationHead):"
        },
        {
            "sha": "930c353f05a093dd1d029ad4a649dfa41c45bf12",
            "filename": "src/transformers/models/t5gemma/modular_t5gemma.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py?ref=a543095c9967ddd3dd0915ffa19df412a4788a44",
            "patch": "@@ -236,7 +236,7 @@ def __init__(self, config, device=None):\n class T5GemmaSelfAttention(Gemma2Attention):\n     def __init__(self, config: T5GemmaModuleConfig, layer_idx: int):\n         super().__init__(config, layer_idx)\n-        # Requied by flash attention: encoder selfattention is non-causal\n+        # Required by flash attention: encoder selfattention is non-causal\n         self.is_causal = config.is_decoder\n \n \n@@ -480,7 +480,7 @@ class T5GemmaPreTrainedModel(Gemma2PreTrainedModel):\n     _no_split_modules = [\"T5GemmaBlock\"]\n \n     def _init_weights(self, module):\n-        # TODO: support intialization for encoders and decoders separately(?)\n+        # TODO: support initialization for encoders and decoders separately(?)\n         PreTrainedModel._init_weights(self, module)\n         std = self.config.initializer_range\n         if isinstance(module, T5GemmaClassificationHead):"
        },
        {
            "sha": "3ea9b45537cb262384873e39ec21157d8728ce10",
            "filename": "src/transformers/models/visual_bert/modeling_visual_bert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fvisual_bert%2Fmodeling_visual_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fvisual_bert%2Fmodeling_visual_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvisual_bert%2Fmodeling_visual_bert.py?ref=a543095c9967ddd3dd0915ffa19df412a4788a44",
            "patch": "@@ -1542,7 +1542,7 @@ def forward(\n         if labels is not None:\n             # scores = batch x selected position x visual_feature\n             # scores = selected_positions.bmm(visual_features.transpose(1,2))\n-            # label = batch x selected_postion x needed position\n+            # label = batch x selected_position x needed position\n             loss_fct = KLDivLoss(reduction=\"batchmean\")\n             log_softmax = LogSoftmax(dim=-1)\n             scores = log_softmax(logits)"
        },
        {
            "sha": "54011bb969fd7ec9f314ee860fc7716defb605b2",
            "filename": "src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_tf_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_tf_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_tf_wav2vec2.py?ref=a543095c9967ddd3dd0915ffa19df412a4788a44",
            "patch": "@@ -96,7 +96,7 @@ def _sample_without_replacement(distribution, num_samples):\n \n def _scatter_values_on_batch_indices(values, batch_indices, output_shape):\n     \"\"\"\n-    Scatter function as in PyTorch with indices in format (batch_dim, indixes)\n+    Scatter function as in PyTorch with indices in format (batch_dim, indices)\n     \"\"\"\n     indices_shape = shape_list(batch_indices)\n     # broadcast batch dim to indices_shape"
        },
        {
            "sha": "01a6aa13ba42e5fcc43774e8d77c87338c624d94",
            "filename": "src/transformers/models/whisper/generation_whisper.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py?ref=a543095c9967ddd3dd0915ffa19df412a4788a44",
            "patch": "@@ -140,7 +140,7 @@ def _pad_to_max_length(\n     timestamp_begin=None,\n ):\n     \"\"\"\n-    skip_ending_double_timestamps: when the segement ended with two timestamp tokens, whether to ignore the last timestamp token\n+    skip_ending_double_timestamps: when the segment ended with two timestamp tokens, whether to ignore the last timestamp token\n     see https://github.com/huggingface/transformers/pull/35750\n \n     _pad_to_max_length is used in different contexts:\n@@ -372,7 +372,7 @@ def _extract_token_timestamps(\n             jump_times = time_indices[jumps] * time_precision\n \n             # each predicted token has a corresponding timestamp, expect the eos token (or last predicted token) for which we don't retrieve cross attentions\n-            # (indeed contrary to OAI that re-run a full foward to retreive cross attentions for each token and therefore also the last one predicted, we retreive\n+            # (indeed contrary to OAI that re-run a full forward to retrieve cross attentions for each token and therefore also the last one predicted, we retrieve\n             # cross attentions directly from the auto-regressive generation, so we don't have cross attentiosn for the token at the end of the sequence. Nevertheless,\n             # that is not important since we expect this last token to be the eos token)\n             # 1. for decoder_input_ids, we set the timestamps to 0.0"
        },
        {
            "sha": "1d89fed8303409372d04de119fa87f44886d7aa8",
            "filename": "src/transformers/models/xglm/modeling_xglm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py?ref=a543095c9967ddd3dd0915ffa19df412a4788a44",
            "patch": "@@ -254,7 +254,7 @@ def forward(\n         attn_output = attn_output.transpose(1, 2)\n \n         # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n-        # partitioned aross GPUs when using tensor-parallelism.\n+        # partitioned across GPUs when using tensor-parallelism.\n         attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n \n         attn_output = self.out_proj(attn_output)"
        },
        {
            "sha": "22a9a13fe3e554cacb70d846e6798c92e4eaf703",
            "filename": "src/transformers/models/xlstm/configuration_xlstm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fxlstm%2Fconfiguration_xlstm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a543095c9967ddd3dd0915ffa19df412a4788a44/src%2Ftransformers%2Fmodels%2Fxlstm%2Fconfiguration_xlstm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlstm%2Fconfiguration_xlstm.py?ref=a543095c9967ddd3dd0915ffa19df412a4788a44",
            "patch": "@@ -171,7 +171,7 @@ def __init__(\n         chunkwise_kernel: ChunkwiseKernelType = \"chunkwise--native_autograd\",\n         sequence_kernel: SequenceKernelType = \"native_sequence__native\",\n         step_kernel: StepKernelType = \"native\",\n-        # nedded to enable generation\n+        # needed to enable generation\n         mode: BackendModeType = \"inference\",\n         chunk_size: int = 64,\n         # needed to be true for generation"
        },
        {
            "sha": "3ce58e4cb24a047a4b755dcdad819007e1263dbe",
            "filename": "tests/models/instructblip/test_modeling_instructblip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a543095c9967ddd3dd0915ffa19df412a4788a44/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a543095c9967ddd3dd0915ffa19df412a4788a44/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py?ref=a543095c9967ddd3dd0915ffa19df412a4788a44",
            "patch": "@@ -658,7 +658,7 @@ def test_sdpa_can_dispatch_composite_models(self):\n         \"\"\"\n         Tests if composite models dispatch correctly on SDPA/eager when requested so when loading the model.\n         This tests only by looking at layer names, as usually SDPA layers are called \"SDPAAttention\".\n-        In contrast to the above test, this one checks if the \"config._attn_implamentation\" is a dict after the model\n+        In contrast to the above test, this one checks if the \"config._attn_implementation\" is a dict after the model\n         is loaded, because we manually replicate requested attn implementation on each sub-config when loading.\n         See https://github.com/huggingface/transformers/pull/32238 for more info\n "
        },
        {
            "sha": "560780662e5cafebd11eb5e936cb0126fa43f840",
            "filename": "tests/models/pop2piano/test_feature_extraction_pop2piano.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a543095c9967ddd3dd0915ffa19df412a4788a44/tests%2Fmodels%2Fpop2piano%2Ftest_feature_extraction_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a543095c9967ddd3dd0915ffa19df412a4788a44/tests%2Fmodels%2Fpop2piano%2Ftest_feature_extraction_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpop2piano%2Ftest_feature_extraction_pop2piano.py?ref=a543095c9967ddd3dd0915ffa19df412a4788a44",
            "patch": "@@ -138,8 +138,8 @@ def test_integration(self):\n         speech_samples = ds.sort(\"id\").select([0])[\"audio\"]\n         input_speech = [x[\"array\"] for x in speech_samples][0]\n         sampling_rate = [x[\"sampling_rate\"] for x in speech_samples][0]\n-        feaure_extractor = Pop2PianoFeatureExtractor.from_pretrained(\"sweetcocoa/pop2piano\")\n-        input_features = feaure_extractor(\n+        feature_extractor = Pop2PianoFeatureExtractor.from_pretrained(\"sweetcocoa/pop2piano\")\n+        input_features = feature_extractor(\n             input_speech, sampling_rate=sampling_rate, return_tensors=\"pt\"\n         ).input_features\n "
        }
    ],
    "stats": {
        "total": 206,
        "additions": 103,
        "deletions": 103
    }
}