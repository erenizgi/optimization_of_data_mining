{
    "author": "mobicham",
    "message": "Hqq serialization (#33141)\n\n* HQQ model serialization attempt\r\n\r\n* fix hqq dispatch and unexpected keys\r\n\r\n* style\r\n\r\n* remove check_old_param\r\n\r\n* revert to check HQQLinear in quantizer_hqq.py\r\n\r\n* revert to check HQQLinear in quantizer_hqq.py\r\n\r\n* update HqqConfig default params\r\n\r\n* make ci happy\r\n\r\n* make ci happy\r\n\r\n* revert to HQQLinear check in quantizer_hqq.py\r\n\r\n* check hqq_min version 0.2.0\r\n\r\n* set axis=1 as default in quantization_config.py\r\n\r\n* validate_env with hqq>=0.2.0 version message\r\n\r\n* deprecated hqq kwargs message\r\n\r\n* make ci happy\r\n\r\n* remove run_expected_keys_check hack + bump to 0.2.1 min hqq version\r\n\r\n* fix unexpected_keys hqq update\r\n\r\n* add pre_quantized check\r\n\r\n* add update_expected_keys to base quantizerr\r\n\r\n* ci base.py fix?\r\n\r\n* ci base.py fix?\r\n\r\n* fix \"quantization typo\" src/transformers/utils/quantization_config.py\r\n\r\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\r\n\r\n* fix post merge\r\n\r\n---------\r\n\r\nCo-authored-by: Marc Sun <marc@huggingface.co>\r\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>",
    "sha": "f5247aca0151de9b53096772d3ba0516ee1b896f",
    "files": [
        {
            "sha": "34608cd64fd8a34eb261dc9ed0a3ab6f6200291a",
            "filename": "docs/source/en/quantization/hqq.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5247aca0151de9b53096772d3ba0516ee1b896f/docs%2Fsource%2Fen%2Fquantization%2Fhqq.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5247aca0151de9b53096772d3ba0516ee1b896f/docs%2Fsource%2Fen%2Fquantization%2Fhqq.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Fhqq.md?ref=f5247aca0151de9b53096772d3ba0516ee1b896f",
            "patch": "@@ -30,13 +30,13 @@ To quantize a model, you need to create an [`HqqConfig`]. There are two ways of\n from transformers import AutoModelForCausalLM, AutoTokenizer, HqqConfig\n \n # Method 1: all linear layers will use the same quantization config\n-quant_config  = HqqConfig(nbits=8, group_size=64, quant_zero=False, quant_scale=False, axis=0) #axis=0 is used by default\n+quant_config  = HqqConfig(nbits=8, group_size=64)\n ```\n \n ``` Python\n # Method 2: each linear layer with the same tag will use a dedicated quantization config\n-q4_config = {'nbits':4, 'group_size':64, 'quant_zero':False, 'quant_scale':False}\n-q3_config = {'nbits':3, 'group_size':32, 'quant_zero':False, 'quant_scale':False}\n+q4_config = {'nbits':4, 'group_size':64}\n+q3_config = {'nbits':3, 'group_size':32}\n quant_config  = HqqConfig(dynamic_config={\n   'self_attn.q_proj':q4_config,\n   'self_attn.k_proj':q4_config,"
        },
        {
            "sha": "162b365668a0474fc4b109627019546e21fb3b85",
            "filename": "src/transformers/integrations/hqq.py",
            "status": "modified",
            "additions": 10,
            "deletions": 2,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5247aca0151de9b53096772d3ba0516ee1b896f/src%2Ftransformers%2Fintegrations%2Fhqq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5247aca0151de9b53096772d3ba0516ee1b896f/src%2Ftransformers%2Fintegrations%2Fhqq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fhqq.py?ref=f5247aca0151de9b53096772d3ba0516ee1b896f",
            "patch": "@@ -66,6 +66,10 @@ def _prepare_for_hqq_linear(model, patch_params, has_been_replaced, current_key_\n \n             has_been_replaced = True\n \n+            # Add these fake parameters to avoid loading fail\n+            for att in [\"W_q\", \"meta\"]:\n+                setattr(module, att, None)\n+\n         if len(list(module.children())) > 0:\n             _, has_been_replaced = _prepare_for_hqq_linear(\n                 module,\n@@ -97,7 +101,7 @@ def prepare_for_hqq_linear(model, quantization_config=None, modules_to_not_conve\n \n     # Convert quantization_config to layer-wise config\n     skip_modules = quantization_config.skip_modules\n-    quant_config = quantization_config.to_dict()\n+    quant_config = quantization_config.quant_config\n     linear_tags = list(set(linear_tags) - set(skip_modules) - set(modules_to_not_convert))\n \n     if any(key in linear_tags for key in quant_config.keys()):\n@@ -113,7 +117,11 @@ def prepare_for_hqq_linear(model, quantization_config=None, modules_to_not_conve\n     )\n \n     # We store quantization config as linear_tag -> hqq quant config\n-    model.config.quantization_config = patch_params\n+    model.config.quantization_config = {\n+        \"quant_config\": quant_config,\n+        \"quant_method\": quantization_config.quant_method,\n+        \"skip_modules\": skip_modules,\n+    }\n \n     if not has_been_replaced:\n         logger.warning(\"No linear modules were found in your model for quantization.\")"
        },
        {
            "sha": "df051956676615545a933f9dd501ef37456354e4",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 10,
            "deletions": 1,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5247aca0151de9b53096772d3ba0516ee1b896f/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5247aca0151de9b53096772d3ba0516ee1b896f/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=f5247aca0151de9b53096772d3ba0516ee1b896f",
            "patch": "@@ -934,12 +934,17 @@ def _load_state_dict_into_meta_model(\n         # For compatibility with PyTorch load_state_dict which converts state dict dtype to existing dtype in model, and which\n         # uses `param.copy_(input_param)` that preserves the contiguity of the parameter in the model.\n         # Reference: https://github.com/pytorch/pytorch/blob/db79ceb110f6646523019a59bbd7b838f43d4a86/torch/nn/modules/module.py#L2040C29-L2040C29\n+\n         old_param = model\n         splits = param_name.split(\".\")\n         for split in splits:\n             old_param = getattr(old_param, split)\n+            # Not all the attributes of a module are Parameters/Tensor\n+            if not isinstance(old_param, (torch.nn.Parameter, torch.Tensor)):\n+                old_param = None\n             if old_param is None:\n                 break\n+\n         if old_param is not None:\n             if dtype is None:\n                 param = param.to(old_param.dtype)\n@@ -3819,6 +3824,7 @@ def from_pretrained(\n         from_pt = not (from_tf | from_flax)\n \n         # load pt weights early so that we know which dtype to init the model under\n+\n         if from_pt:\n             if not is_sharded and state_dict is None:\n                 # Time to load the checkpoint\n@@ -4176,6 +4182,9 @@ def _load_pretrained_model(\n         expected_keys = list(model_state_dict.keys())\n         prefix = model.base_model_prefix\n \n+        if hf_quantizer is not None:\n+            expected_keys = hf_quantizer.update_expected_keys(model, expected_keys, loaded_keys)\n+\n         def _fix_key(key):\n             if \"beta\" in key:\n                 return key.replace(\"beta\", \"bias\")\n@@ -4290,7 +4299,7 @@ def _fix_key(key):\n                     value = torch.empty(*param.size(), dtype=target_dtype)\n                     if (\n                         not is_quantized\n-                        or getattr(hf_quantizer, \"requires_parameters_quantization\", False)\n+                        or (getattr(hf_quantizer, \"requires_parameters_quantization\", False))\n                         or not hf_quantizer.check_quantized_param(\n                             model, param_value=value, param_name=key, state_dict={}\n                         )"
        },
        {
            "sha": "015c0015cf7e7b73e12f6a16cd8f79e46dd6669a",
            "filename": "src/transformers/quantizers/base.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5247aca0151de9b53096772d3ba0516ee1b896f/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5247aca0151de9b53096772d3ba0516ee1b896f/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fbase.py?ref=f5247aca0151de9b53096772d3ba0516ee1b896f",
            "patch": "@@ -109,6 +109,18 @@ def update_missing_keys(self, model, missing_keys: List[str], prefix: str) -> Li\n         \"\"\"\n         return missing_keys\n \n+    def update_expected_keys(self, model, expected_keys: List[str], loaded_keys: List[str]) -> List[str]:\n+        \"\"\"\n+        Override this method if you want to adjust the `update_expected_keys`.\n+\n+        Args:\n+            expected_keys (`List[str]`, *optional*):\n+                The list of the expected keys in the initialized model.\n+            loaded_keys (`List[str]`, *optional*):\n+                The list of the loaded keys in the checkpoint.\n+        \"\"\"\n+        return expected_keys\n+\n     def get_special_dtypes_update(self, model, torch_dtype: \"torch.dtype\") -> Dict[str, \"torch.dtype\"]:\n         \"\"\"\n         returns dtypes for modules that are not quantized - used for the computation of the device_map in case"
        },
        {
            "sha": "775fea8f4901e6116d796bca491e1bab5d74f46d",
            "filename": "src/transformers/quantizers/quantizer_hqq.py",
            "status": "modified",
            "additions": 105,
            "deletions": 8,
            "changes": 113,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5247aca0151de9b53096772d3ba0516ee1b896f/src%2Ftransformers%2Fquantizers%2Fquantizer_hqq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5247aca0151de9b53096772d3ba0516ee1b896f/src%2Ftransformers%2Fquantizers%2Fquantizer_hqq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_hqq.py?ref=f5247aca0151de9b53096772d3ba0516ee1b896f",
            "patch": "@@ -62,7 +62,7 @@ def __init__(self, quantization_config, **kwargs):\n     def validate_environment(self, *args, **kwargs):\n         if not (is_hqq_available()):\n             raise ImportError(\n-                \"HQQ is not available. Please follow the instructions to install it: `https://github.com/mobiusml/hqq/`\"\n+                \"A valid HQQ version (>=0.2.1) is not available. Please follow the instructions to install it: `https://github.com/mobiusml/hqq/`.\"\n             )\n \n         if kwargs.get(\"from_tf\", False) or kwargs.get(\"from_flax\", False):\n@@ -91,6 +91,65 @@ def validate_environment(self, *args, **kwargs):\n             else:\n                 self.using_multi_gpu = len(set(device_map.values())) > 1\n \n+    def update_missing_keys(\n+        self, model: \"PreTrainedModel\", missing_keys: List[str], prefix: str, **kwargs\n+    ) -> List[str]:\n+        if self.pre_quantized:\n+            return [key for key in missing_keys if (\"weight\" not in key)]\n+        else:\n+            return missing_keys\n+\n+    # Adds missing keys for HQQLinear modules that are loaded but the model with initialized with torch.nn.Linear\n+    def update_expected_keys(\n+        self, model: \"PreTrainedModel\", expected_keys: List[str], loaded_keys: List[str]\n+    ) -> List[str]:\n+        if not self.pre_quantized:\n+            return expected_keys\n+\n+        # Collects all quantizable (linear) layers\n+        def _find_hqq_quantizable_layers(model, layers):\n+            for name, module in model.named_children():\n+                if isinstance(module, (torch.nn.Linear)):\n+                    layers.add(module.name)\n+                _find_hqq_quantizable_layers(module, layers)\n+\n+        new_keys = set(expected_keys)\n+        if is_hqq_available():\n+            from hqq.core.quantize import HQQLinear\n+\n+            # Name modules\n+            for name, module in model.named_modules():\n+                module.name = name\n+\n+            # valid modules are Linear layers that have HQQLinear state_dict. We ignore skip_modules and any layers with Linear state_dict() params\n+            _valid_modules = set()\n+            _find_hqq_quantizable_layers(model, _valid_modules)\n+            _valid_modules -= set(model.config.quantization_config[\"skip_modules\"])\n+\n+            # Append new expected layers based on _ref_keys\n+            _ref_keys = HQQLinear(\n+                linear_layer=None, quant_config=None, compute_dtype=torch.float16, device=\"cpu\"\n+            ).state_dict_keys() - {\"bias\"}\n+\n+            # Clean-up\n+            _rm_keys = set()\n+            for key in new_keys:\n+                if any(_module in key for _module in _valid_modules):\n+                    _rm_keys.add(key)\n+            new_keys -= _rm_keys\n+            # At this point, new_keys contains all the keys of the layers that are NOT HQQLinear or torch.nn.Linear\n+\n+            # Re-populate Linear/HQQLinear\n+            for _module in _valid_modules:\n+                if _module + \".weight\" in loaded_keys:\n+                    new_keys.add(_module + \".weight\")\n+                else:\n+                    new_keys.update({_module + \".\" + _ref_key for _ref_key in _ref_keys})\n+                if _module + \".bias\" in loaded_keys:\n+                    new_keys.add(_module + \".bias\")\n+\n+        return list(new_keys)\n+\n     def check_quantized_param(\n         self,\n         model: \"PreTrainedModel\",\n@@ -99,9 +158,18 @@ def check_quantized_param(\n         state_dict: Dict[str, Any],\n         **kwargs,\n     ) -> bool:\n+        if is_hqq_available():\n+            from hqq.core.quantize import HQQLinear\n         module, tensor_name = get_module_from_name(model, param_name)\n \n-        return isinstance(module, torch.nn.Linear) and (tensor_name == \"weight\")\n+        if self.pre_quantized:\n+            return (\n+                (isinstance(module, torch.nn.Linear) or isinstance(module, HQQLinear))\n+                and tensor_name != \"weight\"\n+                and tensor_name != \"bias\"\n+            )\n+        else:\n+            return isinstance(module, torch.nn.Linear) and tensor_name == \"weight\"\n \n     def create_quantized_param(\n         self,\n@@ -122,21 +190,50 @@ def create_quantized_param(\n             from hqq.core.quantize import HQQLinear\n \n         module, tensor_name = get_module_from_name(model, param_name)\n-\n-        layer_name = param_name.replace(\".weight\", \"\").replace(\".bias\", \"\")\n+        layer_name = \".\".join(param_name.split(\".\")[:-1])\n         parent_module = find_parent(model, layer_name)\n         node = layer_name.split(\".\")[-1]\n \n-        # Step 0: set module state_dict\n-        module_state_dict = {key.split(\".\")[-1]: state_dict[key] for key in state_dict if layer_name in key}\n+        # set module state_dict\n+        module_state_dict = {}\n+        for k, v in state_dict.items():\n+            if layer_name + \".\" in k:\n+                module_state_dict[k.split(\".\")[-1]] = v\n+                if unexpected_keys is not None and k in unexpected_keys:\n+                    unexpected_keys.remove(k)\n+\n+        if self.pre_quantized:\n+            if isinstance(module, HQQLinear):\n+                return\n+            else:\n+                hqq_layer = HQQLinear(\n+                    linear_layer=None,\n+                    quant_config=None,\n+                    compute_dtype=self.torch_dtype,\n+                    device=target_device,\n+                )\n+\n+            hqq_layer.load_state_dict(module_state_dict)\n+\n+            if hqq_layer.bias is not None and isinstance(hqq_layer.bias, torch.Tensor):\n+                hqq_layer.bias = torch.nn.Parameter(hqq_layer.bias)\n+\n+            if self.using_multi_gpu:\n+                hqq_layer = self._patch_layer_for_multigpu(hqq_layer)\n+\n+            setattr(parent_module, node, hqq_layer)\n+\n+            # cleanup\n+            del module.__dict__, module\n+            torch.cuda.empty_cache()\n+            return\n \n         # Step 1: populate module with weight/bias from module state dict\n         for key in module_state_dict:\n             setattr(module, key, torch.nn.Parameter(module_state_dict[key]))\n \n         # Step 2: Replace module with either HQQLinear or move it to device. We do this via setattr on the parent as doing on it on the module\n         # directly doesn't work.\n-\n         if hasattr(module, \"quant_config\"):\n             hqq_layer = HQQLinear(\n                 module,\n@@ -192,7 +289,7 @@ def _process_model_after_weight_loading(self, model: \"PreTrainedModel\", **kwargs\n         return model\n \n     def is_serializable(self, safe_serialization=None):\n-        return False\n+        return True\n \n     @property\n     def is_trainable(self) -> bool:"
        },
        {
            "sha": "a98b17e4bd57397303a6509a74e3198690945a12",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5247aca0151de9b53096772d3ba0516ee1b896f/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5247aca0151de9b53096772d3ba0516ee1b896f/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=f5247aca0151de9b53096772d3ba0516ee1b896f",
            "patch": "@@ -92,6 +92,7 @@ def _is_package_available(pkg_name: str, return_version: bool = False) -> Union[\n FSDP_MIN_VERSION = \"1.12.0\"\n GGUF_MIN_VERSION = \"0.10.0\"\n XLA_FSDPV2_MIN_VERSION = \"2.2.0\"\n+HQQ_MIN_VERSION = \"0.2.1\"\n \n \n _accelerate_available, _accelerate_version = _is_package_available(\"accelerate\", return_version=True)\n@@ -181,7 +182,7 @@ def _is_package_available(pkg_name: str, return_version: bool = False) -> Union[\n _torchdistx_available = _is_package_available(\"torchdistx\")\n _torchvision_available = _is_package_available(\"torchvision\")\n _mlx_available = _is_package_available(\"mlx\")\n-_hqq_available = _is_package_available(\"hqq\")\n+_hqq_available, _hqq_version = _is_package_available(\"hqq\", return_version=True)\n _tiktoken_available = _is_package_available(\"tiktoken\")\n _blobfile_available = _is_package_available(\"blobfile\")\n _liger_kernel_available = _is_package_available(\"liger_kernel\")\n@@ -323,8 +324,8 @@ def is_torch_deterministic():\n         return True\n \n \n-def is_hqq_available():\n-    return _hqq_available\n+def is_hqq_available(min_version: str = HQQ_MIN_VERSION):\n+    return _hqq_available and version.parse(_hqq_version) >= version.parse(min_version)\n \n \n def is_pygments_available():"
        },
        {
            "sha": "8be0bb672e51b8a8a7998d0ec0a47e3bb09664d7",
            "filename": "src/transformers/utils/quantization_config.py",
            "status": "modified",
            "additions": 27,
            "deletions": 15,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5247aca0151de9b53096772d3ba0516ee1b896f/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5247aca0151de9b53096772d3ba0516ee1b896f/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fquantization_config.py?ref=f5247aca0151de9b53096772d3ba0516ee1b896f",
            "patch": "@@ -193,15 +193,9 @@ class HqqConfig(QuantizationConfigMixin):\n             Number of bits. Supported values are (8, 4, 3, 2, 1).\n         group_size (`int`, *optional*, defaults to 64):\n             Group-size value. Supported values are any value that is divisble by weight.shape[axis]).\n-        quant_zero (`bool`, *optional*, defaults to `True`):\n-            Quantize the zero-point if set to `True`.\n-        quant_scale (`bool`, *optional*, defaults to `False`):\n-            Quantize the scaling if set to `True`.\n-        offload_meta (`bool`, *optional*, defaults to `False`):\n-            Offload the meta-data to the CPU if set to `True`.\n         view_as_float (`bool`, *optional*, defaults to `False`):\n             View the quantized weight as float (used in distributed training) if set to `True`.\n-        axis (`int`, *optional*, defaults to 0):\n+        axis (`Optional[int]`, *optional*):\n             Axis along which grouping is performed. Supported values are 0 or 1.\n         dynamic_config (dict, *optional*):\n             Parameters for dynamic configuration. The key is the name tag of the layer and the value is a quantization config.\n@@ -216,18 +210,25 @@ def __init__(\n         self,\n         nbits: int = 4,\n         group_size: int = 64,\n-        quant_zero: bool = True,\n-        quant_scale: bool = False,\n-        offload_meta: bool = False,\n         view_as_float: bool = False,\n-        axis: int = 0,\n+        axis: Optional[int] = None,\n         dynamic_config: Optional[dict] = None,\n         skip_modules: List[str] = [\"lm_head\"],\n         **kwargs,\n     ):\n         if is_hqq_available():\n             from hqq.core.quantize import BaseQuantizeConfig as HQQBaseQuantizeConfig\n \n+        for deprecated_key in [\"quant_zero\", \"quant_scale\", \"offload_meta\"]:\n+            if deprecated_key in kwargs:\n+                logger.info(\n+                    deprecated_key + \" is deprecated. This parameter will be ignored in quantization settings.\"\n+                )\n+\n+        if axis is None:\n+            axis = 1\n+            logger.info(\"Setting axis=1 as faster backends such as TorchAO or BitBlas are only compatible with it.\")\n+\n         if axis not in [0, 1]:\n             raise ValueError(\"Invalid axis value. Only 0 and 1 are allowed.\")\n \n@@ -240,9 +241,6 @@ def __init__(\n                 **{\n                     \"nbits\": nbits,\n                     \"group_size\": group_size,\n-                    \"quant_zero\": quant_zero,\n-                    \"quant_scale\": quant_scale,\n-                    \"offload_meta\": offload_meta,\n                     \"view_as_float\": view_as_float,\n                     \"axis\": axis,\n                 }\n@@ -259,12 +257,26 @@ def post_init(self):\n         \"\"\"\n         pass\n \n+    @classmethod\n+    def from_dict(cls, config: Dict[str, Any]):\n+        \"\"\"\n+        Override from_dict, used in AutoQuantizationConfig.from_dict in quantizers/auto.py\n+        \"\"\"\n+        instance = cls()\n+        instance.quant_config = config[\"quant_config\"]\n+        instance.skip_modules = config[\"skip_modules\"]\n+        return instance\n+\n     def to_dict(self) -> Dict[str, Any]:\n         \"\"\"\n         Serializes this instance to a Python dictionary. Returns:\n             `Dict[str, Any]`: Dictionary of all the attributes that make up this configuration instance.\n         \"\"\"\n-        return self.quant_config\n+        return {\n+            \"quant_config\": self.quant_config,\n+            \"quant_method\": self.quant_method,\n+            \"skip_modules\": self.skip_modules,\n+        }\n \n     def __repr__(self):\n         config_dict = self.to_dict()"
        },
        {
            "sha": "6d08a0f0e6691af111f90b591ec9d964e7c94fa1",
            "filename": "tests/quantization/hqq/test_hqq.py",
            "status": "modified",
            "additions": 43,
            "deletions": 28,
            "changes": 71,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5247aca0151de9b53096772d3ba0516ee1b896f/tests%2Fquantization%2Fhqq%2Ftest_hqq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5247aca0151de9b53096772d3ba0516ee1b896f/tests%2Fquantization%2Fhqq%2Ftest_hqq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fhqq%2Ftest_hqq.py?ref=f5247aca0151de9b53096772d3ba0516ee1b896f",
            "patch": "@@ -94,8 +94,7 @@ def test_to_dict(self):\n         quantization_config = HqqConfig()\n         hqq_orig_config = quantization_config.to_dict()\n \n-        for key in hqq_orig_config:\n-            self.assertEqual(quantization_config.quant_config[key], hqq_orig_config[key])\n+        self.assertEqual(quantization_config.quant_config, hqq_orig_config[\"quant_config\"])\n \n \n @slow\n@@ -109,7 +108,7 @@ def test_fp16_quantized_model(self):\n         \"\"\"\n         Simple LLM model testing fp16\n         \"\"\"\n-        quant_config = HqqConfig(nbits=8, group_size=64, quant_zero=False, quant_scale=False, axis=0)\n+        quant_config = HqqConfig(nbits=8, group_size=64)\n \n         hqq_runner = HQQLLMRunner(\n             model_id=MODEL_ID, quant_config=quant_config, compute_dtype=torch.float16, device=torch_device\n@@ -118,26 +117,24 @@ def test_fp16_quantized_model(self):\n         check_hqqlayer(self, hqq_runner.model.model.layers[0].self_attn.v_proj)\n         check_forward(self, hqq_runner.model)\n \n-    def test_f16_quantized_model_with_offloading(self):\n+\n+@slow\n+@require_torch_gpu\n+@require_torch_multi_gpu\n+@require_accelerate\n+class HQQTestMultiGPU(unittest.TestCase):\n+    def tearDown(self):\n+        cleanup()\n+\n+    def test_fp16_quantized_model_multipgpu(self):\n         \"\"\"\n-        Simple LLM model testing bfp16 with meta-data offloading\n+        Simple LLM model testing fp16 with multi-gpu\n         \"\"\"\n-        q4_config = {\"nbits\": 4, \"group_size\": 64, \"quant_zero\": False, \"quant_scale\": False}\n-        q3_config = {\"nbits\": 3, \"group_size\": 32, \"quant_zero\": False, \"quant_scale\": False, \"offload_meta\": True}\n-        quant_config = HqqConfig(\n-            dynamic_config={\n-                \"self_attn.q_proj\": q4_config,\n-                \"self_attn.k_proj\": q4_config,\n-                \"self_attn.v_proj\": q4_config,\n-                \"self_attn.o_proj\": q4_config,\n-                \"mlp.gate_proj\": q3_config,\n-                \"mlp.up_proj\": q3_config,\n-                \"mlp.down_proj\": q3_config,\n-            }\n-        )\n+\n+        quant_config = HqqConfig(nbits=8, group_size=64)\n \n         hqq_runner = HQQLLMRunner(\n-            model_id=MODEL_ID, quant_config=quant_config, compute_dtype=torch.float16, device=torch_device\n+            model_id=MODEL_ID, quant_config=quant_config, compute_dtype=torch.float16, device=\"auto\"\n         )\n \n         check_hqqlayer(self, hqq_runner.model.model.layers[0].self_attn.v_proj)\n@@ -146,22 +143,40 @@ def test_f16_quantized_model_with_offloading(self):\n \n @slow\n @require_torch_gpu\n-@require_torch_multi_gpu\n @require_accelerate\n-class HQQTestMultiGPU(unittest.TestCase):\n+class HQQSerializationTest(unittest.TestCase):\n     def tearDown(self):\n         cleanup()\n \n-    def test_fp16_quantized_model_multipgpu(self):\n+    def test_model_serialization(self):\n         \"\"\"\n-        Simple LLM model testing fp16 with multi-gpu\n+        Simple HQQ LLM save/load test\n         \"\"\"\n-\n-        quant_config = HqqConfig(nbits=8, group_size=64, quant_zero=False, quant_scale=False, axis=0)\n+        quant_config = HqqConfig(nbits=4, group_size=64)\n \n         hqq_runner = HQQLLMRunner(\n-            model_id=MODEL_ID, quant_config=quant_config, compute_dtype=torch.float16, device=\"auto\"\n+            model_id=MODEL_ID, quant_config=quant_config, compute_dtype=torch.float16, device=torch_device\n         )\n \n-        check_hqqlayer(self, hqq_runner.model.model.layers[0].self_attn.v_proj)\n-        check_forward(self, hqq_runner.model)\n+        input_tensor = torch.zeros((1, 8), dtype=torch.int32, device=torch_device)\n+\n+        with torch.no_grad():\n+            logits_ref = hqq_runner.model.forward(input_tensor).logits\n+\n+        # Save\n+        saved_model_id = \"quant_model\"\n+        hqq_runner.model.save_pretrained(saved_model_id)\n+\n+        # Remove old model\n+        del hqq_runner.model\n+        torch.cuda.empty_cache()\n+\n+        # Load and check if the logits match\n+        model_loaded = AutoModelForCausalLM.from_pretrained(\n+            \"quant_model\", torch_dtype=torch.float16, device_map=torch_device, low_cpu_mem_usage=True\n+        )\n+\n+        with torch.no_grad():\n+            logits_loaded = model_loaded.forward(input_tensor).logits\n+\n+        self.assertEqual((logits_loaded - logits_ref).abs().mean().item(), 0)"
        }
    ],
    "stats": {
        "total": 274,
        "additions": 214,
        "deletions": 60
    }
}