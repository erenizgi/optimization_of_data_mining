{
    "author": "SunMarc",
    "message": "[v5] Remove `model_parallel` deprecated feature  (#41166)\n\n* fix\n\n* remove model parallel\n\n* style\n\n* removed a bit too much\n\n* rm comments\n\n* fix",
    "sha": "ad74fba08521b34f36cd72f6254b9b19a4fdb04b",
    "files": [
        {
            "sha": "49efb028d14658f23f4ab22af99a2abe58084c5e",
            "filename": "src/transformers/loss/loss_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Floss%2Floss_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Floss%2Floss_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Floss%2Floss_utils.py?ref=ad74fba08521b34f36cd72f6254b9b19a4fdb04b",
            "patch": "@@ -62,7 +62,6 @@ def ForCausalLMLoss(\n     # Flatten the tokens\n     logits = logits.view(-1, vocab_size)\n     shift_labels = shift_labels.view(-1)\n-    # Enable model parallelism\n     shift_labels = shift_labels.to(logits.device)\n     loss = fixed_cross_entropy(logits, shift_labels, num_items_in_batch, ignore_index, **kwargs)\n     return loss\n@@ -82,7 +81,6 @@ def ForMaskedLMLoss(\n     # Flatten the tokens\n     logits = logits.view(-1, vocab_size)\n     labels = labels.view(-1)\n-    # Enable model parallelism\n \n     labels = labels.to(logits.device)\n     loss = fixed_cross_entropy(logits, labels, num_items_in_batch, ignore_index, **kwargs)"
        },
        {
            "sha": "6809b51bb4bfc69f393c2cbeb94480d33ad1df8e",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=ad74fba08521b34f36cd72f6254b9b19a4fdb04b",
            "patch": "@@ -1866,7 +1866,6 @@ class PreTrainedModel(nn.Module, EmbeddingAccessMixin, ModuleUtilsMixin, PushToH\n           for this model architecture.\n         - **base_model_prefix** (`str`) -- A string indicating the attribute associated to the base model in derived\n           classes of the same architecture adding modules on top of the base model.\n-        - **is_parallelizable** (`bool`) -- A flag indicating whether this model supports model parallelization.\n         - **main_input_name** (`str`) -- The name of the principal input to the model (often `input_ids` for NLP\n           models, `pixel_values` for vision models and `input_values` for speech models).\n         - **can_record_outputs** (dict):\n@@ -1901,7 +1900,6 @@ class PreTrainedModel(nn.Module, EmbeddingAccessMixin, ModuleUtilsMixin, PushToH\n     # a list of `state_dict` keys that are potentially tied to another key in the state_dict.\n     _tied_weights_keys = None\n \n-    is_parallelizable = False\n     supports_gradient_checkpointing = False\n     _is_stateful = False\n "
        },
        {
            "sha": "3698c55db03a9feb3941e1b3397bb57dc6a79c13",
            "filename": "src/transformers/models/bloom/modeling_bloom.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py?ref=ad74fba08521b34f36cd72f6254b9b19a4fdb04b",
            "patch": "@@ -888,7 +888,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # move labels to correct device to enable model parallelism\n+            # move labels to correct device\n             labels = labels.to(lm_logits.device)\n             # Flatten the tokens\n             loss = self.loss_function(\n@@ -1133,7 +1133,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # move labels to correct device to enable model parallelism\n+            # move labels to correct device\n             labels = labels.to(logits.device)\n             batch_size, seq_length = labels.shape\n             loss_fct = CrossEntropyLoss()"
        },
        {
            "sha": "670c7784b2a8b9024e2e36da6937ce40bd41a202",
            "filename": "src/transformers/models/camembert/modeling_camembert.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py?ref=ad74fba08521b34f36cd72f6254b9b19a4fdb04b",
            "patch": "@@ -1012,7 +1012,7 @@ def forward(\n \n         masked_lm_loss = None\n         if labels is not None:\n-            # move labels to correct device to enable model parallelism\n+            # move labels to correct device\n             labels = labels.to(prediction_scores.device)\n             loss_fct = CrossEntropyLoss()\n             masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n@@ -1108,7 +1108,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # move labels to correct device to enable model parallelism\n+            # move labels to correct device\n             labels = labels.to(logits.device)\n             if self.config.problem_type is None:\n                 if self.num_labels == 1:\n@@ -1225,7 +1225,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # move labels to correct device to enable model parallelism\n+            # move labels to correct device\n             labels = labels.to(reshaped_logits.device)\n             loss_fct = CrossEntropyLoss()\n             loss = loss_fct(reshaped_logits, labels)\n@@ -1298,7 +1298,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # move labels to correct device to enable model parallelism\n+            # move labels to correct device\n             labels = labels.to(logits.device)\n             loss_fct = CrossEntropyLoss()\n             loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n@@ -1491,7 +1491,7 @@ def forward(\n \n         lm_loss = None\n         if labels is not None:\n-            # move labels to correct device to enable model parallelism\n+            # move labels to correct device\n             labels = labels.to(prediction_scores.device)\n             lm_loss = self.loss_function(\n                 prediction_scores,"
        },
        {
            "sha": "dca85aae1d7e1789c1cc4b0f35ffa5499a4019b1",
            "filename": "src/transformers/models/camembert/modular_camembert.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodular_camembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodular_camembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodular_camembert.py?ref=ad74fba08521b34f36cd72f6254b9b19a4fdb04b",
            "patch": "@@ -105,7 +105,7 @@ def forward(\n \n         masked_lm_loss = None\n         if labels is not None:\n-            # move labels to correct device to enable model parallelism\n+            # move labels to correct device\n             labels = labels.to(prediction_scores.device)\n             loss_fct = CrossEntropyLoss()\n             masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n@@ -168,7 +168,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # move labels to correct device to enable model parallelism\n+            # move labels to correct device\n             labels = labels.to(logits.device)\n             if self.config.problem_type is None:\n                 if self.num_labels == 1:\n@@ -280,7 +280,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # move labels to correct device to enable model parallelism\n+            # move labels to correct device\n             labels = labels.to(reshaped_logits.device)\n             loss_fct = CrossEntropyLoss()\n             loss = loss_fct(reshaped_logits, labels)\n@@ -344,7 +344,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # move labels to correct device to enable model parallelism\n+            # move labels to correct device\n             labels = labels.to(logits.device)\n             loss_fct = CrossEntropyLoss()\n             loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n@@ -513,7 +513,7 @@ def forward(\n \n         lm_loss = None\n         if labels is not None:\n-            # move labels to correct device to enable model parallelism\n+            # move labels to correct device\n             labels = labels.to(prediction_scores.device)\n             lm_loss = self.loss_function(\n                 prediction_scores,"
        },
        {
            "sha": "1727338c98a220506a329e4a4dc3fddb9d2596ef",
            "filename": "src/transformers/models/codegen/modeling_codegen.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py?ref=ad74fba08521b34f36cd72f6254b9b19a4fdb04b",
            "patch": "@@ -638,7 +638,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # move labels to correct device to enable model parallelism\n+            # move labels to correct device\n             labels = labels.to(lm_logits.device)\n             # Flatten the tokens\n             loss = self.loss_function("
        },
        {
            "sha": "672a652ed08ba4e17a5965b7930fa4fb952b1833",
            "filename": "src/transformers/models/decision_transformer/modeling_decision_transformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 21,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py?ref=ad74fba08521b34f36cd72f6254b9b19a4fdb04b",
            "patch": "@@ -399,7 +399,6 @@ def forward(\n class DecisionTransformerGPT2PreTrainedModel(PreTrainedModel):\n     config: DecisionTransformerConfig\n     base_model_prefix = \"transformer\"\n-    is_parallelizable = True\n     supports_gradient_checkpointing = True\n \n     _can_compile_fullgraph = False\n@@ -448,9 +447,6 @@ def __init__(self, config):\n         )\n         self.ln_f = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_epsilon)\n \n-        # Model parallel\n-        self.model_parallel = False\n-        self.device_map = None\n         self.gradient_checkpointing = False\n \n         # Initialize weights and apply final processing\n@@ -581,17 +577,6 @@ def forward(\n         all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n         all_hidden_states = () if output_hidden_states else None\n         for i, block in enumerate(self.h):\n-            # Model parallel\n-            if self.model_parallel:\n-                torch.cuda.set_device(hidden_states.device)\n-                # Ensure that attention_mask is always on the same device as hidden_states\n-                if attention_mask is not None:\n-                    attention_mask = attention_mask.to(hidden_states.device)\n-                if isinstance(head_mask, torch.Tensor):\n-                    head_mask = head_mask.to(hidden_states.device)\n-            if output_hidden_states:\n-                all_hidden_states = all_hidden_states + (hidden_states,)\n-\n             outputs = block(\n                 hidden_states,\n                 past_key_values if not (self.gradient_checkpointing and self.training) else None,\n@@ -611,12 +596,6 @@ def forward(\n                 if self.config.add_cross_attention:\n                     all_cross_attentions = all_cross_attentions + (outputs[2],)\n \n-            # Model Parallel: If it's the last layer for that device, put things on the next device\n-            if self.model_parallel:\n-                for k, v in self.device_map.items():\n-                    if i == v[-1] and \"cuda:\" + str(k) != self.last_device:\n-                        hidden_states = hidden_states.to(\"cuda:\" + str(k + 1))\n-\n         hidden_states = self.ln_f(hidden_states)\n \n         hidden_states = hidden_states.view(output_shape)"
        },
        {
            "sha": "919a3feaa067501602495976118993cdde814a79",
            "filename": "src/transformers/models/deprecated/gptsan_japanese/modeling_gptsan_japanese.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fmodeling_gptsan_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fmodeling_gptsan_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fmodeling_gptsan_japanese.py?ref=ad74fba08521b34f36cd72f6254b9b19a4fdb04b",
            "patch": "@@ -1214,7 +1214,7 @@ def forward(\n         router_probs = None\n         aux_loss = None\n         if labels is not None:\n-            # move labels to correct device to enable model parallelism\n+            # move labels to correct device\n             labels = labels.to(lm_logits.device)\n \n             loss_fct = nn.CrossEntropyLoss(ignore_index=-100)"
        },
        {
            "sha": "ed0461958a3373e9030b7d76bdb297bee1aaa94a",
            "filename": "src/transformers/models/deprecated/open_llama/modeling_open_llama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fmodeling_open_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fmodeling_open_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fmodeling_open_llama.py?ref=ad74fba08521b34f36cd72f6254b9b19a4fdb04b",
            "patch": "@@ -746,7 +746,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # move labels to correct device to enable model parallelism\n+            # move labels to correct device\n             labels = labels.to(logits.device)\n             # Shift so that tokens < n predict n\n             shift_logits = logits[..., :-1, :].contiguous()\n@@ -755,7 +755,6 @@ def forward(\n             loss_fct = CrossEntropyLoss()\n             shift_logits = shift_logits.view(-1, self.config.vocab_size)\n             shift_labels = shift_labels.view(-1)\n-            # Enable model parallelism\n             shift_labels = shift_labels.to(shift_logits.device)\n             loss = loss_fct(shift_logits, shift_labels)\n "
        },
        {
            "sha": "2cd4f21a928c1ff0f0f51c2fe90ef882d7fd8ae6",
            "filename": "src/transformers/models/falcon_mamba/modeling_falcon_mamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py?ref=ad74fba08521b34f36cd72f6254b9b19a4fdb04b",
            "patch": "@@ -913,7 +913,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # move labels to correct device to enable model parallelism\n+            # move labels to correct device\n             labels = labels.to(logits.device)\n             # Shift so that tokens < n predict n\n             shift_logits = logits[..., :-1, :].contiguous()"
        },
        {
            "sha": "861bed81c820a5ad0344fd104d0899bfe148db7b",
            "filename": "src/transformers/models/gpt2/modeling_gpt2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 208,
            "changes": 208,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py?ref=ad74fba08521b34f36cd72f6254b9b19a4fdb04b",
            "patch": "@@ -16,7 +16,6 @@\n \"\"\"PyTorch OpenAI GPT-2 model.\"\"\"\n \n import math\n-import warnings\n from dataclasses import dataclass\n from typing import Callable, Optional, Union\n \n@@ -41,12 +40,10 @@\n from ...pytorch_utils import Conv1D, find_pruneable_heads_and_indices, prune_conv1d_layer\n from ...utils import (\n     ModelOutput,\n-    add_start_docstrings,\n     auto_docstring,\n     logging,\n )\n from ...utils.deprecation import deprecate_kwarg\n-from ...utils.model_parallel_utils import assert_device_map, get_device_map\n from .configuration_gpt2 import GPT2Config\n \n \n@@ -506,7 +503,6 @@ def forward(\n class GPT2PreTrainedModel(PreTrainedModel):\n     config: GPT2Config\n     base_model_prefix = \"transformer\"\n-    is_parallelizable = True\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"GPT2Block\"]\n     _skip_keys_device_placement = \"past_key_values\"\n@@ -577,58 +573,6 @@ class GPT2DoubleHeadsModelOutput(ModelOutput):\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n \n \n-PARALLELIZE_DOCSTRING = r\"\"\"\n-    This is an experimental feature and is a subject to change at a moment's notice.\n-\n-    Uses a device map to distribute attention modules of the model across several devices. If no device map is given,\n-    it will evenly distribute blocks across all devices.\n-\n-    Args:\n-        device_map (`dict[int, list]`, *optional*):\n-            A dictionary that maps attention modules to devices. Note that the embedding module and LMHead are always\n-            automatically mapped to the first device (for esoteric reasons). That means that the first device should\n-            have fewer attention modules mapped to it than other devices. For reference, the gpt2 models have the\n-            following number of attention modules:\n-\n-                - openai-community/gpt2: 12\n-                - openai-community/gpt2-medium: 24\n-                - openai-community/gpt2-large: 36\n-                - openai-community/gpt2-xl: 48\n-\n-    Example:\n-\n-    ```python\n-    # Here is an example of a device map on a machine with 4 GPUs using gpt2-xl, which has a total of 48 attention modules:\n-    model = GPT2LMHeadModel.from_pretrained(\"openai-community/gpt2-xl\")\n-    device_map = {\n-        0: [0, 1, 2, 3, 4, 5, 6, 7, 8],\n-        1: [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21],\n-        2: [22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34],\n-        3: [35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47],\n-    }\n-    model.parallelize(device_map)\n-    ```\n-\"\"\"\n-DEPARALLELIZE_DOCSTRING = r\"\"\"\n-    Moves the model to cpu from a model parallel state.\n-\n-    Example:\n-\n-    ```python\n-    # On a 4 GPU machine with openai-community/gpt2-large:\n-    model = GPT2LMHeadModel.from_pretrained(\"openai-community/gpt2-large\")\n-    device_map = {\n-        0: [0, 1, 2, 3, 4, 5, 6, 7],\n-        1: [8, 9, 10, 11, 12, 13, 14, 15],\n-        2: [16, 17, 18, 19, 20, 21, 22, 23],\n-        3: [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35],\n-    }\n-    model.parallelize(device_map)  # Splits the model across several devices\n-    model.deparallelize()  # Put the model back on cpu and cleans memory by calling torch.cuda.empty_cache()\n-    ```\n-\"\"\"\n-\n-\n @auto_docstring\n class GPT2Model(GPT2PreTrainedModel):\n     _supports_param_buffer_assignment = False\n@@ -645,59 +589,12 @@ def __init__(self, config):\n         self.h = nn.ModuleList([GPT2Block(config, layer_idx=i) for i in range(config.num_hidden_layers)])\n         self.ln_f = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_epsilon)\n \n-        # Model parallel\n-        self.model_parallel = False\n-        self.device_map = None\n         self.gradient_checkpointing = False\n         self._attn_implementation = config._attn_implementation\n \n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings(PARALLELIZE_DOCSTRING)\n-    def parallelize(self, device_map=None):\n-        # Check validity of device_map\n-        warnings.warn(\n-            \"`GPT2Model.parallelize` is deprecated and will be removed in v5 of Transformers, you should load your\"\n-            \" model with `device_map='balanced'` in the call to `from_pretrained`. You can also provide your own\"\n-            \" `device_map` but it needs to be a dictionary module_name to device, so for instance {'h.0': 0, 'h.1': 1,\"\n-            \" ...}\",\n-            FutureWarning,\n-        )\n-        self.device_map = (\n-            get_device_map(len(self.h), range(torch.cuda.device_count())) if device_map is None else device_map\n-        )\n-        assert_device_map(self.device_map, len(self.h))\n-        self.model_parallel = True\n-        self.first_device = \"cpu\" if \"cpu\" in self.device_map else \"cuda:\" + str(min(self.device_map.keys()))\n-        self.last_device = \"cuda:\" + str(max(self.device_map.keys()))\n-        self.wte = self.wte.to(self.first_device)\n-        self.wpe = self.wpe.to(self.first_device)\n-        # Load onto devices\n-        for k, v in self.device_map.items():\n-            for block in v:\n-                cuda_device = \"cuda:\" + str(k)\n-                self.h[block] = self.h[block].to(cuda_device)\n-        # ln_f to last\n-        self.ln_f = self.ln_f.to(self.last_device)\n-\n-    @add_start_docstrings(DEPARALLELIZE_DOCSTRING)\n-    def deparallelize(self):\n-        warnings.warn(\n-            \"Like `parallelize`, `deparallelize` is deprecated and will be removed in v5 of Transformers.\",\n-            FutureWarning,\n-        )\n-        self.model_parallel = False\n-        self.device_map = None\n-        self.first_device = \"cpu\"\n-        self.last_device = \"cpu\"\n-        self.wte = self.wte.to(\"cpu\")\n-        self.wpe = self.wpe.to(\"cpu\")\n-        for index in range(len(self.h)):\n-            self.h[index] = self.h[index].to(\"cpu\")\n-        self.ln_f = self.ln_f.to(\"cpu\")\n-        torch.cuda.empty_cache()\n-\n     def get_input_embeddings(self):\n         return self.wte\n \n@@ -854,11 +751,6 @@ def forward(\n         all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n         all_hidden_states = () if output_hidden_states else None\n         for i, block in enumerate(self.h):\n-            # Model parallel\n-            if self.model_parallel:\n-                torch.cuda.set_device(hidden_states.device)\n-                if isinstance(head_mask, torch.Tensor):\n-                    head_mask = head_mask.to(hidden_states.device)\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n@@ -882,12 +774,6 @@ def forward(\n                 if self.config.add_cross_attention:\n                     all_cross_attentions = all_cross_attentions + (outputs[2],)\n \n-            # Model Parallel: If it's the last layer for that device, put things on the next device\n-            if self.model_parallel:\n-                for k, v in self.device_map.items():\n-                    if i == v[-1] and \"cuda:\" + str(k) != self.last_device:\n-                        hidden_states = hidden_states.to(\"cuda:\" + str(k + 1))\n-\n         hidden_states = self.ln_f(hidden_states)\n \n         hidden_states = hidden_states.view(output_shape)\n@@ -926,44 +812,9 @@ def __init__(self, config):\n         self.transformer = GPT2Model(config)\n         self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n \n-        # Model parallel\n-        self.model_parallel = False\n-        self.device_map = None\n-\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings(PARALLELIZE_DOCSTRING)\n-    def parallelize(self, device_map=None):\n-        warnings.warn(\n-            \"`GPT2LMHeadModel.parallelize` is deprecated and will be removed in v5 of Transformers, you should load\"\n-            \" your model with `device_map='balanced'` in the call to `from_pretrained`. You can also provide your own\"\n-            \" `device_map` but it needs to be a dictionary module_name to device, so for instance {'transformer.h.0':\"\n-            \" 0, 'transformer.h.1': 1, ...}\",\n-            FutureWarning,\n-        )\n-        self.device_map = (\n-            get_device_map(len(self.transformer.h), range(torch.cuda.device_count()))\n-            if device_map is None\n-            else device_map\n-        )\n-        assert_device_map(self.device_map, len(self.transformer.h))\n-        self.transformer.parallelize(self.device_map)\n-        self.lm_head = self.lm_head.to(self.transformer.first_device)\n-        self.model_parallel = True\n-\n-    @add_start_docstrings(DEPARALLELIZE_DOCSTRING)\n-    def deparallelize(self):\n-        warnings.warn(\n-            \"Like `parallelize`, `deparallelize` is deprecated and will be removed in v5 of Transformers.\",\n-            FutureWarning,\n-        )\n-        self.transformer.deparallelize()\n-        self.transformer = self.transformer.to(\"cpu\")\n-        self.lm_head = self.lm_head.to(\"cpu\")\n-        self.model_parallel = False\n-        torch.cuda.empty_cache()\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -1023,11 +874,6 @@ def forward(\n         )\n         hidden_states = transformer_outputs[0]\n \n-        # Set device for model parallelism\n-        if self.model_parallel:\n-            torch.cuda.set_device(self.transformer.first_device)\n-            hidden_states = hidden_states.to(self.lm_head.weight.device)\n-\n         slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n         logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n@@ -1073,46 +919,9 @@ def __init__(self, config):\n         self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n         self.multiple_choice_head = GPT2SequenceSummary(config)\n \n-        # Model parallel\n-        self.model_parallel = False\n-        self.device_map = None\n-\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings(PARALLELIZE_DOCSTRING)\n-    def parallelize(self, device_map=None):\n-        warnings.warn(\n-            \"`GPT2DoubleHeadsModel.parallelize` is deprecated and will be removed in v5 of Transformers, you should\"\n-            \" load your model with `device_map='balanced'` in the call to `from_pretrained`. You can also provide your\"\n-            \" own `device_map` but it needs to be a dictionary module_name to device, so for instance\"\n-            \" {'transformer.h.0': 0, 'transformer.h.1': 1, ...}\",\n-            FutureWarning,\n-        )\n-        self.device_map = (\n-            get_device_map(len(self.transformer.h), range(torch.cuda.device_count()))\n-            if device_map is None\n-            else device_map\n-        )\n-        assert_device_map(self.device_map, len(self.transformer.h))\n-        self.transformer.parallelize(self.device_map)\n-        self.lm_head = self.lm_head.to(self.transformer.first_device)\n-        self.multiple_choice_head = self.multiple_choice_head.to(self.transformer.first_device)\n-        self.model_parallel = True\n-\n-    @add_start_docstrings(DEPARALLELIZE_DOCSTRING)\n-    def deparallelize(self):\n-        warnings.warn(\n-            \"Like `parallelize`, `deparallelize` is deprecated and will be removed in v5 of Transformers.\",\n-            FutureWarning,\n-        )\n-        self.transformer.deparallelize()\n-        self.transformer = self.transformer.to(\"cpu\")\n-        self.lm_head = self.lm_head.to(\"cpu\")\n-        self.multiple_choice_head = self.multiple_choice_head.to(\"cpu\")\n-        self.model_parallel = False\n-        torch.cuda.empty_cache()\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -1201,11 +1010,6 @@ def forward(\n \n         hidden_states = transformer_outputs[0]\n \n-        # Set device for model parallelism\n-        if self.model_parallel:\n-            torch.cuda.set_device(self.transformer.first_device)\n-            hidden_states = hidden_states.to(self.lm_head.weight.device)\n-\n         lm_logits = self.lm_head(hidden_states)\n         mc_logits = self.multiple_choice_head(hidden_states, mc_token_ids).squeeze(-1)\n \n@@ -1259,10 +1063,6 @@ def __init__(self, config):\n         self.transformer = GPT2Model(config)\n         self.score = nn.Linear(config.n_embd, self.num_labels, bias=False)\n \n-        # Model parallel\n-        self.model_parallel = False\n-        self.device_map = None\n-\n         # Initialize weights and apply final processing\n         self.post_init()\n \n@@ -1392,10 +1192,6 @@ def __init__(self, config):\n         self.dropout = nn.Dropout(classifier_dropout)\n         self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n \n-        # Model parallel\n-        self.model_parallel = False\n-        self.device_map = None\n-\n         # Initialize weights and apply final processing\n         self.post_init()\n \n@@ -1479,10 +1275,6 @@ def __init__(self, config):\n         self.transformer = GPT2Model(config)\n         self.qa_outputs = nn.Linear(config.hidden_size, 2)\n \n-        # Model parallel\n-        self.model_parallel = False\n-        self.device_map = None\n-\n         # Initialize weights and apply final processing\n         self.post_init()\n "
        },
        {
            "sha": "2c08544f9ee95c237e4799b71b595cabadc4ca61",
            "filename": "src/transformers/models/gpt_neo/modeling_gpt_neo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py?ref=ad74fba08521b34f36cd72f6254b9b19a4fdb04b",
            "patch": "@@ -757,7 +757,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # move labels to correct device to enable model parallelism\n+            # move labels to correct device\n             labels = labels.to(lm_logits.device)\n             lm_logits = lm_logits.to(torch.float32)\n "
        },
        {
            "sha": "50f54a4cb64e317ce426912499b48b8341a21198",
            "filename": "src/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py?ref=ad74fba08521b34f36cd72f6254b9b19a4fdb04b",
            "patch": "@@ -724,7 +724,7 @@ def forward(\n \n         lm_loss = None\n         if labels is not None:\n-            # move labels to correct device to enable model parallelism\n+            # move labels to correct device\n             labels = labels.to(lm_logits.device)\n \n             lm_loss = self.loss_function("
        },
        {
            "sha": "b2f0d3793d4f41535dcc8480d23584762e21543a",
            "filename": "src/transformers/models/gptj/modeling_gptj.py",
            "status": "modified",
            "additions": 1,
            "deletions": 168,
            "changes": 169,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py?ref=ad74fba08521b34f36cd72f6254b9b19a4fdb04b",
            "patch": "@@ -14,7 +14,6 @@\n # limitations under the License.\n \"\"\"PyTorch GPT-J model.\"\"\"\n \n-import warnings\n from typing import Optional, Union\n \n import torch\n@@ -36,13 +35,11 @@\n )\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n-    add_start_docstrings,\n     auto_docstring,\n     is_torch_flex_attn_available,\n     is_torch_fx_proxy,\n     logging,\n )\n-from ...utils.model_parallel_utils import assert_device_map, get_device_map\n from .configuration_gptj import GPTJConfig\n \n \n@@ -466,7 +463,6 @@ def forward(\n class GPTJPreTrainedModel(PreTrainedModel):\n     config: GPTJConfig\n     base_model_prefix = \"transformer\"\n-    is_parallelizable = True\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"GPTJBlock\"]\n     _skip_keys_device_placement = \"past_key_values\"\n@@ -492,55 +488,6 @@ def _init_weights(self, module):\n             module.weight.data.fill_(1.0)\n \n \n-PARALLELIZE_DOCSTRING = r\"\"\"\n-    This is an experimental feature and is a subject to change at a moment's notice. Uses a device map to distribute\n-    attention modules of the model across several devices. If no device map is given, it will evenly distribute blocks\n-    across all devices.\n-\n-    Args:\n-        device_map (`dict[int, list]`, *optional*):\n-            A dictionary that maps attention modules to devices. Note that the embedding module and LMHead are always\n-            automatically mapped to the first device (for esoteric reasons). That means that the first device should\n-            have fewer attention modules mapped to it than other devices. For reference, the GPT-J models have the\n-            following number of attention modules:\n-\n-                - gpt-j-6B: 28\n-\n-    Example:\n-\n-    ```python\n-    # Here is an example of a device map on a machine with 4 GPUs using gpt-j-6B, which has a total of 28 attention modules:\n-    model = GPTJForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\")\n-    device_map = {\n-        0: [0, 1, 2, 3, 4, 5, 6],\n-        1: [7, 8, 9, 10, 11, 12, 13],\n-        2: [14, 15, 16, 17, 18, 19, 20],\n-        3: [21, 22, 23, 24, 25, 26, 27],\n-    }\n-    model.parallelize(device_map)\n-    ```\n-\"\"\"\n-\n-DEPARALLELIZE_DOCSTRING = r\"\"\"\n-    Moves the model to CPU from a model parallel state.\n-\n-    Example:\n-\n-    ```python\n-    # On a 4 GPU machine with gpt-j-6B:\n-    model = GPTJForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\")\n-    device_map = {\n-        0: [0, 1, 2, 3, 4, 5, 6],\n-        1: [7, 8, 9, 10, 11, 12, 13],\n-        2: [14, 15, 16, 17, 18, 19, 20],\n-        3: [21, 22, 23, 24, 25, 26, 27],\n-    }\n-    model.parallelize(device_map)  # Splits the model across several devices\n-    model.deparallelize()  # Put the model back on cpu and cleans memory by calling torch.cuda.empty_cache()\n-    ```\n-\"\"\"\n-\n-\n @auto_docstring\n class GPTJModel(GPTJPreTrainedModel):\n     def __init__(self, config):\n@@ -553,56 +500,11 @@ def __init__(self, config):\n         self.h = nn.ModuleList([GPTJBlock(config, layer_idx=i) for i in range(config.n_layer)])\n         self.ln_f = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_epsilon)\n \n-        # Model parallel\n-        self.model_parallel = False\n-        self.device_map = None\n         self.gradient_checkpointing = False\n \n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings(PARALLELIZE_DOCSTRING)\n-    def parallelize(self, device_map=None):\n-        warnings.warn(\n-            \"`GPTJModel.parallelize` is deprecated and will be removed in v5 of Transformers, you should load your\"\n-            \" model with `device_map='balanced'` in the call to `from_pretrained`. You can also provide your own\"\n-            \" `device_map` but it needs to be a dictionary module_name to device, so for instance {'h.0': 0, 'h.1': 1,\"\n-            \" ...}\",\n-            FutureWarning,\n-        )\n-        # Check validity of device_map\n-        self.device_map = (\n-            get_device_map(len(self.h), range(torch.cuda.device_count())) if device_map is None else device_map\n-        )\n-        assert_device_map(self.device_map, len(self.h))\n-        self.model_parallel = True\n-        self.first_device = \"cpu\" if \"cpu\" in self.device_map else \"cuda:\" + str(min(self.device_map.keys()))\n-        self.last_device = \"cuda:\" + str(max(self.device_map.keys()))\n-        self.wte = self.wte.to(self.first_device)\n-        # Load onto devices\n-        for k, v in self.device_map.items():\n-            for block in v:\n-                cuda_device = \"cuda:\" + str(k)\n-                self.h[block] = self.h[block].to(cuda_device)\n-        # ln_f to last\n-        self.ln_f = self.ln_f.to(self.last_device)\n-\n-    @add_start_docstrings(DEPARALLELIZE_DOCSTRING)\n-    def deparallelize(self):\n-        warnings.warn(\n-            \"Like `parallelize`, `deparallelize` is deprecated and will be removed in v5 of Transformers.\",\n-            FutureWarning,\n-        )\n-        self.model_parallel = False\n-        self.device_map = None\n-        self.first_device = \"cpu\"\n-        self.last_device = \"cpu\"\n-        self.wte = self.wte.to(\"cpu\")\n-        for index in range(len(self.h)):\n-            self.h[index] = self.h[index].to(\"cpu\")\n-        self.ln_f = self.ln_f.to(\"cpu\")\n-        torch.cuda.empty_cache()\n-\n     def get_input_embeddings(self):\n         return self.wte\n \n@@ -686,21 +588,6 @@ def forward(\n         all_self_attentions = () if output_attentions else None\n         all_hidden_states = () if output_hidden_states else None\n         for i, block in enumerate(self.h):\n-            # Model parallel\n-            if self.model_parallel:\n-                torch.cuda.set_device(hidden_states.device)\n-\n-                # Ensure layer_past is on same device as hidden_states (might not be correct)\n-                if past_key_values is not None:\n-                    for layer in past_key_values.layers:\n-                        layer.keys = layer.keys.to(hidden_states.device)\n-                        layer.values = layer.values.to(hidden_states.device)\n-\n-                # Ensure that attention_mask is always on the same device as hidden_states\n-                if causal_mask is not None:\n-                    causal_mask = causal_mask.to(hidden_states.device)\n-                if isinstance(head_mask, torch.Tensor):\n-                    head_mask = head_mask.to(hidden_states.device)\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n@@ -719,12 +606,6 @@ def forward(\n             if output_attentions:\n                 all_self_attentions = all_self_attentions + (outputs[1],)\n \n-            # Model Parallel: If it's the last layer for that device, put things on the next device\n-            if self.model_parallel:\n-                for k, v in self.device_map.items():\n-                    if i == v[-1] and \"cuda:\" + str(k) != self.last_device:\n-                        hidden_states = hidden_states.to(\"cuda:\" + str(k + 1))\n-\n         hidden_states = self.ln_f(hidden_states)\n \n         hidden_states = hidden_states.view(output_shape)\n@@ -881,44 +762,9 @@ def __init__(self, config):\n         self.transformer = GPTJModel(config)\n         self.lm_head = nn.Linear(config.n_embd, config.vocab_size)\n \n-        # Model parallel\n-        self.model_parallel = False\n-        self.device_map = None\n-\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings(PARALLELIZE_DOCSTRING)\n-    def parallelize(self, device_map=None):\n-        warnings.warn(\n-            \"`GPTJForCausalLM.parallelize` is deprecated and will be removed in v5 of Transformers, you should load\"\n-            \" your model with `device_map='balanced'` in the call to `from_pretrained`. You can also provide your own\"\n-            \" `device_map` but it needs to be a dictionary module_name to device, so for instance {'transformer.h.0':\"\n-            \" 0, 'transformer.h.1': 1, ...}\",\n-            FutureWarning,\n-        )\n-        self.device_map = (\n-            get_device_map(len(self.transformer.h), range(torch.cuda.device_count()))\n-            if device_map is None\n-            else device_map\n-        )\n-        assert_device_map(self.device_map, len(self.transformer.h))\n-        self.transformer.parallelize(self.device_map)\n-        self.lm_head = self.lm_head.to(self.transformer.first_device)\n-        self.model_parallel = True\n-\n-    @add_start_docstrings(DEPARALLELIZE_DOCSTRING)\n-    def deparallelize(self):\n-        warnings.warn(\n-            \"Like `parallelize`, `deparallelize` is deprecated and will be removed in v5 of Transformers.\",\n-            FutureWarning,\n-        )\n-        self.transformer.deparallelize()\n-        self.transformer = self.transformer.to(\"cpu\")\n-        self.lm_head = self.lm_head.to(\"cpu\")\n-        self.model_parallel = False\n-        torch.cuda.empty_cache()\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -965,19 +811,14 @@ def forward(\n         )\n         hidden_states = transformer_outputs[0]\n \n-        # Set device for model parallelism\n-        if self.model_parallel:\n-            torch.cuda.set_device(self.transformer.first_device)\n-            hidden_states = hidden_states.to(self.lm_head.weight.device)\n-\n         # make sure sampling in fp16 works correctly and\n         # compute loss in fp32 to match with mesh-tf version\n         # https://github.com/EleutherAI/gpt-neo/blob/89ce74164da2fb16179106f54e2269b5da8db333/models/gpt2/gpt2.py#L179\n         lm_logits = self.lm_head(hidden_states).to(torch.float32)\n \n         loss = None\n         if labels is not None:\n-            # move labels to correct device to enable model parallelism\n+            # move labels to correct device\n             labels = labels.to(lm_logits.device)\n             # Flatten the tokens\n             loss = self.loss_function(\n@@ -1023,10 +864,6 @@ def __init__(self, config):\n         self.transformer = GPTJModel(config)\n         self.score = nn.Linear(config.n_embd, self.num_labels, bias=False)\n \n-        # Model parallel\n-        self.model_parallel = False\n-        self.device_map = None\n-\n         # Initialize weights and apply final processing\n         self.post_init()\n \n@@ -1141,10 +978,6 @@ def __init__(self, config):\n         self.transformer = GPTJModel(config)\n         self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)\n \n-        # Model parallel\n-        self.model_parallel = False\n-        self.device_map = None\n-\n         # Initialize weights and apply final processing\n         self.post_init()\n "
        },
        {
            "sha": "35b2c8860e062a85b522aaa8fac3b870932cda0f",
            "filename": "src/transformers/models/imagegpt/modeling_imagegpt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 20,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py?ref=ad74fba08521b34f36cd72f6254b9b19a4fdb04b",
            "patch": "@@ -436,9 +436,6 @@ def __init__(self, config: ImageGPTConfig):\n         self.h = nn.ModuleList([ImageGPTBlock(config, layer_idx=i) for i in range(config.num_hidden_layers)])\n         self.ln_f = ImageGPTLayerNorm(self.embed_dim, eps=config.layer_norm_epsilon)\n \n-        # Model parallel\n-        self.model_parallel = False\n-        self.device_map = None\n         self.gradient_checkpointing = False\n         # Initialize weights and apply final processing\n         self.post_init()\n@@ -605,14 +602,6 @@ def forward(\n         all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n         all_hidden_states = () if output_hidden_states else None\n         for i, block in enumerate(self.h):\n-            # Model parallel\n-            if self.model_parallel:\n-                torch.cuda.set_device(hidden_states.device)\n-                # Ensure that attention_mask is always on the same device as hidden_states\n-                if attention_mask is not None:\n-                    attention_mask = attention_mask.to(hidden_states.device)\n-                if isinstance(head_mask, torch.Tensor):\n-                    head_mask = head_mask.to(hidden_states.device)\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n@@ -634,12 +623,6 @@ def forward(\n                 if self.config.add_cross_attention:\n                     all_cross_attentions = all_cross_attentions + (outputs[2],)\n \n-            # Model Parallel: If it's the last layer for that device, put things on the next device\n-            if self.model_parallel:\n-                for k, v in self.device_map.items():\n-                    if i == v[-1] and \"cuda:\" + str(k) != self.last_device:\n-                        hidden_states = hidden_states.to(\"cuda:\" + str(k + 1))\n-\n         hidden_states = self.ln_f(hidden_states)\n         hidden_states = hidden_states.view(*output_shape)\n \n@@ -677,9 +660,6 @@ def __init__(self, config: ImageGPTConfig):\n         self.transformer = ImageGPTModel(config)\n         self.lm_head = nn.Linear(config.n_embd, config.vocab_size - 1, bias=False)\n \n-        # Model parallel\n-        self.model_parallel = False\n-        self.device_map = None\n         # Initialize weights and apply final processing\n         self.post_init()\n "
        },
        {
            "sha": "4299e8ce768524970c887a78720854e09c34c799",
            "filename": "src/transformers/models/kosmos2_5/modeling_kosmos2_5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fmodeling_kosmos2_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fmodeling_kosmos2_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fmodeling_kosmos2_5.py?ref=ad74fba08521b34f36cd72f6254b9b19a4fdb04b",
            "patch": "@@ -1582,7 +1582,7 @@ def forward(\n \n         lm_loss = None\n         if labels is not None:\n-            # move labels to correct device to enable model parallelism\n+            # move labels to correct device\n             labels = labels.to(lm_logits.device)\n             lm_loss = self.loss_function(\n                 lm_logits,"
        },
        {
            "sha": "5c4734d0a9bdfe64b61067de27961ad5a5b4ed65",
            "filename": "src/transformers/models/lilt/modeling_lilt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Fmodels%2Flilt%2Fmodeling_lilt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Fmodels%2Flilt%2Fmodeling_lilt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flilt%2Fmodeling_lilt.py?ref=ad74fba08521b34f36cd72f6254b9b19a4fdb04b",
            "patch": "@@ -808,7 +808,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # move labels to correct device to enable model parallelism\n+            # move labels to correct device\n             labels = labels.to(logits.device)\n             if self.config.problem_type is None:\n                 if self.num_labels == 1:\n@@ -925,7 +925,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # move labels to correct device to enable model parallelism\n+            # move labels to correct device\n             labels = labels.to(logits.device)\n             loss_fct = CrossEntropyLoss()\n             loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))"
        },
        {
            "sha": "9c50efd047593fff254fd4f183930e7457a62abe",
            "filename": "src/transformers/models/luke/modeling_luke.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Fmodels%2Fluke%2Fmodeling_luke.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Fmodels%2Fluke%2Fmodeling_luke.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fluke%2Fmodeling_luke.py?ref=ad74fba08521b34f36cd72f6254b9b19a4fdb04b",
            "patch": "@@ -1177,7 +1177,7 @@ def forward(\n         mlm_loss = None\n         logits = self.lm_head(outputs.last_hidden_state)\n         if labels is not None:\n-            # move labels to correct device to enable model parallelism\n+            # move labels to correct device\n             labels = labels.to(logits.device)\n             mlm_loss = self.loss_fn(logits.view(-1, self.config.vocab_size), labels.view(-1))\n             if loss is None:\n@@ -1329,7 +1329,7 @@ def forward(\n         if labels is not None:\n             # When the number of dimension of `labels` is 1, cross entropy is used as the loss function. The binary\n             # cross entropy is used otherwise.\n-            # move labels to correct device to enable model parallelism\n+            # move labels to correct device\n             labels = labels.to(logits.device)\n             if labels.ndim == 1:\n                 loss = nn.functional.cross_entropy(logits, labels)\n@@ -1464,7 +1464,7 @@ def forward(\n         if labels is not None:\n             # When the number of dimension of `labels` is 1, cross entropy is used as the loss function. The binary\n             # cross entropy is used otherwise.\n-            # move labels to correct device to enable model parallelism\n+            # move labels to correct device\n             labels = labels.to(logits.device)\n             if labels.ndim == 1:\n                 loss = nn.functional.cross_entropy(logits, labels)\n@@ -1621,7 +1621,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # move labels to correct device to enable model parallelism\n+            # move labels to correct device\n             labels = labels.to(logits.device)\n             # When the number of dimension of `labels` is 2, cross entropy is used as the loss function. The binary\n             # cross entropy is used otherwise.\n@@ -1733,7 +1733,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # move labels to correct device to enable model parallelism\n+            # move labels to correct device\n             labels = labels.to(logits.device)\n             if self.config.problem_type is None:\n                 if self.num_labels == 1:\n@@ -1861,7 +1861,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # move labels to correct device to enable model parallelism\n+            # move labels to correct device\n             labels = labels.to(logits.device)\n             loss_fct = CrossEntropyLoss()\n             loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n@@ -2136,7 +2136,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # move labels to correct device to enable model parallelism\n+            # move labels to correct device\n             labels = labels.to(reshaped_logits.device)\n             loss_fct = CrossEntropyLoss()\n             loss = loss_fct(reshaped_logits, labels)"
        },
        {
            "sha": "ba1c1f44d3f7a229e264400bb3a3b8094e1bcea1",
            "filename": "src/transformers/models/mamba/modeling_mamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py?ref=ad74fba08521b34f36cd72f6254b9b19a4fdb04b",
            "patch": "@@ -853,7 +853,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # move labels to correct device to enable model parallelism\n+            # move labels to correct device\n             labels = labels.to(logits.device)\n             # Shift so that tokens < n predict n\n             shift_logits = logits[..., :-1, :].contiguous()"
        },
        {
            "sha": "20b9db63950c376a7ea01635acc711ef8c900ccd",
            "filename": "src/transformers/models/modernbert_decoder/modeling_modernbert_decoder.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodeling_modernbert_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodeling_modernbert_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodeling_modernbert_decoder.py?ref=ad74fba08521b34f36cd72f6254b9b19a4fdb04b",
            "patch": "@@ -598,7 +598,6 @@ def forward(\n             loss_fct = CrossEntropyLoss()\n             shift_logits = shift_logits.view(-1, self.config.vocab_size)\n             shift_labels = shift_labels.view(-1)\n-            # Enable model parallelism\n             shift_labels = shift_labels.to(shift_logits.device)\n             loss = loss_fct(shift_logits, shift_labels)\n "
        },
        {
            "sha": "0abe98b5a8fd972e9c046050d34b381b65070116",
            "filename": "src/transformers/models/modernbert_decoder/modular_modernbert_decoder.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodular_modernbert_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodular_modernbert_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodular_modernbert_decoder.py?ref=ad74fba08521b34f36cd72f6254b9b19a4fdb04b",
            "patch": "@@ -669,7 +669,6 @@ def forward(\n             loss_fct = CrossEntropyLoss()\n             shift_logits = shift_logits.view(-1, self.config.vocab_size)\n             shift_labels = shift_labels.view(-1)\n-            # Enable model parallelism\n             shift_labels = shift_labels.to(shift_logits.device)\n             loss = loss_fct(shift_logits, shift_labels)\n "
        },
        {
            "sha": "14126f7a64de7d02abf11802fc1260bdfd98c2d9",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=ad74fba08521b34f36cd72f6254b9b19a4fdb04b",
            "patch": "@@ -1022,7 +1022,6 @@ def forward(\n             loss_fct = CrossEntropyLoss()\n \n             labels = labels.masked_fill(labels == self.config.audio_vocab_size, -100).reshape(-1)\n-            # Enable model parallelism\n             labels = labels.to(logits.device)\n             loss = loss_fct(logits.reshape(-1, self.config.audio_vocab_size), labels)\n \n@@ -1550,7 +1549,6 @@ def forward(\n             # Flatten the tokens\n             shift_logits = shift_logits.view(-1, self.config.vocab_size)\n             shift_labels = shift_labels.view(-1)\n-            # Enable model parallelism\n             shift_labels = shift_labels.to(shift_logits.device)\n             loss = self.loss_function(\n                 shift_logits,"
        },
        {
            "sha": "3bcbde5027d2b5d0390120b2e41198690c37a939",
            "filename": "src/transformers/models/mpt/modeling_mpt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Fmodels%2Fmpt%2Fmodeling_mpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Fmodels%2Fmpt%2Fmodeling_mpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmpt%2Fmodeling_mpt.py?ref=ad74fba08521b34f36cd72f6254b9b19a4fdb04b",
            "patch": "@@ -489,7 +489,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # move labels to correct device to enable model parallelism\n+            # move labels to correct device\n             labels = labels.to(lm_logits.device)\n             # Flatten the tokens\n             loss = self.loss_function(\n@@ -709,7 +709,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # move labels to correct device to enable model parallelism\n+            # move labels to correct device\n             labels = labels.to(logits.device)\n             batch_size, seq_length = labels.shape\n             loss_fct = CrossEntropyLoss()"
        },
        {
            "sha": "a2439f05d585e548892a9ddc454453f9ee5c4732",
            "filename": "src/transformers/models/mt5/modeling_mt5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 287,
            "changes": 287,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py?ref=ad74fba08521b34f36cd72f6254b9b19a4fdb04b",
            "patch": "@@ -42,15 +42,13 @@\n from ...utils import (\n     DUMMY_INPUTS,\n     DUMMY_MASK,\n-    add_start_docstrings,\n     auto_docstring,\n     is_torch_flex_attn_available,\n     is_torch_fx_proxy,\n     is_torchdynamo_compiling,\n     logging,\n )\n from ...utils.deprecation import deprecate_kwarg\n-from ...utils.model_parallel_utils import assert_device_map, get_device_map\n from .configuration_mt5 import MT5Config\n \n \n@@ -62,64 +60,6 @@\n logger = logging.get_logger(__name__)\n \n \n-####################################################\n-# This dict contains ids and associated url\n-# for the pretrained weights provided with the models\n-####################################################\n-\n-PARALLELIZE_DOCSTRING = r\"\"\"\n-    This is an experimental feature and is a subject to change at a moment's notice.\n-\n-    Uses a device map to distribute attention modules of the model across several devices. If no device map is given,\n-    it will evenly distribute blocks across all devices.\n-\n-    Args:\n-        device_map (`dict[int, list]`, *optional*):\n-            A dictionary that maps attention modules to devices. Note that the embedding module and LMHead are always\n-            automatically mapped to the first device (for esoteric reasons). That means that the first device should\n-            have fewer attention modules mapped to it than other devices. For reference, the mt5 models have the\n-            following number of attention modules:\n-\n-                - mt5-small: 6\n-                - mt5-base: 12\n-                - mt5-large: 24\n-                - mt5-xl: 24\n-                - mt5-xxl: 24\n-\n-    Example:\n-\n-    ```python\n-    # Here is an example of a device map on a machine with 4 GPUs using mt5-xl, which has a total of 24 attention modules:\n-    model = MT5ForConditionalGeneration.from_pretrained(\"mt5-xl\")\n-    device_map = {\n-        0: [0, 1, 2],\n-        1: [3, 4, 5, 6, 7, 8, 9],\n-        2: [10, 11, 12, 13, 14, 15, 16],\n-        3: [17, 18, 19, 20, 21, 22, 23],\n-    }\n-    model.parallelize(device_map)\n-    ```\n-\"\"\"\n-DEPARALLELIZE_DOCSTRING = r\"\"\"\n-    Moves the model to cpu from a model parallel state.\n-\n-    Example:\n-\n-    ```python\n-    # On a 4 GPU machine with mt5-xl:\n-    model = MT5ForConditionalGeneration.from_pretrained(\"Mt5-xl\")\n-    device_map = {\n-        0: [0, 1, 2],\n-        1: [3, 4, 5, 6, 7, 8, 9],\n-        2: [10, 11, 12, 13, 14, 15, 16],\n-        3: [17, 18, 19, 20, 21, 22, 23],\n-    }\n-    model.parallelize(device_map)  # Splits the model across several devices\n-    model.deparallelize()  # Put the model back on cpu and cleans memory by calling torch.cuda.empty_cache()\n-    ```\n-\"\"\"\n-\n-\n # Copied from transformers.models.t5.modeling_t5.T5LayerNorm with T5->MT5\n class MT5LayerNorm(nn.Module):\n     def __init__(self, hidden_size, eps=1e-6):\n@@ -652,7 +592,6 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n class MT5PreTrainedModel(PreTrainedModel):\n     config: MT5Config\n     base_model_prefix = \"transformer\"\n-    is_parallelizable = True\n     supports_gradient_checkpointing = True\n     _can_compile_fullgraph = True\n \n@@ -768,55 +707,8 @@ def __init__(self, config, embed_tokens=None):\n \n         # Initialize weights and apply final processing\n         self.post_init()\n-        # Model parallel\n-        self.model_parallel = False\n-        self.device_map = None\n         self.gradient_checkpointing = False\n \n-    @add_start_docstrings(PARALLELIZE_DOCSTRING)\n-    def parallelize(self, device_map=None):\n-        warnings.warn(\n-            \"`MT5Stack.parallelize` is deprecated and will be removed in v5 of Transformers, you should load your model\"\n-            \" with `device_map='balanced'` in the call to `from_pretrained`. You can also provide your own\"\n-            \" `device_map` but it needs to be a dictionary module_name to device, so for instance {'block.0': 0,\"\n-            \" 'block.1': 1, ...}\",\n-            FutureWarning,\n-        )\n-        # Check validity of device_map\n-        self.device_map = (\n-            get_device_map(len(self.block), range(torch.cuda.device_count())) if device_map is None else device_map\n-        )\n-        assert_device_map(self.device_map, len(self.block))\n-        self.model_parallel = True\n-        self.first_device = \"cpu\" if \"cpu\" in self.device_map else \"cuda:\" + str(min(self.device_map.keys()))\n-        self.last_device = \"cuda:\" + str(max(self.device_map.keys()))\n-        # Load onto devices\n-        for k, v in self.device_map.items():\n-            for layer in v:\n-                cuda_device = \"cuda:\" + str(k)\n-                self.block[layer] = self.block[layer].to(cuda_device)\n-\n-        # Set embed_tokens to first layer\n-        self.embed_tokens = self.embed_tokens.to(self.first_device)\n-        # Set final layer norm to last device\n-        self.final_layer_norm = self.final_layer_norm.to(self.last_device)\n-\n-    @add_start_docstrings(DEPARALLELIZE_DOCSTRING)\n-    def deparallelize(self):\n-        warnings.warn(\n-            \"Like `parallelize`, `deparallelize` is deprecated and will be removed in v5 of Transformers.\",\n-            FutureWarning,\n-        )\n-        self.model_parallel = False\n-        self.device_map = None\n-        self.first_device = \"cpu\"\n-        self.last_device = \"cpu\"\n-        for i in range(len(self.block)):\n-            self.block[i] = self.block[i].to(\"cpu\")\n-        self.embed_tokens = self.embed_tokens.to(\"cpu\")\n-        self.final_layer_norm = self.final_layer_norm.to(\"cpu\")\n-        torch.cuda.empty_cache()\n-\n     def set_input_embeddings(self, new_embeddings):\n         self.embed_tokens = new_embeddings\n \n@@ -836,10 +728,6 @@ def forward(\n         return_dict=None,\n         cache_position=None,\n     ):\n-        # Model parallel\n-        if self.model_parallel:\n-            torch.cuda.set_device(self.first_device)\n-            self.embed_tokens = self.embed_tokens.to(self.first_device)\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -947,24 +835,6 @@ def forward(\n         for i, layer_module in enumerate(self.block):\n             layer_head_mask = head_mask[i]\n             cross_attn_layer_head_mask = cross_attn_head_mask[i]\n-            # Model parallel\n-            if self.model_parallel:\n-                torch.cuda.set_device(hidden_states.device)\n-                # Ensure that attention_mask is always on the same device as hidden_states\n-                if causal_mask is not None:\n-                    causal_mask = causal_mask.to(hidden_states.device)\n-                if position_bias is not None:\n-                    position_bias = position_bias.to(hidden_states.device)\n-                if encoder_hidden_states is not None:\n-                    encoder_hidden_states = encoder_hidden_states.to(hidden_states.device)\n-                if encoder_extended_attention_mask is not None:\n-                    encoder_extended_attention_mask = encoder_extended_attention_mask.to(hidden_states.device)\n-                if encoder_decoder_position_bias is not None:\n-                    encoder_decoder_position_bias = encoder_decoder_position_bias.to(hidden_states.device)\n-                if layer_head_mask is not None:\n-                    layer_head_mask = layer_head_mask.to(hidden_states.device)\n-                if cross_attn_layer_head_mask is not None:\n-                    cross_attn_layer_head_mask = cross_attn_layer_head_mask.to(hidden_states.device)\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n@@ -998,12 +868,6 @@ def forward(\n                 if self.is_decoder:\n                     all_cross_attentions = all_cross_attentions + (layer_outputs[4],)\n \n-            # Model Parallel: If it's the last layer for that device, put things on the next device\n-            if self.model_parallel:\n-                for k, v in self.device_map.items():\n-                    if i == v[-1] and \"cuda:\" + str(k) != self.last_device:\n-                        hidden_states = hidden_states.to(\"cuda:\" + str(k + 1))\n-\n         hidden_states = self.final_layer_norm(hidden_states)\n         hidden_states = self.dropout(hidden_states)\n \n@@ -1210,45 +1074,6 @@ def __init__(self, config: MT5Config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-        # Model parallel\n-        self.model_parallel = False\n-        self.device_map = None\n-\n-    @add_start_docstrings(PARALLELIZE_DOCSTRING)\n-    # Copied from transformers.models.t5.modeling_t5.T5Model.parallelize\n-    def parallelize(self, device_map=None):\n-        warnings.warn(\n-            \"`T5Model.parallelize` is deprecated and will be removed in v5 of Transformers, you should load your model\"\n-            \" with `device_map='balanced'` in the call to `from_pretrained`. You can also provide your own\"\n-            \" `device_map` but it needs to be a dictionary module_name to device, so for instance {'encoder.block.0':\"\n-            \" 0, 'encoder.block.1': 1, ...}\",\n-            FutureWarning,\n-        )\n-        self.device_map = (\n-            get_device_map(len(self.encoder.block), range(torch.cuda.device_count()))\n-            if device_map is None\n-            else device_map\n-        )\n-        assert_device_map(self.device_map, len(self.encoder.block))\n-        self.encoder.parallelize(self.device_map)\n-        self.decoder.parallelize(self.device_map)\n-        self.model_parallel = True\n-\n-    @add_start_docstrings(DEPARALLELIZE_DOCSTRING)\n-    # Copied from transformers.models.t5.modeling_t5.T5Model.deparallelize\n-    def deparallelize(self):\n-        warnings.warn(\n-            \"Like `parallelize`, `deparallelize` is deprecated and will be removed in v5 of Transformers.\",\n-            FutureWarning,\n-        )\n-        self.encoder.deparallelize()\n-        self.decoder.deparallelize()\n-        self.encoder = self.encoder.to(\"cpu\")\n-        self.decoder = self.decoder.to(\"cpu\")\n-        self.model_parallel = False\n-        self.device_map = None\n-        torch.cuda.empty_cache()\n-\n     # Copied from transformers.models.t5.modeling_t5.T5Model.get_input_embeddings\n     def get_input_embeddings(self):\n         return self.shared\n@@ -1383,17 +1208,6 @@ def forward(\n \n         hidden_states = encoder_outputs[0]\n \n-        # Set device for model parallelism\n-        if self.model_parallel:\n-            torch.cuda.set_device(self.decoder.first_device)\n-            hidden_states = hidden_states.to(self.decoder.first_device)\n-            if decoder_input_ids is not None:\n-                decoder_input_ids = decoder_input_ids.to(self.decoder.first_device)\n-            if attention_mask is not None:\n-                attention_mask = attention_mask.to(self.decoder.first_device)\n-            if decoder_attention_mask is not None:\n-                decoder_attention_mask = decoder_attention_mask.to(self.decoder.first_device)\n-\n         # Decode\n         decoder_outputs = self.decoder(\n             input_ids=decoder_input_ids,\n@@ -1477,47 +1291,6 @@ def __init__(self, config: MT5Config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-        # Model parallel\n-        self.model_parallel = False\n-        self.device_map = None\n-\n-    @add_start_docstrings(PARALLELIZE_DOCSTRING)\n-    # Copied from transformers.models.t5.modeling_t5.T5ForConditionalGeneration.parallelize\n-    def parallelize(self, device_map=None):\n-        warnings.warn(\n-            \"`T5ForConditionalGeneration.parallelize` is deprecated and will be removed in v5 of Transformers, you\"\n-            \" should load your model with `device_map='balanced'` in the call to `from_pretrained`. You can also\"\n-            \" provide your own `device_map` but it needs to be a dictionary module_name to device, so for instance\"\n-            \" {'encoder.block.0': 0, 'encoder.block.1': 1, ...}\",\n-            FutureWarning,\n-        )\n-        self.device_map = (\n-            get_device_map(len(self.encoder.block), range(torch.cuda.device_count()))\n-            if device_map is None\n-            else device_map\n-        )\n-        assert_device_map(self.device_map, len(self.encoder.block))\n-        self.encoder.parallelize(self.device_map)\n-        self.decoder.parallelize(self.device_map)\n-        self.lm_head = self.lm_head.to(self.decoder.first_device)\n-        self.model_parallel = True\n-\n-    @add_start_docstrings(DEPARALLELIZE_DOCSTRING)\n-    # Copied from transformers.models.t5.modeling_t5.T5ForConditionalGeneration.deparallelize\n-    def deparallelize(self):\n-        warnings.warn(\n-            \"Like `parallelize`, `deparallelize` is deprecated and will be removed in v5 of Transformers.\",\n-            FutureWarning,\n-        )\n-        self.encoder.deparallelize()\n-        self.decoder.deparallelize()\n-        self.encoder = self.encoder.to(\"cpu\")\n-        self.decoder = self.decoder.to(\"cpu\")\n-        self.lm_head = self.lm_head.to(\"cpu\")\n-        self.model_parallel = False\n-        self.device_map = None\n-        torch.cuda.empty_cache()\n-\n     def get_input_embeddings(self):\n         return self.shared\n \n@@ -1649,24 +1422,10 @@ def forward(\n \n         hidden_states = encoder_outputs[0]\n \n-        if self.model_parallel:\n-            torch.cuda.set_device(self.decoder.first_device)\n-\n         if labels is not None and decoder_input_ids is None and decoder_inputs_embeds is None:\n             # get decoder inputs from shifting lm labels to the right\n             decoder_input_ids = self._shift_right(labels)\n \n-        # Set device for model parallelism\n-        if self.model_parallel:\n-            torch.cuda.set_device(self.decoder.first_device)\n-            hidden_states = hidden_states.to(self.decoder.first_device)\n-            if decoder_input_ids is not None:\n-                decoder_input_ids = decoder_input_ids.to(self.decoder.first_device)\n-            if attention_mask is not None:\n-                attention_mask = attention_mask.to(self.decoder.first_device)\n-            if decoder_attention_mask is not None:\n-                decoder_attention_mask = decoder_attention_mask.to(self.decoder.first_device)\n-\n         # Decode\n         decoder_outputs = self.decoder(\n             input_ids=decoder_input_ids,\n@@ -1686,12 +1445,6 @@ def forward(\n \n         sequence_output = decoder_outputs[0]\n \n-        # Set device for model parallelism\n-        if self.model_parallel:\n-            torch.cuda.set_device(self.encoder.first_device)\n-            self.lm_head = self.lm_head.to(self.encoder.first_device)\n-            sequence_output = sequence_output.to(self.lm_head.weight.device)\n-\n         if self.config.tie_word_embeddings:\n             sequence_output = sequence_output * (self.model_dim**-0.5)\n \n@@ -1758,42 +1511,6 @@ def __init__(self, config: MT5Config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-        # Model parallel\n-        self.model_parallel = False\n-        self.device_map = None\n-\n-    @add_start_docstrings(PARALLELIZE_DOCSTRING)\n-    # Copied from transformers.models.t5.modeling_t5.T5EncoderModel.parallelize\n-    def parallelize(self, device_map=None):\n-        warnings.warn(\n-            \"`T5EncoderModel.parallelize` is deprecated and will be removed in v5 of Transformers, you should load\"\n-            \" your model with `device_map='balanced'` in the call to `from_pretrained`. You can also provide your own\"\n-            \" `device_map` but it needs to be a dictionary module_name to device, so for instance {'block.0': 0,\"\n-            \" 'block.1': 1, ...}\",\n-            FutureWarning,\n-        )\n-        self.device_map = (\n-            get_device_map(len(self.encoder.block), range(torch.cuda.device_count()))\n-            if device_map is None\n-            else device_map\n-        )\n-        assert_device_map(self.device_map, len(self.encoder.block))\n-        self.encoder.parallelize(self.device_map)\n-        self.model_parallel = True\n-\n-    @add_start_docstrings(DEPARALLELIZE_DOCSTRING)\n-    # Copied from transformers.models.t5.modeling_t5.T5EncoderModel.deparallelize\n-    def deparallelize(self):\n-        warnings.warn(\n-            \"Like `parallelize`, `deparallelize` is deprecated and will be removed in v5 of Transformers.\",\n-            FutureWarning,\n-        )\n-        self.encoder.deparallelize()\n-        self.encoder = self.encoder.to(\"cpu\")\n-        self.model_parallel = False\n-        self.device_map = None\n-        torch.cuda.empty_cache()\n-\n     # Copied from transformers.models.t5.modeling_t5.T5EncoderModel.get_input_embeddings\n     def get_input_embeddings(self):\n         return self.shared\n@@ -1885,8 +1602,6 @@ def __init__(self, config: MT5Config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-        self.model_parallel = False\n-\n     @auto_docstring\n     # Copied from transformers.models.t5.modeling_t5.T5ForSequenceClassification.forward with T5->MT5, t5->mt5\n     def forward(\n@@ -2142,8 +1857,6 @@ def __init__(self, config: MT5Config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-        self.model_parallel = False\n-\n     # Copied from transformers.models.t5.modeling_t5.T5ForQuestionAnswering.get_input_embeddings\n     def get_input_embeddings(self):\n         return self.shared"
        },
        {
            "sha": "fdd0550270bc0349c23d54c073924cc7eea0d0f3",
            "filename": "src/transformers/models/opt/modeling_opt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py?ref=ad74fba08521b34f36cd72f6254b9b19a4fdb04b",
            "patch": "@@ -834,7 +834,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # move labels to correct device to enable model parallelism\n+            # move labels to correct device\n             labels = labels.to(logits.device)\n             loss = self.loss_function(\n                 logits,"
        },
        {
            "sha": "b00603e09c65d6fda8f9c521207efe0af13f171a",
            "filename": "src/transformers/models/pix2struct/modeling_pix2struct.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py?ref=ad74fba08521b34f36cd72f6254b9b19a4fdb04b",
            "patch": "@@ -1229,7 +1229,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # move labels to correct device to enable model parallelism\n+            # move labels to correct device\n             labels = labels.to(logits.device)\n             loss_fct = nn.CrossEntropyLoss(ignore_index=-100, reduction=\"mean\")\n "
        },
        {
            "sha": "9831dfab3e0c25b7127cf98bcbd65ed0696d5e65",
            "filename": "src/transformers/models/pop2piano/modeling_pop2piano.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py?ref=ad74fba08521b34f36cd72f6254b9b19a4fdb04b",
            "patch": "@@ -579,7 +579,6 @@ def forward(\n class Pop2PianoPreTrainedModel(PreTrainedModel):\n     config: Pop2PianoConfig\n     base_model_prefix = \"transformer\"\n-    is_parallelizable = False\n     supports_gradient_checkpointing = True\n \n     _can_compile_fullgraph = False\n@@ -671,9 +670,6 @@ def __init__(self, config, embed_tokens=None):\n \n         # Initialize weights and apply final processing\n         self.post_init()\n-        # Model parallel\n-        self.model_parallel = False\n-        self.device_map = None\n         self.gradient_checkpointing = False\n \n     # Copied from transformers.models.t5.modeling_t5.T5Stack.set_input_embeddings"
        },
        {
            "sha": "5bf4396fdfc4f2e330ed04a05153f1b9311f74d8",
            "filename": "src/transformers/models/roberta/modeling_roberta.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py?ref=ad74fba08521b34f36cd72f6254b9b19a4fdb04b",
            "patch": "@@ -1009,7 +1009,7 @@ def forward(\n \n         lm_loss = None\n         if labels is not None:\n-            # move labels to correct device to enable model parallelism\n+            # move labels to correct device\n             labels = labels.to(prediction_scores.device)\n             lm_loss = self.loss_function(\n                 prediction_scores,\n@@ -1100,7 +1100,7 @@ def forward(\n \n         masked_lm_loss = None\n         if labels is not None:\n-            # move labels to correct device to enable model parallelism\n+            # move labels to correct device\n             labels = labels.to(prediction_scores.device)\n             loss_fct = CrossEntropyLoss()\n             masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n@@ -1205,7 +1205,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # move labels to correct device to enable model parallelism\n+            # move labels to correct device\n             labels = labels.to(logits.device)\n             if self.config.problem_type is None:\n                 if self.num_labels == 1:\n@@ -1322,7 +1322,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # move labels to correct device to enable model parallelism\n+            # move labels to correct device\n             labels = labels.to(reshaped_logits.device)\n             loss_fct = CrossEntropyLoss()\n             loss = loss_fct(reshaped_logits, labels)\n@@ -1395,7 +1395,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # move labels to correct device to enable model parallelism\n+            # move labels to correct device\n             labels = labels.to(logits.device)\n             loss_fct = CrossEntropyLoss()\n             loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))"
        },
        {
            "sha": "5c2b2fd6e54d26be8910ad823167eac5b49ec970",
            "filename": "src/transformers/models/roberta/modular_roberta.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Fmodels%2Froberta%2Fmodular_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Fmodels%2Froberta%2Fmodular_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta%2Fmodular_roberta.py?ref=ad74fba08521b34f36cd72f6254b9b19a4fdb04b",
            "patch": "@@ -286,7 +286,7 @@ def forward(\n \n         lm_loss = None\n         if labels is not None:\n-            # move labels to correct device to enable model parallelism\n+            # move labels to correct device\n             labels = labels.to(prediction_scores.device)\n             lm_loss = self.loss_function(\n                 prediction_scores,\n@@ -377,7 +377,7 @@ def forward(\n \n         masked_lm_loss = None\n         if labels is not None:\n-            # move labels to correct device to enable model parallelism\n+            # move labels to correct device\n             labels = labels.to(prediction_scores.device)\n             loss_fct = CrossEntropyLoss()\n             masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n@@ -482,7 +482,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # move labels to correct device to enable model parallelism\n+            # move labels to correct device\n             labels = labels.to(logits.device)\n             if self.config.problem_type is None:\n                 if self.num_labels == 1:\n@@ -599,7 +599,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # move labels to correct device to enable model parallelism\n+            # move labels to correct device\n             labels = labels.to(reshaped_logits.device)\n             loss_fct = CrossEntropyLoss()\n             loss = loss_fct(reshaped_logits, labels)\n@@ -672,7 +672,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # move labels to correct device to enable model parallelism\n+            # move labels to correct device\n             labels = labels.to(logits.device)\n             loss_fct = CrossEntropyLoss()\n             loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))"
        },
        {
            "sha": "f3383194165ded69548f6e87de9311f0dbde1594",
            "filename": "src/transformers/models/roberta_prelayernorm/modeling_roberta_prelayernorm.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py?ref=ad74fba08521b34f36cd72f6254b9b19a4fdb04b",
            "patch": "@@ -1039,7 +1039,7 @@ def forward(\n \n         lm_loss = None\n         if labels is not None:\n-            # move labels to correct device to enable model parallelism\n+            # move labels to correct device\n             labels = labels.to(prediction_scores.device)\n             lm_loss = self.loss_function(\n                 prediction_scores,\n@@ -1136,7 +1136,7 @@ def forward(\n \n         masked_lm_loss = None\n         if labels is not None:\n-            # move labels to correct device to enable model parallelism\n+            # move labels to correct device\n             labels = labels.to(prediction_scores.device)\n             loss_fct = CrossEntropyLoss()\n             masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n@@ -1243,7 +1243,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # move labels to correct device to enable model parallelism\n+            # move labels to correct device\n             labels = labels.to(logits.device)\n             if self.config.problem_type is None:\n                 if self.num_labels == 1:\n@@ -1361,7 +1361,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # move labels to correct device to enable model parallelism\n+            # move labels to correct device\n             labels = labels.to(reshaped_logits.device)\n             loss_fct = CrossEntropyLoss()\n             loss = loss_fct(reshaped_logits, labels)\n@@ -1435,7 +1435,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # move labels to correct device to enable model parallelism\n+            # move labels to correct device\n             labels = labels.to(logits.device)\n             loss_fct = CrossEntropyLoss()\n             loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))"
        },
        {
            "sha": "acb3f8ccf8266d7828abc7bbca9c861bd1b0d221",
            "filename": "src/transformers/models/t5/modeling_t5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 276,
            "changes": 276,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py?ref=ad74fba08521b34f36cd72f6254b9b19a4fdb04b",
            "patch": "@@ -42,15 +42,13 @@\n from ...utils import (\n     DUMMY_INPUTS,\n     DUMMY_MASK,\n-    add_start_docstrings,\n     auto_docstring,\n     is_torch_flex_attn_available,\n     is_torch_fx_proxy,\n     is_torchdynamo_compiling,\n     logging,\n )\n from ...utils.deprecation import deprecate_kwarg\n-from ...utils.model_parallel_utils import assert_device_map, get_device_map\n from .configuration_t5 import T5Config\n \n \n@@ -63,59 +61,6 @@\n logger = logging.get_logger(__name__)\n \n \n-PARALLELIZE_DOCSTRING = r\"\"\"\n-    This is an experimental feature and is a subject to change at a moment's notice.\n-\n-    Uses a device map to distribute attention modules of the model across several devices. If no device map is given,\n-    it will evenly distribute blocks across all devices.\n-\n-    Args:\n-        device_map (`dict[int, list]`, *optional*):\n-            A dictionary that maps attention modules to devices. Note that the embedding module and LMHead are always\n-            automatically mapped to the first device (for esoteric reasons). That means that the first device should\n-            have fewer attention modules mapped to it than other devices. For reference, the t5 models have the\n-            following number of attention modules:\n-\n-                - google-t5/t5-small: 6\n-                - google-t5/t5-base: 12\n-                - google-t5/t5-large: 24\n-                - google-t5/t5-3b: 24\n-                - google-t5/t5-11b: 24\n-\n-    Example:\n-\n-    ```python\n-    # Here is an example of a device map on a machine with 4 GPUs using google-t5/t5-3b, which has a total of 24 attention modules:\n-    model = T5ForConditionalGeneration.from_pretrained(\"google-t5/t5-3b\")\n-    device_map = {\n-        0: [0, 1, 2],\n-        1: [3, 4, 5, 6, 7, 8, 9],\n-        2: [10, 11, 12, 13, 14, 15, 16],\n-        3: [17, 18, 19, 20, 21, 22, 23],\n-    }\n-    model.parallelize(device_map)\n-    ```\n-\"\"\"\n-DEPARALLELIZE_DOCSTRING = r\"\"\"\n-    Moves the model to cpu from a model parallel state.\n-\n-    Example:\n-\n-    ```python\n-    # On a 4 GPU machine with google-t5/t5-3b:\n-    model = T5ForConditionalGeneration.from_pretrained(\"google-t5/t5-3b\")\n-    device_map = {\n-        0: [0, 1, 2],\n-        1: [3, 4, 5, 6, 7, 8, 9],\n-        2: [10, 11, 12, 13, 14, 15, 16],\n-        3: [17, 18, 19, 20, 21, 22, 23],\n-    }\n-    model.parallelize(device_map)  # Splits the model across several devices\n-    model.deparallelize()  # Put the model back on cpu and cleans memory by calling torch.cuda.empty_cache()\n-    ```\n-\"\"\"\n-\n-\n class T5LayerNorm(nn.Module):\n     def __init__(self, hidden_size, eps=1e-6):\n         \"\"\"\n@@ -652,7 +597,6 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n class T5PreTrainedModel(PreTrainedModel):\n     config: T5Config\n     base_model_prefix = \"transformer\"\n-    is_parallelizable = True\n     supports_gradient_checkpointing = True\n     _can_compile_fullgraph = True\n \n@@ -767,55 +711,8 @@ def __init__(self, config, embed_tokens=None):\n \n         # Initialize weights and apply final processing\n         self.post_init()\n-        # Model parallel\n-        self.model_parallel = False\n-        self.device_map = None\n         self.gradient_checkpointing = False\n \n-    @add_start_docstrings(PARALLELIZE_DOCSTRING)\n-    def parallelize(self, device_map=None):\n-        warnings.warn(\n-            \"`T5Stack.parallelize` is deprecated and will be removed in v5 of Transformers, you should load your model\"\n-            \" with `device_map='balanced'` in the call to `from_pretrained`. You can also provide your own\"\n-            \" `device_map` but it needs to be a dictionary module_name to device, so for instance {'block.0': 0,\"\n-            \" 'block.1': 1, ...}\",\n-            FutureWarning,\n-        )\n-        # Check validity of device_map\n-        self.device_map = (\n-            get_device_map(len(self.block), range(torch.cuda.device_count())) if device_map is None else device_map\n-        )\n-        assert_device_map(self.device_map, len(self.block))\n-        self.model_parallel = True\n-        self.first_device = \"cpu\" if \"cpu\" in self.device_map else \"cuda:\" + str(min(self.device_map.keys()))\n-        self.last_device = \"cuda:\" + str(max(self.device_map.keys()))\n-        # Load onto devices\n-        for k, v in self.device_map.items():\n-            for layer in v:\n-                cuda_device = \"cuda:\" + str(k)\n-                self.block[layer] = self.block[layer].to(cuda_device)\n-\n-        # Set embed_tokens to first layer\n-        self.embed_tokens = self.embed_tokens.to(self.first_device)\n-        # Set final layer norm to last device\n-        self.final_layer_norm = self.final_layer_norm.to(self.last_device)\n-\n-    @add_start_docstrings(DEPARALLELIZE_DOCSTRING)\n-    def deparallelize(self):\n-        warnings.warn(\n-            \"Like `parallelize`, `deparallelize` is deprecated and will be removed in v5 of Transformers.\",\n-            FutureWarning,\n-        )\n-        self.model_parallel = False\n-        self.device_map = None\n-        self.first_device = \"cpu\"\n-        self.last_device = \"cpu\"\n-        for i in range(len(self.block)):\n-            self.block[i] = self.block[i].to(\"cpu\")\n-        self.embed_tokens = self.embed_tokens.to(\"cpu\")\n-        self.final_layer_norm = self.final_layer_norm.to(\"cpu\")\n-        torch.cuda.empty_cache()\n-\n     def set_input_embeddings(self, new_embeddings):\n         self.embed_tokens = new_embeddings\n \n@@ -835,10 +732,6 @@ def forward(\n         return_dict=None,\n         cache_position=None,\n     ):\n-        # Model parallel\n-        if self.model_parallel:\n-            torch.cuda.set_device(self.first_device)\n-            self.embed_tokens = self.embed_tokens.to(self.first_device)\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -946,24 +839,6 @@ def forward(\n         for i, layer_module in enumerate(self.block):\n             layer_head_mask = head_mask[i]\n             cross_attn_layer_head_mask = cross_attn_head_mask[i]\n-            # Model parallel\n-            if self.model_parallel:\n-                torch.cuda.set_device(hidden_states.device)\n-                # Ensure that attention_mask is always on the same device as hidden_states\n-                if causal_mask is not None:\n-                    causal_mask = causal_mask.to(hidden_states.device)\n-                if position_bias is not None:\n-                    position_bias = position_bias.to(hidden_states.device)\n-                if encoder_hidden_states is not None:\n-                    encoder_hidden_states = encoder_hidden_states.to(hidden_states.device)\n-                if encoder_extended_attention_mask is not None:\n-                    encoder_extended_attention_mask = encoder_extended_attention_mask.to(hidden_states.device)\n-                if encoder_decoder_position_bias is not None:\n-                    encoder_decoder_position_bias = encoder_decoder_position_bias.to(hidden_states.device)\n-                if layer_head_mask is not None:\n-                    layer_head_mask = layer_head_mask.to(hidden_states.device)\n-                if cross_attn_layer_head_mask is not None:\n-                    cross_attn_layer_head_mask = cross_attn_layer_head_mask.to(hidden_states.device)\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n@@ -997,12 +872,6 @@ def forward(\n                 if self.is_decoder:\n                     all_cross_attentions = all_cross_attentions + (layer_outputs[4],)\n \n-            # Model Parallel: If it's the last layer for that device, put things on the next device\n-            if self.model_parallel:\n-                for k, v in self.device_map.items():\n-                    if i == v[-1] and \"cuda:\" + str(k) != self.last_device:\n-                        hidden_states = hidden_states.to(\"cuda:\" + str(k + 1))\n-\n         hidden_states = self.final_layer_norm(hidden_states)\n         hidden_states = self.dropout(hidden_states)\n \n@@ -1191,43 +1060,6 @@ def __init__(self, config: T5Config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-        # Model parallel\n-        self.model_parallel = False\n-        self.device_map = None\n-\n-    @add_start_docstrings(PARALLELIZE_DOCSTRING)\n-    def parallelize(self, device_map=None):\n-        warnings.warn(\n-            \"`T5Model.parallelize` is deprecated and will be removed in v5 of Transformers, you should load your model\"\n-            \" with `device_map='balanced'` in the call to `from_pretrained`. You can also provide your own\"\n-            \" `device_map` but it needs to be a dictionary module_name to device, so for instance {'encoder.block.0':\"\n-            \" 0, 'encoder.block.1': 1, ...}\",\n-            FutureWarning,\n-        )\n-        self.device_map = (\n-            get_device_map(len(self.encoder.block), range(torch.cuda.device_count()))\n-            if device_map is None\n-            else device_map\n-        )\n-        assert_device_map(self.device_map, len(self.encoder.block))\n-        self.encoder.parallelize(self.device_map)\n-        self.decoder.parallelize(self.device_map)\n-        self.model_parallel = True\n-\n-    @add_start_docstrings(DEPARALLELIZE_DOCSTRING)\n-    def deparallelize(self):\n-        warnings.warn(\n-            \"Like `parallelize`, `deparallelize` is deprecated and will be removed in v5 of Transformers.\",\n-            FutureWarning,\n-        )\n-        self.encoder.deparallelize()\n-        self.decoder.deparallelize()\n-        self.encoder = self.encoder.to(\"cpu\")\n-        self.decoder = self.decoder.to(\"cpu\")\n-        self.model_parallel = False\n-        self.device_map = None\n-        torch.cuda.empty_cache()\n-\n     def get_input_embeddings(self):\n         return self.shared\n \n@@ -1362,17 +1194,6 @@ def forward(\n \n         hidden_states = encoder_outputs[0]\n \n-        # Set device for model parallelism\n-        if self.model_parallel:\n-            torch.cuda.set_device(self.decoder.first_device)\n-            hidden_states = hidden_states.to(self.decoder.first_device)\n-            if decoder_input_ids is not None:\n-                decoder_input_ids = decoder_input_ids.to(self.decoder.first_device)\n-            if attention_mask is not None:\n-                attention_mask = attention_mask.to(self.decoder.first_device)\n-            if decoder_attention_mask is not None:\n-                decoder_attention_mask = decoder_attention_mask.to(self.decoder.first_device)\n-\n         # Decode\n         decoder_outputs = self.decoder(\n             input_ids=decoder_input_ids,\n@@ -1439,45 +1260,6 @@ def __init__(self, config: T5Config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-        # Model parallel\n-        self.model_parallel = False\n-        self.device_map = None\n-\n-    @add_start_docstrings(PARALLELIZE_DOCSTRING)\n-    def parallelize(self, device_map=None):\n-        warnings.warn(\n-            \"`T5ForConditionalGeneration.parallelize` is deprecated and will be removed in v5 of Transformers, you\"\n-            \" should load your model with `device_map='balanced'` in the call to `from_pretrained`. You can also\"\n-            \" provide your own `device_map` but it needs to be a dictionary module_name to device, so for instance\"\n-            \" {'encoder.block.0': 0, 'encoder.block.1': 1, ...}\",\n-            FutureWarning,\n-        )\n-        self.device_map = (\n-            get_device_map(len(self.encoder.block), range(torch.cuda.device_count()))\n-            if device_map is None\n-            else device_map\n-        )\n-        assert_device_map(self.device_map, len(self.encoder.block))\n-        self.encoder.parallelize(self.device_map)\n-        self.decoder.parallelize(self.device_map)\n-        self.lm_head = self.lm_head.to(self.decoder.first_device)\n-        self.model_parallel = True\n-\n-    @add_start_docstrings(DEPARALLELIZE_DOCSTRING)\n-    def deparallelize(self):\n-        warnings.warn(\n-            \"Like `parallelize`, `deparallelize` is deprecated and will be removed in v5 of Transformers.\",\n-            FutureWarning,\n-        )\n-        self.encoder.deparallelize()\n-        self.decoder.deparallelize()\n-        self.encoder = self.encoder.to(\"cpu\")\n-        self.decoder = self.decoder.to(\"cpu\")\n-        self.lm_head = self.lm_head.to(\"cpu\")\n-        self.model_parallel = False\n-        self.device_map = None\n-        torch.cuda.empty_cache()\n-\n     def get_input_embeddings(self):\n         return self.shared\n \n@@ -1612,24 +1394,10 @@ def forward(\n \n         hidden_states = encoder_outputs[0]\n \n-        if self.model_parallel:\n-            torch.cuda.set_device(self.decoder.first_device)\n-\n         if labels is not None and decoder_input_ids is None and decoder_inputs_embeds is None:\n             # get decoder inputs from shifting lm labels to the right\n             decoder_input_ids = self._shift_right(labels)\n \n-        # Set device for model parallelism\n-        if self.model_parallel:\n-            torch.cuda.set_device(self.decoder.first_device)\n-            hidden_states = hidden_states.to(self.decoder.first_device)\n-            if decoder_input_ids is not None:\n-                decoder_input_ids = decoder_input_ids.to(self.decoder.first_device)\n-            if attention_mask is not None:\n-                attention_mask = attention_mask.to(self.decoder.first_device)\n-            if decoder_attention_mask is not None:\n-                decoder_attention_mask = decoder_attention_mask.to(self.decoder.first_device)\n-\n         # Decode\n         decoder_outputs = self.decoder(\n             input_ids=decoder_input_ids,\n@@ -1649,12 +1417,6 @@ def forward(\n \n         sequence_output = decoder_outputs[0]\n \n-        # Set device for model parallelism\n-        if self.model_parallel:\n-            torch.cuda.set_device(self.encoder.first_device)\n-            self.lm_head = self.lm_head.to(self.encoder.first_device)\n-            sequence_output = sequence_output.to(self.lm_head.weight.device)\n-\n         if self.config.tie_word_embeddings:\n             sequence_output = sequence_output * (self.model_dim**-0.5)\n \n@@ -1704,40 +1466,6 @@ def __init__(self, config: T5Config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-        # Model parallel\n-        self.model_parallel = False\n-        self.device_map = None\n-\n-    @add_start_docstrings(PARALLELIZE_DOCSTRING)\n-    def parallelize(self, device_map=None):\n-        warnings.warn(\n-            \"`T5EncoderModel.parallelize` is deprecated and will be removed in v5 of Transformers, you should load\"\n-            \" your model with `device_map='balanced'` in the call to `from_pretrained`. You can also provide your own\"\n-            \" `device_map` but it needs to be a dictionary module_name to device, so for instance {'block.0': 0,\"\n-            \" 'block.1': 1, ...}\",\n-            FutureWarning,\n-        )\n-        self.device_map = (\n-            get_device_map(len(self.encoder.block), range(torch.cuda.device_count()))\n-            if device_map is None\n-            else device_map\n-        )\n-        assert_device_map(self.device_map, len(self.encoder.block))\n-        self.encoder.parallelize(self.device_map)\n-        self.model_parallel = True\n-\n-    @add_start_docstrings(DEPARALLELIZE_DOCSTRING)\n-    def deparallelize(self):\n-        warnings.warn(\n-            \"Like `parallelize`, `deparallelize` is deprecated and will be removed in v5 of Transformers.\",\n-            FutureWarning,\n-        )\n-        self.encoder.deparallelize()\n-        self.encoder = self.encoder.to(\"cpu\")\n-        self.model_parallel = False\n-        self.device_map = None\n-        torch.cuda.empty_cache()\n-\n     def get_input_embeddings(self):\n         return self.shared\n \n@@ -1827,8 +1555,6 @@ def __init__(self, config: T5Config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-        self.model_parallel = False\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -2080,8 +1806,6 @@ def __init__(self, config: T5Config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-        self.model_parallel = False\n-\n     def get_input_embeddings(self):\n         return self.shared\n "
        },
        {
            "sha": "55dc3cf4ca8434faa3aa71a4c0128934087416ae",
            "filename": "src/transformers/models/umt5/modeling_umt5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py?ref=ad74fba08521b34f36cd72f6254b9b19a4fdb04b",
            "patch": "@@ -1505,8 +1505,6 @@ def __init__(self, config: UMT5Config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-        self.model_parallel = False\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "ff9fad71d20fbb9f39726e3578716f4882136338",
            "filename": "src/transformers/models/xlm_roberta/modeling_xlm_roberta.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py?ref=ad74fba08521b34f36cd72f6254b9b19a4fdb04b",
            "patch": "@@ -1026,7 +1026,7 @@ def forward(\n \n         lm_loss = None\n         if labels is not None:\n-            # move labels to correct device to enable model parallelism\n+            # move labels to correct device\n             labels = labels.to(prediction_scores.device)\n             lm_loss = self.loss_function(\n                 prediction_scores,\n@@ -1117,7 +1117,7 @@ def forward(\n \n         masked_lm_loss = None\n         if labels is not None:\n-            # move labels to correct device to enable model parallelism\n+            # move labels to correct device\n             labels = labels.to(prediction_scores.device)\n             loss_fct = CrossEntropyLoss()\n             masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n@@ -1213,7 +1213,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # move labels to correct device to enable model parallelism\n+            # move labels to correct device\n             labels = labels.to(logits.device)\n             if self.config.problem_type is None:\n                 if self.num_labels == 1:\n@@ -1330,7 +1330,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # move labels to correct device to enable model parallelism\n+            # move labels to correct device\n             labels = labels.to(reshaped_logits.device)\n             loss_fct = CrossEntropyLoss()\n             loss = loss_fct(reshaped_logits, labels)\n@@ -1403,7 +1403,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # move labels to correct device to enable model parallelism\n+            # move labels to correct device\n             labels = labels.to(logits.device)\n             loss_fct = CrossEntropyLoss()\n             loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))"
        },
        {
            "sha": "a00f1a8cd6e80185ac4bb286691117ae5da4b067",
            "filename": "src/transformers/models/xlm_roberta/modular_xlm_roberta.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodular_xlm_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodular_xlm_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodular_xlm_roberta.py?ref=ad74fba08521b34f36cd72f6254b9b19a4fdb04b",
            "patch": "@@ -136,7 +136,7 @@ def forward(\n \n         lm_loss = None\n         if labels is not None:\n-            # move labels to correct device to enable model parallelism\n+            # move labels to correct device\n             labels = labels.to(prediction_scores.device)\n             lm_loss = self.loss_function(\n                 prediction_scores,\n@@ -210,7 +210,7 @@ def forward(\n \n         masked_lm_loss = None\n         if labels is not None:\n-            # move labels to correct device to enable model parallelism\n+            # move labels to correct device\n             labels = labels.to(prediction_scores.device)\n             loss_fct = CrossEntropyLoss()\n             masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n@@ -279,7 +279,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # move labels to correct device to enable model parallelism\n+            # move labels to correct device\n             labels = labels.to(logits.device)\n             if self.config.problem_type is None:\n                 if self.num_labels == 1:\n@@ -392,7 +392,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # move labels to correct device to enable model parallelism\n+            # move labels to correct device\n             labels = labels.to(reshaped_logits.device)\n             loss_fct = CrossEntropyLoss()\n             loss = loss_fct(reshaped_logits, labels)\n@@ -457,7 +457,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # move labels to correct device to enable model parallelism\n+            # move labels to correct device\n             labels = labels.to(logits.device)\n             loss_fct = CrossEntropyLoss()\n             loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))"
        },
        {
            "sha": "467dff5f313027eb15273a518d9ad8a3f150930d",
            "filename": "src/transformers/models/xlstm/modeling_xlstm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Fmodels%2Fxlstm%2Fmodeling_xlstm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Fmodels%2Fxlstm%2Fmodeling_xlstm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlstm%2Fmodeling_xlstm.py?ref=ad74fba08521b34f36cd72f6254b9b19a4fdb04b",
            "patch": "@@ -1605,7 +1605,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # move labels to correct device to enable model parallelism\n+            # move labels to correct device\n             labels = labels.to(logits.device)\n             # Shift so that tokens < nstate predict nstate\n             shift_logits = logits[..., :-1, :].contiguous()"
        },
        {
            "sha": "45ac6ba325b6eb7f8d6ef9dd792367e5edd2428b",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 14,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=ad74fba08521b34f36cd72f6254b9b19a4fdb04b",
            "patch": "@@ -497,26 +497,13 @@ def __init__(\n                 \"https://huggingface.co/docs/transformers/model_doc/auto\"\n             )\n \n-        if getattr(model, \"is_parallelizable\", False) and getattr(model, \"model_parallel\", False):\n-            self.is_model_parallel = True\n-        else:\n-            self.is_model_parallel = False\n-\n+        self.is_model_parallel = False\n         if getattr(model, \"hf_device_map\", None) is not None:\n             devices = [device for device in set(model.hf_device_map.values()) if device not in [\"cpu\", \"disk\"]]\n             if len(devices) > 1:\n                 self.is_model_parallel = True\n             elif len(devices) == 1:\n                 self.is_model_parallel = self.args.device != torch.device(devices[0])\n-            else:\n-                self.is_model_parallel = False\n-\n-            # warn users\n-            if self.is_model_parallel:\n-                logger.info(\n-                    \"You have loaded a model on multiple GPUs. `is_model_parallel` attribute will be force-set\"\n-                    \" to `True` to avoid any unexpected behavior such as device placement mismatching.\"\n-                )\n \n         if self.args.use_liger_kernel:\n             if is_liger_kernel_available():"
        },
        {
            "sha": "7db16b70a75ccdce7bd9c07c5dfc99f42e5c8807",
            "filename": "src/transformers/utils/model_parallel_utils.py",
            "status": "removed",
            "additions": 0,
            "deletions": 55,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Futils%2Fmodel_parallel_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Futils%2Fmodel_parallel_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fmodel_parallel_utils.py?ref=38a08b6e8ae35857109cedad75377997fecbf9d0",
            "patch": "@@ -1,55 +0,0 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-from math import ceil\n-\n-\n-def assert_device_map(device_map, num_blocks):\n-    blocks = list(range(0, num_blocks))\n-\n-    device_map_blocks = [item for sublist in list(device_map.values()) for item in sublist]\n-\n-    # Duplicate check\n-    duplicate_blocks = []\n-    for i in device_map_blocks:\n-        if device_map_blocks.count(i) > 1 and i not in duplicate_blocks:\n-            duplicate_blocks.append(i)\n-    # Missing blocks\n-    missing_blocks = [i for i in blocks if i not in device_map_blocks]\n-    extra_blocks = [i for i in device_map_blocks if i not in blocks]\n-\n-    if len(duplicate_blocks) != 0:\n-        raise ValueError(\n-            \"Duplicate attention blocks specified in device_map. Attention blocks must be specified to one device.\"\n-            \" These attention blocks were specified more than once: \" + str(duplicate_blocks)\n-        )\n-    if len(missing_blocks) != 0:\n-        raise ValueError(\n-            \"There are attention blocks for this model that are not specified in the device_map. Add these attention \"\n-            \"blocks to a device on the device_map: \" + str(missing_blocks)\n-        )\n-    if len(extra_blocks) != 0:\n-        raise ValueError(\n-            \"The device_map contains more attention blocks than this model has. Remove these from the device_map:\"\n-            + str(extra_blocks)\n-        )\n-\n-\n-def get_device_map(n_layers, devices):\n-    \"\"\"Returns a dictionary of layers distributed evenly across all devices.\"\"\"\n-    layers = list(range(n_layers))\n-    n_blocks = int(ceil(n_layers / len(devices)))\n-    layers_list = [layers[i : i + n_blocks] for i in range(0, n_layers, n_blocks)]\n-\n-    return dict(zip(devices, layers_list))"
        },
        {
            "sha": "ceefc65e20964bbe7af3b69ed19130dc424a243a",
            "filename": "tests/models/bark/test_modeling_bark.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/tests%2Fmodels%2Fbark%2Ftest_modeling_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/tests%2Fmodels%2Fbark%2Ftest_modeling_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbark%2Ftest_modeling_bark.py?ref=ad74fba08521b34f36cd72f6254b9b19a4fdb04b",
            "patch": "@@ -532,8 +532,6 @@ class BarkSemanticModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.Te\n     fx_compatible = False\n     test_missing_keys = False\n     test_pruning = False\n-    test_model_parallel = False\n-    # no model_parallel for now\n \n     test_resize_embeddings = True\n \n@@ -622,9 +620,6 @@ class BarkCoarseModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.Test\n     fx_compatible = False\n     test_missing_keys = False\n     test_pruning = False\n-    test_model_parallel = False\n-    # no model_parallel for now\n-\n     test_resize_embeddings = True\n \n     def setUp(self):\n@@ -709,9 +704,6 @@ class BarkFineModelTest(ModelTesterMixin, unittest.TestCase):\n     fx_compatible = False\n     test_missing_keys = False\n     test_pruning = False\n-    # no model_parallel for now\n-    test_model_parallel = False\n-\n     # torchscript disabled for now because forward with an int\n     test_torchscript = False\n "
        },
        {
            "sha": "2c6ddafc5805b17c17c5acd2da9609580bfb61ed",
            "filename": "tests/models/codegen/test_modeling_codegen.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/tests%2Fmodels%2Fcodegen%2Ftest_modeling_codegen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/tests%2Fmodels%2Fcodegen%2Ftest_modeling_codegen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcodegen%2Ftest_modeling_codegen.py?ref=ad74fba08521b34f36cd72f6254b9b19a4fdb04b",
            "patch": "@@ -327,7 +327,6 @@ class CodeGenModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMi\n     fx_compatible = False\n     test_pruning = False\n     test_missing_keys = False\n-    test_model_parallel = False\n     test_head_masking = False\n \n     # special case for DoubleHeads model"
        },
        {
            "sha": "8543e7bf414771387f86a123709ca566af2a1a2a",
            "filename": "tests/models/falcon_mamba/test_modeling_falcon_mamba.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/tests%2Fmodels%2Ffalcon_mamba%2Ftest_modeling_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/tests%2Fmodels%2Ffalcon_mamba%2Ftest_modeling_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffalcon_mamba%2Ftest_modeling_falcon_mamba.py?ref=ad74fba08521b34f36cd72f6254b9b19a4fdb04b",
            "patch": "@@ -273,7 +273,6 @@ class FalconMambaModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTest\n     fx_compatible = False  # FIXME let's try to support this @ArthurZucker\n     test_torchscript = False  # FIXME let's try to support this @ArthurZucker\n     test_missing_keys = False\n-    test_model_parallel = False\n     test_pruning = False\n     test_head_masking = False  # FalconMamba does not have attention heads\n     pipeline_model_mapping = ("
        },
        {
            "sha": "205057a4b447d153f73fa77d96a34c046edd2ff7",
            "filename": "tests/models/fuyu/test_modeling_fuyu.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/tests%2Fmodels%2Ffuyu%2Ftest_modeling_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/tests%2Fmodels%2Ffuyu%2Ftest_modeling_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffuyu%2Ftest_modeling_fuyu.py?ref=ad74fba08521b34f36cd72f6254b9b19a4fdb04b",
            "patch": "@@ -173,7 +173,6 @@ class FuyuModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin\n     test_pruning = False\n     test_cpu_offload = False\n     test_disk_offload = False\n-    test_model_parallel = False\n \n     def setUp(self):\n         self.model_tester = FuyuModelTester(self)"
        },
        {
            "sha": "3c1d82d97805d079bd80b248b5aa107a2721cfb2",
            "filename": "tests/models/gpt2/test_modeling_gpt2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/tests%2Fmodels%2Fgpt2%2Ftest_modeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/tests%2Fmodels%2Fgpt2%2Ftest_modeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt2%2Ftest_modeling_gpt2.py?ref=ad74fba08521b34f36cd72f6254b9b19a4fdb04b",
            "patch": "@@ -172,10 +172,8 @@ class GPT2ModelTest(CausalLMModelTest, unittest.TestCase):\n         if is_torch_available()\n         else {}\n     )\n-    all_parallelizable_model_classes = (GPT2LMHeadModel, GPT2DoubleHeadsModel) if is_torch_available() else ()\n     fx_compatible = False  # Broken by attention refactor cc @Cyrilvallez\n     test_missing_keys = False\n-    test_model_parallel = True\n     model_tester_class = GPT2ModelTester\n \n     def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):"
        },
        {
            "sha": "d39aee3445a0825c798d34f081fc289c44f80af9",
            "filename": "tests/models/gpt_neo/test_modeling_gpt_neo.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/tests%2Fmodels%2Fgpt_neo%2Ftest_modeling_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/tests%2Fmodels%2Fgpt_neo%2Ftest_modeling_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt_neo%2Ftest_modeling_gpt_neo.py?ref=ad74fba08521b34f36cd72f6254b9b19a4fdb04b",
            "patch": "@@ -390,7 +390,6 @@ class GPTNeoModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMix\n     fx_compatible = True\n     test_missing_keys = False\n     test_pruning = False\n-    test_model_parallel = False\n \n     # special case for DoubleHeads model\n     def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):"
        },
        {
            "sha": "54282ab0ed60b754f8da7e1461dece3078f069c3",
            "filename": "tests/models/gpt_neox/test_modeling_gpt_neox.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/tests%2Fmodels%2Fgpt_neox%2Ftest_modeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/tests%2Fmodels%2Fgpt_neox%2Ftest_modeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt_neox%2Ftest_modeling_gpt_neox.py?ref=ad74fba08521b34f36cd72f6254b9b19a4fdb04b",
            "patch": "@@ -287,7 +287,6 @@ class GPTNeoXModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMi\n     )\n     test_pruning = False\n     test_missing_keys = False\n-    test_model_parallel = False\n     test_head_masking = False\n \n     def setUp(self):"
        },
        {
            "sha": "0b4e5684f7f55875b4d2e386b593e53aec362e22",
            "filename": "tests/models/gpt_neox_japanese/test_modeling_gpt_neox_japanese.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/tests%2Fmodels%2Fgpt_neox_japanese%2Ftest_modeling_gpt_neox_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/tests%2Fmodels%2Fgpt_neox_japanese%2Ftest_modeling_gpt_neox_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt_neox_japanese%2Ftest_modeling_gpt_neox_japanese.py?ref=ad74fba08521b34f36cd72f6254b9b19a4fdb04b",
            "patch": "@@ -204,7 +204,6 @@ class GPTNeoXModelJapaneseTest(ModelTesterMixin, GenerationTesterMixin, Pipeline\n     )\n     test_pruning = False\n     test_missing_keys = False\n-    test_model_parallel = False\n     test_head_masking = False\n \n     def setUp(self):"
        },
        {
            "sha": "81f866ee4f57d1a53861e9b2ece1659b8c885b56",
            "filename": "tests/models/gptj/test_modeling_gptj.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/tests%2Fmodels%2Fgptj%2Ftest_modeling_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/tests%2Fmodels%2Fgptj%2Ftest_modeling_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgptj%2Ftest_modeling_gptj.py?ref=ad74fba08521b34f36cd72f6254b9b19a4fdb04b",
            "patch": "@@ -354,7 +354,6 @@ class GPTJModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin\n     fx_compatible = True\n     test_pruning = False\n     test_missing_keys = False\n-    test_model_parallel = False\n     test_head_masking = False\n \n     def test_torch_fx(self):"
        },
        {
            "sha": "8ac0d148bc54c38d9b730798fa840e52eb90db05",
            "filename": "tests/models/longt5/test_modeling_longt5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/tests%2Fmodels%2Flongt5%2Ftest_modeling_longt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/tests%2Fmodels%2Flongt5%2Ftest_modeling_longt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flongt5%2Ftest_modeling_longt5.py?ref=ad74fba08521b34f36cd72f6254b9b19a4fdb04b",
            "patch": "@@ -515,7 +515,6 @@ class LongT5ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMix\n     test_pruning = False\n     test_torchscript = True\n     test_resize_embeddings = True\n-    test_model_parallel = False\n     is_encoder_decoder = True\n \n     def setUp(self):\n@@ -1012,7 +1011,6 @@ class LongT5EncoderOnlyModelTest(ModelTesterMixin, unittest.TestCase):\n     test_pruning = False\n     test_torchscript = True\n     test_resize_embeddings = False\n-    test_model_parallel = False\n \n     def setUp(self):\n         self.model_tester = LongT5EncoderOnlyModelTester(self)"
        },
        {
            "sha": "6a77e0f9e8663663255a9d68398421d853b60e8a",
            "filename": "tests/models/mamba/test_modeling_mamba.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/tests%2Fmodels%2Fmamba%2Ftest_modeling_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/tests%2Fmodels%2Fmamba%2Ftest_modeling_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmamba%2Ftest_modeling_mamba.py?ref=ad74fba08521b34f36cd72f6254b9b19a4fdb04b",
            "patch": "@@ -242,7 +242,6 @@ class MambaModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixi\n     fx_compatible = False  # FIXME let's try to support this @ArthurZucker\n     test_torchscript = False  # FIXME let's try to support this @ArthurZucker\n     test_missing_keys = False\n-    test_model_parallel = False\n     test_pruning = False\n     test_head_masking = False  # Mamba does not have attention heads\n     pipeline_model_mapping = ("
        },
        {
            "sha": "603e30d6b0768f0f447acb721ff01423a82bed1b",
            "filename": "tests/models/mamba2/test_modeling_mamba2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/tests%2Fmodels%2Fmamba2%2Ftest_modeling_mamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/tests%2Fmodels%2Fmamba2%2Ftest_modeling_mamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmamba2%2Ftest_modeling_mamba2.py?ref=ad74fba08521b34f36cd72f6254b9b19a4fdb04b",
            "patch": "@@ -245,7 +245,6 @@ class Mamba2ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMix\n     fx_compatible = False  # FIXME let's try to support this @molbap\n     test_torchscript = False  # FIXME I think this should be doable @molbap @ArthurZucker\n     test_missing_keys = False\n-    test_model_parallel = False\n     test_pruning = False\n     test_head_masking = False  # Mamba does not have attention heads\n "
        },
        {
            "sha": "9a141d60981e08fb8a14aa3e6698cad22094bd9c",
            "filename": "tests/models/mt5/test_modeling_mt5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/tests%2Fmodels%2Fmt5%2Ftest_modeling_mt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/tests%2Fmodels%2Fmt5%2Ftest_modeling_mt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmt5%2Ftest_modeling_mt5.py?ref=ad74fba08521b34f36cd72f6254b9b19a4fdb04b",
            "patch": "@@ -562,11 +562,9 @@ class MT5ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin,\n         if is_torch_available()\n         else {}\n     )\n-    all_parallelizable_model_classes = (MT5Model, MT5ForConditionalGeneration) if is_torch_available() else ()\n     fx_compatible = True\n     test_pruning = False\n     test_resize_embeddings = True\n-    test_model_parallel = True\n     is_encoder_decoder = True\n     # The small MT5 model needs higher percentages for CPU/MP tests\n     model_split_percents = [0.5, 0.8, 0.9]\n@@ -1008,15 +1006,13 @@ class MT5EncoderOnlyModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.Te\n     all_model_classes = (MT5EncoderModel, MT5ForTokenClassification) if is_torch_available() else ()\n     test_pruning = False\n     test_resize_embeddings = False\n-    test_model_parallel = True\n     pipeline_model_mapping = (\n         {\n             \"token-classification\": MT5ForTokenClassification,\n         }\n         if is_torch_available()\n         else {}\n     )\n-    all_parallelizable_model_classes = (MT5EncoderModel,) if is_torch_available() else ()\n \n     def setUp(self):\n         self.model_tester = MT5EncoderOnlyModelTester(self)"
        },
        {
            "sha": "d7a7a4950ac1832bc6fdde47e10de5d1c5bbe90d",
            "filename": "tests/models/patchtsmixer/test_modeling_patchtsmixer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/tests%2Fmodels%2Fpatchtsmixer%2Ftest_modeling_patchtsmixer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/tests%2Fmodels%2Fpatchtsmixer%2Ftest_modeling_patchtsmixer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpatchtsmixer%2Ftest_modeling_patchtsmixer.py?ref=ad74fba08521b34f36cd72f6254b9b19a4fdb04b",
            "patch": "@@ -230,7 +230,6 @@ class PatchTSMixerModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.Test\n     test_resize_embeddings = True\n     test_resize_position_embeddings = False\n     test_mismatched_shapes = True\n-    test_model_parallel = False\n     has_attentions = False\n \n     def setUp(self):"
        },
        {
            "sha": "960c146d2855a7c1dd3a8f3775fd08124e3ac912",
            "filename": "tests/models/patchtst/test_modeling_patchtst.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/tests%2Fmodels%2Fpatchtst%2Ftest_modeling_patchtst.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/tests%2Fmodels%2Fpatchtst%2Ftest_modeling_patchtst.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpatchtst%2Ftest_modeling_patchtst.py?ref=ad74fba08521b34f36cd72f6254b9b19a4fdb04b",
            "patch": "@@ -169,7 +169,6 @@ class PatchTSTModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase\n     test_resize_embeddings = True\n     test_resize_position_embeddings = False\n     test_mismatched_shapes = True\n-    test_model_parallel = False\n     has_attentions = True\n \n     def setUp(self):"
        },
        {
            "sha": "8c1d8b350ced3e5e63a8ea9c8d8bba7a48074f61",
            "filename": "tests/models/pop2piano/test_modeling_pop2piano.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/tests%2Fmodels%2Fpop2piano%2Ftest_modeling_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/tests%2Fmodels%2Fpop2piano%2Ftest_modeling_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpop2piano%2Ftest_modeling_pop2piano.py?ref=ad74fba08521b34f36cd72f6254b9b19a4fdb04b",
            "patch": "@@ -491,11 +491,9 @@ class Pop2PianoModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCas\n     pipeline_model_mapping = (\n         {\"automatic-speech-recognition\": Pop2PianoForConditionalGeneration} if is_torch_available() else {}\n     )\n-    all_parallelizable_model_classes = ()\n     fx_compatible = False\n     test_pruning = False\n     test_resize_embeddings = True\n-    test_model_parallel = False\n     is_encoder_decoder = True\n \n     def setUp(self):"
        },
        {
            "sha": "891c01315376abb7093962302181a3ffa0e47954",
            "filename": "tests/models/rwkv/test_modeling_rwkv.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/tests%2Fmodels%2Frwkv%2Ftest_modeling_rwkv.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/tests%2Fmodels%2Frwkv%2Ftest_modeling_rwkv.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frwkv%2Ftest_modeling_rwkv.py?ref=ad74fba08521b34f36cd72f6254b9b19a4fdb04b",
            "patch": "@@ -222,7 +222,6 @@ class RwkvModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin\n     )\n     fx_compatible = False\n     test_missing_keys = False\n-    test_model_parallel = False\n     test_pruning = False\n     test_head_masking = False  # Rwkv does not support head masking\n "
        },
        {
            "sha": "4690407c7b51d3c140a01d68051f429e270ac2ec",
            "filename": "tests/models/seamless_m4t/test_modeling_seamless_m4t.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/tests%2Fmodels%2Fseamless_m4t%2Ftest_modeling_seamless_m4t.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/tests%2Fmodels%2Fseamless_m4t%2Ftest_modeling_seamless_m4t.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fseamless_m4t%2Ftest_modeling_seamless_m4t.py?ref=ad74fba08521b34f36cd72f6254b9b19a4fdb04b",
            "patch": "@@ -342,7 +342,6 @@ class SeamlessM4TModelWithSpeechInputTest(ModelTesterMixin, unittest.TestCase):\n     fx_compatible = False\n     test_missing_keys = False\n     test_pruning = False\n-    test_model_parallel = False\n     test_resize_embeddings = False\n     test_headmasking = False\n     test_torchscript = False\n@@ -576,7 +575,6 @@ class SeamlessM4TModelWithTextInputTest(ModelTesterMixin, PipelineTesterMixin, u\n     fx_compatible = False\n     test_missing_keys = False\n     test_pruning = False\n-    test_model_parallel = False\n     test_resize_embeddings = True\n     test_headmasking = False\n     test_torchscript = False"
        },
        {
            "sha": "9c73a6ba3b4d0ecf494685b4b954ce384beb2b96",
            "filename": "tests/models/seamless_m4t_v2/test_modeling_seamless_m4t_v2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/tests%2Fmodels%2Fseamless_m4t_v2%2Ftest_modeling_seamless_m4t_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/tests%2Fmodels%2Fseamless_m4t_v2%2Ftest_modeling_seamless_m4t_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fseamless_m4t_v2%2Ftest_modeling_seamless_m4t_v2.py?ref=ad74fba08521b34f36cd72f6254b9b19a4fdb04b",
            "patch": "@@ -368,7 +368,6 @@ class SeamlessM4Tv2ModelWithSpeechInputTest(ModelTesterMixin, unittest.TestCase)\n     fx_compatible = False\n     test_missing_keys = False\n     test_pruning = False\n-    test_model_parallel = False\n     test_resize_embeddings = False\n     test_headmasking = False\n     test_torchscript = False\n@@ -601,7 +600,6 @@ class SeamlessM4Tv2ModelWithTextInputTest(ModelTesterMixin, unittest.TestCase):\n     fx_compatible = False\n     test_missing_keys = False\n     test_pruning = False\n-    test_model_parallel = False\n     test_resize_embeddings = True\n     test_headmasking = False\n     test_torchscript = False"
        },
        {
            "sha": "440d647239959a6c7a83b625547b1ccb2e8c4292",
            "filename": "tests/models/speecht5/test_modeling_speecht5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/tests%2Fmodels%2Fspeecht5%2Ftest_modeling_speecht5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/tests%2Fmodels%2Fspeecht5%2Ftest_modeling_speecht5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fspeecht5%2Ftest_modeling_speecht5.py?ref=ad74fba08521b34f36cd72f6254b9b19a4fdb04b",
            "patch": "@@ -1859,7 +1859,6 @@ class SpeechT5HifiGanTest(ModelTesterMixin, unittest.TestCase):\n     test_head_masking = False\n     test_mismatched_shapes = False\n     test_missing_keys = False\n-    test_model_parallel = False\n     is_encoder_decoder = False\n     has_attentions = False\n "
        },
        {
            "sha": "bdf4b0cc62435f3c894c07d805f49d7a923e70dd",
            "filename": "tests/models/switch_transformers/test_modeling_switch_transformers.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/tests%2Fmodels%2Fswitch_transformers%2Ftest_modeling_switch_transformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/tests%2Fmodels%2Fswitch_transformers%2Ftest_modeling_switch_transformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fswitch_transformers%2Ftest_modeling_switch_transformers.py?ref=ad74fba08521b34f36cd72f6254b9b19a4fdb04b",
            "patch": "@@ -571,7 +571,6 @@ class SwitchTransformersModelTest(ModelTesterMixin, GenerationTesterMixin, Pipel\n     fx_compatible = False\n     test_pruning = False\n     test_resize_embeddings = True\n-    test_model_parallel = False\n     is_encoder_decoder = True\n     test_torchscript = False\n     # The small SWITCH_TRANSFORMERS model needs higher percentages for CPU/MP tests\n@@ -820,7 +819,6 @@ class SwitchTransformersEncoderOnlyModelTest(ModelTesterMixin, unittest.TestCase\n     all_model_classes = (SwitchTransformersEncoderModel,) if is_torch_available() else ()\n     test_pruning = False\n     test_resize_embeddings = False\n-    test_model_parallel = False\n     test_torchscript = False\n \n     def setUp(self):"
        },
        {
            "sha": "30c8eea3bc9af20b98b2439b43a6313583c3a071",
            "filename": "tests/models/t5/test_modeling_t5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py?ref=ad74fba08521b34f36cd72f6254b9b19a4fdb04b",
            "patch": "@@ -570,11 +570,9 @@ class T5ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin,\n         if is_torch_available()\n         else {}\n     )\n-    all_parallelizable_model_classes = (T5Model, T5ForConditionalGeneration) if is_torch_available() else ()\n     fx_compatible = True\n     test_pruning = False\n     test_resize_embeddings = True\n-    test_model_parallel = True\n     is_encoder_decoder = True\n     # The small T5 model needs higher percentages for CPU/MP tests\n     model_split_percents = [0.5, 0.8, 0.9]\n@@ -1014,15 +1012,13 @@ class T5EncoderOnlyModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.Tes\n     all_model_classes = (T5EncoderModel, T5ForTokenClassification) if is_torch_available() else ()\n     test_pruning = False\n     test_resize_embeddings = False\n-    test_model_parallel = True\n     pipeline_model_mapping = (\n         {\n             \"token-classification\": T5ForTokenClassification,\n         }\n         if is_torch_available()\n         else {}\n     )\n-    all_parallelizable_model_classes = (T5EncoderModel,) if is_torch_available() else ()\n \n     def setUp(self):\n         self.model_tester = T5EncoderOnlyModelTester(self)"
        },
        {
            "sha": "e77fbe65ebb57478767c5290399de5ebd9dde992",
            "filename": "tests/models/timesfm/test_modeling_timesfm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/tests%2Fmodels%2Ftimesfm%2Ftest_modeling_timesfm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/tests%2Fmodels%2Ftimesfm%2Ftest_modeling_timesfm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftimesfm%2Ftest_modeling_timesfm.py?ref=ad74fba08521b34f36cd72f6254b9b19a4fdb04b",
            "patch": "@@ -123,11 +123,9 @@ def prepare_config_and_inputs_for_common(self):\n class TimesFmModelTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (TimesFmModelForPrediction,) if is_torch_available() else ()\n     all_generative_model_classes = ()\n-    all_parallelizable_model_classes = ()\n     fx_compatible = False\n     test_pruning = False\n     test_resize_embeddings = False\n-    test_model_parallel = False\n     is_encoder_decoder = False\n     test_inputs_embeds = False\n "
        },
        {
            "sha": "3ed21af6507eb526c8ea73352ef9d338b76573e1",
            "filename": "tests/models/timm_wrapper/test_modeling_timm_wrapper.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/tests%2Fmodels%2Ftimm_wrapper%2Ftest_modeling_timm_wrapper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/tests%2Fmodels%2Ftimm_wrapper%2Ftest_modeling_timm_wrapper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftimm_wrapper%2Ftest_modeling_timm_wrapper.py?ref=ad74fba08521b34f36cd72f6254b9b19a4fdb04b",
            "patch": "@@ -97,7 +97,6 @@ class TimmWrapperModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestC\n     test_head_masking = False\n     test_pruning = False\n     has_attentions = False\n-    test_model_parallel = False\n \n     def setUp(self):\n         self.config_class = TimmWrapperConfig"
        },
        {
            "sha": "fe47a6251975ff073d4d43cf29b06cc89de0ac03",
            "filename": "tests/models/udop/test_modeling_udop.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/tests%2Fmodels%2Fudop%2Ftest_modeling_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/tests%2Fmodels%2Fudop%2Ftest_modeling_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fudop%2Ftest_modeling_udop.py?ref=ad74fba08521b34f36cd72f6254b9b19a4fdb04b",
            "patch": "@@ -280,7 +280,6 @@ class UdopModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin\n     test_torchscript = False\n     test_head_masking = False\n     test_resize_embeddings = True\n-    test_model_parallel = False\n     is_encoder_decoder = True\n     test_cpu_offload = False\n     # The small UDOP model needs higher percentages for CPU/MP tests\n@@ -556,8 +555,6 @@ class UdopEncoderOnlyModelTest(ModelTesterMixin, unittest.TestCase):\n     test_torchscript = False\n     test_head_masking = False\n     test_resize_embeddings = False\n-    test_model_parallel = False\n-    all_parallelizable_model_classes = (UdopEncoderModel,) if is_torch_available() else ()\n \n     def setUp(self):\n         self.model_tester = UdopEncoderOnlyModelTester(self)"
        },
        {
            "sha": "f8f8beb7fa9c82052f84ff4f0bb381e8cb306da4",
            "filename": "tests/models/umt5/test_modeling_umt5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/tests%2Fmodels%2Fumt5%2Ftest_modeling_umt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/tests%2Fmodels%2Fumt5%2Ftest_modeling_umt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fumt5%2Ftest_modeling_umt5.py?ref=ad74fba08521b34f36cd72f6254b9b19a4fdb04b",
            "patch": "@@ -647,15 +647,13 @@ class UMT5EncoderOnlyModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.T\n     all_model_classes = (UMT5EncoderModel, UMT5ForTokenClassification) if is_torch_available() else ()\n     test_pruning = False\n     test_resize_embeddings = False\n-    test_model_parallel = True\n     pipeline_model_mapping = (\n         {\n             \"token-classification\": UMT5ForTokenClassification,\n         }\n         if is_torch_available()\n         else {}\n     )\n-    all_parallelizable_model_classes = (UMT5EncoderModel,) if is_torch_available() else ()\n \n     def setUp(self):\n         self.model_tester = UMT5EncoderOnlyModelTester(self)"
        },
        {
            "sha": "441df35c236de5e3a5340dd780d929cad3a96f50",
            "filename": "tests/models/univnet/test_modeling_univnet.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/tests%2Fmodels%2Funivnet%2Ftest_modeling_univnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/tests%2Fmodels%2Funivnet%2Ftest_modeling_univnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Funivnet%2Ftest_modeling_univnet.py?ref=ad74fba08521b34f36cd72f6254b9b19a4fdb04b",
            "patch": "@@ -112,8 +112,6 @@ class UnivNetModelTest(ModelTesterMixin, unittest.TestCase):\n     test_mismatched_shapes = False\n     # UnivNetModel does not have a base_model_prefix attribute.\n     test_missing_keys = False\n-    # UnivNetModel does not implement a parallelize method.\n-    test_model_parallel = False\n     is_encoder_decoder = False\n     has_attentions = False\n "
        },
        {
            "sha": "eea931253b819b3a0620790c75fd92ea75ff52ca",
            "filename": "tests/models/xlstm/test_modeling_xlstm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/tests%2Fmodels%2Fxlstm%2Ftest_modeling_xlstm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/tests%2Fmodels%2Fxlstm%2Ftest_modeling_xlstm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxlstm%2Ftest_modeling_xlstm.py?ref=ad74fba08521b34f36cd72f6254b9b19a4fdb04b",
            "patch": "@@ -156,7 +156,6 @@ class xLSTMModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixi\n     has_attentions = False  # xLSTM does not support attentions\n     fx_compatible = False\n     test_torchscript = False\n-    test_model_parallel = False\n     test_pruning = False\n     test_head_masking = False  # xLSTM does not have attention heads\n "
        },
        {
            "sha": "a9d1777fcd0e79ec6bc56b7b82191cc3f51e2d18",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 0,
            "deletions": 104,
            "changes": 104,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=ad74fba08521b34f36cd72f6254b9b19a4fdb04b",
            "patch": "@@ -13,7 +13,6 @@\n # limitations under the License.\n import collections\n import copy\n-import gc\n import inspect\n import math\n import os\n@@ -75,10 +74,6 @@\n )\n from transformers.testing_utils import (\n     CaptureLogger,\n-    backend_device_count,\n-    backend_empty_cache,\n-    backend_memory_allocated,\n-    backend_torch_accelerator_module,\n     get_device_properties,\n     hub_retry,\n     is_flaky,\n@@ -583,7 +578,6 @@ class ModelTesterMixin:\n     test_head_masking = True\n     test_mismatched_shapes = True\n     test_missing_keys = True\n-    test_model_parallel = False\n     test_torch_exportable = False\n     # Used in `check_training_gradient_checkpointing` to NOT check all params having gradient (e.g. for some MOE models)\n     test_all_params_have_gradient = True\n@@ -2872,104 +2866,6 @@ def test_multi_gpu_data_parallel_forward(self):\n             with torch.no_grad():\n                 _ = model(**self._prepare_for_class(inputs_dict, model_class))\n \n-    @require_torch_gpu\n-    @require_torch_multi_gpu\n-    def test_model_parallelization(self):\n-        if not self.test_model_parallel:\n-            self.skipTest(reason=\"test_model_parallel is set to False\")\n-\n-        # a candidate for testing_utils\n-        def get_current_gpu_memory_use():\n-            \"\"\"returns a list of VRAM allocations per GPU in MBs\"\"\"\n-\n-            per_device_memory = []\n-            for id in range(backend_device_count(torch_device)):\n-                with backend_torch_accelerator_module(torch_device).device(id):\n-                    per_device_memory.append(backend_memory_allocated(torch_device) >> 20)\n-\n-            return per_device_memory\n-\n-        # Needs a large model to see the difference.\n-        config = self.model_tester.get_large_model_config()\n-\n-        for model_class in self.all_parallelizable_model_classes:\n-            backend_empty_cache(torch_device)\n-\n-            # 1. single gpu memory load + unload + memory measurements\n-            # Retrieve initial memory usage (can easily be ~0.6-1.5GB if cuda-kernels have been preloaded by previous tests)\n-            memory_at_start = get_current_gpu_memory_use()\n-\n-            # Put model on device 0 and take a memory snapshot\n-            model = model_class(config)\n-            model.to(f\"{torch_device}:0\")\n-            memory_after_model_load = get_current_gpu_memory_use()\n-\n-            # The memory use on device 0 should be higher than it was initially.\n-            self.assertGreater(memory_after_model_load[0], memory_at_start[0])\n-\n-            del model\n-            gc.collect()\n-            backend_empty_cache(torch_device)\n-\n-            # 2. MP test\n-            # it's essential to re-calibrate the usage before the next stage\n-            memory_at_start = get_current_gpu_memory_use()\n-\n-            # Spread model layers over multiple devices\n-            model = model_class(config)\n-            model.parallelize()\n-            memory_after_parallelization = get_current_gpu_memory_use()\n-\n-            # Assert that the memory use on all devices is higher than it was when loaded only on CPU\n-            for n in range(len(model.device_map.keys())):\n-                self.assertGreater(memory_after_parallelization[n], memory_at_start[n])\n-\n-            # Assert that the memory use of device 0 is lower than it was when the entire model was loaded on it\n-            self.assertLess(memory_after_parallelization[0], memory_after_model_load[0])\n-\n-            # Assert that the memory use of device 1 is higher than it was when the entire model was loaded\n-            # on device 0 and device 1 wasn't used at all\n-            self.assertGreater(memory_after_parallelization[1], memory_after_model_load[1])\n-\n-            del model\n-            gc.collect()\n-            backend_empty_cache(torch_device)\n-\n-    @require_torch_gpu\n-    @require_torch_multi_gpu\n-    def test_model_parallel_equal_results(self):\n-        if not self.test_model_parallel:\n-            self.skipTest(reason=\"test_model_parallel is set to False\")\n-\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_parallelizable_model_classes:\n-            inputs_dict = self._prepare_for_class(inputs_dict, model_class)\n-\n-            def cast_to_device(dictionary, device):\n-                output = {}\n-                for k, v in dictionary.items():\n-                    if isinstance(v, torch.Tensor):\n-                        output[k] = v.to(device)\n-                    else:\n-                        output[k] = v\n-\n-                return output\n-\n-            model = model_class(config)\n-            output = model(**cast_to_device(inputs_dict, \"cpu\"))\n-\n-            model.parallelize()\n-\n-            parallel_output = model(**cast_to_device(inputs_dict, f\"{torch_device}:0\"))\n-\n-            for value, parallel_value in zip(output, parallel_output):\n-                if isinstance(value, torch.Tensor):\n-                    torch.testing.assert_close(value, parallel_value.to(\"cpu\"), rtol=1e-7, atol=1e-7)\n-                elif isinstance(value, (tuple, list)):\n-                    for value_, parallel_value_ in zip(value, parallel_value):\n-                        torch.testing.assert_close(value_, parallel_value_.to(\"cpu\"), rtol=1e-7, atol=1e-7)\n-\n     def check_device_map_is_respected(self, model, device_map):\n         for param_name, param in model.named_parameters():\n             # Find device in device_map"
        },
        {
            "sha": "dca7f7378508f4a18850657ce3a9890cd7951297",
            "filename": "tests/trainer/test_trainer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 21,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/tests%2Ftrainer%2Ftest_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/tests%2Ftrainer%2Ftest_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer.py?ref=ad74fba08521b34f36cd72f6254b9b19a4fdb04b",
            "patch": "@@ -2764,27 +2764,6 @@ def test_stable_adamw_lr_display_with_scheduler(self):\n         # warm up steps << total steps\n         self.assertTrue(len(decreasing_lrs) > len(increasing_lrs))\n \n-    @require_torch_multi_accelerator\n-    def test_data_is_not_parallelized_when_model_is_parallel(self):\n-        model = RegressionModel()\n-        # Make the Trainer believe it's a parallelized model\n-        model.is_parallelizable = True\n-        model.model_parallel = True\n-        with tempfile.TemporaryDirectory() as tmp_dir:\n-            args = TrainingArguments(\n-                tmp_dir, per_device_train_batch_size=16, per_device_eval_batch_size=16, report_to=\"none\"\n-            )\n-            trainer = Trainer(model, args, train_dataset=RegressionDataset(), eval_dataset=RegressionDataset())\n-            # Check the Trainer was fooled\n-            self.assertTrue(trainer.is_model_parallel)\n-            self.assertEqual(trainer.args.n_gpu, 1)\n-\n-            # The batch size of the training and evaluation dataloaders should be 16, not 16 * n_gpu\n-            self.assertEqual(trainer.get_train_dataloader().total_batch_size, 16)\n-            self.assertEqual(len(trainer.get_train_dataloader()), 64 // 16)\n-            self.assertEqual(trainer.get_eval_dataloader().total_batch_size, 16)\n-            self.assertEqual(len(trainer.get_eval_dataloader()), 64 // 16)\n-\n     def test_evaluate(self):\n         with tempfile.TemporaryDirectory() as tmp_dir:\n             trainer = get_regression_trainer(a=1.5, b=2.5, compute_metrics=AlmostAccuracy(), output_dir=tmp_dir)"
        },
        {
            "sha": "ea358ee696bb76eee57bf4d1e7ae2a7daffda72f",
            "filename": "utils/not_doctested.txt",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/utils%2Fnot_doctested.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad74fba08521b34f36cd72f6254b9b19a4fdb04b/utils%2Fnot_doctested.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fnot_doctested.txt?ref=ad74fba08521b34f36cd72f6254b9b19a4fdb04b",
            "patch": "@@ -810,7 +810,6 @@ src/transformers/utils/hp_naming.py\n src/transformers/utils/hub.py\n src/transformers/utils/import_utils.py\n src/transformers/utils/logging.py\n-src/transformers/utils/model_parallel_utils.py\n src/transformers/utils/notebook.py\n src/transformers/utils/peft_utils.py\n src/transformers/utils/quantization_config.py"
        }
    ],
    "stats": {
        "total": 1362,
        "additions": 61,
        "deletions": 1301
    }
}