{
    "author": "eustlb",
    "message": "[Whisper] Pipeline: handle long form generation (#35750)\n\n* handle long form generation\n\n* add warning\n\n* correct incorrect in place token change\n\n* update test to catch edge case\n\n* make style\n\n* update warning\n\n* add doc",
    "sha": "cfff7ca9a27280338c6a57dfa7722dcf44f51a87",
    "files": [
        {
            "sha": "2a64e599d06579054b7fafc02e0719bcd1eabbc5",
            "filename": "src/transformers/models/whisper/generation_whisper.py",
            "status": "modified",
            "additions": 24,
            "deletions": 9,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/cfff7ca9a27280338c6a57dfa7722dcf44f51a87/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cfff7ca9a27280338c6a57dfa7722dcf44f51a87/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py?ref=cfff7ca9a27280338c6a57dfa7722dcf44f51a87",
            "patch": "@@ -136,7 +136,18 @@ def _pad_to_max_length(\n     cut_off_length=None,\n     return_token_timestamps=False,\n     force_unique_generate_call=False,\n+    skip_ending_double_timestamps=False,\n+    timestamp_begin=None,\n ):\n+    \"\"\"\n+    skip_ending_double_timestamps: when the segement ended with two timestamp tokens, whether to ignore the last timestamp token\n+    see https://github.com/huggingface/transformers/pull/35750\n+\n+    _pad_to_max_length is used in different contexts:\n+    1. At the end of generation: we need to keep both ending timestamp tokens in the segment (see https://github.com/huggingface/transformers/pull/34537).\n+    2. In the middle of generation, e.g. when condition_on_prev_tokens=True and we want to use the last generated tokens as decoder_input_ids:\n+       we must skip one of the double ending timestamp tokens (see https://github.com/huggingface/transformers/pull/35750).\n+    \"\"\"\n     max_total_length = 0\n     sequences = []\n     token_timestamps_list = []\n@@ -166,7 +177,17 @@ def _pad_to_max_length(\n \n     for current_segment_list in current_segments:\n         if current_segment_list is not None and len([d[\"tokens\"] for d in current_segment_list]) > 0:\n-            sequence = torch.cat([d[\"tokens\"] for d in current_segment_list], dim=-1)\n+            sequences_list = []\n+            for d in current_segment_list:\n+                if skip_ending_double_timestamps and len(d[\"tokens\"]) > 2 and d[\"tokens\"][-2] >= timestamp_begin:\n+                    # the segment finishes with two timestamp tokens\n+                    # we need to ignore the last timestamp token\n+                    # see https://github.com/huggingface/transformers/pull/34537\n+                    sequences_list.append(d[\"tokens\"][:-1])\n+                else:\n+                    sequences_list.append(d[\"tokens\"])\n+            sequence = torch.cat(sequences_list, dim=-1)\n+\n             if return_token_timestamps:\n                 token_timestamps = torch.cat(\n                     [d[\"result\"][\"token_timestamps\"][d[\"idxs\"][0] : d[\"idxs\"][1]] for d in current_segment_list],\n@@ -1809,14 +1830,6 @@ def _prepare_decoder_input_ids(\n             # according to https://github.com/openai/whisper/blob/e58f28804528831904c3b6f2c0e473f346223433/whisper/decoding.py#L609\n             active_segments = [current_segments[i] if do_condition_on_prev_tokens[i] else None for i in batch_idx_map]\n \n-            for segments in active_segments:\n-                for seg in segments:\n-                    if len(seg[\"tokens\"]) > 2 and seg[\"tokens\"][-2] >= timestamp_begin:\n-                        # the segment finishes with two timestamp tokens\n-                        # we need to ignore the last timestamp token\n-                        # see https://github.com/huggingface/transformers/pull/34537\n-                        seg[\"tokens\"] = seg[\"tokens\"][:-1]\n-\n             if prompt_ids is not None and generation_config.prompt_condition_type == \"all-segments\":\n                 prev_ids = prompt_ids\n             else:\n@@ -1833,6 +1846,8 @@ def _prepare_decoder_input_ids(\n                 padding=padding,\n                 bos_token_tensor=prev_ids,\n                 cut_off_length=cut_off_length,\n+                skip_ending_double_timestamps=True,\n+                timestamp_begin=timestamp_begin,\n             )\n             decoder_input_ids = torch.cat([prev_tokens, decoder_input_ids], dim=-1)\n "
        },
        {
            "sha": "44f8a745fd071161bbe97c0ebe528b895f4ee2f9",
            "filename": "src/transformers/models/whisper/tokenization_whisper.py",
            "status": "modified",
            "additions": 25,
            "deletions": 2,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/cfff7ca9a27280338c6a57dfa7722dcf44f51a87/src%2Ftransformers%2Fmodels%2Fwhisper%2Ftokenization_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cfff7ca9a27280338c6a57dfa7722dcf44f51a87/src%2Ftransformers%2Fmodels%2Fwhisper%2Ftokenization_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Ftokenization_whisper.py?ref=cfff7ca9a27280338c6a57dfa7722dcf44f51a87",
            "patch": "@@ -910,7 +910,7 @@ def _convert_to_list(token_ids):\n         return token_ids\n \n \n-def _decode_asr(tokenizer, model_outputs, *, return_timestamps, return_language, time_precision):\n+def _decode_asr(tokenizer, model_outputs, *, return_timestamps, return_language, time_precision, segment_size=1500):\n     \"\"\"\n     Internal method meant to only be used by asr pipeline. Handles all the little quirks specific to whisper to handle\n     the various options not allowed in other seq2seq models\n@@ -962,6 +962,12 @@ def new_chunk():\n         last_timestamp = None\n         first_timestamp = timestamp_begin\n \n+        # long form generation: we need to handle the case where the call to generate returns concatenated segments,\n+        # with underlying multiple calls to generate\n+        cur_max_timestamp = 0.0\n+        prev_segments_len = 0.0\n+        penultimate_timestamp = 0.0\n+\n         if \"stride\" in output:\n             chunk_len, stride_left, stride_right = output[\"stride\"]\n             # Offset the timings to account for the other `model_outputs`.\n@@ -1024,7 +1030,24 @@ def new_chunk():\n                     pass\n             elif token >= timestamp_begin:\n                 # 3/ Timestamp token\n-                time = (token - timestamp_begin) * time_precision + time_offset\n+\n+                timestamp = float((token - timestamp_begin) * time_precision)\n+                if timestamp < cur_max_timestamp:\n+                    # next segment has started\n+                    last_was_single_ending = i >= 2 and not (\n+                        token_ids[i - 1] >= timestamp_begin and token_ids[i - 2] >= timestamp_begin\n+                    )\n+                    if last_was_single_ending:\n+                        prev_segments_len += time_precision * segment_size\n+                    else:\n+                        cur_max_timestamp = penultimate_timestamp\n+                        prev_segments_len += penultimate_timestamp\n+\n+                penultimate_timestamp = cur_max_timestamp\n+                cur_max_timestamp = timestamp\n+\n+                time = (token - timestamp_begin) * time_precision + time_offset + prev_segments_len\n+\n                 time = round(time, 2)\n                 if last_timestamp and token >= last_timestamp:\n                     # Whisper outputted a timestamp token, but it falls within"
        },
        {
            "sha": "41ca3b66ac55561c9316cb8a6f9982eebdbdb695",
            "filename": "src/transformers/pipelines/automatic_speech_recognition.py",
            "status": "modified",
            "additions": 10,
            "deletions": 3,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/cfff7ca9a27280338c6a57dfa7722dcf44f51a87/src%2Ftransformers%2Fpipelines%2Fautomatic_speech_recognition.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cfff7ca9a27280338c6a57dfa7722dcf44f51a87/src%2Ftransformers%2Fpipelines%2Fautomatic_speech_recognition.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fautomatic_speech_recognition.py?ref=cfff7ca9a27280338c6a57dfa7722dcf44f51a87",
            "patch": "@@ -283,13 +283,20 @@ def _sanitize_parameters(\n         # No parameters on this pipeline right now\n         preprocess_params = {}\n         if chunk_length_s is not None:\n-            if self.type == \"seq2seq\" and not ignore_warning:\n-                logger.warning(\n+            if self.type in [\"seq2seq\", \"seq2seq_whisper\"] and not ignore_warning:\n+                type_warning = (\n                     \"Using `chunk_length_s` is very experimental with seq2seq models. The results will not necessarily\"\n                     \" be entirely accurate and will have caveats. More information:\"\n                     \" https://github.com/huggingface/transformers/pull/20104. Ignore this warning with pipeline(...,\"\n-                    \" ignore_warning=True)\"\n+                    \" ignore_warning=True).\"\n                 )\n+                if self.type == \"seq2seq_whisper\":\n+                    type_warning += (\n+                        \" To use Whisper for long-form transcription, use rather the model's `generate` method directly \"\n+                        \"as the model relies on it's own chunking mechanism (cf. Whisper original paper, section 3.8. \"\n+                        \"Long-form Transcription).\"\n+                    )\n+                logger.warning(type_warning)\n             preprocess_params[\"chunk_length_s\"] = chunk_length_s\n         if stride_length_s is not None:\n             preprocess_params[\"stride_length_s\"] = stride_length_s"
        },
        {
            "sha": "dbb241f5ad4bef2f47417ec4ca4e89a6e0487700",
            "filename": "tests/models/whisper/test_modeling_whisper.py",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/cfff7ca9a27280338c6a57dfa7722dcf44f51a87/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cfff7ca9a27280338c6a57dfa7722dcf44f51a87/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py?ref=cfff7ca9a27280338c6a57dfa7722dcf44f51a87",
            "patch": "@@ -2031,11 +2031,13 @@ def test_large_timestamp_generation(self):\n         ).input_features\n         input_features = input_features.to(torch_device)\n \n-        generated_ids = model.generate(input_features, max_length=448, return_timestamps=True).to(\"cpu\")\n+        generated_ids = model.generate(\n+            input_features, max_length=448, return_timestamps=True, condition_on_prev_tokens=True\n+        ).to(\"cpu\")\n \n         # fmt: off\n         EXPECTED_OUTPUT = torch.tensor([\n-            50365, 2221, 13, 2326, 388, 391, 307, 264, 50244, 295, 264, 2808, 5359, 11, 293, 321, 366, 5404, 281, 2928, 702, 14943, 13, 50629, 50682, 6966, 307, 2221, 13, 2326, 388, 391, 311, 9060, 1570, 1880, 813, 702, 1871, 13, 50870, 50911, 634, 5112, 505, 300, 412, 341, 42729, 3196, 295, 264, 1064, 11, 365, 5272, 293, 12904, 9256, 450, 10539, 949, 505, 11, 51245, 51287, 1034, 4680, 10117, 490, 3936, 293, 1080, 3542, 5160, 881, 26336, 281, 264, 1575, 13, 51494, 51523, 634, 575, 12525, 22618, 1968, 6144, 35617, 1456, 397, 266, 311, 589, 307, 534, 10281, 934, 439, 11, 51799, 51815, 50365, 293, 393, 4411, 50430\n+            [50365, 2221, 13, 2326, 388, 391, 307, 264, 50244, 295, 264, 2808, 5359, 11, 293, 321, 366, 5404, 281, 2928, 702, 14943, 13, 50629, 50682, 6966, 307, 2221, 13, 2326, 388, 391, 311, 9060, 1570, 1880, 813, 702, 1871, 13, 50870, 50911, 634, 5112, 505, 300, 412, 341, 42729, 3196, 295, 264, 1064, 11, 365, 5272, 293, 12904, 9256, 450, 10539, 949, 505, 11, 51245, 51287, 1034, 4680, 10117, 490, 3936, 293, 1080, 3542, 5160, 881, 26336, 281, 264, 1575, 13, 51494, 51523, 634, 575, 12525, 22618, 1968, 6144, 35617, 1456, 397, 266, 311, 589, 307, 534, 10281, 934, 439, 11, 51799, 51815, 50365, 293, 393, 4411, 50431]\n         ])\n         # fmt: on\n         torch.testing.assert_close(generated_ids[0], EXPECTED_OUTPUT)\n@@ -2078,7 +2080,7 @@ def test_large_timestamp_generation(self):\n                     },\n                     {\n                         \"text\": (\" and can discover\"),\n-                        \"timestamp\": (28.68, 29.98),\n+                        \"timestamp\": (28.68, 30.0),\n                     },\n                 ],\n             }"
        }
    ],
    "stats": {
        "total": 81,
        "additions": 64,
        "deletions": 17
    }
}