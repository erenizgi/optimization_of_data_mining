{
    "author": "alex-jw-brooks",
    "message": "Update Granite Vision Model Path / Tests (#35998)\n\n* Update granite vision model path\r\n\r\nSigned-off-by: Alex-Brooks <Alex.brooks@ibm.com>\r\n\r\n* Enable granite vision test\r\n\r\nSigned-off-by: Alex-Brooks <Alex.brooks@ibm.com>\r\n\r\n---------\r\n\r\nSigned-off-by: Alex-Brooks <Alex.brooks@ibm.com>",
    "sha": "e284c7e954abe12c34b50461c17f8115a0afe115",
    "files": [
        {
            "sha": "e11c806ae6722b8d454c60936f09974a53a38826",
            "filename": "docs/source/en/model_doc/granitevision.md",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/e284c7e954abe12c34b50461c17f8115a0afe115/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranitevision.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e284c7e954abe12c34b50461c17f8115a0afe115/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranitevision.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranitevision.md?ref=e284c7e954abe12c34b50461c17f8115a0afe115",
            "patch": "@@ -31,13 +31,8 @@ Tips:\n Sample inference:\n ```python\n from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\n-from PIL import Image\n-import requests\n \n-# Note: These docs were written prior to the public model release,\n-# and this path is subject to change.\n-# Please see https://huggingface.co/ibm-granite for the current model list.\n-model_path = \"ibm-granite/granite-3.1-2b-instruct-vision\"\n+model_path = \"ibm-granite/granite-vision-3.1-2b-preview\"\n processor = LlavaNextProcessor.from_pretrained(model_path)\n \n model = LlavaNextForConditionalGeneration.from_pretrained(model_path).to(\"cuda\")"
        },
        {
            "sha": "acfd3fde6314a3aa0d8554a5483eafd4deff0145",
            "filename": "tests/models/llava_next/test_modeling_llava_next.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/e284c7e954abe12c34b50461c17f8115a0afe115/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e284c7e954abe12c34b50461c17f8115a0afe115/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py?ref=e284c7e954abe12c34b50461c17f8115a0afe115",
            "patch": "@@ -586,23 +586,21 @@ def test_small_model_integration_test_full_vision_state_selection(self):\n             EXPECTED_DECODED_TEXT,\n         )\n \n-    @unittest.skip(reason=\"Granite multimodal [vision] models are not yet released\")\n     @slow\n     def test_granite_vision(self):\n         \"\"\"\n         Check the expected output of a granite vision model, which leverages\n         multiple vision feature layers and a visual encoder with no CLS (siglip).\n         \"\"\"\n-        # TODO @alex-jw-brooks - update the path and enable this test once the 2b model is released\n-        granite_model_path = \"llava-granite-2b\"\n+        granite_model_path = \"ibm-granite/granite-vision-3.1-2b-preview\"\n         model = LlavaNextForConditionalGeneration.from_pretrained(granite_model_path)\n         self.processor = AutoProcessor.from_pretrained(granite_model_path)\n         prompt = \"<|user|>\\n<image>\\nWhat is shown in this image?\\n<|assistant|>\\n\"\n         inputs = self.processor(prompt, self.image, return_tensors=\"pt\").to(model.device)\n \n         # verify generation\n         output = model.generate(**inputs, max_new_tokens=30)\n-        EXPECTED_DECODED_TEXT = \"<|user|>\\n\\nWhat is shown in this image?\\n<|assistant|>\\nThe image depicts a diagram.\"\n+        EXPECTED_DECODED_TEXT = \"<|user|>\\n\\nWhat is shown in this image?\\n<|assistant|>\\nThe image displays a radar chart comparing the performance of various machine learning models.\"  # fmt: skip\n         self.assertEqual(\n             self.processor.decode(output[0], skip_special_tokens=True),\n             EXPECTED_DECODED_TEXT,"
        }
    ],
    "stats": {
        "total": 13,
        "additions": 3,
        "deletions": 10
    }
}