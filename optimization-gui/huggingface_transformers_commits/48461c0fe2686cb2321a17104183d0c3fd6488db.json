{
    "author": "qubvel",
    "message": "Make `pipeline` able to load `processor` (#32514)\n\n* Refactor get_test_pipeline\r\n\r\n* Fixup\r\n\r\n* Fixing tests\r\n\r\n* Add processor loading in tests\r\n\r\n* Restructure processors loading\r\n\r\n* Add processor to the pipeline\r\n\r\n* Move model loading on tom of the test\r\n\r\n* Update `get_test_pipeline`\r\n\r\n* Fixup\r\n\r\n* Add class-based flags for loading processors\r\n\r\n* Change `is_pipeline_test_to_skip` signature\r\n\r\n* Skip t5 failing test for slow tokenizer\r\n\r\n* Fixup\r\n\r\n* Fix copies for T5\r\n\r\n* Fix typo\r\n\r\n* Add try/except for tokenizer loading (kosmos-2 case)\r\n\r\n* Fixup\r\n\r\n* Llama not fails for long generation\r\n\r\n* Revert processor pass in text-generation test\r\n\r\n* Fix docs\r\n\r\n* Switch back to json file for image processors and feature extractors\r\n\r\n* Add processor type check\r\n\r\n* Remove except for tokenizers\r\n\r\n* Fix docstring\r\n\r\n* Fix empty lists for tests\r\n\r\n* Fixup\r\n\r\n* Fix load check\r\n\r\n* Ensure we have non-empty test cases\r\n\r\n* Update src/transformers/pipelines/__init__.py\r\n\r\nCo-authored-by: Lysandre Debut <hi@lysand.re>\r\n\r\n* Update src/transformers/pipelines/base.py\r\n\r\nCo-authored-by: Lysandre Debut <hi@lysand.re>\r\n\r\n* Rework comment\r\n\r\n* Better docs, add note about pipeline components\r\n\r\n* Change warning to error raise\r\n\r\n* Fixup\r\n\r\n* Refine pipeline docs\r\n\r\n---------\r\n\r\nCo-authored-by: Lysandre Debut <hi@lysand.re>",
    "sha": "48461c0fe2686cb2321a17104183d0c3fd6488db",
    "files": [
        {
            "sha": "40b3dc1015c001be3fc0c46b444a5fa9009f9460",
            "filename": "src/transformers/pipelines/__init__.py",
            "status": "modified",
            "additions": 71,
            "deletions": 9,
            "changes": 80,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/src%2Ftransformers%2Fpipelines%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/src%2Ftransformers%2Fpipelines%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2F__init__.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -28,7 +28,9 @@\n from ..models.auto.feature_extraction_auto import FEATURE_EXTRACTOR_MAPPING, AutoFeatureExtractor\n from ..models.auto.image_processing_auto import IMAGE_PROCESSOR_MAPPING, AutoImageProcessor\n from ..models.auto.modeling_auto import AutoModelForDepthEstimation, AutoModelForImageToImage\n+from ..models.auto.processing_auto import PROCESSOR_MAPPING, AutoProcessor\n from ..models.auto.tokenization_auto import TOKENIZER_MAPPING, AutoTokenizer\n+from ..processing_utils import ProcessorMixin\n from ..tokenization_utils import PreTrainedTokenizer\n from ..utils import (\n     CONFIG_NAME,\n@@ -556,6 +558,7 @@ def pipeline(\n     tokenizer: Optional[Union[str, PreTrainedTokenizer, \"PreTrainedTokenizerFast\"]] = None,\n     feature_extractor: Optional[Union[str, PreTrainedFeatureExtractor]] = None,\n     image_processor: Optional[Union[str, BaseImageProcessor]] = None,\n+    processor: Optional[Union[str, ProcessorMixin]] = None,\n     framework: Optional[str] = None,\n     revision: Optional[str] = None,\n     use_fast: bool = True,\n@@ -571,11 +574,19 @@ def pipeline(\n     \"\"\"\n     Utility factory method to build a [`Pipeline`].\n \n-    Pipelines are made of:\n+    A pipeline consists of:\n \n-        - A [tokenizer](tokenizer) in charge of mapping raw textual input to token.\n-        - A [model](model) to make predictions from the inputs.\n-        - Some (optional) post processing for enhancing model's output.\n+        - One or more components for pre-processing model inputs, such as a [tokenizer](tokenizer),\n+        [image_processor](image_processor), [feature_extractor](feature_extractor), or [processor](processors).\n+        - A [model](model) that generates predictions from the inputs.\n+        - Optional post-processing steps to refine the model's output, which can also be handled by processors.\n+\n+    <Tip>\n+    While there are such optional arguments as `tokenizer`, `feature_extractor`, `image_processor`, and `processor`,\n+    they shouldn't be specified all at once. If these components are not provided, `pipeline` will try to load\n+    required ones automatically. In case you want to provide these components explicitly, please refer to a\n+    specific pipeline in order to get more details regarding what components are required.\n+    </Tip>\n \n     Args:\n         task (`str`):\n@@ -644,6 +655,25 @@ def pipeline(\n             `model` is not specified or not a string, then the default feature extractor for `config` is loaded (if it\n             is a string). However, if `config` is also not given or not a string, then the default feature extractor\n             for the given `task` will be loaded.\n+        image_processor (`str` or [`BaseImageProcessor`], *optional*):\n+            The image processor that will be used by the pipeline to preprocess images for the model. This can be a\n+            model identifier or an actual image processor inheriting from [`BaseImageProcessor`].\n+\n+            Image processors are used for Vision models and multi-modal models that require image inputs. Multi-modal\n+            models will also require a tokenizer to be passed.\n+\n+            If not provided, the default image processor for the given `model` will be loaded (if it is a string). If\n+            `model` is not specified or not a string, then the default image processor for `config` is loaded (if it is\n+            a string).\n+        processor (`str` or [`ProcessorMixin`], *optional*):\n+            The processor that will be used by the pipeline to preprocess data for the model. This can be a model\n+            identifier or an actual processor inheriting from [`ProcessorMixin`].\n+\n+            Processors are used for multi-modal models that require multi-modal inputs, for example, a model that\n+            requires both text and image inputs.\n+\n+            If not provided, the default processor for the given `model` will be loaded (if it is a string). If `model`\n+            is not specified or not a string, then the default processor for `config` is loaded (if it is a string).\n         framework (`str`, *optional*):\n             The framework to use, either `\"pt\"` for PyTorch or `\"tf\"` for TensorFlow. The specified framework must be\n             installed.\n@@ -905,13 +935,17 @@ def pipeline(\n \n     model_config = model.config\n     hub_kwargs[\"_commit_hash\"] = model.config._commit_hash\n-    load_tokenizer = (\n-        type(model_config) in TOKENIZER_MAPPING\n-        or model_config.tokenizer_class is not None\n-        or isinstance(tokenizer, str)\n-    )\n+\n+    load_tokenizer = type(model_config) in TOKENIZER_MAPPING or model_config.tokenizer_class is not None\n     load_feature_extractor = type(model_config) in FEATURE_EXTRACTOR_MAPPING or feature_extractor is not None\n     load_image_processor = type(model_config) in IMAGE_PROCESSOR_MAPPING or image_processor is not None\n+    load_processor = type(model_config) in PROCESSOR_MAPPING or processor is not None\n+\n+    # Check that pipeline class required loading\n+    load_tokenizer = load_tokenizer and pipeline_class._load_tokenizer\n+    load_feature_extractor = load_feature_extractor and pipeline_class._load_feature_extractor\n+    load_image_processor = load_image_processor and pipeline_class._load_image_processor\n+    load_processor = load_processor and pipeline_class._load_processor\n \n     # If `model` (instance of `PretrainedModel` instead of `str`) is passed (and/or same for config), while\n     # `image_processor` or `feature_extractor` is `None`, the loading will fail. This happens particularly for some\n@@ -1074,6 +1108,31 @@ def pipeline(\n                     if not is_pyctcdecode_available():\n                         logger.warning(\"Try to install `pyctcdecode`: `pip install pyctcdecode\")\n \n+    if load_processor:\n+        # Try to infer processor from model or config name (if provided as str)\n+        if processor is None:\n+            if isinstance(model_name, str):\n+                processor = model_name\n+            elif isinstance(config, str):\n+                processor = config\n+            else:\n+                # Impossible to guess what is the right processor here\n+                raise Exception(\n+                    \"Impossible to guess which processor to use. \"\n+                    \"Please provide a processor instance or a path/identifier \"\n+                    \"to a processor.\"\n+                )\n+\n+        # Instantiate processor if needed\n+        if isinstance(processor, (str, tuple)):\n+            processor = AutoProcessor.from_pretrained(processor, _from_pipeline=task, **hub_kwargs, **model_kwargs)\n+            if not isinstance(processor, ProcessorMixin):\n+                raise TypeError(\n+                    \"Processor was loaded, but it is not an instance of `ProcessorMixin`. \"\n+                    f\"Got type `{type(processor)}` instead. Please check that you specified \"\n+                    \"correct pipeline task for the model and model has processor implemented and saved.\"\n+                )\n+\n     if task == \"translation\" and model.config.task_specific_params:\n         for key in model.config.task_specific_params:\n             if key.startswith(\"translation\"):\n@@ -1099,4 +1158,7 @@ def pipeline(\n     if device is not None:\n         kwargs[\"device\"] = device\n \n+    if processor is not None:\n+        kwargs[\"processor\"] = processor\n+\n     return pipeline_class(model=model, framework=framework, task=task, **kwargs)"
        },
        {
            "sha": "042958cbb0c6b3a41d49d1c1f056ae4157e21501",
            "filename": "src/transformers/pipelines/base.py",
            "status": "modified",
            "additions": 36,
            "deletions": 2,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/src%2Ftransformers%2Fpipelines%2Fbase.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/src%2Ftransformers%2Fpipelines%2Fbase.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fbase.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -34,6 +34,7 @@\n from ..image_processing_utils import BaseImageProcessor\n from ..modelcard import ModelCard\n from ..models.auto.configuration_auto import AutoConfig\n+from ..processing_utils import ProcessorMixin\n from ..tokenization_utils import PreTrainedTokenizer\n from ..utils import (\n     ModelOutput,\n@@ -716,6 +717,7 @@ def build_pipeline_init_args(\n     has_tokenizer: bool = False,\n     has_feature_extractor: bool = False,\n     has_image_processor: bool = False,\n+    has_processor: bool = False,\n     supports_binary_output: bool = True,\n ) -> str:\n     docstring = r\"\"\"\n@@ -738,6 +740,12 @@ def build_pipeline_init_args(\n         image_processor ([`BaseImageProcessor`]):\n             The image processor that will be used by the pipeline to encode data for the model. This object inherits from\n             [`BaseImageProcessor`].\"\"\"\n+    if has_processor:\n+        docstring += r\"\"\"\n+        processor ([`ProcessorMixin`]):\n+            The processor that will be used by the pipeline to encode data for the model. This object inherits from\n+            [`ProcessorMixin`]. Processor is a composite object that might contain `tokenizer`, `feature_extractor`, and\n+            `image_processor`.\"\"\"\n     docstring += r\"\"\"\n         modelcard (`str` or [`ModelCard`], *optional*):\n             Model card attributed to the model for this pipeline.\n@@ -774,7 +782,11 @@ def build_pipeline_init_args(\n \n \n PIPELINE_INIT_ARGS = build_pipeline_init_args(\n-    has_tokenizer=True, has_feature_extractor=True, has_image_processor=True, supports_binary_output=True\n+    has_tokenizer=True,\n+    has_feature_extractor=True,\n+    has_image_processor=True,\n+    has_processor=True,\n+    supports_binary_output=True,\n )\n \n \n@@ -787,7 +799,11 @@ def build_pipeline_init_args(\n     )\n \n \n-@add_end_docstrings(build_pipeline_init_args(has_tokenizer=True, has_feature_extractor=True, has_image_processor=True))\n+@add_end_docstrings(\n+    build_pipeline_init_args(\n+        has_tokenizer=True, has_feature_extractor=True, has_image_processor=True, has_processor=True\n+    )\n+)\n class Pipeline(_ScikitCompat, PushToHubMixin):\n     \"\"\"\n     The Pipeline class is the class from which all pipelines inherit. Refer to this class for methods shared across\n@@ -805,6 +821,22 @@ class Pipeline(_ScikitCompat, PushToHubMixin):\n     constructor argument. If set to `True`, the output will be stored in the pickle format.\n     \"\"\"\n \n+    # Historically we have pipelines working with `tokenizer`, `feature_extractor`, and `image_processor`\n+    # as separate processing components. While we have `processor` class that combines them, some pipelines\n+    # might still operate with these components separately.\n+    # With the addition of `processor` to `pipeline`, we want to avoid:\n+    #  - loading `processor` for pipelines that still work with `image_processor` and `tokenizer` separately;\n+    #  - loading `image_processor`/`tokenizer` as a separate component while we operate only with `processor`,\n+    #    because `processor` will load required sub-components by itself.\n+    # Below flags allow granular control over loading components and set to be backward compatible with current\n+    # pipelines logic. You may override these flags when creating your pipeline. For example, for\n+    # `zero-shot-object-detection` pipeline which operates with `processor` you should set `_load_processor=True`\n+    # and all the rest flags to `False` to avoid unnecessary loading of the components.\n+    _load_processor = False\n+    _load_image_processor = True\n+    _load_feature_extractor = True\n+    _load_tokenizer = True\n+\n     default_input_names = None\n \n     def __init__(\n@@ -813,6 +845,7 @@ def __init__(\n         tokenizer: Optional[PreTrainedTokenizer] = None,\n         feature_extractor: Optional[PreTrainedFeatureExtractor] = None,\n         image_processor: Optional[BaseImageProcessor] = None,\n+        processor: Optional[ProcessorMixin] = None,\n         modelcard: Optional[ModelCard] = None,\n         framework: Optional[str] = None,\n         task: str = \"\",\n@@ -830,6 +863,7 @@ def __init__(\n         self.tokenizer = tokenizer\n         self.feature_extractor = feature_extractor\n         self.image_processor = image_processor\n+        self.processor = processor\n         self.modelcard = modelcard\n         self.framework = framework\n "
        },
        {
            "sha": "0175e562eda618c090ec6914150bae3a5fc9823f",
            "filename": "tests/models/altclip/test_modeling_altclip.py",
            "status": "modified",
            "additions": 9,
            "deletions": 2,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Faltclip%2Ftest_modeling_altclip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Faltclip%2Ftest_modeling_altclip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faltclip%2Ftest_modeling_altclip.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -436,9 +436,16 @@ class AltCLIPModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase)\n \n     # TODO: Fix the failed tests when this model gets more usage\n     def is_pipeline_test_to_skip(\n-        self, pipeline_test_casse_name, config_class, model_architecture, tokenizer_name, processor_name\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n     ):\n-        if pipeline_test_casse_name == \"FeatureExtractionPipelineTests\":\n+        if pipeline_test_case_name == \"FeatureExtractionPipelineTests\":\n             return True\n \n         return False"
        },
        {
            "sha": "fb90fec6afff09b4b17a875d8fd898a519f0f6da",
            "filename": "tests/models/audio_spectrogram_transformer/test_modeling_audio_spectrogram_transformer.py",
            "status": "modified",
            "additions": 9,
            "deletions": 2,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Faudio_spectrogram_transformer%2Ftest_modeling_audio_spectrogram_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Faudio_spectrogram_transformer%2Ftest_modeling_audio_spectrogram_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faudio_spectrogram_transformer%2Ftest_modeling_audio_spectrogram_transformer.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -167,9 +167,16 @@ class ASTModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n \n     # TODO: Fix the failed tests when this model gets more usage\n     def is_pipeline_test_to_skip(\n-        self, pipeline_test_casse_name, config_class, model_architecture, tokenizer_name, processor_name\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n     ):\n-        if pipeline_test_casse_name == \"AudioClassificationPipelineTests\":\n+        if pipeline_test_case_name == \"AudioClassificationPipelineTests\":\n             return True\n \n         return False"
        },
        {
            "sha": "745283825f0cf62e70c50c559ed360f6894ee10e",
            "filename": "tests/models/bigbird_pegasus/test_modeling_bigbird_pegasus.py",
            "status": "modified",
            "additions": 9,
            "deletions": 2,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fbigbird_pegasus%2Ftest_modeling_bigbird_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fbigbird_pegasus%2Ftest_modeling_bigbird_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbigbird_pegasus%2Ftest_modeling_bigbird_pegasus.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -276,9 +276,16 @@ class BigBirdPegasusModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineT\n \n     # TODO: Fix the failed tests\n     def is_pipeline_test_to_skip(\n-        self, pipeline_test_casse_name, config_class, model_architecture, tokenizer_name, processor_name\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n     ):\n-        if pipeline_test_casse_name == \"QAPipelineTests\" and not tokenizer_name.endswith(\"Fast\"):\n+        if pipeline_test_case_name == \"QAPipelineTests\" and not tokenizer_name.endswith(\"Fast\"):\n             return True\n \n         return False"
        },
        {
            "sha": "933d8cccc8c1463a7e13510ef6ba8a3fed77d8ce",
            "filename": "tests/models/blenderbot_small/test_modeling_blenderbot_small.py",
            "status": "modified",
            "additions": 9,
            "deletions": 2,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fblenderbot_small%2Ftest_modeling_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fblenderbot_small%2Ftest_modeling_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblenderbot_small%2Ftest_modeling_blenderbot_small.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -236,9 +236,16 @@ class BlenderbotSmallModelTest(ModelTesterMixin, GenerationTesterMixin, Pipeline\n \n     # TODO: Fix the failed tests when this model gets more usage\n     def is_pipeline_test_to_skip(\n-        self, pipeline_test_casse_name, config_class, model_architecture, tokenizer_name, processor_name\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n     ):\n-        return pipeline_test_casse_name == \"TextGenerationPipelineTests\"\n+        return pipeline_test_case_name == \"TextGenerationPipelineTests\"\n \n     def setUp(self):\n         self.model_tester = BlenderbotSmallModelTester(self)"
        },
        {
            "sha": "2d993ad8b411a19da7fce4ab0e45afb53dbec6d2",
            "filename": "tests/models/blenderbot_small/test_modeling_flax_blenderbot_small.py",
            "status": "modified",
            "additions": 9,
            "deletions": 2,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fblenderbot_small%2Ftest_modeling_flax_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fblenderbot_small%2Ftest_modeling_flax_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblenderbot_small%2Ftest_modeling_flax_blenderbot_small.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -321,9 +321,16 @@ class FlaxBlenderbotSmallModelTest(FlaxModelTesterMixin, unittest.TestCase, Flax\n     all_generative_model_classes = (FlaxBlenderbotSmallForConditionalGeneration,) if is_flax_available() else ()\n \n     def is_pipeline_test_to_skip(\n-        self, pipeline_test_casse_name, config_class, model_architecture, tokenizer_name, processor_name\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n     ):\n-        return pipeline_test_casse_name == \"TextGenerationPipelineTests\"\n+        return pipeline_test_case_name == \"TextGenerationPipelineTests\"\n \n     def setUp(self):\n         self.model_tester = FlaxBlenderbotSmallModelTester(self)"
        },
        {
            "sha": "f70637df2dc248a4133e9af5e0d2dd8b00298bfa",
            "filename": "tests/models/blenderbot_small/test_modeling_tf_blenderbot_small.py",
            "status": "modified",
            "additions": 9,
            "deletions": 2,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fblenderbot_small%2Ftest_modeling_tf_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fblenderbot_small%2Ftest_modeling_tf_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblenderbot_small%2Ftest_modeling_tf_blenderbot_small.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -198,9 +198,16 @@ class TFBlenderbotSmallModelTest(TFModelTesterMixin, PipelineTesterMixin, unitte\n     test_onnx = False\n \n     def is_pipeline_test_to_skip(\n-        self, pipeline_test_casse_name, config_class, model_architecture, tokenizer_name, processor_name\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n     ):\n-        return pipeline_test_casse_name == \"TextGenerationPipelineTests\"\n+        return pipeline_test_case_name == \"TextGenerationPipelineTests\"\n \n     def setUp(self):\n         self.model_tester = TFBlenderbotSmallModelTester(self)"
        },
        {
            "sha": "b16a37a226cd135359a6d30dd6ed5a15ccc67d81",
            "filename": "tests/models/bros/test_modeling_bros.py",
            "status": "modified",
            "additions": 8,
            "deletions": 1,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fbros%2Ftest_modeling_bros.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fbros%2Ftest_modeling_bros.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbros%2Ftest_modeling_bros.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -295,7 +295,14 @@ class BrosModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     # BROS requires `bbox` in the inputs which doesn't fit into the above 2 pipelines' input formats.\n     # see https://github.com/huggingface/transformers/pull/26294\n     def is_pipeline_test_to_skip(\n-        self, pipeline_test_casse_name, config_class, model_architecture, tokenizer_name, processor_name\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n     ):\n         return True\n "
        },
        {
            "sha": "870a932cb303ab94f8fad8d7c9a80d822d7b61d4",
            "filename": "tests/models/cpm/test_tokenization_cpm.py",
            "status": "modified",
            "additions": 8,
            "deletions": 1,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fcpm%2Ftest_tokenization_cpm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fcpm%2Ftest_tokenization_cpm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcpm%2Ftest_tokenization_cpm.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -22,7 +22,14 @@\n class CpmTokenizationTest(unittest.TestCase):\n     # There is no `CpmModel`\n     def is_pipeline_test_to_skip(\n-        self, pipeline_test_casse_name, config_class, model_architecture, tokenizer_name, processor_name\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n     ):\n         return True\n "
        },
        {
            "sha": "a9bdddd7bfe25eb9655ef4e42d575de46d13f121",
            "filename": "tests/models/ctrl/test_modeling_ctrl.py",
            "status": "modified",
            "additions": 9,
            "deletions": 2,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fctrl%2Ftest_modeling_ctrl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fctrl%2Ftest_modeling_ctrl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fctrl%2Ftest_modeling_ctrl.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -211,9 +211,16 @@ class CTRLModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin\n \n     # TODO: Fix the failed tests\n     def is_pipeline_test_to_skip(\n-        self, pipeline_test_casse_name, config_class, model_architecture, tokenizer_name, processor_name\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n     ):\n-        if pipeline_test_casse_name == \"ZeroShotClassificationPipelineTests\":\n+        if pipeline_test_case_name == \"ZeroShotClassificationPipelineTests\":\n             # Get `tokenizer does not have a padding token` error for both fast/slow tokenizers.\n             # `CTRLConfig` was never used in pipeline tests, either because of a missing checkpoint or because a tiny\n             # config could not be created."
        },
        {
            "sha": "8a08fe0798849da20a06fdaaa3711e523375e932",
            "filename": "tests/models/ctrl/test_modeling_tf_ctrl.py",
            "status": "modified",
            "additions": 9,
            "deletions": 2,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fctrl%2Ftest_modeling_tf_ctrl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fctrl%2Ftest_modeling_tf_ctrl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fctrl%2Ftest_modeling_tf_ctrl.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -189,9 +189,16 @@ class TFCTRLModelTest(TFModelTesterMixin, PipelineTesterMixin, unittest.TestCase\n \n     # TODO: Fix the failed tests\n     def is_pipeline_test_to_skip(\n-        self, pipeline_test_casse_name, config_class, model_architecture, tokenizer_name, processor_name\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n     ):\n-        if pipeline_test_casse_name == \"ZeroShotClassificationPipelineTests\":\n+        if pipeline_test_case_name == \"ZeroShotClassificationPipelineTests\":\n             # Get `tokenizer does not have a padding token` error for both fast/slow tokenizers.\n             # `CTRLConfig` was never used in pipeline tests, either because of a missing checkpoint or because a tiny\n             # config could not be created."
        },
        {
            "sha": "a1a2b0155cb7387e2fd7cd3ead8f954ffb41d641",
            "filename": "tests/models/falcon/test_modeling_falcon.py",
            "status": "modified",
            "additions": 8,
            "deletions": 1,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Ffalcon%2Ftest_modeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Ffalcon%2Ftest_modeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffalcon%2Ftest_modeling_falcon.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -312,7 +312,14 @@ class FalconModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMix\n \n     # TODO (ydshieh): Check this. See https://app.circleci.com/pipelines/github/huggingface/transformers/79245/workflows/9490ef58-79c2-410d-8f51-e3495156cf9c/jobs/1012146\n     def is_pipeline_test_to_skip(\n-        self, pipeline_test_casse_name, config_class, model_architecture, tokenizer_name, processor_name\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n     ):\n         return True\n "
        },
        {
            "sha": "396d02da956be4ee0cc1e939bea619dfc94d3f71",
            "filename": "tests/models/flaubert/test_modeling_flaubert.py",
            "status": "modified",
            "additions": 9,
            "deletions": 2,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fflaubert%2Ftest_modeling_flaubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fflaubert%2Ftest_modeling_flaubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fflaubert%2Ftest_modeling_flaubert.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -392,10 +392,17 @@ class FlaubertModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase\n \n     # TODO: Fix the failed tests\n     def is_pipeline_test_to_skip(\n-        self, pipeline_test_casse_name, config_class, model_architecture, tokenizer_name, processor_name\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n     ):\n         if (\n-            pipeline_test_casse_name == \"QAPipelineTests\"\n+            pipeline_test_case_name == \"QAPipelineTests\"\n             and tokenizer_name is not None\n             and not tokenizer_name.endswith(\"Fast\")\n         ):"
        },
        {
            "sha": "40b9c1c171f364043fb90a30d8953f0ea3955d61",
            "filename": "tests/models/flaubert/test_modeling_tf_flaubert.py",
            "status": "modified",
            "additions": 9,
            "deletions": 2,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fflaubert%2Ftest_modeling_tf_flaubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fflaubert%2Ftest_modeling_tf_flaubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fflaubert%2Ftest_modeling_tf_flaubert.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -309,10 +309,17 @@ class TFFlaubertModelTest(TFModelTesterMixin, PipelineTesterMixin, unittest.Test\n \n     # TODO: Fix the failed tests\n     def is_pipeline_test_to_skip(\n-        self, pipeline_test_casse_name, config_class, model_architecture, tokenizer_name, processor_name\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n     ):\n         if (\n-            pipeline_test_casse_name == \"QAPipelineTests\"\n+            pipeline_test_case_name == \"QAPipelineTests\"\n             and tokenizer_name is not None\n             and not tokenizer_name.endswith(\"Fast\")\n         ):"
        },
        {
            "sha": "db323500bd1dfbf19710e8585cae6cd0f55a34ed",
            "filename": "tests/models/fnet/test_modeling_fnet.py",
            "status": "modified",
            "additions": 9,
            "deletions": 2,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Ffnet%2Ftest_modeling_fnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Ffnet%2Ftest_modeling_fnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffnet%2Ftest_modeling_fnet.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -298,9 +298,16 @@ class FNetModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n \n     # TODO: Fix the failed tests\n     def is_pipeline_test_to_skip(\n-        self, pipeline_test_casse_name, config_class, model_architecture, tokenizer_name, processor_name\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n     ):\n-        if pipeline_test_casse_name == \"QAPipelineTests\" and not tokenizer_name.endswith(\"Fast\"):\n+        if pipeline_test_case_name == \"QAPipelineTests\" and not tokenizer_name.endswith(\"Fast\"):\n             return True\n \n         return False"
        },
        {
            "sha": "1a6efe5735275eabff7f0cc7f6556284d77e019b",
            "filename": "tests/models/gemma/test_modeling_gemma.py",
            "status": "modified",
            "additions": 8,
            "deletions": 1,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -326,7 +326,14 @@ class GemmaModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixi\n \n     # TODO (ydshieh): Check this. See https://app.circleci.com/pipelines/github/huggingface/transformers/79245/workflows/9490ef58-79c2-410d-8f51-e3495156cf9c/jobs/1012146\n     def is_pipeline_test_to_skip(\n-        self, pipeline_test_casse_name, config_class, model_architecture, tokenizer_name, processor_name\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n     ):\n         return True\n "
        },
        {
            "sha": "6f6fba50dc123ae8c228b94b6007479ef4c7ffb5",
            "filename": "tests/models/gptj/test_modeling_gptj.py",
            "status": "modified",
            "additions": 9,
            "deletions": 2,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fgptj%2Ftest_modeling_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fgptj%2Ftest_modeling_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgptj%2Ftest_modeling_gptj.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -382,10 +382,17 @@ def test_torch_fx_output_loss(self):\n \n     # TODO: Fix the failed tests\n     def is_pipeline_test_to_skip(\n-        self, pipeline_test_casse_name, config_class, model_architecture, tokenizer_name, processor_name\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n     ):\n         if (\n-            pipeline_test_casse_name == \"QAPipelineTests\"\n+            pipeline_test_case_name == \"QAPipelineTests\"\n             and tokenizer_name is not None\n             and not tokenizer_name.endswith(\"Fast\")\n         ):"
        },
        {
            "sha": "295c08aebb4d7a1b938f953ae99305561d1f3fcb",
            "filename": "tests/models/gptj/test_modeling_tf_gptj.py",
            "status": "modified",
            "additions": 9,
            "deletions": 2,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fgptj%2Ftest_modeling_tf_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fgptj%2Ftest_modeling_tf_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgptj%2Ftest_modeling_tf_gptj.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -322,10 +322,17 @@ class TFGPTJModelTest(TFModelTesterMixin, TFCoreModelTesterMixin, PipelineTester\n \n     # TODO: Fix the failed tests\n     def is_pipeline_test_to_skip(\n-        self, pipeline_test_casse_name, config_class, model_architecture, tokenizer_name, processor_name\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n     ):\n         if (\n-            pipeline_test_casse_name == \"QAPipelineTests\"\n+            pipeline_test_case_name == \"QAPipelineTests\"\n             and tokenizer_name is not None\n             and not tokenizer_name.endswith(\"Fast\")\n         ):"
        },
        {
            "sha": "396a4179388fb8cf6b60af574d5e65d959e81d7c",
            "filename": "tests/models/kosmos2/test_modeling_kosmos2.py",
            "status": "modified",
            "additions": 9,
            "deletions": 2,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fkosmos2%2Ftest_modeling_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fkosmos2%2Ftest_modeling_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fkosmos2%2Ftest_modeling_kosmos2.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -260,9 +260,16 @@ class Kosmos2ModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase)\n \n     # TODO: `image-to-text` pipeline for this model needs Processor.\n     def is_pipeline_test_to_skip(\n-        self, pipeline_test_casse_name, config_class, model_architecture, tokenizer_name, processor_name\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n     ):\n-        return pipeline_test_casse_name == \"ImageToTextPipelineTests\"\n+        return pipeline_test_case_name == \"ImageToTextPipelineTests\"\n \n     def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):\n         inputs_dict = copy.deepcopy(inputs_dict)"
        },
        {
            "sha": "d62a7273bd589881630dc3f9303b669dc3ed10b9",
            "filename": "tests/models/layoutlmv3/test_modeling_layoutlmv3.py",
            "status": "modified",
            "additions": 8,
            "deletions": 1,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Flayoutlmv3%2Ftest_modeling_layoutlmv3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Flayoutlmv3%2Ftest_modeling_layoutlmv3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flayoutlmv3%2Ftest_modeling_layoutlmv3.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -292,7 +292,14 @@ class LayoutLMv3ModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCa\n \n     # TODO: Fix the failed tests\n     def is_pipeline_test_to_skip(\n-        self, pipeline_test_casse_name, config_class, model_architecture, tokenizer_name, processor_name\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n     ):\n         # `DocumentQuestionAnsweringPipeline` is expected to work with this model, but it combines the text and visual\n         # embedding along the sequence dimension (dim 1), which causes an error during post-processing as `p_mask` has"
        },
        {
            "sha": "ad0528f8da04d8994c533ee3a792d7851f760946",
            "filename": "tests/models/layoutlmv3/test_modeling_tf_layoutlmv3.py",
            "status": "modified",
            "additions": 8,
            "deletions": 1,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Flayoutlmv3%2Ftest_modeling_tf_layoutlmv3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Flayoutlmv3%2Ftest_modeling_tf_layoutlmv3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flayoutlmv3%2Ftest_modeling_tf_layoutlmv3.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -288,7 +288,14 @@ class TFLayoutLMv3ModelTest(TFModelTesterMixin, PipelineTesterMixin, unittest.Te\n \n     # TODO: Fix the failed tests\n     def is_pipeline_test_to_skip(\n-        self, pipeline_test_casse_name, config_class, model_architecture, tokenizer_name, processor_name\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n     ):\n         return True\n "
        },
        {
            "sha": "8e31758a4395b8ad8499a5bf66e872a6ba798b75",
            "filename": "tests/models/led/test_modeling_led.py",
            "status": "modified",
            "additions": 9,
            "deletions": 2,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fled%2Ftest_modeling_led.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fled%2Ftest_modeling_led.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fled%2Ftest_modeling_led.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -302,9 +302,16 @@ class LEDModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin,\n \n     # TODO: Fix the failed tests when this model gets more usage\n     def is_pipeline_test_to_skip(\n-        self, pipeline_test_casse_name, config_class, model_architecture, tokenizer_name, processor_name\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n     ):\n-        if pipeline_test_casse_name == \"QAPipelineTests\" and not tokenizer_name.endswith(\"Fast\"):\n+        if pipeline_test_case_name == \"QAPipelineTests\" and not tokenizer_name.endswith(\"Fast\"):\n             return True\n \n         return False"
        },
        {
            "sha": "dc3aaaa4ee5e2dfb030283b529cbae1ee8e14744",
            "filename": "tests/models/lilt/test_modeling_lilt.py",
            "status": "modified",
            "additions": 8,
            "deletions": 1,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Flilt%2Ftest_modeling_lilt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Flilt%2Ftest_modeling_lilt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flilt%2Ftest_modeling_lilt.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -245,7 +245,14 @@ class LiltModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin\n \n     # TODO: Fix the failed tests\n     def is_pipeline_test_to_skip(\n-        self, pipeline_test_casse_name, config_class, model_architecture, tokenizer_name, processor_name\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n     ):\n         return True\n "
        },
        {
            "sha": "23765fe8ce274fd63da23dc46fefcf35a4c320c9",
            "filename": "tests/models/longformer/test_modeling_longformer.py",
            "status": "modified",
            "additions": 9,
            "deletions": 2,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Flongformer%2Ftest_modeling_longformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Flongformer%2Ftest_modeling_longformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flongformer%2Ftest_modeling_longformer.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -331,10 +331,17 @@ class LongformerModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCa\n \n     # TODO: Fix the failed tests\n     def is_pipeline_test_to_skip(\n-        self, pipeline_test_casse_name, config_class, model_architecture, tokenizer_name, processor_name\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n     ):\n         if (\n-            pipeline_test_casse_name == \"QAPipelineTests\"\n+            pipeline_test_case_name == \"QAPipelineTests\"\n             and tokenizer_name is not None\n             and not tokenizer_name.endswith(\"Fast\")\n         ):"
        },
        {
            "sha": "7e0c57c49a0a46f70d5ab35587de8686526db448",
            "filename": "tests/models/longformer/test_modeling_tf_longformer.py",
            "status": "modified",
            "additions": 9,
            "deletions": 2,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Flongformer%2Ftest_modeling_tf_longformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Flongformer%2Ftest_modeling_tf_longformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flongformer%2Ftest_modeling_tf_longformer.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -303,10 +303,17 @@ class TFLongformerModelTest(TFModelTesterMixin, PipelineTesterMixin, unittest.Te\n \n     # TODO: Fix the failed tests\n     def is_pipeline_test_to_skip(\n-        self, pipeline_test_casse_name, config_class, model_architecture, tokenizer_name, processor_name\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n     ):\n         if (\n-            pipeline_test_casse_name == \"QAPipelineTests\"\n+            pipeline_test_case_name == \"QAPipelineTests\"\n             and tokenizer_name is not None\n             and not tokenizer_name.endswith(\"Fast\")\n         ):"
        },
        {
            "sha": "8fd68cd3240ea506342c1658f2f5a6453a03e02a",
            "filename": "tests/models/luke/test_modeling_luke.py",
            "status": "modified",
            "additions": 9,
            "deletions": 2,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fluke%2Ftest_modeling_luke.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fluke%2Ftest_modeling_luke.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fluke%2Ftest_modeling_luke.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -621,9 +621,16 @@ class LukeModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n \n     # TODO: Fix the failed tests\n     def is_pipeline_test_to_skip(\n-        self, pipeline_test_casse_name, config_class, model_architecture, tokenizer_name, processor_name\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n     ):\n-        if pipeline_test_casse_name in [\"QAPipelineTests\", \"ZeroShotClassificationPipelineTests\"]:\n+        if pipeline_test_case_name in [\"QAPipelineTests\", \"ZeroShotClassificationPipelineTests\"]:\n             return True\n \n         return False"
        },
        {
            "sha": "4fe0902c615b14fcc98587ac5608032fff8eeff8",
            "filename": "tests/models/m2m_100/test_modeling_m2m_100.py",
            "status": "modified",
            "additions": 9,
            "deletions": 2,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fm2m_100%2Ftest_modeling_m2m_100.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fm2m_100%2Ftest_modeling_m2m_100.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fm2m_100%2Ftest_modeling_m2m_100.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -258,9 +258,16 @@ class M2M100ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMix\n \n     # TODO: Fix the failed tests\n     def is_pipeline_test_to_skip(\n-        self, pipeline_test_casse_name, config_class, model_architecture, tokenizer_name, processor_name\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n     ):\n-        if pipeline_test_casse_name == \"TranslationPipelineTests\":\n+        if pipeline_test_case_name == \"TranslationPipelineTests\":\n             # Get `ValueError: Translation requires a `src_lang` and a `tgt_lang` for this model`.\n             # `M2M100Config` was never used in pipeline tests: cannot create a simple tokenizer.\n             return True"
        },
        {
            "sha": "8dfcee2484bd4b2208b028947e12439c1971e174",
            "filename": "tests/models/markuplm/test_modeling_markuplm.py",
            "status": "modified",
            "additions": 8,
            "deletions": 1,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fmarkuplm%2Ftest_modeling_markuplm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fmarkuplm%2Ftest_modeling_markuplm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmarkuplm%2Ftest_modeling_markuplm.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -301,7 +301,14 @@ class MarkupLMModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase\n \n     # TODO: Fix the failed tests\n     def is_pipeline_test_to_skip(\n-        self, pipeline_test_casse_name, config_class, model_architecture, tokenizer_name, processor_name\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n     ):\n         # ValueError: Nodes must be of type `List[str]` (single pretokenized example), or `List[List[str]]`\n         # (batch of pretokenized examples)."
        },
        {
            "sha": "10b59877f19a6c5227b09b4ffb6948527e5e6322",
            "filename": "tests/models/mbart/test_modeling_mbart.py",
            "status": "modified",
            "additions": 9,
            "deletions": 2,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fmbart%2Ftest_modeling_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fmbart%2Ftest_modeling_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmbart%2Ftest_modeling_mbart.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -252,9 +252,16 @@ class MBartModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixi\n \n     # TODO: Fix the failed tests\n     def is_pipeline_test_to_skip(\n-        self, pipeline_test_casse_name, config_class, model_architecture, tokenizer_name, processor_name\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n     ):\n-        if pipeline_test_casse_name == \"QAPipelineTests\" and not tokenizer_name.endswith(\"Fast\"):\n+        if pipeline_test_case_name == \"QAPipelineTests\" and not tokenizer_name.endswith(\"Fast\"):\n             return True\n \n         return False"
        },
        {
            "sha": "d2a672c7913325a2a5cdf3dc627b99fbee0f561d",
            "filename": "tests/models/mbart/test_modeling_tf_mbart.py",
            "status": "modified",
            "additions": 9,
            "deletions": 2,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fmbart%2Ftest_modeling_tf_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fmbart%2Ftest_modeling_tf_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmbart%2Ftest_modeling_tf_mbart.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -175,9 +175,16 @@ class TFMBartModelTest(TFModelTesterMixin, PipelineTesterMixin, unittest.TestCas\n \n     # TODO: Fix the failed tests\n     def is_pipeline_test_to_skip(\n-        self, pipeline_test_casse_name, config_class, model_architecture, tokenizer_name, processor_name\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n     ):\n-        if pipeline_test_casse_name != \"FeatureExtractionPipelineTests\":\n+        if pipeline_test_case_name != \"FeatureExtractionPipelineTests\":\n             # Exception encountered when calling layer '...'\n             return True\n "
        },
        {
            "sha": "885795a12912078010019aadd7c63e7f245fe5a8",
            "filename": "tests/models/mistral/test_modeling_mistral.py",
            "status": "modified",
            "additions": 8,
            "deletions": 1,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -313,7 +313,14 @@ class MistralModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMi\n \n     # TODO (ydshieh): Check this. See https://app.circleci.com/pipelines/github/huggingface/transformers/79245/workflows/9490ef58-79c2-410d-8f51-e3495156cf9c/jobs/1012146\n     def is_pipeline_test_to_skip(\n-        self, pipeline_test_casse_name, config_class, model_architecture, tokenizer_name, processor_name\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n     ):\n         return True\n "
        },
        {
            "sha": "448b40fc44c858f0e513fe3a8979a9828b8679f6",
            "filename": "tests/models/mistral/test_modeling_tf_mistral.py",
            "status": "modified",
            "additions": 8,
            "deletions": 1,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fmistral%2Ftest_modeling_tf_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fmistral%2Ftest_modeling_tf_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmistral%2Ftest_modeling_tf_mistral.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -266,7 +266,14 @@ class TFMistralModelTest(TFModelTesterMixin, TFGenerationIntegrationTests, Pipel\n \n     # TODO (ydshieh): Check this. See https://app.circleci.com/pipelines/github/huggingface/transformers/79245/workflows/9490ef58-79c2-410d-8f51-e3495156cf9c/jobs/1012146\n     def is_pipeline_test_to_skip(\n-        self, pipeline_test_casse_name, config_class, model_architecture, tokenizer_name, processor_name\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n     ):\n         return True\n "
        },
        {
            "sha": "a2655bb773dcddc01cd96f0b292807d544008e09",
            "filename": "tests/models/mixtral/test_modeling_mixtral.py",
            "status": "modified",
            "additions": 8,
            "deletions": 1,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fmixtral%2Ftest_modeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fmixtral%2Ftest_modeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmixtral%2Ftest_modeling_mixtral.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -313,7 +313,14 @@ class MixtralModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMi\n \n     # TODO (ydshieh): Check this. See https://app.circleci.com/pipelines/github/huggingface/transformers/79245/workflows/9490ef58-79c2-410d-8f51-e3495156cf9c/jobs/1012146\n     def is_pipeline_test_to_skip(\n-        self, pipeline_test_casse_name, config_class, model_architecture, tokenizer_name, processor_name\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n     ):\n         return True\n "
        },
        {
            "sha": "6e912ec3607d4066522a7beecb31ea313ca9de8c",
            "filename": "tests/models/mt5/test_modeling_mt5.py",
            "status": "modified",
            "additions": 28,
            "deletions": 1,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fmt5%2Ftest_modeling_mt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fmt5%2Ftest_modeling_mt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmt5%2Ftest_modeling_mt5.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -582,7 +582,14 @@ def setUp(self):\n     # `QAPipelineTests` is not working well with slow tokenizers (for some models) and we don't want to touch the file\n     # `src/transformers/data/processors/squad.py` (where this test fails for this model)\n     def is_pipeline_test_to_skip(\n-        self, pipeline_test_case_name, config_class, model_architecture, tokenizer_name, processor_name\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n     ):\n         if tokenizer_name is None:\n             return True\n@@ -1055,6 +1062,26 @@ def test_with_token_classification_head(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_with_token_classification_head(*config_and_inputs)\n \n+    def is_pipeline_test_to_skip(\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n+    ):\n+        if tokenizer_name is None:\n+            return True\n+\n+        # `MT5EncoderOnlyModelTest` is not working well with slow tokenizers (for some models) and we don't want to touch the file\n+        # `src/transformers/data/processors/squad.py` (where this test fails for this model)\n+        if pipeline_test_case_name == \"TokenClassificationPipelineTests\" and not tokenizer_name.endswith(\"Fast\"):\n+            return True\n+\n+        return False\n+\n \n @require_torch\n @require_sentencepiece"
        },
        {
            "sha": "f07574bdb54e1a0b70120d2d4c02496435f40fb6",
            "filename": "tests/models/mvp/test_modeling_mvp.py",
            "status": "modified",
            "additions": 9,
            "deletions": 2,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fmvp%2Ftest_modeling_mvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fmvp%2Ftest_modeling_mvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmvp%2Ftest_modeling_mvp.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -441,10 +441,17 @@ class MvpModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin,\n \n     # TODO: Fix the failed tests\n     def is_pipeline_test_to_skip(\n-        self, pipeline_test_casse_name, config_class, model_architecture, tokenizer_name, processor_name\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n     ):\n         if (\n-            pipeline_test_casse_name == \"QAPipelineTests\"\n+            pipeline_test_case_name == \"QAPipelineTests\"\n             and tokenizer_name is not None\n             and not tokenizer_name.endswith(\"Fast\")\n         ):"
        },
        {
            "sha": "f341fb078ef9394bc4c8f88bde7efa9d6df3bf90",
            "filename": "tests/models/nllb_moe/test_modeling_nllb_moe.py",
            "status": "modified",
            "additions": 8,
            "deletions": 1,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fnllb_moe%2Ftest_modeling_nllb_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fnllb_moe%2Ftest_modeling_nllb_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fnllb_moe%2Ftest_modeling_nllb_moe.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -266,7 +266,14 @@ class NllbMoeModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMi\n \n     # TODO: Fix the failed tests when this model gets more usage\n     def is_pipeline_test_to_skip(\n-        self, pipeline_test_casse_name, config_class, model_architecture, tokenizer_name, processor_name\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n     ):\n         # Saving the slow tokenizer after saving the fast tokenizer causes the loading of the later hanging forever.\n         return True"
        },
        {
            "sha": "f27302f8e19ad142179dc76882560c8f3b07058b",
            "filename": "tests/models/oneformer/test_modeling_oneformer.py",
            "status": "modified",
            "additions": 9,
            "deletions": 2,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Foneformer%2Ftest_modeling_oneformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Foneformer%2Ftest_modeling_oneformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Foneformer%2Ftest_modeling_oneformer.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -247,9 +247,16 @@ class OneFormerModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCas\n \n     # TODO: Fix the failed tests when this model gets more usage\n     def is_pipeline_test_to_skip(\n-        self, pipeline_test_casse_name, config_class, model_architecture, tokenizer_name, processor_name\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n     ):\n-        if pipeline_test_casse_name == \"FeatureExtractionPipelineTests\":\n+        if pipeline_test_case_name == \"FeatureExtractionPipelineTests\":\n             return True\n \n         return False"
        },
        {
            "sha": "8a61b05e831d154db0f84bfbdcd32b039253f92a",
            "filename": "tests/models/openai/test_modeling_openai.py",
            "status": "modified",
            "additions": 9,
            "deletions": 2,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fopenai%2Ftest_modeling_openai.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fopenai%2Ftest_modeling_openai.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fopenai%2Ftest_modeling_openai.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -211,9 +211,16 @@ class OpenAIGPTModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTester\n \n     # TODO: Fix the failed tests\n     def is_pipeline_test_to_skip(\n-        self, pipeline_test_casse_name, config_class, model_architecture, tokenizer_name, processor_name\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n     ):\n-        if pipeline_test_casse_name == \"ZeroShotClassificationPipelineTests\":\n+        if pipeline_test_case_name == \"ZeroShotClassificationPipelineTests\":\n             # Get `tokenizer does not have a padding token` error for both fast/slow tokenizers.\n             # `OpenAIGPTConfig` was never used in pipeline tests, either because of a missing checkpoint or because a\n             # tiny config could not be created."
        },
        {
            "sha": "43ffe4592a2839b5b13d2f983ba06b4411f59b27",
            "filename": "tests/models/openai/test_modeling_tf_openai.py",
            "status": "modified",
            "additions": 9,
            "deletions": 2,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fopenai%2Ftest_modeling_tf_openai.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fopenai%2Ftest_modeling_tf_openai.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fopenai%2Ftest_modeling_tf_openai.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -217,9 +217,16 @@ class TFOpenAIGPTModelTest(TFModelTesterMixin, PipelineTesterMixin, unittest.Tes\n \n     # TODO: Fix the failed tests\n     def is_pipeline_test_to_skip(\n-        self, pipeline_test_casse_name, config_class, model_architecture, tokenizer_name, processor_name\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n     ):\n-        if pipeline_test_casse_name == \"ZeroShotClassificationPipelineTests\":\n+        if pipeline_test_case_name == \"ZeroShotClassificationPipelineTests\":\n             # Get `tokenizer does not have a padding token` error for both fast/slow tokenizers.\n             # `OpenAIGPTConfig` was never used in pipeline tests, either because of a missing checkpoint or because a\n             # tiny config could not be created."
        },
        {
            "sha": "41622c19606117b6a2cdbdd1579312d6781a602d",
            "filename": "tests/models/opt/test_modeling_opt.py",
            "status": "modified",
            "additions": 9,
            "deletions": 2,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fopt%2Ftest_modeling_opt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fopt%2Ftest_modeling_opt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fopt%2Ftest_modeling_opt.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -221,10 +221,17 @@ class OPTModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin,\n \n     # TODO: Fix the failed tests\n     def is_pipeline_test_to_skip(\n-        self, pipeline_test_casse_name, config_class, model_architecture, tokenizer_name, processor_name\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n     ):\n         if (\n-            pipeline_test_casse_name == \"QAPipelineTests\"\n+            pipeline_test_case_name == \"QAPipelineTests\"\n             and tokenizer_name is not None\n             and not tokenizer_name.endswith(\"Fast\")\n         ):"
        },
        {
            "sha": "c17f69a499866b0de0e1b0e0fe4fbe8efdcb2c9f",
            "filename": "tests/models/phi/test_modeling_phi.py",
            "status": "modified",
            "additions": 8,
            "deletions": 1,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fphi%2Ftest_modeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fphi%2Ftest_modeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fphi%2Ftest_modeling_phi.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -304,7 +304,14 @@ class PhiModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin,\n \n     # TODO (ydshieh): Check this. See https://app.circleci.com/pipelines/github/huggingface/transformers/79292/workflows/fa2ba644-8953-44a6-8f67-ccd69ca6a476/jobs/1012905\n     def is_pipeline_test_to_skip(\n-        self, pipeline_test_casse_name, config_class, model_architecture, tokenizer_name, processor_name\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n     ):\n         return True\n "
        },
        {
            "sha": "2ecf0bb3bed6153b1a30ad7560e1ae7f1d0ded13",
            "filename": "tests/models/phi3/test_modeling_phi3.py",
            "status": "modified",
            "additions": 8,
            "deletions": 1,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fphi3%2Ftest_modeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fphi3%2Ftest_modeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fphi3%2Ftest_modeling_phi3.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -351,7 +351,14 @@ class Phi3ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin\n \n     # TODO (ydshieh): Check this. See https://app.circleci.com/pipelines/github/huggingface/transformers/79292/workflows/fa2ba644-8953-44a6-8f67-ccd69ca6a476/jobs/1012905\n     def is_pipeline_test_to_skip(\n-        self, pipeline_test_casse_name, config_class, model_architecture, tokenizer_name, processor_name\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n     ):\n         return True\n "
        },
        {
            "sha": "6f38539a63419463354a106ee0d2ab0219fdfab4",
            "filename": "tests/models/plbart/test_modeling_plbart.py",
            "status": "modified",
            "additions": 9,
            "deletions": 2,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fplbart%2Ftest_modeling_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fplbart%2Ftest_modeling_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fplbart%2Ftest_modeling_plbart.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -245,9 +245,16 @@ class PLBartModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMix\n \n     # TODO: Fix the failed tests\n     def is_pipeline_test_to_skip(\n-        self, pipeline_test_casse_name, config_class, model_architecture, tokenizer_name, processor_name\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n     ):\n-        if pipeline_test_casse_name == \"TranslationPipelineTests\":\n+        if pipeline_test_case_name == \"TranslationPipelineTests\":\n             # Get `ValueError: Translation requires a `src_lang` and a `tgt_lang` for this model`.\n             # `PLBartConfig` was never used in pipeline tests: cannot create a simple tokenizer.\n             return True"
        },
        {
            "sha": "91da243dbbbe2f93cf163a8334c499ba4db88103",
            "filename": "tests/models/prophetnet/test_modeling_prophetnet.py",
            "status": "modified",
            "additions": 9,
            "deletions": 2,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fprophetnet%2Ftest_modeling_prophetnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fprophetnet%2Ftest_modeling_prophetnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fprophetnet%2Ftest_modeling_prophetnet.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -906,9 +906,16 @@ class ProphetNetModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTeste\n \n     # TODO: Fix the failed tests\n     def is_pipeline_test_to_skip(\n-        self, pipeline_test_casse_name, config_class, model_architecture, tokenizer_name, processor_name\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n     ):\n-        if pipeline_test_casse_name == \"TextGenerationPipelineTests\":\n+        if pipeline_test_case_name == \"TextGenerationPipelineTests\":\n             # Get `ValueError: AttributeError: 'NoneType' object has no attribute 'new_ones'` or `AssertionError`.\n             # `ProphetNetConfig` was never used in pipeline tests: cannot create a simple\n             # tokenizer."
        },
        {
            "sha": "5bcf08258a8343064d7b5683b1d572e1edacd073",
            "filename": "tests/models/qwen2/test_modeling_qwen2.py",
            "status": "modified",
            "additions": 8,
            "deletions": 1,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -322,7 +322,14 @@ class Qwen2ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixi\n \n     # TODO (ydshieh): Check this. See https://app.circleci.com/pipelines/github/huggingface/transformers/79245/workflows/9490ef58-79c2-410d-8f51-e3495156cf9c/jobs/1012146\n     def is_pipeline_test_to_skip(\n-        self, pipeline_test_casse_name, config_class, model_architecture, tokenizer_name, processor_name\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n     ):\n         return True\n "
        },
        {
            "sha": "d2292f1ea640557b01a435dbe3fc031b5841c27d",
            "filename": "tests/models/qwen2_moe/test_modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 8,
            "deletions": 1,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fqwen2_moe%2Ftest_modeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fqwen2_moe%2Ftest_modeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_moe%2Ftest_modeling_qwen2_moe.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -349,7 +349,14 @@ class Qwen2MoeModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterM\n \n     # TODO (ydshieh): Check this. See https://app.circleci.com/pipelines/github/huggingface/transformers/79245/workflows/9490ef58-79c2-410d-8f51-e3495156cf9c/jobs/1012146\n     def is_pipeline_test_to_skip(\n-        self, pipeline_test_casse_name, config_class, model_architecture, tokenizer_name, processor_name\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n     ):\n         return True\n "
        },
        {
            "sha": "542955f9fa4511071cf5662e2b2d7265868e6973",
            "filename": "tests/models/recurrent_gemma/test_modeling_recurrent_gemma.py",
            "status": "modified",
            "additions": 8,
            "deletions": 1,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Frecurrent_gemma%2Ftest_modeling_recurrent_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Frecurrent_gemma%2Ftest_modeling_recurrent_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frecurrent_gemma%2Ftest_modeling_recurrent_gemma.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -305,7 +305,14 @@ class RecurrentGemmaModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineT\n \n     # TODO (ydshieh): Check this. See https://app.circleci.com/pipelines/github/huggingface/transformers/79245/workflows/9490ef58-79c2-410d-8f51-e3495156cf9c/jobs/1012146\n     def is_pipeline_test_to_skip(\n-        self, pipeline_test_casse_name, config_class, model_architecture, tokenizer_name, processor_name\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n     ):\n         return True\n "
        },
        {
            "sha": "cc36b01287bf08e5ee43902a16563239af08207c",
            "filename": "tests/models/reformer/test_modeling_reformer.py",
            "status": "modified",
            "additions": 9,
            "deletions": 2,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Freformer%2Ftest_modeling_reformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Freformer%2Ftest_modeling_reformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Freformer%2Ftest_modeling_reformer.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -727,10 +727,17 @@ class ReformerLSHAttnModelTest(\n \n     # TODO: Fix the failed tests\n     def is_pipeline_test_to_skip(\n-        self, pipeline_test_casse_name, config_class, model_architecture, tokenizer_name, processor_name\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n     ):\n         if (\n-            pipeline_test_casse_name == \"QAPipelineTests\"\n+            pipeline_test_case_name == \"QAPipelineTests\"\n             and tokenizer_name is not None\n             and not tokenizer_name.endswith(\"Fast\")\n         ):"
        },
        {
            "sha": "b1c7fa6521340d5ea640cb7aed853b8b086ce54e",
            "filename": "tests/models/roc_bert/test_modeling_roc_bert.py",
            "status": "modified",
            "additions": 9,
            "deletions": 2,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Froc_bert%2Ftest_modeling_roc_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Froc_bert%2Ftest_modeling_roc_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Froc_bert%2Ftest_modeling_roc_bert.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -587,9 +587,16 @@ class RoCBertModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase)\n \n     # TODO: Fix the failed tests when this model gets more usage\n     def is_pipeline_test_to_skip(\n-        self, pipeline_test_casse_name, config_class, model_architecture, tokenizer_name, processor_name\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n     ):\n-        if pipeline_test_casse_name in [\n+        if pipeline_test_case_name in [\n             \"FillMaskPipelineTests\",\n             \"FeatureExtractionPipelineTests\",\n             \"TextClassificationPipelineTests\","
        },
        {
            "sha": "8e10a092af13ed6416767ee05c4709e00db8fddf",
            "filename": "tests/models/roformer/test_modeling_tf_roformer.py",
            "status": "modified",
            "additions": 9,
            "deletions": 2,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Froformer%2Ftest_modeling_tf_roformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Froformer%2Ftest_modeling_tf_roformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Froformer%2Ftest_modeling_tf_roformer.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -275,9 +275,16 @@ class TFRoFormerModelTest(TFModelTesterMixin, PipelineTesterMixin, unittest.Test\n \n     # TODO: add `prepare_inputs_for_generation` for `TFRoFormerForCausalLM`\n     def is_pipeline_test_to_skip(\n-        self, pipeline_test_casse_name, config_class, model_architecture, tokenizer_name, processor_name\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n     ):\n-        if pipeline_test_casse_name == \"TextGenerationPipelineTests\":\n+        if pipeline_test_case_name == \"TextGenerationPipelineTests\":\n             return True\n \n         return False"
        },
        {
            "sha": "8ec97d7b2e5d8b381d7f2afed06ff4c6b42a9602",
            "filename": "tests/models/sam/test_modeling_sam.py",
            "status": "modified",
            "additions": 8,
            "deletions": 1,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fsam%2Ftest_modeling_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fsam%2Ftest_modeling_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam%2Ftest_modeling_sam.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -299,7 +299,14 @@ class SamModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n \n     # TODO: Fix me @Arthur: `run_batch_test` in `tests/test_pipeline_mixin.py` not working\n     def is_pipeline_test_to_skip(\n-        self, pipeline_test_casse_name, config_class, model_architecture, tokenizer_name, processor_name\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n     ):\n         return True\n "
        },
        {
            "sha": "3cb98e683a98ed7b5797f6f9551bae0dea709959",
            "filename": "tests/models/sam/test_modeling_tf_sam.py",
            "status": "modified",
            "additions": 8,
            "deletions": 1,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fsam%2Ftest_modeling_tf_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fsam%2Ftest_modeling_tf_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam%2Ftest_modeling_tf_sam.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -290,7 +290,14 @@ class TFSamModelTest(TFModelTesterMixin, PipelineTesterMixin, unittest.TestCase)\n \n     # TODO: Fix me @Arthur: `run_batch_test` in `tests/test_pipeline_mixin.py` not working\n     def is_pipeline_test_to_skip(\n-        self, pipeline_test_casse_name, config_class, model_architecture, tokenizer_name, processor_name\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n     ):\n         return True\n "
        },
        {
            "sha": "4b730017d9760d019dcbe3be43cd3447f5259a5c",
            "filename": "tests/models/splinter/test_modeling_splinter.py",
            "status": "modified",
            "additions": 10,
            "deletions": 3,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fsplinter%2Ftest_modeling_splinter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fsplinter%2Ftest_modeling_splinter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsplinter%2Ftest_modeling_splinter.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -225,11 +225,18 @@ class SplinterModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase\n \n     # TODO: Fix the failed tests when this model gets more usage\n     def is_pipeline_test_to_skip(\n-        self, pipeline_test_casse_name, config_class, model_architecture, tokenizer_name, processor_name\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n     ):\n-        if pipeline_test_casse_name == \"QAPipelineTests\":\n+        if pipeline_test_case_name == \"QAPipelineTests\":\n             return True\n-        elif pipeline_test_casse_name == \"FeatureExtractionPipelineTests\" and tokenizer_name.endswith(\"Fast\"):\n+        elif pipeline_test_case_name == \"FeatureExtractionPipelineTests\" and tokenizer_name.endswith(\"Fast\"):\n             return True\n \n         return False"
        },
        {
            "sha": "32d28143d72ffa038c862d9ab965a56402354c85",
            "filename": "tests/models/starcoder2/test_modeling_starcoder2.py",
            "status": "modified",
            "additions": 8,
            "deletions": 1,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fstarcoder2%2Ftest_modeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fstarcoder2%2Ftest_modeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fstarcoder2%2Ftest_modeling_starcoder2.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -311,7 +311,14 @@ class Starcoder2ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTeste\n \n     # TODO (ydshieh): Check this. See https://app.circleci.com/pipelines/github/huggingface/transformers/79245/workflows/9490ef58-79c2-410d-8f51-e3495156cf9c/jobs/1012146\n     def is_pipeline_test_to_skip(\n-        self, pipeline_test_casse_name, config_class, model_architecture, tokenizer_name, processor_name\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n     ):\n         return True\n "
        },
        {
            "sha": "fe9b40a54abef75aaf93c364ede686f2c6e02830",
            "filename": "tests/models/t5/test_modeling_t5.py",
            "status": "modified",
            "additions": 28,
            "deletions": 1,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -585,7 +585,14 @@ def setUp(self):\n     # `QAPipelineTests` is not working well with slow tokenizers (for some models) and we don't want to touch the file\n     # `src/transformers/data/processors/squad.py` (where this test fails for this model)\n     def is_pipeline_test_to_skip(\n-        self, pipeline_test_case_name, config_class, model_architecture, tokenizer_name, processor_name\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n     ):\n         if tokenizer_name is None:\n             return True\n@@ -1056,6 +1063,26 @@ def test_with_token_classification_head(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_with_token_classification_head(*config_and_inputs)\n \n+    def is_pipeline_test_to_skip(\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n+    ):\n+        if tokenizer_name is None:\n+            return True\n+\n+        # `T5EncoderOnlyModelTest` is not working well with slow tokenizers (for some models) and we don't want to touch the file\n+        # `src/transformers/data/processors/squad.py` (where this test fails for this model)\n+        if pipeline_test_case_name == \"TokenClassificationPipelineTests\" and not tokenizer_name.endswith(\"Fast\"):\n+            return True\n+\n+        return False\n+\n \n def use_task_specific_params(model, task):\n     model.config.update(model.config.task_specific_params[task])"
        },
        {
            "sha": "4ee159d6bddd1d5c799ca7d014cde25d76513fbd",
            "filename": "tests/models/tapas/test_modeling_tapas.py",
            "status": "modified",
            "additions": 8,
            "deletions": 1,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Ftapas%2Ftest_modeling_tapas.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Ftapas%2Ftest_modeling_tapas.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftapas%2Ftest_modeling_tapas.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -492,7 +492,14 @@ def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):\n \n     # TODO: Fix the failed tests\n     def is_pipeline_test_to_skip(\n-        self, pipeline_test_casse_name, config_class, model_architecture, tokenizer_name, processor_name\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n     ):\n         return True\n "
        },
        {
            "sha": "b70ec13452b35c7f1511eba12e654e77c3649ad5",
            "filename": "tests/models/tapas/test_modeling_tf_tapas.py",
            "status": "modified",
            "additions": 8,
            "deletions": 1,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Ftapas%2Ftest_modeling_tf_tapas.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Ftapas%2Ftest_modeling_tf_tapas.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftapas%2Ftest_modeling_tf_tapas.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -447,7 +447,14 @@ class TFTapasModelTest(TFModelTesterMixin, PipelineTesterMixin, unittest.TestCas\n \n     # TODO: Fix the failed tests\n     def is_pipeline_test_to_skip(\n-        self, pipeline_test_casse_name, config_class, model_architecture, tokenizer_name, processor_name\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n     ):\n         return True\n "
        },
        {
            "sha": "1bd01da8e6caec8206988f845a0cdf822b5c4fb8",
            "filename": "tests/models/umt5/test_modeling_umt5.py",
            "status": "modified",
            "additions": 29,
            "deletions": 2,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fumt5%2Ftest_modeling_umt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fumt5%2Ftest_modeling_umt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fumt5%2Ftest_modeling_umt5.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -322,9 +322,16 @@ def setUp(self):\n     # `QAPipelineTests` is not working well with slow tokenizers (for some models) and we don't want to touch the file\n     # `src/transformers/data/processors/squad.py` (where this test fails for this model)\n     def is_pipeline_test_to_skip(\n-        self, pipeline_test_casse_name, config_class, model_architecture, tokenizer_name, processor_name\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n     ):\n-        if pipeline_test_casse_name == \"QAPipelineTests\" and not tokenizer_name.endswith(\"Fast\"):\n+        if pipeline_test_case_name == \"QAPipelineTests\" and not tokenizer_name.endswith(\"Fast\"):\n             return True\n \n         return False\n@@ -723,6 +730,26 @@ def test_with_token_classification_head(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_with_token_classification_head(*config_and_inputs)\n \n+    def is_pipeline_test_to_skip(\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n+    ):\n+        if tokenizer_name is None:\n+            return True\n+\n+        # `UMT5EncoderOnlyModelTest` is not working well with slow tokenizers (for some models) and we don't want to touch the file\n+        # `src/transformers/data/processors/squad.py` (where this test fails for this model)\n+        if pipeline_test_case_name == \"TokenClassificationPipelineTests\" and not tokenizer_name.endswith(\"Fast\"):\n+            return True\n+\n+        return False\n+\n \n @require_torch\n @require_sentencepiece"
        },
        {
            "sha": "b24c577a16e575c46640fdec4a701ce540e94c70",
            "filename": "tests/models/whisper/test_modeling_whisper.py",
            "status": "modified",
            "additions": 9,
            "deletions": 2,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -397,9 +397,16 @@ class WhisperModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMi\n \n     # TODO: Fix the failed tests\n     def is_pipeline_test_to_skip(\n-        self, pipeline_test_casse_name, config_class, model_architecture, tokenizer_name, processor_name\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n     ):\n-        if pipeline_test_casse_name in [\n+        if pipeline_test_case_name in [\n             \"AutomaticSpeechRecognitionPipelineTests\",\n             \"AudioClassificationPipelineTests\",\n         ]:"
        },
        {
            "sha": "f9172a1b4eb05ddf2270e90440ff13a8b5647c9a",
            "filename": "tests/models/xlm/test_modeling_tf_xlm.py",
            "status": "modified",
            "additions": 9,
            "deletions": 2,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fxlm%2Ftest_modeling_tf_xlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fxlm%2Ftest_modeling_tf_xlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxlm%2Ftest_modeling_tf_xlm.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -312,10 +312,17 @@ class TFXLMModelTest(TFModelTesterMixin, PipelineTesterMixin, unittest.TestCase)\n \n     # TODO: Fix the failed tests\n     def is_pipeline_test_to_skip(\n-        self, pipeline_test_casse_name, config_class, model_architecture, tokenizer_name, processor_name\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n     ):\n         if (\n-            pipeline_test_casse_name == \"QAPipelineTests\"\n+            pipeline_test_case_name == \"QAPipelineTests\"\n             and tokenizer_name is not None\n             and not tokenizer_name.endswith(\"Fast\")\n         ):"
        },
        {
            "sha": "556f97c0b2c9353b2405ea78f022e1d1da729d77",
            "filename": "tests/models/xlm/test_modeling_xlm.py",
            "status": "modified",
            "additions": 9,
            "deletions": 2,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fxlm%2Ftest_modeling_xlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fxlm%2Ftest_modeling_xlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxlm%2Ftest_modeling_xlm.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -393,10 +393,17 @@ class XLMModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin,\n \n     # TODO: Fix the failed tests\n     def is_pipeline_test_to_skip(\n-        self, pipeline_test_casse_name, config_class, model_architecture, tokenizer_name, processor_name\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n     ):\n         if (\n-            pipeline_test_casse_name == \"QAPipelineTests\"\n+            pipeline_test_case_name == \"QAPipelineTests\"\n             and tokenizer_name is not None\n             and not tokenizer_name.endswith(\"Fast\")\n         ):"
        },
        {
            "sha": "5b426d27799fbb363831f9864dcf992c0dd759a8",
            "filename": "tests/models/xlm_roberta_xl/test_modeling_xlm_roberta_xl.py",
            "status": "modified",
            "additions": 9,
            "deletions": 2,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fxlm_roberta_xl%2Ftest_modeling_xlm_roberta_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fxlm_roberta_xl%2Ftest_modeling_xlm_roberta_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxlm_roberta_xl%2Ftest_modeling_xlm_roberta_xl.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -392,9 +392,16 @@ class XLMRobertaXLModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTes\n \n     # TODO: Fix the failed tests\n     def is_pipeline_test_to_skip(\n-        self, pipeline_test_casse_name, config_class, model_architecture, tokenizer_name, processor_name\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n     ):\n-        if pipeline_test_casse_name == \"QAPipelineTests\" and not tokenizer_name.endswith(\"Fast\"):\n+        if pipeline_test_case_name == \"QAPipelineTests\" and not tokenizer_name.endswith(\"Fast\"):\n             return True\n \n         return False"
        },
        {
            "sha": "71a308bf52f5d48ccdc28808d0765c7e3ff658b5",
            "filename": "tests/models/xlnet/test_modeling_tf_xlnet.py",
            "status": "modified",
            "additions": 8,
            "deletions": 1,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fxlnet%2Ftest_modeling_tf_xlnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fxlnet%2Ftest_modeling_tf_xlnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxlnet%2Ftest_modeling_tf_xlnet.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -372,7 +372,14 @@ def test_xla_generate_contrastive(self):\n \n     # TODO: Fix the failed tests\n     def is_pipeline_test_to_skip(\n-        self, pipeline_test_casse_name, config_class, model_architecture, tokenizer_name, processor_name\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n     ):\n         # Exception encountered when calling layer '...'\n         return True"
        },
        {
            "sha": "630c8d2e634eaf7ecc2db266c4c78dad878d68fe",
            "filename": "tests/models/xlnet/test_modeling_xlnet.py",
            "status": "modified",
            "additions": 9,
            "deletions": 2,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fxlnet%2Ftest_modeling_xlnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fxlnet%2Ftest_modeling_xlnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxlnet%2Ftest_modeling_xlnet.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -543,9 +543,16 @@ class XLNetModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixi\n \n     # TODO: Fix the failed tests\n     def is_pipeline_test_to_skip(\n-        self, pipeline_test_casse_name, config_class, model_architecture, tokenizer_name, processor_name\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n     ):\n-        if pipeline_test_casse_name == \"QAPipelineTests\" and not tokenizer_name.endswith(\"Fast\"):\n+        if pipeline_test_case_name == \"QAPipelineTests\" and not tokenizer_name.endswith(\"Fast\"):\n             return True\n \n         return False"
        },
        {
            "sha": "ae9a35c5d6371fbd5e8738149791040d44de14f4",
            "filename": "tests/models/xmod/test_modeling_xmod.py",
            "status": "modified",
            "additions": 9,
            "deletions": 2,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fxmod%2Ftest_modeling_xmod.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fmodels%2Fxmod%2Ftest_modeling_xmod.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxmod%2Ftest_modeling_xmod.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -386,9 +386,16 @@ class XmodModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin\n \n     # TODO: Fix the failed tests\n     def is_pipeline_test_to_skip(\n-        self, pipeline_test_casse_name, config_class, model_architecture, tokenizer_name, processor_name\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n     ):\n-        if pipeline_test_casse_name == \"QAPipelineTests\" and not tokenizer_name.endswith(\"Fast\"):\n+        if pipeline_test_case_name == \"QAPipelineTests\" and not tokenizer_name.endswith(\"Fast\"):\n             return True\n \n         return False"
        },
        {
            "sha": "10b8a859ff7fb3ae99e4a345ae9f1f1cf0068ee5",
            "filename": "tests/pipelines/test_pipelines_audio_classification.py",
            "status": "modified",
            "additions": 15,
            "deletions": 2,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fpipelines%2Ftest_pipelines_audio_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fpipelines%2Ftest_pipelines_audio_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_audio_classification.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -37,9 +37,22 @@ class AudioClassificationPipelineTests(unittest.TestCase):\n     model_mapping = MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING\n     tf_model_mapping = TF_MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING\n \n-    def get_test_pipeline(self, model, tokenizer, processor, torch_dtype=\"float32\"):\n+    def get_test_pipeline(\n+        self,\n+        model,\n+        tokenizer=None,\n+        image_processor=None,\n+        feature_extractor=None,\n+        processor=None,\n+        torch_dtype=\"float32\",\n+    ):\n         audio_classifier = AudioClassificationPipeline(\n-            model=model, feature_extractor=processor, torch_dtype=torch_dtype\n+            model=model,\n+            tokenizer=tokenizer,\n+            feature_extractor=feature_extractor,\n+            image_processor=image_processor,\n+            processor=processor,\n+            torch_dtype=torch_dtype,\n         )\n \n         # test with a raw waveform"
        },
        {
            "sha": "5d16f94cd90889ad41be53a4c34d90d2c1d397a7",
            "filename": "tests/pipelines/test_pipelines_automatic_speech_recognition.py",
            "status": "modified",
            "additions": 15,
            "deletions": 2,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fpipelines%2Ftest_pipelines_automatic_speech_recognition.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fpipelines%2Ftest_pipelines_automatic_speech_recognition.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_automatic_speech_recognition.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -67,14 +67,27 @@ class AutomaticSpeechRecognitionPipelineTests(unittest.TestCase):\n         + (MODEL_FOR_CTC_MAPPING.items() if MODEL_FOR_CTC_MAPPING else [])\n     )\n \n-    def get_test_pipeline(self, model, tokenizer, processor, torch_dtype=\"float32\"):\n+    def get_test_pipeline(\n+        self,\n+        model,\n+        tokenizer=None,\n+        image_processor=None,\n+        feature_extractor=None,\n+        processor=None,\n+        torch_dtype=\"float32\",\n+    ):\n         if tokenizer is None:\n             # Side effect of no Fast Tokenizer class for these model, so skipping\n             # But the slow tokenizer test should still run as they're quite small\n             self.skipTest(reason=\"No tokenizer available\")\n \n         speech_recognizer = AutomaticSpeechRecognitionPipeline(\n-            model=model, tokenizer=tokenizer, feature_extractor=processor, torch_dtype=torch_dtype\n+            model=model,\n+            tokenizer=tokenizer,\n+            feature_extractor=feature_extractor,\n+            image_processor=image_processor,\n+            processor=processor,\n+            torch_dtype=torch_dtype,\n         )\n \n         # test with a raw waveform"
        },
        {
            "sha": "ce1f83f53e0285a4f47a146d40958b2b4ab07e44",
            "filename": "tests/pipelines/test_pipelines_depth_estimation.py",
            "status": "modified",
            "additions": 17,
            "deletions": 2,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fpipelines%2Ftest_pipelines_depth_estimation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fpipelines%2Ftest_pipelines_depth_estimation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_depth_estimation.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -58,8 +58,23 @@ def hashimage(image: Image) -> str:\n class DepthEstimationPipelineTests(unittest.TestCase):\n     model_mapping = MODEL_FOR_DEPTH_ESTIMATION_MAPPING\n \n-    def get_test_pipeline(self, model, tokenizer, processor, torch_dtype=\"float32\"):\n-        depth_estimator = DepthEstimationPipeline(model=model, image_processor=processor, torch_dtype=torch_dtype)\n+    def get_test_pipeline(\n+        self,\n+        model,\n+        tokenizer=None,\n+        image_processor=None,\n+        feature_extractor=None,\n+        processor=None,\n+        torch_dtype=\"float32\",\n+    ):\n+        depth_estimator = DepthEstimationPipeline(\n+            model=model,\n+            tokenizer=tokenizer,\n+            feature_extractor=feature_extractor,\n+            image_processor=image_processor,\n+            processor=processor,\n+            torch_dtype=torch_dtype,\n+        )\n         return depth_estimator, [\n             \"./tests/fixtures/tests_samples/COCO/000000039769.png\",\n             \"./tests/fixtures/tests_samples/COCO/000000039769.png\","
        },
        {
            "sha": "305fe0c558943c29b2beb6960b5718c43c9f5934",
            "filename": "tests/pipelines/test_pipelines_document_question_answering.py",
            "status": "modified",
            "additions": 14,
            "deletions": 5,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fpipelines%2Ftest_pipelines_document_question_answering.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fpipelines%2Ftest_pipelines_document_question_answering.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_document_question_answering.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -15,7 +15,7 @@\n import unittest\n \n from transformers import MODEL_FOR_DOCUMENT_QUESTION_ANSWERING_MAPPING, AutoTokenizer, is_vision_available\n-from transformers.pipelines import pipeline\n+from transformers.pipelines import DocumentQuestionAnsweringPipeline, pipeline\n from transformers.pipelines.document_question_answering import apply_tesseract\n from transformers.testing_utils import (\n     is_pipeline_test,\n@@ -61,12 +61,21 @@ class DocumentQuestionAnsweringPipelineTests(unittest.TestCase):\n \n     @require_pytesseract\n     @require_vision\n-    def get_test_pipeline(self, model, tokenizer, processor, torch_dtype=\"float32\"):\n-        dqa_pipeline = pipeline(\n-            \"document-question-answering\",\n+    def get_test_pipeline(\n+        self,\n+        model,\n+        tokenizer=None,\n+        image_processor=None,\n+        feature_extractor=None,\n+        processor=None,\n+        torch_dtype=\"float32\",\n+    ):\n+        dqa_pipeline = DocumentQuestionAnsweringPipeline(\n             model=model,\n             tokenizer=tokenizer,\n-            image_processor=processor,\n+            feature_extractor=feature_extractor,\n+            image_processor=image_processor,\n+            processor=processor,\n             torch_dtype=torch_dtype,\n         )\n "
        },
        {
            "sha": "bceb48d6ffa692a5cc61f228df89d10f1e0f8ef7",
            "filename": "tests/pipelines/test_pipelines_feature_extraction.py",
            "status": "modified",
            "additions": 17,
            "deletions": 4,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fpipelines%2Ftest_pipelines_feature_extraction.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fpipelines%2Ftest_pipelines_feature_extraction.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_feature_extraction.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -174,7 +174,15 @@ def get_shape(self, input_, shape=None):\n             raise TypeError(\"We expect lists of floats, nothing else\")\n         return shape\n \n-    def get_test_pipeline(self, model, tokenizer, processor, torch_dtype=\"float32\"):\n+    def get_test_pipeline(\n+        self,\n+        model,\n+        tokenizer=None,\n+        image_processor=None,\n+        feature_extractor=None,\n+        processor=None,\n+        torch_dtype=\"float32\",\n+    ):\n         if tokenizer is None:\n             self.skipTest(reason=\"No tokenizer\")\n         elif (\n@@ -193,10 +201,15 @@ def get_test_pipeline(self, model, tokenizer, processor, torch_dtype=\"float32\"):\n                 For now ignore those.\n                 \"\"\"\n             )\n-        feature_extractor = FeatureExtractionPipeline(\n-            model=model, tokenizer=tokenizer, feature_extractor=processor, torch_dtype=torch_dtype\n+        feature_extractor_pipeline = FeatureExtractionPipeline(\n+            model=model,\n+            tokenizer=tokenizer,\n+            feature_extractor=feature_extractor,\n+            image_processor=image_processor,\n+            processor=processor,\n+            torch_dtype=torch_dtype,\n         )\n-        return feature_extractor, [\"This is a test\", \"This is another test\"]\n+        return feature_extractor_pipeline, [\"This is a test\", \"This is another test\"]\n \n     def run_pipeline_test(self, feature_extractor, examples):\n         outputs = feature_extractor(\"This is a test\")"
        },
        {
            "sha": "14061eaef7cf21a55c51ec992ff5413c39b23e05",
            "filename": "tests/pipelines/test_pipelines_fill_mask.py",
            "status": "modified",
            "additions": 17,
            "deletions": 2,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fpipelines%2Ftest_pipelines_fill_mask.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fpipelines%2Ftest_pipelines_fill_mask.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_fill_mask.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -251,11 +251,26 @@ def test_model_no_pad_tf(self):\n         unmasker.tokenizer.pad_token = None\n         self.run_pipeline_test(unmasker, [])\n \n-    def get_test_pipeline(self, model, tokenizer, processor, torch_dtype=\"float32\"):\n+    def get_test_pipeline(\n+        self,\n+        model,\n+        tokenizer=None,\n+        image_processor=None,\n+        feature_extractor=None,\n+        processor=None,\n+        torch_dtype=\"float32\",\n+    ):\n         if tokenizer is None or tokenizer.mask_token_id is None:\n             self.skipTest(reason=\"The provided tokenizer has no mask token, (probably reformer or wav2vec2)\")\n \n-        fill_masker = FillMaskPipeline(model=model, tokenizer=tokenizer, torch_dtype=torch_dtype)\n+        fill_masker = FillMaskPipeline(\n+            model=model,\n+            tokenizer=tokenizer,\n+            feature_extractor=feature_extractor,\n+            image_processor=image_processor,\n+            processor=processor,\n+            torch_dtype=torch_dtype,\n+        )\n         examples = [\n             f\"This is another {tokenizer.mask_token} test\",\n         ]"
        },
        {
            "sha": "5f926dd2f075176838eed9b29590942e6d6d2e8b",
            "filename": "tests/pipelines/test_pipelines_image_classification.py",
            "status": "modified",
            "additions": 16,
            "deletions": 2,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fpipelines%2Ftest_pipelines_image_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fpipelines%2Ftest_pipelines_image_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_image_classification.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -58,9 +58,23 @@ class ImageClassificationPipelineTests(unittest.TestCase):\n     model_mapping = MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING\n     tf_model_mapping = TF_MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING\n \n-    def get_test_pipeline(self, model, tokenizer, processor, torch_dtype=\"float32\"):\n+    def get_test_pipeline(\n+        self,\n+        model,\n+        tokenizer=None,\n+        image_processor=None,\n+        feature_extractor=None,\n+        processor=None,\n+        torch_dtype=\"float32\",\n+    ):\n         image_classifier = ImageClassificationPipeline(\n-            model=model, image_processor=processor, top_k=2, torch_dtype=torch_dtype\n+            model=model,\n+            tokenizer=tokenizer,\n+            feature_extractor=feature_extractor,\n+            image_processor=image_processor,\n+            processor=processor,\n+            torch_dtype=torch_dtype,\n+            top_k=2,\n         )\n         examples = [\n             Image.open(\"./tests/fixtures/tests_samples/COCO/000000039769.png\"),"
        },
        {
            "sha": "67140f91226d631194535510f38cbfc54bec62de",
            "filename": "tests/pipelines/test_pipelines_image_feature_extraction.py",
            "status": "modified",
            "additions": 18,
            "deletions": 5,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fpipelines%2Ftest_pipelines_image_feature_extraction.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fpipelines%2Ftest_pipelines_image_feature_extraction.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_image_feature_extraction.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -157,8 +157,16 @@ def test_return_tensors_tf(self):\n         outputs = feature_extractor(img, return_tensors=True)\n         self.assertTrue(tf.is_tensor(outputs))\n \n-    def get_test_pipeline(self, model, tokenizer, processor, torch_dtype=\"float32\"):\n-        if processor is None:\n+    def get_test_pipeline(\n+        self,\n+        model,\n+        tokenizer=None,\n+        image_processor=None,\n+        feature_extractor=None,\n+        processor=None,\n+        torch_dtype=\"float32\",\n+    ):\n+        if image_processor is None:\n             self.skipTest(reason=\"No image processor\")\n \n         elif type(model.config) in TOKENIZER_MAPPING:\n@@ -175,11 +183,16 @@ def get_test_pipeline(self, model, tokenizer, processor, torch_dtype=\"float32\"):\n                 \"\"\"\n             )\n \n-        feature_extractor = ImageFeatureExtractionPipeline(\n-            model=model, image_processor=processor, torch_dtype=torch_dtype\n+        feature_extractor_pipeline = ImageFeatureExtractionPipeline(\n+            model=model,\n+            tokenizer=tokenizer,\n+            feature_extractor=feature_extractor,\n+            image_processor=image_processor,\n+            processor=processor,\n+            torch_dtype=torch_dtype,\n         )\n         img = prepare_img()\n-        return feature_extractor, [img, img]\n+        return feature_extractor_pipeline, [img, img]\n \n     def run_pipeline_test(self, feature_extractor, examples):\n         imgs = examples"
        },
        {
            "sha": "44ed4e54534b26f1cd1247c2caa9e56b2381611b",
            "filename": "tests/pipelines/test_pipelines_image_segmentation.py",
            "status": "modified",
            "additions": 17,
            "deletions": 2,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fpipelines%2Ftest_pipelines_image_segmentation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fpipelines%2Ftest_pipelines_image_segmentation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_image_segmentation.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -89,8 +89,23 @@ class ImageSegmentationPipelineTests(unittest.TestCase):\n         + (MODEL_FOR_INSTANCE_SEGMENTATION_MAPPING.items() if MODEL_FOR_INSTANCE_SEGMENTATION_MAPPING else [])\n     )\n \n-    def get_test_pipeline(self, model, tokenizer, processor, torch_dtype=\"float32\"):\n-        image_segmenter = ImageSegmentationPipeline(model=model, image_processor=processor, torch_dtype=torch_dtype)\n+    def get_test_pipeline(\n+        self,\n+        model,\n+        tokenizer=None,\n+        image_processor=None,\n+        feature_extractor=None,\n+        processor=None,\n+        torch_dtype=\"float32\",\n+    ):\n+        image_segmenter = ImageSegmentationPipeline(\n+            model=model,\n+            tokenizer=tokenizer,\n+            feature_extractor=feature_extractor,\n+            image_processor=image_processor,\n+            processor=processor,\n+            torch_dtype=torch_dtype,\n+        )\n         return image_segmenter, [\n             \"./tests/fixtures/tests_samples/COCO/000000039769.png\",\n             \"./tests/fixtures/tests_samples/COCO/000000039769.png\","
        },
        {
            "sha": "0996b399b2882d3acf55e16526a88edba395ce6b",
            "filename": "tests/pipelines/test_pipelines_image_to_text.py",
            "status": "modified",
            "additions": 15,
            "deletions": 2,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fpipelines%2Ftest_pipelines_image_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fpipelines%2Ftest_pipelines_image_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_image_to_text.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -47,9 +47,22 @@ class ImageToTextPipelineTests(unittest.TestCase):\n     model_mapping = MODEL_FOR_VISION_2_SEQ_MAPPING\n     tf_model_mapping = TF_MODEL_FOR_VISION_2_SEQ_MAPPING\n \n-    def get_test_pipeline(self, model, tokenizer, processor, torch_dtype=\"float32\"):\n+    def get_test_pipeline(\n+        self,\n+        model,\n+        tokenizer=None,\n+        image_processor=None,\n+        feature_extractor=None,\n+        processor=None,\n+        torch_dtype=\"float32\",\n+    ):\n         pipe = ImageToTextPipeline(\n-            model=model, tokenizer=tokenizer, image_processor=processor, torch_dtype=torch_dtype\n+            model=model,\n+            tokenizer=tokenizer,\n+            feature_extractor=feature_extractor,\n+            image_processor=image_processor,\n+            processor=processor,\n+            torch_dtype=torch_dtype,\n         )\n         examples = [\n             Image.open(\"./tests/fixtures/tests_samples/COCO/000000039769.png\"),"
        },
        {
            "sha": "4adfff49e18be110d17839ad0a428095bb604c57",
            "filename": "tests/pipelines/test_pipelines_mask_generation.py",
            "status": "modified",
            "additions": 17,
            "deletions": 2,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fpipelines%2Ftest_pipelines_mask_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fpipelines%2Ftest_pipelines_mask_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_mask_generation.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -67,8 +67,23 @@ class MaskGenerationPipelineTests(unittest.TestCase):\n         (list(TF_MODEL_FOR_MASK_GENERATION_MAPPING.items()) if TF_MODEL_FOR_MASK_GENERATION_MAPPING else [])\n     )\n \n-    def get_test_pipeline(self, model, tokenizer, processor, torch_dtype=\"float32\"):\n-        image_segmenter = MaskGenerationPipeline(model=model, image_processor=processor, torch_dtype=torch_dtype)\n+    def get_test_pipeline(\n+        self,\n+        model,\n+        tokenizer=None,\n+        image_processor=None,\n+        feature_extractor=None,\n+        processor=None,\n+        torch_dtype=\"float32\",\n+    ):\n+        image_segmenter = MaskGenerationPipeline(\n+            model=model,\n+            tokenizer=tokenizer,\n+            feature_extractor=feature_extractor,\n+            image_processor=image_processor,\n+            processor=processor,\n+            torch_dtype=torch_dtype,\n+        )\n         return image_segmenter, [\n             \"./tests/fixtures/tests_samples/COCO/000000039769.png\",\n             \"./tests/fixtures/tests_samples/COCO/000000039769.png\","
        },
        {
            "sha": "d58f3749ec3f250a9ec0a52add46f72315368a39",
            "filename": "tests/pipelines/test_pipelines_object_detection.py",
            "status": "modified",
            "additions": 17,
            "deletions": 2,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fpipelines%2Ftest_pipelines_object_detection.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fpipelines%2Ftest_pipelines_object_detection.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_object_detection.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -56,8 +56,23 @@ def open(*args, **kwargs):\n class ObjectDetectionPipelineTests(unittest.TestCase):\n     model_mapping = MODEL_FOR_OBJECT_DETECTION_MAPPING\n \n-    def get_test_pipeline(self, model, tokenizer, processor, torch_dtype=\"float32\"):\n-        object_detector = ObjectDetectionPipeline(model=model, image_processor=processor, torch_dtype=torch_dtype)\n+    def get_test_pipeline(\n+        self,\n+        model,\n+        tokenizer=None,\n+        image_processor=None,\n+        feature_extractor=None,\n+        processor=None,\n+        torch_dtype=\"float32\",\n+    ):\n+        object_detector = ObjectDetectionPipeline(\n+            model=model,\n+            tokenizer=tokenizer,\n+            feature_extractor=feature_extractor,\n+            image_processor=image_processor,\n+            processor=processor,\n+            torch_dtype=torch_dtype,\n+        )\n         return object_detector, [\"./tests/fixtures/tests_samples/COCO/000000039769.png\"]\n \n     def run_pipeline_test(self, object_detector, examples):"
        },
        {
            "sha": "d051a1435b9eec263c5782761ce1cc1453c5a139",
            "filename": "tests/pipelines/test_pipelines_question_answering.py",
            "status": "modified",
            "additions": 17,
            "deletions": 2,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fpipelines%2Ftest_pipelines_question_answering.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fpipelines%2Ftest_pipelines_question_answering.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_question_answering.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -50,12 +50,27 @@ class QAPipelineTests(unittest.TestCase):\n             config: model for config, model in tf_model_mapping.items() if config.__name__ not in _TO_SKIP\n         }\n \n-    def get_test_pipeline(self, model, tokenizer, processor, torch_dtype=\"float32\"):\n+    def get_test_pipeline(\n+        self,\n+        model,\n+        tokenizer=None,\n+        image_processor=None,\n+        feature_extractor=None,\n+        processor=None,\n+        torch_dtype=\"float32\",\n+    ):\n         if isinstance(model.config, LxmertConfig):\n             # This is an bimodal model, we need to find a more consistent way\n             # to switch on those models.\n             return None, None\n-        question_answerer = QuestionAnsweringPipeline(model, tokenizer, torch_dtype=torch_dtype)\n+        question_answerer = QuestionAnsweringPipeline(\n+            model=model,\n+            tokenizer=tokenizer,\n+            feature_extractor=feature_extractor,\n+            image_processor=image_processor,\n+            processor=processor,\n+            torch_dtype=torch_dtype,\n+        )\n \n         examples = [\n             {\"question\": \"Where was HuggingFace founded ?\", \"context\": \"HuggingFace was founded in Paris.\"},"
        },
        {
            "sha": "465dba9743c648db7951f218aca87570a941cd47",
            "filename": "tests/pipelines/test_pipelines_summarization.py",
            "status": "modified",
            "additions": 17,
            "deletions": 2,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fpipelines%2Ftest_pipelines_summarization.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fpipelines%2Ftest_pipelines_summarization.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_summarization.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -32,8 +32,23 @@ class SummarizationPipelineTests(unittest.TestCase):\n     model_mapping = MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING\n     tf_model_mapping = TF_MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING\n \n-    def get_test_pipeline(self, model, tokenizer, processor, torch_dtype=\"float32\"):\n-        summarizer = SummarizationPipeline(model=model, tokenizer=tokenizer, torch_dtype=torch_dtype)\n+    def get_test_pipeline(\n+        self,\n+        model,\n+        tokenizer=None,\n+        image_processor=None,\n+        feature_extractor=None,\n+        processor=None,\n+        torch_dtype=\"float32\",\n+    ):\n+        summarizer = SummarizationPipeline(\n+            model=model,\n+            tokenizer=tokenizer,\n+            feature_extractor=feature_extractor,\n+            image_processor=image_processor,\n+            processor=processor,\n+            torch_dtype=torch_dtype,\n+        )\n         return summarizer, [\"(CNN)The Palestinian Authority officially became\", \"Some other text\"]\n \n     def run_pipeline_test(self, summarizer, _):"
        },
        {
            "sha": "a41f41fadfe588910b7fc3378b32326c4991da9c",
            "filename": "tests/pipelines/test_pipelines_text2text_generation.py",
            "status": "modified",
            "additions": 17,
            "deletions": 2,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fpipelines%2Ftest_pipelines_text2text_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fpipelines%2Ftest_pipelines_text2text_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_text2text_generation.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -35,8 +35,23 @@ class Text2TextGenerationPipelineTests(unittest.TestCase):\n     model_mapping = MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING\n     tf_model_mapping = TF_MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING\n \n-    def get_test_pipeline(self, model, tokenizer, processor, torch_dtype=\"float32\"):\n-        generator = Text2TextGenerationPipeline(model=model, tokenizer=tokenizer, torch_dtype=torch_dtype)\n+    def get_test_pipeline(\n+        self,\n+        model,\n+        tokenizer=None,\n+        image_processor=None,\n+        feature_extractor=None,\n+        processor=None,\n+        torch_dtype=\"float32\",\n+    ):\n+        generator = Text2TextGenerationPipeline(\n+            model=model,\n+            tokenizer=tokenizer,\n+            feature_extractor=feature_extractor,\n+            image_processor=image_processor,\n+            processor=processor,\n+            torch_dtype=torch_dtype,\n+        )\n         return generator, [\"Something to write\", \"Something else\"]\n \n     def run_pipeline_test(self, generator, _):"
        },
        {
            "sha": "1f3b31b85832656903bfbe87588b9f7402b2e8ea",
            "filename": "tests/pipelines/test_pipelines_text_classification.py",
            "status": "modified",
            "additions": 17,
            "deletions": 2,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fpipelines%2Ftest_pipelines_text_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fpipelines%2Ftest_pipelines_text_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_text_classification.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -179,8 +179,23 @@ def test_tf_bert(self):\n         outputs = text_classifier(\"Birds are a type of animal\")\n         self.assertEqual(nested_simplify(outputs), [{\"label\": \"POSITIVE\", \"score\": 0.988}])\n \n-    def get_test_pipeline(self, model, tokenizer, processor, torch_dtype=\"float32\"):\n-        text_classifier = TextClassificationPipeline(model=model, tokenizer=tokenizer, torch_dtype=torch_dtype)\n+    def get_test_pipeline(\n+        self,\n+        model,\n+        tokenizer=None,\n+        image_processor=None,\n+        feature_extractor=None,\n+        processor=None,\n+        torch_dtype=\"float32\",\n+    ):\n+        text_classifier = TextClassificationPipeline(\n+            model=model,\n+            tokenizer=tokenizer,\n+            feature_extractor=feature_extractor,\n+            image_processor=image_processor,\n+            processor=processor,\n+            torch_dtype=torch_dtype,\n+        )\n         return text_classifier, [\"HuggingFace is in\", \"This is another test\"]\n \n     def run_pipeline_test(self, text_classifier, _):"
        },
        {
            "sha": "277c870b4d1074bfd7da1ae36ffe8e9fc30efab6",
            "filename": "tests/pipelines/test_pipelines_text_generation.py",
            "status": "modified",
            "additions": 18,
            "deletions": 2,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fpipelines%2Ftest_pipelines_text_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fpipelines%2Ftest_pipelines_text_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_text_generation.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -377,8 +377,23 @@ def test_small_chat_model_tf(self):\n             ],\n         )\n \n-    def get_test_pipeline(self, model, tokenizer, processor, torch_dtype=\"float32\"):\n-        text_generator = TextGenerationPipeline(model=model, tokenizer=tokenizer, torch_dtype=torch_dtype)\n+    def get_test_pipeline(\n+        self,\n+        model,\n+        tokenizer=None,\n+        image_processor=None,\n+        feature_extractor=None,\n+        processor=None,\n+        torch_dtype=\"float32\",\n+    ):\n+        text_generator = TextGenerationPipeline(\n+            model=model,\n+            tokenizer=tokenizer,\n+            feature_extractor=feature_extractor,\n+            image_processor=image_processor,\n+            processor=processor,\n+            torch_dtype=torch_dtype,\n+        )\n         return text_generator, [\"This is a test\", \"Another test\"]\n \n     def test_stop_sequence_stopping_criteria(self):\n@@ -471,6 +486,7 @@ def run_pipeline_test(self, text_generator, _):\n             \"GPTNeoXForCausalLM\",\n             \"GPTNeoXJapaneseForCausalLM\",\n             \"FuyuForCausalLM\",\n+            \"LlamaForCausalLM\",\n         ]\n         if (\n             tokenizer.model_max_length < 10000"
        },
        {
            "sha": "e07e2ad392a3e6242f257ed7e3fc28f9dd9e8960",
            "filename": "tests/pipelines/test_pipelines_text_to_audio.py",
            "status": "modified",
            "additions": 17,
            "deletions": 2,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fpipelines%2Ftest_pipelines_text_to_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fpipelines%2Ftest_pipelines_text_to_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_text_to_audio.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -250,8 +250,23 @@ def test_generative_model_kwargs(self):\n         outputs = music_generator(\"This is a test\", forward_params=forward_params, generate_kwargs=generate_kwargs)\n         self.assertListEqual(outputs[\"audio\"].tolist(), audio.tolist())\n \n-    def get_test_pipeline(self, model, tokenizer, processor, torch_dtype=\"float32\"):\n-        speech_generator = TextToAudioPipeline(model=model, tokenizer=tokenizer, torch_dtype=torch_dtype)\n+    def get_test_pipeline(\n+        self,\n+        model,\n+        tokenizer=None,\n+        image_processor=None,\n+        feature_extractor=None,\n+        processor=None,\n+        torch_dtype=\"float32\",\n+    ):\n+        speech_generator = TextToAudioPipeline(\n+            model=model,\n+            tokenizer=tokenizer,\n+            feature_extractor=feature_extractor,\n+            image_processor=image_processor,\n+            processor=processor,\n+            torch_dtype=torch_dtype,\n+        )\n         return speech_generator, [\"This is a test\", \"Another test\"]\n \n     def run_pipeline_test(self, speech_generator, _):"
        },
        {
            "sha": "5e4b18f91699b5f5b90a0a4ef544270639dbc7f1",
            "filename": "tests/pipelines/test_pipelines_token_classification.py",
            "status": "modified",
            "additions": 17,
            "deletions": 2,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fpipelines%2Ftest_pipelines_token_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fpipelines%2Ftest_pipelines_token_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_token_classification.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -61,8 +61,23 @@ class TokenClassificationPipelineTests(unittest.TestCase):\n             config: model for config, model in tf_model_mapping.items() if config.__name__ not in _TO_SKIP\n         }\n \n-    def get_test_pipeline(self, model, tokenizer, processor, torch_dtype=\"float32\"):\n-        token_classifier = TokenClassificationPipeline(model=model, tokenizer=tokenizer, torch_dtype=torch_dtype)\n+    def get_test_pipeline(\n+        self,\n+        model,\n+        tokenizer=None,\n+        image_processor=None,\n+        feature_extractor=None,\n+        processor=None,\n+        torch_dtype=\"float32\",\n+    ):\n+        token_classifier = TokenClassificationPipeline(\n+            model=model,\n+            tokenizer=tokenizer,\n+            feature_extractor=feature_extractor,\n+            image_processor=image_processor,\n+            processor=processor,\n+            torch_dtype=torch_dtype,\n+        )\n         return token_classifier, [\"A simple string\", \"A simple string that is quite a bit longer\"]\n \n     def run_pipeline_test(self, token_classifier, _):"
        },
        {
            "sha": "1d6e6a33b1a47534e217803ea7364ed5e3b32ef2",
            "filename": "tests/pipelines/test_pipelines_translation.py",
            "status": "modified",
            "additions": 25,
            "deletions": 3,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fpipelines%2Ftest_pipelines_translation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fpipelines%2Ftest_pipelines_translation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_translation.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -35,14 +35,36 @@ class TranslationPipelineTests(unittest.TestCase):\n     model_mapping = MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING\n     tf_model_mapping = TF_MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING\n \n-    def get_test_pipeline(self, model, tokenizer, processor, torch_dtype=\"float32\"):\n+    def get_test_pipeline(\n+        self,\n+        model,\n+        tokenizer=None,\n+        image_processor=None,\n+        feature_extractor=None,\n+        processor=None,\n+        torch_dtype=\"float32\",\n+    ):\n         if isinstance(model.config, MBartConfig):\n             src_lang, tgt_lang = list(tokenizer.lang_code_to_id.keys())[:2]\n             translator = TranslationPipeline(\n-                model=model, tokenizer=tokenizer, src_lang=src_lang, tgt_lang=tgt_lang, torch_dtype=torch_dtype\n+                model=model,\n+                tokenizer=tokenizer,\n+                feature_extractor=feature_extractor,\n+                image_processor=image_processor,\n+                processor=processor,\n+                torch_dtype=torch_dtype,\n+                src_lang=src_lang,\n+                tgt_lang=tgt_lang,\n             )\n         else:\n-            translator = TranslationPipeline(model=model, tokenizer=tokenizer, torch_dtype=torch_dtype)\n+            translator = TranslationPipeline(\n+                model=model,\n+                tokenizer=tokenizer,\n+                feature_extractor=feature_extractor,\n+                image_processor=image_processor,\n+                processor=processor,\n+                torch_dtype=torch_dtype,\n+            )\n         return translator, [\"Some string\", \"Some other text\"]\n \n     def run_pipeline_test(self, translator, _):"
        },
        {
            "sha": "8b910e94af3b471a7b8a7d0b3f99cc5672c45252",
            "filename": "tests/pipelines/test_pipelines_video_classification.py",
            "status": "modified",
            "additions": 16,
            "deletions": 2,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fpipelines%2Ftest_pipelines_video_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fpipelines%2Ftest_pipelines_video_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_video_classification.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -38,12 +38,26 @@\n class VideoClassificationPipelineTests(unittest.TestCase):\n     model_mapping = MODEL_FOR_VIDEO_CLASSIFICATION_MAPPING\n \n-    def get_test_pipeline(self, model, tokenizer, processor, torch_dtype=\"float32\"):\n+    def get_test_pipeline(\n+        self,\n+        model,\n+        tokenizer=None,\n+        image_processor=None,\n+        feature_extractor=None,\n+        processor=None,\n+        torch_dtype=\"float32\",\n+    ):\n         example_video_filepath = hf_hub_download(\n             repo_id=\"nateraw/video-demo\", filename=\"archery.mp4\", repo_type=\"dataset\"\n         )\n         video_classifier = VideoClassificationPipeline(\n-            model=model, image_processor=processor, top_k=2, torch_dtype=torch_dtype\n+            model=model,\n+            tokenizer=tokenizer,\n+            feature_extractor=feature_extractor,\n+            image_processor=image_processor,\n+            processor=processor,\n+            torch_dtype=torch_dtype,\n+            top_k=2,\n         )\n         examples = [\n             example_video_filepath,"
        },
        {
            "sha": "3c79dc7faa183323a03a811efcc3cd90383d9107",
            "filename": "tests/pipelines/test_pipelines_visual_question_answering.py",
            "status": "modified",
            "additions": 12,
            "deletions": 2,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fpipelines%2Ftest_pipelines_visual_question_answering.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fpipelines%2Ftest_pipelines_visual_question_answering.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_visual_question_answering.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -55,9 +55,19 @@ def open(*args, **kwargs):\n class VisualQuestionAnsweringPipelineTests(unittest.TestCase):\n     model_mapping = MODEL_FOR_VISUAL_QUESTION_ANSWERING_MAPPING\n \n-    def get_test_pipeline(self, model, tokenizer, processor, torch_dtype=\"float32\"):\n+    def get_test_pipeline(\n+        self,\n+        model,\n+        tokenizer=None,\n+        image_processor=None,\n+        feature_extractor=None,\n+        processor=None,\n+        torch_dtype=\"float32\",\n+    ):\n         vqa_pipeline = pipeline(\n-            \"visual-question-answering\", model=\"hf-internal-testing/tiny-vilt-random-vqa\", torch_dtype=torch_dtype\n+            \"visual-question-answering\",\n+            model=\"hf-internal-testing/tiny-vilt-random-vqa\",\n+            torch_dtype=torch_dtype,\n         )\n         examples = [\n             {"
        },
        {
            "sha": "be5a59f55d0920b8c1d242a4d983a97e2c295230",
            "filename": "tests/pipelines/test_pipelines_zero_shot.py",
            "status": "modified",
            "additions": 16,
            "deletions": 2,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fpipelines%2Ftest_pipelines_zero_shot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fpipelines%2Ftest_pipelines_zero_shot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_zero_shot.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -53,9 +53,23 @@ class ZeroShotClassificationPipelineTests(unittest.TestCase):\n             config: model for config, model in tf_model_mapping.items() if config.__name__ not in _TO_SKIP\n         }\n \n-    def get_test_pipeline(self, model, tokenizer, processor, torch_dtype=\"float32\"):\n+    def get_test_pipeline(\n+        self,\n+        model,\n+        tokenizer=None,\n+        image_processor=None,\n+        feature_extractor=None,\n+        processor=None,\n+        torch_dtype=\"float32\",\n+    ):\n         classifier = ZeroShotClassificationPipeline(\n-            model=model, tokenizer=tokenizer, candidate_labels=[\"polics\", \"health\"], torch_dtype=torch_dtype\n+            model=model,\n+            tokenizer=tokenizer,\n+            feature_extractor=feature_extractor,\n+            image_processor=image_processor,\n+            processor=processor,\n+            torch_dtype=torch_dtype,\n+            candidate_labels=[\"polics\", \"health\"],\n         )\n         return classifier, [\"Who are you voting for in 2020?\", \"My stomach hurts.\"]\n "
        },
        {
            "sha": "48cdb9bd15ca53b34694fb1c0bfcde5e900ae2a6",
            "filename": "tests/pipelines/test_pipelines_zero_shot_object_detection.py",
            "status": "modified",
            "additions": 9,
            "deletions": 1,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fpipelines%2Ftest_pipelines_zero_shot_object_detection.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Fpipelines%2Ftest_pipelines_zero_shot_object_detection.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_zero_shot_object_detection.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -43,7 +43,15 @@ def open(*args, **kwargs):\n class ZeroShotObjectDetectionPipelineTests(unittest.TestCase):\n     model_mapping = MODEL_FOR_ZERO_SHOT_OBJECT_DETECTION_MAPPING\n \n-    def get_test_pipeline(self, model, tokenizer, processor, torch_dtype=\"float32\"):\n+    def get_test_pipeline(\n+        self,\n+        model,\n+        tokenizer=None,\n+        image_processor=None,\n+        feature_extractor=None,\n+        processor=None,\n+        torch_dtype=\"float32\",\n+    ):\n         object_detector = pipeline(\n             \"zero-shot-object-detection\",\n             model=\"hf-internal-testing/tiny-random-owlvit-object-detection\","
        },
        {
            "sha": "7d03089360619023f9626beb1244ab809a648b8e",
            "filename": "tests/test_pipeline_mixin.py",
            "status": "modified",
            "additions": 199,
            "deletions": 69,
            "changes": 268,
            "blob_url": "https://github.com/huggingface/transformers/blob/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Ftest_pipeline_mixin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48461c0fe2686cb2321a17104183d0c3fd6488db/tests%2Ftest_pipeline_mixin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_pipeline_mixin.py?ref=48461c0fe2686cb2321a17104183d0c3fd6488db",
            "patch": "@@ -36,6 +36,7 @@\n     ZeroShotImageClassificationInput,\n )\n \n+from transformers.models.auto.processing_auto import PROCESSOR_MAPPING_NAMES\n from transformers.pipelines import (\n     AudioClassificationPipeline,\n     AutomaticSpeechRecognitionPipeline,\n@@ -183,43 +184,87 @@ def run_task_tests(self, task, torch_dtype=\"float32\"):\n         model_architectures = self.pipeline_model_mapping[task]\n         if not isinstance(model_architectures, tuple):\n             model_architectures = (model_architectures,)\n-        if not isinstance(model_architectures, tuple):\n-            raise TypeError(f\"`model_architectures` must be a tuple. Got {type(model_architectures)} instead.\")\n+\n+        # We are going to run tests for multiple model architectures, some of them might be skipped\n+        # with this flag we are control if at least one model were tested or all were skipped\n+        at_least_one_model_is_tested = False\n \n         for model_architecture in model_architectures:\n             model_arch_name = model_architecture.__name__\n+            model_type = model_architecture.config_class.model_type\n \n             # Get the canonical name\n             for _prefix in [\"Flax\", \"TF\"]:\n                 if model_arch_name.startswith(_prefix):\n                     model_arch_name = model_arch_name[len(_prefix) :]\n                     break\n \n-            tokenizer_names = []\n-            processor_names = []\n+            if model_arch_name not in tiny_model_summary:\n+                continue\n+\n+            tokenizer_names = tiny_model_summary[model_arch_name][\"tokenizer_classes\"]\n+\n+            # Sort image processors and feature extractors from tiny-models json file\n+            image_processor_names = []\n+            feature_extractor_names = []\n+\n+            processor_classes = tiny_model_summary[model_arch_name][\"processor_classes\"]\n+            for cls_name in processor_classes:\n+                if \"ImageProcessor\" in cls_name:\n+                    image_processor_names.append(cls_name)\n+                elif \"FeatureExtractor\" in cls_name:\n+                    feature_extractor_names.append(cls_name)\n+                else:\n+                    raise ValueError(f\"Unknown processor class: {cls_name}\")\n+\n+            # Processor classes are not in tiny models JSON file, so extract them from the mapping\n+            # processors are mapped to instance, e.g. \"XxxProcessor\"\n+            processor_names = PROCESSOR_MAPPING_NAMES.get(model_type, None)\n+            if not isinstance(processor_names, (list, tuple)):\n+                processor_names = [processor_names]\n+\n             commit = None\n-            if model_arch_name in tiny_model_summary:\n-                tokenizer_names = tiny_model_summary[model_arch_name][\"tokenizer_classes\"]\n-                processor_names = tiny_model_summary[model_arch_name][\"processor_classes\"]\n-                if \"sha\" in tiny_model_summary[model_arch_name]:\n-                    commit = tiny_model_summary[model_arch_name][\"sha\"]\n-            # Adding `None` (if empty) so we can generate tests\n-            tokenizer_names = [None] if len(tokenizer_names) == 0 else tokenizer_names\n-            processor_names = [None] if len(processor_names) == 0 else processor_names\n+            if model_arch_name in tiny_model_summary and \"sha\" in tiny_model_summary[model_arch_name]:\n+                commit = tiny_model_summary[model_arch_name][\"sha\"]\n \n             repo_name = f\"tiny-random-{model_arch_name}\"\n             if TRANSFORMERS_TINY_MODEL_PATH != \"hf-internal-testing\":\n                 repo_name = model_arch_name\n \n             self.run_model_pipeline_tests(\n-                task, repo_name, model_architecture, tokenizer_names, processor_names, commit, torch_dtype\n+                task,\n+                repo_name,\n+                model_architecture,\n+                tokenizer_names=tokenizer_names,\n+                image_processor_names=image_processor_names,\n+                feature_extractor_names=feature_extractor_names,\n+                processor_names=processor_names,\n+                commit=commit,\n+                torch_dtype=torch_dtype,\n             )\n         if task in task_to_pipeline_and_spec_mapping:\n             pipeline, hub_spec = task_to_pipeline_and_spec_mapping[task]\n             compare_pipeline_args_to_hub_spec(pipeline, hub_spec)\n \n+            at_least_one_model_is_tested = True\n+\n+        if not at_least_one_model_is_tested:\n+            self.skipTest(\n+                f\"{self.__class__.__name__}::test_pipeline_{task.replace('-', '_')}_{torch_dtype} is skipped: Could not find any \"\n+                f\"model architecture in the tiny models JSON file for `{task}`.\"\n+            )\n+\n     def run_model_pipeline_tests(\n-        self, task, repo_name, model_architecture, tokenizer_names, processor_names, commit, torch_dtype=\"float32\"\n+        self,\n+        task,\n+        repo_name,\n+        model_architecture,\n+        tokenizer_names,\n+        image_processor_names,\n+        feature_extractor_names,\n+        processor_names,\n+        commit,\n+        torch_dtype=\"float32\",\n     ):\n         \"\"\"Run pipeline tests for a specific `task` with the give model class and tokenizer/processor class names\n \n@@ -232,8 +277,12 @@ def run_model_pipeline_tests(\n                 A subclass of `PretrainedModel` or `PretrainedModel`.\n             tokenizer_names (`List[str]`):\n                 A list of names of a subclasses of `PreTrainedTokenizerFast` or `PreTrainedTokenizer`.\n+            image_processor_names (`List[str]`):\n+                A list of names of subclasses of `BaseImageProcessor`.\n+            feature_extractor_names (`List[str]`):\n+                A list of names of subclasses of `FeatureExtractionMixin`.\n             processor_names (`List[str]`):\n-                A list of names of subclasses of `BaseImageProcessor` or `FeatureExtractionMixin`.\n+                A list of names of subclasses of `ProcessorMixin`.\n             commit (`str`):\n                 The commit hash of the model repository on the Hub.\n             torch_dtype (`str`, `optional`, defaults to `'float32'`):\n@@ -243,27 +292,73 @@ def run_model_pipeline_tests(\n         # `run_pipeline_test`.\n         pipeline_test_class_name = pipeline_test_mapping[task][\"test\"].__name__\n \n-        for tokenizer_name in tokenizer_names:\n-            for processor_name in processor_names:\n-                if self.is_pipeline_test_to_skip(\n-                    pipeline_test_class_name,\n-                    model_architecture.config_class,\n-                    model_architecture,\n-                    tokenizer_name,\n-                    processor_name,\n-                ):\n-                    logger.warning(\n-                        f\"{self.__class__.__name__}::test_pipeline_{task.replace('-', '_')}_{torch_dtype} is skipped: test is \"\n-                        f\"currently known to fail for: model `{model_architecture.__name__}` | tokenizer \"\n-                        f\"`{tokenizer_name}` | processor `{processor_name}`.\"\n-                    )\n-                    continue\n-                self.run_pipeline_test(\n-                    task, repo_name, model_architecture, tokenizer_name, processor_name, commit, torch_dtype\n+        # If no image processor or feature extractor is found, we still need to test the pipeline with None\n+        # otherwise for any empty list we might skip all the tests\n+        tokenizer_names = tokenizer_names or [None]\n+        image_processor_names = image_processor_names or [None]\n+        feature_extractor_names = feature_extractor_names or [None]\n+        processor_names = processor_names or [None]\n+\n+        test_cases = [\n+            {\n+                \"tokenizer_name\": tokenizer_name,\n+                \"image_processor_name\": image_processor_name,\n+                \"feature_extractor_name\": feature_extractor_name,\n+                \"processor_name\": processor_name,\n+            }\n+            for tokenizer_name in tokenizer_names\n+            for image_processor_name in image_processor_names\n+            for feature_extractor_name in feature_extractor_names\n+            for processor_name in processor_names\n+        ]\n+\n+        for test_case in test_cases:\n+            tokenizer_name = test_case[\"tokenizer_name\"]\n+            image_processor_name = test_case[\"image_processor_name\"]\n+            feature_extractor_name = test_case[\"feature_extractor_name\"]\n+            processor_name = test_case[\"processor_name\"]\n+\n+            do_skip_test_case = self.is_pipeline_test_to_skip(\n+                pipeline_test_class_name,\n+                model_architecture.config_class,\n+                model_architecture,\n+                tokenizer_name,\n+                image_processor_name,\n+                feature_extractor_name,\n+                processor_name,\n+            )\n+\n+            if do_skip_test_case:\n+                logger.warning(\n+                    f\"{self.__class__.__name__}::test_pipeline_{task.replace('-', '_')}_{torch_dtype} is skipped: test is \"\n+                    f\"currently known to fail for: model `{model_architecture.__name__}` | tokenizer \"\n+                    f\"`{tokenizer_name}` | image processor `{image_processor_name}` | feature extractor {feature_extractor_name}.\"\n                 )\n+                continue\n+\n+            self.run_pipeline_test(\n+                task,\n+                repo_name,\n+                model_architecture,\n+                tokenizer_name=tokenizer_name,\n+                image_processor_name=image_processor_name,\n+                feature_extractor_name=feature_extractor_name,\n+                processor_name=processor_name,\n+                commit=commit,\n+                torch_dtype=torch_dtype,\n+            )\n \n     def run_pipeline_test(\n-        self, task, repo_name, model_architecture, tokenizer_name, processor_name, commit, torch_dtype=\"float32\"\n+        self,\n+        task,\n+        repo_name,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n+        commit,\n+        torch_dtype=\"float32\",\n     ):\n         \"\"\"Run pipeline tests for a specific `task` with the give model class and tokenizer/processor class name\n \n@@ -278,67 +373,86 @@ def run_pipeline_test(\n                 A subclass of `PretrainedModel` or `PretrainedModel`.\n             tokenizer_name (`str`):\n                 The name of a subclass of `PreTrainedTokenizerFast` or `PreTrainedTokenizer`.\n+            image_processor_name (`str`):\n+                The name of a subclass of `BaseImageProcessor`.\n+            feature_extractor_name (`str`):\n+                The name of a subclass of `FeatureExtractionMixin`.\n             processor_name (`str`):\n-                The name of a subclass of `BaseImageProcessor` or `FeatureExtractionMixin`.\n+                The name of a subclass of `ProcessorMixin`.\n             commit (`str`):\n                 The commit hash of the model repository on the Hub.\n             torch_dtype (`str`, `optional`, defaults to `'float32'`):\n                 The torch dtype to use for the model. Can be used for FP16/other precision inference.\n         \"\"\"\n         repo_id = f\"{TRANSFORMERS_TINY_MODEL_PATH}/{repo_name}\"\n+        model_type = model_architecture.config_class.model_type\n+\n         if TRANSFORMERS_TINY_MODEL_PATH != \"hf-internal-testing\":\n-            model_type = model_architecture.config_class.model_type\n             repo_id = os.path.join(TRANSFORMERS_TINY_MODEL_PATH, model_type, repo_name)\n \n+        # -------------------- Load model --------------------\n+\n+        # TODO: We should check if a model file is on the Hub repo. instead.\n+        try:\n+            model = model_architecture.from_pretrained(repo_id, revision=commit)\n+        except Exception:\n+            logger.warning(\n+                f\"{self.__class__.__name__}::test_pipeline_{task.replace('-', '_')}_{torch_dtype} is skipped: Could not find or load \"\n+                f\"the model from `{repo_id}` with `{model_architecture}`.\"\n+            )\n+            self.skipTest(f\"Could not find or load the model from {repo_id} with {model_architecture}.\")\n+\n+        # -------------------- Load tokenizer --------------------\n+\n         tokenizer = None\n         if tokenizer_name is not None:\n             tokenizer_class = getattr(transformers_module, tokenizer_name)\n             tokenizer = tokenizer_class.from_pretrained(repo_id, revision=commit)\n \n-        processor = None\n-        if processor_name is not None:\n-            processor_class = getattr(transformers_module, processor_name)\n-            # If the required packages (like `Pillow` or `torchaudio`) are not installed, this will fail.\n-            try:\n-                processor = processor_class.from_pretrained(repo_id, revision=commit)\n-            except Exception:\n-                logger.warning(\n-                    f\"{self.__class__.__name__}::test_pipeline_{task.replace('-', '_')}_{torch_dtype} is skipped: Could not load the \"\n-                    f\"processor from `{repo_id}` with `{processor_name}`.\"\n-                )\n-                self.skipTest(f\"Could not load the processor from {repo_id} with {processor_name}.\")\n+        # -------------------- Load processors --------------------\n+\n+        processors = {}\n+        for key, name in zip(\n+            [\"image_processor\", \"feature_extractor\", \"processor\"],\n+            [image_processor_name, feature_extractor_name, processor_name],\n+        ):\n+            if name is not None:\n+                try:\n+                    # Can fail if some extra dependencies are not installed\n+                    processor_class = getattr(transformers_module, name)\n+                    processor = processor_class.from_pretrained(repo_id, revision=commit)\n+                    processors[key] = processor\n+                except Exception:\n+                    logger.warning(\n+                        f\"{self.__class__.__name__}::test_pipeline_{task.replace('-', '_')}_{torch_dtype} is skipped: \"\n+                        f\"Could not load the {key} from `{repo_id}` with `{name}`.\"\n+                    )\n+                    self.skipTest(f\"Could not load the {key} from {repo_id} with {name}.\")\n+\n+        # ---------------------------------------------------------\n \n         # TODO: Maybe not upload such problematic tiny models to Hub.\n-        if tokenizer is None and processor is None:\n+        if tokenizer is None and \"image_processor\" not in processors and \"feature_extractor\" not in processors:\n             logger.warning(\n                 f\"{self.__class__.__name__}::test_pipeline_{task.replace('-', '_')}_{torch_dtype} is skipped: Could not find or load \"\n-                f\"any tokenizer / processor from `{repo_id}`.\"\n+                f\"any tokenizer / image processor / feature extractor from `{repo_id}`.\"\n             )\n             self.skipTest(f\"Could not find or load any tokenizer / processor from {repo_id}.\")\n \n-        # TODO: We should check if a model file is on the Hub repo. instead.\n-        try:\n-            model = model_architecture.from_pretrained(repo_id, revision=commit)\n-        except Exception:\n-            logger.warning(\n-                f\"{self.__class__.__name__}::test_pipeline_{task.replace('-', '_')}_{torch_dtype} is skipped: Could not find or load \"\n-                f\"the model from `{repo_id}` with `{model_architecture}`.\"\n-            )\n-            self.skipTest(f\"Could not find or load the model from {repo_id} with {model_architecture}.\")\n-\n         pipeline_test_class_name = pipeline_test_mapping[task][\"test\"].__name__\n-        if self.is_pipeline_test_to_skip_more(pipeline_test_class_name, model.config, model, tokenizer, processor):\n+        if self.is_pipeline_test_to_skip_more(pipeline_test_class_name, model.config, model, tokenizer, **processors):\n             logger.warning(\n                 f\"{self.__class__.__name__}::test_pipeline_{task.replace('-', '_')}_{torch_dtype} is skipped: test is \"\n                 f\"currently known to fail for: model `{model_architecture.__name__}` | tokenizer \"\n-                f\"`{tokenizer_name}` | processor `{processor_name}`.\"\n+                f\"`{tokenizer_name}` | image processor `{image_processor_name}` | feature extractor `{feature_extractor_name}`.\"\n             )\n             self.skipTest(\n-                f\"Test is known to fail for: model `{model_architecture.__name__}` | tokenizer `{tokenizer_name}` | processor `{processor_name}`.\"\n+                f\"Test is known to fail for: model `{model_architecture.__name__}` | tokenizer `{tokenizer_name}` \"\n+                f\"| image processor `{image_processor_name}` | feature extractor `{feature_extractor_name}`.\"\n             )\n \n         # validate\n-        validate_test_components(self, task, model, tokenizer, processor)\n+        validate_test_components(model, tokenizer)\n \n         if hasattr(model, \"eval\"):\n             model = model.eval()\n@@ -347,7 +461,7 @@ def run_pipeline_test(\n         # `run_pipeline_test`.\n         task_test = pipeline_test_mapping[task][\"test\"]()\n \n-        pipeline, examples = task_test.get_test_pipeline(model, tokenizer, processor, torch_dtype=torch_dtype)\n+        pipeline, examples = task_test.get_test_pipeline(model, tokenizer, **processors, torch_dtype=torch_dtype)\n         if pipeline is None:\n             # The test can disable itself, but it should be very marginal\n             # Concerns: Wav2Vec2ForCTC without tokenizer test (FastTokenizer don't exist)\n@@ -674,15 +788,22 @@ def test_pipeline_zero_shot_object_detection_fp16(self):\n \n     # This contains the test cases to be skipped without model architecture being involved.\n     def is_pipeline_test_to_skip(\n-        self, pipeline_test_casse_name, config_class, model_architecture, tokenizer_name, processor_name\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n     ):\n         \"\"\"Skip some tests based on the classes or their names without the instantiated objects.\n \n         This is to avoid calling `from_pretrained` (so reducing the runtime) if we already know the tests will fail.\n         \"\"\"\n         # No fix is required for this case.\n         if (\n-            pipeline_test_casse_name == \"DocumentQuestionAnsweringPipelineTests\"\n+            pipeline_test_case_name == \"DocumentQuestionAnsweringPipelineTests\"\n             and tokenizer_name is not None\n             and not tokenizer_name.endswith(\"Fast\")\n         ):\n@@ -691,11 +812,20 @@ def is_pipeline_test_to_skip(\n \n         return False\n \n-    def is_pipeline_test_to_skip_more(self, pipeline_test_casse_name, config, model, tokenizer, processor):  # noqa\n+    def is_pipeline_test_to_skip_more(\n+        self,\n+        pipeline_test_case_name,\n+        config,\n+        model,\n+        tokenizer,\n+        image_processor=None,\n+        feature_extractor=None,\n+        processor=None,\n+    ):  # noqa\n         \"\"\"Skip some more tests based on the information from the instantiated objects.\"\"\"\n         # No fix is required for this case.\n         if (\n-            pipeline_test_casse_name == \"QAPipelineTests\"\n+            pipeline_test_case_name == \"QAPipelineTests\"\n             and tokenizer is not None\n             and getattr(tokenizer, \"pad_token\", None) is None\n             and not tokenizer.__class__.__name__.endswith(\"Fast\")\n@@ -706,7 +836,7 @@ def is_pipeline_test_to_skip_more(self, pipeline_test_casse_name, config, model,\n         return False\n \n \n-def validate_test_components(test_case, task, model, tokenizer, processor):\n+def validate_test_components(model, tokenizer):\n     # TODO: Move this to tiny model creation script\n     # head-specific (within a model type) necessary changes to the config\n     # 1. for `BlenderbotForCausalLM`"
        }
    ],
    "stats": {
        "total": 1551,
        "additions": 1311,
        "deletions": 240
    }
}