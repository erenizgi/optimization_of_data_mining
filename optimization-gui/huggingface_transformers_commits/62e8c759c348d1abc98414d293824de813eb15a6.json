{
    "author": "yonigozlan",
    "message": "rename all test_processing_*.py to test_processor_*.py (#33878)\n\n* rename all test_processing_*.py to test_processor_*.py ans fix duplicate test processor paligemma\r\n\r\n* fix copies\r\n\r\n* fix broken tests\r\n\r\n* fix-copies\r\n\r\n* fix test processor bridgetower",
    "sha": "62e8c759c348d1abc98414d293824de813eb15a6",
    "files": [
        {
            "sha": "bf9dbd951b5b06f982b4128fbf62b0d5b0ad063f",
            "filename": "src/transformers/models/omdet_turbo/modeling_omdet_turbo.py",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/62e8c759c348d1abc98414d293824de813eb15a6/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62e8c759c348d1abc98414d293824de813eb15a6/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py?ref=62e8c759c348d1abc98414d293824de813eb15a6",
            "patch": "@@ -21,7 +21,7 @@\n from dataclasses import dataclass\n from functools import lru_cache\n from pathlib import Path\n-from typing import Optional, Tuple, Union\n+from typing import List, Optional, Tuple, Union\n \n import torch\n import torch.nn.functional as F\n@@ -208,7 +208,10 @@ def load_cuda_kernels():\n \n # Copied from transformers.models.deformable_detr.modeling_deformable_detr.multi_scale_deformable_attention\n def multi_scale_deformable_attention(\n-    value: Tensor, value_spatial_shapes: Tensor, sampling_locations: Tensor, attention_weights: Tensor\n+    value: Tensor,\n+    value_spatial_shapes: Union[Tensor, List[Tuple]],\n+    sampling_locations: Tensor,\n+    attention_weights: Tensor,\n ) -> Tensor:\n     batch_size, _, num_heads, hidden_dim = value.shape\n     _, num_queries, num_heads, num_levels, num_points, _ = sampling_locations.shape"
        },
        {
            "sha": "aa63855da43a2454c83f8de04510f5343c40fd53",
            "filename": "tests/models/blip/test_processor_blip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/62e8c759c348d1abc98414d293824de813eb15a6/tests%2Fmodels%2Fblip%2Ftest_processor_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62e8c759c348d1abc98414d293824de813eb15a6/tests%2Fmodels%2Fblip%2Ftest_processor_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip%2Ftest_processor_blip.py?ref=62e8c759c348d1abc98414d293824de813eb15a6",
            "patch": "@@ -152,7 +152,7 @@ def test_unstructured_kwargs_batched(self):\n         self.skip_processor_without_typed_kwargs(processor)\n \n         input_str = [\"lower newer\", \"upper older longer string\"]\n-        image_input = self.prepare_image_inputs() * 2\n+        image_input = self.prepare_image_inputs(batch_size=2)\n         inputs = processor(\n             text=input_str,\n             images=image_input,"
        },
        {
            "sha": "7eb5bedc2be7a7f450a1ae942a0702ee8603624f",
            "filename": "tests/models/blip_2/test_processor_blip_2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 28,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/62e8c759c348d1abc98414d293824de813eb15a6/tests%2Fmodels%2Fblip_2%2Ftest_processor_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62e8c759c348d1abc98414d293824de813eb15a6/tests%2Fmodels%2Fblip_2%2Ftest_processor_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip_2%2Ftest_processor_blip_2.py?ref=62e8c759c348d1abc98414d293824de813eb15a6",
            "patch": "@@ -17,7 +17,7 @@\n \n import pytest\n \n-from transformers.testing_utils import require_torch, require_vision\n+from transformers.testing_utils import require_vision\n from transformers.utils import is_vision_available\n \n from ...test_processing_common import ProcessorTesterMixin\n@@ -139,30 +139,3 @@ def test_model_input_names(self):\n \n         # For now the processor supports only ['pixel_values', 'input_ids', 'attention_mask']\n         self.assertCountEqual(list(inputs.keys()), [\"input_ids\", \"pixel_values\", \"attention_mask\"])\n-\n-    @require_torch\n-    @require_vision\n-    def test_unstructured_kwargs_batched(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\")\n-        if not tokenizer.pad_token:\n-            tokenizer.pad_token = \"[TEST_PAD]\"\n-        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        input_str = [\"lower newer\", \"upper older longer string\"]\n-        image_input = self.prepare_image_inputs() * 2\n-        inputs = processor(\n-            text=input_str,\n-            images=image_input,\n-            return_tensors=\"pt\",\n-            crop_size={\"height\": 214, \"width\": 214},\n-            size={\"height\": 214, \"width\": 214},\n-            padding=\"longest\",\n-            max_length=76,\n-        )\n-        self.assertEqual(inputs[\"pixel_values\"].shape[2], 214)\n-\n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 11)"
        },
        {
            "sha": "2ccfde803edb207672831fbca2d15b634e357d25",
            "filename": "tests/models/bridgetower/test_processor_bridgetower.py",
            "status": "renamed",
            "additions": 2,
            "deletions": 17,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/62e8c759c348d1abc98414d293824de813eb15a6/tests%2Fmodels%2Fbridgetower%2Ftest_processor_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62e8c759c348d1abc98414d293824de813eb15a6/tests%2Fmodels%2Fbridgetower%2Ftest_processor_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbridgetower%2Ftest_processor_bridgetower.py?ref=62e8c759c348d1abc98414d293824de813eb15a6",
            "patch": "@@ -15,17 +15,13 @@\n import tempfile\n import unittest\n \n-import numpy as np\n-\n from transformers.testing_utils import require_torch, require_vision\n from transformers.utils import is_vision_available\n \n from ...test_processing_common import ProcessorTesterMixin\n \n \n if is_vision_available():\n-    from PIL import Image\n-\n     from transformers import (\n         AutoProcessor,\n         BridgeTowerImageProcessor,\n@@ -35,7 +31,7 @@\n \n \n @require_vision\n-class Blip2ProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n+class BridgeTowerProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n     processor_class = BridgeTowerProcessor\n \n     def setUp(self):\n@@ -57,17 +53,6 @@ def get_image_processor(self, **kwargs):\n     def tearDown(self):\n         shutil.rmtree(self.tmpdirname)\n \n-    def prepare_image_inputs(self):\n-        \"\"\"This function prepares a list of PIL images, or a list of numpy arrays if one specifies numpify=True,\n-        or a list of PyTorch tensors if one specifies torchify=True.\n-        \"\"\"\n-\n-        image_inputs = [np.random.randint(255, size=(3, 30, 400), dtype=np.uint8)]\n-\n-        image_inputs = [Image.fromarray(np.moveaxis(x, 0, -1)) for x in image_inputs]\n-\n-        return image_inputs\n-\n     # Some kwargs tests are overriden from common tests to handle shortest_edge\n     # and size_divisor behaviour\n \n@@ -149,7 +134,7 @@ def test_unstructured_kwargs_batched(self):\n         self.skip_processor_without_typed_kwargs(processor)\n \n         input_str = [\"lower newer\", \"upper older longer string\"]\n-        image_input = self.prepare_image_inputs() * 2\n+        image_input = self.prepare_image_inputs(batch_size=2)\n         inputs = processor(\n             text=input_str,\n             images=image_input,",
            "previous_filename": "tests/models/bridgetower/test_processing_bridgetower.py"
        },
        {
            "sha": "cf720e17b0d9d5351ed84b26d646a78aab023353",
            "filename": "tests/models/donut/test_processor_donut.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 31,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/62e8c759c348d1abc98414d293824de813eb15a6/tests%2Fmodels%2Fdonut%2Ftest_processor_donut.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62e8c759c348d1abc98414d293824de813eb15a6/tests%2Fmodels%2Fdonut%2Ftest_processor_donut.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdonut%2Ftest_processor_donut.py?ref=62e8c759c348d1abc98414d293824de813eb15a6",
            "patch": "@@ -18,10 +18,6 @@\n import unittest\n \n from transformers import DonutImageProcessor, DonutProcessor, XLMRobertaTokenizerFast\n-from transformers.testing_utils import (\n-    require_torch,\n-    require_vision,\n-)\n \n from ...test_processing_common import ProcessorTesterMixin\n \n@@ -65,30 +61,3 @@ def test_token2json(self):\n         actual_json = self.processor.token2json(sequence)\n \n         self.assertDictEqual(actual_json, expected_json)\n-\n-    @require_torch\n-    @require_vision\n-    def test_unstructured_kwargs_batched(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\")\n-        if not tokenizer.pad_token:\n-            tokenizer.pad_token = \"[TEST_PAD]\"\n-        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        input_str = [\"lower newer\", \"upper older longer string\"]\n-        image_input = self.prepare_image_inputs() * 2\n-        inputs = processor(\n-            text=input_str,\n-            images=image_input,\n-            return_tensors=\"pt\",\n-            crop_size={\"height\": 214, \"width\": 214},\n-            size={\"height\": 214, \"width\": 214},\n-            padding=\"longest\",\n-            max_length=76,\n-        )\n-        self.assertEqual(inputs[\"pixel_values\"].shape[2], 214)\n-\n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 7)",
            "previous_filename": "tests/models/donut/test_processing_donut.py"
        },
        {
            "sha": "39a47293040bdd320ff3d6b09727b72310e788e8",
            "filename": "tests/models/fuyu/test_processor_fuyu.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/62e8c759c348d1abc98414d293824de813eb15a6/tests%2Fmodels%2Ffuyu%2Ftest_processor_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62e8c759c348d1abc98414d293824de813eb15a6/tests%2Fmodels%2Ffuyu%2Ftest_processor_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffuyu%2Ftest_processor_fuyu.py?ref=62e8c759c348d1abc98414d293824de813eb15a6",
            "previous_filename": "tests/models/fuyu/test_processing_fuyu.py"
        },
        {
            "sha": "2fd569f99141af088adde78e6cc0bd740b433297",
            "filename": "tests/models/idefics2/test_processor_idefics2.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/62e8c759c348d1abc98414d293824de813eb15a6/tests%2Fmodels%2Fidefics2%2Ftest_processor_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62e8c759c348d1abc98414d293824de813eb15a6/tests%2Fmodels%2Fidefics2%2Ftest_processor_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics2%2Ftest_processor_idefics2.py?ref=62e8c759c348d1abc98414d293824de813eb15a6",
            "previous_filename": "tests/models/idefics2/test_processing_idefics2.py"
        },
        {
            "sha": "a53109b02b695113e879e9f2e25d6edf85b4c875",
            "filename": "tests/models/idefics3/test_processor_idefics3.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/62e8c759c348d1abc98414d293824de813eb15a6/tests%2Fmodels%2Fidefics3%2Ftest_processor_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62e8c759c348d1abc98414d293824de813eb15a6/tests%2Fmodels%2Fidefics3%2Ftest_processor_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics3%2Ftest_processor_idefics3.py?ref=62e8c759c348d1abc98414d293824de813eb15a6",
            "previous_filename": "tests/models/idefics3/test_processing_idefics3.py"
        },
        {
            "sha": "55f5980bfa1579d7a1f87308254de864790867e1",
            "filename": "tests/models/llava_onevision/test_processor_llava_onevision.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/62e8c759c348d1abc98414d293824de813eb15a6/tests%2Fmodels%2Fllava_onevision%2Ftest_processor_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62e8c759c348d1abc98414d293824de813eb15a6/tests%2Fmodels%2Fllava_onevision%2Ftest_processor_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_onevision%2Ftest_processor_llava_onevision.py?ref=62e8c759c348d1abc98414d293824de813eb15a6",
            "previous_filename": "tests/models/llava_onevision/test_processing_llava_onevision.py"
        },
        {
            "sha": "be1e855725d701b847644eeaf895b0d3fdd5e7c1",
            "filename": "tests/models/musicgen/test_processor_musicgen.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/62e8c759c348d1abc98414d293824de813eb15a6/tests%2Fmodels%2Fmusicgen%2Ftest_processor_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62e8c759c348d1abc98414d293824de813eb15a6/tests%2Fmodels%2Fmusicgen%2Ftest_processor_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmusicgen%2Ftest_processor_musicgen.py?ref=62e8c759c348d1abc98414d293824de813eb15a6",
            "previous_filename": "tests/models/musicgen/test_processing_musicgen.py"
        },
        {
            "sha": "04fb94c64c3da8c9738f2717f6f1197bdc13fb48",
            "filename": "tests/models/musicgen_melody/test_processor_musicgen_melody.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/62e8c759c348d1abc98414d293824de813eb15a6/tests%2Fmodels%2Fmusicgen_melody%2Ftest_processor_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62e8c759c348d1abc98414d293824de813eb15a6/tests%2Fmodels%2Fmusicgen_melody%2Ftest_processor_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmusicgen_melody%2Ftest_processor_musicgen_melody.py?ref=62e8c759c348d1abc98414d293824de813eb15a6",
            "patch": "@@ -50,7 +50,7 @@ def floats_list(shape, scale=1.0, rng=None, name=None):\n @require_torch\n @require_sentencepiece\n @require_torchaudio\n-# Copied from tests.models.musicgen.test_processing_musicgen.MusicgenProcessorTest with Musicgen->MusicgenMelody, Encodec->MusicgenMelody, padding_mask->attention_mask, input_values->input_features\n+# Copied from tests.models.musicgen.test_processor_musicgen.MusicgenProcessorTest with Musicgen->MusicgenMelody, Encodec->MusicgenMelody, padding_mask->attention_mask, input_values->input_features\n class MusicgenMelodyProcessorTest(unittest.TestCase):\n     def setUp(self):\n         # Ignore copy"
        },
        {
            "sha": "33b31507e17df2ae26ee88357a5e3a6f84e81ab2",
            "filename": "tests/models/paligemma/test_processing_paligemma.py",
            "status": "removed",
            "additions": 0,
            "deletions": 84,
            "changes": 84,
            "blob_url": "https://github.com/huggingface/transformers/blob/2f25ab95dbe220350aedf1b66fe6f2e12346c605/tests%2Fmodels%2Fpaligemma%2Ftest_processing_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2f25ab95dbe220350aedf1b66fe6f2e12346c605/tests%2Fmodels%2Fpaligemma%2Ftest_processing_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpaligemma%2Ftest_processing_paligemma.py?ref=2f25ab95dbe220350aedf1b66fe6f2e12346c605",
            "patch": "@@ -1,84 +0,0 @@\n-# Copyright 2024 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-import shutil\n-import tempfile\n-import unittest\n-\n-from transformers import AutoProcessor, GemmaTokenizerFast, PaliGemmaProcessor\n-from transformers.testing_utils import require_read_token, require_vision\n-from transformers.utils import is_vision_available\n-\n-from ...test_processing_common import ProcessorTesterMixin\n-\n-\n-if is_vision_available():\n-    from transformers import SiglipImageProcessor\n-\n-\n-@require_vision\n-@require_read_token\n-class PaliGemmaProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n-    processor_class = PaliGemmaProcessor\n-\n-    def setUp(self):\n-        self.tmpdirname = tempfile.mkdtemp()\n-        image_processor = SiglipImageProcessor(do_center_crop=False)\n-        tokenizer = GemmaTokenizerFast.from_pretrained(\"google/gemma-7b\")\n-        image_processor.image_seq_length = 32\n-\n-        processor = PaliGemmaProcessor(image_processor=image_processor, tokenizer=tokenizer)\n-        processor.save_pretrained(self.tmpdirname)\n-\n-    def get_tokenizer(self, **kwargs):\n-        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).tokenizer\n-\n-    def get_image_processor(self, **kwargs):\n-        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).image_processor\n-\n-    def tearDown(self):\n-        shutil.rmtree(self.tmpdirname)\n-\n-    def test_text_with_image_tokens(self):\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\")\n-\n-        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n-        text_multi_images = \"<image><image><bos>Dummy text!\"\n-        text_single_image = \"<image><bos>Dummy text!\"\n-        text_no_image = \"Dummy text!\"\n-\n-        image = self.prepare_image_inputs()[0]\n-\n-        out_noimage = processor(text=text_no_image, images=image, return_tensors=\"np\")\n-        out_singlimage = processor(text=text_single_image, images=image, return_tensors=\"np\")\n-        for k in out_noimage:\n-            self.assertTrue(out_noimage[k].tolist() == out_singlimage[k].tolist())\n-\n-        out_multiimages = processor(text=text_multi_images, images=[image, image], return_tensors=\"np\")\n-        out_noimage = processor(text=text_no_image, images=[[image, image]], return_tensors=\"np\")\n-\n-        # We can't be sure what is users intention, whether user want \"one text + two images\" or user forgot to add the second text\n-        with self.assertRaises(ValueError):\n-            out_noimage = processor(text=text_no_image, images=[image, image], return_tensors=\"np\")\n-\n-        for k in out_noimage:\n-            self.assertTrue(out_noimage[k].tolist() == out_multiimages[k].tolist())\n-\n-        text_batched = [\"Dummy text!\", \"Dummy text!\"]\n-        text_batched_with_image = [\"<image><bos>Dummy text!\", \"<image><bos>Dummy text!\"]\n-        out_images = processor(text=text_batched_with_image, images=[image, image], return_tensors=\"np\")\n-        out_noimage_nested = processor(text=text_batched, images=[[image], [image]], return_tensors=\"np\")\n-        out_noimage = processor(text=text_batched, images=[image, image], return_tensors=\"np\")\n-        for k in out_noimage:\n-            self.assertTrue(out_noimage[k].tolist() == out_images[k].tolist() == out_noimage_nested[k].tolist())"
        },
        {
            "sha": "245aff594125cf3e532bb5983b02a295cb31600a",
            "filename": "tests/models/paligemma/test_processor_paligemma.py",
            "status": "modified",
            "additions": 36,
            "deletions": 6,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/62e8c759c348d1abc98414d293824de813eb15a6/tests%2Fmodels%2Fpaligemma%2Ftest_processor_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62e8c759c348d1abc98414d293824de813eb15a6/tests%2Fmodels%2Fpaligemma%2Ftest_processor_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpaligemma%2Ftest_processor_paligemma.py?ref=62e8c759c348d1abc98414d293824de813eb15a6",
            "patch": "@@ -16,19 +16,15 @@\n import tempfile\n import unittest\n \n-from transformers import GemmaTokenizer\n+from transformers import GemmaTokenizer, PaliGemmaProcessor\n from transformers.testing_utils import get_tests_dir, require_torch, require_vision\n from transformers.utils import is_vision_available\n \n from ...test_processing_common import ProcessorTesterMixin\n \n \n if is_vision_available():\n-    from transformers import (\n-        PaliGemmaProcessor,\n-        SiglipImageProcessor,\n-        is_vision_available,\n-    )\n+    from transformers import SiglipImageProcessor\n \n SAMPLE_VOCAB = get_tests_dir(\"fixtures/test_sentencepiece.model\")\n \n@@ -61,3 +57,37 @@ def test_image_seq_length(self):\n             text=input_str, images=image_input, return_tensors=\"pt\", max_length=112, padding=\"max_length\"\n         )\n         self.assertEqual(len(inputs[\"input_ids\"][0]), 112 + 14)\n+\n+    def test_text_with_image_tokens(self):\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\")\n+\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        text_multi_images = \"<image><image><bos>Dummy text!\"\n+        text_single_image = \"<image><bos>Dummy text!\"\n+        text_no_image = \"Dummy text!\"\n+\n+        image = self.prepare_image_inputs()\n+\n+        out_noimage = processor(text=text_no_image, images=image, return_tensors=\"np\")\n+        out_singlimage = processor(text=text_single_image, images=image, return_tensors=\"np\")\n+        for k in out_noimage:\n+            self.assertTrue(out_noimage[k].tolist() == out_singlimage[k].tolist())\n+\n+        out_multiimages = processor(text=text_multi_images, images=[image, image], return_tensors=\"np\")\n+        out_noimage = processor(text=text_no_image, images=[[image, image]], return_tensors=\"np\")\n+\n+        # We can't be sure what is users intention, whether user want \"one text + two images\" or user forgot to add the second text\n+        with self.assertRaises(ValueError):\n+            out_noimage = processor(text=text_no_image, images=[image, image], return_tensors=\"np\")\n+\n+        for k in out_noimage:\n+            self.assertTrue(out_noimage[k].tolist() == out_multiimages[k].tolist())\n+\n+        text_batched = [\"Dummy text!\", \"Dummy text!\"]\n+        text_batched_with_image = [\"<image><bos>Dummy text!\", \"<image><bos>Dummy text!\"]\n+        out_images = processor(text=text_batched_with_image, images=[image, image], return_tensors=\"np\")\n+        out_noimage_nested = processor(text=text_batched, images=[[image], [image]], return_tensors=\"np\")\n+        out_noimage = processor(text=text_batched, images=[image, image], return_tensors=\"np\")\n+        for k in out_noimage:\n+            self.assertTrue(out_noimage[k].tolist() == out_images[k].tolist() == out_noimage_nested[k].tolist())"
        },
        {
            "sha": "a360fc98f4c58484c560c8fe4b9198249b18520a",
            "filename": "tests/models/qwen2_vl/test_processor_qwen2_vl.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/62e8c759c348d1abc98414d293824de813eb15a6/tests%2Fmodels%2Fqwen2_vl%2Ftest_processor_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62e8c759c348d1abc98414d293824de813eb15a6/tests%2Fmodels%2Fqwen2_vl%2Ftest_processor_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_processor_qwen2_vl.py?ref=62e8c759c348d1abc98414d293824de813eb15a6",
            "previous_filename": "tests/models/qwen2_vl/test_processing_qwen2_vl.py"
        }
    ],
    "stats": {
        "total": 216,
        "additions": 46,
        "deletions": 170
    }
}