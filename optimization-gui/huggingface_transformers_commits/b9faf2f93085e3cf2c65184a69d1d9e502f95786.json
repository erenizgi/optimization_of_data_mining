{
    "author": "dbleyl",
    "message": "Fix TypeError: 'NoneType' object is not iterable for esm (#38667) (#38668)\n\nAdd post_init() calls to EsmForMaskedLM, EsmForTokenClassification and EsmForSequenceClassification.",
    "sha": "b9faf2f93085e3cf2c65184a69d1d9e502f95786",
    "files": [
        {
            "sha": "7505268bb37cc4a7334083c89f256994c5add2ed",
            "filename": "src/transformers/models/esm/modeling_esm.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b9faf2f93085e3cf2c65184a69d1d9e502f95786/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b9faf2f93085e3cf2c65184a69d1d9e502f95786/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py?ref=b9faf2f93085e3cf2c65184a69d1d9e502f95786",
            "patch": "@@ -1023,6 +1023,8 @@ def __init__(self, config):\n \n         self.init_weights()\n \n+        self.post_init()\n+\n     def get_output_embeddings(self):\n         return self.lm_head.decoder\n \n@@ -1127,6 +1129,8 @@ def __init__(self, config):\n \n         self.init_weights()\n \n+        self.post_init()\n+\n     @auto_docstring\n     def forward(\n         self,\n@@ -1210,6 +1214,8 @@ def __init__(self, config):\n \n         self.init_weights()\n \n+        self.post_init()\n+\n     @auto_docstring\n     def forward(\n         self,"
        }
    ],
    "stats": {
        "total": 6,
        "additions": 6,
        "deletions": 0
    }
}