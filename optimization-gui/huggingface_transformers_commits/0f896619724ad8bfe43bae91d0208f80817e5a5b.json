{
    "author": "romitjain",
    "message": "Added kernels from kernel hub for Bamba model (#41540)\n\n* Added kernels from kernel hub for Bamba model\n\n* Updated kernel loading\n\nSigned-off-by: romit <romit@ibm.com>\n\n* Remove einops\n\nSigned-off-by: romit <romit@ibm.com>\n\n* Removed global vars\n\nSigned-off-by: romit <romit@ibm.com>\n\n* Fixed make style\n\nSigned-off-by: romit <romit@ibm.com>\n\n* Nit\n\nSigned-off-by: romit <romit@ibm.com>\n\n* Added modeling files\n\nSigned-off-by: romit <romit@ibm.com>\n\n* Fixed merge conflict\n\nSigned-off-by: romit <romit@ibm.com>\n\n* fixed lint\n\nSigned-off-by: romitjain <romit@ibm.com>\n\n* Removed global import\n\n* Small updates\n\n* Updated\n\n* Resolved merge conflicts\n\n* Fixed the nested import\n\nSigned-off-by: romit <romit@ibm.com>\n\n* Moved imports inside mixer\n\nSigned-off-by: romit <romit@ibm.com>\n\n* CI CD fix\n\nSigned-off-by: romit <romit@ibm.com>\n\n---------\n\nSigned-off-by: romit <romit@ibm.com>\nSigned-off-by: romitjain <romit@ibm.com>\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>",
    "sha": "0f896619724ad8bfe43bae91d0208f80817e5a5b",
    "files": [
        {
            "sha": "c2d874474750a66ed1e23e29f53f9b130d07fe11",
            "filename": "src/transformers/integrations/hub_kernels.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f896619724ad8bfe43bae91d0208f80817e5a5b/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f896619724ad8bfe43bae91d0208f80817e5a5b/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py?ref=0f896619724ad8bfe43bae91d0208f80817e5a5b",
            "patch": "@@ -370,7 +370,7 @@ def lazy_load_kernel(kernel_name: str, mapping: dict[str, ModuleType | None] = _\n         if callable(is_kernel_available) and is_kernel_available():\n             # Try to import the module \"{kernel_name}\" from parent package level\n             try:\n-                module = importlib.import_module(f\"{kernel_name}\")\n+                module = importlib.import_module(f\"{new_kernel_name}\")\n                 mapping[kernel_name] = module\n                 return module\n             except Exception:"
        },
        {
            "sha": "afcce31c7c62dd06e5d60453835692d4fff74871",
            "filename": "src/transformers/models/bamba/modeling_bamba.py",
            "status": "modified",
            "additions": 15,
            "deletions": 16,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f896619724ad8bfe43bae91d0208f80817e5a5b/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f896619724ad8bfe43bae91d0208f80817e5a5b/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py?ref=0f896619724ad8bfe43bae91d0208f80817e5a5b",
            "patch": "@@ -36,6 +36,7 @@\n from ...cache_utils import Cache\n from ...generation import GenerationMixin\n from ...integrations import use_kernel_forward_from_hub, use_kernelized_func\n+from ...integrations.hub_kernels import lazy_load_kernel\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n@@ -44,22 +45,9 @@\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n from ...utils.generic import maybe_autocast\n-from ...utils.import_utils import is_causal_conv1d_available, is_mamba_2_ssm_available\n from .configuration_bamba import BambaConfig\n \n \n-if is_mamba_2_ssm_available():\n-    from mamba_ssm.ops.triton.selective_state_update import selective_state_update\n-    from mamba_ssm.ops.triton.ssd_combined import mamba_chunk_scan_combined, mamba_split_conv1d_scan_combined\n-else:\n-    selective_state_update = None\n-\n-if is_causal_conv1d_available():\n-    from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\n-else:\n-    causal_conv1d_update, causal_conv1d_fn = None, None\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -501,9 +489,6 @@ def apply_mask_to_padding_states(hidden_states, attention_mask):\n     return hidden_states\n \n \n-is_fast_path_available = all((selective_state_update, causal_conv1d_fn, causal_conv1d_update))\n-\n-\n # Adapted from transformers.models.mamba2.modeling_mamba2.Mamba2Mixer\n class BambaMixer(nn.Module):\n     \"\"\"\n@@ -575,6 +560,20 @@ def __init__(self, config: BambaConfig, layer_idx: int):\n \n         self.out_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=self.use_bias)\n \n+        global causal_conv1d_update, causal_conv1d_fn\n+        causal_conv1d = lazy_load_kernel(\"causal-conv1d\")\n+        causal_conv1d_update = getattr(causal_conv1d, \"causal_conv1d_update\", None)\n+        causal_conv1d_fn = getattr(causal_conv1d, \"causal_conv1d_fn\", None)\n+\n+        global selective_state_update, mamba_chunk_scan_combined, mamba_split_conv1d_scan_combined\n+        mamba_ssm = lazy_load_kernel(\"mamba-ssm\")\n+        selective_state_update = getattr(mamba_ssm, \"selective_state_update\", None)\n+        mamba_chunk_scan_combined = getattr(mamba_ssm, \"mamba_chunk_scan_combined\", None)\n+        mamba_split_conv1d_scan_combined = getattr(mamba_ssm, \"mamba_split_conv1d_scan_combined\", None)\n+\n+        global is_fast_path_available\n+        is_fast_path_available = all((selective_state_update, causal_conv1d_fn, causal_conv1d_update))\n+\n         if not is_fast_path_available:\n             logger.warning_once(\n                 \"The fast path is not available because one of `(selective_state_update, causal_conv1d_fn, causal_conv1d_update)`\""
        },
        {
            "sha": "de8e7e77693dd0780b04cc24bdd7627a494264e0",
            "filename": "src/transformers/models/bamba/modular_bamba.py",
            "status": "modified",
            "additions": 15,
            "deletions": 15,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f896619724ad8bfe43bae91d0208f80817e5a5b/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f896619724ad8bfe43bae91d0208f80817e5a5b/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py?ref=0f896619724ad8bfe43bae91d0208f80817e5a5b",
            "patch": "@@ -43,6 +43,7 @@\n )\n \n from ... import initialization as init\n+from ...integrations.hub_kernels import lazy_load_kernel\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_utils import PreTrainedModel\n@@ -52,24 +53,9 @@\n     can_return_tuple,\n     logging,\n )\n-from ...utils.import_utils import is_causal_conv1d_available, is_mamba_2_ssm_available\n from .configuration_bamba import BambaConfig\n \n \n-if is_mamba_2_ssm_available():\n-    from mamba_ssm.ops.triton.selective_state_update import selective_state_update\n-    from mamba_ssm.ops.triton.ssd_combined import mamba_chunk_scan_combined, mamba_split_conv1d_scan_combined\n-else:\n-    selective_state_update = None\n-\n-if is_causal_conv1d_available():\n-    from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\n-else:\n-    causal_conv1d_update, causal_conv1d_fn = None, None\n-\n-is_fast_path_available = all((selective_state_update, causal_conv1d_fn, causal_conv1d_update))\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -276,6 +262,20 @@ def __init__(self, config: BambaConfig, layer_idx: int):\n \n         self.out_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=self.use_bias)\n \n+        global causal_conv1d_update, causal_conv1d_fn\n+        causal_conv1d = lazy_load_kernel(\"causal-conv1d\")\n+        causal_conv1d_update = getattr(causal_conv1d, \"causal_conv1d_update\", None)\n+        causal_conv1d_fn = getattr(causal_conv1d, \"causal_conv1d_fn\", None)\n+\n+        global selective_state_update, mamba_chunk_scan_combined, mamba_split_conv1d_scan_combined\n+        mamba_ssm = lazy_load_kernel(\"mamba-ssm\")\n+        selective_state_update = getattr(mamba_ssm, \"selective_state_update\", None)\n+        mamba_chunk_scan_combined = getattr(mamba_ssm, \"mamba_chunk_scan_combined\", None)\n+        mamba_split_conv1d_scan_combined = getattr(mamba_ssm, \"mamba_split_conv1d_scan_combined\", None)\n+\n+        global is_fast_path_available\n+        is_fast_path_available = all((selective_state_update, causal_conv1d_fn, causal_conv1d_update))\n+\n         if not is_fast_path_available:\n             logger.warning_once(\n                 \"The fast path is not available because one of `(selective_state_update, causal_conv1d_fn, causal_conv1d_update)`\""
        },
        {
            "sha": "0af5a547a2c4cedadcd11b10c631ca8d40259dc7",
            "filename": "src/transformers/models/granitemoehybrid/modeling_granitemoehybrid.py",
            "status": "modified",
            "additions": 15,
            "deletions": 16,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f896619724ad8bfe43bae91d0208f80817e5a5b/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f896619724ad8bfe43bae91d0208f80817e5a5b/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py?ref=0f896619724ad8bfe43bae91d0208f80817e5a5b",
            "patch": "@@ -32,6 +32,7 @@\n from ...cache_utils import Cache\n from ...generation import GenerationMixin\n from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub, use_kernelized_func\n+from ...integrations.hub_kernels import lazy_load_kernel\n from ...masking_utils import create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n@@ -40,22 +41,9 @@\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n from ...utils.generic import check_model_inputs, maybe_autocast\n-from ...utils.import_utils import is_causal_conv1d_available, is_mamba_2_ssm_available\n from .configuration_granitemoehybrid import GraniteMoeHybridConfig\n \n \n-if is_mamba_2_ssm_available():\n-    from mamba_ssm.ops.triton.selective_state_update import selective_state_update\n-    from mamba_ssm.ops.triton.ssd_combined import mamba_chunk_scan_combined, mamba_split_conv1d_scan_combined\n-else:\n-    selective_state_update = None\n-\n-if is_causal_conv1d_available():\n-    from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\n-else:\n-    causal_conv1d_update, causal_conv1d_fn = None, None\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -376,9 +364,6 @@ def apply_mask_to_padding_states(hidden_states, attention_mask):\n     return hidden_states\n \n \n-is_fast_path_available = all((selective_state_update, causal_conv1d_fn, causal_conv1d_update))\n-\n-\n # Adapted from transformers.models.mamba2.modeling_mamba2.Mamba2Mixer\n class GraniteMoeHybridMambaLayer(nn.Module):\n     \"\"\"\n@@ -450,6 +435,20 @@ def __init__(self, config: GraniteMoeHybridConfig, layer_idx: int):\n \n         self.out_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=self.use_bias)\n \n+        global causal_conv1d_update, causal_conv1d_fn\n+        causal_conv1d = lazy_load_kernel(\"causal-conv1d\")\n+        causal_conv1d_update = getattr(causal_conv1d, \"causal_conv1d_update\", None)\n+        causal_conv1d_fn = getattr(causal_conv1d, \"causal_conv1d_fn\", None)\n+\n+        global selective_state_update, mamba_chunk_scan_combined, mamba_split_conv1d_scan_combined\n+        mamba_ssm = lazy_load_kernel(\"mamba-ssm\")\n+        selective_state_update = getattr(mamba_ssm, \"selective_state_update\", None)\n+        mamba_chunk_scan_combined = getattr(mamba_ssm, \"mamba_chunk_scan_combined\", None)\n+        mamba_split_conv1d_scan_combined = getattr(mamba_ssm, \"mamba_split_conv1d_scan_combined\", None)\n+\n+        global is_fast_path_available\n+        is_fast_path_available = all((selective_state_update, causal_conv1d_fn, causal_conv1d_update))\n+\n         if not is_fast_path_available:\n             logger.warning_once(\n                 \"The fast path is not available because one of `(selective_state_update, causal_conv1d_fn, causal_conv1d_update)`\""
        },
        {
            "sha": "0797dbb31dd056b2a2794279a24feabf3ce6810d",
            "filename": "src/transformers/models/jamba/modeling_jamba.py",
            "status": "modified",
            "additions": 17,
            "deletions": 18,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f896619724ad8bfe43bae91d0208f80817e5a5b/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f896619724ad8bfe43bae91d0208f80817e5a5b/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py?ref=0f896619724ad8bfe43bae91d0208f80817e5a5b",
            "patch": "@@ -33,29 +33,17 @@\n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub, use_kernelized_func\n+from ...integrations.hub_kernels import lazy_load_kernel\n from ...masking_utils import create_causal_mask\n from ...modeling_layers import GenericForSequenceClassification, GradientCheckpointingLayer\n from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n from ...utils.generic import OutputRecorder, check_model_inputs\n-from ...utils.import_utils import is_causal_conv1d_available, is_mamba_ssm_available\n from .configuration_jamba import JambaConfig\n \n \n-if is_mamba_ssm_available():\n-    from mamba_ssm.ops.selective_scan_interface import mamba_inner_fn, selective_scan_fn\n-    from mamba_ssm.ops.triton.selective_state_update import selective_state_update\n-else:\n-    selective_state_update, selective_scan_fn, mamba_inner_fn = None, None, None\n-\n-if is_causal_conv1d_available():\n-    from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\n-else:\n-    causal_conv1d_update, causal_conv1d_fn = None, None\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -306,11 +294,6 @@ def forward(\n         return attn_output, attn_weights\n \n \n-is_fast_path_available = all(\n-    (selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)\n-)\n-\n-\n class JambaMambaMixer(nn.Module):\n     \"\"\"\n     Compute âˆ†, A, B, C, and D the state space parameters and compute the `contextualized_states`.\n@@ -364,6 +347,22 @@ def __init__(self, config: JambaConfig, layer_idx):\n         self.b_layernorm = JambaRMSNorm(self.ssm_state_size, eps=config.rms_norm_eps)\n         self.c_layernorm = JambaRMSNorm(self.ssm_state_size, eps=config.rms_norm_eps)\n \n+        global causal_conv1d_update, causal_conv1d_fn\n+        causal_conv1d = lazy_load_kernel(\"causal-conv1d\")\n+        causal_conv1d_update = getattr(causal_conv1d, \"causal_conv1d_update\", None)\n+        causal_conv1d_fn = getattr(causal_conv1d, \"causal_conv1d_fn\", None)\n+\n+        global selective_state_update, mamba_inner_fn, selective_scan_fn\n+        mamba_ssm = lazy_load_kernel(\"mamba-ssm\")\n+        selective_state_update = getattr(mamba_ssm, \"selective_state_update\", None)\n+        mamba_inner_fn = getattr(mamba_ssm, \"mamba_inner_fn\", None)\n+        selective_scan_fn = getattr(mamba_ssm, \"selective_scan_fn\", None)\n+\n+        global is_fast_path_available\n+        is_fast_path_available = all(\n+            (selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)\n+        )\n+\n         if not is_fast_path_available:\n             logger.warning_once(\n                 \"The fast path is not available because on of `(selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)`\""
        },
        {
            "sha": "4bfdbc51c7cea1d5a205b9d2ece22f24051f63f2",
            "filename": "src/transformers/models/jamba/modular_jamba.py",
            "status": "modified",
            "additions": 17,
            "deletions": 17,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f896619724ad8bfe43bae91d0208f80817e5a5b/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodular_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f896619724ad8bfe43bae91d0208f80817e5a5b/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodular_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodular_jamba.py?ref=0f896619724ad8bfe43bae91d0208f80817e5a5b",
            "patch": "@@ -25,36 +25,20 @@\n \n from ... import initialization as init\n from ...activations import ACT2FN\n+from ...integrations.hub_kernels import lazy_load_kernel\n from ...masking_utils import create_causal_mask\n from ...modeling_layers import GenericForSequenceClassification, GradientCheckpointingLayer\n from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, logging\n from ...utils.generic import OutputRecorder, check_model_inputs\n-from ...utils.import_utils import is_causal_conv1d_available, is_mamba_ssm_available\n from ..llama.modeling_llama import LlamaAttention, LlamaRMSNorm, eager_attention_forward\n from ..mistral.modeling_mistral import MistralMLP\n from ..mixtral.modeling_mixtral import MixtralExperts, MixtralForCausalLM\n from .configuration_jamba import JambaConfig\n \n \n-if is_mamba_ssm_available():\n-    from mamba_ssm.ops.selective_scan_interface import mamba_inner_fn, selective_scan_fn\n-    from mamba_ssm.ops.triton.selective_state_update import selective_state_update\n-else:\n-    selective_state_update, selective_scan_fn, mamba_inner_fn = None, None, None\n-\n-if is_causal_conv1d_available():\n-    from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\n-else:\n-    causal_conv1d_update, causal_conv1d_fn = None, None\n-\n-is_fast_path_available = all(\n-    (selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)\n-)\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -258,6 +242,22 @@ def __init__(self, config: JambaConfig, layer_idx):\n         self.b_layernorm = JambaRMSNorm(self.ssm_state_size, eps=config.rms_norm_eps)\n         self.c_layernorm = JambaRMSNorm(self.ssm_state_size, eps=config.rms_norm_eps)\n \n+        global causal_conv1d_update, causal_conv1d_fn\n+        causal_conv1d = lazy_load_kernel(\"causal-conv1d\")\n+        causal_conv1d_update = getattr(causal_conv1d, \"causal_conv1d_update\", None)\n+        causal_conv1d_fn = getattr(causal_conv1d, \"causal_conv1d_fn\", None)\n+\n+        global selective_state_update, mamba_inner_fn, selective_scan_fn\n+        mamba_ssm = lazy_load_kernel(\"mamba-ssm\")\n+        selective_state_update = getattr(mamba_ssm, \"selective_state_update\", None)\n+        mamba_inner_fn = getattr(mamba_ssm, \"mamba_inner_fn\", None)\n+        selective_scan_fn = getattr(mamba_ssm, \"selective_scan_fn\", None)\n+\n+        global is_fast_path_available\n+        is_fast_path_available = all(\n+            (selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)\n+        )\n+\n         if not is_fast_path_available:\n             logger.warning_once(\n                 \"The fast path is not available because on of `(selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)`\""
        },
        {
            "sha": "caa370b5a309e85834d89f2ec4c3e1b5aa02e119",
            "filename": "src/transformers/models/mamba2/modeling_mamba2.py",
            "status": "modified",
            "additions": 23,
            "deletions": 23,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f896619724ad8bfe43bae91d0208f80817e5a5b/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f896619724ad8bfe43bae91d0208f80817e5a5b/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py?ref=0f896619724ad8bfe43bae91d0208f80817e5a5b",
            "patch": "@@ -24,42 +24,20 @@\n from ... import initialization as init\n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n+from ...integrations.hub_kernels import lazy_load_kernel\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n     ModelOutput,\n     auto_docstring,\n     logging,\n )\n-from ...utils.import_utils import is_causal_conv1d_available, is_mamba_2_ssm_available\n from .configuration_mamba2 import Mamba2Config\n \n \n logger = logging.get_logger(__name__)\n \n \n-if is_mamba_2_ssm_available():\n-    from mamba_ssm.ops.triton.selective_state_update import selective_state_update\n-    from mamba_ssm.ops.triton.ssd_combined import mamba_chunk_scan_combined, mamba_split_conv1d_scan_combined\n-else:\n-    mamba_chunk_scan_combined, mamba_split_conv1d_scan_combined, selective_state_update = None, None, None\n-\n-if is_causal_conv1d_available():\n-    from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\n-else:\n-    causal_conv1d_update, causal_conv1d_fn = None, None\n-\n-is_fast_path_available = all(\n-    (\n-        selective_state_update,\n-        mamba_chunk_scan_combined,\n-        mamba_split_conv1d_scan_combined,\n-        causal_conv1d_fn,\n-        causal_conv1d_update,\n-    )\n-)\n-\n-\n # Helper methods for segment sum computation\n \n \n@@ -286,6 +264,28 @@ def __init__(self, config: Mamba2Config, layer_idx: int):\n         self.out_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=config.use_bias)\n         self.use_bias = config.use_bias\n \n+        global causal_conv1d_update, causal_conv1d_fn\n+        causal_conv1d = lazy_load_kernel(\"causal-conv1d\")\n+        causal_conv1d_update = getattr(causal_conv1d, \"causal_conv1d_update\", None)\n+        causal_conv1d_fn = getattr(causal_conv1d, \"causal_conv1d_fn\", None)\n+\n+        global selective_state_update, mamba_chunk_scan_combined, mamba_split_conv1d_scan_combined\n+        mamba_ssm = lazy_load_kernel(\"mamba-ssm\")\n+        selective_state_update = getattr(mamba_ssm, \"selective_state_update\", None)\n+        mamba_chunk_scan_combined = getattr(mamba_ssm, \"mamba_chunk_scan_combined\", None)\n+        mamba_split_conv1d_scan_combined = getattr(mamba_ssm, \"mamba_split_conv1d_scan_combined\", None)\n+\n+        global is_fast_path_available\n+        is_fast_path_available = all(\n+            (\n+                selective_state_update,\n+                mamba_chunk_scan_combined,\n+                mamba_split_conv1d_scan_combined,\n+                causal_conv1d_fn,\n+                causal_conv1d_update,\n+            )\n+        )\n+\n         if not is_fast_path_available:\n             logger.warning_once(\n                 \"The fast path is not available because one of `(selective_state_update, causal_conv1d_fn, causal_conv1d_update)`\""
        },
        {
            "sha": "535ec9a8a075e94128b0144568e84791f67e2077",
            "filename": "src/transformers/models/qwen3_next/modeling_qwen3_next.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f896619724ad8bfe43bae91d0208f80817e5a5b/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodeling_qwen3_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f896619724ad8bfe43bae91d0208f80817e5a5b/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodeling_qwen3_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodeling_qwen3_next.py?ref=0f896619724ad8bfe43bae91d0208f80817e5a5b",
            "patch": "@@ -45,10 +45,7 @@\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n from ...utils.generic import OutputRecorder, check_model_inputs, maybe_autocast\n-from ...utils.import_utils import (\n-    is_causal_conv1d_available,\n-    is_flash_linear_attention_available,\n-)\n+from ...utils.import_utils import is_causal_conv1d_available, is_flash_linear_attention_available\n from .configuration_qwen3_next import Qwen3NextConfig\n \n "
        }
    ],
    "stats": {
        "total": 214,
        "additions": 104,
        "deletions": 110
    }
}