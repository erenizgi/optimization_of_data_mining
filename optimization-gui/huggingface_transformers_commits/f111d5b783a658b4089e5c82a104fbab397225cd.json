{
    "author": "yonigozlan",
    "message": "Uniformize kwargs for Paligemma processor and update docs (#33571)\n\n* Uniformize paligemma processor\r\n\r\n* nit",
    "sha": "f111d5b783a658b4089e5c82a104fbab397225cd",
    "files": [
        {
            "sha": "41d785bba29dba6aa8552de2e9f8bf4131b11d32",
            "filename": "docs/source/en/model_doc/paligemma.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f111d5b783a658b4089e5c82a104fbab397225cd/docs%2Fsource%2Fen%2Fmodel_doc%2Fpaligemma.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f111d5b783a658b4089e5c82a104fbab397225cd/docs%2Fsource%2Fen%2Fmodel_doc%2Fpaligemma.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fpaligemma.md?ref=f111d5b783a658b4089e5c82a104fbab397225cd",
            "patch": "@@ -41,7 +41,7 @@ processor = AutoProcessor.from_pretrained(model_id)\n prompt = \"What is on the flower?\"\n image_file = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg?download=true\"\n raw_image = Image.open(requests.get(image_file, stream=True).raw)\n-inputs = processor(prompt, raw_image, return_tensors=\"pt\")\n+inputs = processor(raw_image, prompt, return_tensors=\"pt\")\n output = model.generate(**inputs, max_new_tokens=20)\n \n print(processor.decode(output[0], skip_special_tokens=True)[len(prompt):])\n@@ -53,7 +53,7 @@ print(processor.decode(output[0], skip_special_tokens=True)[len(prompt):])\n ```python\n prompt = \"What is on the flower?\"\n answer = \"a bee\"\n-inputs = processor(text=prompt, images=raw_image, suffix=answer, return_tensors=\"pt\")\n+inputs = processor(images=raw_image, text=prompt, suffix=answer, return_tensors=\"pt\")\n ```\n \n ## Resources"
        },
        {
            "sha": "48fffb6b428df76067618055abfda5f7b687dd93",
            "filename": "src/transformers/models/paligemma/modeling_paligemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f111d5b783a658b4089e5c82a104fbab397225cd/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f111d5b783a658b4089e5c82a104fbab397225cd/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py?ref=f111d5b783a658b4089e5c82a104fbab397225cd",
            "patch": "@@ -443,7 +443,7 @@ def forward(\n         >>> url = \"https://huggingface.co/gv-hf/PaliGemma-test-224px-hf/resolve/main/cow_beach_1.png\"\n         >>> image = Image.open(requests.get(url, stream=True).raw)\n \n-        >>> inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n+        >>> inputs = processor(images=image, text=prompt,  return_tensors=\"pt\")\n \n         >>> # Generate\n         >>> generate_ids = model.generate(**inputs, max_length=30)"
        },
        {
            "sha": "4457b6fe957bf3935b9a4618bbd5d762078ba5ba",
            "filename": "src/transformers/models/paligemma/processing_paligemma.py",
            "status": "modified",
            "additions": 52,
            "deletions": 62,
            "changes": 114,
            "blob_url": "https://github.com/huggingface/transformers/blob/f111d5b783a658b4089e5c82a104fbab397225cd/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fprocessing_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f111d5b783a658b4089e5c82a104fbab397225cd/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fprocessing_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fprocessing_paligemma.py?ref=f111d5b783a658b4089e5c82a104fbab397225cd",
            "patch": "@@ -21,15 +21,19 @@\n \n from ...feature_extraction_utils import BatchFeature\n from ...image_utils import ImageInput, is_valid_image\n-from ...processing_utils import ProcessorMixin\n+from ...processing_utils import (\n+    ImagesKwargs,\n+    ProcessingKwargs,\n+    ProcessorMixin,\n+    TextKwargs,\n+    Unpack,\n+    _validate_images_text_input_order,\n+)\n from ...tokenization_utils_base import (\n     AddedToken,\n-    PaddingStrategy,\n     PreTokenizedInput,\n     TextInput,\n-    TruncationStrategy,\n )\n-from ...utils import TensorType\n \n \n logger = logging.getLogger(__name__)\n@@ -38,6 +42,27 @@\n EXTRA_TOKENS = [f\"<loc{i:0>4}>\" for i in range(1024)] + [f\"<seg{i:0>3}>\" for i in range(128)]\n \n \n+class PaliGemmaTextKwargs(TextKwargs):\n+    suffix: Optional[Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]]]\n+\n+\n+class PaliGemmaImagesKwargs(ImagesKwargs):\n+    do_convert_rgb: Optional[bool]\n+\n+\n+class PaliGemmaProcessorKwargs(ProcessingKwargs, total=False):\n+    text_kwargs: PaliGemmaTextKwargs\n+    images_kwargs: PaliGemmaImagesKwargs\n+    _defaults = {\n+        \"text_kwargs\": {\n+            \"padding\": False,\n+        },\n+        \"images_kwargs\": {\n+            \"data_format\": \"channels_first\",\n+        },\n+    }\n+\n+\n # Copied from transformers.models.idefics2.processing_idefics2.is_url\n def is_url(val) -> bool:\n     return isinstance(val, str) and val.startswith(\"http\")\n@@ -122,27 +147,11 @@ def __init__(\n \n     def __call__(\n         self,\n-        text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]] = None,\n         images: ImageInput = None,\n-        tokenize_newline_separately: bool = True,\n-        padding: Union[bool, str, PaddingStrategy] = False,\n-        truncation: Union[bool, str, TruncationStrategy] = None,\n-        max_length=None,\n-        return_tensors: Optional[Union[str, TensorType]] = TensorType.PYTORCH,\n-        do_resize: bool = None,\n-        do_normalize: bool = None,\n-        image_mean: Optional[Union[float, List[float]]] = None,\n-        image_std: Optional[Union[float, List[float]]] = None,\n-        data_format: Optional[\"ChannelDimension\"] = \"channels_first\",  # noqa: F821\n-        input_data_format: Optional[\n-            Union[str, \"ChannelDimension\"]  # noqa: F821\n-        ] = None,\n-        resample: \"PILImageResampling\" = None,  # noqa: F821\n-        do_convert_rgb: bool = None,\n-        do_thumbnail: bool = None,\n-        do_align_long_axis: bool = None,\n-        do_rescale: bool = None,\n-        suffix: Optional[Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]]] = None,\n+        text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]] = None,\n+        audio=None,\n+        videos=None,\n+        **kwargs: Unpack[PaliGemmaProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\"\n         Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n@@ -171,29 +180,14 @@ def __call__(\n \n \n         Args:\n-            text (`str`, `List[str]`, `List[List[str]]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n             images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):\n                 The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n                 tensor. In case of a NumPy array/PyTorch tensor, each image should be of shape (C, H, W), where C is a\n                 number of channels, H and W are image height and width.\n-            tokenize_newline_separately (`bool`, defaults to `True`):\n-                Adds a separately tokenized '\\n' at the end of the prompt.\n-            padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n-                Select a strategy to pad the returned sequences (according to the model's padding side and padding\n-                index) among:\n-                - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n-                  sequence if provided).\n-                - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n-                  acceptable input length for the model if that argument is not provided.\n-                - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n-                  lengths).\n-            max_length (`int`, *optional*):\n-                Maximum length of the returned list and optionally padding length (see above).\n-            truncation (`bool`, *optional*):\n-                Activates truncation to cut input sequences longer than `max_length` to `max_length`.\n+            text (`str`, `List[str]`, `List[List[str]]`):\n+                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n+                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n+                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n             return_tensors (`str` or [`~utils.TensorType`], *optional*):\n                 If set, will return tensors of a particular framework. Acceptable values are:\n \n@@ -216,6 +210,15 @@ def __call__(\n             - **pixel_values** -- Pixel values to be fed to a model. Returned when `images` is not `None`.\n             - **labels** -- Labels compatible with training if `suffix` is not None\n         \"\"\"\n+        # check if images and text inputs are reversed for BC\n+        images, text = _validate_images_text_input_order(images, text)\n+\n+        output_kwargs = self._merge_kwargs(\n+            PaliGemmaProcessorKwargs,\n+            tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n+            **kwargs,\n+        )\n+        suffix = output_kwargs[\"text_kwargs\"].pop(\"suffix\", None)\n \n         return_token_type_ids = True if suffix is not None else False\n \n@@ -251,30 +254,17 @@ def __call__(\n             for prompt in text\n         ]\n \n-        pixel_values = self.image_processor(\n-            images,\n-            do_resize=do_resize,\n-            do_normalize=do_normalize,\n-            return_tensors=return_tensors,\n-            image_mean=image_mean,\n-            image_std=image_std,\n-            input_data_format=input_data_format,\n-            data_format=data_format,\n-            resample=resample,\n-            do_convert_rgb=do_convert_rgb,\n-        )[\"pixel_values\"]\n-\n-        if max_length is not None:\n-            max_length += self.image_seq_length  # max_length has to account for the image tokens\n+        pixel_values = self.image_processor(images, **output_kwargs[\"images_kwargs\"])[\"pixel_values\"]\n+\n+        # max_length has to account for the image tokens\n+        if output_kwargs[\"text_kwargs\"].get(\"max_length\", None) is not None:\n+            output_kwargs[\"text_kwargs\"][\"max_length\"] += self.image_seq_length\n \n         inputs = self.tokenizer(\n             input_strings,\n             text_pair=suffix,\n-            return_tensors=return_tensors,\n-            padding=padding,\n-            max_length=max_length,\n-            truncation=truncation,\n             return_token_type_ids=return_token_type_ids,\n+            **output_kwargs[\"text_kwargs\"],\n         )\n \n         return_data = {**inputs, \"pixel_values\": pixel_values}"
        },
        {
            "sha": "3918292133b406d0dec371610c1c9eeade5588aa",
            "filename": "tests/models/paligemma/test_modeling_paligemma.py",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/f111d5b783a658b4089e5c82a104fbab397225cd/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f111d5b783a658b4089e5c82a104fbab397225cd/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py?ref=f111d5b783a658b4089e5c82a104fbab397225cd",
            "patch": "@@ -337,7 +337,7 @@ def test_small_model_integration_test(self):\n             \"https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/cow_beach_1.png\"\n         )\n         raw_image = Image.open(requests.get(image_file, stream=True).raw)\n-        inputs = self.processor(text=prompt, images=raw_image, return_tensors=\"pt\")\n+        inputs = self.processor(images=raw_image, text=prompt, return_tensors=\"pt\")\n         EXPECTED_INPUT_IDS = torch.tensor([[257152] * 256 + [2, 108]])\n         self.assertTrue(torch.equal(inputs[\"input_ids\"], EXPECTED_INPUT_IDS))\n \n@@ -360,7 +360,7 @@ def test_small_model_integration_test_paligemma_VQA(self):\n             \"https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/cow_beach_1.png\"\n         )\n         raw_image = Image.open(requests.get(image_file, stream=True).raw)\n-        inputs = self.processor(text=prompt, images=raw_image, return_tensors=\"pt\").to(torch.float16)\n+        inputs = self.processor(images=raw_image, text=prompt, return_tensors=\"pt\").to(torch.float16)\n \n         output = model.generate(**inputs, max_new_tokens=900, do_sample=False)\n         EXPECTED_DECODED_TEXT = \"answer en Where is the cow standing?\\nbeach\"  # fmt: skip\n@@ -382,7 +382,7 @@ def test_small_model_integration_test_paligemma_empty_prompt(self):\n             \"https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/cow_beach_1.png\"\n         )\n         raw_image = Image.open(requests.get(image_file, stream=True).raw)\n-        inputs = self.processor(text=prompt, images=raw_image, return_tensors=\"pt\").to(torch.float16)\n+        inputs = self.processor(images=raw_image, text=prompt, return_tensors=\"pt\").to(torch.float16)\n \n         output = model.generate(**inputs, max_new_tokens=900, do_sample=False)\n         EXPECTED_DECODED_TEXT = \"\\ncow on the beach\"  # fmt: skip\n@@ -412,7 +412,7 @@ def test_small_model_integration_test_paligemma_batched(self):\n         )\n         image2 = image1\n \n-        inputs = self.processor(text=prompts, images=[image1, image2], return_tensors=\"pt\", padding=True)\n+        inputs = self.processor(images=[image1, image2], text=prompts, return_tensors=\"pt\", padding=True)\n \n         output = model.generate(**inputs, max_new_tokens=20)\n \n@@ -443,7 +443,7 @@ def test_small_model_integration_test_paligemma_batched_bf16(self):\n         image2 = image1\n \n         inputs = (\n-            self.processor(text=prompts, images=[image1, image2], return_tensors=\"pt\", padding=True)\n+            self.processor(images=[image1, image2], text=prompts, return_tensors=\"pt\", padding=True)\n             .to(torch.bfloat16)\n             .to(torch_device)\n         )\n@@ -475,7 +475,7 @@ def test_small_model_integration_test_paligemma_batched_f16(self):\n         image2 = image1\n \n         inputs = (\n-            self.processor(text=prompts, images=[image1, image2], return_tensors=\"pt\", padding=True)\n+            self.processor(images=[image1, image2], text=prompts, return_tensors=\"pt\", padding=True)\n             .to(torch.float16)\n             .to(torch_device)\n         )\n@@ -504,7 +504,7 @@ def test_integration_detection_bug(self):\n             ).raw\n         )\n \n-        inputs = self.processor(text=prompt, images=image, return_tensors=\"pt\").to(torch.bfloat16).to(torch_device)\n+        inputs = self.processor(images=image, text=prompt, return_tensors=\"pt\").to(torch.bfloat16).to(torch_device)\n \n         output = model.generate(**inputs, max_new_tokens=20)\n \n@@ -528,8 +528,8 @@ def test_paligemma_index_error_bug(self):\n \n         raw_image = Image.open(requests.get(image_file, stream=True).raw)\n         inputs = self.processor(\n-            text=prompt,\n             images=raw_image,\n+            text=prompt,\n             return_tensors=\"pt\",\n         ).to(torch.float16)\n \n@@ -561,7 +561,7 @@ def test_paligemma_finetuning_with_suffixes_bf16(self):\n         image2 = image1\n \n         inputs = (\n-            self.processor(text=prompts, suffix=suffixes, images=[image1, image2], return_tensors=\"pt\", padding=True)\n+            self.processor(images=[image1, image2], text=prompts, suffix=suffixes, return_tensors=\"pt\", padding=True)\n             .to(torch.bfloat16)\n             .to(torch_device)\n         )"
        },
        {
            "sha": "47810f1832416f62e0eb442baa0c58e6fddebaa0",
            "filename": "tests/models/paligemma/test_processor_paligemma.py",
            "status": "added",
            "additions": 89,
            "deletions": 0,
            "changes": 89,
            "blob_url": "https://github.com/huggingface/transformers/blob/f111d5b783a658b4089e5c82a104fbab397225cd/tests%2Fmodels%2Fpaligemma%2Ftest_processor_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f111d5b783a658b4089e5c82a104fbab397225cd/tests%2Fmodels%2Fpaligemma%2Ftest_processor_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpaligemma%2Ftest_processor_paligemma.py?ref=f111d5b783a658b4089e5c82a104fbab397225cd",
            "patch": "@@ -0,0 +1,89 @@\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import shutil\n+import tempfile\n+import unittest\n+\n+from transformers import GemmaTokenizer\n+from transformers.testing_utils import get_tests_dir, require_torch, require_vision\n+from transformers.utils import is_vision_available\n+\n+from ...test_processing_common import ProcessorTesterMixin\n+\n+\n+if is_vision_available():\n+    from transformers import (\n+        PaliGemmaProcessor,\n+        SiglipImageProcessor,\n+        is_vision_available,\n+    )\n+\n+SAMPLE_VOCAB = get_tests_dir(\"fixtures/test_sentencepiece.model\")\n+\n+\n+@require_vision\n+class PaliGemmaProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n+    processor_class = PaliGemmaProcessor\n+\n+    def setUp(self):\n+        self.tmpdirname = tempfile.mkdtemp()\n+        image_processor = SiglipImageProcessor.from_pretrained(\"google/siglip-so400m-patch14-384\")\n+        image_processor.image_seq_length = 0\n+        tokenizer = GemmaTokenizer(SAMPLE_VOCAB, keep_accents=True)\n+        processor = PaliGemmaProcessor(image_processor=image_processor, tokenizer=tokenizer)\n+        processor.save_pretrained(self.tmpdirname)\n+\n+    def tearDown(self):\n+        shutil.rmtree(self.tmpdirname)\n+\n+    @require_torch\n+    @require_vision\n+    def test_image_seq_length(self):\n+        input_str = \"lower newer\"\n+        image_input = self.prepare_image_inputs()\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\", max_length=112, padding=\"max_length\")\n+        image_processor.image_seq_length = 14\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        inputs = processor(\n+            text=input_str, images=image_input, return_tensors=\"pt\", max_length=112, padding=\"max_length\"\n+        )\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 112 + 14)\n+\n+    @require_torch\n+    @require_vision\n+    def test_unstructured_kwargs_batched(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\")\n+\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = [\"lower newer\", \"upper older longer string\"]\n+        image_input = self.prepare_image_inputs() * 2\n+        inputs = processor(\n+            text=input_str,\n+            images=image_input,\n+            return_tensors=\"pt\",\n+            size={\"height\": 214, \"width\": 214},\n+            padding=\"longest\",\n+            max_length=76,\n+        )\n+\n+        self.assertEqual(inputs[\"pixel_values\"].shape[2], 214)\n+\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 10)"
        }
    ],
    "stats": {
        "total": 227,
        "additions": 153,
        "deletions": 74
    }
}