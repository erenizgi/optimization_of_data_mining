{
    "author": "avihu111",
    "message": "Granite speech - minor fixes to support training with the HF trainer (#38833)\n\n* ensure the query is updated during training\n\navoid unused parameters that DDP does not like\n\n* avoid a crash when `kwargs` contain `padding=True`\n\ntrainers often pass this argument automatically\n\n* minor\n\n* Remove mel_spec lazy init, and rename to mel_filters.\nthis ensures save_pretrained will not crash when saving the processor during training\nhttps://github.com/huggingface/transformers/blob/d5d007a1a0f0c11a726a54c8f00bd71825f84d02/src/transformers/feature_extraction_utils.py#L595\n\n* minor - most feature extractors has a `sampling_rate` property",
    "sha": "be10d4df60bec044ac0c1ab6fd326479874baafc",
    "files": [
        {
            "sha": "5441af1210812fd000318c2813ca2c6d13caad25",
            "filename": "src/transformers/models/granite_speech/feature_extraction_granite_speech.py",
            "status": "modified",
            "additions": 5,
            "deletions": 22,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/be10d4df60bec044ac0c1ab6fd326479874baafc/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Ffeature_extraction_granite_speech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/be10d4df60bec044ac0c1ab6fd326479874baafc/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Ffeature_extraction_granite_speech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Ffeature_extraction_granite_speech.py?ref=be10d4df60bec044ac0c1ab6fd326479874baafc",
            "patch": "@@ -50,15 +50,16 @@ def __init__(\n         **kwargs,\n     ):\n         super().__init__(**kwargs)\n+        self.sampling_rate = sampling_rate\n         self.melspec_kwargs = {\n             \"sample_rate\": sampling_rate,\n             \"n_fft\": n_fft,\n             \"win_length\": win_length,\n             \"hop_length\": hop_length,\n             \"n_mels\": n_mels,\n         }\n-        # Currently lazily initialized\n-        self.melspec = None\n+        requires_backends(self, [\"torchaudio\"])\n+        self.mel_filters = torchaudio.transforms.MelSpectrogram(**self.melspec_kwargs)\n         self.projector_window_size = projector_window_size\n         self.projector_downsample_rate = projector_downsample_rate\n \n@@ -91,34 +92,16 @@ def __call__(\n         ).view(-1, 1)\n         return BatchFeature(data=speech_inputs)\n \n-    def _ensure_melspec_transform_is_initialized(self):\n-        \"\"\"\n-        Ensures the mel spectrogram transform on this instance is initialized.\n-\n-        We do this for now since some logging explodes since the mel spectrogram\n-        transform is not JSON serializable.\n-        \"\"\"\n-        requires_backends(self, [\"torchaudio\"])\n-\n-        if self.melspec is None:\n-            # TODO (@alex-jw-brooks / @eustlb) move this to common batch\n-            # feature extraction in audio utils once they are written!\n-            self.melspec = torchaudio.transforms.MelSpectrogram(**self.melspec_kwargs)\n-\n     def _extract_mel_spectrograms(self, audio: \"torch.Tensor\", device=\"cpu\"):\n         \"\"\"\n         Compute the Mel features to be passed to the conformer encoder.\n         \"\"\"\n         requires_backends(self, [\"torchaudio\"])\n-\n-        # Initialize the mel spectrogram if isn't not already and\n-        # move the melspec / audio to the computation device.\n-        self._ensure_melspec_transform_is_initialized()\n         if device is not None:\n-            melspec = self.melspec.to(device)\n+            melspec = self.mel_filters.to(device)\n             audio = audio.to(device)\n         else:\n-            melspec = self.melspec\n+            melspec = self.mel_filters\n \n         bsz = audio.shape[0]\n         with torch.no_grad():"
        },
        {
            "sha": "d30254ca62af1e6b5d7934431d1e1035127bd85a",
            "filename": "src/transformers/models/granite_speech/modeling_granite_speech.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/be10d4df60bec044ac0c1ab6fd326479874baafc/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fmodeling_granite_speech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/be10d4df60bec044ac0c1ab6fd326479874baafc/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fmodeling_granite_speech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fmodeling_granite_speech.py?ref=be10d4df60bec044ac0c1ab6fd326479874baafc",
            "patch": "@@ -83,7 +83,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         hidden_states = hidden_states.view(batch_size * nblocks, self.window_size, dim)\n \n         query_output = self.qformer(\n-            query_embeds=self.query.data,\n+            query_embeds=self.query,\n             encoder_hidden_states=hidden_states,\n             encoder_attention_mask=None,\n             return_dict=True,"
        },
        {
            "sha": "84515d173c471198b987081198aeeed9415252c9",
            "filename": "src/transformers/models/granite_speech/processing_granite_speech.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/be10d4df60bec044ac0c1ab6fd326479874baafc/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fprocessing_granite_speech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/be10d4df60bec044ac0c1ab6fd326479874baafc/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fprocessing_granite_speech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fprocessing_granite_speech.py?ref=be10d4df60bec044ac0c1ab6fd326479874baafc",
            "patch": "@@ -88,7 +88,9 @@ def __call__(\n         else:\n             audio_inputs = {}\n \n-        text_inputs = self.tokenizer(prompt_strings, padding=True, **kwargs)\n+        if \"padding\" not in kwargs:\n+            kwargs[\"padding\"] = True\n+        text_inputs = self.tokenizer(prompt_strings, **kwargs)\n         return BatchFeature(data={**text_inputs, **audio_inputs})\n \n     def _get_validated_text(self, text: Union[str, list]) -> list[str]:"
        }
    ],
    "stats": {
        "total": 33,
        "additions": 9,
        "deletions": 24
    }
}