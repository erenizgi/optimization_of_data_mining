{
    "author": "Cyrilvallez",
    "message": "[device_map] Accelerate loading by computing device_map much faster (#41548)\n\n* start\n\n* add the important fix\n\n* continue\n\n* big cleanup\n\n* type hints\n\n* add method\n\n* fix typehints\n\n* typehints\n\n* fix\n\n* oupsi\n\n* remove space\n\n* improve function\n\n* CI",
    "sha": "5db730786d87bde8397a207bd84ae0014054fc37",
    "files": [
        {
            "sha": "79ef98d8a4dc51abbd740f2419b4cad89f879721",
            "filename": "src/transformers/integrations/accelerate.py",
            "status": "modified",
            "additions": 217,
            "deletions": 12,
            "changes": 229,
            "blob_url": "https://github.com/huggingface/transformers/blob/5db730786d87bde8397a207bd84ae0014054fc37/src%2Ftransformers%2Fintegrations%2Faccelerate.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5db730786d87bde8397a207bd84ae0014054fc37/src%2Ftransformers%2Fintegrations%2Faccelerate.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Faccelerate.py?ref=5db730786d87bde8397a207bd84ae0014054fc37",
            "patch": "@@ -12,20 +12,23 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \"\"\"\n-Since, https://github.com/huggingface/transformers/pull/36963, loading is always performed with models on meta\n-device. But since the `init_empty_weights` and `find_tied_parameters` functions are from accelerate, and accelerate is\n-somewhat still a soft dependency, we copy the functions here to be used natively in Transformers.\n-\n-The `init_empty_weights` and `init_on_device` functions were copied from `accelerate.big_modeling.py`, and the\n-`find_tied_parameters` was copied from `accelerate.utils.modeling.py`\n+Some of the functions here are derived from the `accelerate` library, with some tweaks for better performances\n+and simplicity/ease of use.\n \"\"\"\n \n-import collections\n+import copy\n import inspect\n import os\n+from collections import defaultdict\n from contextlib import contextmanager\n-\n-from ..utils import is_accelerate_available, is_torch_available, logging\n+from typing import TYPE_CHECKING\n+\n+from ..utils import (\n+    is_accelerate_available,\n+    is_torch_available,\n+    is_torch_xpu_available,\n+    logging,\n+)\n from ..utils.quantization_config import QuantizationMethod\n from .deepspeed import is_deepspeed_zero3_enabled\n from .fsdp import is_fsdp_enabled\n@@ -36,7 +39,12 @@\n     import torch.nn as nn\n \n if is_accelerate_available():\n-    from accelerate import dispatch_model\n+    from accelerate import dispatch_model, infer_auto_device_map\n+    from accelerate.utils import check_tied_parameters_on_same_device, get_max_memory\n+\n+if TYPE_CHECKING:\n+    from ..modeling_utils import PreTrainedModel\n+    from ..quantizers import HfQuantizer\n \n \n logger = logging.get_logger(__name__)\n@@ -205,7 +213,7 @@ def find_tied_parameters(model: \"nn.Module\", **kwargs):\n     return [sorted([weight] + list(set(tied))) for weight, tied in tied_param_groups.items()]\n \n \n-def check_and_set_device_map(device_map):\n+def check_and_set_device_map(device_map: \"torch.device | int | str | dict | None\") -> dict | str | None:\n     from ..modeling_utils import get_torch_context_manager_or_global_device\n \n     # Potentially detect context manager or global device, and use it (only if no device_map was provided)\n@@ -249,6 +257,203 @@ def check_and_set_device_map(device_map):\n     return device_map\n \n \n+def compute_module_sizes(\n+    model: \"PreTrainedModel\", hf_quantizer: \"HfQuantizer | None\"\n+) -> tuple[dict[str, int], dict[str, int]]:\n+    \"\"\"\n+    Compute the size of each submodule of a given model (in bytes).\n+    Returns a tuple of 2 dicts, the fist one containing a mapping of all the modules and the corresponding size\n+    in bytes, and the 2nd one containing a mapping from all leaf modules (modules containing parameters, the end of\n+    the model graph) and the corresponding sizes.\n+    \"\"\"\n+    all_module_sizes = defaultdict(int)\n+    leaves_module_sizes = defaultdict(int)\n+    for name, param in model.state_dict().items():\n+        if hf_quantizer is not None:\n+            dtype_size = hf_quantizer.param_element_size(model, name)\n+        else:\n+            dtype_size = param.element_size()\n+        size = param.numel() * dtype_size\n+        name_parts = name.split(\".\")\n+        for idx in range(len(name_parts)):\n+            all_module_sizes[\".\".join(name_parts[:idx])] += size\n+        if \".\" in name:\n+            leaves_module_sizes[name.rsplit(\".\", 1)[0]] += size\n+\n+    return all_module_sizes, leaves_module_sizes\n+\n+\n+def get_balanced_memory(\n+    model: \"PreTrainedModel\",\n+    max_memory: dict[int | str, int | str] | None = None,\n+    no_split_module_classes: list[str] | None = None,\n+    hf_quantizer: \"HfQuantizer | None\" = None,\n+    low_zero: bool = False,\n+):\n+    \"\"\"\n+    Compute a `max_memory` dictionary for [`infer_auto_device_map`] that will balance the use of each available GPU.\n+\n+    <Tip>\n+\n+    All computation is done analyzing sizes and dtypes of the model parameters. As a result, the model can be on the\n+    meta device (as it would if initialized within the `init_empty_weights` context manager).\n+\n+    </Tip>\n+\n+    Args:\n+        model (`PreTrainedModel`):\n+            The model to analyze.\n+        max_memory (`Dict`, *optional*):\n+            A dictionary device identifier to maximum memory. Will default to the maximum memory available if unset.\n+            Example: `max_memory={0: \"1GB\"}`.\n+        no_split_module_classes (`List[str]`, *optional*):\n+            A list of layer class names that should never be split across device (for instance any layer that has a\n+            residual connection).\n+        hf_quantizer (`HfQuantizer`, *optional*):\n+            A quantizer for the model.\n+        low_zero (`bool`, *optional*):\n+            Minimizes the number of weights on GPU 0, which is convenient when it's used for other operations (like the\n+            Transformers generate function).\n+    \"\"\"\n+    # Get default / clean up max_memory\n+    user_not_set_max_memory = max_memory is None\n+    max_memory = get_max_memory(max_memory)\n+    # Check the number of accelerators available\n+    accelerator_max_memory = copy.deepcopy(max_memory)\n+    _, _ = accelerator_max_memory.pop(\"cpu\", None), accelerator_max_memory.pop(\"disk\", None)\n+    num_devices = len([d for d in accelerator_max_memory if accelerator_max_memory[d] > 0])\n+\n+    if num_devices == 0:\n+        return max_memory\n+\n+    if num_devices == 1:\n+        # We cannot do low_zero on just one GPU, but we will still reserve some memory for the buffer\n+        low_zero = False\n+        # If user just asked us to handle memory usage, we should avoid OOM\n+        if user_not_set_max_memory:\n+            for key in max_memory.keys():\n+                if isinstance(key, int):\n+                    max_memory[key] *= 0.9  # 90% is a good compromise\n+                    logger.info(\n+                        f\"We will use 90% of the memory on device {key} for storing the model, and 10% for the buffer to avoid OOM. \"\n+                        \"You can set `max_memory` in to a higher value to use more memory (at your own risk).\"\n+                    )\n+                    break  # only one device\n+\n+    module_sizes, leave_modules_sizes = compute_module_sizes(model, hf_quantizer)\n+    per_gpu = module_sizes[\"\"] // (num_devices - 1 if low_zero else num_devices)\n+\n+    # We can't just set the memory to model_size // num_devices as it will end being too small: each GPU will get\n+    # slightly less layers and some layers will end up offload at the end. So this function computes a buffer size to\n+    # add which is the biggest of:\n+    # - the size of no split block (if applicable)\n+    # - the mean of the layer sizes\n+    if no_split_module_classes is None:\n+        no_split_module_classes = []\n+    elif not isinstance(no_split_module_classes, (list, tuple)):\n+        no_split_module_classes = [no_split_module_classes]\n+\n+    # Identify the size of the no_split_block modules\n+    buffer = 0\n+    if len(no_split_module_classes) > 0:\n+        no_split_children = {}\n+        for name, size in module_sizes.items():\n+            if name == \"\":\n+                continue\n+            submodule = model.get_submodule(name)\n+            class_name = submodule.__class__.__name__\n+            if class_name in no_split_module_classes and class_name not in no_split_children:\n+                no_split_children[class_name] = size\n+\n+            if set(no_split_children.keys()) == set(no_split_module_classes):\n+                break\n+        buffer = max(no_split_children.values()) if len(no_split_children) > 0 else 0\n+\n+    mean_leaves = int(sum(leave_modules_sizes.values()) / max(len(leave_modules_sizes), 1))\n+    buffer = int(1.25 * max(buffer, mean_leaves))\n+    per_gpu += buffer\n+\n+    # Sorted list of GPUs id (we may have some gpu ids not included in the our max_memory list - let's ignore them)\n+    gpus_idx_list = sorted(\n+        device_id for device_id, device_mem in max_memory.items() if isinstance(device_id, int) and device_mem > 0\n+    )\n+    # The last device is left with max_memory just in case the buffer is not enough.\n+    for idx in gpus_idx_list[:-1]:\n+        max_memory[idx] = min(max_memory[0] if low_zero and idx == 0 else per_gpu, max_memory[idx])\n+\n+    if low_zero:\n+        min_zero = max(0, module_sizes[\"\"] - sum([max_memory[i] for i in range(1, num_devices)]))\n+        max_memory[0] = min(min_zero, max_memory[0])\n+\n+    return max_memory\n+\n+\n+def _get_device_map(\n+    model: \"PreTrainedModel\",\n+    device_map: dict | str | None,\n+    max_memory: dict | None,\n+    hf_quantizer: \"HfQuantizer | None\",\n+    dtype: torch.dtype | None,\n+) -> dict:\n+    \"\"\"Compute the final `device_map` to use if we passed a value in ['auto', 'balanced', 'balanced_low_0', 'sequential'].\n+    Otherwise, we check for any device inconsistencies in the device_map.\n+    \"\"\"\n+    if isinstance(device_map, str):\n+        special_dtypes = {}\n+        if hf_quantizer is not None:\n+            special_dtypes = hf_quantizer.get_special_dtypes_update(model, dtype)\n+\n+        target_dtype = dtype\n+        if hf_quantizer is not None:\n+            target_dtype = hf_quantizer.adjust_target_dtype(target_dtype)\n+\n+        no_split_modules = model._get_no_split_modules(device_map)\n+\n+        if device_map != \"sequential\":\n+            inferred_max_memory = get_balanced_memory(\n+                model,\n+                max_memory=max_memory,\n+                no_split_module_classes=no_split_modules,\n+                hf_quantizer=hf_quantizer,\n+                low_zero=(device_map == \"balanced_low_0\"),\n+            )\n+        else:\n+            inferred_max_memory = get_max_memory(max_memory)\n+        if hf_quantizer is not None:\n+            inferred_max_memory = hf_quantizer.adjust_max_memory(inferred_max_memory)\n+\n+        # `inferred_max_memory` contains non-reserved memory. There may be *unused* reserved memory in the GPU,\n+        # which we can use to allocate parameters.\n+        for device_name in inferred_max_memory:\n+            if isinstance(device_name, int):  # it's a GPU device\n+                if is_torch_xpu_available():\n+                    unused_memory = torch.xpu.memory_reserved(device_name) - torch.xpu.memory_allocated(device_name)\n+                else:\n+                    unused_memory = torch.cuda.memory_reserved(device_name) - torch.cuda.memory_allocated(device_name)\n+                inferred_max_memory[device_name] += unused_memory\n+            # respect the `max_memory` passed by the user\n+            if max_memory is not None and device_name in max_memory:\n+                inferred_max_memory[device_name] = min(inferred_max_memory[device_name], max_memory[device_name])\n+\n+        device_map = infer_auto_device_map(\n+            model,\n+            max_memory=inferred_max_memory,\n+            dtype=target_dtype,\n+            no_split_module_classes=no_split_modules,\n+            special_dtypes=special_dtypes,\n+        )\n+\n+        if hf_quantizer is not None:\n+            hf_quantizer.validate_environment(device_map=device_map)\n+\n+    elif device_map is not None:\n+        tied_params = find_tied_parameters(model)\n+        # check if we don't have tied param in different devices\n+        check_tied_parameters_on_same_device(tied_params, device_map)\n+\n+    return device_map\n+\n+\n def accelerate_dispatch(model, hf_quantizer, device_map, offload_folder, offload_index, offload_buffers):\n     device_map_kwargs = {\n         \"device_map\": device_map,\n@@ -281,7 +486,7 @@ def get_disk_only_shard_files(device_map, weight_map):\n     \"\"\"\n     Returns the list of shard files containing only weights offloaded to disk.\n     \"\"\"\n-    files_content = collections.defaultdict(list)\n+    files_content = defaultdict(list)\n     for weight_name, filename in weight_map.items():\n         while len(weight_name) > 0 and weight_name not in device_map:\n             weight_name = \".\".join(weight_name.split(\".\")[:-1])"
        },
        {
            "sha": "80727de16c7d213a15bc087e5cc1ac77e32d6311",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 37,
            "deletions": 132,
            "changes": 169,
            "blob_url": "https://github.com/huggingface/transformers/blob/5db730786d87bde8397a207bd84ae0014054fc37/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5db730786d87bde8397a207bd84ae0014054fc37/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=5db730786d87bde8397a207bd84ae0014054fc37",
            "patch": "@@ -50,6 +50,7 @@\n from .generation import CompileConfig, GenerationConfig\n from .integrations import PeftAdapterMixin, deepspeed_config, is_deepspeed_zero3_enabled, is_fsdp_enabled\n from .integrations.accelerate import (\n+    _get_device_map,\n     accelerate_disk_offload,\n     accelerate_dispatch,\n     check_and_set_device_map,\n@@ -109,7 +110,6 @@\n     is_torch_mlu_available,\n     is_torch_npu_available,\n     is_torch_xla_available,\n-    is_torch_xpu_available,\n     logging,\n )\n from .utils.generic import _CAN_RECORD_REGISTRY, GeneralInterface, OutputRecorder\n@@ -125,13 +125,9 @@\n \n \n if is_accelerate_available():\n-    from accelerate import infer_auto_device_map\n     from accelerate.hooks import add_hook_to_module\n     from accelerate.utils import (\n-        check_tied_parameters_on_same_device,\n         extract_model_from_parallel,\n-        get_balanced_memory,\n-        get_max_memory,\n         offload_weight,\n         save_offload_index,\n     )\n@@ -277,27 +273,6 @@ def _wrapper(*args, **kwargs):\n     return _wrapper\n \n \n-def get_keep_in_fp32_regex(model, hf_quantizer, dtype):\n-    # Find fp32 modules if needed\n-    keep_in_fp32_modules = []\n-    # The _keep_in_fp32_modules flag is only used to avoid bf16 -> fp16 casting precision issues. It was introduced\n-    # in case of force loading a model that should stay bf16 in fp16 (which includes a few quantizers as this is a pre-processing\n-    # step for e.g. bitsandbytes). See https://github.com/huggingface/transformers/issues/20287 for details.\n-    if model._keep_in_fp32_modules is not None and (\n-        dtype == torch.float16 or getattr(hf_quantizer, \"use_keep_in_fp32_modules\", False)\n-    ):\n-        keep_in_fp32_modules.extend(model._keep_in_fp32_modules)\n-\n-    if model._keep_in_fp32_modules_strict is not None and (dtype == torch.float16 or dtype == torch.bfloat16):\n-        keep_in_fp32_modules.extend(model._keep_in_fp32_modules_strict)\n-\n-    keep_in_fp32_regex = None\n-    if keep_in_fp32_modules:\n-        # We need to match exact layers, so we add either `.` on each side, or start/end of string\n-        keep_in_fp32_regex = re.compile(\"|\".join([rf\"((^|\\.){module}($|\\.))\" for module in keep_in_fp32_modules]))\n-    return keep_in_fp32_regex\n-\n-\n def get_torch_context_manager_or_global_device():\n     \"\"\"\n     Test if a device context manager is currently in use, or if it is not the case, check if the default device\n@@ -559,7 +534,6 @@ def _infer_parameter_dtype(\n     model: \"PreTrainedModel\",\n     param_name: str,\n     empty_param: torch.Tensor,\n-    keep_in_fp32_regex: Optional[re.Pattern] = None,\n     hf_quantizer: Optional[HfQuantizer] = None,\n ) -> Union[bool, Optional[torch.dtype]]:\n     try:\n@@ -580,11 +554,8 @@ def _infer_parameter_dtype(\n     casting_dtype = None\n     is_param_float8_e4m3fn = is_torch_e4m3fn_available and empty_param.dtype == torch.float8_e4m3fn\n     if empty_param.dtype.is_floating_point and not is_param_float8_e4m3fn:\n-        # First fp32 if part of the exception list\n-        if keep_in_fp32_regex is not None and keep_in_fp32_regex.search(param_name):\n-            casting_dtype = torch.float32\n-        # Then dtype that was instantiated in the meta model -- note that this respects subconfigs dtypes\n-        elif hf_quantizer is not None:\n+        # dtype that was instantiated in the meta model -- note that this respects subconfigs dtypes\n+        if hf_quantizer is not None:\n             casting_dtype = model.config._pre_quantization_dtype\n         else:\n             casting_dtype = old_param.dtype\n@@ -608,7 +579,6 @@ def _load_state_dict_into_meta_model(\n     disk_offload_folder: Optional[str] = None,\n     disk_offload_index: Optional[dict] = None,\n     hf_quantizer: Optional[HfQuantizer] = None,\n-    keep_in_fp32_regex: Optional[re.Pattern] = None,\n     device_mesh: Optional[\"torch.distributed.device_mesh.DeviceMesh\"] = None,\n ) -> tuple[Optional[dict], Optional[dict]]:\n     \"\"\"Load parameters from `meta_state_dict` into the model. The parameters of the `meta_state_dict` are on the meta\n@@ -638,13 +608,7 @@ def _load_state_dict_into_meta_model(\n             param = file_pointer.get_slice(serialized_param_name)\n         else:\n             param = empty_param.to(tensor_device)  # It is actually not empty!\n-        to_contiguous, casting_dtype = _infer_parameter_dtype(\n-            model,\n-            param_name,\n-            empty_param,\n-            keep_in_fp32_regex,\n-            hf_quantizer,\n-        )\n+        to_contiguous, casting_dtype = _infer_parameter_dtype(model, param_name, empty_param, hf_quantizer)\n \n         if device_mesh is not None:\n             if not is_quantized or not hf_quantizer.param_needs_quantization(model, param_name):\n@@ -745,7 +709,6 @@ def load_shard_file(args):\n         reverse_key_renaming_mapping,\n         disk_offload_folder,\n         disk_offload_index,\n-        keep_in_fp32_regex,\n         device_mesh,\n     ) = args\n \n@@ -780,7 +743,6 @@ def load_shard_file(args):\n             disk_offload_folder=disk_offload_folder,\n             disk_offload_index=disk_offload_index,\n             hf_quantizer=hf_quantizer,\n-            keep_in_fp32_regex=keep_in_fp32_regex,\n             device_mesh=device_mesh,\n         )\n \n@@ -1211,82 +1173,6 @@ def _get_dtype(\n     return config, dtype, dtype_orig\n \n \n-def _get_device_map(\n-    model: \"PreTrainedModel\",\n-    device_map: Optional[Union[dict, str]],\n-    max_memory: Optional[dict],\n-    hf_quantizer: Optional[HfQuantizer],\n-    dtype: Optional[torch.dtype],\n-    keep_in_fp32_regex: Optional[re.Pattern],\n-) -> dict:\n-    \"\"\"Compute the final `device_map` to use if we passed a value in ['auto', 'balanced', 'balanced_low_0', 'sequential'].\n-    Otherwise, we check for any device inconsistencies in the device_map.\n-    \"\"\"\n-    if isinstance(device_map, str):\n-        special_dtypes = {}\n-        if hf_quantizer is not None:\n-            special_dtypes.update(hf_quantizer.get_special_dtypes_update(model, dtype))\n-        if keep_in_fp32_regex is not None:\n-            special_dtypes.update(\n-                {name: torch.float32 for name, _ in model.named_parameters() if keep_in_fp32_regex.search(name)}\n-            )\n-\n-        target_dtype = dtype\n-\n-        if hf_quantizer is not None:\n-            target_dtype = hf_quantizer.adjust_target_dtype(target_dtype)\n-\n-        no_split_modules = model._get_no_split_modules(device_map)\n-        device_map_kwargs = {\"no_split_module_classes\": no_split_modules}\n-\n-        if \"special_dtypes\" in inspect.signature(infer_auto_device_map).parameters:\n-            device_map_kwargs[\"special_dtypes\"] = special_dtypes\n-        elif len(special_dtypes) > 0:\n-            logger.warning(\n-                \"This model has some weights that should be kept in higher precision, you need to upgrade \"\n-                \"`accelerate` to properly deal with them (`pip install --upgrade accelerate`).\"\n-            )\n-\n-        if device_map != \"sequential\":\n-            inferred_max_memory = get_balanced_memory(\n-                model,\n-                dtype=target_dtype,\n-                low_zero=(device_map == \"balanced_low_0\"),\n-                max_memory=max_memory,\n-                **device_map_kwargs,\n-            )\n-        else:\n-            inferred_max_memory = get_max_memory(max_memory)\n-        if hf_quantizer is not None:\n-            inferred_max_memory = hf_quantizer.adjust_max_memory(inferred_max_memory)\n-\n-        # `inferred_max_memory` contains non-reserved memory. There may be *unused* reserved memory in the GPU,\n-        # which we can use to allocate parameters.\n-        for device_name in inferred_max_memory:\n-            if isinstance(device_name, int):  # it's a GPU device\n-                if is_torch_xpu_available():\n-                    unused_memory = torch.xpu.memory_reserved(device_name) - torch.xpu.memory_allocated(device_name)\n-                else:\n-                    unused_memory = torch.cuda.memory_reserved(device_name) - torch.cuda.memory_allocated(device_name)\n-                inferred_max_memory[device_name] += unused_memory\n-            # respect the `max_memory` passed by the user\n-            if max_memory is not None and device_name in max_memory:\n-                inferred_max_memory[device_name] = min(inferred_max_memory[device_name], max_memory[device_name])\n-        device_map_kwargs[\"max_memory\"] = inferred_max_memory\n-\n-        device_map = infer_auto_device_map(model, dtype=target_dtype, **device_map_kwargs)\n-\n-        if hf_quantizer is not None:\n-            hf_quantizer.validate_environment(device_map=device_map)\n-\n-    elif device_map is not None:\n-        tied_params = find_tied_parameters(model)\n-        # check if we don't have tied param in different devices\n-        check_tied_parameters_on_same_device(tied_params, device_map)\n-\n-    return device_map\n-\n-\n def _find_missing_and_unexpected_keys(\n     model: \"PreTrainedModel\",\n     original_checkpoint_keys: list[str],\n@@ -4585,14 +4471,14 @@ def from_pretrained(\n             # Let's make sure we don't run the init function of buffer modules\n             model = cls(config, *model_args, **model_kwargs)\n \n+        # Potentially upcast some modules to avoid loosing precision\n+        model.upcast_modules_in_fp32(hf_quantizer, dtype)\n         # Make sure to tie the weights correctly\n         model.tie_weights()\n \n         # make sure we use the model's config since the __init__ call might have copied it\n         config = model.config\n \n-        # Regex to keep a fixed dtype\n-        keep_in_fp32_regex = get_keep_in_fp32_regex(model, hf_quantizer, dtype)\n         if hf_quantizer is not None:  # replace module with quantized modules (does not touch weights)\n             hf_quantizer.preprocess_model(\n                 model=model,\n@@ -4608,7 +4494,7 @@ def from_pretrained(\n \n         # Prepare the full device map\n         if device_map is not None:\n-            device_map = _get_device_map(model, device_map, max_memory, hf_quantizer, dtype, keep_in_fp32_regex)\n+            device_map = _get_device_map(model, device_map, max_memory, hf_quantizer, dtype)\n \n         # restore default dtype\n         if dtype_orig is not None:\n@@ -4626,7 +4512,6 @@ def from_pretrained(\n             disk_offload_folder=offload_folder,\n             dtype=dtype,\n             hf_quantizer=hf_quantizer,\n-            keep_in_fp32_regex=keep_in_fp32_regex,\n             device_mesh=device_mesh,\n             key_mapping=key_mapping,\n             weights_only=weights_only,\n@@ -4806,7 +4691,6 @@ def _load_pretrained_model(\n         disk_offload_folder: Optional[str] = None,\n         dtype: Optional[torch.dtype] = None,\n         hf_quantizer: Optional[HfQuantizer] = None,\n-        keep_in_fp32_regex: Optional[re.Pattern] = None,\n         device_mesh: Optional[\"torch.distributed.device_mesh.DeviceMesh\"] = None,\n         key_mapping: Optional[dict[str, str]] = None,\n         weights_only: bool = True,\n@@ -4873,13 +4757,6 @@ def _load_pretrained_model(\n         # correctly initialize the missing (and potentially mismatched) keys\n         model._initialize_missing_keys(missing_keys + mismatched_keys, is_quantized)\n \n-        # Set some modules to fp32 if needed\n-        if keep_in_fp32_regex is not None:\n-            for name, param in model.named_parameters():\n-                if keep_in_fp32_regex.search(name):\n-                    # param = param.to(torch.float32) does not work here as only in the local scope.\n-                    param.data = param.data.to(torch.float32)\n-\n         # Get reverse key mapping\n         reverse_key_renaming_mapping = {v: k for k, v in key_renaming_mapping.items()}\n \n@@ -4931,7 +4808,6 @@ def _load_pretrained_model(\n                 reverse_key_renaming_mapping,\n                 disk_offload_folder,\n                 disk_offload_index,\n-                keep_in_fp32_regex,\n                 device_mesh,\n             )\n             for shard_file in checkpoint_files\n@@ -4979,7 +4855,7 @@ def _load_pretrained_model(\n                     if param.device.type == \"meta\":\n                         continue\n                     # Shard the param\n-                    to_contiguous, casting_dtype = _infer_parameter_dtype(model, name, param, keep_in_fp32_regex)\n+                    to_contiguous, casting_dtype = _infer_parameter_dtype(model, name, param)\n                     shard_and_distribute_module(\n                         model,\n                         param.to(tp_device),\n@@ -5360,6 +5236,35 @@ def train(self, mode: bool = True):\n     def eval(self):\n         return self.train(False)\n \n+    def upcast_modules_in_fp32(self, hf_quantizer: HfQuantizer | None, dtype: torch.dtype) -> None:\n+        \"\"\"\n+        Upcast modules defined in `_keep_in_fp32_modules` and `_keep_in_fp32_modules_strict` in fp32, if\n+        `dtype` is different than fp32.\n+        \"\"\"\n+        # If the dtype is already fp32, we can skip\n+        if dtype == torch.float32:\n+            return\n+\n+        keep_in_fp32_modules = []\n+        # The _keep_in_fp32_modules flag is only used to avoid bf16 -> fp16 casting precision issues. It was introduced\n+        # in case of force loading a model that should stay bf16 in fp16 (which includes a few quantizers as this is a pre-processing\n+        # step for e.g. bitsandbytes). See https://github.com/huggingface/transformers/issues/20287 for details.\n+        if self._keep_in_fp32_modules is not None and (\n+            dtype == torch.float16 or getattr(hf_quantizer, \"use_keep_in_fp32_modules\", False)\n+        ):\n+            keep_in_fp32_modules.extend(self._keep_in_fp32_modules)\n+\n+        if self._keep_in_fp32_modules_strict is not None and (dtype == torch.float16 or dtype == torch.bfloat16):\n+            keep_in_fp32_modules.extend(self._keep_in_fp32_modules_strict)\n+\n+        if len(keep_in_fp32_modules) > 0:\n+            # We need to match exact layers, so we add either `.` on each side, or start/end of string\n+            keep_in_fp32_regex = re.compile(\"|\".join([rf\"((^|\\.){module}($|\\.))\" for module in keep_in_fp32_modules]))\n+            for name, param in self.named_parameters():\n+                if keep_in_fp32_regex.search(name):\n+                    # param = param.to(torch.float32) does not work here as only in the local scope.\n+                    param.data = param.data.to(torch.float32)\n+\n \n PreTrainedModel.push_to_hub = copy_func(PreTrainedModel.push_to_hub)\n if PreTrainedModel.push_to_hub.__doc__ is not None:"
        },
        {
            "sha": "1a8c8b593afa028ff5900e99ed0a641f2aca5812",
            "filename": "src/transformers/quantizers/base.py",
            "status": "modified",
            "additions": 16,
            "deletions": 0,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/5db730786d87bde8397a207bd84ae0014054fc37/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5db730786d87bde8397a207bd84ae0014054fc37/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fbase.py?ref=5db730786d87bde8397a207bd84ae0014054fc37",
            "patch": "@@ -128,6 +128,22 @@ def adjust_target_dtype(self, dtype: \"torch.dtype\") -> \"torch.dtype\":\n         \"\"\"\n         return dtype\n \n+    def param_element_size(self, model: \"PreTrainedModel\", param_name: str) -> float:\n+        \"Return the element size (in bytes) for `param_name`.\"\n+        if self.param_needs_quantization(model, param_name):\n+            from accelerate.utils import CustomDtype\n+\n+            mapping = {\n+                torch.int8: 1,\n+                CustomDtype.INT4: 0.5,\n+                CustomDtype.FP8: 1,\n+                CustomDtype.INT2: 0.25,\n+            }\n+            # The value passed is actually not used when the method is overriden\n+            if (custom_dtype := self.adjust_target_dtype(torch.float16)) in mapping:\n+                return mapping[custom_dtype]\n+        return model.get_parameter_or_buffer(param_name).element_size()\n+\n     def update_missing_keys(self, model, missing_keys: list[str], prefix: str) -> list[str]:\n         \"\"\"\n         Override this method if you want to adjust the `missing_keys`."
        }
    ],
    "stats": {
        "total": 414,
        "additions": 270,
        "deletions": 144
    }
}