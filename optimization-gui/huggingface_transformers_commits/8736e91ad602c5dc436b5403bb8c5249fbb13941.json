{
    "author": "ArthurZucker",
    "message": "[ `tests`] remove some flash attention class tests (#35817)\n\nremove class from tests",
    "sha": "8736e91ad602c5dc436b5403bb8c5249fbb13941",
    "files": [
        {
            "sha": "bde6e07effc9aa97d4e370b3f7514f0420119c2d",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 0,
            "deletions": 31,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/8736e91ad602c5dc436b5403bb8c5249fbb13941/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8736e91ad602c5dc436b5403bb8c5249fbb13941/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=8736e91ad602c5dc436b5403bb8c5249fbb13941",
            "patch": "@@ -4628,43 +4628,12 @@ def test_flash_attn_2_from_config(self):\n             dummy_input = torch.LongTensor([[0, 2, 3, 4], [0, 2, 3, 4]]).to(torch_device)\n             dummy_attention_mask = torch.LongTensor([[1, 1, 1, 1], [0, 1, 1, 1]]).to(torch_device)\n \n-            fa2_correctly_converted = False\n-\n-            for _, module in fa2_model.named_modules():\n-                if \"FlashAttention\" in module.__class__.__name__:\n-                    fa2_correctly_converted = True\n-                    break\n-\n-            fa2_correctly_converted = (\n-                fa2_correctly_converted\n-                if not model_class._supports_flex_attn\n-                else fa2_model.config._attn_implementation == \"flash_attention_2\"\n-            )\n-            self.assertTrue(fa2_correctly_converted)\n-\n             _ = fa2_model(input_ids=dummy_input, attention_mask=dummy_attention_mask)\n-\n             with tempfile.TemporaryDirectory() as tmpdirname:\n                 fa2_model.save_pretrained(tmpdirname)\n-\n                 model_from_pretrained = model_class.from_pretrained(tmpdirname)\n-\n                 self.assertTrue(model_from_pretrained.config._attn_implementation != \"flash_attention_2\")\n \n-                fa2_correctly_converted = False\n-\n-                for _, module in model_from_pretrained.named_modules():\n-                    if \"FlashAttention\" in module.__class__.__name__:\n-                        fa2_correctly_converted = True\n-                        break\n-\n-                fa2_correctly_converted = (\n-                    fa2_correctly_converted\n-                    if not model_class._supports_flex_attn\n-                    else model_from_pretrained.config._attn_implementation == \"flash_attention_2\"\n-                )\n-                self.assertFalse(fa2_correctly_converted)\n-\n     def _get_custom_4d_mask_test_data(self):\n         # Sequence in which all but the last token is the same\n         input_ids = torch.tensor("
        }
    ],
    "stats": {
        "total": 31,
        "additions": 0,
        "deletions": 31
    }
}