{
    "author": "Reshan123",
    "message": "mobilebert model card update (#37256)\n\n* mobilebert model card update\n\n* Updates to model card mobilebert\n\n---------\n\nCo-authored-by: Reshan Gomis <reshang@verdentra.com>",
    "sha": "8cd57eb73107de0353d1fe94cfe99484b738172a",
    "files": [
        {
            "sha": "2104d0a4573f1df3cf7f4d90f45076510db2eb5d",
            "filename": "docs/source/en/model_doc/mobilebert.md",
            "status": "modified",
            "additions": 63,
            "deletions": 34,
            "changes": 97,
            "blob_url": "https://github.com/huggingface/transformers/blob/8cd57eb73107de0353d1fe94cfe99484b738172a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilebert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/8cd57eb73107de0353d1fe94cfe99484b738172a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilebert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilebert.md?ref=8cd57eb73107de0353d1fe94cfe99484b738172a",
            "patch": "@@ -14,52 +14,81 @@ rendered properly in your Markdown viewer.\n \n -->\n \n-# MobileBERT\n \n-<div class=\"flex flex-wrap space-x-1\">\n-<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+        <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+        <img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n+    </div>\n </div>\n \n-## Overview\n+# MobileBERT\n+\n+[MobileBERT](https://huggingface.co/papers/2004.02984) is a lightweight and efficient variant of BERT, specifically designed for resource-limited devices such as mobile phones. It retains BERT's architecture but significantly reduces model size and inference latency while maintaining strong performance on NLP tasks. MobileBERT achieves this through a bottleneck structure and carefully balanced self-attention and feedforward networks. The model is trained by knowledge transfer from a large BERT model with an inverted bottleneck structure.\n+\n+You can find the original MobileBERT checkpoint under the [Google](https://huggingface.co/google/mobilebert-uncased) organization.\n+> [!TIP]\n+> Click on the MobileBERT models in the right sidebar for more examples of how to apply MobileBERT to different language tasks.\n+\n+The example below demonstrates how to predict the `[MASK]` token with [`Pipeline`], [`AutoModel`], and from the command line.\n+\n+<hfoptions id=\"usage\">\n+<hfoption id=\"Pipeline\">\n+\n+```py\n+import torch\n+from transformers import pipeline\n+\n+pipeline = pipeline(\n+    task=\"fill-mask\",\n+    model=\"google/mobilebert-uncased\",\n+    torch_dtype=torch.float16,\n+    device=0\n+)\n+pipeline(\"The capital of France is [MASK].\")\n+```\n+</hfoption>\n+<hfoption id=\"AutoModel\">\n+\n+```py\n+import torch\n+from transformers import AutoModelForMaskedLM, AutoTokenizer\n+\n+tokenizer = AutoTokenizer.from_pretrained(\n+    \"google/mobilebert-uncased\",\n+)\n+model = AutoModelForMaskedLM.from_pretrained(\n+    \"google/mobilebert-uncased\",\n+    torch_dtype=torch.float16,\n+    device_map=\"auto\",\n+)\n+inputs = tokenizer(\"The capital of France is [MASK].\", return_tensors=\"pt\").to(\"cuda\")\n \n-The MobileBERT model was proposed in [MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices](https://arxiv.org/abs/2004.02984) by Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny\n-Zhou. It's a bidirectional transformer based on the BERT model, which is compressed and accelerated using several\n-approaches.\n+with torch.no_grad():\n+    outputs = model(**inputs)\n+    predictions = outputs.logits\n \n-The abstract from the paper is the following:\n+masked_index = torch.where(inputs['input_ids'] == tokenizer.mask_token_id)[1]\n+predicted_token_id = predictions[0, masked_index].argmax(dim=-1)\n+predicted_token = tokenizer.decode(predicted_token_id)\n \n-*Natural Language Processing (NLP) has recently achieved great success by using huge pre-trained models with hundreds\n-of millions of parameters. However, these models suffer from heavy model sizes and high latency such that they cannot\n-be deployed to resource-limited mobile devices. In this paper, we propose MobileBERT for compressing and accelerating\n-the popular BERT model. Like the original BERT, MobileBERT is task-agnostic, that is, it can be generically applied to\n-various downstream NLP tasks via simple fine-tuning. Basically, MobileBERT is a thin version of BERT_LARGE, while\n-equipped with bottleneck structures and a carefully designed balance between self-attentions and feed-forward networks.\n-To train MobileBERT, we first train a specially designed teacher model, an inverted-bottleneck incorporated BERT_LARGE\n-model. Then, we conduct knowledge transfer from this teacher to MobileBERT. Empirical studies show that MobileBERT is\n-4.3x smaller and 5.5x faster than BERT_BASE while achieving competitive results on well-known benchmarks. On the\n-natural language inference tasks of GLUE, MobileBERT achieves a GLUEscore o 77.7 (0.6 lower than BERT_BASE), and 62 ms\n-latency on a Pixel 4 phone. On the SQuAD v1.1/v2.0 question answering task, MobileBERT achieves a dev F1 score of\n-90.0/79.2 (1.5/2.1 higher than BERT_BASE).*\n+print(f\"The predicted token is: {predicted_token}\")\n+```\n \n-This model was contributed by [vshampor](https://huggingface.co/vshampor). The original code can be found [here](https://github.com/google-research/google-research/tree/master/mobilebert).\n+</hfoption>\n+<hfoption id=\"transformers-cli\">\n \n-## Usage tips\n+```bash\n+echo -e \"The capital of France is [MASK].\" | transformers-cli run --task fill-mask --model google/mobilebert-uncased --device 0\n+```\n \n-- MobileBERT is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather\n-  than the left.\n-- MobileBERT is similar to BERT and therefore relies on the masked language modeling (MLM) objective. It is therefore\n-  efficient at predicting masked tokens and at NLU in general, but is not optimal for text generation. Models trained\n-  with a causal language modeling (CLM) objective are better in that regard.\n+</hfoption>\n+</hfoptions>\n \n \n-## Resources\n+## Notes\n \n-- [Text classification task guide](../tasks/sequence_classification)\n-- [Token classification task guide](../tasks/token_classification)\n-- [Question answering task guide](../tasks/question_answering)\n-- [Masked language modeling task guide](../tasks/masked_language_modeling)\n-- [Multiple choice task guide](../tasks/multiple_choice)\n+- Inputs should be padded on the right because BERT uses absolute position embeddings.\n \n ## MobileBertConfig\n "
        }
    ],
    "stats": {
        "total": 97,
        "additions": 63,
        "deletions": 34
    }
}