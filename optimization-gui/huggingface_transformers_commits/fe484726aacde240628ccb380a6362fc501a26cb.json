{
    "author": "g-prz",
    "message": "Add falcon gguf (#33437)\n\n* feat(gguf): add falcon q2 k\r\n\r\n* fix(gguf): remove useless renaming\r\n\r\n* feat(gguf): seperate falcon 7b and 40b\r\n\r\n* feat(gguf): apply fixup\r\n\r\n* fix(test): error rebase\r\n\r\n* feat(gguf): add fp16 weight comparison for falcon\r\n\r\n* feat(gguf): test weight of all layers\r\n\r\n* test(gguf): add falcon 40b under skip decorator\r\n\r\n* feat(gguf): quick example for extracting model size",
    "sha": "fe484726aacde240628ccb380a6362fc501a26cb",
    "files": [
        {
            "sha": "890ca04248815431bed7f71598dca465d762eed5",
            "filename": "docs/source/en/gguf.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/fe484726aacde240628ccb380a6362fc501a26cb/docs%2Fsource%2Fen%2Fgguf.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/fe484726aacde240628ccb380a6362fc501a26cb/docs%2Fsource%2Fen%2Fgguf.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fgguf.md?ref=fe484726aacde240628ccb380a6362fc501a26cb",
            "patch": "@@ -81,6 +81,7 @@ For now the supported model architectures are the architectures that have been v\n - Qwen2Moe\n - Phi3\n - Bloom\n+- Falcon\n \n ## Example usage\n "
        },
        {
            "sha": "92371415918150ca7f176f0ee86a6992c9e76b42",
            "filename": "src/transformers/convert_slow_tokenizer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/fe484726aacde240628ccb380a6362fc501a26cb/src%2Ftransformers%2Fconvert_slow_tokenizer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fe484726aacde240628ccb380a6362fc501a26cb/src%2Ftransformers%2Fconvert_slow_tokenizer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconvert_slow_tokenizer.py?ref=fe484726aacde240628ccb380a6362fc501a26cb",
            "patch": "@@ -345,7 +345,6 @@ def converted(self, vocab: Dict[str, int] = None, merges: List[Tuple[str, str]]\n             )\n         )\n \n-        add_prefix_space = False\n         add_prefix_space = getattr(self.original_tokenizer, \"add_prefix_space\", False)\n         tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=add_prefix_space)\n         tokenizer.decoder = decoders.ByteLevel()"
        },
        {
            "sha": "ca39b5ef5f917a3b4481e55f3db0123f97cefa57",
            "filename": "src/transformers/integrations/ggml.py",
            "status": "modified",
            "additions": 36,
            "deletions": 0,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/fe484726aacde240628ccb380a6362fc501a26cb/src%2Ftransformers%2Fintegrations%2Fggml.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fe484726aacde240628ccb380a6362fc501a26cb/src%2Ftransformers%2Fintegrations%2Fggml.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fggml.py?ref=fe484726aacde240628ccb380a6362fc501a26cb",
            "patch": "@@ -120,6 +120,29 @@\n         \"output.weight\": \"lm_head.weight\",\n         \"output_norm\": \"transformer.ln_f\",\n     },\n+    \"falcon7b\": {\n+        \"token_embd\": \"word_embeddings\",\n+        \"blk\": \"h\",\n+        \"ffn_up\": \"mlp.dense_h_to_4h\",\n+        \"ffn_down\": \"mlp.dense_4h_to_h\",\n+        \"attn_norm\": \"input_layernorm\",\n+        \"attn_qkv\": \"self_attention.query_key_value\",\n+        \"attn_output\": \"self_attention.dense\",\n+        \".output.\": \".lm_head.\",\n+        \"output_norm\": \"ln_f\",\n+    },\n+    \"falcon40b\": {\n+        \"token_embd\": \"word_embeddings\",\n+        \"blk\": \"h\",\n+        \"ffn_up\": \"mlp.dense_h_to_4h\",\n+        \"ffn_down\": \"mlp.dense_4h_to_h\",\n+        \".attn_norm.\": \".ln_mlp.\",\n+        \"attn_norm_2\": \"ln_attn\",\n+        \"attn_qkv\": \"self_attention.query_key_value\",\n+        \"attn_output\": \"self_attention.dense\",\n+        \".output.\": \".lm_head.\",\n+        \"output_norm\": \"ln_f\",\n+    },\n }\n \n \n@@ -178,6 +201,18 @@\n         \"attention.layer_norm_rms_epsilon\": \"rms_norm_eps\",\n         \"vocab_size\": \"vocab_size\",\n     },\n+    \"falcon\": {\n+        \"context_length\": \"max_position_embeddings\",\n+        \"block_count\": \"num_hidden_layers\",\n+        \"feed_forward_length\": \"intermediate_size\",\n+        \"embedding_length\": \"hidden_size\",\n+        \"rope.dimension_count\": None,\n+        \"rope.freq_base\": \"rope_theta\",\n+        \"attention.head_count\": \"num_attention_heads\",\n+        \"attention.head_count_kv\": \"num_key_value_heads\",\n+        \"attention.layer_norm_rms_epsilon\": \"rms_norm_eps\",\n+        \"vocab_size\": \"vocab_size\",\n+    },\n     \"tokenizer\": {\n         \"ggml.bos_token_id\": \"bos_token_id\",\n         \"ggml.eos_token_id\": \"eos_token_id\",\n@@ -530,6 +565,7 @@ def converted(self) -> Tokenizer:\n     \"qwen2_moe\": GGUFQwen2Converter,\n     \"phi3\": GGUFPhi3Converter,\n     \"bloom\": GGUFBloomConverter,\n+    \"falcon\": GGUFBloomConverter,\n }\n \n "
        },
        {
            "sha": "3bca05b1251f3f31ed7f24f73002f849c40c691c",
            "filename": "src/transformers/modeling_gguf_pytorch_utils.py",
            "status": "modified",
            "additions": 16,
            "deletions": 11,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/fe484726aacde240628ccb380a6362fc501a26cb/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fe484726aacde240628ccb380a6362fc501a26cb/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py?ref=fe484726aacde240628ccb380a6362fc501a26cb",
            "patch": "@@ -14,6 +14,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n+import re\n from typing import Optional\n \n import numpy as np\n@@ -99,8 +100,20 @@ def load_gguf_checkpoint(gguf_checkpoint_path, return_tensors=False):\n     if \"qwen2moe\" in architecture:\n         updated_architecture = \"qwen2_moe\"\n \n-    if architecture not in GGUF_SUPPORTED_ARCHITECTURES:\n-        raise ValueError(f\"Architecture {architecture} not supported\")\n+    model_size = \"\"\n+    # extract the number of params from file name as architectures can differ ;\n+    # eg. for falcon : `...falcon-7b-...`\n+    if \"falcon\" in architecture:\n+        gguf_file_name = gguf_checkpoint_path.split(\"/\")[-1].lower()\n+        m = re.search(r\"-\\d+b-\", gguf_file_name)  # regex to catch `-7b-`\n+        if m is None:\n+            raise ValueError(\n+                f\"From file name, cannot determine the number of parameters for {architecture} architecture\"\n+            )\n+        model_size = m.group().strip(\"-\")  # only keeps `7b`\n+\n+    if architecture + model_size not in GGUF_SUPPORTED_ARCHITECTURES:\n+        raise ValueError(f\"Architecture {architecture + model_size} not supported\")\n \n     # List all key-value pairs in a columnized format\n     for gguf_key, field in reader.fields.items():\n@@ -146,17 +159,9 @@ def load_gguf_checkpoint(gguf_checkpoint_path, return_tensors=False):\n             )\n \n     if return_tensors:\n-        tensor_key_mapping = GGUF_TO_TRANSFORMERS_MAPPING[\"tensors\"][architecture]\n+        tensor_key_mapping = GGUF_TO_TRANSFORMERS_MAPPING[\"tensors\"][architecture + model_size]\n \n         for tensor in tqdm(reader.tensors, desc=\"Converting and de-quantizing GGUF tensors...\"):\n-            renamed_tensor_name = tensor.name\n-\n-            for tensor_name_mapping in GGUF_TO_TRANSFORMERS_MAPPING[\"tensors\"]:\n-                if tensor_name_mapping in renamed_tensor_name:\n-                    renamed_tensor_name = renamed_tensor_name.replace(\n-                        tensor_name_mapping, GGUF_TO_TRANSFORMERS_MAPPING[\"tensors\"][tensor_name_mapping]\n-                    )\n-\n             name = tensor.name\n \n             weights = dequantize(tensor.data, tensor.tensor_type)"
        },
        {
            "sha": "ddc6288f36dd316a2c878f4417b92822db96191b",
            "filename": "tests/quantization/ggml/test_ggml.py",
            "status": "modified",
            "additions": 58,
            "deletions": 0,
            "changes": 58,
            "blob_url": "https://github.com/huggingface/transformers/blob/fe484726aacde240628ccb380a6362fc501a26cb/tests%2Fquantization%2Fggml%2Ftest_ggml.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fe484726aacde240628ccb380a6362fc501a26cb/tests%2Fquantization%2Fggml%2Ftest_ggml.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fggml%2Ftest_ggml.py?ref=fe484726aacde240628ccb380a6362fc501a26cb",
            "patch": "@@ -44,6 +44,9 @@ class GgufIntegrationTests(unittest.TestCase):\n     phi3_model_id = \"microsoft/Phi-3-mini-4k-instruct-gguf\"\n     bloom_model_id = \"afrideva/bloom-560m-GGUF\"\n     original_bloom_model_id = \"bigscience/bloom-560m\"\n+    falcon7b_model_id = \"xaviviro/falcon-7b-quantized-gguf\"\n+    falcon40b_model_id = \"maddes8cht/tiiuae-falcon-40b-gguf\"\n+    original_flacon7b_model_id = \"tiiuae/falcon-7b\"\n \n     # standard quants\n     q4_0_gguf_model_id = \"tinyllama-1.1b-chat-v1.0.Q4_0.gguf\"\n@@ -74,6 +77,9 @@ class GgufIntegrationTests(unittest.TestCase):\n     fp16_bloom_model_id = \"bloom-560m.fp16.gguf\"\n     q8_bloom_model_id = \"bloom-560m.q8_0.gguf\"\n     f16_tinyllama_model_id = \"TinyLlama-1.1B-Chat-v1.0.FP16.gguf\"\n+    q2_k_falcon7b_model_id = \"falcon-7b-q2_k.gguf\"\n+    fp16_falcon7b_model_id = \"falcon-7b-fp16.gguf\"\n+    q2_k_falcon40b_model_id = \"tiiuae-falcon-40b-Q2_K.gguf\"\n \n     example_text = \"Hello\"\n \n@@ -445,6 +451,58 @@ def test_bloom_weights_conversion_fp16(self):\n                 self.assertTrue(quantized_param.shape == original_param.shape)\n                 torch.testing.assert_close(quantized_param, original_param)\n \n+    @unittest.skip(reason=\"Heavy memory\")\n+    def test_falcon40b_q2_k(self):\n+        tokenizer = AutoTokenizer.from_pretrained(self.falcon40b_model_id, gguf_file=self.q2_k_falcon40b_model_id)\n+        model = AutoModelForCausalLM.from_pretrained(\n+            self.falcon40b_model_id,\n+            gguf_file=self.q2_k_falcon40b_model_id,\n+            device_map=\"auto\",\n+            torch_dtype=torch.float16,\n+        )\n+\n+        text = tokenizer(self.example_text, return_tensors=\"pt\").to(torch_device)\n+        out = model.generate(**text, max_new_tokens=10)\n+\n+        EXPECTED_TEXT = \"Hello All,\\nI am new to this forum.\"\n+        self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)\n+\n+    def test_falcon7b_q2_k(self):\n+        tokenizer = AutoTokenizer.from_pretrained(self.falcon7b_model_id, gguf_file=self.q2_k_falcon7b_model_id)\n+        model = AutoModelForCausalLM.from_pretrained(\n+            self.falcon7b_model_id,\n+            gguf_file=self.q2_k_falcon7b_model_id,\n+            device_map=\"auto\",\n+            torch_dtype=torch.float16,\n+        )\n+\n+        text = tokenizer(self.example_text, return_tensors=\"pt\").to(torch_device)\n+        out = model.generate(**text, max_new_tokens=10)\n+\n+        EXPECTED_TEXT = \"Hello All,\\nI am new to this forum.\"\n+        self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)\n+\n+    def test_falcon7b_weights_conversion_fp16(self):\n+        quantized_model = AutoModelForCausalLM.from_pretrained(\n+            self.falcon7b_model_id,\n+            gguf_file=self.fp16_falcon7b_model_id,\n+            device_map=\"auto\",\n+            torch_dtype=torch.float16,\n+        )\n+        original_model = AutoModelForCausalLM.from_pretrained(\n+            self.original_flacon7b_model_id,\n+            device_map=\"auto\",\n+            torch_dtype=torch.float16,\n+        )\n+\n+        quantized_state_dict = quantized_model.state_dict()\n+        original_state_dict = original_model.state_dict()\n+\n+        for layer_name, original_params in original_state_dict.items():\n+            if layer_name in quantized_state_dict:\n+                self.assertTrue(original_params.shape == quantized_state_dict[layer_name].shape)\n+                torch.testing.assert_close(original_params, quantized_state_dict[layer_name])\n+\n     def test_tokenization_xnli(self):\n         import tqdm\n         from datasets import load_dataset"
        }
    ],
    "stats": {
        "total": 123,
        "additions": 111,
        "deletions": 12
    }
}