{
    "author": "zucchini-nlp",
    "message": "Update migration guide - generation config (#42470)\n\n* update nit\n\n* ig doc builder is complaining about this unclosed code block?\n\n* ?\n\n* ??\n\n* then let's just not have another embedded code block in python block",
    "sha": "554fb40e8275fede4df538c6825f777d9b18dcd5",
    "files": [
        {
            "sha": "c529d87a2ab8ac2491d446b01db905da5a8685b7",
            "filename": "MIGRATION_GUIDE_V5.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/554fb40e8275fede4df538c6825f777d9b18dcd5/MIGRATION_GUIDE_V5.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/554fb40e8275fede4df538c6825f777d9b18dcd5/MIGRATION_GUIDE_V5.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/MIGRATION_GUIDE_V5.md?ref=554fb40e8275fede4df538c6825f777d9b18dcd5",
            "patch": "@@ -330,6 +330,7 @@ model_4bit = AutoModelForCausalLM.from_pretrained(\n - It is no longer possible to load a config class from a URL file. Configs must be loaded from either a local path or a repo on the Hub. See [#42383](https://github.com/huggingface/transformers/pull/42383).\n - All parameters for configuring model's rotary embedding are now stored under `mode.rope_parameters`, including the `rope_theta` and `rope_type`. Model's `config.rope_parameters` is a simple dictionaty in most cases, and can also be a nested dict in special cases (i.e. Gemma3 and ModernBert) with different rope parameterization for each layer type. See [#39847](https://github.com/huggingface/transformers/pull/39847)\n - Qwen-VL family configuration is in a nested format and trying to access keys directly will throw an error (e.g. `config.vocab_size`). Users are expected to access keys from their respective sub-configs (`config.text_config.vocab_size`).\n+- Configurations of non-generative models (any model that doesn't call `model.generate()`) will no longer have a `generation_config` and `model.config.generation_config` will throw an attribute error.\n \n ## Processing\n "
        },
        {
            "sha": "1fec1919ad3f823c9d1f97eebc610e1b487b8dc4",
            "filename": "src/transformers/models/cohere/tokenization_cohere.py",
            "status": "modified",
            "additions": 6,
            "deletions": 7,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/554fb40e8275fede4df538c6825f777d9b18dcd5/src%2Ftransformers%2Fmodels%2Fcohere%2Ftokenization_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/554fb40e8275fede4df538c6825f777d9b18dcd5/src%2Ftransformers%2Fmodels%2Fcohere%2Ftokenization_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Ftokenization_cohere.py?ref=554fb40e8275fede4df538c6825f777d9b18dcd5",
            "patch": "@@ -280,8 +280,8 @@ def apply_tool_use_template(\n         Examples:\n \n         ```python\n-        tokenizer = CohereTokenizer.from_pretrained(\"CohereForAI/c4ai-command-r-v01\")\n-        tools = [\n+        >> tokenizer = CohereTokenizer.from_pretrained(\"CohereForAI/c4ai-command-r-v01\")\n+        >> tools = [\n             {\n                 \"name\": \"internet_search\",\n                 \"description\": \"Returns a list of relevant document snippets for a textual query retrieved from the internet\",\n@@ -299,16 +299,15 @@ def apply_tool_use_template(\n                 \"parameter_definitions\": {},\n             },\n         ]\n-        conversation = [\n+        >> conversation = [\n             {\"role\": \"user\", \"content\": \"Whats the biggest penguin in the world?\"},\n         ]\n-        # Render the prompt, ready for user to inspect, or for input into the model\n-        prompt = tokenizer.apply_tool_use_template(conversation, tools=tools, tokenize=False, add_generation_prompt=True)\n-        print(prompt)\n+        >> # Render the prompt, ready for user to inspect, or for input into the model\n+        >> prompt = tokenizer.apply_tool_use_template(conversation, tools=tools, tokenize=False, add_generation_prompt=True)\n+        >> print(prompt)\n         >> inputs = tokenizer.encode(grounded_generation_prompt, add_special_tokens=False, return_tensors='pt')\n         >> outputs = model.generate(inputs, max_new_tokens=128)\n         >> print(tokenizer.decode(outputs[0]))\n-        Action: ```json\n         [\n             {\n                 \"tool_name\": \"internet_search\","
        }
    ],
    "stats": {
        "total": 14,
        "additions": 7,
        "deletions": 7
    }
}