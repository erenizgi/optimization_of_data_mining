{
    "author": "zucchini-nlp",
    "message": "[Model] Cohere2 Vision (#39810)\n\n* Add cohere2_vision to support CohereLabs/command-a-vision-07-2025\n\n* update and add modualr file\n\n* update processors and check with orig impl later\n\n* delete unused files\n\n* image processor reduce LOC and re-use GotOCR2\n\n* update the config to use modular\n\n* model tests pass\n\n* processor fixes\n\n* check model outputs decorator\n\n* address one more comment\n\n* Update tokens. Temp - need to read from tokenizer'\n\n* fix for multi-gpu\n\n* Fix image token handling\n\n* upadte image token expansion logic\n\n* fix a few issues with remote code loading\n\n* not related but modular forces us to change all files now\n\n* Add overview and code sample to cohere vision docs\n\n* add scripts. TMP.\n\n* Update inference script\n\n* Create script\n\n* set dtype in export script\n\n* TO revert: modular export fix\n\n* Fix scripts\n\n* Revert \"TO revert: modular export fix\"\n\nThis reverts commit bdb2f305b61027a05f0032ce70d6ca698879191c.\n\n* Use modular weights\n\n* Upload to hub\n\nRemoved OOD weights ad script\n\n* Updated docs\n\n* fix import error\n\nUpdate docs\n\nAdded pipeline test\n\n* Updated docs\n\n* Run modular script\n\nremove modular for config\n\nAdded patch_size\n\nAdded docstrings in modular\n\nFix OOM\n\nAdd docs, fixup integration tests. 8-gpu passing\n\n* tiny updates\n\n* address comments + fixup\n\n* add test for chat template\n\n* check model outputs workaround\n\n* aya vision fix check model inputs\n\n* Revert \"add test for chat template\"\n\nThis reverts commit 42c756e397f588d76b449ff1f93292d8ee0202d8.\n\n* reveert more changes\n\n* last revert\n\n* skip and merge\n\n* faulty copy from\n\n---------\n\nCo-authored-by: Julian Mack <julian.mack@cohere.com>\nCo-authored-by: kyle-cohere <kyle@cohere.com>",
    "sha": "e1688d28d3cce0b3e4c59e99fb970801970231c1",
    "files": [
        {
            "sha": "f20b9a2d802181d2ffe37016f35e89554e329330",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e1688d28d3cce0b3e4c59e99fb970801970231c1/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/e1688d28d3cce0b3e4c59e99fb970801970231c1/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=e1688d28d3cce0b3e4c59e99fb970801970231c1",
            "patch": "@@ -411,6 +411,8 @@\n         title: Cohere\n       - local: model_doc/cohere2\n         title: Cohere2\n+      - local: model_doc/cohere2_vision\n+        title: Cohere2Vision\n       - local: model_doc/convbert\n         title: ConvBERT\n       - local: model_doc/cpm"
        },
        {
            "sha": "123f9573b957daf8b962f99eaacd8f5ba10c0b11",
            "filename": "docs/source/en/model_doc/cohere2_vision.md",
            "status": "added",
            "additions": 92,
            "deletions": 0,
            "changes": 92,
            "blob_url": "https://github.com/huggingface/transformers/blob/e1688d28d3cce0b3e4c59e99fb970801970231c1/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere2_vision.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e1688d28d3cce0b3e4c59e99fb970801970231c1/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere2_vision.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere2_vision.md?ref=e1688d28d3cce0b3e4c59e99fb970801970231c1",
            "patch": "@@ -0,0 +1,92 @@\n+# Command A Vision\n+\n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"Tensor parallelism\" src=\"https://img.shields.io/badge/Tensor%20parallelism-06b6d4?style=flat&logoColor=white\">\n+</div>\n+\n+## Overview\n+\n+Command A Vision is a state-of-the-art multimodal model designed to seamlessly integrate visual and textual information for a wide range of applications. By combining advanced computer vision techniques with natural language processing capabilities, Command A Vision enables users to analyze, understand, and generate insights from both visual and textual data.\n+\n+The model excels at tasks including image captioning, visual question answering, document understanding, and chart understanding. This makes it a versatile tool for AI practitioners. Its ability to process complex visual and textual inputs makes it useful in settings where text-only representations are imprecise or unavailable, like real-world image understanding and graphics-heavy document processing.\n+\n+Command A Vision is built upon a robust architecture that leverages the latest advancements in VLMs. It's highly performant and efficient, even when dealing with large-scale datasets. The model's flexibility makes it suitable for a wide range of use cases, from content moderation and image search to medical imaging analysis and robotics.\n+\n+## Usage tips\n+\n+The model and image processor can be loaded as follows:\n+\n+```python\n+\n+import torch\n+from transformers import AutoProcessor, AutoModelForImageTextToText\n+\n+model_id = \"CohereLabs/command-a-vision-07-2025\"\n+\n+processor = AutoProcessor.from_pretrained(model_id)\n+model = AutoModelForImageTextToText.from_pretrained(\n+    model_id, device_map=\"auto\", torch_dtype=torch.float16\n+)\n+\n+# Format message with the Command-A-Vision chat template\n+messages = [\n+    {\n+        \"role\": \"user\",\n+        \"content\": [\n+            {\n+                \"type\": \"image\",\n+                \"url\": \"https://images.pexels.com/photos/1108099/pexels-photo-1108099.jpeg\",\n+            },\n+            {\"type\": \"text\", \"text\": \"what is in this image?\"},\n+        ],\n+    },\n+]\n+\n+inputs = processor.apply_chat_template(\n+    messages,\n+    padding=True,\n+    add_generation_prompt=True,\n+    tokenize=True,\n+    return_dict=True,\n+    return_tensors=\"pt\",\n+).to(model.device)\n+\n+gen_tokens = model.generate(\n+    **inputs,\n+    max_new_tokens=300,\n+    do_sample=True,\n+    temperature=0.3,\n+)\n+\n+print(\n+    processor.tokenizer.decode(\n+        gen_tokens[0][inputs.input_ids.shape[1] :], skip_special_tokens=True\n+    )\n+)\n+```\n+\n+## Cohere2VisionConfig\n+\n+[[autodoc]] Cohere2VisionConfig\n+\n+## Cohere2VisionForConditionalGeneration\n+\n+[[autodoc]] Cohere2VisionForConditionalGeneration\n+    - forward\n+\n+## Cohere2VisionModel\n+\n+[[autodoc]] Cohere2VisionModel\n+    - forward\n+\n+## Cohere2VisionImageProcessorFast\n+\n+[[autodoc]] Cohere2VisionImageProcessorFast\n+    - preprocess\n+\n+## Cohere2VisionProcessor\n+\n+[[autodoc]] Cohere2VisionProcessor"
        },
        {
            "sha": "b502400dbb93052bf9ec59970b8e4d77bc4a3cfe",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e1688d28d3cce0b3e4c59e99fb970801970231c1/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e1688d28d3cce0b3e4c59e99fb970801970231c1/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=e1688d28d3cce0b3e4c59e99fb970801970231c1",
            "patch": "@@ -1164,7 +1164,7 @@ def append_new_layers(self, layer_idx: int) -> None:\n         while len(self.layers) <= layer_idx:\n             kwargs = self.layer_init_kwargs.copy()\n             if self.layer_init_kwargs.get(\"layer_device_map\", None) is not None:\n-                kwargs[\"device\"] = kwargs.pop(\"layer_device_map\")[layer_idx]\n+                kwargs[\"device\"] = kwargs.pop(\"layer_device_map\")[len(self.layers)]\n \n             new_layer_class = (\n                 self.layer_classes[len(self.layers)] if isinstance(self.layer_classes, list) else self.layer_classes"
        },
        {
            "sha": "56c2f3fcdcf75087a67a823fc5de8a8aa42a0b15",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e1688d28d3cce0b3e4c59e99fb970801970231c1/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e1688d28d3cce0b3e4c59e99fb970801970231c1/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=e1688d28d3cce0b3e4c59e99fb970801970231c1",
            "patch": "@@ -63,6 +63,7 @@\n     from .codegen import *\n     from .cohere import *\n     from .cohere2 import *\n+    from .cohere2_vision import *\n     from .colpali import *\n     from .colqwen2 import *\n     from .conditional_detr import *"
        },
        {
            "sha": "c1278bee03ae3090ab8a27602d7b3926c0837547",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e1688d28d3cce0b3e4c59e99fb970801970231c1/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e1688d28d3cce0b3e4c59e99fb970801970231c1/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=e1688d28d3cce0b3e4c59e99fb970801970231c1",
            "patch": "@@ -1106,7 +1106,7 @@ def set_decoder(self, decoder):\n         self.model.set_decoder(decoder)\n \n     def get_decoder(self):\n-        return self.model.get_decoder\n+        return self.model.get_decoder()\n \n     def get_image_features(\n         self,"
        },
        {
            "sha": "6d78356c7205741dec64f36bbc6e0b11fcae80b2",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e1688d28d3cce0b3e4c59e99fb970801970231c1/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e1688d28d3cce0b3e4c59e99fb970801970231c1/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=e1688d28d3cce0b3e4c59e99fb970801970231c1",
            "patch": "@@ -81,6 +81,7 @@\n         (\"codegen\", \"CodeGenConfig\"),\n         (\"cohere\", \"CohereConfig\"),\n         (\"cohere2\", \"Cohere2Config\"),\n+        (\"cohere2_vision\", \"Cohere2VisionConfig\"),\n         (\"colpali\", \"ColPaliConfig\"),\n         (\"colqwen2\", \"ColQwen2Config\"),\n         (\"conditional_detr\", \"ConditionalDetrConfig\"),\n@@ -476,6 +477,7 @@\n         (\"codegen\", \"CodeGen\"),\n         (\"cohere\", \"Cohere\"),\n         (\"cohere2\", \"Cohere2\"),\n+        (\"cohere2_vision\", \"Cohere2Vision\"),\n         (\"colpali\", \"ColPali\"),\n         (\"colqwen2\", \"ColQwen2\"),\n         (\"conditional_detr\", \"Conditional DETR\"),"
        },
        {
            "sha": "5e28f2ac2db96ac80451d7c6a92c28f478634abd",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e1688d28d3cce0b3e4c59e99fb970801970231c1/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e1688d28d3cce0b3e4c59e99fb970801970231c1/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=e1688d28d3cce0b3e4c59e99fb970801970231c1",
            "patch": "@@ -72,6 +72,7 @@\n             (\"chinese_clip\", (\"ChineseCLIPImageProcessor\", \"ChineseCLIPImageProcessorFast\")),\n             (\"clip\", (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")),\n             (\"clipseg\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n+            (\"cohere2_vision\", (\"Cohere2VisionImageProcessorFast\",)),\n             (\"conditional_detr\", (\"ConditionalDetrImageProcessor\", \"ConditionalDetrImageProcessorFast\")),\n             (\"convnext\", (\"ConvNextImageProcessor\", \"ConvNextImageProcessorFast\")),\n             (\"convnextv2\", (\"ConvNextImageProcessor\", \"ConvNextImageProcessorFast\")),"
        },
        {
            "sha": "ea69ed911d14d63de70ca1bc97bba2516fb0d127",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e1688d28d3cce0b3e4c59e99fb970801970231c1/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e1688d28d3cce0b3e4c59e99fb970801970231c1/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=e1688d28d3cce0b3e4c59e99fb970801970231c1",
            "patch": "@@ -77,6 +77,7 @@\n         (\"codegen\", \"CodeGenModel\"),\n         (\"cohere\", \"CohereModel\"),\n         (\"cohere2\", \"Cohere2Model\"),\n+        (\"cohere2_vision\", \"Cohere2VisionModel\"),\n         (\"conditional_detr\", \"ConditionalDetrModel\"),\n         (\"convbert\", \"ConvBertModel\"),\n         (\"convnext\", \"ConvNextModel\"),\n@@ -944,6 +945,7 @@\n         (\"blip\", \"BlipForConditionalGeneration\"),\n         (\"blip-2\", \"Blip2ForConditionalGeneration\"),\n         (\"chameleon\", \"ChameleonForConditionalGeneration\"),\n+        (\"cohere2_vision\", \"Cohere2VisionForConditionalGeneration\"),\n         (\"deepseek_vl\", \"DeepseekVLForConditionalGeneration\"),\n         (\"deepseek_vl_hybrid\", \"DeepseekVLHybridForConditionalGeneration\"),\n         (\"emu3\", \"Emu3ForConditionalGeneration\"),"
        },
        {
            "sha": "8da528626081b203ce3aa8687a6cd4d4df9df233",
            "filename": "src/transformers/models/auto/processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e1688d28d3cce0b3e4c59e99fb970801970231c1/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e1688d28d3cce0b3e4c59e99fb970801970231c1/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py?ref=e1688d28d3cce0b3e4c59e99fb970801970231c1",
            "patch": "@@ -60,6 +60,7 @@\n         (\"clip\", \"CLIPProcessor\"),\n         (\"clipseg\", \"CLIPSegProcessor\"),\n         (\"clvp\", \"ClvpProcessor\"),\n+        (\"cohere2_vision\", \"Cohere2VisionProcessor\"),\n         (\"colpali\", \"ColPaliProcessor\"),\n         (\"colqwen2\", \"ColQwen2Processor\"),\n         (\"deepseek_vl\", \"DeepseekVLProcessor\"),"
        },
        {
            "sha": "1b79c156a9582201f14dfcc367a6918ca2153f79",
            "filename": "src/transformers/models/aya_vision/configuration_aya_vision.py",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/e1688d28d3cce0b3e4c59e99fb970801970231c1/src%2Ftransformers%2Fmodels%2Faya_vision%2Fconfiguration_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e1688d28d3cce0b3e4c59e99fb970801970231c1/src%2Ftransformers%2Fmodels%2Faya_vision%2Fconfiguration_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fconfiguration_aya_vision.py?ref=e1688d28d3cce0b3e4c59e99fb970801970231c1",
            "patch": "@@ -33,9 +33,9 @@ class AyaVisionConfig(PretrainedConfig):\n     documentation from [`PretrainedConfig`] for more information.\n \n     Args:\n-        vision_config (`Union[AutoConfig, dict]`,  *optional*, defaults to `CLIPVisionConfig`):\n+        vision_config (`Union[AutoConfig, dict]`,  *optional*, defaults to `SiglipVisionConfig`):\n             The config object or dictionary of the vision backbone.\n-        text_config (`Union[AutoConfig, dict]`, *optional*, defaults to `LlamaConfig`):\n+        text_config (`Union[AutoConfig, dict]`, *optional*, defaults to `Cohere2Config`):\n             The config object or dictionary of the text backbone.\n         vision_feature_select_strategy (`str`, *optional*, defaults to `\"full\"`):\n             The feature selection strategy used to select the vision feature from the vision backbone.\n@@ -81,7 +81,9 @@ def __init__(\n         self.vision_feature_layer = vision_feature_layer\n \n         if isinstance(vision_config, dict):\n-            vision_config[\"model_type\"] = vision_config.get(\"model_type\", \"clip_vision_model\")\n+            vision_config[\"model_type\"] = (\n+                vision_config[\"model_type\"] if \"model_type\" in vision_config else \"siglip_vision_model\"\n+            )\n             vision_config = CONFIG_MAPPING[vision_config[\"model_type\"]](**vision_config)\n         elif vision_config is None:\n             vision_config = CONFIG_MAPPING[\"siglip_vision_model\"](\n@@ -97,7 +99,7 @@ def __init__(\n         self.vision_config = vision_config\n \n         if isinstance(text_config, dict):\n-            text_config[\"model_type\"] = text_config.get(\"model_type\", \"llama\")\n+            text_config[\"model_type\"] = text_config[\"model_type\"] if \"model_type\" in text_config else \"cohere2\"\n             text_config = CONFIG_MAPPING[text_config[\"model_type\"]](**text_config)\n         elif text_config is None:\n             text_config = CONFIG_MAPPING[\"cohere2\"]()"
        },
        {
            "sha": "dd922c63ce8a3cf05bb459ce9f7196a40fb51284",
            "filename": "src/transformers/models/aya_vision/modeling_aya_vision.py",
            "status": "modified",
            "additions": 7,
            "deletions": 13,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/e1688d28d3cce0b3e4c59e99fb970801970231c1/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e1688d28d3cce0b3e4c59e99fb970801970231c1/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py?ref=e1688d28d3cce0b3e4c59e99fb970801970231c1",
            "patch": "@@ -33,6 +33,7 @@\n from ...modeling_utils import PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_torchdynamo_compiling\n+from ...utils.generic import check_model_inputs\n from ..auto import AutoModel\n from .configuration_aya_vision import AyaVisionConfig\n \n@@ -99,6 +100,10 @@ class AyaVisionPreTrainedModel(PreTrainedModel):\n     _can_compile_fullgraph = False\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n+    _can_record_outputs = {\n+        \"hidden_states\": \"DecoderLayer\",\n+        \"attentions\": \"Attention\",\n+    }\n \n \n @dataclass\n@@ -237,7 +242,7 @@ def get_image_features(\n         image_features = self.multi_modal_projector(selected_image_feature)\n         return image_features\n \n-    @can_return_tuple\n+    @check_model_inputs\n     @auto_docstring\n     def forward(\n         self,\n@@ -250,17 +255,9 @@ def forward(\n         vision_feature_layer: Optional[Union[int, list[int]]] = None,\n         vision_feature_select_strategy: Optional[str] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[tuple, AyaVisionModelOutputWithPast]:\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n         vision_feature_layer = (\n             vision_feature_layer if vision_feature_layer is not None else self.config.vision_feature_layer\n         )\n@@ -308,9 +305,6 @@ def forward(\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=True,\n             cache_position=cache_position,\n             **kwargs,\n         )\n@@ -357,7 +351,7 @@ def set_decoder(self, decoder):\n         self.model.set_decoder(decoder)\n \n     def get_decoder(self):\n-        return self.model.get_decoder\n+        return self.model.get_decoder()\n \n     def get_image_features(\n         self,"
        },
        {
            "sha": "5a7b0950cd29b1010b895f29643f333b2bfb9119",
            "filename": "src/transformers/models/aya_vision/modular_aya_vision.py",
            "status": "modified",
            "additions": 7,
            "deletions": 13,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/e1688d28d3cce0b3e4c59e99fb970801970231c1/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodular_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e1688d28d3cce0b3e4c59e99fb970801970231c1/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodular_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodular_aya_vision.py?ref=e1688d28d3cce0b3e4c59e99fb970801970231c1",
            "patch": "@@ -32,7 +32,8 @@\n from ...cache_utils import Cache\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...processing_utils import Unpack\n-from ...utils import auto_docstring, can_return_tuple, is_torchdynamo_compiling, logging\n+from ...utils import auto_docstring, is_torchdynamo_compiling, logging\n+from ...utils.generic import check_model_inputs\n from .configuration_aya_vision import AyaVisionConfig\n \n \n@@ -91,6 +92,10 @@ def pixel_shuffle(self, image_features):  # B, S, D\n \n class AyaVisionPreTrainedModel(LlavaPreTrainedModel):\n     _can_compile_fullgraph = False\n+    _can_record_outputs = {\n+        \"hidden_states\": \"DecoderLayer\",\n+        \"attentions\": \"Attention\",\n+    }\n \n \n class AyaVisionCausalLMOutputWithPast(LlavaCausalLMOutputWithPast):\n@@ -158,7 +163,7 @@ def get_image_features(\n         image_features = self.multi_modal_projector(selected_image_feature)\n         return image_features\n \n-    @can_return_tuple\n+    @check_model_inputs\n     @auto_docstring\n     def forward(\n         self,\n@@ -171,17 +176,9 @@ def forward(\n         vision_feature_layer: Optional[Union[int, list[int]]] = None,\n         vision_feature_select_strategy: Optional[str] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[tuple, AyaVisionModelOutputWithPast]:\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n         vision_feature_layer = (\n             vision_feature_layer if vision_feature_layer is not None else self.config.vision_feature_layer\n         )\n@@ -229,9 +226,6 @@ def forward(\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=True,\n             cache_position=cache_position,\n             **kwargs,\n         )"
        },
        {
            "sha": "9b20eb3c1e0a8b42669761c9c45ca292b490df27",
            "filename": "src/transformers/models/cohere2_vision/__init__.py",
            "status": "added",
            "additions": 29,
            "deletions": 0,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/e1688d28d3cce0b3e4c59e99fb970801970231c1/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e1688d28d3cce0b3e4c59e99fb970801970231c1/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2F__init__.py?ref=e1688d28d3cce0b3e4c59e99fb970801970231c1",
            "patch": "@@ -0,0 +1,29 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_cohere2_vision import *\n+    from .image_processing_cohere2_vision_fast import *\n+    from .modeling_cohere2_vision import *\n+    from .processing_cohere2_vision import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "e4e670e4a66266f0e9a6690ad05c6d5b873ebe61",
            "filename": "src/transformers/models/cohere2_vision/configuration_cohere2_vision.py",
            "status": "added",
            "additions": 84,
            "deletions": 0,
            "changes": 84,
            "blob_url": "https://github.com/huggingface/transformers/blob/e1688d28d3cce0b3e4c59e99fb970801970231c1/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fconfiguration_cohere2_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e1688d28d3cce0b3e4c59e99fb970801970231c1/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fconfiguration_cohere2_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fconfiguration_cohere2_vision.py?ref=e1688d28d3cce0b3e4c59e99fb970801970231c1",
            "patch": "@@ -0,0 +1,84 @@\n+# Copyright 2025 the Cohere Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from ...configuration_utils import PretrainedConfig\n+from ..auto import CONFIG_MAPPING, AutoConfig\n+\n+\n+class Cohere2VisionConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`Cohere2VisionForConditionalGeneration`]. It is used to instantiate an\n+    Cohere2 Vision model according to the specified arguments, defining the model architecture.\n+\n+    [CohereLabs/command-a-vision-07-2025](https://huggingface.co/CohereLabs/command-a-vision-07-2025)\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        vision_config (`Union[AutoConfig, dict]`,  *optional*, defaults to `SiglipVisionConfig`):\n+            The config object or dictionary of the vision backbone.\n+        text_config (`Union[AutoConfig, dict]`, *optional*, defaults to `Cohere2Config`):\n+            The config object or dictionary of the text backbone.\n+        downsample_factor (`int`, *optional*, defaults to 2):\n+            The factor by which to downsample the input image.\n+        image_token_id (`int`, *optional*, defaults to 255036):\n+            The token ID to use as placeholder for the image input.\n+        alignment_intermediate_size (`int`, *optional*, defaults to 36864):\n+            The size of the intermediate layer for alignment.\n+    \"\"\"\n+\n+    model_type = \"cohere2_vision\"\n+    sub_configs = {\"text_config\": AutoConfig, \"vision_config\": AutoConfig}\n+\n+    def __init__(\n+        self,\n+        vision_config=None,\n+        text_config=None,\n+        downsample_factor=2,\n+        image_token_id=255036,\n+        alignment_intermediate_size=36864,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+        self.downsample_factor = downsample_factor\n+        self.image_token_id = image_token_id\n+        self.alignment_intermediate_size = alignment_intermediate_size\n+\n+        if isinstance(vision_config, dict):\n+            vision_config[\"model_type\"] = (\n+                vision_config[\"model_type\"] if \"model_type\" in vision_config else \"siglip_vision_model\"\n+            )\n+            vision_config = CONFIG_MAPPING[vision_config[\"model_type\"]](**vision_config)\n+        elif vision_config is None:\n+            vision_config = CONFIG_MAPPING[\"siglip_vision_model\"](\n+                hidden_size=1152,\n+                intermediate_size=3072,\n+                image_size=512,\n+                num_hidden_layers=27,\n+                num_attention_heads=12,\n+            )\n+\n+        self.vision_config = vision_config\n+\n+        if isinstance(text_config, dict):\n+            text_config[\"model_type\"] = text_config[\"model_type\"] if \"model_type\" in text_config else \"cohere2\"\n+            text_config = CONFIG_MAPPING[text_config[\"model_type\"]](**text_config)\n+        elif text_config is None:\n+            text_config = CONFIG_MAPPING[\"cohere2\"](tie_word_embeddings=True)\n+\n+        self.text_config = text_config\n+\n+\n+__all__ = [\"Cohere2VisionConfig\"]"
        },
        {
            "sha": "6c1aaa48a397b05e5010899977513f9413ac9168",
            "filename": "src/transformers/models/cohere2_vision/image_processing_cohere2_vision_fast.py",
            "status": "added",
            "additions": 309,
            "deletions": 0,
            "changes": 309,
            "blob_url": "https://github.com/huggingface/transformers/blob/e1688d28d3cce0b3e4c59e99fb970801970231c1/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fimage_processing_cohere2_vision_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e1688d28d3cce0b3e4c59e99fb970801970231c1/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fimage_processing_cohere2_vision_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fimage_processing_cohere2_vision_fast.py?ref=e1688d28d3cce0b3e4c59e99fb970801970231c1",
            "patch": "@@ -0,0 +1,309 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/cohere2_vision/modular_cohere2_vision.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_cohere2_vision.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright 2025 the Cohere Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from functools import lru_cache\n+from typing import Optional, Union\n+\n+import numpy as np\n+import torch\n+\n+from ...image_processing_utils import BatchFeature\n+from ...image_processing_utils_fast import (\n+    BaseImageProcessorFast,\n+    DefaultFastImageProcessorKwargs,\n+    group_images_by_shape,\n+    reorder_images,\n+)\n+from ...image_utils import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD, ImageInput, PILImageResampling, SizeDict\n+from ...processing_utils import Unpack\n+from ...utils import TensorType, auto_docstring, is_torchvision_v2_available\n+\n+\n+if is_torchvision_v2_available():\n+    from torchvision.transforms.v2 import functional as F\n+else:\n+    from torchvision.transforms import functional as F\n+\n+\n+class Cohere2VisionFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n+    \"\"\"\n+    crop_to_patches (`bool`, *optional*, defaults to `False`):\n+        Whether to crop the image to patches. Can be overridden by the `crop_to_patches` parameter in the\n+        `preprocess` method.\n+    min_patches (`int`, *optional*, defaults to 1):\n+        The minimum number of patches to be extracted from the image. Only has an effect if `crop_to_patches` is\n+        set to `True`. Can be overridden by the `min_patches` parameter in the `preprocess` method.\n+    max_patches (`int`, *optional*, defaults to 12):\n+        The maximum number of patches to be extracted from the image. Only has an effect if `crop_to_patches` is\n+        set to `True`. Can be overridden by the `max_patches` parameter in the `preprocess` method.\n+    \"\"\"\n+\n+    crop_to_patches: Optional[bool]\n+    min_patches: Optional[int]\n+    max_patches: Optional[int]\n+\n+\n+@lru_cache(maxsize=10)\n+def get_all_supported_aspect_ratios(max_image_tiles: int) -> list[tuple[int, int]]:\n+    \"\"\"\n+    Computes all allowed aspect ratios for a given maximum number of input tiles.\n+\n+    This function calculates all possible arrangements of tiles that can be formed\n+    within the constraint of the maximum number of tiles. Each arrangement is\n+    represented by its aspect ratio (width/height) and the corresponding tile configuration.\n+\n+    Args:\n+        max_image_tiles (`int`):\n+            The maximum number of tiles allowed.\n+\n+    Returns:\n+        `list[tuple[int, int]]`: A list of tuples, each tuple representing a valid (width, height)\n+        configuration in terms of number of tiles.\n+\n+    Example:\n+        >>> get_all_supported_aspect_ratios(4)\n+        [(1, 1), (1, 2), (1, 3), (1, 4), (2, 1), (2, 2), (3, 1), (4, 1)]\n+\n+    \"\"\"\n+    aspect_ratios = []\n+    for width in range(1, max_image_tiles + 1):\n+        for height in range(1, max_image_tiles + 1):\n+            if width * height <= max_image_tiles:\n+                aspect_ratios.append((width, height))\n+    return aspect_ratios\n+\n+\n+def get_optimal_tiled_canvas(\n+    original_image_size: tuple[int, int],\n+    target_tile_size: tuple[int, int],\n+    min_image_tiles: int,\n+    max_image_tiles: int,\n+) -> tuple[int, int]:\n+    possible_resolutions = get_all_supported_aspect_ratios(max_image_tiles)\n+    possible_resolutions = sorted(possible_resolutions, key=lambda x: x[0] * x[1])\n+    image_height, image_width = original_image_size\n+    patch_size_height, patch_size_width = target_tile_size  # (height == width)\n+\n+    candidate_resolutions = np.array(possible_resolutions) * patch_size_height\n+    original_size = np.stack([image_height, image_width])\n+    required_scales = candidate_resolutions / original_size\n+    required_scale = np.min(required_scales, axis=-1, keepdims=True)  # [n_resolutions, 1]\n+    if np.all(required_scale < 1):\n+        # We are forced to downscale, so try to minimize the amount of downscaling\n+        best_grid = possible_resolutions[np.argmax(required_scale)]\n+    else:\n+        # Pick the resolution that required the least upscaling so that it most closely fits the image\n+        required_scale = np.where(required_scale < 1.0, 10e9, required_scale)\n+        best_grid = possible_resolutions[np.argmin(required_scale)]\n+    return best_grid\n+\n+\n+@auto_docstring\n+class Cohere2VisionImageProcessorFast(BaseImageProcessorFast):\n+    resample = PILImageResampling.BICUBIC\n+    image_mean = OPENAI_CLIP_MEAN\n+    image_std = OPENAI_CLIP_STD\n+    size = {\"height\": 512, \"width\": 512}\n+    do_resize = True\n+    do_rescale = True\n+    do_normalize = True\n+    do_convert_rgb = True\n+    crop_to_patches = True\n+    min_patches = 1\n+    max_patches = 12\n+    valid_kwargs = Cohere2VisionFastImageProcessorKwargs\n+    patch_size = 16\n+\n+    def __init__(self, **kwargs: Unpack[Cohere2VisionFastImageProcessorKwargs]):\n+        super().__init__(**kwargs)\n+\n+    @auto_docstring\n+    def preprocess(self, images: ImageInput, **kwargs: Unpack[Cohere2VisionFastImageProcessorKwargs]) -> BatchFeature:\n+        return super().preprocess(images, **kwargs)\n+\n+    def crop_image_to_patches(\n+        self,\n+        images: \"torch.Tensor\",\n+        min_patches: int,\n+        max_patches: int,\n+        use_thumbnail: bool = True,\n+        patch_size: Optional[Union[tuple, int, dict]] = None,\n+        interpolation: Optional[\"F.InterpolationMode\"] = None,\n+    ):\n+        \"\"\"\n+        Crop the images to patches and return a list of cropped images.\n+        The number of patches and their grid arrangement are determined by the original image size,\n+        the target patch size and the minimum and maximum number of patches.\n+        The aspect ratio of the patches grid is chosen to be the closest to the original image aspect ratio.\n+\n+        Args:\n+            images (`torch.Tensor`):\n+                The images to be cropped.\n+            min_patches (`int`):\n+                The minimum number of patches to be extracted from the image.\n+            max_patches (`int`):\n+                The maximum number of patches to be extracted from the image.\n+            use_thumbnail (`bool`, *optional*, defaults to `True`):\n+                Whether to add a thumbnail image to the list of cropped patches.\n+            patch_size (`int`, `tuple[int, int]`, `dict`, *optional*):\n+                The size of the output patches.\n+                The format of the image data. If `None`, the format is inferred from the input image.\n+\n+        Returns:\n+            list[`PIL.Image.Image`] or list[np.ndarray]: The list of cropped images.\n+        \"\"\"\n+        patch_size_height, patch_size_width = patch_size.height, patch_size.width\n+        original_height, original_width = images.shape[-2:]\n+        # find the closest aspect ratio to the target\n+        num_columns, num_rows = get_optimal_tiled_canvas(\n+            (original_height, original_width), (patch_size_height, patch_size_width), min_patches, max_patches\n+        )\n+\n+        # calculate the target width and height\n+        target_width = patch_size_width * num_columns\n+        target_height = patch_size_height * num_rows\n+        num_blocks = num_columns * num_rows\n+\n+        # resize the image so that each patch is of patch_size\n+        resized_image = self.resize(\n+            images, SizeDict(height=target_height, width=target_width), interpolation=interpolation\n+        )\n+        # split the image into patches\n+        processed_images = []\n+        for i in range(num_blocks):\n+            column = i % num_columns\n+            row = i // num_columns\n+            box = (\n+                column * patch_size_width,\n+                row * patch_size_height,\n+                (column + 1) * patch_size_width,\n+                (row + 1) * patch_size_height,\n+            )\n+            # split the image\n+            patch_image = resized_image[..., box[1] : box[3], box[0] : box[2]]\n+            processed_images.append(patch_image)\n+\n+        if use_thumbnail and len(processed_images) != 1:\n+            thumbnail_img = self.resize(images, patch_size, interpolation=interpolation)\n+            processed_images.append(thumbnail_img)\n+\n+        processed_images = torch.stack(processed_images, dim=0).transpose(0, 1).contiguous()\n+\n+        return processed_images\n+\n+    def _preprocess(\n+        self,\n+        images: list[\"torch.Tensor\"],\n+        do_resize: bool,\n+        size: SizeDict,\n+        crop_to_patches: bool,\n+        min_patches: int,\n+        max_patches: int,\n+        interpolation: Optional[\"F.InterpolationMode\"],\n+        do_center_crop: bool,\n+        crop_size: SizeDict,\n+        do_rescale: bool,\n+        rescale_factor: float,\n+        do_normalize: bool,\n+        image_mean: Optional[Union[float, list[float]]],\n+        image_std: Optional[Union[float, list[float]]],\n+        disable_grouping: Optional[bool],\n+        return_tensors: Optional[Union[str, TensorType]],\n+    ) -> BatchFeature:\n+        if crop_to_patches:\n+            grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n+            processed_images_grouped = {}\n+            num_patches = {}\n+            for shape, stacked_images in grouped_images.items():\n+                stacked_images = self.crop_image_to_patches(\n+                    stacked_images,\n+                    min_patches,\n+                    max_patches,\n+                    patch_size=size,\n+                    interpolation=interpolation,\n+                )\n+                processed_images_grouped[shape] = stacked_images\n+                num_patches[shape] = [stacked_images.shape[1]] * stacked_images.shape[0]\n+            images = reorder_images(processed_images_grouped, grouped_images_index)\n+            images = [image for images_list in images for image in images_list]\n+            num_patches = reorder_images(num_patches, grouped_images_index)\n+        else:\n+            num_patches = [1] * len(images)\n+\n+        # Group images by size for batched resizing\n+        grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n+        resized_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            if do_resize:\n+                stacked_images = self.resize(image=stacked_images, size=size, interpolation=interpolation)\n+            resized_images_grouped[shape] = stacked_images\n+        resized_images = reorder_images(resized_images_grouped, grouped_images_index)\n+\n+        # Group images by size for further processing\n+        # Needed in case do_resize is False, or resize returns images with different sizes\n+        grouped_images, grouped_images_index = group_images_by_shape(resized_images, disable_grouping=disable_grouping)\n+        processed_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            if do_center_crop:\n+                stacked_images = self.center_crop(stacked_images, crop_size)\n+            # Fused rescale and normalize\n+            stacked_images = self.rescale_and_normalize(\n+                stacked_images, do_rescale, rescale_factor, do_normalize, image_mean, image_std\n+            )\n+            processed_images_grouped[shape] = stacked_images\n+\n+        processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n+        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n+\n+        return BatchFeature(\n+            data={\"pixel_values\": processed_images, \"num_patches\": num_patches}, tensor_type=return_tensors\n+        )\n+\n+    def get_number_of_image_patches(self, height: int, width: int, images_kwargs=None):\n+        \"\"\"\n+        A utility that returns number patches for a given image size.\n+\n+        Args:\n+            height (`int`):\n+                Height of the input image.\n+            width (`int`):\n+                Width of the input image.\n+            images_kwargs (`dict`, *optional*)\n+                Any kwargs to override defaults of the image processor.\n+        Returns:\n+            `int`: Number of patches per image.\n+        \"\"\"\n+        min_patches = images_kwargs.get(\"min_patches\", self.min_patches)\n+        max_patches = images_kwargs.get(\"max_patches\", self.max_patches)\n+        patch_size = images_kwargs.get(\"patch_size\", self.size)\n+        crop_to_patches = images_kwargs.get(\"crop_to_patches\", self.crop_to_patches)\n+\n+        num_patches = 1\n+        if crop_to_patches and max_patches > 1:\n+            num_columns, num_rows = get_optimal_tiled_canvas(\n+                (height, width), (patch_size[\"height\"], patch_size[\"width\"]), min_patches, max_patches\n+            )\n+            num_patches += num_columns * num_rows\n+\n+        return num_patches\n+\n+\n+__all__ = [\"Cohere2VisionImageProcessorFast\"]"
        },
        {
            "sha": "91f84cdd6b8cd5c5b6fd1f8d5c0ec15ad46bfa4c",
            "filename": "src/transformers/models/cohere2_vision/modeling_cohere2_vision.py",
            "status": "added",
            "additions": 432,
            "deletions": 0,
            "changes": 432,
            "blob_url": "https://github.com/huggingface/transformers/blob/e1688d28d3cce0b3e4c59e99fb970801970231c1/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fmodeling_cohere2_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e1688d28d3cce0b3e4c59e99fb970801970231c1/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fmodeling_cohere2_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fmodeling_cohere2_vision.py?ref=e1688d28d3cce0b3e4c59e99fb970801970231c1",
            "patch": "@@ -0,0 +1,432 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/cohere2_vision/modular_cohere2_vision.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_cohere2_vision.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright 2025 the Cohere Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from dataclasses import dataclass\n+from typing import Optional, Union\n+\n+import torch\n+from torch import nn\n+\n+from ...cache_utils import Cache\n+from ...generation import GenerationMixin\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_outputs import BaseModelOutputWithPast, ModelOutput\n+from ...modeling_utils import PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs, auto_docstring\n+from ...utils.generic import check_model_inputs\n+from ..auto import AutoModel\n+from .configuration_cohere2_vision import Cohere2VisionConfig\n+\n+\n+class Cohere2VisionMultiModalProjector(nn.Module):\n+    def __init__(self, config: Cohere2VisionConfig):\n+        super().__init__()\n+        self.config = config\n+        self.downsample_factor = config.downsample_factor\n+        self.intermediate_size = config.alignment_intermediate_size\n+        self.linear_1 = nn.Linear(\n+            config.vision_config.hidden_size * (config.downsample_factor**2), self.intermediate_size, bias=True\n+        )\n+        self.act = nn.SiLU()\n+        self.linear_2 = nn.Linear(self.intermediate_size // 2, config.text_config.hidden_size, bias=True)\n+\n+    def pixel_shuffle(self, image_features):  # B, S, D\n+        batch_size, seq_length, feature_dim = image_features.shape\n+        height = width = int(seq_length**0.5)\n+        image_features = image_features.reshape(image_features.shape[0], width, height, -1)\n+        channels = image_features.shape[-1]\n+        image_features = image_features.reshape(\n+            batch_size, width, int(height / self.downsample_factor), int(channels * self.downsample_factor)\n+        )\n+        image_features = image_features.permute(0, 2, 1, 3)\n+        image_features = image_features.reshape(\n+            batch_size, int(height / self.downsample_factor), int(width / self.downsample_factor), -1\n+        )\n+        image_features = image_features.permute(0, 2, 1, 3)\n+        return image_features\n+\n+    def forward(self, image_features):\n+        image_features = self.pixel_shuffle(image_features)\n+        hidden_states = self.linear_1(image_features)\n+\n+        # Split along last dimension and apply SwiGLU\n+        x, gate = hidden_states.chunk(2, dim=-1)\n+        hidden_states = self.act(gate) * x\n+\n+        hidden_states = self.linear_2(hidden_states)\n+        return hidden_states\n+\n+\n+@dataclass\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Base class for Cohere2Vision outputs, with hidden states and attentions.\n+    \"\"\"\n+)\n+class Cohere2VisionModelOutputWithPast(BaseModelOutputWithPast):\n+    r\"\"\"\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n+    image_hidden_states (`torch.FloatTensor`, *optional*):\n+        A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n+        image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    \"\"\"\n+\n+    image_hidden_states: Optional[torch.FloatTensor] = None\n+\n+\n+@dataclass\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Base class for Cohere2Vision causal language model (or autoregressive) outputs.\n+    \"\"\"\n+)\n+class Cohere2VisionCausalLMOutputWithPast(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Language modeling loss (for next-token prediction).\n+    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n+    image_hidden_states (`torch.FloatTensor`, *optional*):\n+        A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n+        image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    \"\"\"\n+\n+    loss: Optional[torch.FloatTensor] = None\n+    logits: Optional[torch.FloatTensor] = None\n+    past_key_values: Optional[list[torch.FloatTensor]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n+    attentions: Optional[tuple[torch.FloatTensor]] = None\n+    image_hidden_states: Optional[torch.FloatTensor] = None\n+\n+\n+@auto_docstring\n+class Cohere2VisionPreTrainedModel(PreTrainedModel):\n+    config: Cohere2VisionConfig\n+    base_model_prefix = \"\"\n+    supports_gradient_checkpointing = True\n+    _skip_keys_device_placement = \"past_key_values\"\n+\n+    _supports_flash_attn = True\n+    _supports_sdpa = True\n+    _can_compile_fullgraph = False\n+    _supports_flex_attn = True\n+    _supports_attention_backend = True\n+    _can_record_outputs = {\n+        \"hidden_states\": \"DecoderLayer\",\n+        \"attentions\": \"Attention\",\n+    }\n+\n+\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    The Cohere2Vision model which consists of a vision backbone and a language model, without a language modeling head.\n+    \"\"\"\n+)\n+class Cohere2VisionModel(Cohere2VisionPreTrainedModel):\n+    _checkpoint_conversion_mapping = {}\n+\n+    def __init__(self, config: Cohere2VisionConfig):\n+        super().__init__(config)\n+        self.vision_tower = AutoModel.from_config(config.vision_config)\n+\n+        self.multi_modal_projector = Cohere2VisionMultiModalProjector(config)\n+        self.language_model = AutoModel.from_config(config.text_config)\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.language_model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.language_model.set_input_embeddings(value)\n+\n+    def set_decoder(self, decoder):\n+        self.language_model = decoder\n+\n+    def get_decoder(self):\n+        return self.language_model\n+\n+    def get_image_features(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        image_num_patches: torch.Tensor,\n+    ):\n+        \"\"\"\n+        Obtains image last hidden states from the vision tower and apply multimodal projection.\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor]` of shape `(batch_size, num_patches, channels, height, width)`)\n+               The tensors corresponding to the input images.\n+            image_num_patches (`torch.Tensor` of shape `(num_images)`)\n+                Number of patches for each image.\n+        Returns:\n+            image_features (List[`torch.Tensor`]): List of image feature tensor, each contains all the visual feature of all patches\n+            and are of shape `(num_patches, image_length, embed_dim)`).\n+        \"\"\"\n+\n+        image_features = self.vision_tower(pixel_values, output_hidden_states=True)\n+        selected_image_feature = image_features.last_hidden_state\n+        image_features = self.multi_modal_projector(selected_image_feature)\n+        return image_features\n+\n+    @check_model_inputs\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n+        image_num_patches: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> Union[tuple, Cohere2VisionModelOutputWithPast]:\n+        r\"\"\"\n+        image_num_patches (`torch.Tensor` of shape `(num_images,)`):\n+            Number of patches per input image.\n+        \"\"\"\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.get_input_embeddings()(input_ids)\n+\n+        if pixel_values is not None:\n+            image_features = self.get_image_features(pixel_values, image_num_patches=image_num_patches)\n+            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+\n+            if input_ids is None:\n+                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+                special_image_mask = special_image_mask.all(-1)\n+            else:\n+                special_image_mask = input_ids == self.config.image_token_id\n+\n+            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n+\n+        outputs = self.language_model(\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+        return Cohere2VisionModelOutputWithPast(\n+            last_hidden_state=outputs.last_hidden_state,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            image_hidden_states=image_features if pixel_values is not None else None,\n+        )\n+\n+\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    The COHERE2_VISION model which consists of a vision backbone and a language model.\n+    \"\"\"\n+)\n+class Cohere2VisionForConditionalGeneration(Cohere2VisionPreTrainedModel, GenerationMixin):\n+    _checkpoint_conversion_mapping = {}\n+    _tied_weights_keys = [\"lm_head.weight\"]\n+\n+    def __init__(self, config: Cohere2VisionConfig):\n+        super().__init__(config)\n+        self.model = Cohere2VisionModel(config)\n+        self.lm_head = nn.Linear(config.text_config.hidden_size, config.text_config.vocab_size, bias=False)\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.model.set_input_embeddings(value)\n+\n+    def get_output_embeddings(self) -> nn.Module:\n+        return self.lm_head\n+\n+    def set_decoder(self, decoder):\n+        self.model.set_decoder(decoder)\n+\n+    def get_decoder(self):\n+        return self.model.get_decoder()\n+\n+    def get_image_features(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        image_num_patches: torch.Tensor,\n+    ):\n+        return self.model.get_image_features(\n+            pixel_values=pixel_values,\n+            image_num_patches=image_num_patches,\n+        )\n+\n+    # Make modules available throught conditional class for BC\n+    @property\n+    def language_model(self):\n+        return self.model.language_model\n+\n+    @property\n+    def vision_tower(self):\n+        return self.model.vision_tower\n+\n+    @property\n+    def multi_modal_projector(self):\n+        return self.model.multi_modal_projector\n+\n+    @check_model_inputs\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        image_num_patches: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        image_sizes: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[tuple, Cohere2VisionCausalLMOutputWithPast]:\n+        r\"\"\"\n+        image_num_patches (`torch.Tensor` of shape `(num_images,)`):\n+            Number of patches per input image.\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+\n+        Example:\n+\n+        ```python\n+        >>> from transformers import AutoProcessor, Cohere2VisionForConditionalGeneration\n+        >>> import torch\n+\n+        >>> processor = AutoProcessor.from_pretrained(\"CohereLabs/command-a-vision-07-2025\", use_fast=True)\n+        >>> model = Cohere2VisionForConditionalGeneration.from_pretrained(\"CohereLabs/command-a-vision-07-2025\", device_map=\"auto\")\n+\n+        >>> messages = [\n+        ...     {\n+        ...         \"role\": \"user\",\n+        ...         \"content\": [\n+        ...             {\n+        ...                 \"type\": \"image\",\n+        ...                 \"url\": \"https://images.pexels.com/photos/1108099/pexels-photo-1108099.jpeg\",\n+        ...             },\n+        ...             {\"type\": \"text\", \"text\": \"what is in this image?\"},\n+        ...         ],\n+        ...     },\n+        ... ]\n+\n+        >>> inputs = processor.apply_chat_template(\n+        ...     messages, padding=True, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\",\n+        ... ).to(model.device)\n+\n+        >>> gen_tokens = model.generate(**inputs, max_new_tokens=300, do_sample=True, temperature=0.3)\n+        >>> processor.tokenizer.decode(gen_tokens[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n+        ```\"\"\"\n+        outputs = self.model(\n+            input_ids=input_ids,\n+            pixel_values=pixel_values,\n+            image_num_patches=image_num_patches,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            image_sizes=image_sizes,\n+            **kwargs,\n+        )\n+\n+        hidden_states = outputs[0]\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(\n+                logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size, **kwargs\n+            )\n+\n+        return Cohere2VisionCausalLMOutputWithPast(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            image_hidden_states=outputs.image_hidden_states,\n+        )\n+\n+    def prepare_inputs_for_generation(\n+        self,\n+        input_ids,\n+        past_key_values=None,\n+        inputs_embeds=None,\n+        pixel_values=None,\n+        attention_mask=None,\n+        cache_position=None,\n+        logits_to_keep=None,\n+        **kwargs,\n+    ):\n+        # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n+\n+        model_inputs = super().prepare_inputs_for_generation(\n+            input_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            logits_to_keep=logits_to_keep,\n+            **kwargs,\n+        )\n+\n+        if cache_position[0] == 0:\n+            # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n+            # Otherwise we need pixel values to be passed to model\n+            model_inputs[\"pixel_values\"] = pixel_values\n+\n+        return model_inputs\n+\n+\n+__all__ = [\"Cohere2VisionForConditionalGeneration\", \"Cohere2VisionPreTrainedModel\", \"Cohere2VisionModel\"]"
        },
        {
            "sha": "90cf7defe7ce836b4a588598fb6cde24210e59af",
            "filename": "src/transformers/models/cohere2_vision/modular_cohere2_vision.py",
            "status": "added",
            "additions": 351,
            "deletions": 0,
            "changes": 351,
            "blob_url": "https://github.com/huggingface/transformers/blob/e1688d28d3cce0b3e4c59e99fb970801970231c1/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fmodular_cohere2_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e1688d28d3cce0b3e4c59e99fb970801970231c1/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fmodular_cohere2_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fmodular_cohere2_vision.py?ref=e1688d28d3cce0b3e4c59e99fb970801970231c1",
            "patch": "@@ -0,0 +1,351 @@\n+# coding=utf-8\n+# Copyright 2025 the Cohere Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"PyTorch AyaVision model.\"\"\"\n+\n+from functools import lru_cache\n+from typing import Optional, Union\n+\n+import numpy as np\n+import torch\n+from torch import nn\n+\n+from transformers.models.aya_vision.modeling_aya_vision import (\n+    AyaVisionCausalLMOutputWithPast,\n+    AyaVisionForConditionalGeneration,\n+    AyaVisionModel,\n+    AyaVisionModelOutputWithPast,\n+)\n+from transformers.models.got_ocr2.image_processing_got_ocr2_fast import GotOcr2ImageProcessorFast\n+\n+from ...cache_utils import Cache\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...processing_utils import Unpack\n+from ...utils import (\n+    TransformersKwargs,\n+    auto_docstring,\n+    logging,\n+)\n+from ...utils.generic import check_model_inputs\n+from .configuration_cohere2_vision import Cohere2VisionConfig\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class Cohere2VisionMultiModalProjector(nn.Module):\n+    def __init__(self, config: Cohere2VisionConfig):\n+        super().__init__()\n+        self.config = config\n+        self.downsample_factor = config.downsample_factor\n+        self.intermediate_size = config.alignment_intermediate_size\n+        self.linear_1 = nn.Linear(\n+            config.vision_config.hidden_size * (config.downsample_factor**2), self.intermediate_size, bias=True\n+        )\n+        self.act = nn.SiLU()\n+        self.linear_2 = nn.Linear(self.intermediate_size // 2, config.text_config.hidden_size, bias=True)\n+\n+    def pixel_shuffle(self, image_features):  # B, S, D\n+        batch_size, seq_length, feature_dim = image_features.shape\n+        height = width = int(seq_length**0.5)\n+        image_features = image_features.reshape(image_features.shape[0], width, height, -1)\n+        channels = image_features.shape[-1]\n+        image_features = image_features.reshape(\n+            batch_size, width, int(height / self.downsample_factor), int(channels * self.downsample_factor)\n+        )\n+        image_features = image_features.permute(0, 2, 1, 3)\n+        image_features = image_features.reshape(\n+            batch_size, int(height / self.downsample_factor), int(width / self.downsample_factor), -1\n+        )\n+        image_features = image_features.permute(0, 2, 1, 3)\n+        return image_features\n+\n+    def forward(self, image_features):\n+        image_features = self.pixel_shuffle(image_features)\n+        hidden_states = self.linear_1(image_features)\n+\n+        # Split along last dimension and apply SwiGLU\n+        x, gate = hidden_states.chunk(2, dim=-1)\n+        hidden_states = self.act(gate) * x\n+\n+        hidden_states = self.linear_2(hidden_states)\n+        return hidden_states\n+\n+\n+class Cohere2VisionModelOutputWithPast(AyaVisionModelOutputWithPast):\n+    pass\n+\n+\n+class Cohere2VisionCausalLMOutputWithPast(AyaVisionCausalLMOutputWithPast):\n+    pass\n+\n+\n+class Cohere2VisionModel(AyaVisionModel):\n+    _checkpoint_conversion_mapping = {}\n+\n+    def get_image_features(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        image_num_patches: torch.Tensor,\n+    ):\n+        \"\"\"\n+        Obtains image last hidden states from the vision tower and apply multimodal projection.\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor]` of shape `(batch_size, num_patches, channels, height, width)`)\n+               The tensors corresponding to the input images.\n+            image_num_patches (`torch.Tensor` of shape `(num_images)`)\n+                Number of patches for each image.\n+        Returns:\n+            image_features (List[`torch.Tensor`]): List of image feature tensor, each contains all the visual feature of all patches\n+            and are of shape `(num_patches, image_length, embed_dim)`).\n+        \"\"\"\n+\n+        image_features = self.vision_tower(pixel_values, output_hidden_states=True)\n+        selected_image_feature = image_features.last_hidden_state\n+        image_features = self.multi_modal_projector(selected_image_feature)\n+        return image_features\n+\n+    @check_model_inputs\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n+        image_num_patches: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> Union[tuple, Cohere2VisionModelOutputWithPast]:\n+        r\"\"\"\n+        image_num_patches (`torch.Tensor` of shape `(num_images,)`):\n+            Number of patches per input image.\n+        \"\"\"\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.get_input_embeddings()(input_ids)\n+\n+        if pixel_values is not None:\n+            image_features = self.get_image_features(pixel_values, image_num_patches=image_num_patches)\n+            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+\n+            if input_ids is None:\n+                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+                special_image_mask = special_image_mask.all(-1)\n+            else:\n+                special_image_mask = input_ids == self.config.image_token_id\n+\n+            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n+\n+        outputs = self.language_model(\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+        return Cohere2VisionModelOutputWithPast(\n+            last_hidden_state=outputs.last_hidden_state,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            image_hidden_states=image_features if pixel_values is not None else None,\n+        )\n+\n+\n+class Cohere2VisionForConditionalGeneration(AyaVisionForConditionalGeneration):\n+    _checkpoint_conversion_mapping = {}\n+\n+    def get_image_features(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        image_num_patches: torch.Tensor,\n+    ):\n+        return self.model.get_image_features(\n+            pixel_values=pixel_values,\n+            image_num_patches=image_num_patches,\n+        )\n+\n+    @check_model_inputs\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        image_num_patches: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        image_sizes: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[tuple, Cohere2VisionCausalLMOutputWithPast]:\n+        r\"\"\"\n+        image_num_patches (`torch.Tensor` of shape `(num_images,)`):\n+            Number of patches per input image.\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+\n+        Example:\n+\n+        ```python\n+        >>> from transformers import AutoProcessor, Cohere2VisionForConditionalGeneration\n+        >>> import torch\n+\n+        >>> processor = AutoProcessor.from_pretrained(\"CohereLabs/command-a-vision-07-2025\", use_fast=True)\n+        >>> model = Cohere2VisionForConditionalGeneration.from_pretrained(\"CohereLabs/command-a-vision-07-2025\", device_map=\"auto\")\n+\n+        >>> messages = [\n+        ...     {\n+        ...         \"role\": \"user\",\n+        ...         \"content\": [\n+        ...             {\n+        ...                 \"type\": \"image\",\n+        ...                 \"url\": \"https://images.pexels.com/photos/1108099/pexels-photo-1108099.jpeg\",\n+        ...             },\n+        ...             {\"type\": \"text\", \"text\": \"what is in this image?\"},\n+        ...         ],\n+        ...     },\n+        ... ]\n+\n+        >>> inputs = processor.apply_chat_template(\n+        ...     messages, padding=True, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\",\n+        ... ).to(model.device)\n+\n+        >>> gen_tokens = model.generate(**inputs, max_new_tokens=300, do_sample=True, temperature=0.3)\n+        >>> processor.tokenizer.decode(gen_tokens[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n+        ```\"\"\"\n+        outputs = self.model(\n+            input_ids=input_ids,\n+            pixel_values=pixel_values,\n+            image_num_patches=image_num_patches,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            image_sizes=image_sizes,\n+            **kwargs,\n+        )\n+\n+        hidden_states = outputs[0]\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(\n+                logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size, **kwargs\n+            )\n+\n+        return Cohere2VisionCausalLMOutputWithPast(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            image_hidden_states=outputs.image_hidden_states,\n+        )\n+\n+\n+@lru_cache(maxsize=10)\n+def get_all_supported_aspect_ratios(max_image_tiles: int) -> list[tuple[int, int]]:\n+    \"\"\"\n+    Computes all allowed aspect ratios for a given maximum number of input tiles.\n+\n+    This function calculates all possible arrangements of tiles that can be formed\n+    within the constraint of the maximum number of tiles. Each arrangement is\n+    represented by its aspect ratio (width/height) and the corresponding tile configuration.\n+\n+    Args:\n+        max_image_tiles (`int`):\n+            The maximum number of tiles allowed.\n+\n+    Returns:\n+        `list[tuple[int, int]]`: A list of tuples, each tuple representing a valid (width, height)\n+        configuration in terms of number of tiles.\n+\n+    Example:\n+        >>> get_all_supported_aspect_ratios(4)\n+        [(1, 1), (1, 2), (1, 3), (1, 4), (2, 1), (2, 2), (3, 1), (4, 1)]\n+\n+    \"\"\"\n+    aspect_ratios = []\n+    for width in range(1, max_image_tiles + 1):\n+        for height in range(1, max_image_tiles + 1):\n+            if width * height <= max_image_tiles:\n+                aspect_ratios.append((width, height))\n+    return aspect_ratios\n+\n+\n+def get_optimal_tiled_canvas(\n+    original_image_size: tuple[int, int],\n+    target_tile_size: tuple[int, int],\n+    min_image_tiles: int,\n+    max_image_tiles: int,\n+) -> tuple[int, int]:\n+    possible_resolutions = get_all_supported_aspect_ratios(max_image_tiles)\n+    possible_resolutions = sorted(possible_resolutions, key=lambda x: x[0] * x[1])\n+    image_height, image_width = original_image_size\n+    patch_size_height, patch_size_width = target_tile_size  # (height == width)\n+\n+    candidate_resolutions = np.array(possible_resolutions) * patch_size_height\n+    original_size = np.stack([image_height, image_width])\n+    required_scales = candidate_resolutions / original_size\n+    required_scale = np.min(required_scales, axis=-1, keepdims=True)  # [n_resolutions, 1]\n+    if np.all(required_scale < 1):\n+        # We are forced to downscale, so try to minimize the amount of downscaling\n+        best_grid = possible_resolutions[np.argmax(required_scale)]\n+    else:\n+        # Pick the resolution that required the least upscaling so that it most closely fits the image\n+        required_scale = np.where(required_scale < 1.0, 10e9, required_scale)\n+        best_grid = possible_resolutions[np.argmin(required_scale)]\n+    return best_grid\n+\n+\n+@auto_docstring\n+class Cohere2VisionImageProcessorFast(GotOcr2ImageProcessorFast):\n+    size = {\"height\": 512, \"width\": 512}\n+    min_patches = 1\n+    max_patches = 12\n+    crop_to_patches = True\n+    patch_size = 16\n+\n+\n+__all__ = [\n+    \"Cohere2VisionForConditionalGeneration\",\n+    \"Cohere2VisionPreTrainedModel\",  # noqa: F822\n+    \"Cohere2VisionModel\",\n+    \"Cohere2VisionImageProcessorFast\",\n+]"
        },
        {
            "sha": "b72e1512ead9612c9e601101326f532ed3bf0d1a",
            "filename": "src/transformers/models/cohere2_vision/processing_cohere2_vision.py",
            "status": "added",
            "additions": 216,
            "deletions": 0,
            "changes": 216,
            "blob_url": "https://github.com/huggingface/transformers/blob/e1688d28d3cce0b3e4c59e99fb970801970231c1/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fprocessing_cohere2_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e1688d28d3cce0b3e4c59e99fb970801970231c1/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fprocessing_cohere2_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fprocessing_cohere2_vision.py?ref=e1688d28d3cce0b3e4c59e99fb970801970231c1",
            "patch": "@@ -0,0 +1,216 @@\n+# coding=utf-8\n+# Copyright 2025 HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import Optional, Union\n+\n+import numpy as np\n+\n+from ...image_processing_utils import BatchFeature\n+from ...image_utils import ImageInput\n+from ...processing_utils import ImagesKwargs, MultiModalData, ProcessingKwargs, ProcessorMixin, Unpack\n+from ...tokenization_utils_base import PreTokenizedInput, TextInput\n+\n+\n+class Cohere2VisionImagesKwargs(ImagesKwargs, total=False):\n+    max_patches: Optional[int]\n+\n+\n+class Cohere2VisionProcessorKwargs(ProcessingKwargs, total=False):\n+    images_kwargs: Cohere2VisionImagesKwargs\n+    _defaults = {\n+        \"text_kwargs\": {\n+            \"padding_side\": \"left\",\n+            \"padding\": True,\n+            \"return_mm_token_type_ids\": False,\n+        },\n+    }\n+\n+\n+class Cohere2VisionProcessor(ProcessorMixin):\n+    r\"\"\"\n+    Constructs a Cohere2Vision processor which wraps a [`AutoImageProcessor`] and\n+    [`PretrainedTokenizerFast`] tokenizer into a single processor that inherits both the image processor and\n+    tokenizer functionalities. See the [`~Cohere2VisionProcessor.__call__`] and [`~Cohere2VisionProcessor.decode`] for more information.\n+    Args:\n+        image_processor ([`AutoImageProcessor`], *optional*):\n+            The image processor is a required input.\n+        tokenizer ([`PreTrainedTokenizer`, `PreTrainedTokenizerFast`], *optional*):\n+            The tokenizer is a required input.\n+        chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n+            in a chat into a tokenizable string.\n+    \"\"\"\n+\n+    attributes = [\"image_processor\", \"tokenizer\"]\n+    image_processor_class = \"AutoImageProcessor\"\n+    tokenizer_class = \"AutoTokenizer\"\n+\n+    def __init__(\n+        self,\n+        image_processor=None,\n+        tokenizer=None,\n+        chat_template=None,\n+        **kwargs,\n+    ):\n+        super().__init__(image_processor, tokenizer, chat_template=chat_template)\n+\n+        self.patch_size = self.image_processor.patch_size\n+        self.boi_token = tokenizer.boi_token\n+        self.eoi_token = tokenizer.eoi_token\n+        self.image_token = tokenizer.image_token\n+        self.img_line_break_token = tokenizer.img_line_break_token\n+        self.image_token_id = tokenizer.image_token_id\n+\n+        self.image_ids = tokenizer.convert_tokens_to_ids(\n+            [\n+                self.image_token,\n+                self.boi_token,\n+                self.eoi_token,\n+                self.img_line_break_token,\n+            ]\n+        )\n+\n+    def __call__(\n+        self,\n+        images: Optional[ImageInput] = None,\n+        text: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]] = None,\n+        **kwargs: Unpack[Cohere2VisionProcessorKwargs],\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n+        and `kwargs` arguments to PreTrainedTokenizerFast's [`~PreTrainedTokenizerFast.__call__`] to encode the text.\n+        To prepare the vision inputs, this method forwards the `images` and `kwargs` arguments to\n+        GotOcr2ImageProcessor's [`~GotOcr2ImageProcessor.__call__`] if `images` is not `None`.\n+\n+        Args:\n+            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):\n+                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n+                tensor. Both channels-first and channels-last formats are supported.\n+            text (`str`, `list[str]`, `list[list[str]]`):\n+                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n+                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n+                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n+            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n+                If set, will return tensors of a particular framework. Acceptable values are:\n+                - `'tf'`: Return TensorFlow `tf.constant` objects.\n+                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n+                - `'np'`: Return NumPy `np.ndarray` objects.\n+                - `'jax'`: Return JAX `jnp.ndarray` objects.\n+\n+        Returns:\n+            [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n+\n+            - **input_ids** -- List of token ids to be fed to a model. Returned when `text` is not `None`.\n+            - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n+              `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names` and if `text` is not\n+              `None`).\n+            - **pixel_values** -- Pixel values to be fed to a model. Returned when `images` is not `None`.\n+        \"\"\"\n+        if text is None:\n+            raise ValueError(\"You have to specify text.\")\n+        elif not isinstance(text, (list, tuple)):\n+            text = [text]\n+\n+        output_kwargs = self._merge_kwargs(\n+            Cohere2VisionProcessorKwargs,\n+            tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n+            **kwargs,\n+        )\n+\n+        # Process images\n+        image_inputs = {}\n+        if images is not None:\n+            image_inputs = self.image_processor(images=images, **output_kwargs[\"images_kwargs\"])\n+            batch_num_patches = iter(image_inputs.pop(\"num_patches\"))\n+            processed_text = []\n+            for sample in text:\n+                while self.image_token in sample:\n+                    num_patches = next(batch_num_patches)\n+                    img_patches_per_tile = int(self.patch_size**2)\n+\n+                    img_string = f\"{self.boi_token}\"\n+                    for idx in range(1, num_patches):\n+                        img_string += \"<placeholder>\" * img_patches_per_tile + self.img_line_break_token\n+                    img_string += \"<placeholder>\" * img_patches_per_tile + self.img_line_break_token\n+                    img_string += f\"{self.eoi_token}\"\n+\n+                    sample = sample.replace(self.image_token, img_string, 1)\n+                processed_text.append(sample)\n+            text = [sample.replace(\"<placeholder>\", self.image_token) for sample in processed_text]\n+\n+        return_tensors = output_kwargs[\"text_kwargs\"].pop(\"return_tensors\", None)\n+        return_mm_token_type_ids = output_kwargs[\"text_kwargs\"].pop(\"return_mm_token_type_ids\", False)\n+        text_inputs = self.tokenizer(text, **output_kwargs[\"text_kwargs\"], return_tensors=None)\n+\n+        if return_mm_token_type_ids:\n+            array_ids = np.array(text_inputs[\"input_ids\"])\n+            mm_token_type_ids = np.zeros_like(text_inputs[\"input_ids\"])\n+            mm_token_type_ids[np.isin(array_ids, self.image_ids)] = 1\n+            text_inputs[\"mm_token_type_ids\"] = mm_token_type_ids.tolist()\n+\n+        return BatchFeature(data={**text_inputs, **image_inputs}, tensor_type=return_tensors)\n+\n+    def _get_num_multimodal_tokens(self, image_sizes=None, **kwargs):\n+        \"\"\"\n+        Computes the number of placeholder tokens needed for multimodal inputs with the given sizes.\n+\n+        Args:\n+            image_sizes (`list[list[int]]`, *optional*):\n+                The input sizes formatted as (height, width) per each image.\n+\n+        Returns:\n+            `MultiModalData`: A `MultiModalData` object holding number of tokens per each of the provided\n+            input modalities, along with other useful data.\n+        \"\"\"\n+\n+        vision_data = {}\n+        if image_sizes is not None:\n+            images_kwargs = Cohere2VisionProcessorKwargs._defaults.get(\"images_kwargs\", {})\n+            images_kwargs.update(kwargs)\n+\n+            num_image_patches = [\n+                self.image_processor.get_number_of_image_patches(*image_size, images_kwargs)\n+                for image_size in image_sizes\n+            ]\n+\n+            token_per_patch = int(self.patch_size**2)\n+            num_image_tokens = [\n+                2 + sum(token_per_patch + 1 for _ in range(num_patches)) for num_patches in num_image_patches\n+            ]  # Add +2 and +1 for BOI/EOI and image break tokens\n+            vision_data.update({\"num_image_tokens\": num_image_tokens, \"num_image_patches\": num_image_patches})\n+\n+        return MultiModalData(**vision_data)\n+\n+    def batch_decode(self, *args, **kwargs):\n+        \"\"\"\n+        This method forwards all its arguments to PreTrainedTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n+        refer to the docstring of this method for more information.\n+        \"\"\"\n+        return self.tokenizer.batch_decode(*args, **kwargs)\n+\n+    def decode(self, *args, **kwargs):\n+        \"\"\"\n+        This method forwards all its arguments to PreTrainedTokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n+        the docstring of this method for more information.\n+        \"\"\"\n+        return self.tokenizer.decode(*args, **kwargs)\n+\n+    @property\n+    def model_input_names(self):\n+        tokenizer_input_names = self.tokenizer.model_input_names\n+        image_processor_input_names = self.image_processor.model_input_names\n+        return list(tokenizer_input_names) + list(image_processor_input_names)\n+\n+\n+__all__ = [\"Cohere2VisionProcessor\"]"
        },
        {
            "sha": "8d087ef76282b1e9600dcd495d720d1bfafee377",
            "filename": "src/transformers/models/got_ocr2/image_processing_got_ocr2_fast.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/e1688d28d3cce0b3e4c59e99fb970801970231c1/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e1688d28d3cce0b3e4c59e99fb970801970231c1/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2_fast.py?ref=e1688d28d3cce0b3e4c59e99fb970801970231c1",
            "patch": "@@ -45,7 +45,7 @@\n         from torchvision.transforms import functional as F\n \n \n-class GotOcr2ImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n+class GotOcr2FastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n     \"\"\"\n     crop_to_patches (`bool`, *optional*, defaults to `False`):\n         Whether to crop the image to patches. Can be overridden by the `crop_to_patches` parameter in the\n@@ -76,13 +76,13 @@ class GotOcr2ImageProcessorFast(BaseImageProcessorFast):\n     crop_to_patches = False\n     min_patches = 1\n     max_patches = 12\n-    valid_kwargs = GotOcr2ImageProcessorKwargs\n+    valid_kwargs = GotOcr2FastImageProcessorKwargs\n \n-    def __init__(self, **kwargs: Unpack[GotOcr2ImageProcessorKwargs]):\n+    def __init__(self, **kwargs: Unpack[GotOcr2FastImageProcessorKwargs]):\n         super().__init__(**kwargs)\n \n     @auto_docstring\n-    def preprocess(self, images: ImageInput, **kwargs: Unpack[GotOcr2ImageProcessorKwargs]) -> BatchFeature:\n+    def preprocess(self, images: ImageInput, **kwargs: Unpack[GotOcr2FastImageProcessorKwargs]) -> BatchFeature:\n         return super().preprocess(images, **kwargs)\n \n     def crop_image_to_patches("
        },
        {
            "sha": "0d55eb3abc9f71bde436759440973e0bd77eeed8",
            "filename": "src/transformers/models/got_ocr2/modeling_got_ocr2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e1688d28d3cce0b3e4c59e99fb970801970231c1/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e1688d28d3cce0b3e4c59e99fb970801970231c1/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py?ref=e1688d28d3cce0b3e4c59e99fb970801970231c1",
            "patch": "@@ -678,7 +678,7 @@ def set_decoder(self, decoder):\n         self.model.set_decoder(decoder)\n \n     def get_decoder(self):\n-        return self.model.get_decoder\n+        return self.model.get_decoder()\n \n     def get_image_features(\n         self,"
        },
        {
            "sha": "8e3963cfabc6b6cb147b8e1e3f389a841eb46053",
            "filename": "src/transformers/models/internvl/modeling_internvl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e1688d28d3cce0b3e4c59e99fb970801970231c1/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e1688d28d3cce0b3e4c59e99fb970801970231c1/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py?ref=e1688d28d3cce0b3e4c59e99fb970801970231c1",
            "patch": "@@ -824,7 +824,7 @@ def set_decoder(self, decoder):\n         self.model.set_decoder(decoder)\n \n     def get_decoder(self):\n-        return self.model.get_decoder\n+        return self.model.get_decoder()\n \n     def get_image_features(\n         self,"
        },
        {
            "sha": "e5145554dc4b6aa45e4419516f353b6c2b7ce6e9",
            "filename": "src/transformers/models/llava/modeling_llava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e1688d28d3cce0b3e4c59e99fb970801970231c1/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e1688d28d3cce0b3e4c59e99fb970801970231c1/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py?ref=e1688d28d3cce0b3e4c59e99fb970801970231c1",
            "patch": "@@ -341,7 +341,7 @@ def set_decoder(self, decoder):\n         self.model.set_decoder(decoder)\n \n     def get_decoder(self):\n-        return self.model.get_decoder\n+        return self.model.get_decoder()\n \n     def get_image_features(\n         self,"
        },
        {
            "sha": "7e8dabef15d237d63ef40979e8a83c61014c5c58",
            "filename": "src/transformers/models/mistral3/modeling_mistral3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e1688d28d3cce0b3e4c59e99fb970801970231c1/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e1688d28d3cce0b3e4c59e99fb970801970231c1/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py?ref=e1688d28d3cce0b3e4c59e99fb970801970231c1",
            "patch": "@@ -378,7 +378,7 @@ def set_decoder(self, decoder):\n         self.model.set_decoder(decoder)\n \n     def get_decoder(self):\n-        return self.model.get_decoder\n+        return self.model.get_decoder()\n \n     def get_image_features(\n         self,"
        },
        {
            "sha": "cb99ca8e1966fccfe3c22ad4c8bb0df234543411",
            "filename": "src/transformers/models/perception_lm/modeling_perception_lm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e1688d28d3cce0b3e4c59e99fb970801970231c1/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodeling_perception_lm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e1688d28d3cce0b3e4c59e99fb970801970231c1/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodeling_perception_lm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodeling_perception_lm.py?ref=e1688d28d3cce0b3e4c59e99fb970801970231c1",
            "patch": "@@ -316,7 +316,7 @@ def set_decoder(self, decoder):\n         self.model.set_decoder(decoder)\n \n     def get_decoder(self):\n-        return self.model.get_decoder\n+        return self.model.get_decoder()\n \n     @can_return_tuple\n     @auto_docstring"
        },
        {
            "sha": "3c9d5cd0eeac0fd596f65c16cca0f1352c9b488e",
            "filename": "src/transformers/models/vipllava/modeling_vipllava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e1688d28d3cce0b3e4c59e99fb970801970231c1/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e1688d28d3cce0b3e4c59e99fb970801970231c1/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py?ref=e1688d28d3cce0b3e4c59e99fb970801970231c1",
            "patch": "@@ -303,7 +303,7 @@ def set_decoder(self, decoder):\n         self.model.set_decoder(decoder)\n \n     def get_decoder(self):\n-        return self.model.get_decoder\n+        return self.model.get_decoder()\n \n     def get_image_features(\n         self, pixel_values: torch.FloatTensor, vision_feature_layers: Optional[Union[int, list[int]]] = None"
        },
        {
            "sha": "ea3942a08f7570ed5452fe3a635eb74b95dbc376",
            "filename": "src/transformers/utils/generic.py",
            "status": "modified",
            "additions": 9,
            "deletions": 2,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/e1688d28d3cce0b3e4c59e99fb970801970231c1/src%2Ftransformers%2Futils%2Fgeneric.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e1688d28d3cce0b3e4c59e99fb970801970231c1/src%2Ftransformers%2Futils%2Fgeneric.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fgeneric.py?ref=e1688d28d3cce0b3e4c59e99fb970801970231c1",
            "patch": "@@ -974,11 +974,13 @@ class OutputRecorder:\n         target_class (Type): The class (e.g., nn.Module) to which the hook will be attached.\n         index (Optional[int]): If the output is a tuple/list, optionally record only at a specific index.\n         layer_name (Optional[str]): Name of the submodule to target (if needed), e.g., \"transformer.layer.3.attn\".\n+        class_name (Optional[str]): Name of the class to which the hook will be attached. Could be the suffix of class name in some cases.\n     \"\"\"\n \n     target_class: \"type[torch.nn.Module]\"\n     index: Optional[int] = 0\n     layer_name: Optional[str] = None\n+    class_name: Optional[str] = None\n \n \n def check_model_inputs(func):\n@@ -1049,12 +1051,17 @@ def wrapped_forward(*args, **kwargs):\n                 for specs in layer_specs:\n                     if not isinstance(specs, OutputRecorder):\n                         index = 0 if \"hidden_states\" in key else 1\n-                        specs = OutputRecorder(target_class=specs, index=index)\n+                        class_name = None if not isinstance(specs, str) else specs\n+                        target_class = specs if not isinstance(specs, str) else None\n+                        specs = OutputRecorder(target_class=target_class, index=index, class_name=class_name)\n                     capture_tasks.append((key, specs))\n \n             for name, module in self.named_modules():\n                 for key, specs in capture_tasks:\n-                    if isinstance(module, specs.target_class):\n+                    # The second check is for multimodals where only backbone layer suffix is available\n+                    if (specs.target_class is not None and isinstance(module, specs.target_class)) or (\n+                        specs.class_name is not None and name.endswith(specs.class_name)\n+                    ):\n                         if specs.layer_name is not None and specs.layer_name not in name:\n                             continue\n                         # Monkey patch forward"
        },
        {
            "sha": "e193afc5136137aebee22ced9b0b5e9504e1d5b1",
            "filename": "src/transformers/utils/hub.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/e1688d28d3cce0b3e4c59e99fb970801970231c1/src%2Ftransformers%2Futils%2Fhub.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e1688d28d3cce0b3e4c59e99fb970801970231c1/src%2Ftransformers%2Futils%2Fhub.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fhub.py?ref=e1688d28d3cce0b3e4c59e99fb970801970231c1",
            "patch": "@@ -167,7 +167,10 @@ def list_repo_templates(\n             return [\n                 entry.path.removeprefix(f\"{CHAT_TEMPLATE_DIR}/\")\n                 for entry in list_repo_tree(\n-                    repo_id=repo_id, revision=revision, path_in_repo=CHAT_TEMPLATE_DIR, recursive=False\n+                    repo_id=repo_id,\n+                    revision=revision,\n+                    path_in_repo=CHAT_TEMPLATE_DIR,\n+                    recursive=False,\n                 )\n                 if entry.path.endswith(\".jinja\")\n             ]"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/cohere2_vision/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/e1688d28d3cce0b3e4c59e99fb970801970231c1/tests%2Fmodels%2Fcohere2_vision%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e1688d28d3cce0b3e4c59e99fb970801970231c1/tests%2Fmodels%2Fcohere2_vision%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcohere2_vision%2F__init__.py?ref=e1688d28d3cce0b3e4c59e99fb970801970231c1"
        },
        {
            "sha": "7ab3bf70d57b2ddd05f5dace2f93b2c241d9fd06",
            "filename": "tests/models/cohere2_vision/test_image_processing_cohere2_vision.py",
            "status": "added",
            "additions": 192,
            "deletions": 0,
            "changes": 192,
            "blob_url": "https://github.com/huggingface/transformers/blob/e1688d28d3cce0b3e4c59e99fb970801970231c1/tests%2Fmodels%2Fcohere2_vision%2Ftest_image_processing_cohere2_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e1688d28d3cce0b3e4c59e99fb970801970231c1/tests%2Fmodels%2Fcohere2_vision%2Ftest_image_processing_cohere2_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcohere2_vision%2Ftest_image_processing_cohere2_vision.py?ref=e1688d28d3cce0b3e4c59e99fb970801970231c1",
            "patch": "@@ -0,0 +1,192 @@\n+# Copyright 2025 HuggingFace Inc.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import unittest\n+\n+import numpy as np\n+\n+from transformers.testing_utils import require_torch, require_vision\n+from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n+\n+from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+if is_vision_available():\n+    from PIL import Image\n+\n+    if is_torchvision_available():\n+        from transformers import Cohere2VisionImageProcessorFast\n+\n+\n+class Cohere2VisionImageProcessingTester(unittest.TestCase):\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=7,\n+        num_channels=3,\n+        image_size=18,\n+        min_resolution=30,\n+        max_resolution=400,\n+        do_resize=True,\n+        size=None,\n+        do_normalize=True,\n+        image_mean=[0.48145466, 0.4578275, 0.40821073],\n+        image_std=[0.26862954, 0.26130258, 0.27577711],\n+        do_convert_rgb=True,\n+    ):\n+        super().__init__()\n+        size = size if size is not None else {\"height\": 30, \"width\": 30}\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.num_channels = num_channels\n+        self.image_size = image_size\n+        self.min_resolution = min_resolution\n+        self.max_resolution = max_resolution\n+        self.do_resize = do_resize\n+        self.size = size\n+        self.do_normalize = do_normalize\n+        self.image_mean = image_mean\n+        self.image_std = image_std\n+        self.do_convert_rgb = do_convert_rgb\n+\n+    def prepare_image_processor_dict(self):\n+        return {\n+            \"do_resize\": self.do_resize,\n+            \"size\": self.size,\n+            \"do_normalize\": self.do_normalize,\n+            \"image_mean\": self.image_mean,\n+            \"image_std\": self.image_std,\n+            \"do_convert_rgb\": self.do_convert_rgb,\n+        }\n+\n+    def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=False):\n+        return prepare_image_inputs(\n+            batch_size=self.batch_size,\n+            num_channels=self.num_channels,\n+            min_resolution=self.min_resolution,\n+            max_resolution=self.max_resolution,\n+            equal_resolution=equal_resolution,\n+            numpify=numpify,\n+            torchify=torchify,\n+        )\n+\n+\n+@require_torch\n+@require_vision\n+class Cohere2VisionProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n+    fast_image_processing_class = Cohere2VisionImageProcessorFast if is_torchvision_available() else None\n+    test_slow_image_processor = False\n+\n+    def setUp(self):\n+        super().setUp()\n+        self.image_processor_tester = Cohere2VisionImageProcessingTester(self)\n+\n+    @property\n+    def image_processor_dict(self):\n+        return self.image_processor_tester.prepare_image_processor_dict()\n+\n+    def test_image_processor_properties(self):\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class(**self.image_processor_dict)\n+            self.assertTrue(hasattr(image_processor, \"do_resize\"))\n+            self.assertTrue(hasattr(image_processor, \"size\"))\n+            self.assertTrue(hasattr(image_processor, \"do_normalize\"))\n+            self.assertTrue(hasattr(image_processor, \"image_mean\"))\n+            self.assertTrue(hasattr(image_processor, \"image_std\"))\n+            self.assertTrue(hasattr(image_processor, \"do_convert_rgb\"))\n+\n+    def test_call_pil(self):\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processing\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            # create random PIL images\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=True)\n+            for image in image_inputs:\n+                self.assertIsInstance(image, Image.Image)\n+\n+            # Test not batched input\n+            encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").pixel_values\n+            self.assertEqual(tuple(encoded_images.shape), (10, 3, 30, 30))\n+\n+            # Test batched\n+            encoded_images = image_processing(image_inputs, return_tensors=\"pt\").pixel_values\n+            self.assertEqual(tuple(encoded_images.shape), (70, 3, 30, 30))\n+\n+    def test_call_numpy(self):\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processing\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            # create random numpy tensors\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=True, numpify=True)\n+            for image in image_inputs:\n+                self.assertIsInstance(image, np.ndarray)\n+\n+            # Test not batched input\n+            encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").pixel_values\n+            self.assertEqual(tuple(encoded_images.shape), (10, 3, 30, 30))\n+\n+            # Test batched\n+            encoded_images = image_processing(image_inputs, return_tensors=\"pt\").pixel_values\n+            self.assertEqual(tuple(encoded_images.shape), (70, 3, 30, 30))\n+\n+    def test_call_pytorch(self):\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processing\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            # create random PyTorch tensors\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=True, torchify=True)\n+\n+            for image in image_inputs:\n+                self.assertIsInstance(image, torch.Tensor)\n+\n+            # Test not batched input\n+            encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").pixel_values\n+            self.assertEqual(tuple(encoded_images.shape), (10, 3, 30, 30))\n+\n+            # Test batched\n+            encoded_images = image_processing(image_inputs, return_tensors=\"pt\").pixel_values\n+            self.assertEqual(tuple(encoded_images.shape), (70, 3, 30, 30))\n+\n+    def test_call_numpy_4_channels(self):\n+        for image_processing_class in self.image_processor_list:\n+            # Test that can process images which have an arbitrary number of channels\n+            # Initialize image_processing\n+            image_processor = image_processing_class(**self.image_processor_dict)\n+\n+            # create random numpy tensors\n+            self.image_processor_tester.num_channels = 4\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=True, numpify=True)\n+\n+            # Test not batched input\n+            encoded_images = image_processor(\n+                image_inputs[0],\n+                return_tensors=\"pt\",\n+                input_data_format=\"channels_last\",\n+                image_mean=0,\n+                image_std=1,\n+            ).pixel_values\n+            self.assertEqual(tuple(encoded_images.shape), (10, 4, 30, 30))\n+\n+            # Test batched\n+            encoded_images = image_processor(\n+                image_inputs,\n+                return_tensors=\"pt\",\n+                input_data_format=\"channels_last\",\n+                image_mean=0,\n+                image_std=1,\n+            ).pixel_values\n+            self.assertEqual(tuple(encoded_images.shape), (70, 4, 30, 30))"
        },
        {
            "sha": "4e49baa3030a1583992b465f4d8d043d6357bb10",
            "filename": "tests/models/cohere2_vision/test_modeling_cohere2_vision.py",
            "status": "added",
            "additions": 470,
            "deletions": 0,
            "changes": 470,
            "blob_url": "https://github.com/huggingface/transformers/blob/e1688d28d3cce0b3e4c59e99fb970801970231c1/tests%2Fmodels%2Fcohere2_vision%2Ftest_modeling_cohere2_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e1688d28d3cce0b3e4c59e99fb970801970231c1/tests%2Fmodels%2Fcohere2_vision%2Ftest_modeling_cohere2_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcohere2_vision%2Ftest_modeling_cohere2_vision.py?ref=e1688d28d3cce0b3e4c59e99fb970801970231c1",
            "patch": "@@ -0,0 +1,470 @@\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch GotOcr2 model.\"\"\"\n+\n+import unittest\n+\n+from transformers import (\n+    AutoProcessor,\n+    Cohere2VisionConfig,\n+    is_torch_available,\n+)\n+from transformers.testing_utils import (\n+    Expectations,\n+    cleanup,\n+    get_device_properties,\n+    require_deterministic_for_xpu,\n+    require_read_token,\n+    require_torch,\n+    require_torch_accelerator,\n+    slow,\n+    torch_device,\n+)\n+\n+from ...generation.test_utils import GenerationTesterMixin\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor\n+from ...test_pipeline_mixin import PipelineTesterMixin\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+    from transformers import (\n+        Cohere2VisionForConditionalGeneration,\n+        Cohere2VisionModel,\n+    )\n+\n+\n+class Cohere2VisionText2TextModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=3,\n+        seq_length=7,\n+        downsample_factor=2,\n+        alignment_intermediate_size=32,\n+        ignore_index=-100,\n+        image_token_id=2,\n+        num_channels=3,\n+        image_size=64,\n+        is_training=True,\n+        text_config={\n+            \"model_type\": \"cohere2\",\n+            \"vocab_size\": 99,\n+            \"hidden_size\": 128,\n+            \"intermediate_size\": 37,\n+            \"num_hidden_layers\": 4,\n+            \"num_attention_heads\": 4,\n+            \"output_channels\": 64,\n+            \"hidden_act\": \"silu\",\n+            \"max_position_embeddings\": 512,\n+            \"tie_word_embeddings\": True,\n+            \"bos_token_id\": 0,\n+            \"eos_token_id\": 0,\n+            \"pad_token_id\": 0,\n+        },\n+        vision_config={\n+            \"model_type\": \"siglip_vision_model\",\n+            \"hidden_size\": 32,\n+            \"num_hidden_layers\": 2,\n+            \"num_attention_heads\": 4,\n+            \"intermediate_size\": 128,\n+            \"image_size\": 64,\n+            \"patch_size\": 8,\n+            \"vision_use_head\": False,\n+        },\n+    ):\n+        self.parent = parent\n+        self.ignore_index = ignore_index\n+        self.bos_token_id = text_config[\"bos_token_id\"]\n+        self.eos_token_id = text_config[\"eos_token_id\"]\n+        self.pad_token_id = text_config[\"pad_token_id\"]\n+        self.image_token_id = image_token_id\n+        self.text_config = text_config\n+        self.vision_config = vision_config\n+        self.batch_size = batch_size\n+        self.downsample_factor = downsample_factor\n+        self.alignment_intermediate_size = alignment_intermediate_size\n+        self.is_training = is_training\n+        self.num_channels = num_channels\n+        self.image_size = image_size\n+        self.image_seq_length = 16\n+        self.seq_length = seq_length + self.image_seq_length\n+\n+        self.num_hidden_layers = text_config[\"num_hidden_layers\"]\n+        self.vocab_size = text_config[\"vocab_size\"]\n+        self.hidden_size = text_config[\"hidden_size\"]\n+        self.num_attention_heads = text_config[\"num_attention_heads\"]\n+\n+    def get_config(self):\n+        return Cohere2VisionConfig(\n+            text_config=self.text_config,\n+            vision_config=self.vision_config,\n+            image_token_id=self.image_token_id,\n+            downsample_factor=self.downsample_factor,\n+            alignment_intermediate_size=self.alignment_intermediate_size,\n+        )\n+\n+    def prepare_config_and_inputs(self):\n+        config = self.get_config()\n+        pixel_values = floats_tensor([self.batch_size, self.num_channels, self.image_size, self.image_size])\n+        image_num_patches = torch.tensor([1] * self.batch_size).to(torch_device)\n+\n+        return config, pixel_values, image_num_patches\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        config, pixel_values, image_num_patches = config_and_inputs\n+        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n+        attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=torch_device)\n+        input_ids[input_ids == self.image_token_id] = self.pad_token_id\n+        input_ids[:, : self.image_seq_length] = self.image_token_id\n+\n+        inputs_dict = {\n+            \"pixel_values\": pixel_values,\n+            \"input_ids\": input_ids,\n+            \"attention_mask\": attention_mask,\n+            \"image_num_patches\": image_num_patches,\n+        }\n+        return config, inputs_dict\n+\n+\n+@require_torch\n+class Cohere2ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+    all_model_classes = (\n+        (\n+            Cohere2VisionModel,\n+            Cohere2VisionForConditionalGeneration,\n+        )\n+        if is_torch_available()\n+        else ()\n+    )\n+    all_generative_model_classes = (Cohere2VisionForConditionalGeneration,) if is_torch_available() else ()\n+    pipeline_model_mapping = (\n+        {\n+            \"image-text-to-text\": Cohere2VisionForConditionalGeneration,\n+        }\n+        if is_torch_available()\n+        else {}\n+    )\n+    fx_compatible = False\n+    test_pruning = False\n+    test_torchscript = False\n+    test_head_masking = False\n+    _is_composite = True\n+\n+    def setUp(self):\n+        self.model_tester = Cohere2VisionText2TextModelTester(self)\n+        self.config_tester = ConfigTester(self, config_class=Cohere2VisionConfig, has_text_modality=False)\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n+    @unittest.skip(reason=\"Siglip backbone uses the same initialization scheme as the Flax original implementation\")\n+    def test_initialization(self):\n+        pass\n+\n+\n+@require_read_token\n+@require_torch\n+class Cohere2IntegrationTest(unittest.TestCase):\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.model_checkpoint = \"CohereLabs/command-a-vision-07-2025\"\n+        cls.model = None\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        del cls.model\n+        cleanup(torch_device, gc_collect=True)\n+\n+    def tearDown(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n+    @classmethod\n+    def get_model(cls):\n+        # Use 4-bit on T4\n+        device_type, major, _ = get_device_properties()\n+        load_in_4bit = (device_type == \"cuda\") and (major < 8)\n+        torch_dtype = None if load_in_4bit else torch.float16\n+\n+        if cls.model is None:\n+            cls.model = Cohere2VisionForConditionalGeneration.from_pretrained(\n+                cls.model_checkpoint,\n+                device_map=\"auto\",\n+                torch_dtype=torch_dtype,\n+                load_in_4bit=load_in_4bit,\n+            )\n+        return cls.model\n+\n+    @slow\n+    @require_torch_accelerator\n+    def test_model_integration_forward(self):\n+        processor = AutoProcessor.from_pretrained(self.model_checkpoint)\n+        model = self.get_model()\n+        messages = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\"type\": \"image\", \"url\": \"http://images.cocodataset.org/val2017/000000039769.jpg\"},\n+                    {\"type\": \"text\", \"text\": \"Please describe the image explicitly.\"},\n+                ],\n+            }\n+        ]\n+\n+        inputs = processor.apply_chat_template(\n+            messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\"\n+        ).to(torch_device, dtype=torch.float16)\n+        # Forward\n+        with torch.inference_mode():\n+            output = model(**inputs)\n+\n+        actual_logits = output.logits[0, -1, :5].cpu()\n+\n+        EXPECTED_LOGITS = Expectations(\n+            {\n+                (\"xpu\", 3): [0.4109, 0.1532, 0.8018, 2.1328, 0.5483],\n+                # 4-bit\n+                (\"cuda\", 7): [0.1097, 0.3481, 3.8340, 9.7969, 2.0488],\n+                (\"cuda\", 8): [2.4277, 1.6875, 1.8789, 2.1875, 1.9375],\n+            }\n+        )  # fmt: skip\n+        expected_logits = torch.tensor(EXPECTED_LOGITS.get_expectation(), dtype=torch.float16)\n+\n+        self.assertTrue(\n+            torch.allclose(actual_logits, expected_logits, atol=0.1),\n+            f\"Actual logits: {actual_logits}\"\n+            f\"\\nExpected logits: {expected_logits}\"\n+            f\"\\nDifference: {torch.abs(actual_logits - expected_logits)}\",\n+        )\n+\n+    @slow\n+    @require_torch_accelerator\n+    @require_deterministic_for_xpu\n+    def test_model_integration_generate_text_only(self):\n+        processor = AutoProcessor.from_pretrained(self.model_checkpoint)\n+        model = self.get_model()\n+        messages = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\"type\": \"text\", \"text\": \"Write a haiku\"},\n+                ],\n+            }\n+        ]\n+\n+        inputs = processor.apply_chat_template(\n+            messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\"\n+        ).to(torch_device, dtype=torch.float16)\n+        with torch.no_grad():\n+            generate_ids = model.generate(**inputs, max_new_tokens=25, do_sample=False)\n+            decoded_output = processor.decode(\n+                generate_ids[0, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True\n+            )\n+\n+        expected_outputs = Expectations(\n+            {\n+                (\"xpu\", 3): \"Whispers on the breeze,\\nLeaves dance under moonlit skies,\\nNature's quiet song.\",\n+                # 4-bit\n+                (\"cuda\", 7): \"Sure, here's a haiku for you:\\n\\nMorning dew sparkles,\\nPetals unfold in sunlight,\\n\",\n+                (\"cuda\", 8): \"**Haiku**\\n\\n*Softly falls the snow*\\n*Blanketing the earth in white*\\n*\",\n+            }\n+        )  # fmt: skip\n+        expected_output = expected_outputs.get_expectation()\n+\n+        self.assertEqual(decoded_output, expected_output)\n+\n+    @slow\n+    @require_torch_accelerator\n+    @require_deterministic_for_xpu\n+    def test_model_integration_generate_chat_template(self):\n+        processor = AutoProcessor.from_pretrained(self.model_checkpoint)\n+        model = self.get_model()\n+        messages = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\"type\": \"image\", \"url\": \"http://images.cocodataset.org/val2017/000000039769.jpg\"},\n+                    {\"type\": \"text\", \"text\": \"Please describe the image explicitly.\"},\n+                ],\n+            }\n+        ]\n+\n+        inputs = processor.apply_chat_template(\n+            messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\"\n+        ).to(torch_device, dtype=torch.float16)\n+        with torch.no_grad():\n+            generate_ids = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n+            decoded_output = processor.decode(\n+                generate_ids[0, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True\n+            )\n+\n+        expected_outputs = Expectations(\n+            {\n+                (\"xpu\", 3): 'The image depicts a cozy scene of two cats resting on a bright pink blanket. The cats,',\n+                # 4-bit\n+                (\"cuda\", 7): 'The image depicts two cats comfortably resting on a pink blanket spread across a sofa. The cats,',\n+                (\"cuda\", 8): 'The image depicts two cats lying on a bright pink blanket that covers a red couch. The cat',\n+            }\n+        )  # fmt: skip\n+        expected_output = expected_outputs.get_expectation()\n+\n+        self.assertEqual(decoded_output, expected_output)\n+\n+    @slow\n+    @require_torch_accelerator\n+    def test_model_integration_batched_generate(self):\n+        processor = AutoProcessor.from_pretrained(self.model_checkpoint)\n+        model = self.get_model()\n+        # Prepare inputs\n+        messages = [\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\"type\": \"image\", \"url\": \"https://llava-vl.github.io/static/images/view.jpg\"},\n+                        {\"type\": \"text\", \"text\": \"Write a haiku for this image\"},\n+                    ],\n+                },\n+            ],\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"},\n+                        {\"type\": \"text\", \"text\": \"Describe this image\"},\n+                    ],\n+                },\n+            ],\n+        ]\n+        inputs = processor.apply_chat_template(\n+            messages, padding=True, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\"\n+        ).to(model.device, dtype=torch.float16)\n+\n+        output = model.generate(**inputs, do_sample=False, max_new_tokens=25)\n+\n+        # Check first output\n+        decoded_output = processor.decode(output[0, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True)\n+        expected_outputs = Expectations(\n+            {\n+                (\"xpu\", 3): \"Wooden path to water,\\nMountains echo in stillness,\\nPeaceful forest lake.\",\n+                # 4-bit\n+                (\"cuda\", 7): \"Wooden bridge stretches\\nMirrored lake below, mountains rise\\nPeaceful, serene\",\n+                (\"cuda\", 8): 'Dock stretches to calm,  \\nMountains whisper through the trees,  \\nLake mirrors the sky.',\n+            }\n+        )  # fmt: skip\n+        expected_output = expected_outputs.get_expectation()\n+\n+        self.assertEqual(\n+            decoded_output,\n+            expected_output,\n+            f\"Decoded output: {decoded_output}\\nExpected output: {expected_output}\",\n+        )\n+\n+        # Check second output\n+        decoded_output = processor.decode(output[1, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True)\n+\n+        expected_outputs = Expectations(\n+            {\n+                (\"xpu\", 3): 'This image captures a vibrant street scene in a bustling urban area, likely in an Asian city. The focal point is a',\n+                # 4-bit\n+                (\"cuda\", 7): 'This vibrant image captures a bustling street scene in a multicultural urban area, featuring a traditional Chinese gate adorned with intricate red and',\n+                (\"cuda\", 8): 'The image depicts a vibrant street scene in what appears to be a Chinatown district, likely in an urban area. The focal',\n+            }\n+        )  # fmt: skip\n+        expected_output = expected_outputs.get_expectation()\n+\n+        self.assertEqual(\n+            decoded_output,\n+            expected_output,\n+            f\"Decoded output: {decoded_output}\\nExpected output: {expected_output}\",\n+        )\n+\n+    @slow\n+    @require_torch_accelerator\n+    @require_deterministic_for_xpu\n+    def test_model_integration_batched_generate_multi_image(self):\n+        processor = AutoProcessor.from_pretrained(self.model_checkpoint)\n+        model = self.get_model()\n+        # Prepare inputs\n+        messages = [\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\"type\": \"image\", \"url\": \"https://llava-vl.github.io/static/images/view.jpg\"},\n+                        {\"type\": \"text\", \"text\": \"Write a haiku for this image\"},\n+                    ],\n+                },\n+            ],\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\n+                            \"type\": \"image\",\n+                            \"url\": \"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\",\n+                        },\n+                        {\n+                            \"type\": \"image\",\n+                            \"url\": \"https://thumbs.dreamstime.com/b/golden-gate-bridge-san-francisco-purple-flowers-california-echium-candicans-36805947.jpg\",\n+                        },\n+                        {\n+                            \"type\": \"text\",\n+                            \"text\": \"These images depict two different landmarks. Can you identify them?\",\n+                        },\n+                    ],\n+                },\n+            ],\n+        ]\n+        inputs = processor.apply_chat_template(\n+            messages, padding=True, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\"\n+        ).to(model.device, dtype=torch.float16)\n+        output = model.generate(**inputs, do_sample=False, max_new_tokens=25)\n+\n+        # Check first output\n+        decoded_output = processor.decode(output[0, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True)\n+        # Batching seems to alter the output slightly, but it is also the case in the original implementation. This seems to be expected: https://github.com/huggingface/transformers/issues/23017#issuecomment-1649630232\n+        expected_outputs = Expectations(\n+            {\n+                (\"xpu\", 3): \"Wooden path to water,\\nMountains echo in stillness,\\nPeaceful forest lake.\",\n+                (\"cuda\", 7): 'Wooden bridge stretches\\nMirrored lake below, mountains rise\\nPeaceful, serene',\n+                (\"cuda\", 8): 'Dock stretches to calm,  \\nMountains whisper through the trees,  \\nLake mirrors the sky.',\n+            }\n+        )  # fmt: skip\n+        expected_output = expected_outputs.get_expectation()\n+\n+        self.assertEqual(\n+            decoded_output,\n+            expected_output,\n+            f\"Decoded output: {decoded_output}\\nExpected output: {expected_output}\",\n+        )\n+\n+        # Check second output\n+        decoded_output = processor.decode(output[1, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True)\n+        expected_outputs = Expectations(\n+            {\n+                (\"xpu\", 3): \"The first image showcases the Statue of Liberty, a colossal neoclassical sculpture on Liberty Island in New York Harbor. Standing at \",\n+                (\"cuda\", 7): 'The first image showcases the Statue of Liberty, a monumental sculpture located on Liberty Island in New York Harbor. Standing atop a',\n+                (\"cuda\", 8): 'The two landmarks depicted in the images are the Statue of Liberty and the Golden Gate Bridge. \\n\\n1. **Statue',\n+            }\n+        )  # fmt: skip\n+        expected_output = expected_outputs.get_expectation()\n+\n+        self.assertEqual(\n+            decoded_output,\n+            expected_output,\n+            f\"Decoded output: {decoded_output}\\nExpected output: {expected_output}\",\n+        )"
        },
        {
            "sha": "6573611423ecc86b42411c9b1ff02b7c649ba40b",
            "filename": "tests/models/cohere2_vision/test_processing_cohere2_vision.py",
            "status": "added",
            "additions": 139,
            "deletions": 0,
            "changes": 139,
            "blob_url": "https://github.com/huggingface/transformers/blob/e1688d28d3cce0b3e4c59e99fb970801970231c1/tests%2Fmodels%2Fcohere2_vision%2Ftest_processing_cohere2_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e1688d28d3cce0b3e4c59e99fb970801970231c1/tests%2Fmodels%2Fcohere2_vision%2Ftest_processing_cohere2_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcohere2_vision%2Ftest_processing_cohere2_vision.py?ref=e1688d28d3cce0b3e4c59e99fb970801970231c1",
            "patch": "@@ -0,0 +1,139 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import shutil\n+import tempfile\n+import unittest\n+\n+from transformers import AutoProcessor, AutoTokenizer, Cohere2VisionProcessor\n+from transformers.testing_utils import require_read_token, require_torch, require_vision\n+from transformers.utils import is_torch_available, is_torchvision_available\n+\n+from ...test_processing_common import ProcessorTesterMixin\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+if is_torchvision_available():\n+    from transformers import Cohere2VisionImageProcessorFast\n+\n+\n+@require_read_token\n+@require_vision\n+@unittest.skip(\"Model not released yet!\")\n+class Cohere2VisionProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n+    processor_class = Cohere2VisionProcessor\n+\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.tmpdirname = tempfile.mkdtemp()\n+        image_processor = Cohere2VisionImageProcessorFast(\n+            size={\"height\": 20, \"width\": 20},\n+            max_patches=3,\n+        )\n+        tokenizer = AutoTokenizer.from_pretrained(\"CohereLabs/command-a-vision-07-2025\")\n+\n+        processor_kwargs = cls.prepare_processor_dict()\n+        processor = Cohere2VisionProcessor(\n+            image_processor=image_processor,\n+            tokenizer=tokenizer,\n+            **processor_kwargs,\n+        )\n+        processor.save_pretrained(cls.tmpdirname)\n+        cls.image_token = processor.image_token\n+\n+    @staticmethod\n+    def prepare_processor_dict():\n+        return {}\n+\n+    def get_tokenizer(self, **kwargs):\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).tokenizer\n+\n+    def get_image_processor(self, **kwargs):\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).image_processor\n+\n+    def get_processor(self, **kwargs):\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs)\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        shutil.rmtree(cls.tmpdirname, ignore_errors=True)\n+\n+    @require_torch\n+    def test_process_interleaved_images_videos(self):\n+        processor = self.get_processor()\n+\n+        messages = [\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\n+                            \"type\": \"image\",\n+                            \"url\": \"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\",\n+                        },\n+                        {\n+                            \"type\": \"image\",\n+                            \"url\": \"https://thumbs.dreamstime.com/b/golden-gate-bridge-san-francisco-purple-flowers-california-echium-candicans-36805947.jpg\",\n+                        },\n+                        {\"type\": \"text\", \"text\": \"What are the differences between these two images?\"},\n+                    ],\n+                },\n+            ],\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\n+                            \"type\": \"image\",\n+                            \"url\": \"https://llava-vl.github.io/static/images/view.jpg\",\n+                        },\n+                        {\"type\": \"text\", \"text\": \"Write a haiku for this image\"},\n+                    ],\n+                }\n+            ],\n+        ]\n+\n+        inputs_batched = processor.apply_chat_template(\n+            messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+            return_tensors=\"pt\",\n+            padding=True,\n+        )\n+\n+        # Process non batched inputs to check if the pixel_values and input_ids are reconstructed in the correct order when batched together\n+        images_patches_index = 0\n+        for i, message in enumerate(messages):\n+            inputs = processor.apply_chat_template(\n+                message,\n+                add_generation_prompt=True,\n+                tokenize=True,\n+                return_dict=True,\n+                return_tensors=\"pt\",\n+                padding=True,\n+            )\n+            # We slice with [-inputs[\"input_ids\"].shape[1] :] as the input_ids are left padded\n+            torch.testing.assert_close(\n+                inputs[\"input_ids\"][0], inputs_batched[\"input_ids\"][i][-inputs[\"input_ids\"].shape[1] :]\n+            )\n+            torch.testing.assert_close(\n+                inputs[\"pixel_values\"],\n+                inputs_batched[\"pixel_values\"][\n+                    images_patches_index : images_patches_index + inputs[\"pixel_values\"].shape[0]\n+                ],\n+            )\n+            images_patches_index += inputs[\"pixel_values\"].shape[0]"
        },
        {
            "sha": "e60537e302f51acf7cf912e3e77baf9b6552b907",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/e1688d28d3cce0b3e4c59e99fb970801970231c1/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e1688d28d3cce0b3e4c59e99fb970801970231c1/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=e1688d28d3cce0b3e4c59e99fb970801970231c1",
            "patch": "@@ -4677,9 +4677,13 @@ def update_config_for_flex(config):\n                 sub_config = getattr(config, key)\n                 update_config_for_flex(sub_config)\n \n-            model = model_class(config).to(device=torch_device)\n-            model.set_attn_implementation(\"flex_attention\")\n-            self.assertTrue(model.config._attn_implementation == \"flex_attention\")\n+            if model_class._can_set_attn_implementation():\n+                model = model_class(config).to(device=torch_device)\n+                model.set_attn_implementation(\"flex_attention\")\n+                self.assertTrue(model.config._attn_implementation == \"flex_attention\")\n+            else:\n+                config._attn_implementation = \"flex_attention\"\n+                model = model_class(config).to(device=torch_device)\n \n             # Elaborate workaround for encoder-decoder models as some do not specify their main input\n             dummy_inputs = {model.main_input_name: inputs_dict[model.main_input_name].to(torch_device)}"
        }
    ],
    "stats": {
        "total": 2423,
        "additions": 2375,
        "deletions": 48
    }
}