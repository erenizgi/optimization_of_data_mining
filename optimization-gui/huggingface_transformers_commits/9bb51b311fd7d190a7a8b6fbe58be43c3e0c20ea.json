{
    "author": "githubnemo",
    "message": "Share embedding modules in BART, not only weights (#41821)\n\n* Share embedding modules in BART, not only weights\n\nEmbedding modules are now shared between encoder, decoder\nand shared - it is the same module, like in the T5 implementation.\n\nThis has the benefit that it does not matter which module is returned\nin `get_input_embeddings`, the caller of the latter can be sure that\nmodifications done to that (e.g., hooks) apply to the embeddings.\n\nBackground: While revamping the gradient checkpointing tests in PEFT via\npeft#2860 we found that the gradient enable step\n(`modeling_utils.enable_input_require_grads`) does not work for BART.\nThis leads to gradient checkpointing with `use_reentrant=True` to\nfail as it will not detect any gradients. The reason for this is that\nthe returned value by `get_input_embeddings` (`self.shared`) is not\nthe module that is called in the encoder, therefore any hooks added\nto `self.shared` are not run - in this case the hook set by\n`enable_input_require_grads`.\n\nSince the background is a missing hook I've added a test that tests\ndirectly the ability to define hooks and their ability to being called.\n\n* Add explanatory comment\n\n* Don't initialize embeddings when not neccessary\n\n* make fix-copies\n\n---------\n\nCo-authored-by: nemo <git@ningu.net>",
    "sha": "9bb51b311fd7d190a7a8b6fbe58be43c3e0c20ea",
    "files": [
        {
            "sha": "b903becf5e9c7a03e754bb012acba210b7dc231a",
            "filename": "src/transformers/models/bart/modeling_bart.py",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/9bb51b311fd7d190a7a8b6fbe58be43c3e0c20ea/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9bb51b311fd7d190a7a8b6fbe58be43c3e0c20ea/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py?ref=9bb51b311fd7d190a7a8b6fbe58be43c3e0c20ea",
            "patch": "@@ -538,12 +538,12 @@ def __init__(self, config: BartConfig, embed_tokens: Optional[nn.Embedding] = No\n         self.max_source_positions = config.max_position_embeddings\n         embed_scale = math.sqrt(embed_dim) if config.scale_embedding else 1.0\n \n-        self.embed_tokens = BartScaledWordEmbedding(\n-            config.vocab_size, embed_dim, self.padding_idx, embed_scale=embed_scale\n-        )\n-\n         if embed_tokens is not None:\n-            self.embed_tokens.weight = embed_tokens.weight\n+            self.embed_tokens = embed_tokens\n+        else:\n+            self.embed_tokens = BartScaledWordEmbedding(\n+                config.vocab_size, embed_dim, self.padding_idx, embed_scale=embed_scale\n+            )\n \n         self.embed_positions = BartLearnedPositionalEmbedding(\n             config.max_position_embeddings,\n@@ -682,12 +682,12 @@ def __init__(self, config: BartConfig, embed_tokens: Optional[nn.Embedding] = No\n         self.max_target_positions = config.max_position_embeddings\n         embed_scale = math.sqrt(config.d_model) if config.scale_embedding else 1.0\n \n-        self.embed_tokens = BartScaledWordEmbedding(\n-            config.vocab_size, config.d_model, self.padding_idx, embed_scale=embed_scale\n-        )\n-\n         if embed_tokens is not None:\n-            self.embed_tokens.weight = embed_tokens.weight\n+            self.embed_tokens = embed_tokens\n+        else:\n+            self.embed_tokens = BartScaledWordEmbedding(\n+                config.vocab_size, config.d_model, self.padding_idx, embed_scale=embed_scale\n+            )\n \n         self.embed_positions = BartLearnedPositionalEmbedding(\n             config.max_position_embeddings,"
        },
        {
            "sha": "9a80a46f6265ddc30effaac8748a3c888a9cbf75",
            "filename": "src/transformers/models/plbart/modeling_plbart.py",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/9bb51b311fd7d190a7a8b6fbe58be43c3e0c20ea/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9bb51b311fd7d190a7a8b6fbe58be43c3e0c20ea/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py?ref=9bb51b311fd7d190a7a8b6fbe58be43c3e0c20ea",
            "patch": "@@ -343,12 +343,12 @@ def __init__(self, config: PLBartConfig, embed_tokens: Optional[nn.Embedding] =\n         self.max_source_positions = config.max_position_embeddings\n         embed_scale = math.sqrt(embed_dim) if config.scale_embedding else 1.0\n \n-        self.embed_tokens = PLBartScaledWordEmbedding(\n-            config.vocab_size, embed_dim, self.padding_idx, embed_scale=embed_scale\n-        )\n-\n         if embed_tokens is not None:\n-            self.embed_tokens.weight = embed_tokens.weight\n+            self.embed_tokens = embed_tokens\n+        else:\n+            self.embed_tokens = PLBartScaledWordEmbedding(\n+                config.vocab_size, embed_dim, self.padding_idx, embed_scale=embed_scale\n+            )\n \n         self.embed_positions = PLBartLearnedPositionalEmbedding(\n             config.max_position_embeddings,\n@@ -595,12 +595,12 @@ def __init__(self, config: PLBartConfig, embed_tokens: Optional[nn.Embedding] =\n         self.max_target_positions = config.max_position_embeddings\n         embed_scale = math.sqrt(config.d_model) if config.scale_embedding else 1.0\n \n-        self.embed_tokens = PLBartScaledWordEmbedding(\n-            config.vocab_size, config.d_model, self.padding_idx, embed_scale=embed_scale\n-        )\n-\n         if embed_tokens is not None:\n-            self.embed_tokens.weight = embed_tokens.weight\n+            self.embed_tokens = embed_tokens\n+        else:\n+            self.embed_tokens = PLBartScaledWordEmbedding(\n+                config.vocab_size, config.d_model, self.padding_idx, embed_scale=embed_scale\n+            )\n \n         self.embed_positions = PLBartLearnedPositionalEmbedding(\n             config.max_position_embeddings,"
        },
        {
            "sha": "eabff66bc6bc02d333daa06ee41b74c76430a4c1",
            "filename": "tests/models/bart/test_modeling_bart.py",
            "status": "modified",
            "additions": 18,
            "deletions": 0,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/9bb51b311fd7d190a7a8b6fbe58be43c3e0c20ea/tests%2Fmodels%2Fbart%2Ftest_modeling_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9bb51b311fd7d190a7a8b6fbe58be43c3e0c20ea/tests%2Fmodels%2Fbart%2Ftest_modeling_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbart%2Ftest_modeling_bart.py?ref=9bb51b311fd7d190a7a8b6fbe58be43c3e0c20ea",
            "patch": "@@ -16,6 +16,7 @@\n import copy\n import tempfile\n import unittest\n+import unittest.mock\n from functools import cached_property\n \n import timeout_decorator  # noqa\n@@ -477,6 +478,23 @@ def test_inputs_embeds(self):\n             with torch.no_grad():\n                 model(**inputs)[0]\n \n+    def test_input_embeddings_support_forward_hook(self):\n+        # Make sure that registering hooks on the input embeddings are indeed called\n+        # in forward. This is necessary for gradient checkpointing in PEFT, see also #41821.\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs()\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            hook = unittest.mock.MagicMock(return_value=None)\n+            model.get_input_embeddings().register_forward_hook(hook)\n+\n+            inputs = copy.deepcopy(self._prepare_for_class(inputs_dict, model_class))\n+            model(**inputs)\n+\n+            self.assertGreater(hook.call_count, 0)\n+\n     @require_torch_fp16\n     def test_generate_fp16(self):\n         config, input_dict = self.model_tester.prepare_config_and_inputs()"
        }
    ],
    "stats": {
        "total": 58,
        "additions": 38,
        "deletions": 20
    }
}