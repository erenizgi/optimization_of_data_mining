{
    "author": "qubvel",
    "message": "[modular] speedup check_modular_conversion with multiprocessing (#37456)\n\n* Change topological sort to return level-based output (lists of lists)\n\n* Update main for modular converter\n\n* Update test\n\n* update check_modular_conversion\n\n* Update gitignore\n\n* Fix missing conversion for glm4\n\n* Update\n\n* Fix error msg\n\n* Fixup\n\n* fix docstring\n\n* update docs\n\n* Add comment\n\n* delete qwen3_moe",
    "sha": "fe1a5b73e62c54adafcedb47ac2430e058dc4312",
    "files": [
        {
            "sha": "cdf189505dc73e798046de3f631b281d08aba96a",
            "filename": ".gitignore",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/fe1a5b73e62c54adafcedb47ac2430e058dc4312/.gitignore",
            "raw_url": "https://github.com/huggingface/transformers/raw/fe1a5b73e62c54adafcedb47ac2430e058dc4312/.gitignore",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.gitignore?ref=fe1a5b73e62c54adafcedb47ac2430e058dc4312",
            "patch": "@@ -167,3 +167,6 @@ tags\n \n # ruff\n .ruff_cache\n+\n+# modular conversion\n+*.modular_backup"
        },
        {
            "sha": "65d31c2203c21dccc2f75f7b23313274982ba5e1",
            "filename": "tests/repo_utils/modular/test_conversion_order.py",
            "status": "modified",
            "additions": 14,
            "deletions": 4,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/fe1a5b73e62c54adafcedb47ac2430e058dc4312/tests%2Frepo_utils%2Fmodular%2Ftest_conversion_order.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fe1a5b73e62c54adafcedb47ac2430e058dc4312/tests%2Frepo_utils%2Fmodular%2Ftest_conversion_order.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Frepo_utils%2Fmodular%2Ftest_conversion_order.py?ref=fe1a5b73e62c54adafcedb47ac2430e058dc4312",
            "patch": "@@ -38,20 +38,29 @@\n     os.path.join(MODEL_ROOT, \"mistral\", \"modular_mistral.py\"),\n     os.path.join(MODEL_ROOT, \"phi3\", \"modular_phi3.py\"),\n     os.path.join(MODEL_ROOT, \"cohere\", \"modular_cohere.py\"),\n+    os.path.join(MODEL_ROOT, \"glm4\", \"modular_glm4.py\"),\n ]\n \n \n-def appear_after(model1: str, model2: str, priority_list: list[str]) -> bool:\n+def appear_after(model1: str, model2: str, priority_list: list[list[str]]) -> bool:\n     \"\"\"Return True if `model1` appear after `model2` in `priority_list`.\"\"\"\n-    return priority_list.index(model1) > priority_list.index(model2)\n+    model1_index, model2_index = None, None\n+    for i, level in enumerate(priority_list):\n+        if model1 in level:\n+            model1_index = i\n+        if model2 in level:\n+            model2_index = i\n+    if model1_index is None or model2_index is None:\n+        raise ValueError(f\"Model {model1} or {model2} not found in {priority_list}\")\n+    return model1_index > model2_index\n \n \n class ConversionOrderTest(unittest.TestCase):\n     def test_conversion_order(self):\n         # Find the order\n         priority_list, _ = create_dependency_mapping.find_priority_list(FILES_TO_PARSE)\n-        # Extract just the model names\n-        model_priority_list = [file.rsplit(\"modular_\")[-1].replace(\".py\", \"\") for file in priority_list]\n+        # Extract just the model names (list of lists)\n+        model_priority_list = [[file.split(\"/\")[-2] for file in level] for level in priority_list]\n \n         # These are based on what the current library order should be (as of 09/01/2025)\n         self.assertTrue(appear_after(\"mixtral\", \"mistral\", model_priority_list))\n@@ -62,3 +71,4 @@ def test_conversion_order(self):\n         self.assertTrue(appear_after(\"cohere2\", \"gemma2\", model_priority_list))\n         self.assertTrue(appear_after(\"cohere2\", \"cohere\", model_priority_list))\n         self.assertTrue(appear_after(\"phi3\", \"mistral\", model_priority_list))\n+        self.assertTrue(appear_after(\"glm4\", \"glm\", model_priority_list))"
        },
        {
            "sha": "15dcf418d6792de3e951e6eb4162835f4e58feaa",
            "filename": "utils/check_modular_conversion.py",
            "status": "modified",
            "additions": 83,
            "deletions": 38,
            "changes": 121,
            "blob_url": "https://github.com/huggingface/transformers/blob/fe1a5b73e62c54adafcedb47ac2430e058dc4312/utils%2Fcheck_modular_conversion.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fe1a5b73e62c54adafcedb47ac2430e058dc4312/utils%2Fcheck_modular_conversion.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_modular_conversion.py?ref=fe1a5b73e62c54adafcedb47ac2430e058dc4312",
            "patch": "@@ -2,7 +2,11 @@\n import difflib\n import glob\n import logging\n+import multiprocessing\n+import os\n+import shutil\n import subprocess\n+from functools import partial\n from io import StringIO\n \n from create_dependency_mapping import find_priority_list\n@@ -17,8 +21,15 @@\n logging.getLogger().setLevel(logging.ERROR)\n console = Console()\n \n+BACKUP_EXT = \".modular_backup\"\n \n-def process_file(modular_file_path, generated_modeling_content, file_type=\"modeling_\", fix_and_overwrite=False):\n+\n+def process_file(\n+    modular_file_path,\n+    generated_modeling_content,\n+    file_type=\"modeling_\",\n+    show_diff=True,\n+):\n     file_name_prefix = file_type.split(\"*\")[0]\n     file_name_suffix = file_type.split(\"*\")[-1] if \"*\" in file_type else \"\"\n     file_path = modular_file_path.replace(\"modular_\", f\"{file_name_prefix}_\").replace(\".py\", f\"{file_name_suffix}.py\")\n@@ -38,11 +49,14 @@ def process_file(modular_file_path, generated_modeling_content, file_type=\"model\n     diff_list = list(diff)\n     # Check for differences\n     if diff_list:\n-        if fix_and_overwrite:\n-            with open(file_path, \"w\", encoding=\"utf-8\", newline=\"\\n\") as modeling_file:\n-                modeling_file.write(generated_modeling_content[file_type][0])\n-            console.print(f\"[bold blue]Overwritten {file_path} with the generated content.[/bold blue]\")\n-        else:\n+        # first save the copy of the original file, to be able to restore it later\n+        if os.path.exists(file_path):\n+            shutil.copy(file_path, file_path + BACKUP_EXT)\n+        # we always save the generated content, to be able to update dependant files\n+        with open(file_path, \"w\", encoding=\"utf-8\", newline=\"\\n\") as modeling_file:\n+            modeling_file.write(generated_modeling_content[file_type][0])\n+        console.print(f\"[bold blue]Overwritten {file_path} with the generated content.[/bold blue]\")\n+        if show_diff:\n             console.print(f\"\\n[bold red]Differences found between the generated code and {file_path}:[/bold red]\\n\")\n             diff_text = \"\\n\".join(diff_list)\n             syntax = Syntax(diff_text, \"diff\", theme=\"ansi_dark\", line_numbers=True)\n@@ -53,12 +67,12 @@ def process_file(modular_file_path, generated_modeling_content, file_type=\"model\n         return 0\n \n \n-def compare_files(modular_file_path, fix_and_overwrite=False):\n+def compare_files(modular_file_path, show_diff=True):\n     # Generate the expected modeling content\n     generated_modeling_content = convert_modular_file(modular_file_path)\n     diff = 0\n     for file_type in generated_modeling_content.keys():\n-        diff += process_file(modular_file_path, generated_modeling_content, file_type, fix_and_overwrite)\n+        diff += process_file(modular_file_path, generated_modeling_content, file_type, show_diff)\n     return diff\n \n \n@@ -120,16 +134,20 @@ def guaranteed_no_diff(modular_file_path, dependencies, models_in_diff):\n     parser.add_argument(\n         \"--fix_and_overwrite\", action=\"store_true\", help=\"Overwrite the modeling_xxx.py file if differences are found.\"\n     )\n+    parser.add_argument(\"--check_all\", action=\"store_true\", help=\"Check all files, not just the ones in the diff.\")\n     parser.add_argument(\n         \"--num_workers\",\n-        default=1,\n+        default=-1,\n         type=int,\n-        help=\"The number of workers to run. No effect if `fix_and_overwrite` is specified.\",\n+        help=\"The number of workers to run. Default is -1, which means the number of CPU cores.\",\n     )\n     args = parser.parse_args()\n     if args.files == [\"all\"]:\n         args.files = glob.glob(\"src/transformers/models/**/modular_*.py\", recursive=True)\n \n+    if args.num_workers == -1:\n+        args.num_workers = multiprocessing.cpu_count()\n+\n     # Assuming there is a topological sort on the dependency mapping: if the file being checked and its dependencies\n     # are not in the diff, then there it is guaranteed to have no differences. If no models are in the diff, then this\n     # script will do nothing.\n@@ -141,44 +159,71 @@ def guaranteed_no_diff(modular_file_path, dependencies, models_in_diff):\n         models_in_diff = {file_path.split(\"/\")[-2] for file_path in args.files}\n     else:\n         models_in_diff = get_models_in_diff()\n-        if not models_in_diff:\n+        if not models_in_diff and not args.check_all:\n             console.print(\n                 \"[bold green]No models files or model tests in the diff, skipping modular checks[/bold green]\"\n             )\n             exit(0)\n \n     skipped_models = set()\n-    non_matching_files = 0\n+    non_matching_files = []\n     ordered_files, dependencies = find_priority_list(args.files)\n-    if args.fix_and_overwrite or args.num_workers == 1:\n-        for modular_file_path in ordered_files:\n-            is_guaranteed_no_diff = guaranteed_no_diff(modular_file_path, dependencies, models_in_diff)\n-            if is_guaranteed_no_diff:\n-                model_name = modular_file_path.rsplit(\"modular_\", 1)[1].replace(\".py\", \"\")\n-                skipped_models.add(model_name)\n+    flat_ordered_files = [item for sublist in ordered_files for item in sublist]\n+\n+    # ordered_files is a *sorted* list of lists of filepaths\n+    #  - files from the first list do NOT depend on other files\n+    #  - files in the second list depend on files from the first list\n+    #  - files in the third list depend on files from the second and (optionally) the first list\n+    #  - ... and so on\n+    # files (models) within the same list are *independent* of each other;\n+    # we start applying modular conversion to each list in parallel, starting from the first list\n+\n+    console.print(f\"[bold yellow]Number of dependency levels: {len(ordered_files)}[/bold yellow]\")\n+    console.print(f\"[bold yellow]Files per level: {tuple([len(x) for x in ordered_files])}[/bold yellow]\")\n+\n+    try:\n+        for dependency_level_files in ordered_files:\n+            # Filter files guaranteed no diff\n+            files_to_check = []\n+            for file_path in dependency_level_files:\n+                if not args.check_all and guaranteed_no_diff(file_path, dependencies, models_in_diff):\n+                    skipped_models.add(file_path.split(\"/\")[-2])  # save model folder name\n+                else:\n+                    files_to_check.append(file_path)\n+\n+            if not files_to_check:\n                 continue\n-            non_matching_files += compare_files(modular_file_path, args.fix_and_overwrite)\n-            if current_branch != \"main\":\n-                models_in_diff = get_models_in_diff()  # When overwriting, the diff changes\n-    else:\n-        new_ordered_files = []\n-        for modular_file_path in ordered_files:\n-            is_guaranteed_no_diff = guaranteed_no_diff(modular_file_path, dependencies, models_in_diff)\n-            if is_guaranteed_no_diff:\n-                model_name = modular_file_path.rsplit(\"modular_\", 1)[1].replace(\".py\", \"\")\n-                skipped_models.add(model_name)\n-            else:\n-                new_ordered_files.append(modular_file_path)\n-\n-        import multiprocessing\n-\n-        with multiprocessing.Pool(args.num_workers) as p:\n-            outputs = p.map(compare_files, new_ordered_files)\n-        for output in outputs:\n-            non_matching_files += output\n+\n+            # Process files with diff\n+            num_workers = min(args.num_workers, len(files_to_check))\n+            with multiprocessing.Pool(num_workers) as p:\n+                is_changed_flags = p.map(\n+                    partial(compare_files, show_diff=not args.fix_and_overwrite),\n+                    files_to_check,\n+                )\n+\n+            # Collect changed files and their original paths\n+            for is_changed, file_path in zip(is_changed_flags, files_to_check):\n+                if is_changed:\n+                    non_matching_files.append(file_path)\n+\n+                    # Update changed models, after each round of conversions\n+                    # (save model folder name)\n+                    models_in_diff.add(file_path.split(\"/\")[-2])\n+\n+    finally:\n+        # Restore overwritten files by modular (if needed)\n+        backup_files = glob.glob(\"**/*\" + BACKUP_EXT, recursive=True)\n+        for backup_file_path in backup_files:\n+            overwritten_path = backup_file_path.replace(BACKUP_EXT, \"\")\n+            if not args.fix_and_overwrite and os.path.exists(overwritten_path):\n+                shutil.copy(backup_file_path, overwritten_path)\n+            os.remove(backup_file_path)\n \n     if non_matching_files and not args.fix_and_overwrite:\n-        raise ValueError(\"Some diff and their modeling code did not match.\")\n+        diff_models = set(file_path.split(\"/\")[-2] for file_path in non_matching_files)  # noqa\n+        models_str = \"\\n - \" + \"\\n - \".join(sorted(diff_models))\n+        raise ValueError(f\"Some diff and their modeling code did not match. Models in diff:{models_str}\")\n \n     if skipped_models:\n         console.print("
        },
        {
            "sha": "7590fda98c2d999c079529d45a176830cd9d0ba2",
            "filename": "utils/create_dependency_mapping.py",
            "status": "modified",
            "additions": 26,
            "deletions": 5,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/fe1a5b73e62c54adafcedb47ac2430e058dc4312/utils%2Fcreate_dependency_mapping.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fe1a5b73e62c54adafcedb47ac2430e058dc4312/utils%2Fcreate_dependency_mapping.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcreate_dependency_mapping.py?ref=fe1a5b73e62c54adafcedb47ac2430e058dc4312",
            "patch": "@@ -3,7 +3,19 @@\n \n \n # Function to perform topological sorting\n-def topological_sort(dependencies: dict):\n+def topological_sort(dependencies: dict) -> list[list[str]]:\n+    \"\"\"Given the dependencies graph construct sorted list of list of modular files\n+\n+    For example, returned list of lists might be:\n+        [\n+            [\"../modular_llama.py\", \"../modular_gemma.py\"],    # level 0\n+            [\"../modular_llama4.py\", \"../modular_gemma2.py\"],  # level 1\n+            [\"../modular_glm4.py\"],                            # level 2\n+        ]\n+        which means llama and gemma do not depend on any other modular models, while llama4 and gemma2\n+        depend on the models in the first list, and glm4 depends on the models in the second and (optionally) in the first list.\n+    \"\"\"\n+\n     # Nodes are the name of the models to convert (we only add those to the graph)\n     nodes = {node.rsplit(\"modular_\", 1)[1].replace(\".py\", \"\") for node in dependencies.keys()}\n     # This will be a graph from models to convert, to models to convert that should be converted before (as they are a dependency)\n@@ -20,12 +32,12 @@ def topological_sort(dependencies: dict):\n     while len(graph) > 0:\n         # Find the nodes with 0 out-degree\n         leaf_nodes = {node for node in graph if len(graph[node]) == 0}\n-        # Add them to the list\n-        sorting_list += list(leaf_nodes)\n+        # Add them to the list as next level\n+        sorting_list.append([name_mapping[node] for node in leaf_nodes])\n         # Remove the leafs from the graph (and from the deps of other nodes)\n         graph = {node: deps - leaf_nodes for node, deps in graph.items() if node not in leaf_nodes}\n \n-    return [name_mapping[x] for x in sorting_list]\n+    return sorting_list\n \n \n # Function to extract class and import info from a file\n@@ -63,7 +75,16 @@ def find_priority_list(py_files):\n         py_files: List of paths to the modular files\n \n     Returns:\n-        A tuple with the ordered files (list) and their dependencies (dict)\n+        Ordered list of lists of files and their dependencies (dict)\n+\n+        For example, ordered_files might be:\n+        [\n+            [\"../modular_llama.py\", \"../modular_gemma.py\"],    # level 0\n+            [\"../modular_llama4.py\", \"../modular_gemma2.py\"],  # level 1\n+            [\"../modular_glm4.py\"],                            # level 2\n+        ]\n+        which means llama and gemma do not depend on any other modular models, while llama4 and gemma2\n+        depend on the models in the first list, and glm4 depends on the models in the second and (optionally) in the first list.\n     \"\"\"\n     dependencies = map_dependencies(py_files)\n     ordered_files = topological_sort(dependencies)"
        },
        {
            "sha": "731839b0629baccc5a687a234233e274ded3479e",
            "filename": "utils/modular_model_converter.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/fe1a5b73e62c54adafcedb47ac2430e058dc4312/utils%2Fmodular_model_converter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fe1a5b73e62c54adafcedb47ac2430e058dc4312/utils%2Fmodular_model_converter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fmodular_model_converter.py?ref=fe1a5b73e62c54adafcedb47ac2430e058dc4312",
            "patch": "@@ -1775,6 +1775,7 @@ def save_modeling_file(modular_file, converted_file):\n                 files_to_parse[i] = full_path\n \n     priority_list, _ = find_priority_list(files_to_parse)\n+    priority_list = [item for sublist in priority_list for item in sublist]  # flatten the list of lists\n     assert len(priority_list) == len(files_to_parse), \"Some files will not be converted\"\n \n     for file_name in priority_list:"
        }
    ],
    "stats": {
        "total": 174,
        "additions": 127,
        "deletions": 47
    }
}