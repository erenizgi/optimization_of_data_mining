{
    "author": "secrettoad",
    "message": "add docstring example for compute_loss_func (#35020)",
    "sha": "f0dec874f08a236ffa8b33d009dbcfa27122ddac",
    "files": [
        {
            "sha": "13d9d45f19a88f5a12bc41e03711fa56a5097185",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0dec874f08a236ffa8b33d009dbcfa27122ddac/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0dec874f08a236ffa8b33d009dbcfa27122ddac/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=f0dec874f08a236ffa8b33d009dbcfa27122ddac",
            "patch": "@@ -360,8 +360,7 @@ class Trainer:\n             inner layers, dropout probabilities etc).\n         compute_loss_func (`Callable`, *optional*):\n             A function that accepts the raw model outputs, labels, and the number of items in the entire accumulated\n-            batch (batch_size * gradient_accumulation_steps) and returns the loss. For example, here is one using\n-            the loss function from `transformers`\n+            batch (batch_size * gradient_accumulation_steps) and returns the loss. For example, see the default [loss function](https://github.com/huggingface/transformers/blob/052e652d6d53c2b26ffde87e039b723949a53493/src/transformers/trainer.py#L3618) used by [`Trainer`].\n         compute_metrics (`Callable[[EvalPrediction], Dict]`, *optional*):\n             The function that will be used to compute metrics at evaluation. Must take a [`EvalPrediction`] and return\n             a dictionary string to metric values. *Note* When passing TrainingArgs with `batch_eval_metrics` set to"
        }
    ],
    "stats": {
        "total": 3,
        "additions": 1,
        "deletions": 2
    }
}